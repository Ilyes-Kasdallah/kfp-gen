[
  {
    "repo": "kubeflow/pipelines",
    "file": "snowflake_unload_data.py",
    "simplified_prompt": "Create a pipeline called `snowflake_data_unload` to do a single operation: unloading data from a Snowflake database to a Google Cloud Storage (GCS) bucket.\n\nIt includes one component: `snowflake_unload_op`. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "kubeflow/pipelines",
    "file": "after.py",
    "simplified_prompt": "Create a pipeline called `pipeline-with-after` to do sequential execution of three tasks.  It includes three components, each using a custom containerized component `Print Text`.\n\nThe `Print Text` component takes a single string input named `text` and outputs nothing.  It simply prints the input text to standard output using an Alpine Linux container.\n\nThe pipeline's workflow is as follows:\n\n1. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "kubeflow/pipelines",
    "file": "fail_v2.py",
    "simplified_prompt": "Create a pipeline called `fail-pipeline` to do a single failure operation.\n\nIt includes 1 component:\n\n* **`fail` component:** This component simply exits with a return code of 1, simulating a failure.  It has no inputs or outputs.\n\nThe pipeline's control flow is straightforward; it only executes the `fail` component.  No parallel execution or conditional logic is present. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "kubeflow/pipelines",
    "file": "lightweight_python_functions_v2_pipeline.py",
    "simplified_prompt": "Create a pipeline called `lightweight_python_functions_v2` to do data preprocessing and model training.  It includes two components:\n\n1. **`preprocess` component:** This component takes a string message as input (`message`). It outputs:\n    - `output_dataset_one`: A Dataset written to a file containing the input message. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "kubeflow/pipelines",
    "file": "lightweight_python_functions_v2_with_outputs.py",
    "simplified_prompt": "Create a pipeline called `functions-with-outputs` to do a series of operations involving string concatenation, integer addition, and artifact creation.  It includes four components:\n\n1. **`concat_message`:** This component takes two string inputs (`first`, `second`) and returns their concatenation as a string output.\n\n2. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "kubeflow/pipelines",
    "file": "metrics_visualization_v2.py",
    "simplified_prompt": "Create a pipeline called `metrics_visualization_v2` to do two separate machine learning classification tasks.  It includes two components:\n\n1. **`digit_classification` component:** This component uses scikit-learn to perform digit classification using a Logistic Regression model on the Iris dataset. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "kubeflow/pipelines",
    "file": "placeholder_concat.py",
    "simplified_prompt": "Create a pipeline called `one-step-pipeline-with-concat-placeholder` to do a single string concatenation operation.  It includes one component.\n\nThis component, named \"Component with concat placeholder\", takes two string inputs: `input_one` and `input_two`.  It concatenates these inputs with a '+' symbol and the string '=three', creating the output string 'one+two=three'. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "kubeflow/pipelines",
    "file": "placeholder_if_v2.py",
    "simplified_prompt": "Create a pipeline called `one-step-pipeline-with-if-placeholder-supply-both` to do a single operation.  This pipeline consists of one component.\n\nThe pipeline has two variants:\n\n**Variant 1: `one-step-pipeline-with-if-placeholder-supply-both`**\n\nThis pipeline uses a single component, `component_op`, which takes three inputs:\n\n* `required_input`: A string (required). Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "kubeflow/pipelines",
    "file": "two_step_with_uri_placeholder.py",
    "simplified_prompt": "Create a pipeline called `two-step-with-uri-placeholders` to do two sequential steps. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "kubeflow/pipelines",
    "file": "collected_artifacts.py",
    "simplified_prompt": "Create a pipeline called `collecting_artifacts` to do data processing and model generation.  It includes 9 components.\n\n1. **`split_ids` component:** This component takes a comma-separated string of model IDs (`model_ids`) as input and returns a list of individual IDs.  It uses simple string splitting.\n\n2. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "kubeflow/pipelines",
    "file": "collected_parameters.py",
    "simplified_prompt": "Create a pipeline called `collected_param_pipeline` to do a series of operations on a list of model IDs.  It includes five components:\n\n1. **`split_ids`:** This component takes a comma-separated string of model IDs (e.g., \"s1,s2,s3\") as input and splits it into a list of individual IDs.  The output is a list of strings.\n\n2. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "kubeflow/pipelines",
    "file": "component_with_optional_inputs.py",
    "simplified_prompt": "Create a pipeline called `v2-component-optional-input` to do a single operation.  It includes one component:\n\n- **`component_op`**: This component takes several optional inputs of various types (string, boolean, dictionary, list, integer). Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "kubeflow/pipelines",
    "file": "hello_world.py",
    "simplified_prompt": "Create a pipeline called `hello-world` to do a simple greeting.  It includes one component.\n\nThis pipeline takes a single string input, `text` (defaulting to 'hi there'), and passes it to a single component.\n\nThe single component, `hello_world`, is a function that takes a string as input (`text`) and prints it to the standard output.  It then returns the same string. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "kubeflow/pipelines",
    "file": "parallel_after_dependency.py",
    "simplified_prompt": "Create a pipeline called `loop_with_after_dependency_set` to do a series of print operations with parallel execution and dependencies.  It includes three components:\n\n1. **`print_op` Component:** This component takes a string message as input and prints it to the standard output. It then returns the same message as output.  This component is used three times within the pipeline.\n\n2. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "kubeflow/pipelines",
    "file": "parallel_consume_upstream.py",
    "simplified_prompt": "Create a pipeline called `loop_consume_upstream` to do a series of file operations within a parallel loop.  It includes four components:\n\n1. **`split_input`**: This component takes a comma-separated string as input (`input: str`). It splits the string into a list of strings and returns this list (`-> list`).  The input string is 'component1,component2,component3'.\n\n2. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "kubeflow/pipelines",
    "file": "pipeline_container_no_input.py",
    "simplified_prompt": "Create a pipeline called `v2-container-component-no-input` to do a single \"hello world\" operation.\n\nIt includes one component:\n\n1. **`container_no_input`**: This component utilizes a Docker container with the image `python:3.7`.  It executes the command `echo hello world`. It has no inputs and no explicit outputs. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "kubeflow/pipelines",
    "file": "pipeline_with_env.py",
    "simplified_prompt": "Create a pipeline called `pipeline-with-env` to do environment variable checks.  It includes two components:\n\n1. **`print_env_op`**: This component prints the values of environment variables `ENV1` and `ENV2`.  It takes no inputs and produces no explicit outputs; its output is the printed text to the component's logs.  The pipeline sets the `ENV1` variable to `val1` within this component.\n\n2. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "kubeflow/pipelines",
    "file": "pipeline_with_importer.py",
    "simplified_prompt": "Create a pipeline called `pipeline-with-importer` to do a simple machine learning training task.  It includes two components:\n\n1. **`importer` Component:** This component uses the `kfp.dsl.importer` function to import a dataset from a Google Cloud Storage (GCS) URI: `gs://ml-pipeline-playground/shakespeare1.txt`.  The imported artifact is of type `Dataset`. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "kubeflow/pipelines",
    "file": "pipeline_with_input_status_state.py",
    "simplified_prompt": "Create a pipeline called `status_state_pipeline` to do a simple task with an exit handler.  It includes two components.\n\nThe first component, `echo_state`, takes a `dsl.PipelineTaskFinalStatus` object as input.  This object represents the status of a previous pipeline task. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "kubeflow/pipelines",
    "file": "pipeline_with_placeholders.py",
    "simplified_prompt": "Create a pipeline called `pipeline-with-placeholders` to do a single assertion check.  It includes 1 component.\n\nThe single component, `print_all_placeholders`, takes five string inputs representing placeholders for job and task information within a Kubeflow pipeline execution:\n\n*   `job_name`:  A string placeholder representing the pipeline job name. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "riiid/krsh",
    "file": "pipeline.py",
    "simplified_prompt": "Create a pipeline called `pipeline` to do a placeholder operation.  The pipeline contains zero components.  There are no inputs, outputs, or control flow elements defined. No specific tools or libraries are used beyond the Kubeflow Pipelines SDK itself. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "riiid/krsh",
    "file": "pipeline.py",
    "simplified_prompt": "Create a pipeline called `pipeline-1` to do no specific operations.  The pipeline currently contains zero components.  Therefore, no inputs, outputs, control flow, or specific tools/libraries are used.  The pipeline is essentially a placeholder. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "riiid/krsh",
    "file": "pipeline.py",
    "simplified_prompt": "Create a pipeline called `pipeline-2` to do no specific operations because it is currently empty.  The pipeline has zero components.  No inputs or outputs are defined. There is no control flow specified.  No external tools or libraries are used beyond the Kubeflow Pipelines SDK.  The pipeline is currently a placeholder and requires the addition of components to define its functionality. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "omerbsezer/Fast-Kubeflow",
    "file": "pipeline.py",
    "simplified_prompt": "Create a pipeline called `ML Models Pipeline` to do a classification task using five different machine learning models.  It includes six components.\n\nThe first component, named `download`, is responsible for downloading the input data. It has no input and produces a dataset as its output. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "ksalama/kubeflow-examples",
    "file": "workflow.py",
    "simplified_prompt": "Create a pipeline called `A Simple CI pipeline` to do a series of arithmetic operations.  It includes three components:\n\n1. **`add_op`:** This component takes two integer inputs, `x_value` and `y_value`, and outputs their sum as `sum`.  It is called twice in the pipeline. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "getindata/kedro-kubeflow",
    "file": "test_kfpclient.py",
    "simplified_prompt": "The provided Python code is a unit test suite for a Kubeflow client, not a Kubeflow pipeline definition.  Therefore, I cannot generate a prompt to recreate a pipeline based on this code.  The code lacks the `@dsl.pipeline` and `@dsl.component` decorators necessary to define a Kubeflow pipeline. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "getindata/kedro-kubeflow",
    "file": "test_one_pod_pipeline_generator.py",
    "simplified_prompt": "Create a pipeline called `pipeline` to do a single data transformation operation.\n\nIt includes 1 component:\n\n1. **`pipeline` component:** This component takes three parameters (`param1`, `param2`, `param3`) as input.  `param1` is a float, `param2` is an integer, and `param3` is a date. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "getindata/kedro-kubeflow",
    "file": "test_pod_per_node_pipeline_generator.py",
    "simplified_prompt": "Create a pipeline called `pipeline` to do data processing.  This pipeline consists of 6 components.\n\nThe pipeline uses the `kedro` framework and generates Kubeflow pipeline components.  It leverages Kubernetes for deployment and includes a volume management component for data persistence. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "getindata/kedro-kubeflow",
    "file": "one_pod_pipeline_generator.py",
    "simplified_prompt": "Create a pipeline called `convert_kedro_pipeline_to_kfp` to do a Kedro pipeline execution within a single Kubernetes pod.  It includes 1 component.\n\nThe single component, named implicitly based on the Kedro pipeline name passed as an argument, executes a Kedro pipeline using the `kedro run` command. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "getindata/kedro-kubeflow",
    "file": "pod_per_node_pipeline_generator.py",
    "simplified_prompt": "Create a pipeline called `convert_kedro_pipeline_to_kfp` to do the conversion of a Kedro pipeline into a Kubeflow Pipelines (KFP) container graph.  It includes a variable number of components, one for each node in the input Kedro pipeline.  The exact number of components is determined dynamically based on the `pipelines[pipeline].node_dependencies` dictionary from the Kedro context. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "sbakiu/kubeflow-spark",
    "file": "kubeflow_pipeline.py",
    "simplified_prompt": "Create a pipeline called `Spark Operator job pipeline` to do a Spark job submission and monitors its status.  It includes 3 main components:\n\n1. **Spark Job Submission:** This component uses a Kubernetes `apply` operation (loaded from `k8s-apply-component.yaml`) to submit a Spark job defined in `spark-job-python.yaml`.  The input is the Spark job definition (a YAML file). Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "FernandoLpz/Kubeflow_Pipelines",
    "file": "pipeline.py",
    "simplified_prompt": "Create a pipeline called `First Pipeline` to do a classification task using Decision Tree and Logistic Regression models.  It includes four components:\n\n1. **`download`:** This component downloads the dataset.  Its output is a dataset (presumably in a format like CSV).  It's loaded from `download_data/download_data.yaml`.\n\n2. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "ciandt-d1/chicago-taxi-forecast",
    "file": "build_pipeline.py",
    "simplified_prompt": "Create a pipeline called `Time-Series-Forecast for Chicago Taxi dataset` to do time series forecasting on the Chicago Taxi dataset.  This pipeline consists of seven components.\n\n1. **`read_metadata`**: This component reads metadata from BigQuery, determining community areas and z-normalization statistics. It uses the `gcr. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "gnovack/kubeflow-pipelines",
    "file": "pipeline.py",
    "simplified_prompt": "Create a pipeline called `Boston Housing Pipeline` to do a machine learning workflow for predicting Boston housing prices.  It includes four components:\n\n1. **Preprocess Data:** This component preprocesses the Boston housing dataset. It uses the image `gnovack/boston_pipeline_preprocessing:latest` and has no input arguments.  Its outputs are four files: `x_train.npy`, `x_test.npy`, `y_train. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "dermatologist/kedro-multimodal",
    "file": "build_kubeflow_pipeline.py",
    "simplified_prompt": "Create a pipeline called `Kedro pipeline` to do a Kedro pipeline execution.  This pipeline consists of a variable number of components, one for each node in the underlying Kedro pipeline.  The exact number of components is determined dynamically at runtime based on the Kedro pipeline definition. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "Mastercard/mastercard-labs-ml-pipeline",
    "file": "santander-trnx-classification.py",
    "simplified_prompt": "Create a pipeline called `Santander Customer Transaction Prediction` to do customer transaction classification.  It includes six components:\n\n1. **Dataflow TF Transform:** This component preprocesses the training and evaluation datasets (`train`, `evaluation` - both presumably CSV files located in GCS).  It takes as input the paths to the training and evaluation data, a schema (\"not. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "Mastercard/mastercard-labs-ml-pipeline",
    "file": "santander-trnx-classification_release.py",
    "simplified_prompt": "Create a pipeline called `Santander Customer Transaction Prediction Release Pipeline` to do model deployment and web application launching.  It includes two main components, and its behavior is conditional on the `platform` variable (either 'GCP' or other).\n\n**Components:**\n\n1. **Deployment Component:** This component, loaded from `pipeline_steps/serving/deployer/component. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "Mastercard/mastercard-labs-ml-pipeline",
    "file": "kubeflow_katib_launcher_op.py",
    "simplified_prompt": "Create a pipeline called `Kubeflow Katib Study Job Launcher` to do hyperparameter optimization using Katib.  It includes a single component.\n\nThis pipeline uses a single component, `kubeflow_studyjob_launcher_op`, which launches a Katib Study Job.  This component takes numerous arguments as input, including:\n\n* **`name`**: The name of the Katib Study. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "dvdbisong/kubeflow-for-poets",
    "file": "crypto_pipeline.py",
    "simplified_prompt": "Create a pipeline called `crypto` to do Bitcoin closing price prediction.  This pipeline consists of four components.\n\n1. **`raw_data_transfer`**: This component transfers the raw dataset from an unspecified source (likely GitHub) to a Google Cloud Storage (GCS) bucket specified by the `target_bucket` parameter.  The output is the path to the target bucket stored in `/output.txt`. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "Ark-kun/pipeline_components",
    "file": "recursive_training.py",
    "simplified_prompt": "Create a pipeline called `train_until_good_pipeline` to do iterative model training and evaluation until a specified error threshold is met.  It includes 7 components and uses XGBoost for model training and prediction, along with Pandas for data manipulation.  It leverages pre-built Kubeflow components found at a specified GitHub URL. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "Ark-kun/pipeline_components",
    "file": "continue_training_from_prod.py",
    "simplified_prompt": "Create a pipeline called `continuous_training_pipeline` to do continuous model training, potentially warm-starting from a production model.  It includes 11 components.\n\n1. **`chicago_taxi_dataset_op`:** Downloads a Chicago Taxi Trips dataset, filtered by specified start and end dates.  Output:  a CSV file containing the dataset.\n\n2. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "Ark-kun/pipeline_components",
    "file": "train_until_good.py",
    "simplified_prompt": "Create a pipeline called `train_until_low_error` to do iterative model training until a satisfactory error threshold is reached.  The pipeline uses pre-built components from the `Ark-kun/pipeline_components` GitHub repository and consists of several components orchestrated using a recursive `graph_component`.\n\nThe pipeline contains the following components:\n\n1. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "StatCan/aaw-kubeflow-mlops",
    "file": "cnn.py",
    "simplified_prompt": "Create a pipeline called `Tacos vs. Burritos` to do convolutional neural network (CNN) training.  It includes three components:\n\n1. **Exit Handler:** This component uses `curl` to send a completion message (including pipeline status) to a callback URL (`kubemlopsbot-svc.kubeflow.svc.cluster.local:8080`) upon pipeline termination. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "StatCan/aaw-kubeflow-mlops",
    "file": "cnn_databricks.py",
    "simplified_prompt": "Create a pipeline called `Tacos vs. Burritos` to do convolutional neural network (CNN) training.  It includes three components.\n\n1. **`databricks data processing`**: This component uses a Databricks notebook (image: `k8scc01covidmlopsacr.azurecr.io/mlops/databricks-notebook:latest`) to process data. It takes `dsl.RUN_ID_PLACEHOLDER` (the Kubeflow run ID) and a JSON string as parameters. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "StatCan/aaw-kubeflow-mlops",
    "file": "default.py",
    "simplified_prompt": "Create a pipeline called `Default` to do a simple training process and sends callback messages to a URL.  It includes three components:\n\n1. **Start:** This component uses a `busybox` image to print a \"Pipeline starting\" message. It includes an init container that sends a \"Training Started\" event to `kubemlopsbot-svc.kubeflow.svc.cluster.local:8080` via a `curl` command. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "redhat-ai-services/kubeflow-pipelines-examples",
    "file": "00_compiled_pipeline.py",
    "simplified_prompt": "Create a pipeline called `add_pipeline` to do two additions.  It includes two components.\n\nThe first component, named `add`, takes two float inputs, `a` and `b`, and returns their sum as a float.  It uses a base image of  `image-registry.openshift-image-registry.svc:5000/openshift/python:latest`. This component is called twice within the pipeline. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "redhat-ai-services/kubeflow-pipelines-examples",
    "file": "02_submitted_pipeline_via_route.py",
    "simplified_prompt": "Create a pipeline called `add_pipeline` to do two additions.  It includes two components.\n\nThe first component, named `add`, takes two float inputs, `a` and `b`, and returns their sum as a float.  It uses a base image of `\"image-registry.openshift-image-registry.svc:5000/openshift/python:latest\"`.  In the pipeline, this component is called with `a` as a pipeline parameter and `b` set to 4.0. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "redhat-ai-services/kubeflow-pipelines-examples",
    "file": "02_submitted_pipeline_via_service.py",
    "simplified_prompt": "Create a pipeline called `add_pipeline` to do two sequential additions.  It includes two components.\n\nThe first component, named `add`, takes two float arguments, `a` and `b`, and returns their sum as a float.  It uses a base image `image-registry.openshift-image-registry.svc:5000/openshift/python:latest`. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "redhat-ai-services/kubeflow-pipelines-examples",
    "file": "03_outputs_pipeline.py",
    "simplified_prompt": "Create a pipeline called `Submitted Pipeline` to do two arithmetic operations.  It includes two components.\n\nThe first component, `return_multiple_values`, takes two float inputs (`a` and `b`) and returns a NamedTuple containing the sum and product of these inputs as floats.  The output is a NamedTuple with fields \"sum\" and \"product\".  It uses a base image `image-registry.openshift-image-registry. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "redhat-ai-services/kubeflow-pipelines-examples",
    "file": "04_artifact_pipeline.py",
    "simplified_prompt": "Create a pipeline called `Artifact Pipeline` to do data serialization and deserialization using Python's `pickle` library.  This pipeline consists of two components.\n\nThe first component, `create_artifact`, uses a base image `image-registry.openshift-image-registry.svc:5000/openshift/python:latest`. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "redhat-ai-services/kubeflow-pipelines-examples",
    "file": "05_metrics_pipeline.py",
    "simplified_prompt": "Create a pipeline called `metrics pipeline` to do a single metric generation task.  It includes one component:\n\n1. **`produce_metrics`**: This component generates a JSON file containing two metrics: `accuracy-score` (a percentage value representing accuracy) and `mse-score` (a raw value representing mean squared error). Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "redhat-ai-services/kubeflow-pipelines-examples",
    "file": "06_visualization_pipeline.py",
    "simplified_prompt": "Create a pipeline called `Metadata Example Pipeline` to do a single visualization task.  It includes one component:\n\n1. **`confusion_matrix_viz`**: This component generates a confusion matrix in CSV format and associated metadata for visualization within the Kubeflow UI.  It takes no inputs. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "redhat-ai-services/kubeflow-pipelines-examples",
    "file": "07_container_components_pipeline.py",
    "simplified_prompt": "Create a pipeline called `container-pipeline` to do a simple addition operation.  It includes one component.\n\nThis pipeline takes two floating-point inputs: `a` (defaulting to 1.0) and `b` (defaulting to 7.0).  It uses a single component named \"add\".\n\nThe \"add\" component:\n\n* Uses the Docker image `quay.io/rhiap/kubeflow-example:latest`.\n* Executes the Python script `components/add. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "redhat-ai-services/kubeflow-pipelines-examples",
    "file": "08_additional_packages_pipeline.py",
    "simplified_prompt": "Create a pipeline called `Additional Packages Pipeline` to do data processing using scikit-learn and pandas.  It includes one component:\n\n1. **`get_iris_data` component:** This component loads the Iris dataset from scikit-learn, converts it into a Pandas DataFrame with columns \"sepalLength\", \"sepalWidth\", \"petalLength\", \"petalWidth\", and \"species\", and prints the head of the DataFrame. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "redhat-ai-services/kubeflow-pipelines-examples",
    "file": "09_secrets_cm_pipeline.py",
    "simplified_prompt": "Create a pipeline called `Env Vars Pipeline` to do two tasks to demonstrate accessing secrets and config maps.  It includes two components.\n\nThe first component, `print_envvar`, takes a single string input `env_var` representing the name of an environment variable. It then prints the value of that environment variable to the standard output. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "redhat-ai-services/kubeflow-pipelines-examples",
    "file": "10_mount_pvc_pipeline.py",
    "simplified_prompt": "Create a pipeline called `PVC Pipeline` to do two additions.  It includes two components, both named `add`.\n\nThe first `add` component takes two float inputs, `a` and `b` (where `b` is hardcoded to 4.0 within the component). It calculates their sum and outputs a float result.  A Persistent Volume Claim (PVC) named \"my-data\" is mounted to this component at `/opt/data`. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "redhat-ai-services/kubeflow-pipelines-examples",
    "file": "11_iris_training_pipeline.py",
    "simplified_prompt": "Create a pipeline called `iris_training_pipeline` to do a machine learning workflow on the Iris dataset.  It includes three components:\n\n1. **`data_prep` component:** This component preprocesses the Iris dataset. It loads the dataset using `sklearn.datasets.load_iris`, splits it into training and testing sets (70/30 split using `train_test_split` from `sklearn. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "redhat-ai-services/kubeflow-pipelines-examples",
    "file": "12_gpu_task_pipeline.py",
    "simplified_prompt": "Create a pipeline called `nvidia_smi_pipeline` to do a single GPU-based task.  It includes one component:\n\n* **`nvidia_smi` component:** This component executes the `nvidia-smi` command to retrieve information about the NVIDIA GPU. It uses the `quay.io/modh/cuda-notebooks:cuda-jupyter-minimal-ubi9-python-3.11-20250326` Docker image. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "google/vertex-pipelines-boilerplate",
    "file": "sample_pipeline.py",
    "simplified_prompt": "Create a pipeline called `sample-pipeline` to do a simple file writing operation to Google Cloud Storage (GCS).  It includes one component.\n\nThis component, named `_save_message_to_file`, takes two inputs:\n\n* `message`: A string containing the message to be written.\n* `gcs_filepath`: A string specifying the GCS path where the message should be saved. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "lynnmatrix/kfp-local",
    "file": "local_client_test.py",
    "simplified_prompt": "Create a pipeline called `test-run-local-pipeline` to do a series of operations.  It includes six components:\n\n1. **`hello` component:** This component takes a string input `name` and returns a string that concatenates \"hello \" with the input name.  For example, if the input is \"world\", the output is \"hello world\".\n\n2. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "deployKF/kubeflow-pipelines-gitops",
    "file": "pipeline.py",
    "simplified_prompt": "Create a pipeline called `my_pipeline` to do two sequential steps.\n\nIt includes 2 components:\n\n1. **`step_0`**: This component uses a Python function (`step_0__func`) to get the current UTC epoch timestamp and day of the week.  It outputs a named tuple containing `utc_epoch` (integer) and `day_of_week` (string).  No inputs are required.\n\n2. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "muhammadyaseen/kubeflow-on-linode",
    "file": "md_pipeline.py",
    "simplified_prompt": "Create a pipeline called `md-pipeline` to do Markdown visualization.  This pipeline consists of four components.\n\n1. **`write_simple_markdown_table`**: This component creates a Markdown file containing a simple table with animal names.  It takes no input and outputs a Markdown artifact (`markdown_artifact`) containing the table data. The output is a string representation of a Markdown table.\n\n2. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "muhammadyaseen/kubeflow-on-linode",
    "file": "minio_census_pipeline.py",
    "simplified_prompt": "Create a pipeline called `census_data_pipeline` to do the following operations:  This pipeline processes Census data, checking for its existence in MinIO storage and downloading it from the Census API if needed.  It consists of three components.\n\n1. **`check_if_table_data_exists_already`**: This component checks if Census table data already exists in a specified MinIO bucket. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "muhammadyaseen/kubeflow-on-linode",
    "file": "cleaning_and_prep_pipeline.py",
    "simplified_prompt": "Create a pipeline called `cleaning_and_prep_pipeline` to do data cleaning and preparation.  It includes three components:\n\n1. **`check_if_raw_data_exists_already`**: This component checks if raw data (specified by `object_name`) already exists in a Minio bucket (`bucket_name`).  It uses the `minio` Python library to interact with the Minio object storage. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "muhammadyaseen/kubeflow-on-linode",
    "file": "train_eval_pipeline.py",
    "simplified_prompt": "Create a pipeline called `train_eval_pipeline` to do a machine learning training and evaluation workflow.  It includes three components:\n\n1. **`train_eval_baseline_model`:** This containerized component trains multiple baseline models (presumably using scikit-learn). Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "e2fyi/kfx",
    "file": "__init__.py",
    "simplified_prompt": "Create a pipeline called `demo` to do text echoing and visualization.  It includes two components.\n\n**Component 1: `echo`**\n\n* **Function:** This component takes a string as input and prints it to the standard output, returning the same string. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "e2fyi/kfx",
    "file": "_artifact_location.py",
    "simplified_prompt": "Create a pipeline called `simple_pipeline` to do environment variable injection into Kubeflow pipeline tasks.  It includes two components:\n\n1. **`echo_workflow_vars`:** This component prints the value of the environment variable `WORKFLOW_NAME`. It uses the `set_workflow_env` function to inject the workflow name as an environment variable. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "e2fyi/kfx",
    "file": "_artifact_location_test.py",
    "simplified_prompt": "Create a pipeline called `test_pipeline` to do environment variable injection into a single containerized component.  It includes one component.\n\n**Component:**\n\n* **Name:** `test_op`\n* **Function:** This component is a simple function that prints the value of the environment variable `WORKFLOW_NAME`.  It does not perform any substantial computation.\n* **Inputs:**  None\n* **Outputs:** None. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "e2fyi/kfx",
    "file": "_transformers.py",
    "simplified_prompt": "Create a pipeline called `demo` to do transformations on ContainerOps.  The pipeline contains no components in the provided code that define data processing steps. Instead, it demonstrates a mechanism to apply transformations to all ContainerOps within a pipeline using `kfp.dsl.get_pipeline_conf().add_op_transformer()`. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "e2fyi/kfx",
    "file": "_transformers_test.py",
    "simplified_prompt": "Create a pipeline called `KubeflowContainerOpTransformPipeline` to do several transformations on a `ContainerOp`.  It includes a single component which applies a series of transformations to a pre-existing ContainerOp.  This component uses the `kfx.dsl._transformers.ContainerOpTransform` library. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "glukicov/llm_pipelines_demo",
    "file": "demo.py",
    "simplified_prompt": "Create a pipeline called `demo_pipeline` to do three sequential steps using the Kubeflow Pipelines DSL.\n\nIt includes 3 components:\n\n1. **`get_data` component:** This component takes a `data_source` string as input (presumably from `job_params.data_source`).  It simulates fetching data and returns a string \"data\" as its output.  The base image used is specified by `job_constants.BASE_IMAGE`.\n\n2. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "canonical/kfp-operators",
    "file": "pipeline_container_no_input.py",
    "simplified_prompt": "Create a pipeline called `v2-container-component-no-input` to do a single operation.\n\nIt includes 1 component:\n\n1. **`container_no_input`**: This component is a containerized task that utilizes a Docker image `python:3.7`.  It executes a simple `echo \"hello world\"` command.  It has no inputs and no explicitly defined outputs. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "Anvil-Late/Kubeflow_advanced_pipeline",
    "file": "pipeline.py",
    "simplified_prompt": "Create a pipeline called `Emission prediction pipeline` to do emission prediction.  It includes 9 components. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "jhammarstedt/MLOps-Kubeflow_in_GCP",
    "file": "main.py",
    "simplified_prompt": "Create a pipeline called `ml-demo` to do a machine learning workflow consisting of three components.\n\nThe pipeline uses the `kfp` and `kfp.dsl` libraries.  It leverages GCP secrets via `use_gcp_secret('user-gcp-sa')` for authentication. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "ferneutron/mlops",
    "file": "iris_pipeline.py",
    "simplified_prompt": "Create a pipeline called `The-Iris-Pipeline-v1` to do machine learning on the Iris dataset.  It includes five components:\n\n1. **Load data from BigQuery:** This component loads the Iris dataset from a BigQuery table specified by the `project_id`, `bq_dataset`, and `bq_table` parameters.  It outputs two datasets: `train_dataset` and `test_dataset`.  It uses BigQuery as a data source.\n\n2. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "ferneutron/mlops",
    "file": "data.py",
    "simplified_prompt": "Create a pipeline called `iris_pipeline` to do end-to-end machine learning on the Iris dataset.  It includes two components.\n\nThe first component, `load_data`, loads data from a Google BigQuery table.  It takes the following inputs:\n\n* `project_id`:  A string representing the Google Cloud project ID.\n* `bq_dataset`: A string representing the BigQuery dataset name. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "ferneutron/mlops",
    "file": "evaluation.py",
    "simplified_prompt": "Create a pipeline called `model_evaluation` to do model comparison and selection.  It includes two components.\n\nThe first component, implicitly named `choose_best_model`, takes three inputs: a test dataset (`test_dataset`) as a Kubeflow Dataset, a pre-trained decision tree model (`decision_tree_model`) and a pre-trained random forest model (`random_forest_model`), both as Kubeflow Models. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "ferneutron/mlops",
    "file": "models.py",
    "simplified_prompt": "Create a pipeline called `IrisClassificationPipeline` to do machine learning model training and evaluation on the Iris dataset.  It includes two components.\n\nThe first component, named `decision_tree`, takes a CSV dataset as input (`train_dataset` of type `Dataset`). It uses the `sklearn` library to train a DecisionTreeClassifier model. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "ferneutron/mlops",
    "file": "register.py",
    "simplified_prompt": "Create a pipeline called `model_upload_pipeline` to do model upload to Google Vertex AI.  It includes one component.\n\nThe single component, `upload_model`, uploads a scikit-learn model to Google Vertex AI.  It takes the following inputs:\n\n* `project_id`: (string) The Google Cloud Project ID.\n* `location`: (string) The Google Cloud region (e.g., \"us-central1\"). Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "ferneutron/mlops",
    "file": "utils.py",
    "simplified_prompt": "Create a pipeline called `iris_model_upload` to do model deployment to Vertex AI.  It includes one component.\n\nThe single component, named `upload_model`, uploads a scikit-learn model to Google Vertex AI.  It takes a single input:  `model`, which is a Kubeflow Model object representing the path to a serialized scikit-learn model file. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "iQuantC/Kubeflow-pipeline",
    "file": "kubeflow_pipeline.py",
    "simplified_prompt": "Create a pipeline called `iris_pipeline` to do machine learning on the Iris dataset.  It includes two components:\n\n1. **`load_data` component:** This component loads the Iris dataset using scikit-learn,  transforms it into a Pandas DataFrame, and saves it as a CSV file.  It uses the `sklearn` and `pandas` libraries. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "DanielAvdar/protocol-task",
    "file": "executors.py",
    "simplified_prompt": "Create a pipeline called `ArtifactProcessingPipeline` to do data processing using two components.\n\nIt includes two components:\n\n1. **`artifacttaskinitexecutor`**: This component initializes a task.  It takes a dictionary `task_params` containing task parameters, a string `task_module` specifying the task module, and produces an output artifact `output_artifact`. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "kfous/kubeflow-pipeline",
    "file": "mlflow_pipeline.py",
    "simplified_prompt": "Create a pipeline called `MLflow Logging Pipeline` to do MLflow logging.  This pipeline contains one component.\n\nThe pipeline's single component, `mlflow_logging_op`, is responsible for logging parameters and metrics to an MLflow tracking server.  It dynamically installs the `mlflow` library, sets the tracking URI to `http://host.docker. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "pharmbio/kubeflow-pipelines",
    "file": "pipeline.py",
    "simplified_prompt": "Create a pipeline called `Kensert_CNN_test` to do a CNN workflow.  It includes one component:\n\n1. **preprocessing:** This component uses the `pharmbio/pipelines-kensert-preprocess:test` Docker image. It takes the following pipeline parameters as input: `model_type`, `checkpoint_preprocess`, and `workspace_name`.  `model_type` determines the type of model used (defaulting to \"Inception_v3\"). Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "NusretOzates/autonlp-classification-kubeflow",
    "file": "pipeline.py",
    "simplified_prompt": "Create a pipeline called `MyKubeflowPipeline` to do a machine learning workflow consisting of six components.\n\nThe pipeline processes a dataset using Hugging Face's `datasets` library and trains a model using transformers.\n\n1. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "NusretOzates/autonlp-classification-kubeflow",
    "file": "pipeline_runner.py",
    "simplified_prompt": "Create a pipeline called `classification_training_pipeline` to do text classification.  It includes three components.\n\n1. **Data Ingestion Component:** This component ingests a dataset specified by the `dataset_name` and `dataset_subset` parameters.  The `dataset_name` parameter can be \"tweet_eval\" and `dataset_subset` parameter can be \"emotion\". Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "sbakiu/ml-kf-pipeline",
    "file": "tokenize_pipeline.py",
    "simplified_prompt": "Create a pipeline called `Pipeline` to do a machine learning workflow on a Reddit response dataset.  It includes four components:\n\n1. **`tokenize`**: This component takes `reddit_train.csv` as implicit input (though not explicitly defined as an argument in the code). It uses a Python script (`src.steps.tokenize.pipeline_step`) to tokenize the input data. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "Ark-kun/kfp_sdk_components",
    "file": "_python_op.py",
    "simplified_prompt": "Create a pipeline called `pipeline_from_func` to do a series of data processing steps.  It includes several components, but the provided code snippet only shows the definitions of input/output types for components rather than the components themselves. Therefore,  I cannot specify the exact number of components or their functions. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "s102401002/kubeflowPipeline0722",
    "file": "HeartDisease_prediction.py",
    "simplified_prompt": "Create a pipeline called `HeartDiseasePrediction` to do heart disease prediction.  It includes three components:\n\n1. **`load_data` component:** This component loads a CSV dataset from a URL (https://raw.githubusercontent.com/s102401002/kubeflowPipeline0722/main/heart_2020_cleaned. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "s102401002/kubeflowPipeline0722",
    "file": "compose.py",
    "simplified_prompt": "Create a pipeline called `Data_Processing_and_Hyperparameter_Tuning` to do data preprocessing and hyperparameter optimization using Katib.  It includes three components.\n\n1. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "s102401002/kubeflowPipeline0722",
    "file": "diabetes_prediction.py",
    "simplified_prompt": "Create a pipeline called `diabetes_prediction` to do diabetes prediction using a machine learning model.  It includes two components.\n\nThe first component, `load_data`, loads data from two CSV files located at specified URLs:  \"https://raw.githubusercontent.com/daniel88516/diabetes-data/main/10k.csv\" and \"https://raw.githubusercontent.com/s102401002/kubeflowPipeline/main/data1.csv\". Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "s102401002/kubeflowPipeline0722",
    "file": "hypertension_prediction.py",
    "simplified_prompt": "Create a pipeline called `HypertensionPredictionPipeline` to do hypertension prediction.  It includes three components:\n\n1. **`loadData` component:** This component loads hypertension data from a CSV file located at `https://raw.githubusercontent.com/s102401002/kubeflowPipeline0722/main/hypertension_data.csv`. It uses the `pandas` library to read the CSV. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "s102401002/kubeflowPipeline0722",
    "file": "kubeflowPipeline0722.py",
    "simplified_prompt": "Create a pipeline called `diabetes_prediction_pipeline` to do diabetes prediction.  It includes two components.\n\nThe first component, named `load_data`, loads data from two CSV files located at specified URLs. These URLs are hardcoded within the component. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "s102401002/kubeflowPipeline0722",
    "file": "kubeflowPipeline_xgboost.py",
    "simplified_prompt": "Create a pipeline called `diabetes_prediction_pipeline` to do diabetes prediction using XGBoost.  It includes three components:\n\n1. **`load_data` component:** This component downloads diabetes datasets from two specified URLs (\"https://raw.githubusercontent.com/daniel88516/diabetes-data/main/10k.csv\" and \"https://raw.githubusercontent.com/s102401002/kubeflowPipeline/main/data1.csv\"). Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "s102401002/kubeflowPipeline0722",
    "file": "kubeflow_NAS.py",
    "simplified_prompt": "Create a pipeline called `DiabetesPredictionPipeline` to do diabetes prediction using a RandomForestClassifier.  It includes two components.\n\nThe first component, `load_data`, takes a string input `nas_mount_path` specifying the location of the data on a Network Attached Storage (NAS) and outputs a CSV file (Artifact) containing preprocessed data. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "s102401002/kubeflowPipeline0722",
    "file": "stroke_prediction.py",
    "simplified_prompt": "Create a pipeline called `Stroke Prediction Pipeline` to do stroke prediction using a machine learning model.  It includes three components:\n\n1. **`load_data` component:** This component reads stroke prediction data from two CSV files located at specific GitHub URLs (`https://raw.githubusercontent.com/s102401002/kubeflowPipeline0722/main/stroke.csv` and `https://raw.githubusercontent. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "paul-sud/demo-pipeline",
    "file": "toy.py",
    "simplified_prompt": "Create a pipeline called `fastq_processing` to do quality control and visualization on FASTQ files.  It includes two components:\n\n1. **`trim` component:** This component uses Trimmomatic (via a Java subprocess) to trim a FASTQ file (`fastq`). It takes as input the path to the untrimmed FASTQ file (`fastq`), along with trimming parameters: `leading`, `trailing`, `minlen`, and `sliding_window`. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "tonouchi510/kfp-project",
    "file": "head-pose-dataset-pipeline.py",
    "simplified_prompt": "Create a pipeline called `head-pose-dataset pipeline` to do the creation of a TFRecord dataset for a head-pose pipeline.  This pipeline consists of three components:\n\n1. **`data-chunk-spliter` component:** This component takes as input the pipeline name (`pipeline`), bucket name (`bucket_name`), job ID (`job_id`), dataset path (`dataset`), and chunk size (`chunk_size`). Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "tonouchi510/kfp-project",
    "file": "head-pose-pipeline.py",
    "simplified_prompt": "Create a pipeline called `head-pose-pipeline` to do head-pose estimation training and evaluation.  It includes three components:\n\n1. **Training Component:** This component, named `training`, trains a head-pose estimation model. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "tonouchi510/kfp-project",
    "file": "hello-world-pipeline.py",
    "simplified_prompt": "Create a pipeline called `hello-world-pipeline` to do a simple \"hello world\" output.  It includes one component.\n\n**Component Details:**\n\n* **Component Name:**  `hello` (This component's internal implementation is not provided, but we assume it takes a message as input and outputs a message. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "tonouchi510/kfp-project",
    "file": "optuna-pipeline.py",
    "simplified_prompt": "Create a pipeline called `optuna pipeline` to do hyperparameter optimization using Optuna.  It includes two components:\n\n1. **`optuna-worker` component:** This component is responsible for running the hyperparameter optimization process using Optuna. It takes the following inputs:\n\n    * `pipeline_name`: (string) Name of the pipeline. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "tonouchi510/kfp-project",
    "file": "simple-training-pipeline.py",
    "simplified_prompt": "Create a pipeline called `simple-training-pipeline` to do image classification training.  It includes three components:\n\n1. **`training` component:** This component is responsible for training a machine learning model (specified by `model_type`, defaulting to \"resnet\"). Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "hiruna72/miniKF_example_pipeline",
    "file": "small_pipeline.py",
    "simplified_prompt": "Create a pipeline called `My pipeline` to do a simple data processing workflow.  It includes two components:\n\n1. **`multiply` component:** This component takes an input file path (`input_file`), a multiplier value (`multiplier`), and an output URI (`output_uri`). Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "hiruna72/miniKF_example_pipeline",
    "file": "norok_reusable_compo_pipeline.py",
    "simplified_prompt": "Create a pipeline called `My pipeline` to do a single operation using a reusable component.\n\nIt includes 1 component:\n\n1. **`echo` component:** This component takes a single input, `input_1_uri`, which is a URI (in this case, a URL pointing to a text file: `https://www.w3.org/TR/PNG/iso_8859-1.txt`). Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "shashnavad/wine-quality-mlops",
    "file": "test_pipeline.py",
    "simplified_prompt": "Create a pipeline called `wine_quality_pipeline` to do wine quality prediction.  It includes three components:\n\n1. **`validate_data` component:** This component validates the input wine quality dataset using Great Expectations. It takes a CSV file path as input and outputs a JSON file containing validation metrics.  The output is a `Metrics` artifact.\n\n2. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "shashnavad/wine-quality-mlops",
    "file": "wine_quality_pipeline.py",
    "simplified_prompt": "Create a pipeline called `wine_quality_pipeline` to do wine quality prediction.  It includes two components:\n\n1. **`preprocess` component:** This component takes a CSV file path (`data_path`) as input. It preprocesses the data by splitting it into features and labels, scaling the features using `StandardScaler` from `sklearn`, and saving the preprocessed features, labels, and the scaler object. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "shashnavad/wine-quality-mlops",
    "file": "wine_quality_pipeline-checkpoint.py",
    "simplified_prompt": "Create a pipeline called `wine_quality_pipeline` to do a complete machine learning workflow on the red wine quality dataset.  It includes three components.\n\n1. **`download_data` component:** This component downloads the red wine quality dataset from a UCI Machine Learning Repository URL (\"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"). Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "levitomer/kubeflow-pipeline-demo",
    "file": "pipeline.py",
    "simplified_prompt": "Create a pipeline called `Pipeline` to do a machine learning workflow consisting of four components.\n\nThe pipeline uses the Kubeflow Pipelines SDK (`kfp` and `dsl`) and defines its components using the `@dsl.component` decorator (implicitly in this case).  It leverages Docker containers for each component's execution.\n\nThe four components are:\n\n1. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "anifort/kubeflow-pipelines-mlops",
    "file": "pipeline.py",
    "simplified_prompt": "Create a pipeline called `A Simple CI pipeline` to do data preprocessing.  It includes one component. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "AnsealArt/MLOps-Kubeflow",
    "file": "pipeline.py",
    "simplified_prompt": "Create a pipeline called `Breast Cancer Classification Pipeline` to do breast cancer classification using three different machine learning models.  It includes five components:\n\n1. **`download_data`**: This component downloads the breast cancer dataset.  Its output is the dataset, which is passed to the subsequent model training components. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "AnsealArt/MLOps-Kubeflow",
    "file": "pipeline.py",
    "simplified_prompt": "Create a pipeline called `California Housing prediction Pipeline` to do a machine learning workflow for predicting California housing prices.  It includes four components:\n\n1. **`Download and preprocess data`:** This component downloads and preprocesses the California housing dataset. It takes `output_path`, `run_date`, and `test_size` as inputs. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "s102401002/kubeflowPipeline",
    "file": "kubeflowPipeline0720.py",
    "simplified_prompt": "Create a pipeline called `diabetes_prediction_pipeline` to do diabetes prediction using a logistic regression model.  It includes four components:\n\n1. **`load_data` component:** This component loads a diabetes dataset from a specified URL (https://raw.githubusercontent.com/daniel88516/diabetes-data/main/10k. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "s102401002/kubeflowPipeline",
    "file": "kubeflowPipeline0722.py",
    "simplified_prompt": "Create a pipeline called `diabetes_prediction_pipeline` to do diabetes prediction.  It includes two components.\n\nThe first component, named `load_data`, downloads data from two specified URLs, cleans and preprocesses it (handling missing values and converting categorical features like 'gender' to numerical representations), and then saves the resulting cleaned dataset as a CSV file. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "s102401002/kubeflowPipeline",
    "file": "kubeflowPipeline_parquet.py",
    "simplified_prompt": "Create a pipeline called `diabetes_prediction_pipeline` to do diabetes prediction using a Logistic Regression model.  It includes three components:\n\n1. **`load_data` component:** This component loads diabetes data from a remote CSV URL. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "s102401002/kubeflowPipeline",
    "file": "kubeflowPipeline_xgboost.py",
    "simplified_prompt": "Create a pipeline called `diabetes_prediction_pipeline` to do diabetes prediction.  It includes two components.\n\nThe first component, `load_data`, downloads data from two specified URLs,  \"https://raw.githubusercontent.com/daniel88516/diabetes-data/main/10k.csv\" and \"https://raw.githubusercontent.com/s102401002/kubeflowPipeline/main/data1.csv\". Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "stackdemos/yolo4",
    "file": "component.py",
    "simplified_prompt": "Create a pipeline called `process_and_download` to do data processing and artifact downloading.  It includes three components.\n\n1. **`download-artifact` component:** This component downloads a file from a URL specified as input (`url`). It uses the `curl` command within a container image  `appropriate/curl`. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "stackdemos/yolo4",
    "file": "component.py",
    "simplified_prompt": "Create a pipeline called `my_pipeline` to do a data download and subsequent model training.  It includes two components.\n\nThe first component, named `download-artifact`, downloads a file from a URL.  It uses the `curl` command within a Docker container (`appropriate/curl`).  The input to this component is a URL and an expected MD5 checksum. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "aiffel-socar-cv/kubeflow-pipeline",
    "file": "pipeline.py",
    "simplified_prompt": "Create a pipeline called `viai-retrain` to do a two-step retraining process.  It includes two components:\n\n1. **Check files:** This component, implemented using a Docker container image `tseo/check_bucket:0.3`, checks the existence and number of files in a specified location (not explicitly defined in the code).  It outputs a JSON file named `/file_nums. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "aiffel-socar-cv/kubeflow-pipeline",
    "file": "retrain_pipeline.py",
    "simplified_prompt": "Create a pipeline called `retrain` to do a model retraining process.  It includes two components:\n\n1. **`check_cnt`:** This component counts the number of newly added images in a Google Cloud Storage bucket (\"images-original\"). It takes the bucket name (`bucket_name`, implicitly \"images-original\" in the provided code) and data type (`data_type`, not explicitly used but implied) as input. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "lauramorillo/kubeflow-example",
    "file": "taxi-on-prem.py",
    "simplified_prompt": "Create a pipeline called `taxi-on-prem` to do taxi fare prediction.  This pipeline consists of three components.\n\n1. **`dataflow_tf_data_validation_op`**: This component performs data validation using TensorFlow Data Validation (TDFV) and Apache Beam.  It takes as input `inference_data`, `validation_data`, `column_names`, `key_columns`, `project`, and `mode`.  The outputs are a schema (`/schema. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "pragadeeshraju/kubeflow-sample-pipeline",
    "file": "pipeline.py",
    "simplified_prompt": "Create a pipeline called `Docker test` to do a classification task using Decision Tree and Logistic Regression.  It includes four components:\n\n1. **`getdata`:** This component loads the initial dataset.  Its output is the raw dataset.  (The specific input is not explicitly defined in the provided code.)\n\n2. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "pragadeeshraju/kubeflow-sample-pipeline",
    "file": "pipeline2.py",
    "simplified_prompt": "Create a pipeline called `KServe pipeline` to do the deployment of a KServe model.  This pipeline consists of 1 component.\n\nThe single component, loaded from a URL (`https://raw.githubusercontent.com/kubeflow/pipelines/master/components/kserve/component.yaml`), uses the `kserve` operator to deploy a model. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "dedmari/kubeflow_trident_pipeline",
    "file": "pipeline.py",
    "simplified_prompt": "Create a pipeline called `resnet_cifar10` to do end-to-end model training and deployment.  This pipeline consists of four components:\n\n1. **PreprocessOp:** This component takes an `input_dir` as input and produces preprocessed data in an `output_dir`.  It outputs a file named `/output.txt`.  The docker image used is `muneer7589/k_pipeline_preprocess:latest`.\n\n2. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "Alexander6463/Kubeflow_MNIST",
    "file": "pipeline_dev.py",
    "simplified_prompt": "Create a pipeline called `End-to-End MNIST Pipeline` to do end-to-end machine learning, including data download, model training, evaluation, export, and deployment using KFServing.  It includes five components:\n\n1. **Download Dataset:** This component downloads a dataset (`datasets.tar.gz`) from an input bucket (specified as a pipeline parameter, defaulting to \"pipelines-tutorial-data\"). Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "jonrossclaytor/kubeflow-mlb",
    "file": "baseball-pipeline-branch.py",
    "simplified_prompt": "Create a pipeline called `Sequential pipeline` to do baseball pitch type classification.  It includes 8 distinct components, processing data for three pitch types: FT, FS, and CH.\n\nThe pipeline begins with a `Collect Stats` component, which presumably gathers raw baseball statistics (no inputs/outputs explicitly defined). Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "jonrossclaytor/kubeflow-mlb",
    "file": "baseball-pipeline-enhance.py",
    "simplified_prompt": "Create a pipeline called `baseball-pipeline-enhance` to do baseball pitch type classification.  It includes 12 components.\n\nThe pipeline uses the following components:\n\n1. **Collect Stats:** This component (`collect_stats_op`) uses the image `gcr.io/ross-kubeflow/collect-stats:latest` to collect baseball statistics.  It has no explicit inputs or outputs defined in the code. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "jonrossclaytor/kubeflow-mlb",
    "file": "baseball-pipeline-single.py",
    "simplified_prompt": "Create a pipeline called `Sequential pipeline` to do baseball pitch type classification.  It includes 8 components, and utilizes the `kfp` and `kfp.gcp` libraries for pipeline definition and GCP integration respectively.  Each component is a Docker container deployed from a Google Container Registry (GCR) image.  The pipeline uses the `gcp. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "xxxtrillionarie/KubeFlow_MLOps_Pipelines",
    "file": "after.py",
    "simplified_prompt": "Create a pipeline called `pipeline-with-after` to do three sequential text printing operations.  It includes three components.\n\nThe first component, named \"Print Text\", takes a string input named `text` and prints it to standard output.  It uses an Alpine Docker image.  The first instance of this component (task1) receives the input string '1st task'. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "xxxtrillionarie/KubeFlow_MLOps_Pipelines",
    "file": "after_test.py",
    "simplified_prompt": "Create a pipeline called `my_pipeline` to do data processing and model training.  This pipeline consists of two components.\n\nThe first component, implicitly named within `after.py` (and not directly visible in the provided code), takes no explicit inputs and outputs a single artifact. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "xxxtrillionarie/KubeFlow_MLOps_Pipelines",
    "file": "cache_v2_compatible_test.py",
    "simplified_prompt": "Create a pipeline called `two_step_pipeline` to do a two-step data processing and training workflow.  It includes two components:\n\n1. **`preprocess` component:** This component takes an integer parameter `some_int` and a string parameter `uri` as input. It produces an output artifact named `output_dataset_one` (type: `system. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "xxxtrillionarie/KubeFlow_MLOps_Pipelines",
    "file": "fail.py",
    "simplified_prompt": "Create a pipeline called `fail-pipeline` to do a single operation designed to fail.\n\nIt includes 1 component:\n\n* **`fail_task`**: This component calls a function named `fail`, which is implemented in Python.  The `fail` function intentionally exits with a return code of 1, causing the pipeline to fail. It uses the `alpine:latest` Docker image.  No inputs or outputs are explicitly defined. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "xxxtrillionarie/KubeFlow_MLOps_Pipelines",
    "file": "fail_parameter_value_missing.py",
    "simplified_prompt": "Create a pipeline called `parameter_value_missing` to do a simple echo operation.  It includes one component.\n\nThis pipeline uses the `kfp.deprecated` library (note:  deprecation warning should be addressed in a real-world scenario).\n\n**Components:**\n\n1. **Echo:** This component takes a single string input named `text` and outputs the same string to standard output. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "xxxtrillionarie/KubeFlow_MLOps_Pipelines",
    "file": "fail_parameter_value_missing_test.py",
    "simplified_prompt": "Create a pipeline called `pipeline` to do a simple data processing task.  It includes a single component.\n\nThe pipeline uses the Kubeflow Pipelines (KFP) library.  There are no external tools or libraries like scikit-learn (sklearn) or Snowflake used within the components themselves. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "xxxtrillionarie/KubeFlow_MLOps_Pipelines",
    "file": "fail_test.py",
    "simplified_prompt": "Create a pipeline called `fail_pipeline` to do a simple failure test.  The pipeline contains two versions, one using the V1 legacy engine and another using the V2 engine.  Both versions contain a single component named 'fail' which is designed to fail.  This allows testing the pipeline's failure handling. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "xxxtrillionarie/KubeFlow_MLOps_Pipelines",
    "file": "fail_v2.py",
    "simplified_prompt": "Create a pipeline called `fail-pipeline` to do a single failure operation.\n\nIt includes 1 component:\n\n* **`fail` component:** This component simply exits with a return code of 1, simulating a failure. It takes no inputs and produces no outputs.  It uses the Python `sys` module.\n\nThe pipeline's control flow is straightforward; it only contains the `fail` component, executed once. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "xxxtrillionarie/KubeFlow_MLOps_Pipelines",
    "file": "legacy_data_passing.py",
    "simplified_prompt": "Create a pipeline called `data_passing_pipeline` to do data passing tests.  The pipeline contains 12 components, demonstrating various data passing methods between components and utilizing pipeline parameters.\n\nThe pipeline includes three producer components:\n\n1. `produce_anything`: This component writes the string \"produce_anything\" to an output file path. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "xxxtrillionarie/KubeFlow_MLOps_Pipelines",
    "file": "legacy_exit_handler.py",
    "simplified_prompt": "Create a pipeline called `Exit Handler` to do a simple data download and message printing operation, incorporating an exit handler.  It includes three components:\n\n1. **`GCS - Download`:** This component uses the `google/cloud-sdk:279.0.0` Docker image to download a file from a specified Google Cloud Storage (GCS) URL (`url` input). Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "xxxtrillionarie/KubeFlow_MLOps_Pipelines",
    "file": "legacy_exit_handler_test.py",
    "simplified_prompt": "Create a pipeline called `download_and_print` to do a simple data download and printing operation.  It includes a single component.\n\nThis component, `download_and_print`, downloads a file from a URL (the URL is passed as an input parameter) and then prints the contents of the downloaded file to the standard output. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "xxxtrillionarie/KubeFlow_MLOps_Pipelines",
    "file": "lightweight_python_functions_v2_pipeline.py",
    "simplified_prompt": "Create a pipeline called `lightweight_python_functions_v2` to do data preprocessing and model training.  It includes two components:\n\n1. **`preprocess` component:** This component takes a string message as input. It produces five outputs:\n    - `output_dataset_one`: A Dataset written to a file containing the input message. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "xxxtrillionarie/KubeFlow_MLOps_Pipelines",
    "file": "lightweight_python_functions_v2_with_outputs.py",
    "simplified_prompt": "Create a pipeline called `functions-with-outputs` to do a series of operations involving string concatenation, number addition, and artifact creation.  It includes four components:\n\n1. **`concat_message`:** This component takes two string inputs (`first` and `second`) and concatenates them, returning a single string output. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "xxxtrillionarie/KubeFlow_MLOps_Pipelines",
    "file": "lightweight_python_functions_v2_with_outputs_test.py",
    "simplified_prompt": "Create a pipeline called `lightweight_python_functions_v2_with_outputs` to do data processing and aggregation.  It includes three components.\n\n1. **`first_component`:** This component takes no input. It generates two strings: \"first\" and \"second\", and outputs them to an intermediate artifact.\n\n2. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "xxxtrillionarie/KubeFlow_MLOps_Pipelines",
    "file": "metrics_visualization_v1.py",
    "simplified_prompt": "Create a pipeline called `metrics-visualization-v1-pipeline` to do data visualization.  It includes five components.\n\n1. **`confusion_visualization` component:** This component generates a confusion matrix visualization.  It does not appear to take any explicit inputs or produce any explicit outputs visible in the provided code.\n\n2. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "xxxtrillionarie/KubeFlow_MLOps_Pipelines",
    "file": "metrics_visualization_v1_test.py",
    "simplified_prompt": "Create a pipeline called `metrics_visualization_v1_pipeline` to do data visualization and analysis.  This pipeline consists of a single component. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "xxxtrillionarie/KubeFlow_MLOps_Pipelines",
    "file": "metrics_visualization_v2.py",
    "simplified_prompt": "Create a pipeline called `metrics_visualization_v2` to do two classification tasks and logs their metrics.  It includes two components:\n\n1. **`digit_classification` component:** This component uses scikit-learn to perform digit classification on the Iris dataset. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "xxxtrillionarie/KubeFlow_MLOps_Pipelines",
    "file": "metrics_visualization_v2_test.py",
    "simplified_prompt": "Create a pipeline called `metrics_visualization_pipeline` to do a series of machine learning tasks and visualizes their metrics.  It includes five components:\n\n1. **`wine-classification`:** This component performs wine classification.  It has no inputs and outputs a metrics artifact (containing confidence metrics).\n\n2. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "xxxtrillionarie/KubeFlow_MLOps_Pipelines",
    "file": "parameter_with_format.py",
    "simplified_prompt": "Create a pipeline called `pipeline-with-pipelineparam-containing-format` to do a simple string manipulation and printing.  It includes two components.\n\nThe first component, `print_op`, takes a string as input (`name`) and prints it to the standard output. It then returns the input string.  This component uses no external libraries. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "xxxtrillionarie/KubeFlow_MLOps_Pipelines",
    "file": "parameter_with_format_test.py",
    "simplified_prompt": "Create a pipeline called `my_pipeline` to do data processing and model training.  This pipeline consists of a single component.\n\nThe pipeline uses the `kfp.deprecated` library (Note: this is likely using an older version of Kubeflow Pipelines and should be updated to the current `kfp` library for production use). Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "mengdong/kubeflow-pipeline-nvidia-example",
    "file": "JoC_end2end_serve.py",
    "simplified_prompt": "Create a pipeline called `End2end Resnet50 Classification` to do end-to-end image classification using a ResNet50 model.  It includes one component:\n\n1. **preprocessing:** This component, containerized with image `gcr.io/<project_name>/gcp-joc-end2end-demo-preprocessing`, downloads and preprocesses image data.  It uses a Python script (`download. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "mengdong/kubeflow-pipeline-nvidia-example",
    "file": "end2end_serve.py",
    "simplified_prompt": "Create a pipeline called `End2end Resnet50 Classification` to do end-to-end image classification using a ResNet50 model.  It includes four components:\n\n1. **Preprocessing:** This component takes raw image data from a specified input directory (`raw_data_dir`), preprocesses it, and saves the processed data to an output directory (`processed_data_dir`). Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "sanghunmoon/pytorch_classifier_pipeline",
    "file": "pipeline.py",
    "simplified_prompt": "Create a pipeline called `pytorch_classifier_test_2` to do a basic Python classification task.  It includes one component.\n\nThis pipeline uses a single Docker container.\n\n* **Component 1: `training pipeline`**: This component is defined as a `dsl.ContainerOp` using the Docker image `lego0142/pytorch_classifier:1.1`. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "tuhinaprasad28/Machine-Learning-Pipeline-using-Kubeflow",
    "file": "pipeline.py",
    "simplified_prompt": "Create a pipeline called `First Pipeline` to do a classification task using Decision Tree and Logistic Regression algorithms.  It includes four components:\n\n1. **`download`:** This component downloads the necessary data.  Its output is a dataset that serves as input to the subsequent components.  It's loaded from `download_data/download_data.yaml`.\n\n2. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "tuhinaprasad28/Machine-Learning-Pipeline-using-Kubeflow",
    "file": "pipeline.py",
    "simplified_prompt": "Create a pipeline called `First Pipeline` to do a classification task using Decision Tree and Logistic Regression models.  It includes four components:\n\n1. **`download`:** This component downloads the necessary data.  It has no inputs and produces a single output representing the downloaded dataset.  The component is loaded from `download_data/download_data.yaml`.\n\n2. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "nnkkmto/sample-vertex-pipelines",
    "file": "pipeline.py",
    "simplified_prompt": "Create a pipeline called `sample-pipeline` to do text processing.  It includes two components:\n\n1. **`split_sentence` component:** This component takes a single string as input (`sentence`) and outputs a list of strings (`words`), representing the words in the input sentence.  It's assumed this component uses basic string splitting functionality (e.g., splitting on spaces). Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "butuzov/kubeflow-pipline-pytorch-tacatron",
    "file": "pipeline.py",
    "simplified_prompt": "Create a pipeline called `example` to do a machine learning workflow.  It includes three components:\n\n1. **`dataset download`**: This component downloads a dataset from a specified URL (`https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2`) and saves it to a specified directory (`/mnt/kf/`). Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "hermesribeiro/kubeflow_pytorch",
    "file": "pipeline.py",
    "simplified_prompt": "Create a pipeline called `CIFAR Pytorch` to do a model training and evaluation workflow.  It includes two components:\n\n1. **Model Train:** This component uses the Docker image `hermesribeiro/cifar:latest` and executes the `train.py` script. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "litovn/kubeflow-autopipe",
    "file": "pipeline_manager.py",
    "simplified_prompt": "Create a pipeline called `DAG_Pipeline` to do a series of data processing steps.  It includes multiple components, the exact number determined by a YAML configuration file (`dag_path`). Each component is a Docker containerized process that takes an input path and produces an output path. The pipeline's overall structure is determined by dependencies specified in the YAML configuration file. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "speg03/kfp-toolbox",
    "file": "test_cli.py",
    "simplified_prompt": "Create a pipeline called `echo-pipeline` to do a simple echo operation.  It includes one component.\n\n**Components:**\n\n* **`echo` component:** This component takes no input and returns a string literal \"hello, world\".  It uses no external tools or libraries.\n\n**Pipeline Structure:**\n\nThe pipeline is straightforward; it simply executes the `echo` component once. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "speg03/kfp-toolbox",
    "file": "test_decorators.py",
    "simplified_prompt": "Create a pipeline called `echo-pipeline` to do a simple echo operation.  It includes one component.\n\n**Component Details:**\n\n* **Component Name:** `echo`\n* **Function:** This component takes no input and returns the string \"hello, world\".\n* **Inputs:** None\n* **Outputs:** A string output named `output`. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "speg03/kfp-toolbox",
    "file": "test_pipeline_parser.py",
    "simplified_prompt": "Create a pipeline called `echo-pipeline` to do a simple echo operation.  This pipeline contains one component. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "speg03/kfp-toolbox",
    "file": "test_pipelines.py",
    "simplified_prompt": "Create a pipeline called `echo-pipeline` to do a simple echo operation.  It includes 1 component.\n\nThe single component, named `echo`, takes several parameters as input:\n\n* `no_default_param`: An integer with no default value.\n* `int_param`: An integer with a default value of 1.\n* `float_param`: A float with a default value of 1.5.\n* `str_param`: A string with a default value of \"string_value\". Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "speg03/kfp-toolbox",
    "file": "pipelines.py",
    "simplified_prompt": "Create a pipeline called `timestamp-pipeline` to do a single timestamp generation.  It includes 1 component.\n\nThe pipeline's single component, named `timestamp`, generates a timestamp string based on provided parameters.  \n\n* **Component:** `timestamp`\n    * **Function:** Generates a timestamp string according to a specified format. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "ZoieD/kfp-resnet",
    "file": "pipeline.py",
    "simplified_prompt": "Create a pipeline called `resnet_cifar10_pipeline` to do end-to-end training and serving of a ResNet model on the CIFAR-10 dataset.  It includes four components:\n\n1. **PreprocessOp:** This component preprocesses the raw data. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "shawar8/sfcrime-prediction-kubeflow",
    "file": "data_ingest_fns.py",
    "simplified_prompt": "Create a pipeline called `data_ingestion_pipeline` to do data preprocessing and label mapping for a crime dataset.  It includes two components.\n\n**Component 1: `preprocess_data`**\n\nThis component takes as input a CSV dataset (`df_path`) and a list of columns to keep (`to_keep`).  It also takes a string input (`data`) which determines whether the data is for training or serving. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "shawar8/sfcrime-prediction-kubeflow",
    "file": "monitoring_fns.py",
    "simplified_prompt": "Create a pipeline called `monitoring_pipeline` to do automated model monitoring and retraining.  It includes two components.\n\nThe first component, `read_bq_data`, reads data from a Google BigQuery table and a pre-trained model stored in Google Cloud Storage. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "shawar8/sfcrime-prediction-kubeflow",
    "file": "serving_fns.py",
    "simplified_prompt": "Create a pipeline called `Serving Pipeline` to do real-time data ingestion, transformation, and potentially serving (details on serving are omitted from the provided code).  It includes two components:\n\n1. **`get_data` component:** This component retrieves crime incident data from the San Francisco Open Data portal (using the `sodapy` library) for the last two days. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "shawar8/sfcrime-prediction-kubeflow",
    "file": "training_functions.py",
    "simplified_prompt": "Create a pipeline called `training_pipeline` to do data preprocessing and model training.  It includes two components:\n\n1. **`get_train_test_split` component:** This component takes a CSV dataset path (`df_path`), a label column name (`label_column`), a test size (`test_size`), and the number of tries (`n_tries`) as inputs. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "Davidnet/breast-cancer-detection-nnx-pipeline",
    "file": "pipeline.py",
    "simplified_prompt": "Create a pipeline called `CBIS-DDSM-Training-Pipeline` to do image classification on the CBIS-DDSM dataset.  It includes three components.\n\n1. **Download Dataset:** This component, named `Download Dataset`, downloads the CBIS-DDSM dataset using the `davidnet/cbis_ddsm_dataloader:1.0.2` Docker image.  It takes no input and outputs an `Artifact` containing the downloaded dataset. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "pavan-kumar-99/ml-ops",
    "file": "TestTrainSplit.py",
    "simplified_prompt": "Create a pipeline called `DataSplitPipeline` to do a train-test split on a dataset.  It includes two components:\n\n1. **`getRawData`:** This component, loaded from a remote YAML file (`https://raw.githubusercontent.com/pavan-kumar-99/ml-ops/master/components/getRawData.yaml`), retrieves a CSV dataset.  Its output is a file path to the downloaded dataset. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "pavan-kumar-99/ml-ops",
    "file": "getDataFromRawUri.py",
    "simplified_prompt": "Create a pipeline called `getDataFromRawUri` to do data retrieval and processing.  It includes one component.\n\n**Component 1: `GetRawData`**\n\n* **Function:** This component retrieves raw data from a specified URI using the `requests` library.  It takes a single input, a URI (string), and returns the raw data (presumably as a `requests.Response` object).  The component is defined using `kfp. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "pavan-kumar-99/ml-ops",
    "file": "train-plot.py",
    "simplified_prompt": "Create a pipeline called `TrainPlotPipeline` to do a machine learning training and plotting workflow.  It includes two components:\n\n1. **`traintest_op` component:** This component is loaded from a URL (`https://raw.githubusercontent.com/pavan-kumar-99/ml-ops/master/components/test-train.yaml`). Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "pavan-kumar-99/ml-ops",
    "file": "dependency-pipeline.py",
    "simplified_prompt": "Create a pipeline called `my-pipeline` to do a simple two-step process.  It includes two components:\n\n1. **`echo`:** This component is a containerized operation using the `alpine` image. It executes a shell command (`echo Hi Kubeflow`) and doesn't have any explicit inputs or outputs beyond printing to standard output.\n\n2. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "pavan-kumar-99/ml-ops",
    "file": "echo-sh.py",
    "simplified_prompt": "Create a pipeline called `my-pipeline` to do a simple echo operation.  It includes one component.\n\nThis pipeline uses a single component named \"echo\".  This component utilizes a Docker image (\"alpine\") and the `sh` command to execute the command `echo Hi Kubeflow`. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "03sarath/Kubeflow-pipelines-mlops",
    "file": "pipeline.py",
    "simplified_prompt": "Create a pipeline called `First Pipeline` to do a classification task using Decision Tree and Logistic Regression algorithms.  It includes four components:\n\n1. **`download`:** This component downloads the necessary data.  It takes no input and outputs a dataset (presumably in a format accessible to the subsequent components).  The component is loaded from `'download_data/download_data.yaml'`.\n\n2. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "rakesh283343/kubeflow-pipelines-clv",
    "file": "batch_predict_pipeline.py",
    "simplified_prompt": "Create a pipeline called `CLV Batch Predict` to do customer lifetime value (CLV) prediction.  It includes four components.\n\n1. **`load_sales_transactions`:** This component loads sales transactions data from a Google Cloud Storage (GCS) path or a BigQuery table into a BigQuery dataset. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "rakesh283343/kubeflow-pipelines-clv",
    "file": "train_pipeline.py",
    "simplified_prompt": "Create a pipeline called `CLV Training` to do Customer Lifetime Value (CLV) prediction.  It includes 7 components.\n\n1. **`load_sales_transactions`:** This component loads sales transactions data.  It takes no explicit inputs shown in the provided code, and its output is not explicitly defined. It likely outputs a dataset representing the sales transactions. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "rakesh283343/kubeflow-pipelines-clv",
    "file": "helper_components.py",
    "simplified_prompt": "Create a pipeline called `sales_pipeline` to do customer lifetime value (CLV) prediction.  It includes two components.\n\n1. **`Load transactions`:** This component loads historical sales transactions into a BigQuery table. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "anupr567/kubeflow_pipeline",
    "file": "kfp_pipeline.py",
    "simplified_prompt": "Create a pipeline called `First Pipeline` to do data extraction and preprocessing, followed by parallel model training using logistic regression and random forest classifiers.\n\nIt includes four components:\n\n1. **`extract_data` component:** This component extracts raw data.  Its output is passed as input to the `preprocess_data` component.  No specific input is explicitly shown.\n\n2. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "9rince/kfp",
    "file": "after.py",
    "simplified_prompt": "Create a pipeline called `pipeline-with-after` to do three sequential text printing operations.  It includes three components, all using a custom containerized component.\n\n* **Component 1 (task1):** This component prints the text \"1st task\" to standard output.  It has no inputs. Its output is not explicitly used in the pipeline definition. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "9rince/kfp",
    "file": "after_test.py",
    "simplified_prompt": "Create a pipeline called `my_pipeline` to do a simple data processing task.  It includes one component.\n\nThis component, called (implicitly) `my_pipeline` (because it's directly within the `@dsl.pipeline` decorator), takes no explicit inputs and produces no explicit outputs. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "9rince/kfp",
    "file": "cache_v2_test.py",
    "simplified_prompt": "Create a pipeline called `two_step_pipeline` to do a two-step data processing and model training workflow.  It includes two components:\n\n1. **`preprocess` component:** This component takes an integer input (`some_int`) and a URI input (`uri`). Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "9rince/kfp",
    "file": "fail.py",
    "simplified_prompt": "Create a pipeline called `fail_pipeline` to do a simple failure test.  It includes one component.\n\nThis pipeline uses the `kfp` library and the `alpine:latest` Docker image.\n\n**Components:**\n\n1. **`fail_task`:** This component calls a function `fail` which unconditionally exits with a return code of 1, simulating a pipeline failure. It does not take any inputs or produce any outputs. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "9rince/kfp",
    "file": "fail_parameter_value_missing.py",
    "simplified_prompt": "Create a pipeline called `parameter_value_missing` to do a simple echo operation.  It includes one component.\n\nThe single component, named `Echo`, takes a string input named `text` and outputs the same string to standard output.  It uses a container image based on `alpine` to execute the `echo` command. The input `text` is provided by a pipeline parameter named `parameter`. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "9rince/kfp",
    "file": "legacy_data_passing.py",
    "simplified_prompt": "Create a pipeline called `data_passing_pipeline` to do a comprehensive test of data passing mechanisms.  The pipeline contains 12 components, demonstrating six different data passing scenarios. Each scenario involves passing data (constant values, pipeline parameters, or component outputs) to other components, consuming them either as values or as files. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "9rince/kfp",
    "file": "legacy_data_passing_test.py",
    "simplified_prompt": "Create a pipeline called `data_passing_pipeline` to do data passing operations.  This pipeline consists of one main component, `data_passing_pipeline`,  although the exact internal structure isn't explicitly defined in the provided code.  The provided code snippet suggests the pipeline uses Kubeflow Pipelines' legacy V1 mode (`PipelineExecutionMode.V1_LEGACY`). Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "9rince/kfp",
    "file": "legacy_exit_handler.py",
    "simplified_prompt": "Create a pipeline called `Exit Handler` to do a simple data download and echo operation, demonstrating the use of an exit handler.  It includes three components:\n\n1. **`GCS - Download`:** This component uses the `google/cloud-sdk:279.0.0` Docker image and the `gsutil` command to download a file from a Google Cloud Storage (GCS) URL. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "9rince/kfp",
    "file": "legacy_exit_handler_test.py",
    "simplified_prompt": "Create a pipeline called `download_and_print_pipeline` to do a simple data download and printing operation.  It includes one component.\n\nThis component, `download_and_print`, downloads a file (the specific URL is not explicitly defined in the provided code) and then prints its contents. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "9rince/kfp",
    "file": "lightweight_python_functions_v2_pipeline.py",
    "simplified_prompt": "Create a pipeline called `lightweight_python_functions_v2_pipeline` to do data preprocessing and model training.  It includes two components:\n\n1. **`preprocess` component:** This component takes a string message as input. It outputs:\n    - `output_dataset_one`: A Dataset written to a file containing the input message. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "9rince/kfp",
    "file": "lightweight_python_functions_v2_pipeline_test.py",
    "simplified_prompt": "Create a pipeline called `lightweight_python_functions_v2_pipeline` to do data preprocessing and model training.  It includes two components.\n\nThe first component, named `preprocess`, takes two string parameters as input: `message` and `empty_message`. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "9rince/kfp",
    "file": "lightweight_python_functions_v2_with_outputs.py",
    "simplified_prompt": "Create a pipeline called `functions-with-outputs` to do a series of operations involving string concatenation, numerical addition, and artifact creation.  It includes four components:\n\n1. **`concat_message` component:** This component takes two string inputs (`first`, `second`) and returns a single string output which is the concatenation of the two input strings.\n\n2. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "9rince/kfp",
    "file": "lightweight_python_functions_v2_with_outputs_test.py",
    "simplified_prompt": "Create a pipeline called `lightweight_python_functions_v2_with_outputs` to do data processing and output writing.  It includes three components:\n\n1. **`component_1`:** This component takes no input. It generates two strings, \"first\" and \"second\", and writes them to separate files. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "9rince/kfp",
    "file": "metrics_visualization_v1.py",
    "simplified_prompt": "Create a pipeline called `metrics-visualization-v1-pipeline` to do data visualization.  It includes five independent components.\n\n1. **`confusion_visualization` component:** This component generates a confusion matrix visualization.  It does not take any explicit inputs or produce outputs visible in the provided code.\n\n2. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "9rince/kfp",
    "file": "metrics_visualization_v1_test.py",
    "simplified_prompt": "Create a pipeline called `metrics_visualization_v1_pipeline` to do data visualization tasks.  It includes five components: `table-visualization`, `markdown-visualization`, `roc-visualization`, `html-visualization`, and `confusion-visualization`.\n\nEach component takes no explicit inputs but produces an output artifact named `mlpipeline_ui_metadata`. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "9rince/kfp",
    "file": "metrics_visualization_v2.py",
    "simplified_prompt": "Create a pipeline called `metrics_visualization_v2` to do two classification tasks and logs their metrics.  It includes two components:\n\n1. **`digit_classification` component:** This component uses the `sklearn` library to perform digit classification on the Iris dataset using Logistic Regression. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "9rince/kfp",
    "file": "metrics_visualization_v2_test.py",
    "simplified_prompt": "Create a pipeline called `metrics_visualization_pipeline` to do a series of machine learning tasks and visualizes their results.  It includes five components:\n\n1. **`wine-classification`**: This component performs a wine classification task.  It does not have visible inputs, but outputs an artifact named \"metrics\" containing classification metrics.\n\n2. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "9rince/kfp",
    "file": "parameter_with_format.py",
    "simplified_prompt": "Create a pipeline called `pipeline-with-pipelineparam-containing-format` to do a simple string manipulation and printing operation.  It includes two components:\n\n1. **`print_op` component:** This component takes a single string input (`name`) and prints it to the standard output.  It then returns the input string as its output. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "9rince/kfp",
    "file": "parameter_with_format_test.py",
    "simplified_prompt": "Create a pipeline called `my_pipeline` to do a series of operations.  This pipeline consists of one component.  The pipeline uses no external tools or libraries beyond the Kubeflow Pipelines SDK itself.\n\nThe single component, implicitly defined within the `my_pipeline` function, does not have a explicitly named function but takes parameters from the pipeline execution and processes them. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "9rince/kfp",
    "file": "placeholder_concat.py",
    "simplified_prompt": "Create a pipeline called `one-step-pipeline-with-concat-placeholder` to do a single string concatenation operation.  It includes one component.\n\nThis component, named `Component with concat placeholder`, takes two string inputs: `input_one` and `input_two`. It concatenates these inputs with a '+' symbol and the string '=three', and then checks if the result equals 'one+two=three'. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "raviranjan0309/kubeflow-fairing-pipeline",
    "file": "pipeline.py",
    "simplified_prompt": "Create a pipeline called `Kubeflow Fairing Pipeline` to do a LightGBM model training.  It includes one component.\n\nThis component, named `lightgbm_training`, uses a Docker image `gcr.io/<GCP PROJECT ID>/lightgbm-model:latest` to train a LightGBM model.  The specific details of the training data and parameters are not explicitly defined in the provided code.  The component leverages the `kfp.gcp. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "fybrik/kfp-components",
    "file": "pipeline-argo.py",
    "simplified_prompt": "Create a pipeline called `Fybrik housing price estimate pipeline` to do a data-driven housing price estimation.  It includes four components:\n\n1. **`getDataEndpoints`:** This component takes `train_dataset_id` and `test_dataset_id` as inputs, along with the `namespace` and `run_name` for context. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "fybrik/kfp-components",
    "file": "pipeline-tekton.py",
    "simplified_prompt": "Create a pipeline called `Fybrik housing price estimate pipeline` to do data access, analysis, model training, and result submission.  It includes four components:\n\n1. **`getDataEndpoints`:** This component retrieves endpoints for training and testing datasets, as well as a result endpoint, based on provided `train_dataset_id` and `test_dataset_id`. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "ajinkya933/Kubeflow",
    "file": "mnist_REST_API.py",
    "simplified_prompt": "Create a pipeline called `MNIST Pipeline` to do a complete MNIST model training workflow.  It includes one component.\n\nThe single component, named `mnist_training_container`, performs the following steps:\n\n1. **Data Loading and Preprocessing:** Loads the Fashion MNIST dataset from Keras, normalizes the image data by dividing by 255.0.\n2. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "ajinkya933/Kubeflow",
    "file": "mnist_REST_API_temp.py",
    "simplified_prompt": "Create a pipeline called `MNIST Pipeline` to do a complete MNIST model training workflow.  It includes one component.\n\nThe pipeline uses the `kfp` and `tensorflow` libraries.  It leverages a Docker container (`tensorflow/tensorflow:latest-gpu-py3`) for training.\n\n**Component Details:**\n\n1. **`train` component:** This component trains a Keras model on the MNIST Fashion dataset. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "ajinkya933/Kubeflow",
    "file": "mnist_complete_train.py",
    "simplified_prompt": "Create a pipeline called `mnist_pipeline` to do a complete MNIST training and prediction workflow.  It includes two components:\n\n1. **`train` component:** This component takes a `data_path` (string) and a `model_file` (string) as input.  It performs the following steps:\n    * Loads the Fashion MNIST dataset.\n    * Normalizes the dataset.\n    * Creates a Keras sequential model. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "omkarakaya/kubeflow-recommender",
    "file": "pipeline.py",
    "simplified_prompt": "Create a pipeline called `Kubeflow Pipeline Test` to do a machine learning training workflow.  It includes three components:\n\n1. **`create_pvc` (VolumeOp):** This component creates a PersistentVolumeClaim (PVC) named \"my-pvc\" with a size of 1Gi and ReadWriteOnce (RWO) mode.  It doesn't have any inputs and serves as a prerequisite for the subsequent components. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "II-VSB-II/TaxiClassificationKubeflowPipelineMinio",
    "file": "Taxi-Pipeline.py",
    "simplified_prompt": "Create a pipeline called `TFX Taxi Cab Classification Pipeline Example` to do taxi cab classification.  It includes seven components.\n\n1. **Data Validation:**  A component (`dataflow_tf_data_validation_op`) loaded from a GitHub URL that validates the input taxi data. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "stackdemos/kf-flatcar2",
    "file": "component.py",
    "simplified_prompt": "Create a pipeline called `my-pipeline` to do data processing and model training.  This pipeline consists of three components.\n\nThe first component, named `download-artifact`, downloads a dataset from a URL.  It takes a URL and a local file path as input, and it outputs the downloaded file.  It uses the `curl` command to perform the download and verifies the MD5 checksum of the downloaded file. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "stackdemos/kf-flatcar2",
    "file": "component.py",
    "simplified_prompt": "Create a pipeline called `my-pipeline` to do a data download and model training workflow.  It includes two components.\n\nThe first component, named `download-artifact`, downloads a data artifact from a URL using `curl`.  It takes a URL, a local download path (`download_to`), and an MD5 checksum (`md5sum`) as inputs.  The output is the downloaded file at the specified `download_to` path. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "stackdemos/jjtest-ml",
    "file": "component.py",
    "simplified_prompt": "Create a pipeline called `MyDataPipeline` to do data processing and artifact downloading.  It includes three components.\n\n1. **`download-artifact` component:** This component downloads a file from a URL specified as input.  The URL, local download path, and expected MD5 checksum are provided as inputs. The output is the downloaded file at the specified path. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "stackdemos/jjtest-ml",
    "file": "component.py",
    "simplified_prompt": "Create a pipeline called `my_pipeline` to do a data download and model training.  It includes two components:\n\n1. **Data Download Component:** This component, named `download-artifact`, uses the `curl` command within a container to download a dataset from a URL.  The URL, local download path, and expected MD5 checksum are provided as inputs. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "stackdemos/jj-test2",
    "file": "component.py",
    "simplified_prompt": "Create a pipeline called `my-data-processing-pipeline` to do data processing using three components.\n\nIt includes the following components:\n\n1. **`download-artifact`:** This component downloads a file from a URL using `curl`.  It takes a URL, a local download path (`download_to`), and an MD5 checksum (`md5sum`) as inputs.  It outputs the downloaded file to the specified `download_to` path. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "stackdemos/jj-test2",
    "file": "component.py",
    "simplified_prompt": "Create a pipeline called `my_pipeline` to do a data download and model training.  It includes two components:\n\n1. **`download-artifact` component:** This component downloads a data artifact from a URL using `curl`.  It takes the following inputs:\n    * `url`: The URL of the data artifact (string).\n    * `download_to`: The local path to save the downloaded artifact (string). Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "stackdemos/kfp125",
    "file": "component.py",
    "simplified_prompt": "Create a pipeline called `my-data-processing-pipeline` to do data processing and model training.  This pipeline consists of three components.\n\nThe first component is `download-artifact`, which downloads a data file from a URL using `curl`.  It takes a URL and a local file path as input and outputs the downloaded file.  It also uses an MD5 checksum for verification. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "stackdemos/kfp125",
    "file": "component.py",
    "simplified_prompt": "Create a pipeline called `my_pipeline` to do a data download and subsequent training.  It includes two components:\n\n1. **`download-artifact` component:** This component downloads a file from a URL specified as an input.  The input is the URL of the data file.  It uses the `appropriate/curl` image.  It calculates the MD5 sum of the downloaded file and compares it against a provided MD5 sum. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "stackdemos/kcdemo",
    "file": "component.py",
    "simplified_prompt": "Create a pipeline called `pipeline_name_needs_to_be_specified_from_the_python_code` to do data processing and download.  It includes three components.\n\n1. **`http_download_op`**: This component downloads a file from a given URL (`url` input) to a specified location (`download_to` output) using `curl`.  It performs an MD5 checksum verification (`md5sum` input) to avoid redundant downloads. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "stackdemos/kcdemo",
    "file": "component.py",
    "simplified_prompt": "Create a pipeline called `my_pipeline` to do a data download and subsequent model training.  It includes two components.\n\nThe first component, named `download-artifact`, downloads a file from a URL.  It takes a URL, a local download path (`download_to`), and an MD5 checksum (`md5sum`) as inputs.  Its output is the downloaded file at the specified `download_to` path. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "stackdemos/ghsumm2",
    "file": "component.py",
    "simplified_prompt": "Create a pipeline called `pipeline_example` to do data processing and model training.  It includes three components:\n\n1. **`download-artifact` component:** This component downloads a data file from a URL using `curl`.  It takes a URL (`url`), a local download path (`download_to`), and an MD5 checksum (`md5sum`) as inputs.  It outputs the downloaded file to `download_to`. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "stackdemos/ghsumm2",
    "file": "component.py",
    "simplified_prompt": "Create a pipeline called `my_pipeline` to do a data download and model training.  It includes two components.\n\nThe first component, named `download-artifact`, downloads a file from a URL using `curl`.  It takes a URL, a local download path, and an MD5 checksum as inputs. It verifies the downloaded file's checksum before proceeding. The output is the downloaded file at the specified path. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "stackdemos/ml-kbf",
    "file": "component.py",
    "simplified_prompt": "Create a pipeline called `my_data_pipeline` to do data processing and model training.  This pipeline consists of three components.\n\n1. **`download-artifact` component:** This component downloads a data artifact from a URL using `curl`.  It takes a URL, a local download path (`download_to`), and an MD5 checksum (`md5sum`) as inputs. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "stackdemos/ml-kbf",
    "file": "component.py",
    "simplified_prompt": "Create a pipeline called `my_pipeline` to do data download and model training.  It includes two components:\n\n1. **`download-artifact` Component:** This component downloads a dataset from a URL using `curl`.  It takes a URL (`url`), a local file path (`download_to`), and an MD5 checksum (`md5sum`) as inputs. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "s102401002/kubeflowPipeline2",
    "file": "kubeflowPipeline_xgboost.py",
    "simplified_prompt": "Create a pipeline called `diabetes_prediction_pipeline` to do diabetes prediction using XGBoost.  It includes two components:\n\n1. **`load_data` component:** This component loads data from two CSV files located at specified URLs.  It handles missing values and performs data cleaning, including renaming columns based on a mapping and converting categorical data (gender) to numerical representations. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "stackdemos/kf4",
    "file": "component.py",
    "simplified_prompt": "Create a pipeline called `my-data-processing-pipeline` to do data processing.  It includes three components:\n\n1. **`download-artifact`**: This component downloads a data file from a URL using `curl`.  It takes a URL, a local file path (`download_to`), and an MD5 checksum (`md5sum`) as inputs.  It outputs the downloaded file to the specified `download_to` path. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "stackdemos/kf4",
    "file": "component.py",
    "simplified_prompt": "Create a pipeline called `my_pipeline` to do data ingestion and model training.  This pipeline consists of two components:\n\n1. **`download-artifact` component:** This component downloads a data artifact from a URL using `curl`.  The input is a URL (`url`), a local download path (`download_to`), and an MD5 checksum (`md5sum`) for validation. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "stackdemos/joe-kubeflow-test",
    "file": "component.py",
    "simplified_prompt": "Create a pipeline called `my-data-processing-pipeline` to do data processing and analysis.  It includes three components.\n\n1. **`download-artifact`**: This component downloads a data artifact from a URL using `curl`. It takes a URL, a local download path (`download_to`), and an MD5 checksum (`md5sum`) as inputs. It outputs the downloaded file to `download_to`. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "stackdemos/joe-kubeflow-test",
    "file": "component.py",
    "simplified_prompt": "Create a pipeline called `my_pipeline` to do data ingestion, preprocessing, and model training.  It includes three components.\n\nThe first component, named `download-artifact`, uses the `curl` command within a container (`appropriate/curl` image) to download a file from a given URL (`url` input) to a specified location (`download_to` output). Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "stackdemos/kfp11",
    "file": "component.py",
    "simplified_prompt": "Create a pipeline called `pipeline_example` to do data processing.  It includes three components.\n\n1. **`download-artifact`**: This component downloads a file from a URL. It takes a URL and a local file path as input, and produces the downloaded file as an output.  It uses the `curl` command to download the file and verifies the MD5 checksum.  The `appropriate/curl` Docker image is used. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "stackdemos/kfp11",
    "file": "component.py",
    "simplified_prompt": "Create a pipeline called `my-pipeline` to do a data download and then model training.  It includes two components.\n\nThe first component, named `download-artifact`, downloads a dataset from a URL using `curl`.  It takes as input a URL and a target download path, along with an MD5 checksum for verification. If the file exists and the checksum matches, it skips the download. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "stackdemos/specs-kf5",
    "file": "component.py",
    "simplified_prompt": "Create a pipeline called `my-data-processing-pipeline` to do data processing.  It includes three components.\n\nThe first component, named `download-artifact`, downloads a file from a given URL to a specified location.  It uses the `curl` command and verifies the MD5 checksum to avoid unnecessary redownloads. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "stackdemos/specs-kf5",
    "file": "component.py",
    "simplified_prompt": "Create a pipeline called `my_pipeline` to do data downloading and model training.  It includes two components.\n\nThe first component, named `download-artifact`, downloads a dataset from a URL using `curl`.  It takes a URL (`url`), a local download path (`download_to`), and an MD5 checksum (`md5sum`) as inputs.  It outputs the downloaded file to the specified `download_to` path. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "akranga/anton-ml1",
    "file": "component.py",
    "simplified_prompt": "Create a pipeline called `my-data-processing-pipeline` to do data processing.  This pipeline consists of three components.\n\n1. **`download-artifact` component:** This component downloads a file from a URL using `curl`.  It takes a URL (`url`), a local file path (`download_to`), and an MD5 checksum (`md5sum`) as inputs. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "akranga/anton-ml1",
    "file": "component.py",
    "simplified_prompt": "Create a pipeline called `my_pipeline` to do a data download and then model training.  It includes two components.\n\nThe first component, named `download-artifact`, downloads a file from a URL using `curl`.  It takes as input a URL (`url`), a local file path (`download_to`), and an MD5 checksum (`md5sum`).  It outputs the downloaded file to the path specified by `download_to`. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "baebae-dev/kubeflow-pipelines",
    "file": "pipeline.py",
    "simplified_prompt": "Create a pipeline called `Boston Housing Pipeline` to do a machine learning workflow for predicting Boston housing prices.  It includes four components:\n\n1. **Preprocess Data:** This component preprocesses the Boston housing dataset.  It uses the Docker image `gnovack/boston_pipeline_preprocessing:latest` and has no input arguments. Its outputs are four files: `x_train.npy`, `x_test. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "stackdemos/secrets-tetst-01",
    "file": "component.py",
    "simplified_prompt": "Create a pipeline called `pipeline_1` to do data processing and model training.  It includes three components.\n\nThe first component, named `download-artifact`, downloads a data file from a URL using `curl`. It takes a URL, a local download path (`download_to`), and an MD5 checksum (`md5sum`) as inputs. The output is the downloaded file at the specified path. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "stackdemos/secrets-tetst-01",
    "file": "component.py",
    "simplified_prompt": "Create a pipeline called `my_pipeline` to do a data download and subsequent model training.  It includes two components:\n\n1. **`download-artifact` Component:** This component downloads a data artifact from a URL using `curl`.  The input is a URL and an expected MD5 checksum. The output is the downloaded file written to a specified path. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "stackdemos/kfappx",
    "file": "component.py",
    "simplified_prompt": "Create a pipeline called `pipeline_name` to do data processing and model training.  It includes three components:\n\n1. **`download-artifact` component:** This component downloads a data artifact from a URL using `curl`.  It takes the URL and a local file path as input. It also takes an MD5 checksum for validation, skipping the download if the file already exists and matches the checksum. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "stackdemos/kfappx",
    "file": "component.py",
    "simplified_prompt": "Create a pipeline called `my-pipeline` to do a data download and model training workflow.  It includes two components.\n\nThe first component, named `download-artifact`, downloads a file from a URL using `curl`.  Its input is a URL and an MD5 checksum (`md5sum`).  It outputs the downloaded file to a specified path (`download_to`). Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "akranga/machine-learning1",
    "file": "component.py",
    "simplified_prompt": "Create a pipeline called `my_data_pipeline` to do data processing and artifact download.  It includes three components:\n\n1. **`download-artifact` Component:** This component downloads a file from a given URL using `curl`.  The input is a URL and an expected MD5 checksum.  The output is the downloaded file at a specified local path.  It uses the `appropriate/curl` Docker image. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "akranga/machine-learning1",
    "file": "component.py",
    "simplified_prompt": "Create a pipeline called `my-pipeline` to do data preparation and model training.  It includes three components:\n\n1. **`download-artifact` component:** This component downloads a data artifact from a URL using `curl`.  It takes the URL (`url`) and a local download path (`download_to`) as input. It also takes an MD5 checksum (`md5sum`) to verify the downloaded file. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "ArianFotouhi/kubeflowPipelineSpamDetector",
    "file": "script.py",
    "simplified_prompt": "Create a pipeline called `sms_spam_classifier` to do spam detection on the SMS Spam Collection dataset.  It includes three components:\n\n1. **`extract_data`**: This component downloads a zip file containing the SMS Spam Collection dataset from a public URL (`https://archive.ics.uci.edu/static/public/228/sms+spam+collection. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "felipeacunago/kubeflow",
    "file": "prophet_prediction.py",
    "simplified_prompt": "Create a pipeline called `Prophet` to do time series prediction using fbprophet.  It includes three components:\n\n1. **BigQuery Query:** This component queries data from a BigQuery database using a provided SQL query (`dataset_query`). The output is a CSV file saved to Google Cloud Storage at a specified path (`original_dataset_path`). Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "rakesh283343/kubeflow-sample-pipelines",
    "file": "condition.py",
    "simplified_prompt": "Create a pipeline called `Conditional execution pipeline` to do a series of conditional operations based on the outcome of a coin flip.  It includes four components:\n\n1. **Flip coin:** This component uses a Python script to simulate a coin flip, randomly outputting either \"heads\" or \"tails\" to a file named `/tmp/output`. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "rakesh283343/kubeflow-sample-pipelines",
    "file": "execution_order.py",
    "simplified_prompt": "Create a pipeline called `Execution order pipeline` to do two simple echo operations.  It includes two components:\n\n1. **echo1:** This component takes a single string input (`text1`, defaulting to 'message 1') and outputs the same string to the standard output. It uses a bash container (`library/bash:4.4.23`) to execute the echo command. Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  },
  {
    "repo": "rakesh283343/kubeflow-sample-pipelines",
    "file": "exit_handler.py",
    "simplified_prompt": "Create a pipeline called `Exit Handler` to do a simple data download and echo operation, demonstrating the use of an exit handler.  It includes three components:\n\n1. **`GCS - Download`:** This component downloads a text file from a Google Cloud Storage (GCS) URL using `gsutil`.  The input is the GCS URL (`url`, defaulting to `gs://ml-pipeline/shakespeare/shakespeare1.txt`). Also, follow these rules to ensure correctness: Use @component decorators for each function intended as a Kubeflow step. Ensure all required modules are imported explicitly, especially kfp, kfp.dsl, and kfp.components. Include @dsl.pipeline decorator with a defined name. Use proper Python typing for all function arguments and return values. Avoid undefined or external module references. Use valid YAML-friendly parameter types in pipeline definitions."
  }
]