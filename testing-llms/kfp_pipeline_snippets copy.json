[
  {
    "repo": "9rince/kfp",
    "file_path": "samples/test/metrics_visualization_v1_test.py",
    "raw_url": "https://raw.githubusercontent.com/9rince/kfp/develop/samples/test/metrics_visualization_v1_test.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp_server_api\nimport unittest\nfrom pprint import pprint\nfrom .metrics_visualization_v1 import metrics_visualization_v1_pipeline\nfrom .util import KfpMlmdClient, run_pipeline_func, TestCase\n\nimport kfp\nimport kfp_server_api\n\n\ndef verify(\n    run: kfp_server_api.ApiRun, mlmd_connection_config, argo_workflow_name: str,\n    **kwargs\n):\n    t = unittest.TestCase()\n    t.maxDiff = None  # we always want to see full diff\n    t.assertEqual(run.status, 'Succeeded')\n    client = KfpMlmdClient(mlmd_connection_config=mlmd_connection_config)\n    tasks = client.get_tasks(argo_workflow_name=argo_workflow_name)\n    pprint(tasks)\n\n    table_visualization = tasks['table-visualization']\n    markdown_visualization = tasks['markdown-visualization']\n    roc_visualization = tasks['roc-visualization']\n    html_visualization = tasks['html-visualization']\n    confusion_visualization = tasks['confusion-visualization']\n\n    t.assertEqual(table_visualization.get_dict()['outputs']['artifacts'][0]['name'],\n                  'mlpipeline_ui_metadata')\n    t.assertEqual(markdown_visualization.get_dict()['outputs']['artifacts'][0]['name'],\n                  'mlpipeline_ui_metadata')\n    t.assertEqual(roc_visualization.get_dict()['outputs']['artifacts'][0]['name'],\n                  'mlpipeline_ui_metadata')\n    t.assertEqual(html_visualization.get_dict()['outputs']['artifacts'][0]['name'],\n                  'mlpipeline_ui_metadata')\n    t.assertEqual(confusion_visualization.get_dict()['outputs']['artifacts'][0]['name'],\n                  'mlpipeline_ui_metadata')\n\n\nrun_pipeline_func([TestCase(pipeline_func=metrics_visualization_v1_pipeline,\n                            mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE\n                            )])\n"
  },
  {
    "repo": "9rince/kfp",
    "file_path": "samples/test/metrics_visualization_v2.py",
    "raw_url": "https://raw.githubusercontent.com/9rince/kfp/develop/samples/test/metrics_visualization_v2.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport os\n\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import (\n    component,\n    Output,\n    ClassificationMetrics,\n    Metrics,\n    HTML,\n    Markdown\n)\n\n# In tests, we install a KFP package from the PR under test. Users should not\n# normally need to specify `kfp_package_path` in their component definitions.\n_KFP_PACKAGE_PATH = os.getenv('KFP_PACKAGE_PATH')\n\n\n@component(\n    packages_to_install=['sklearn'],\n    base_image='python:3.9',\n    kfp_package_path=_KFP_PACKAGE_PATH,\n)\ndef digit_classification(metrics: Output[Metrics]):\n    from sklearn import model_selection\n    from sklearn.linear_model import LogisticRegression\n    from sklearn import datasets\n    from sklearn.metrics import accuracy_score\n\n    # Load digits dataset\n    iris = datasets.load_iris()\n\n    # # Create feature matrix\n    X = iris.data\n\n    # Create target vector\n    y = iris.target\n\n    #test size\n    test_size = 0.33\n\n    seed = 7\n    #cross-validation settings\n    kfold = model_selection.KFold(n_splits=10, random_state=seed, shuffle=True)\n\n    #Model instance\n    model = LogisticRegression()\n    scoring = 'accuracy'\n    results = model_selection.cross_val_score(model, X, y, cv=kfold, scoring=scoring)\n\n    #split data\n    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=test_size, random_state=seed)\n    #fit model\n    model.fit(X_train, y_train)\n\n    #accuracy on test set\n    result = model.score(X_test, y_test)\n    metrics.log_metric('accuracy', (result*100.0))\n\n@component(\n    packages_to_install=['sklearn'],\n    base_image='python:3.9',\n    kfp_package_path=_KFP_PACKAGE_PATH,\n)\ndef wine_classification(metrics: Output[ClassificationMetrics]):\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.metrics import roc_curve\n    from sklearn.datasets import load_wine\n    from sklearn.model_selection import train_test_split, cross_val_predict\n\n    X, y = load_wine(return_X_y=True)\n    # Binary classification problem for label 1.\n    y = y == 1\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n    rfc = RandomForestClassifier(n_estimators=10, random_state=42)\n    rfc.fit(X_train, y_train)\n    y_scores = cross_val_predict(rfc, X_train, y_train, cv=3, method='predict_proba')\n    y_predict = cross_val_predict(rfc, X_train, y_train, cv=3, method='predict')\n    fpr, tpr, thresholds = roc_curve(y_true=y_train, y_score=y_scores[:,1], pos_label=True)\n    metrics.log_roc_curve(fpr, tpr, thresholds)\n\n@component(\n    packages_to_install=['sklearn'],\n    base_image='python:3.9',\n    kfp_package_path=_KFP_PACKAGE_PATH,\n)\ndef iris_sgdclassifier(test_samples_fraction: float, metrics: Output[ClassificationMetrics]):\n    from sklearn import datasets, model_selection\n    from sklearn.linear_model import SGDClassifier\n    from sklearn.metrics import confusion_matrix\n\n    iris_dataset = datasets.load_iris()\n    train_x, test_x, train_y, test_y = model_selection.train_test_split(\n        iris_dataset['data'], iris_dataset['target'], test_size=test_samples_fraction)\n\n\n    classifier = SGDClassifier()\n    classifier.fit(train_x, train_y)\n    predictions = model_selection.cross_val_predict(classifier, train_x, train_y, cv=3)\n    metrics.log_confusion_matrix(\n        ['Setosa', 'Versicolour', 'Virginica'],\n        confusion_matrix(train_y, predictions).tolist() # .tolist() to convert np array to list.\n    )\n\n@component(\n    kfp_package_path=_KFP_PACKAGE_PATH,\n)\ndef html_visualization(html_artifact: Output[HTML]):\n    html_content = '<!DOCTYPE html><html><body><h1>Hello world</h1></body></html>'\n    with open(html_artifact.path, 'w') as f:\n        f.write(html_content)\n\n@component(\n    kfp_package_path=_KFP_PACKAGE_PATH,\n)\ndef markdown_visualization(markdown_artifact: Output[Markdown]):\n    markdown_content = '## Hello world \\n\\n Markdown content'\n    with open(markdown_artifact.path, 'w') as f:\n        f.write(markdown_content)\n\n@dsl.pipeline(\n    name='metrics-visualization-pipeline')\ndef metrics_visualization_pipeline():\n    wine_classification_op = wine_classification()\n    iris_sgdclassifier_op = iris_sgdclassifier(test_samples_fraction=0.3)\n    digit_classification_op = digit_classification()\n    html_visualization_op = html_visualization()\n    markdown_visualization_op = markdown_visualization()\n"
  },
  {
    "repo": "9rince/kfp",
    "file_path": "samples/test/metrics_visualization_v2_test.py",
    "raw_url": "https://raw.githubusercontent.com/9rince/kfp/develop/samples/test/metrics_visualization_v2_test.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport unittest.mock as mock\nfrom pprint import pprint\nimport kfp\nimport kfp_server_api\n\nfrom .metrics_visualization_v2 import metrics_visualization_pipeline\nfrom .util import run_pipeline_func, TestCase, KfpMlmdClient\nfrom ml_metadata.proto import Execution\n\n\ndef verify(run: kfp_server_api.ApiRun, mlmd_connection_config, **kwargs):\n    t = unittest.TestCase()\n    t.maxDiff = None  # we always want to see full diff\n    t.assertEqual(run.status, 'Succeeded')\n    client = KfpMlmdClient(mlmd_connection_config=mlmd_connection_config)\n    tasks = client.get_tasks(run_id=run.id)\n\n    task_names = [*tasks.keys()]\n    t.assertCountEqual(\n        task_names,\n        ['wine-classification', 'iris-sgdclassifier', 'digit-classification', 'html-visualization', 'markdown-visualization'],\n        'task names'\n    )\n\n    wine_classification = tasks['wine-classification']\n    iris_sgdclassifier = tasks['iris-sgdclassifier']\n    digit_classification = tasks['digit-classification']\n    html_visualization = tasks['html-visualization']\n    markdown_visualization = tasks['markdown-visualization']\n\n    pprint('======= wine classification task =======')\n    pprint(wine_classification.get_dict())\n    pprint('======= iris sgdclassifier task =======')\n    pprint(iris_sgdclassifier.get_dict())\n    pprint('======= digit classification task =======')\n    pprint(digit_classification.get_dict())\n    pprint('======= HTML task =======')\n    pprint(html_visualization.get_dict())\n    pprint('======= Markdown task =======')\n    pprint(markdown_visualization.get_dict())\n\n    t.assertEqual({\n        'inputs': {\n            'artifacts': [],\n            'parameters': {}\n        },\n        'name': 'wine-classification',\n        'outputs': {\n            'artifacts': [{\n                'metadata': {\n                    'display_name': 'metrics',\n                    'confidenceMetrics': {\n                        'list': [{\n                            'confidenceThreshold': 2.0,\n                            'falsePositiveRate': 0.0,\n                            'recall': 0.0\n                        }, {\n                            'confidenceThreshold': 1.0,\n                            'falsePositiveRate': 0.0,\n                            'recall': 0.33962264150943394\n                        }, {\n                            'confidenceThreshold': 0.9,\n                            'falsePositiveRate': 0.0,\n                            'recall': 0.6037735849056604\n                        }, {\n                            'confidenceThreshold': 0.8,\n                            'falsePositiveRate': 0.0,\n                            'recall': 0.8490566037735849\n                        }, {\n                            'confidenceThreshold': 0.6,\n                            'falsePositiveRate': 0.0,\n                            'recall': 0.8867924528301887\n                        }, {\n                            'confidenceThreshold': 0.5,\n                            'falsePositiveRate': 0.0125,\n                            'recall': 0.9245283018867925\n                        }, {\n                            'confidenceThreshold': 0.4,\n                            'falsePositiveRate': 0.075,\n                            'recall': 0.9622641509433962\n                        }, {\n                            'confidenceThreshold': 0.3,\n                            'falsePositiveRate': 0.0875,\n                            'recall': 1.0\n                        }, {\n                            'confidenceThreshold': 0.2,\n                            'falsePositiveRate': 0.2375,\n                            'recall': 1.0\n                        }, {\n                            'confidenceThreshold': 0.1,\n                            'falsePositiveRate': 0.475,\n                            'recall': 1.0\n                        }, {\n                            'confidenceThreshold': 0.0,\n                            'falsePositiveRate': 1.0,\n                            'recall': 1.0\n                        }]\n                    }\n                },\n                'name': 'metrics',\n                'type': 'system.ClassificationMetrics'\n            }],\n            'parameters': {}\n        },\n        'type': 'system.ContainerExecution',\n        'state': Execution.State.COMPLETE,\n    }, wine_classification.get_dict())\n    t.assertEqual(\n        {\n            'inputs': {\n                'artifacts': [],\n                'parameters': {\n                    'test_samples_fraction': 0.3\n                }\n            },\n            'name': 'iris-sgdclassifier',\n            'outputs': {\n                'artifacts': [{\n                    'metadata': {\n                        'display_name': 'metrics',\n                        'confusionMatrix': {'struct': {\n                            'annotationSpecs': [{\n                                'displayName': 'Setosa'\n                            }, {\n                                'displayName': 'Versicolour'\n                            }, {\n                                'displayName': 'Virginica'\n                            }],\n                            'rows': [{ # these numbers can be random during execution\n                                'row': [mock.ANY, mock.ANY, mock.ANY]\n                            }, {\n                                'row': [mock.ANY, mock.ANY, mock.ANY]\n                            }, {\n                                'row': [mock.ANY, mock.ANY, mock.ANY]\n                            }]\n                        }}\n                    },\n                    'name': 'metrics',\n                    'type': 'system.ClassificationMetrics'\n                }],\n                'parameters': {}\n            },\n            'type': 'system.ContainerExecution',\n            'state': Execution.State.COMPLETE,\n        }, iris_sgdclassifier.get_dict()\n    )\n    rows = iris_sgdclassifier.get_dict()['outputs']['artifacts'][0]['metadata'][\n        'confusionMatrix']['struct']['rows']\n    for i, row in enumerate(rows):\n        for j, item in enumerate(row['row']):\n            t.assertIsInstance(\n                item, float,\n                f'value of confusion matrix row {i}, col {j} is not a number'\n            )\n\n    t.assertEqual({\n        'inputs': {\n            'artifacts': [],\n            'parameters': {}\n        },\n        'name': 'digit-classification',\n        'outputs': {\n            'artifacts': [{\n                'metadata': {\n                    'display_name': 'metrics',\n                    'accuracy': 92.0,\n                },\n                'name': 'metrics',\n                'type': 'system.Metrics'\n            }],\n            'parameters': {}\n        },\n        'type': 'system.ContainerExecution',\n        'state': Execution.State.COMPLETE,\n    }, digit_classification.get_dict())\n\n    t.assertEqual({\n        'inputs': {\n            'artifacts': [], \n            'parameters': {}\n        },\n        'name': 'html-visualization',\n        'outputs': {\n            'artifacts': [{\n                'metadata': {\n                    'display_name': 'html_artifact'\n                },\n                'name': 'html_artifact',\n                'type': 'system.HTML'\n            }],\n            'parameters': {}\n        },\n        'state': Execution.State.COMPLETE,\n        'type': 'system.ContainerExecution'\n    }, html_visualization.get_dict())\n\n    t.assertEqual({\n        'inputs': {\n            'artifacts': [], \n            'parameters': {}\n        },\n        'name': 'markdown-visualization',\n        'outputs': {\n            'artifacts': [{\n                'metadata': {\n                    'display_name': 'markdown_artifact'\n                },\n                'name': 'markdown_artifact',\n                'type': 'system.Markdown'\n            }],\n            'parameters': {}\n        },\n        'state': Execution.State.COMPLETE,\n        'type': 'system.ContainerExecution'\n    }, markdown_visualization.get_dict())\n\n\nrun_pipeline_func([\n    TestCase(\n        pipeline_func=metrics_visualization_pipeline,\n        verify_func=verify,\n        mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE\n    ),\n    TestCase(\n        pipeline_func=metrics_visualization_pipeline,\n        verify_func=verify,\n        mode=kfp.dsl.PipelineExecutionMode.V2_ENGINE\n    ),\n])\n"
  },
  {
    "repo": "9rince/kfp",
    "file_path": "samples/test/parameter_with_format.py",
    "raw_url": "https://raw.githubusercontent.com/9rince/kfp/develop/samples/test/parameter_with_format.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom kfp import components\nfrom kfp import dsl\n\n\n@components.create_component_from_func\ndef print_op(name: str) -> str:\n    print(name)\n    return name\n\n\n@dsl.pipeline(name='pipeline-with-pipelineparam-containing-format')\ndef my_pipeline(name: str = 'KFP'):\n    print_task = print_op('Hello {}'.format(name))\n    print_op('{}, again.'.format(print_task.output))\n"
  },
  {
    "repo": "9rince/kfp",
    "file_path": "samples/test/parameter_with_format_test.py",
    "raw_url": "https://raw.githubusercontent.com/9rince/kfp/develop/samples/test/parameter_with_format_test.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp\nfrom .parameter_with_format import my_pipeline\nfrom .util import run_pipeline_func, TestCase\n\n\ndef verify(run, run_id: str, **kwargs):\n    assert run.status == 'Succeeded'\n    # TODO(Bobgy): verify output\n\n\nrun_pipeline_func([\n    TestCase(\n        pipeline_func=my_pipeline,\n        verify_func=verify,\n    ),\n    TestCase(\n        pipeline_func=my_pipeline,\n        verify_func=verify,\n        mode=kfp.dsl.PipelineExecutionMode.V2_ENGINE,\n    ),\n    TestCase(\n        pipeline_func=my_pipeline,\n        mode=kfp.dsl.PipelineExecutionMode.V1_LEGACY,\n    ),\n])\n"
  },
  {
    "repo": "9rince/kfp",
    "file_path": "samples/test/placeholder_concat.py",
    "raw_url": "https://raw.githubusercontent.com/9rince/kfp/develop/samples/test/placeholder_concat.py",
    "content": "# Copyright 2020 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom kfp import components\nfrom kfp.v2 import dsl\nimport kfp.v2.compiler as compiler\n\ncomponent_op = components.load_component_from_text(\"\"\"\nname: Component with concat placeholder\ninputs:\n- {name: input_one, type: String}\n- {name: input_two, type: String}\nimplementation:\n  container:\n    image: gcr.io/google-containers/busybox\n    command:\n    - sh\n    - -ec\n    args:\n    - echo \"$0\" > /tmp/test && [[ \"$0\" == 'one+two=three' ]]\n    - concat: [{inputValue: input_one}, '+', {inputValue: input_two}, '=three']\n\"\"\")\n\n\n@dsl.pipeline(name='one-step-pipeline-with-concat-placeholder')\ndef pipeline_with_concat_placeholder():\n  component = component_op(input_one='one', input_two='two')\n"
  },
  {
    "repo": "raviranjan0309/kubeflow-fairing-pipeline",
    "file_path": "pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/raviranjan0309/kubeflow-fairing-pipeline/master/pipeline.py",
    "content": "import kfp.dsl as dsl\nimport kfp.gcp as gcp\nimport kfp\nfrom kubernetes.client.models import V1EnvVar\nimport warnings\nwarnings.filterwarnings('ignore')\n\n@dsl.pipeline(\n    name='Kubeflow Fairing Pipeline',\n    description='Embedding Kubeflow fairing inside Kubeflow Pipeline',\n)\ndef pipeline():\n    lightgbm_train = dsl.ContainerOp(\n        name='lightgbm_training',\n        image='gcr.io/<GCP PROJECT ID>/lightgbm-model:latest'          \n        ).apply(gcp.use_gcp_secret(secret_name='user-gcp-sa'))\n\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(pipeline, __file__ + '.zip')\n"
  },
  {
    "repo": "fybrik/kfp-components",
    "file_path": "samples/house_price_estimates/pipeline-argo.py",
    "raw_url": "https://raw.githubusercontent.com/fybrik/kfp-components/main/samples/house_price_estimates/pipeline-argo.py",
    "content": "import kfp.dsl as dsl\nimport kfp.components as components\n\n@dsl.pipeline(\n    name = \"Fybrik housing price estimate pipeline\",\n    description = \"Pipeline that provides data policy governed access to cataloged data, analyses data, trains model, and writes the results and catalogs them\"\n)\n\ndef get_current_namespace():\n    import kubernetes\n    import os\n\n    ns_path = \"/var/run/secrets/kubernetes.io/serviceaccount/namespace\"\n    if os.path.exists(ns_path):\n        with open(ns_path) as f:\n            return f.read().strip()\n    try:\n        _, active_context = kubernetes.config.list_kube_config_contexts()\n        return active_context[\"context\"][\"namespace\"]\n    except KeyError:\n        return \"kubeflow\"\n\ndef houseprice_pipeline(\n    test_dataset_id: str,\n    train_dataset_id: str\n):\n    # Get the namespace in which kfp is running\n    namespace = get_current_namespace()\n\n    # Get the ID of the run.  Make sure it's lower case and starts with a letter for our use\n    run_name = \"run-\" + dsl.RUN_ID_PLACEHOLDER.lower()\n\n    # Where to store parameters passed between workflow steps\n    result_name = \"submission-\" + str(run_name)\n\n    getDataEndpointsOp = components.load_component_from_file('../../get_data_endpoints/component.yaml')\n    getDataEndpointsStep = getDataEndpointsOp(train_dataset_id=train_dataset_id, test_dataset_id=test_dataset_id, namespace=namespace, run_name=run_name, result_name=result_name)\n    \n    visualizeTableOp = components.load_component_from_file('./visualize_table/component.yaml')\n    visualizeTableStep = visualizeTableOp(train_endpoint='%s'% getDataEndpointsStep.outputs['train_endpoint'], train_dataset_id=train_dataset_id, namespace=namespace)\n    visualizeTableStep.after(getDataEndpointsStep)\n\n    trainModelOp = components.load_component_from_file('./train_model/component.yaml')\n    trainModelStep = trainModelOp(train_endpoint_path='%s' % getDataEndpointsStep.outputs['train_endpoint'],\n                                test_endpoint_path='%s' % getDataEndpointsStep.outputs['test_endpoint'],\n                                result_name=result_name,\n                                result_endpoint_path='%s' % getDataEndpointsStep.outputs['result_endpoint'],\n                                train_dataset_id=train_dataset_id,\n                                test_dataset_id=test_dataset_id,\n                                namespace=namespace)\n    trainModelStep.after(visualizeTableStep)\n\n    sumbitResultOp = components.load_component_from_file('./submit_result/component.yaml')\n    submitResultStep = sumbitResultOp(getDataEndpointsStep.outputs['result_catalogid'])\n    submitResultStep.after(trainModelStep)\n \nif __name__ == '__main__':\n\n    # Set environment values to ensure persistent volumes used to pass parameters are allocated successfully \n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--test_dataset_id', type=str)\n    parser.add_argument('--train_dataset_id', type=str)\n \n    args = parser.parse_args()\n    import kfp.compiler as compiler\n    compiler.Compiler().compile(pipeline_func=houseprice_pipeline, package_path=__file__.replace('.py', '.yaml'))\n    "
  },
  {
    "repo": "fybrik/kfp-components",
    "file_path": "samples/house_price_estimates/pipeline-tekton.py",
    "raw_url": "https://raw.githubusercontent.com/fybrik/kfp-components/main/samples/house_price_estimates/pipeline-tekton.py",
    "content": "import kfp.dsl as dsl\nimport kfp.components as components\n\n@dsl.pipeline(\n    name = \"Fybrik housing price estimate pipeline\",\n    description = \"Pipeline that provides data policy governed access to cataloged data, analyses data, trains model, and writes the results and catalogs them\"\n)\n\ndef get_current_namespace():\n    import kubernetes\n    import os\n\n    ns_path = \"/var/run/secrets/kubernetes.io/serviceaccount/namespace\"\n    if os.path.exists(ns_path):\n        with open(ns_path) as f:\n            return f.read().strip()\n    try:\n        _, active_context = kubernetes.config.list_kube_config_contexts()\n        return active_context[\"context\"][\"namespace\"]\n    except KeyError:\n        return \"kubeflow\"\n\ndef houseprice_pipeline(\n    test_dataset_id: str,\n    train_dataset_id: str\n):\n    # Get the namespace in which kfp is running\n    namespace = get_current_namespace()\n\n    # Get the ID of the run.  Make sure it's lower case and starts with a letter for our use\n    run_name = \"run-\" + dsl.RUN_ID_PLACEHOLDER.lower()\n\n    # Where to store parameters passed between workflow steps\n    result_name = \"submission-\" + str(run_name)\n\n    getDataEndpointsOp = components.load_component_from_file('../../get_data_endpoints/component.yaml')\n    getDataEndpointsStep = getDataEndpointsOp(train_dataset_id=train_dataset_id, test_dataset_id=test_dataset_id, namespace=namespace, run_name=run_name, result_name=result_name)\n    \n    visualizeTableOp = components.load_component_from_file('./visualize_table/component.yaml')\n    visualizeTableStep = visualizeTableOp(train_endpoint='%s'% getDataEndpointsStep.outputs['train_endpoint'], train_dataset_id=train_dataset_id, namespace=namespace)\n    visualizeTableStep.after(getDataEndpointsStep)\n\n    trainModelOp = components.load_component_from_file('./train_model/component.yaml')\n    trainModelStep = trainModelOp(train_endpoint_path='%s' % getDataEndpointsStep.outputs['train_endpoint'],\n                                test_endpoint_path='%s' % getDataEndpointsStep.outputs['test_endpoint'],\n                                result_name=result_name,\n                                result_endpoint_path='%s' % getDataEndpointsStep.outputs['result_endpoint'],\n                                train_dataset_id=train_dataset_id,\n                                test_dataset_id=test_dataset_id,\n                                namespace=namespace)\n    trainModelStep.after(visualizeTableStep)\n\n    sumbitResultOp = components.load_component_from_file('./submit_result/component.yaml')\n    submitResultStep = sumbitResultOp(getDataEndpointsStep.outputs['result_catalogid'])\n    submitResultStep.after(trainModelStep)\n \nif __name__ == '__main__':\n\n    # Set environment values to ensure persistent volumes used to pass parameters are allocated successfully \n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--test_dataset_id', type=str)\n    parser.add_argument('--train_dataset_id', type=str)\n\n    args = parser.parse_args()\n    from kfp_tekton.compiler import TektonCompiler\n \n    TektonCompiler().compile(houseprice_pipeline, __file__.replace('.py', '.yaml'))\n    "
  },
  {
    "repo": "ajinkya933/Kubeflow",
    "file_path": "mnist_REST_API.py",
    "raw_url": "https://raw.githubusercontent.com/ajinkya933/Kubeflow/master/mnist_REST_API.py",
    "content": "import kfp\nimport kfp.dsl as dsl\nimport kfp.components as comp\n\n\ndef train(data_path, model_file):\n    '''\n        This definition contains MNIST training steps:\n            * Data Import\n            * Data Preprocessing\n            * Keras model creation\n            * Model optimizer : adam\n            * Training with specified epoch\n            * Print test accuracy\n            * Save the model\n    '''\n    import pickle\n    import tensorflow as tf\n    from tensorflow.python import keras\n\n    # Load dataset\n    fashion_mnist = keras.datasets.fashion_mnist\n    (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n\n    # Normalize dataset\n    train_images = train_images / 255.0\n    test_images = test_images / 255.0\n\n    # Define the model using keras\n    model = keras.Sequential([\n        keras.layers.Flatten(input_shape=(28, 28)),\n        keras.layers.Dense(128, activation='relu'),\n        keras.layers.Dense(10)\n\n    ])\n\n    model.compile(optimizer='adam', loss=keras.losses.SparseCategoricalCrossentropy(\n        from_logits=True), metrics=['accuracy'])\n    model.fit(train_images, train_labels, epochs=10)\n    test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n    print('Test accuracy', test_acc)\n\n    # IMP this specifies the path inside the Docker container where our model will be saved\n    model.save(f'{data_path}/{model_file}')\n\n    # Save test data pickle file\n    with open(f'{data_path}/test_data', 'wb') as f:\n        pickle.dump((test_images, test_labels), f)\n\n\n# Glue together training function to the docker container\ntrain_op = comp.func_to_container_op(train, base_image='tensorflow/tensorflow:latest-gpu-py3')\n\n\n# define pipeline metadata like name, description etc.\n@dsl.pipeline(\n    name='MNIST Pipeline',\n    description='Pipeline that trains MNIST models on GPU'\n)\n# define virtual HDD space that the pipeline will take to run\ndef mnist_container_pipeline(data_path='/mnt', model_file='mnist_model.h5'):\n    vop = dsl.VolumeOp(\n        name='create_volume',\n        resource_name='data-volume',\n        size='1Gi',\n        modes=dsl.VOLUME_MODE_RWM\n    )\n\n# We have already created a volume and a Glued component(Docker+Python script). This Glued component needs \n# to communicate with the volume so lets attach a volume to Glued component \n    mnist_training_container = train_op(data_path, model_file) \\\n        .add_pvolumes({data_path: vop.volume})\n\n\n\n# pass these paths inside the volume that we made\npipeline_func = mnist_container_pipeline\nexperiment_name = 'fashion_mnist_kubeflow_training2'\nrun_name = pipeline_func.__name__ + ' run'\n\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(pipeline_func, __file__ + '.yaml')\n\n\n\n\n"
  },
  {
    "repo": "ajinkya933/Kubeflow",
    "file_path": "mnist_REST_API_temp.py",
    "raw_url": "https://raw.githubusercontent.com/ajinkya933/Kubeflow/master/mnist_REST_API_temp.py",
    "content": "import kfp\nimport sys\nimport kfp.dsl as dsl\nimport kfp.components as comp\n\n\ndef train(data_path, model_file):\n    '''\n        This definition contains MNIST training steps:\n            * Data Import\n            * Data Preprocessing\n            * Keras model creation\n            * Model optimizer : adam\n            * Training with specified epoch\n            * Print test accuracy\n            * Save the model\n    '''\n    import pickle\n    import tensorflow as tf\n    from tensorflow.python import keras\n\n    # Load dataset\n    fashion_mnist = keras.datasets.fashion_mnist\n    (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n\n    # Normalize dataset\n    train_images = train_images / 255.0\n    test_images = test_images / 255.0\n\n    # Define the model using keras\n    model = keras.Sequential([\n        keras.layers.Flatten(input_shape=(28, 28)),\n        keras.layers.Dense(128, activation='relu'),\n        keras.layers.Dense(10)\n\n    ])\n\n    model.compile(optimizer='adam', loss=keras.losses.SparseCategoricalCrossentropy(\n        from_logits=True), metrics=['accuracy'])\n    model.fit(train_images, train_labels, epochs=10)\n    test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n    print('Test accuracy', test_acc)\n\n    # IMP this specifies the path inside the Docker container where our model will be saved\n    model.save(f'{data_path}/{model_file}')\n\n    # Save test data pickle file\n    with open(f'{data_path}/test_data', 'wb') as f:\n        pickle.dump((test_images, test_labels), f)\n\n\n# Glue together training function to the docker container\ntrain_op = comp.func_to_container_op(train, base_image='tensorflow/tensorflow:latest-gpu-py3')\n\n\n# define pipeline metadata like name, description etc.\n@dsl.pipeline(\n    name='MNIST Pipeline',\n    description='Pipeline that trains MNIST models on GPU'\n)\n# define virtual HDD space that the pipeline will take to run\ndef mnist_container_pipeline(data_path='/mnt', model_file='mnist_model.h5'):\n    vop = dsl.VolumeOp(\n        name='create_volume',\n        resource_name='data-volume',\n        size='1Gi',\n        modes=dsl.VOLUME_MODE_RWM\n    )\n\n# We have already created a volume and a Glued component(Docker+Python script). This Glued component needs \n# to communicate with the volume so lets attach a volume to Glued component \n    mnist_training_container = train_op(data_path, model_file) \\\n        .add_pvolumes({data_path: vop.volume})\n\n\n\n# pass these paths inside the volume that we made\npipeline_func = mnist_container_pipeline\n#experiment_name = 'fashion_mnist_kubeflow_training2'\nexperiment_name = sys.argv[0]\nrun_name = pipeline_func.__name__ + ' run'\n\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(pipeline_func, __file__ + '.yaml')\n\n\n\n\n"
  },
  {
    "repo": "ajinkya933/Kubeflow",
    "file_path": "mnist_complete_train.py",
    "raw_url": "https://raw.githubusercontent.com/ajinkya933/Kubeflow/master/mnist_complete_train.py",
    "content": "import kfp\nimport kfp.dsl as dsl\nimport kfp.components as comp\n\n\ndef train(data_path, model_file):\n    '''\n        This definition contains MNIST training steps:\n            * Data Import\n            * Data Preprocessing\n            * Keras model creation\n            * Model optimizer : adam\n            * Training with specified epoch\n            * Print test accuracy\n            * Save the model\n    '''\n    import pickle\n    import tensorflow as tf\n    from tensorflow.python import keras\n\n    # Load dataset\n    fashion_mnist = keras.datasets.fashion_mnist\n    (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n\n    # Normalize dataset\n    train_images = train_images / 255.0\n    test_images = test_images / 255.0\n\n    # Define the model using keras\n    model = keras.Sequential([\n        keras.layers.Flatten(input_shape=(28, 28)),\n        keras.layers.Dense(128, activation='relu'),\n        keras.layers.Dense(10)\n\n    ])\n\n    model.compile(optimizer='adam', loss=keras.losses.SparseCategoricalCrossentropy(\n        from_logits=True), metrics=['accuracy'])\n    model.fit(train_images, train_labels, epochs=10)\n    test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n    print('Test accuracy', test_acc)\n\n    # IMP this specifies the path inside the Docker container where our model will be saved\n    model.save(f'{data_path}/{model_file}')\n\n    # Save test data pickle file\n    with open(f'{data_path}/test_data', 'wb') as f:\n        pickle.dump((test_images, test_labels), f)\n\ndef predict(data_path, model_file, image_number):\n    \n    # func_to_container_op requires packages to be imported inside of the function.\n    import pickle\n\n    import tensorflow as tf\n    from tensorflow import keras\n\n    import numpy as np\n    \n    # Load the saved Keras model\n    model = keras.models.load_model(f'{data_path}/{model_file}')\n\n    # Load and unpack the test_data\n    with open(f'{data_path}/test_data','rb') as f:\n        test_data = pickle.load(f)\n    # Separate the test_images from the test_labels.\n    test_images, test_labels = test_data\n    # Define the class names.\n    class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n                   'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n\n    # Define a Softmax layer to define outputs as probabilities\n    probability_model = tf.keras.Sequential([model, \n                                            tf.keras.layers.Softmax()])\n\n    # See https://github.com/kubeflow/pipelines/issues/2320 for explanation on this line.\n    image_number = int(image_number)\n\n    # Grab an image from the test dataset.\n    img = test_images[image_number]\n\n    # Add the image to a batch where it is the only member.\n    img = (np.expand_dims(img,0))\n\n    # Predict the label of the image.\n    predictions = probability_model.predict(img)\n\n    # Take the prediction with the highest probability\n    prediction = np.argmax(predictions[0])\n\n    # Retrieve the true label of the image from the test labels.\n    true_label = test_labels[image_number]\n    \n    class_prediction = class_names[prediction]\n    confidence = 100*np.max(predictions)\n    actual = class_names[true_label]\n    \n    \n    with open(f'{data_path}/result.txt', 'w') as result:\n        result.write(\" Prediction: {} | Confidence: {:2.0f}% | Actual: {}\".format(class_prediction,\n                                                                        confidence,\n                                                                        actual))\n    \n    print('Prediction has be saved successfully!')\n\n# Glue together training and inference function to the docker container\ntrain_op = comp.func_to_container_op(train, base_image='tensorflow/tensorflow:latest-gpu-py3')\npredict_op = comp.func_to_container_op(predict, base_image='tensorflow/tensorflow:latest-gpu-py3')\n\n\n# define pipeline metadata like name, description etc.\n@dsl.pipeline(\n    name='MNIST Pipeline for train and prediction',\n    description='Pipeline that trains MNIST models on GPU'\n)\n# define virtual HDD space that the pipeline will take to run\ndef mnist_container_pipeline(data_path='/mnt', model_file='mnist_model.h5', IMAGE_NUMBER='0'):\n    vop = dsl.VolumeOp(\n        name='create_volume',\n        resource_name='data-volume',\n        size='1Gi',\n        modes=dsl.VOLUME_MODE_RWM\n    )\n\n# We have already created a volume and a Glued component(Docker+Python script). This Glued component needs \n# to communicate with the volume so lets attach a volume to Glued component \n    mnist_training_container = train_op(data_path, model_file) \\\n        .add_pvolumes({data_path: vop.volume})\n\n    # Create MNIST prediction component.\n    mnist_predict_container = predict_op(data_path, model_file, IMAGE_NUMBER) \\\n                                    .add_pvolumes({data_path: mnist_training_container.pvolume})\n\n\n    # Print the result of the prediction\n    mnist_result_container = dsl.ContainerOp(\n        name=\"print_prediction\",\n        image='library/bash:4.4.23',\n        pvolumes={data_path: mnist_predict_container.pvolume},\n        arguments=['cat', f'{data_path}/result.txt']\n    )\n\n    #vop.delete().after(mnist_result_container)\n    #vop.delete()\n\n# pass these paths inside the volume that we made\npipeline_func = mnist_container_pipeline\n# experiment_name = 'fashion_mnist_kubeflow_training_prediction'\nrun_name = pipeline_func.__name__ + ' run'\n\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(pipeline_func, __file__ + '.yaml')\n\n\n"
  },
  {
    "repo": "omkarakaya/kubeflow-recommender",
    "file_path": "pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/omkarakaya/kubeflow-recommender/main/pipeline.py",
    "content": "import kfp.dsl as dsl\nimport kfp.gcp as gcp\nimport kfp.onprem as onprem\n\nfrom string import Template\nimport json\n\n@dsl.pipeline(\n  name='Kubeflow Pipeline Test',\n  description='Kubeflow Pipeline Test'\n)\ndef xgb_train_pipeline(\n    output,\n    project,\n    region='us-central1',\n    train_data='gs://ml-pipeline-playground/sfpd/train.csv',\n    eval_data='gs://ml-pipeline-playground/sfpd/eval.csv',\n    schema='gs://ml-pipeline-playground/sfpd/schema.json',\n    target='resolution',\n    rounds=200,\n    workers=2,\n    true_label='ACTION'):\n    #vol_common = dsl.PipelineVolume()\n    vol_common = dsl.VolumeOp(\n        name=\"create_pvc\",\n        resource_name=\"my-pvc\",\n        modes=dsl.VOLUME_MODE_RWO,\n        size=\"1Gi\"\n    )\n\n    preprocess = dsl.ContainerOp(\n        name='preprocess',\n        image='gcr.io/compose-flask/hub:v6',\n        arguments=[\n            '--project', project,\n            '--mode', 'cloud',\n        ],\n        file_outputs={'output': '/tmp/output.txt'},\n        pvolumes={\"/data\": vol_common.volume}\n    )\n    preprocess.after(vol_common)\n    build = dsl.ContainerOp(\n        name='build',\n        image='gcr.io/compose-flask/build:v6',\n        arguments=[\n            '--project', project,\n            '--mode', 'cloud',\n            '--preprocessed', preprocess.outputs['output'],\n            '--preprocessed2', preprocess.output\n        ],\n        file_outputs={'output': '/tmp/output.txt'},\n        pvolumes={\"/data\": vol_common.volume}\n    )\n    build.after(vol_common)\n    build.after(preprocess)"
  },
  {
    "repo": "II-VSB-II/TaxiClassificationKubeflowPipelineMinio",
    "file_path": "Taxi-Pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/II-VSB-II/TaxiClassificationKubeflowPipelineMinio/main/Taxi-Pipeline.py",
    "content": "import kfp\r\nfrom kfp import components\r\nfrom kfp import dsl\r\nfrom kfp import gcp\r\nfrom kfp import onprem\r\nfrom kubernetes.client.models import V1EnvVar\r\n\r\n\r\nsecretKey = V1EnvVar(name='MINIO_SECRET_KEY', value='minio123')\r\naccessKey = V1EnvVar(name='MINIO_ACCESS_KEY', value='minio')\r\nminio_endpoint = V1EnvVar(name='MINIO_ENDPOINT', value='minio-service:9000')\r\n\r\nplatform = 'local'\r\n\r\n#proxy=\"http://test:8080\"\r\nproxy = \"\"\r\n\r\ndataflow_tf_data_validation_op  = components.load_component_from_url(\r\n  'https://raw.githubusercontent.com/II-VSB-II/TaxiClassificationKubeflowPipelineMinio/main/yamls/tfdv_component.yaml')\r\ndataflow_tf_transform_op        = components.load_component_from_url(\r\n  'https://raw.githubusercontent.com/II-VSB-II/TaxiClassificationKubeflowPipelineMinio/main/yamls/tft_component.yaml')\r\ntf_train_op                     = components.load_component_from_url(\r\n  'https://raw.githubusercontent.com/II-VSB-II/TaxiClassificationKubeflowPipelineMinio/main/yamls/dnntrainer_component.yaml')\r\ndataflow_tf_model_analyze_op    = components.load_component_from_url(\r\n  'https://raw.githubusercontent.com/II-VSB-II/TaxiClassificationKubeflowPipelineMinio/main/yamls/tfma_component.yaml')\r\ndataflow_tf_predict_op          = components.load_component_from_url(\r\n  'https://raw.githubusercontent.com/II-VSB-II/TaxiClassificationKubeflowPipelineMinio/main/yamls/predict_component.yaml')\r\n\r\nconfusion_matrix_op             = components.load_component_from_url(\r\n  'https://raw.githubusercontent.com/II-VSB-II/TaxiClassificationKubeflowPipelineMinio/main/yamls/confusion_matrix_component.yaml')\r\nroc_op                          = components.load_component_from_url(\r\n  'https://raw.githubusercontent.com/II-VSB-II/TaxiClassificationKubeflowPipelineMinio/main/yamls/roc_component.yaml')\r\n\r\n@dsl.pipeline(\r\n  name='TFX Taxi Cab Classification Pipeline Example',\r\n  description='Example pipeline that does classification with model analysis based on a public BigQuery dataset.'\r\n)\r\ndef taxi_cab_classification(\r\n    project,\r\n    output=\"/mnt/shared\",\r\n    column_names='/mnt/shared/pipelines/column-names.json',\r\n    key_columns='trip_start_timestamp',\r\n    train='/mnt/shared/pipelines/train.csv',\r\n    evaluation='/mnt/shared/pipelines/eval.csv',\r\n    mode='local',\r\n    preprocess_module='/mnt/shared/pipelines/preprocessing.py',\r\n    learning_rate=0.1,\r\n    hidden_layer_size='1500',\r\n    steps=3000,\r\n    analyze_slice_column='trip_start_hour'\r\n):\r\n    output_template = str(output) + '/{{workflow.uid}}/{{pod.name}}/data'\r\n    target_lambda = \"\"\"lambda x: (x['target'] > x['fare'] * 0.2)\"\"\"\r\n    target_class_lambda = \"\"\"lambda x: 1 if (x['target'] > x['fare'] * 0.2) else 0\"\"\"\r\n\r\n    tf_server_name = 'taxi-cab-classification-model-{{workflow.uid}}'\r\n\r\n    if platform != 'GCP':\r\n        vop = dsl.VolumeOp(\r\n            name=\"create_pvc\",\r\n            resource_name=\"pipeline-pvc\",\r\n            modes=dsl.VOLUME_MODE_RWM,\r\n            size=\"1Gi\"\r\n        )\r\n        if proxy != \"\":\r\n            checkout = dsl.ContainerOp(\r\n            name=\"checkout\",\r\n            image=\"alpine/git:latest\",\r\n            command=[\"git\", \"clone\", \"https://github.com/II-VSB-II/TaxiClassificationKubeflowPipelineMinio.git\", str(output) + \r\n                     \"/pipelines\", \"-c\", \"http.proxy={}\".format(proxy)],\r\n        ).apply(onprem.mount_pvc(vop.outputs[\"name\"], 'local-storage', output))\r\n        else:\r\n            checkout = dsl.ContainerOp(\r\n            name=\"checkout\",\r\n            image=\"alpine/git:latest\",\r\n            command=[\"git\", \"clone\", \"https://github.com/II-VSB-II/TaxiClassificationKubeflowPipelineMinio.git\", str(output) + \"/pipelines\"],\r\n        ).apply(onprem.mount_pvc(vop.outputs[\"name\"], 'local-storage', output))\r\n            \r\n        \r\n        checkout.after(vop)\r\n\r\n    validation = dataflow_tf_data_validation_op(\r\n        inference_data=train,\r\n        validation_data=evaluation,\r\n        column_names=column_names,\r\n        key_columns=key_columns,\r\n        gcp_project=project,\r\n        run_mode=mode,\r\n        validation_output=output_template,\r\n    )\r\n    if platform != 'GCP':\r\n        validation.after(checkout)\r\n\r\n    preprocess = dataflow_tf_transform_op(\r\n        training_data_file_pattern=train,\r\n        evaluation_data_file_pattern=evaluation,\r\n        schema=validation.outputs['schema'],\r\n        gcp_project=project,\r\n        run_mode=mode,\r\n        preprocessing_module=preprocess_module,\r\n        transformed_data_dir=output_template\r\n    )\r\n\r\n    training = tf_train_op(\r\n        transformed_data_dir=preprocess.output,\r\n        schema=validation.outputs['schema'],\r\n        learning_rate=learning_rate,\r\n        hidden_layer_size=hidden_layer_size,\r\n        steps=steps,\r\n        target='tips',\r\n        preprocessing_module=preprocess_module,\r\n        training_output_dir=output_template\r\n    )\r\n\r\n    analysis = dataflow_tf_model_analyze_op(\r\n        model=training.output,\r\n        evaluation_data=evaluation,\r\n        schema=validation.outputs['schema'],\r\n        gcp_project=project,\r\n        run_mode=mode,\r\n        slice_columns=analyze_slice_column,\r\n        analysis_results_dir=output_template\r\n    ).add_env_variable(secretKey).add_env_variable(accessKey).add_env_variable(minio_endpoint)\r\n\r\n    prediction = dataflow_tf_predict_op(\r\n        data_file_pattern=evaluation,\r\n        schema=validation.outputs['schema'],\r\n        target_column='tips',\r\n        model=training.output,\r\n        run_mode=mode,\r\n        gcp_project=project,\r\n        predictions_dir=output_template\r\n    ).add_env_variable(secretKey).add_env_variable(accessKey).add_env_variable(minio_endpoint)\r\n\r\n\r\n    cm = confusion_matrix_op(\r\n        predictions=prediction.output,\r\n        target_lambda=target_lambda,\r\n        output_dir=output_template\r\n    ).add_env_variable(secretKey).add_env_variable(accessKey).add_env_variable(minio_endpoint)\r\n\r\n    roc = roc_op(\r\n        predictions_dir=prediction.output,\r\n        target_lambda=target_class_lambda,\r\n        output_dir=output_template\r\n    ).add_env_variable(secretKey).add_env_variable(accessKey).add_env_variable(minio_endpoint)\r\n\r\n\r\n    steps = [validation, preprocess, training, analysis, prediction, cm, roc]\r\n    \r\n    for step in steps:\r\n        step.apply(onprem.mount_pvc(vop.outputs[\"name\"], 'local-storage', output))\r\n\r\n\r\nif __name__ == '__main__':\r\n    kfp.compiler.Compiler().compile(taxi_cab_classification, \"TaxiPipelineMinio\" + '.zip')\r\n"
  },
  {
    "repo": "stackdemos/kf-flatcar2",
    "file_path": "components/golang/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/kf-flatcar2/master/components/golang/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef processing_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('GOLANG_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=[script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n"
  },
  {
    "repo": "stackdemos/kf-flatcar2",
    "file_path": "components/training/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/kf-flatcar2/master/components/training/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef training_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('TRAINING_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=['/usr/local/bin/python', script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n\n"
  },
  {
    "repo": "stackdemos/jjtest-ml",
    "file_path": "components/golang/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/jjtest-ml/master/components/golang/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef processing_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('GOLANG_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=[script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n"
  },
  {
    "repo": "stackdemos/jjtest-ml",
    "file_path": "components/training/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/jjtest-ml/master/components/training/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef training_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('TRAINING_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=['/usr/local/bin/python', script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n\n"
  },
  {
    "repo": "stackdemos/jj-test2",
    "file_path": "components/golang/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/jj-test2/master/components/golang/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef processing_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('GOLANG_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=[script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n"
  },
  {
    "repo": "stackdemos/jj-test2",
    "file_path": "components/training/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/jj-test2/master/components/training/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef training_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('TRAINING_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=['/usr/local/bin/python', script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n\n"
  },
  {
    "repo": "stackdemos/kfp125",
    "file_path": "components/golang/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/kfp125/master/components/golang/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef processing_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('GOLANG_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=[script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n"
  },
  {
    "repo": "stackdemos/kfp125",
    "file_path": "components/training/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/kfp125/master/components/training/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef training_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('TRAINING_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=['/usr/local/bin/python', script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n\n"
  },
  {
    "repo": "stackdemos/kcdemo",
    "file_path": "components/golang/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/kcdemo/master/components/golang/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef processing_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('GOLANG_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=[script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n"
  },
  {
    "repo": "stackdemos/kcdemo",
    "file_path": "components/training/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/kcdemo/master/components/training/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef training_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('TRAINING_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=['/usr/local/bin/python', script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n\n"
  },
  {
    "repo": "stackdemos/ghsumm2",
    "file_path": "components/golang/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/ghsumm2/master/components/golang/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef processing_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('GOLANG_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=[script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n"
  },
  {
    "repo": "stackdemos/ghsumm2",
    "file_path": "components/training/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/ghsumm2/master/components/training/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef training_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('TRAINING_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=['/usr/local/bin/python', script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n\n"
  },
  {
    "repo": "stackdemos/ml-kbf",
    "file_path": "components/golang/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/ml-kbf/master/components/golang/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef processing_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('GOLANG_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=[script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n"
  },
  {
    "repo": "stackdemos/ml-kbf",
    "file_path": "components/training/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/ml-kbf/master/components/training/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef training_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('TRAINING_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=['/usr/local/bin/python', script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n\n"
  },
  {
    "repo": "s102401002/kubeflowPipeline2",
    "file_path": "kubeflowPipeline_xgboost.py",
    "raw_url": "https://raw.githubusercontent.com/s102401002/kubeflowPipeline2/main/kubeflowPipeline_xgboost.py",
    "content": "# test\n\nfrom typing import NewType\n\nimport kfp\nimport kfp.compiler\n\nfrom kfp import dsl\nfrom kfp.dsl import OutputPath, InputPath, Input, Output, Artifact\n\nfrom pandas import DataFrame\n# from kfp.components import func_to_container_op\n\nDF = NewType('DF', DataFrame)\n\n@dsl.component(\n    base_image='python:3.9',\n    packages_to_install=['pandas==2.2.2']\n)\ndef load_data(data_output: Output[Artifact]):\n    import pandas as pd\n    \n    urls = [\n        \"https://raw.githubusercontent.com/daniel88516/diabetes-data/main/10k.csv\",\n        \"https://raw.githubusercontent.com/s102401002/kubeflowPipeline/main/data1.csv\"\n    ]\n    \n    standard_name_mapping = {\n        'gender': ['gender', 'gen', 'Gender', 'sex', 'Sex'],\n        'age': ['age', 'Age', 'AGE'],\n        'bmi': ['bmi', 'BMI', 'Bmi'],\n        'HbA1c_level': ['HbA1c_level', 'HbA1c', 'hba1c'],\n        'blood_glucose_level': ['blood_glucose_level', 'glucose', 'BloodGlucose'],\n        'diabetes': ['diabetes', 'Diabetes']\n    }\n\n    datas = [] # download all the csv in urls as a array\n    for url in urls:\n        df = pd.read_csv(url)\n        for standard_name, variants in standard_name_mapping.items():\n            for variant in variants:\n                if variant in df.columns:\n                    df.rename(columns={variant: standard_name}, inplace=True) # inplace=True: changing directly instead of creating a new column\n                    break\n        \n        datas.append(df)\n\n    df_data = pd.concat(datas, ignore_index=True)\n    \n    df_data = df_data.drop(df_data[df_data['diabetes'] == 'No Info'].index)\n    df_data = df_data[['gender','age', 'bmi', 'HbA1c_level', 'blood_glucose_level', 'diabetes']]\n    df_data = df_data.dropna(thresh=4)\n    \n    gender_map = {'Male': 0 , 'Female': 1  , 'Other': 2}\n    df_data['gender'] = df_data['gender'].map(gender_map)\n    df_data = df_data[df_data['gender'] != 2]\n    df_data['age'] = df_data['age'].replace('No Info', df_data['age'].mean())\n    df_data['bmi'] = df_data['bmi'].replace('No Info', df_data['bmi'].mean())\n    df_data['HbA1c_level'] = df_data['HbA1c_level'].replace('No Info', df_data['HbA1c_level'].mean())\n    df_data['blood_glucose_level'] = df_data['blood_glucose_level'].replace('No Info', df_data['blood_glucose_level'].mean())\n\n    df_data.to_csv(data_output.path)\n\n@dsl.component(\n    base_image='python:3.9',\n    packages_to_install=['pandas==2.2.2', 'scikit-learn==1.5.1']\n)\ndef prepare_data(\n    data_input: Input[Artifact], \n    x_train_output: Output[Artifact], x_test_output: Output[Artifact],\n    y_train_output: Output[Artifact], y_test_output: Output[Artifact]\n):\n    import pandas as pd\n    from sklearn.model_selection import train_test_split\n\n    df_data = pd.read_csv(data_input.path)\n\n    x = df_data.drop(labels=['diabetes'], axis=1)\n    y = df_data[['diabetes']]\n    \n    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n    \n    x_train_df = pd.DataFrame(x_train)\n    x_test_df = pd.DataFrame(x_test)\n    y_train_df = pd.DataFrame(y_train)\n    y_test_df = pd.DataFrame(y_test)\n\n    x_train_df.to_csv(x_train_output.path, index=False)\n    x_test_df.to_csv(x_test_output.path, index=False)\n    y_train_df.to_csv(y_train_output.path, index=False)\n    y_test_df.to_csv(y_test_output.path, index=False)\n\n@dsl.component(\n    base_image='python:3.9',\n    packages_to_install=['pandas==2.2.2', 'scikit-learn==1.5.1', 'joblib==1.4.2', 'xgboost==2.0.3']\n)\ndef train_model(x_train: Input[Artifact], y_train: Input[Artifact], train_model_output: Output[Artifact]):\n    import pandas as pd\n    from xgboost import XGBClassifier\n    import joblib\n    \n    x_train = pd.read_csv(x_train.path)\n    y_train = pd.read_csv(y_train.path)\n    \n    model = XGBClassifier(n_estimators=1000, learning_rate= 0.01)\n    model.fit(x_train, y_train.values.ravel())\n    \n    joblib.dump(model, train_model_output.path)\n\n@dsl.component(\n    base_image='python:3.9',\n    packages_to_install=['pandas==2.2.2', 'scikit-learn==1.5.1', 'joblib==1.4.2', 'xgboost==2.0.3']\n)\ndef evaluate_model(model_path: Input[Artifact], x_test: Input[Artifact], y_test: Input[Artifact]) -> str:\n    import pandas as pd\n    from sklearn.metrics import accuracy_score\n    import joblib\n\n    model = joblib.load(filename=model_path.path)\n\n    x_test_df = pd.read_csv(x_test.path)\n    y_test_df = pd.read_csv(y_test.path)\n    \n    y_pred = model.predict(x_test_df)\n    accuracy = accuracy_score(y_test_df, y_pred)\n    \n    return f'Test accuracy: {accuracy}'\n\n@dsl.pipeline(\n    name='Diabetes Prediction Pipeline',\n    description='Using kubeflow pipeline to train and evaluate a diabetes prediction model'\n)\ndef diabetes_prediction_pipeline() -> str:\n    load_data_task = load_data()\n\n    prepare_data_task = prepare_data(data_input=load_data_task.outputs['data_output'])\n    \n    train_model_task = train_model(\n        x_train = prepare_data_task.outputs['x_train_output'], \n        y_train = prepare_data_task.outputs['y_train_output']\n    )\n    \n    evaluate_model_task = evaluate_model(\n        model_path = train_model_task.outputs['train_model_output'], \n        x_test = prepare_data_task.outputs['x_test_output'], \n        y_test = prepare_data_task.outputs['y_test_output']\n    )\n    \n    return evaluate_model_task.output\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(diabetes_prediction_pipeline, 'diabetes_prediction_pipeline_xgboost.yaml')"
  },
  {
    "repo": "stackdemos/kf4",
    "file_path": "components/golang/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/kf4/master/components/golang/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef processing_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('GOLANG_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=[script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n"
  },
  {
    "repo": "stackdemos/kf4",
    "file_path": "components/training/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/kf4/master/components/training/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef training_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('TRAINING_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=['/usr/local/bin/python', script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n\n"
  },
  {
    "repo": "stackdemos/joe-kubeflow-test",
    "file_path": "components/golang/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/joe-kubeflow-test/master/components/golang/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef processing_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('GOLANG_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=[script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n"
  },
  {
    "repo": "stackdemos/joe-kubeflow-test",
    "file_path": "components/training/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/joe-kubeflow-test/master/components/training/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef training_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('TRAINING_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=['/usr/local/bin/python', script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n\n"
  },
  {
    "repo": "stackdemos/kfp11",
    "file_path": "components/golang/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/kfp11/master/components/golang/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef processing_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('GOLANG_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=[script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n"
  },
  {
    "repo": "stackdemos/kfp11",
    "file_path": "components/training/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/kfp11/master/components/training/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef training_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('TRAINING_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=['/usr/local/bin/python', script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n\n"
  },
  {
    "repo": "stackdemos/specs-kf5",
    "file_path": "components/golang/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/specs-kf5/master/components/golang/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef processing_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('GOLANG_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=[script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n"
  },
  {
    "repo": "stackdemos/specs-kf5",
    "file_path": "components/training/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/specs-kf5/master/components/training/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef training_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('TRAINING_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=['/usr/local/bin/python', script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n\n"
  },
  {
    "repo": "akranga/anton-ml1",
    "file_path": "components/golang/component.py",
    "raw_url": "https://raw.githubusercontent.com/akranga/anton-ml1/master/components/golang/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef processing_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('GOLANG_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=[script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n"
  },
  {
    "repo": "akranga/anton-ml1",
    "file_path": "components/training/component.py",
    "raw_url": "https://raw.githubusercontent.com/akranga/anton-ml1/master/components/training/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef training_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('TRAINING_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=['/usr/local/bin/python', script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n\n"
  },
  {
    "repo": "baebae-dev/kubeflow-pipelines",
    "file_path": "boston_housing/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/baebae-dev/kubeflow-pipelines/main/boston_housing/pipeline.py",
    "content": "import kfp\nfrom kfp import dsl\n\ndef preprocess_op():\n\n    return dsl.ContainerOp(\n        name='Preprocess Data',\n        image='gnovack/boston_pipeline_preprocessing:latest',\n        arguments=[],\n        file_outputs={\n            'x_train': '/app/x_train.npy',\n            'x_test': '/app/x_test.npy',\n            'y_train': '/app/y_train.npy',\n            'y_test': '/app/y_test.npy',\n        }\n    )\n\ndef train_op(x_train, y_train):\n\n    return dsl.ContainerOp(\n        name='Train Model',\n        image='gnovack/boston_pipeline_train:latest',\n        arguments=[\n            '--x_train', x_train,\n            '--y_train', y_train\n        ],\n        file_outputs={\n            'model': '/app/model.pkl'\n        }\n    )\n\ndef test_op(x_test, y_test, model):\n\n    return dsl.ContainerOp(\n        name='Test Model',\n        image='gnovack/boston_pipeline_test:latest',\n        arguments=[\n            '--x_test', x_test,\n            '--y_test', y_test,\n            '--model', model\n        ],\n        file_outputs={\n            'mean_squared_error': '/app/output.txt'\n        }\n    )\n\ndef deploy_model_op(model):\n\n    return dsl.ContainerOp(\n        name='Deploy Model',\n        image='gnovack/boston_pipeline_deploy_model:latest',\n        arguments=[\n            '--model', model\n        ]\n    )\n\n@dsl.pipeline(\n   name='Boston Housing Pipeline',\n   description='An example pipeline that trains and logs a regression model.'\n)\ndef boston_pipeline():\n    _preprocess_op = preprocess_op()\n    \n    _train_op = train_op(\n        dsl.InputArgumentPath(_preprocess_op.outputs['x_train']),\n        dsl.InputArgumentPath(_preprocess_op.outputs['y_train'])\n    ).after(_preprocess_op)\n\n    _test_op = test_op(\n        dsl.InputArgumentPath(_preprocess_op.outputs['x_test']),\n        dsl.InputArgumentPath(_preprocess_op.outputs['y_test']),\n        dsl.InputArgumentPath(_train_op.outputs['model'])\n    ).after(_train_op)\n\n    deploy_model_op(\n        dsl.InputArgumentPath(_train_op.outputs['model'])\n    ).after(_test_op)\n\nclient = kfp.Client()\nclient.create_run_from_pipeline_func(boston_pipeline, arguments={})"
  },
  {
    "repo": "stackdemos/secrets-tetst-01",
    "file_path": "components/golang/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/secrets-tetst-01/master/components/golang/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef processing_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('GOLANG_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=[script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n"
  },
  {
    "repo": "stackdemos/secrets-tetst-01",
    "file_path": "components/training/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/secrets-tetst-01/master/components/training/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef training_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('TRAINING_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=['/usr/local/bin/python', script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n\n"
  },
  {
    "repo": "stackdemos/kfappx",
    "file_path": "components/golang/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/kfappx/master/components/golang/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef processing_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('GOLANG_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=[script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n"
  },
  {
    "repo": "stackdemos/kfappx",
    "file_path": "components/training/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/kfappx/master/components/training/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef training_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('TRAINING_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=['/usr/local/bin/python', script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n\n"
  },
  {
    "repo": "akranga/machine-learning1",
    "file_path": "components/golang/component.py",
    "raw_url": "https://raw.githubusercontent.com/akranga/machine-learning1/master/components/golang/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef processing_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('GOLANG_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=[script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n"
  },
  {
    "repo": "akranga/machine-learning1",
    "file_path": "components/training/component.py",
    "raw_url": "https://raw.githubusercontent.com/akranga/machine-learning1/master/components/training/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef training_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('TRAINING_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=['/usr/local/bin/python', script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n\n"
  },
  {
    "repo": "ArianFotouhi/kubeflowPipelineSpamDetector",
    "file_path": "script.py",
    "raw_url": "https://raw.githubusercontent.com/ArianFotouhi/kubeflowPipelineSpamDetector/main/script.py",
    "content": "import kfp.dsl as dsl\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn import metrics\nimport string\nfrom typing import List\nimport joblib\n\n\n# Component 1: Extract data\n@dsl.component(\n    base_image='python:3.8',  # Specifying the base image\n    packages_to_install=['requests', 'pandas']\n)\ndef extract_data() -> dsl.OutputPath(str):\n    import requests\n    import zipfile\n    import io\n    import pandas as pd\n\n    url = 'https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip'\n\n    # Step 1: Download the zip file\n    response = requests.get(url)\n    response.raise_for_status()  # Check if the request was successful\n\n    # Step 2: Extract the contents of the zip file\n    with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n        with z.open('SMSSpamCollection') as f:\n            # Step 3: Read the contents into a DataFrame\n            df = pd.read_csv(f, sep='\\t', names=[\"label\", \"message\"], header=None)\n\n    # Step 4: Save the DataFrame to a CSV file\n    output_path = '/mnt/data/smsspamcollection.csv'\n    df.to_csv(output_path, index=False)\n\n    return output_path\n\n\n# Component 2: Data Preprocessing\n@dsl.component(\n    base_image='python:3.8',\n    packages_to_install=['pandas']\n)\ndef preprocess_data(file_path: dsl.InputPath(str)) -> None:\n    import pandas as pd\n    import string\n    \n    df = pd.read_csv(file_path)\n    \n    # Add 'length' and 'punct' features\n    df['length'] = df['message'].apply(len)\n    df['punct'] = df['message'].apply(lambda x: sum([1 for char in x if char in string.punctuation]))\n    \n    # Save the preprocessed data\n    df.to_csv('/mnt/data/preprocessed_smsspamcollection.csv', index=False)\n\n\n# Component 3: Exploratory Data Analysis (EDA)\n@dsl.component(\n    base_image='python:3.8',\n    packages_to_install=['pandas', 'matplotlib', 'numpy']\n)\ndef eda() -> None:\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    import numpy as np\n\n    df = pd.read_csv('/mnt/data/preprocessed_smsspamcollection.csv')\n\n    print('Missing values: ')\n    print(df.isnull().sum(),'\\n')\n\n    print('Categories: ',df['label'].unique(),'\\n')\n\n    print('Rate of each category: ')\n    print(df['label'].value_counts())\n\n    plt.xscale('log')\n    bins = 1.15**(np.arange(0,50))\n    plt.hist(df[df['label']=='ham']['length'], bins=bins,alpha=0.8)\n    plt.hist(df[df['label']=='spam']['length'], bins=bins,alpha=0.8)\n    plt.legend(('ham','spam'))\n    plt.title('Inference: usually spams are longer in text compared to ham')\n    plt.xlabel('Text length')\n    plt.ylabel('Category rate')\n    plt.savefig('/mnt/data/length_histogram.png')\n    plt.clf()\n\n    plt.xscale('log')\n    bins = 1.15**(np.arange(0,50))\n    plt.hist(df[df['label']=='ham']['punct'], bins=bins,alpha=0.8)\n    plt.hist(df[df['label']=='spam']['punct'], bins=bins,alpha=0.8)\n    plt.legend(('ham','spam'))\n    plt.title('Inference: a small tendency of spams towards more punctutations (not a firm inference)')\n    plt.xlabel('Text length')\n    plt.ylabel('Category rate')\n    plt.savefig('/mnt/data/punct_histogram.png')\n\n\n# Component 4: Train Model\n@dsl.component(\n    base_image='python:3.8',\n    packages_to_install=['pandas', 'scikit-learn', 'joblib']\n)\ndef train_model() -> None:\n    import pandas as pd\n    from sklearn.model_selection import train_test_split\n    from sklearn.pipeline import Pipeline\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.feature_extraction.text import TfidfVectorizer\n    from sklearn import metrics\n    import joblib\n\n    df = pd.read_csv('/mnt/data/preprocessed_smsspamcollection.csv')\n\n    X = df['message']\n    y = df['label']\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)\n\n    text_clf = Pipeline([('tfidf', TfidfVectorizer()), ('clf', RandomForestClassifier())])\n    text_clf.fit(X_train, y_train)\n\n    predictions = text_clf.predict(X_test)\n\n    df_conf_mat = pd.DataFrame(metrics.confusion_matrix(y_test, predictions), index=['ham', 'spam'], columns=['ham', 'spam'])\n    print(df_conf_mat, '\\n')\n\n    clf_report = metrics.classification_report(y_test, predictions)\n    print(clf_report, '\\n')\n\n    acc = metrics.accuracy_score(y_test, predictions)\n    print('Model accuracy: ', acc * 100)\n\n    # Save the model to a file\n    joblib.dump(text_clf, '/mnt/data/text_clf.joblib')\n\n\n# Component 5: Test Model\n@dsl.component(\n    base_image='python:3.8',\n    packages_to_install=['scikit-learn', 'joblib']\n)\ndef test_model( sample_messages: List[str]) -> List[str]:\n    import joblib\n    model = joblib.load('/mnt/data/text_clf.joblib')\n    predictions = model.predict(sample_messages)\n    return predictions\n\n\n# Define the pipeline\n@dsl.pipeline(\n    name='SMS Spam Detection Pipeline',\n    description='A pipeline for detecting spam messages from SMS data'\n)\ndef sms_spam_detection_pipeline():\n    # Step 1: Extract data\n    extracted_data = extract_data()\n    print(extracted_data.output)\n    # Step 2: Preprocess data\n    preprocess_data(\n        file_path=extracted_data.output,\n    )\n\n    # Step 3: Perform EDA\n    eda()\n\n    # Step 4: Train model\n    train_model()\n\n    # Step 5: Test model\n    test_samples = ['Hi, how you doing?', 'Congratulations! You have won a $1000 prize! Text 1 to 1423.']\n    test_output = test_model(sample_messages=test_samples)\n    print(test_output)\n\n# Compile the pipeline\nif __name__ == '__main__':\n    import kfp.compiler as compiler\n    compiler.Compiler().compile(sms_spam_detection_pipeline, 'sms_spam_detection_pipeline.yaml')\n"
  },
  {
    "repo": "felipeacunago/kubeflow",
    "file_path": "pipelines/prophet_prediction.py",
    "raw_url": "https://raw.githubusercontent.com/felipeacunago/kubeflow/master/pipelines/prophet_prediction.py",
    "content": "\"\"\"\nKubeflow Pipelines for timeseries prediction using fbprophet\nRun this script to compile pipeline\n\"\"\"\n\n\nimport kfp.dsl as dsl\nimport kfp.components as comp\nimport kfp.gcp as gcp\nimport json\n\nbigquery_query_op = comp.load_component_from_url(\n    'https://raw.githubusercontent.com/kubeflow/pipelines/e598176c02f45371336ccaa819409e8ec83743df/components/gcp/bigquery/query/component.yaml')\n\nranker_op = comp.load_component_from_url(\n    'https://raw.githubusercontent.com/membrilloski/kubeflow/master/ranker/component.yaml'\n)\n\n@dsl.pipeline(\n  name='Prophet',\n  description='A pipeline to train and serve the MNIST example.'\n)\ndef prophet_pipeline(dataset_query='',\n                   dictionary_query='',\n                   val_dataset_query='',\n                   minimum_length=10,\n                   training_date='2019-10-05',\n                   changepoint_prior_scale=0.01,\n                   evaluation_date='',\n                   evaluation_maximum_distance=1\n                    ):\n\n    \"\"\"\n      Pipeline with three stages:\n        1. Query data from Bigquery and save to storage\n        2. Generate a timeseries prediction for every distinct element in split_column\n        3. Generate a ranking based on prediction data and compare it to a ranking made with real data from that date\n    \"\"\"\n\n    rank_path_names=[''] # name of paths used for ranking (for example clicks)\n    ranking_factors=[] # factors used for ranking (score: a*x+b*y, ranking_factors=[a,b])\n    project_id='' # project name\n    split_column='' # column used to split the time series from the dataset\n    gcs_root='' # root path where all experiments files will be saved\n    prediction_y='' # y column name used for the timeseries\n    predict_periods=1 # number of periods used for timeseries prediction\n    dataset_location='US' # dataset location\n    ds_column='' # name of the column used for time\n    predict_freq='D' # prediction frequency (D: days)\n    order_ds = 'asc' # used to order items by date after splitting\n\n    original_dataset_path='{}/input/dataset.csv'.format(gcs_root)\n    val_dataset_path='{}/input/val_dataset.csv'.format(gcs_root)\n    val_split_output_path='{}/output/validation/'.format(gcs_root)\n    preprocess_output_path='{}/output/training/'.format(gcs_root)\n    model_output_path='{}/models/'.format(gcs_root)\n    predictions_path='{}/predictions/'.format(gcs_root)\n    dictionary_file_path = '{}/dictionary/dictionary.csv'.format(gcs_root)\n    prophet_rank_output_path= '{}/rankings/prediction/'.format(gcs_root)\n    validation_rank_output='{}/rankings/validation/'.format(gcs_root)\n    results_output='{}/results/'.format(gcs_root)\n\n    \n\n    dataset_query_op = bigquery_query_op(\n        query=dataset_query, \n        project_id=project_id,\n        output_gcs_path=original_dataset_path, \n        dataset_location=dataset_location, \n        job_config='').apply(gcp.use_gcp_secret('user-gcp-sa'))\n\n    preprocess = dsl.ContainerOp(\n        name='preprocess-split',\n        image='docker.io/felipeacunago/dataset-preprocess:latest',\n        arguments=[\n            \"task.py\",\n            \"--dataset-path\", dataset_query_op.outputs['output_gcs_path'],\n            \"--output-path\", preprocess_output_path,\n            \"--split-column\", split_column,\n            \"--ds-column\", ds_column,\n            \"--y-column\", prediction_y,\n            \"--minimum-length\", minimum_length,\n            \"--order-ds\", 'asc',\n            \"--training-date\", training_date\n            ]\n    ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n\n    timeseries_prophet_train = dsl.ContainerOp(\n        name='prophet-train',\n        image='docker.io/felipeacunago/timeseries-prophet-train:latest',\n        arguments=[\n            \"train.py\",\n            \"--dataset-path\", preprocess_output_path,\n            \"--changepoint-prior-scale\", changepoint_prior_scale,\n            \"--model-output-path\", model_output_path\n            ]\n    ).apply(gcp.use_gcp_secret('user-gcp-sa')).after(preprocess)\n\n    timeseries_prophet_predict = dsl.ContainerOp(\n        name='prophet-predict',\n        image='docker.io/felipeacunago/timeseries-prophet-predict:latest',\n        arguments=[\n            \"predict.py\",\n            \"--predict-periods\", predict_periods,\n            \"--predict-freq\", predict_freq,\n            \"--model-path\", model_output_path,\n            \"--predictions-path\", predictions_path\n            ]\n    ).apply(gcp.use_gcp_secret('user-gcp-sa')).after(timeseries_prophet_train)\n\n    dataset_query_op = bigquery_query_op(\n        query=dictionary_query, \n        project_id=project_id,\n        output_gcs_path=dictionary_file_path, \n        dataset_location=dataset_location, \n        job_config='').apply(gcp.use_gcp_secret('user-gcp-sa'))\n\n    prophet_rank = ranker_op(\n        input_path=predictions_path,\n        input_path_names=rank_path_names,\n        ranking_factors=ranking_factors,\n        input_dictionary=dataset_query_op.outputs['output_gcs_path'],\n        training_date=training_date,\n        ranking_output_path=prophet_rank_output_path,\n        prediction_periods=predict_periods\n    ).apply(gcp.use_gcp_secret('user-gcp-sa')).after(timeseries_prophet_predict)\n\n    val_query_op = bigquery_query_op(\n        query=val_dataset_query, \n        project_id=project_id,\n        output_gcs_path=val_dataset_path, \n        dataset_location=dataset_location, \n        job_config='').apply(gcp.use_gcp_secret('user-gcp-sa'))\n\n    val_preprocess = dsl.ContainerOp(\n        name='validation-preprocess-split',\n        image='docker.io/felipeacunago/dataset-preprocess:latest',\n        arguments=[\n            \"task.py\",\n            \"--dataset-path\", val_query_op.outputs['output_gcs_path'],\n            \"--output-path\", val_split_output_path,\n            \"--split-column\", split_column,\n            \"--ds-column\", ds_column,\n            \"--y-column\", prediction_y,\n            \"--minimum-length\", 1,\n            \"--order-ds\", order_ds,\n            \"--training-date\", evaluation_date\n            ]\n    ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n\n    val_rank = ranker_op(\n        input_path=val_split_output_path,\n        input_path_names=rank_path_names,\n        ranking_factors=ranking_factors,\n        input_dictionary=dataset_query_op.outputs['output_gcs_path'],\n        training_date=evaluation_date,\n        ranking_output_path=validation_rank_output,\n        prediction_periods=predict_periods\n    ).apply(gcp.use_gcp_secret('user-gcp-sa')).after(val_preprocess)\n\n    rank_evaluation = dsl.ContainerOp(\n        name='rank-evaluation',\n        image='docker.io/felipeacunago/ranker-eval:latest',\n        arguments=[\n            \"task.py\",\n            \"--predicted-ranking-path\", prophet_rank.outputs['ranking_output_path_file'],\n            \"--real-ranking-path\", val_rank.outputs['ranking_output_path_file'],\n            \"--eval-date\", training_date,\n            \"--maximum-distance\", evaluation_maximum_distance,\n            \"--output\", results_output,\n            ],\n        output_artifact_paths={\n            'mlpipeline-ui-metadata': '/mlpipeline-ui-metadata.json',\n        }\n    ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n\nif __name__ == '__main__':\n  import kfp.compiler as compiler\n  compiler.Compiler().compile(prophet_pipeline, __file__ + '.tar.gz')"
  },
  {
    "repo": "rakesh283343/kubeflow-sample-pipelines",
    "file_path": "core/condition/condition.py",
    "raw_url": "https://raw.githubusercontent.com/rakesh283343/kubeflow-sample-pipelines/master/core/condition/condition.py",
    "content": "#!/usr/bin/env python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp\nfrom kfp import dsl\n\n\ndef random_num_op(low, high):\n    \"\"\"Generate a random number between low and high.\"\"\"\n    return dsl.ContainerOp(\n        name='Generate random number',\n        image='python:alpine3.6',\n        command=['sh', '-c'],\n        arguments=['python -c \"import random; print(random.randint($0, $1))\" | tee $2', str(low), str(high), '/tmp/output'],\n        file_outputs={'output': '/tmp/output'}\n    )\n\n\ndef flip_coin_op():\n    \"\"\"Flip a coin and output heads or tails randomly.\"\"\"\n    return dsl.ContainerOp(\n        name='Flip coin',\n        image='python:alpine3.6',\n        command=['sh', '-c'],\n        arguments=['python -c \"import random; result = \\'heads\\' if random.randint(0,1) == 0 '\n                  'else \\'tails\\'; print(result)\" | tee /tmp/output'],\n        file_outputs={'output': '/tmp/output'}\n    )\n\n\ndef print_op(msg):\n    \"\"\"Print a message.\"\"\"\n    return dsl.ContainerOp(\n        name='Print',\n        image='alpine:3.6',\n        command=['echo', msg],\n    )\n    \n\n@dsl.pipeline(\n    name='Conditional execution pipeline',\n    description='Shows how to use dsl.Condition().'\n)\ndef flipcoin_pipeline():\n    flip = flip_coin_op()\n    with dsl.Condition(flip.output == 'heads'):\n        random_num_head = random_num_op(0, 9)\n        with dsl.Condition(random_num_head.output > 5):\n            print_op('heads and %s > 5!' % random_num_head.output)\n        with dsl.Condition(random_num_head.output <= 5):\n            print_op('heads and %s <= 5!' % random_num_head.output)\n\n    with dsl.Condition(flip.output == 'tails'):\n        random_num_tail = random_num_op(10, 19)\n        with dsl.Condition(random_num_tail.output > 15):\n            print_op('tails and %s > 15!' % random_num_tail.output)\n        with dsl.Condition(random_num_tail.output <= 15):\n            print_op('tails and %s <= 15!' % random_num_tail.output)\n\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(flipcoin_pipeline, __file__ + '.yaml')\n"
  },
  {
    "repo": "rakesh283343/kubeflow-sample-pipelines",
    "file_path": "core/execution_order/execution_order.py",
    "raw_url": "https://raw.githubusercontent.com/rakesh283343/kubeflow-sample-pipelines/master/core/execution_order/execution_order.py",
    "content": "#!/usr/bin/env python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport kfp\nfrom kfp import dsl\n\n\ndef echo1_op(text1):\n  return dsl.ContainerOp(\n      name='echo1',\n      image='library/bash:4.4.23',\n      command=['sh', '-c'],\n      arguments=['echo \"$0\"', text1])\n\n\ndef echo2_op(text2):\n  return dsl.ContainerOp(\n      name='echo2',\n      image='library/bash:4.4.23',\n      command=['sh', '-c'],\n      arguments=['echo \"$0\"', text2])\n\n\n@dsl.pipeline(\n    name='Execution order pipeline',\n    description='A pipeline to demonstrate execution order management.'\n)\ndef execution_order_pipeline(text1='message 1', text2='message 2'):\n  \"\"\"A two step pipeline with an explicitly defined execution order.\"\"\"\n  step1_task = echo1_op(text1)\n  step2_task = echo2_op(text2)\n  step2_task.after(step1_task)\n\nif __name__ == '__main__':\n  kfp.compiler.Compiler().compile(execution_order_pipeline, __file__ + '.yaml')"
  },
  {
    "repo": "rakesh283343/kubeflow-sample-pipelines",
    "file_path": "core/exit_handler/exit_handler.py",
    "raw_url": "https://raw.githubusercontent.com/rakesh283343/kubeflow-sample-pipelines/master/core/exit_handler/exit_handler.py",
    "content": "#!/usr/bin/env python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport kfp\nfrom kfp import dsl\n\n\ndef gcs_download_op(url):\n    return dsl.ContainerOp(\n        name='GCS - Download',\n        image='google/cloud-sdk:279.0.0',\n        command=['sh', '-c'],\n        arguments=['gsutil cat $0 | tee $1', url, '/tmp/results.txt'],\n        file_outputs={\n            'data': '/tmp/results.txt',\n        }\n    )\n\n\ndef echo_op(text):\n    return dsl.ContainerOp(\n        name='echo',\n        image='library/bash:4.4.23',\n        command=['sh', '-c'],\n        arguments=['echo \"$0\"', text],\n    )\n\n\n@dsl.pipeline(\n    name='Exit Handler',\n    description='Downloads a message and prints it. The exit handler will run after the pipeline finishes (successfully or not).'\n)\ndef download_and_print(url='gs://ml-pipeline/shakespeare/shakespeare1.txt'):\n    \"\"\"A sample pipeline showing exit handler.\"\"\"\n\n    exit_task = echo_op('exit!')\n\n    with dsl.ExitHandler(exit_task):\n        download_task = gcs_download_op(url)\n        echo_task = echo_op(download_task.output)\n\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(download_and_print, __file__ + '.yaml')\n"
  },
  {
    "repo": "rakesh283343/kubeflow-sample-pipelines",
    "file_path": "core/hello_world/hello_world.py",
    "raw_url": "https://raw.githubusercontent.com/rakesh283343/kubeflow-sample-pipelines/master/core/hello_world/hello_world.py",
    "content": "# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp\nfrom kfp import dsl\n\ndef echo_op():\n    return dsl.ContainerOp(\n        name='echo',\n        image='library/bash:4.4.23',\n        command=['sh', '-c'],\n        arguments=['echo \"hello world\"']\n    )\n\n@dsl.pipeline(\n    name='My first pipeline',\n    description='A hello world pipeline.'\n)\ndef hello_world_pipeline():\n    echo_task = echo_op()\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(hello_world_pipeline, __file__ + '.yaml')"
  },
  {
    "repo": "rakesh283343/kubeflow-sample-pipelines",
    "file_path": "core/imagepullsecrets/imagepullsecrets.py",
    "raw_url": "https://raw.githubusercontent.com/rakesh283343/kubeflow-sample-pipelines/master/core/imagepullsecrets/imagepullsecrets.py",
    "content": "# Copyright 2018 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Toy example demonstrating how to specify imagepullsecrets to access protected\ncontainer registry.\n\"\"\"\n\nimport kfp\nimport kfp.dsl as dsl\nfrom kubernetes import client as k8s_client\n\n\nclass GetFrequentWordOp(dsl.ContainerOp):\n  \"\"\"A get frequent word class representing a component in ML Pipelines.\n\n  The class provides a nice interface to users by hiding details such as container,\n  command, arguments.\n  \"\"\"\n  def __init__(self, name, message):\n    \"\"\"Args:\n         name: An identifier of the step which needs to be unique within a pipeline.\n         message: a dsl.PipelineParam object representing an input message.\n    \"\"\"\n    super(GetFrequentWordOp, self).__init__(\n        name=name,\n        image='python:3.5-jessie',\n        command=['sh', '-c'],\n        arguments=['python -c \"from collections import Counter; '\n                   'words = Counter(\\'%s\\'.split()); print(max(words, key=words.get))\" '\n                   '| tee /tmp/message.txt' % message],\n        file_outputs={'word': '/tmp/message.txt'})\n\n@dsl.pipeline(\n  name='Save Most Frequent',\n  description='Get Most Frequent Word and Save to GCS'\n)\ndef save_most_frequent_word(message: str):\n  \"\"\"A pipeline function describing the orchestration of the workflow.\"\"\"\n\n  counter = GetFrequentWordOp(\n          name='get-Frequent',\n          message=message)\n  # Call set_image_pull_secrets after get_pipeline_conf().\n  dsl.get_pipeline_conf()\\\n    .set_image_pull_secrets([k8s_client.V1ObjectReference(name=\"secretA\")])\n\nif __name__ == '__main__':\n  kfp.compiler.Compiler().compile(save_most_frequent_word, __file__ + '.yaml')\n"
  },
  {
    "repo": "rakesh283343/kubeflow-sample-pipelines",
    "file_path": "core/loop_output/loop_output.py",
    "raw_url": "https://raw.githubusercontent.com/rakesh283343/kubeflow-sample-pipelines/master/core/loop_output/loop_output.py",
    "content": "# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport kfp\nfrom kfp import dsl\n\n\n@dsl.pipeline(name='my-pipeline')\ndef pipeline():\n    op0 = dsl.ContainerOp(\n        name=\"my-out-cop0\",\n        image='python:alpine3.6',\n        command=[\"sh\", \"-c\"],\n        arguments=[\n            'python -c \"import json; import sys; json.dump([i for i in range(20, 31)], open(\\'/tmp/out.json\\', \\'w\\'))\"'],\n        file_outputs={'out': '/tmp/out.json'},\n    )\n\n    with dsl.ParallelFor(op0.output) as item:\n        op1 = dsl.ContainerOp(\n            name=\"my-in-cop1\",\n            image=\"library/bash:4.4.23\",\n            command=[\"sh\", \"-c\"],\n            arguments=[\"echo do output op1 item: %s\" % item],\n        )\n\n    op_out = dsl.ContainerOp(\n        name=\"my-out-cop2\",\n        image=\"library/bash:4.4.23\",\n        command=[\"sh\", \"-c\"],\n        arguments=[\"echo do output op2, outp: %s\" % op0.output],\n    )\n\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(pipeline, __file__ + '.yaml')\n"
  },
  {
    "repo": "rakesh283343/kubeflow-sample-pipelines",
    "file_path": "core/loop_parameter/loop_parameter.py",
    "raw_url": "https://raw.githubusercontent.com/rakesh283343/kubeflow-sample-pipelines/master/core/loop_parameter/loop_parameter.py",
    "content": "# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp\nfrom kfp import dsl\n\n\n@dsl.pipeline(name='my-pipeline')\ndef pipeline(loopidy_doop=[{'a': 1, 'b': 2}, {'a': 10, 'b': 20}]):\n    op0 = dsl.ContainerOp(\n        name=\"my-out-cop0\",\n        image='python:alpine3.6',\n        command=[\"sh\", \"-c\"],\n        arguments=[\n            'python -c \"import json; import sys; json.dump([i for i in range(20, 31)], open(\\'/tmp/out.json\\', \\'w\\'))\"'],\n        file_outputs={'out': '/tmp/out.json'},\n    )\n\n    with dsl.ParallelFor(loopidy_doop) as item:\n        op1 = dsl.ContainerOp(\n            name=\"my-in-cop1\",\n            image=\"library/bash:4.4.23\",\n            command=[\"sh\", \"-c\"],\n            arguments=[\"echo no output global op1, item.a: %s\" % item.a],\n        ).after(op0)\n\n    op_out = dsl.ContainerOp(\n        name=\"my-out-cop2\",\n        image=\"library/bash:4.4.23\",\n        command=[\"sh\", \"-c\"],\n        arguments=[\"echo no output global op2, outp: %s\" % op0.output],\n    )\n\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(pipeline, __file__ + '.yaml')\n"
  },
  {
    "repo": "rakesh283343/kubeflow-sample-pipelines",
    "file_path": "core/loop_static/loop_static.py",
    "raw_url": "https://raw.githubusercontent.com/rakesh283343/kubeflow-sample-pipelines/master/core/loop_static/loop_static.py",
    "content": "# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp.dsl as dsl\nimport kfp\n\n\n@dsl.pipeline(name='my-pipeline')\ndef pipeline(my_pipe_param=10):\n    loop_args = [{'A_a': 1, 'B_b': 2}, {'A_a': 10, 'B_b': 20}]\n    with dsl.ParallelFor(loop_args) as item:\n        op1 = dsl.ContainerOp(\n            name=\"my-in-coop1\",\n            image=\"library/bash:4.4.23\",\n            command=[\"sh\", \"-c\"],\n            arguments=[\"echo op1 %s %s\" % (item.A_a, my_pipe_param)],\n        )\n\n        op2 = dsl.ContainerOp(\n            name=\"my-in-coop2\",\n            image=\"library/bash:4.4.23\",\n            command=[\"sh\", \"-c\"],\n            arguments=[\"echo op2 %s\" % item.B_b],\n        )\n\n    op_out = dsl.ContainerOp(\n        name=\"my-out-cop\",\n        image=\"library/bash:4.4.23\",\n        command=[\"sh\", \"-c\"],\n        arguments=[\"echo %s\" % my_pipe_param],\n    )\n\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(pipeline, __file__ + '.yaml')\n"
  },
  {
    "repo": "rakesh283343/kubeflow-sample-pipelines",
    "file_path": "core/parallel_join/parallel_join.py",
    "raw_url": "https://raw.githubusercontent.com/rakesh283343/kubeflow-sample-pipelines/master/core/parallel_join/parallel_join.py",
    "content": "#!/usr/bin/env python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport kfp\nfrom kfp import dsl\n\ndef gcs_download_op(url):\n    return dsl.ContainerOp(\n        name='GCS - Download',\n        image='google/cloud-sdk:279.0.0',\n        command=['sh', '-c'],\n        arguments=['gsutil cat $0 | tee $1', url, '/tmp/results.txt'],\n        file_outputs={\n            'data': '/tmp/results.txt',\n        }\n    )\n\n\ndef echo2_op(text1, text2):\n    return dsl.ContainerOp(\n        name='echo',\n        image='library/bash:4.4.23',\n        command=['sh', '-c'],\n        arguments=['echo \"Text 1: $0\"; echo \"Text 2: $1\"', text1, text2]\n    )\n\n\n@dsl.pipeline(\n  name='Parallel pipeline',\n  description='Download two messages in parallel and prints the concatenated result.'\n)\ndef download_and_join(\n    url1='gs://ml-pipeline/sample-data/shakespeare/shakespeare1.txt',\n    url2='gs://ml-pipeline/sample-data/shakespeare/shakespeare2.txt'\n):\n    \"\"\"A three-step pipeline with first two running in parallel.\"\"\"\n\n    download1_task = gcs_download_op(url1)\n    download2_task = gcs_download_op(url2)\n\n    echo_task = echo2_op(download1_task.output, download2_task.output)\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(download_and_join, __file__ + '.yaml')\n"
  },
  {
    "repo": "rakesh283343/kubeflow-sample-pipelines",
    "file_path": "core/pipeline_parallelism_limits/pipeline_parallelism_limits.py",
    "raw_url": "https://raw.githubusercontent.com/rakesh283343/kubeflow-sample-pipelines/master/core/pipeline_parallelism_limits/pipeline_parallelism_limits.py",
    "content": "#!/usr/bin/env python3\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport kfp\nfrom kfp import dsl\n\n\ndef print_op(msg):\n  \"\"\"Print a message.\"\"\"\n  return dsl.ContainerOp(\n      name='Print',\n      image='alpine:3.6',\n      command=['echo', msg],\n  )\n\n\n@dsl.pipeline(\n    name='Pipeline service account',\n    description='The pipeline shows how to set the max number of parallel pods in a pipeline.'\n)\ndef pipeline_parallelism():\n  op1 = print_op('hey, what are you up to?')\n  op2 = print_op('train my model.')\n  dsl.get_pipeline_conf().set_parallelism(1)\n\nif __name__ == '__main__':\n  kfp.compiler.Compiler().compile(pipeline_parallelism, __file__ + '.yaml')\n"
  },
  {
    "repo": "rakesh283343/kubeflow-sample-pipelines",
    "file_path": "core/pipeline_transformers/pipeline_transformers.py",
    "raw_url": "https://raw.githubusercontent.com/rakesh283343/kubeflow-sample-pipelines/master/core/pipeline_transformers/pipeline_transformers.py",
    "content": "#!/usr/bin/env python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport kfp\nfrom kfp import dsl\n\ndef print_op(msg):\n  \"\"\"Print a message.\"\"\"\n  return dsl.ContainerOp(\n      name='Print',\n      image='alpine:3.6',\n      command=['echo', msg],\n  )\n\ndef add_annotation(op):\n  op.add_pod_annotation(name='hobby', value='football')\n  return op\n\n@dsl.pipeline(\n    name='Pipeline transformer',\n    description='The pipeline shows how to apply functions to all ops in the pipeline by pipeline transformers'\n)\ndef transform_pipeline():\n  op1 = print_op('hey, what are you up to?')\n  op2 = print_op('train my model.')\n  dsl.get_pipeline_conf().add_op_transformer(add_annotation)\n\nif __name__ == '__main__':\n  kfp.compiler.Compiler().compile(transform_pipeline, __file__ + '.yaml')\n"
  },
  {
    "repo": "rakesh283343/kubeflow-sample-pipelines",
    "file_path": "core/recursion/recursion.py",
    "raw_url": "https://raw.githubusercontent.com/rakesh283343/kubeflow-sample-pipelines/master/core/recursion/recursion.py",
    "content": "#!/usr/bin/env python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Notice: caching is tricky when recursion is involved. Please be careful and \n# set proper max_cache_staleness in case of infinite loop.\n\nimport kfp\nfrom kfp import dsl\n\n\ndef flip_coin_op():\n    \"\"\"Flip a coin and output heads or tails randomly.\"\"\"\n    return dsl.ContainerOp(\n        name='Flip coin',\n        image='python:alpine3.6',\n        command=['sh', '-c'],\n        arguments=['python -c \"import random; result = \\'heads\\' if random.randint(0,1) == 0 '\n                  'else \\'tails\\'; print(result)\" | tee /tmp/output'],\n        file_outputs={'output': '/tmp/output'}\n    )\n\n\ndef print_op(msg):\n    \"\"\"Print a message.\"\"\"\n    return dsl.ContainerOp(\n        name='Print',\n        image='alpine:3.6',\n        command=['echo', msg],\n    )\n\n\n# Use the dsl.graph_component to decorate pipeline functions that can be\n# recursively called.\n@dsl.graph_component\ndef flip_component(flip_result):\n    print_flip = print_op(flip_result)\n    flipA = flip_coin_op().after(print_flip)\n    # set max_cache_staleness to 0 to prevent infinite loop due to caching\n    flipA.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n    with dsl.Condition(flipA.output == 'heads'):\n        # When the flip_component is called recursively, the flipA.output\n        # from inside the graph component will be passed to the next flip_component\n        # as the input whereas the flip_result in the current graph component\n        # comes from the flipA.output in the flipcoin function.\n        flip_component(flipA.output)\n\n\n@dsl.pipeline(\n    name='Recursive loop pipeline',\n    description='Shows how to create recursive loops.'\n)\ndef flipcoin():\n    first_flip = flip_coin_op()\n    # set max_cache_staleness to 0 to prevent infinite loop due to caching\n    first_flip.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n    flip_loop = flip_component(first_flip.output)\n    # flip_loop is a graph_component with the outputs field\n    # filled with the returned dictionary.\n    print_op('cool, it is over.').after(flip_loop)\n\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(flipcoin, __file__ + '.yaml')\n"
  },
  {
    "repo": "rakesh283343/kubeflow-sample-pipelines",
    "file_path": "core/resource_ops/resource_ops.py",
    "raw_url": "https://raw.githubusercontent.com/rakesh283343/kubeflow-sample-pipelines/master/core/resource_ops/resource_ops.py",
    "content": "# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n\"\"\"\nThis example demonstrates how to use ResourceOp to specify the value of env var.\n\"\"\"\n\nimport json\nimport kfp\nimport kfp.dsl as dsl\n\n\n_CONTAINER_MANIFEST = \"\"\"\n{\n    \"apiVersion\": \"batch/v1\",\n    \"kind\": \"Job\",\n    \"metadata\": {\n        \"generateName\": \"resourceop-basic-job-\"\n    },\n    \"spec\": {\n        \"template\": {\n            \"metadata\": {\n                \"name\": \"resource-basic\"\n            },\n            \"spec\": {\n                \"containers\": [{\n                    \"name\": \"sample-container\",\n                    \"image\": \"k8s.gcr.io/busybox\",\n                    \"command\": [\"/usr/bin/env\"]\n                }],\n                \"restartPolicy\": \"Never\"\n            }\n        },\n        \"backoffLimit\": 4      \n    }\n}\n\"\"\"\n\n\n@dsl.pipeline(\n    name=\"ResourceOp Basic\",\n    description=\"A Basic Example on ResourceOp Usage.\"\n)\ndef resourceop_basic():\n\n    # Start a container. Print out env vars.\n    op = dsl.ResourceOp(\n        name='test-step',\n        k8s_resource=json.loads(_CONTAINER_MANIFEST),\n        action='create'\n    )\n\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(resourceop_basic, __file__ + '.yaml')\n"
  },
  {
    "repo": "rakesh283343/kubeflow-sample-pipelines",
    "file_path": "core/retry/retry.py",
    "raw_url": "https://raw.githubusercontent.com/rakesh283343/kubeflow-sample-pipelines/master/core/retry/retry.py",
    "content": "#!/usr/bin/env python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport kfp\nfrom kfp import dsl\n\n\ndef random_failure_op(exit_codes):\n    \"\"\"A component that fails randomly.\"\"\"\n    return dsl.ContainerOp(\n        name='random_failure',\n        image='python:alpine3.6',\n        command=['python', '-c'],\n        arguments=['import random; import sys; exit_code = int(random.choice(sys.argv[1].split(\",\"))); print(exit_code); sys.exit(exit_code)', exit_codes]\n    )\n\n\n@dsl.pipeline(\n    name='Retry random failures',\n    description='The pipeline includes two steps which fail randomly. It shows how to use ContainerOp(...).set_retry(...).'\n)\ndef retry_sample_pipeline():\n    op1 = random_failure_op('0,1,2,3').set_retry(10)\n    op2 = random_failure_op('0,1').set_retry(5)\n\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(retry_sample_pipeline, __file__ + '.yaml')\n"
  },
  {
    "repo": "rakesh283343/kubeflow-sample-pipelines",
    "file_path": "core/rnd/rnd.py",
    "raw_url": "https://raw.githubusercontent.com/rakesh283343/kubeflow-sample-pipelines/master/core/rnd/rnd.py",
    "content": "#!/usr/bin/env python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport kfp\nfrom kfp import dsl\n\n\ndef random_failure_op(exit_codes):\n    \"\"\"A component that fails randomly.\"\"\"\n    return dsl.ContainerOp(\n        name='random_failure',\n        image='python:alpine3.6',\n        command=['python', '-c'],\n        arguments=['import random; import sys; exit_code = random.choice(sys.argv[1].split(\",\")); print(exit_code); sys.exit(exit_code)', exit_codes]\n    )\n\n\n@dsl.pipeline(\n    name='Retry random failures',\n    description='The pipeline includes two steps which fail randomly. It shows how to use ContainerOp(...).set_retry(...).'\n)\ndef retry_sample_pipeline():\n    op1 = random_failure_op('0,1,2,3').set_retry(10)\n    op2 = random_failure_op('0,1').set_retry(5)\n\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(retry_sample_pipeline, __file__ + '.yaml')\n"
  },
  {
    "repo": "rakesh283343/kubeflow-sample-pipelines",
    "file_path": "core/sequential/sequential.py",
    "raw_url": "https://raw.githubusercontent.com/rakesh283343/kubeflow-sample-pipelines/master/core/sequential/sequential.py",
    "content": "#!/usr/bin/env python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport kfp\nfrom kfp import dsl\n\n\ndef gcs_download_op(url):\n    return dsl.ContainerOp(\n        name='GCS - Download',\n        image='google/cloud-sdk:279.0.0',\n        command=['sh', '-c'],\n        arguments=['gsutil cat $0 | tee $1', url, '/tmp/results.txt'],\n        file_outputs={\n            'data': '/tmp/results.txt',\n        }\n    )\n\n\ndef echo_op(text):\n    return dsl.ContainerOp(\n        name='echo',\n        image='library/bash:4.4.23',\n        command=['sh', '-c'],\n        arguments=['echo \"$0\"', text]\n    )\n\n@dsl.pipeline(\n    name='Sequential pipeline',\n    description='A pipeline with two sequential steps.'\n)\ndef sequential_pipeline(url='gs://ml-pipeline/sample-data/shakespeare/shakespeare1.txt'):\n    \"\"\"A pipeline with two sequential steps.\"\"\"\n\n    download_task = gcs_download_op(url)\n    echo_task = echo_op(download_task.output)\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(sequential_pipeline, __file__ + '.yaml')\n"
  },
  {
    "repo": "rakesh283343/kubeflow-sample-pipelines",
    "file_path": "core/sidecar/sidecar.py",
    "raw_url": "https://raw.githubusercontent.com/rakesh283343/kubeflow-sample-pipelines/master/core/sidecar/sidecar.py",
    "content": "#!/usr/bin/env python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp\nimport kfp.dsl as dsl\n\n@dsl.pipeline(\n    name=\"pipeline_with_sidecar\", \n    description=\"A pipeline that demonstrates how to add a sidecar to an operation.\"\n)\ndef pipeline_with_sidecar(sleep_sec: int = 30):\n\n    # sidecar with sevice that reply \"hello world\" to any GET request\n    echo = dsl.Sidecar(\n        name=\"echo\",\n        image=\"hashicorp/http-echo:latest\",\n        args=['-text=\"hello world\"'],\n    )\n\n    # container op with sidecar\n    op1 = dsl.ContainerOp(\n        name=\"download\",\n        image=\"busybox:latest\",\n        command=[\"sh\", \"-c\"],\n        arguments=[\n            \"sleep %s; wget localhost:5678 -O /tmp/results.txt\" % sleep_sec\n        ],  # sleep for X sec and call the sidecar and save results to output\n        sidecars=[echo],\n        file_outputs={\"downloaded\": \"/tmp/results.txt\"},\n    )\n\n    op2 = dsl.ContainerOp(\n        name=\"echo\",\n        image=\"library/bash\",\n        command=[\"sh\", \"-c\"],\n        arguments=[\"echo %s\" % op1.output],  # print out content of op1 output\n    )\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(pipeline_with_sidecar, __file__ + '.yaml')"
  },
  {
    "repo": "rakesh283343/kubeflow-sample-pipelines",
    "file_path": "core/volume_ops_readme/volume_ops.py",
    "raw_url": "https://raw.githubusercontent.com/rakesh283343/kubeflow-sample-pipelines/master/core/volume_ops_readme/volume_ops.py",
    "content": "# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp\nimport kfp.dsl as dsl\n\n\n@dsl.pipeline(\n    name=\"VolumeOp Basic\",\n    description=\"A Basic Example on VolumeOp Usage.\"\n)\ndef volumeop_basic(size):\n    vop = dsl.VolumeOp(\n        name=\"create-pvc\",\n        resource_name=\"my-pvc\",\n        modes=dsl.VOLUME_MODE_RWO,\n        size=size\n    )\n\n    cop = dsl.ContainerOp(\n        name=\"cop\",\n        image=\"library/bash:4.4.23\",\n        command=[\"sh\", \"-c\"],\n        arguments=[\"echo foo > /mnt/file1\"],\n        pvolumes={\"/mnt\": vop.volume}\n    )\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(volumeop_basic, __file__ + '.yaml')\n"
  },
  {
    "repo": "stackdemos/kubeflow-acmecr",
    "file_path": "components/golang/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/kubeflow-acmecr/master/components/golang/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef training_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('TRAINING_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=['/usr/local/bin/python', script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n\n"
  },
  {
    "repo": "stackdemos/kubeflow-acmecr",
    "file_path": "components/training/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/kubeflow-acmecr/master/components/training/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef training_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('TRAINING_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=['/usr/local/bin/python', script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n\n"
  },
  {
    "repo": "ajperry2/kubeflow_pipeline",
    "file_path": "kubeflow_pipeline/components/nuscenes/download_nuscene.py",
    "raw_url": "https://raw.githubusercontent.com/ajperry2/kubeflow_pipeline/main/kubeflow_pipeline/components/nuscenes/download_nuscene.py",
    "content": "from typing import Dict\n\nfrom kfp import dsl\nfrom kfp.dsl import Dataset\n\n\n@dsl.component(\n    base_image=\"python:3.12\", \n    packages_to_install=[\n        'requests==2.31.0', \n        'tqdm==4.65.0'])\ndef download_nuscene(\n    nuscene_email: str,\n    nuscene_password: str,\n    region:str\n) -> Dataset:\n    import requests\n    import os\n    import hashlib\n    from tqdm import tqdm\n    import tarfile\n    import gzip\n    import json \n    from pathlib import Path\n\n    download_files = {\n        \"v1.0-test_meta.tgz\":\"b0263f5c41b780a5a10ede2da99539eb\",\n        \"v1.0-test_blobs.tgz\":\"e065445b6019ecc15c70ad9d99c47b33\",\n        \"v1.0-trainval01_blobs.tgz\":\"cbf32d2ea6996fc599b32f724e7ce8f2\",\n        \"v1.0-trainval02_blobs.tgz\":\"aeecea4878ec3831d316b382bb2f72da\",\n        \"v1.0-trainval03_blobs.tgz\":\"595c29528351060f94c935e3aaf7b995\",\n        \"v1.0-trainval04_blobs.tgz\":\"b55eae9b4aa786b478858a3fc92fb72d\",\n        \"v1.0-trainval05_blobs.tgz\":\"1c815ed607a11be7446dcd4ba0e71ed0\",\n        \"v1.0-trainval06_blobs.tgz\":\"7273eeea36e712be290472859063a678\",\n        \"v1.0-trainval07_blobs.tgz\":\"46674d2b2b852b7a857d2c9a87fc755f\",\n        \"v1.0-trainval08_blobs.tgz\":\"37524bd4edee2ab99678909334313adf\",\n        \"v1.0-trainval09_blobs.tgz\":\"a7fcd6d9c0934e4052005aa0b84615c0\",\n        \"v1.0-trainval10_blobs.tgz\":\"31e795f2c13f62533c727119b822d739\",\n        \"v1.0-trainval_meta.tgz\":\"537d3954ec34e5bcb89a35d4f6fb0d4a\",\n    }\n\n    def login(username, password):\n        headers = {\n            \"Content-Type\": \"application/x-amz-json-1.1\",\n            \"X-Amz-Target\": \"AWSCognitoIdentityProviderService.InitiateAuth\",\n        }\n\n        # Use json.dumps() for correct JSON formatting\n        data = json.dumps({\n            \"AuthFlow\": \"USER_PASSWORD_AUTH\",\n            \"ClientId\": \"7fq5jvs5ffs1c50hd3toobb3b9\",\n            \"AuthParameters\": {\n                \"USERNAME\": username,\n                \"PASSWORD\": password\n            },\n            \"ClientMetadata\": {}\n        })\n\n        response = requests.post(\n            \"https://cognito-idp.us-east-1.amazonaws.com/\",\n            headers=headers,\n            data=data,\n        )\n\n        if response.status_code == 200:\n            try:\n                token = json.loads(response.content)[\"AuthenticationResult\"][\"IdToken\"]\n                return token\n            except KeyError:\n                print(\"Authentication failed. 'AuthenticationResult' not found in the response.\")\n        else:\n            print(\"Failed to login. Status code:\", response.status_code)\n\n        return None\n\n    def download_file(url, save_file,md5):\n        response = requests.get(url, stream=True)\n        if save_file.endswith(\".tgz\"):\n            content_type = response.headers.get('Content-Type', '')\n            if content_type == 'application/x-tar':\n                save_file = save_file.replace('.tgz', '.tar')\n            elif content_type != 'application/octet-stream':\n                print(\"unknow content type\",content_type)\n                return save_file\n\n        if os.path.exists(save_file):\n            print(save_file,\"has downloaded\")\n            # check md5\n            md5obj = hashlib.md5()\n            with open(save_file, 'rb') as file:\n                for chunk in file:\n                    md5obj.update(chunk)\n            hash = md5obj.hexdigest()\n            if hash != md5:\n                print(save_file,\"check md5 failed,download again\")\n            else:\n                print(save_file,\"check md5 success\")\n                return save_file\n            \n        file_size = int(response.headers.get('Content-Length', 0))\n        progress_bar = tqdm(total=file_size, unit='B', unit_scale=True, unit_divisor=1024,desc=save_file, ascii=True)\n\n\n        # save file & check md5\n        md5obj = hashlib.md5()\n        with open(save_file, 'wb') as file:\n            for chunk in response.iter_content(chunk_size=1024):\n                if chunk:\n                    md5obj.update(chunk)\n                    file.write(chunk)\n                    progress_bar.update(len(chunk))\n        progress_bar.close()\n\n        hash = md5obj.hexdigest()\n        if hash != md5:\n            print(save_file,\"check md5 failed\")\n        else:\n            print(save_file,\"check md5 success\")\n\n        return save_file\n\n\n    output_dataset = Dataset(name=\"nuscene\", uri=dsl.get_uri(), metadata={})\n\n    dataset_path = Path(output_dataset.path)\n\n    print(\"Loginging...\")\n    bearer_token = login(nuscene_email, nuscene_password)\n    # set request header\n    headers = {\n        'Authorization': f'Bearer {bearer_token}',\n        'Content-Type': 'application/json',\n    }\n\n    print(\"Getting download urls...\")\n    download_data = {}\n    for filename, md5 in download_files.items():\n        api_url = f'https://o9k5xn5546.execute-api.us-east-1.amazonaws.com/v1/archives/v1.0/{filename}?region={region}&project=nuScenes'\n\n        response = requests.get(api_url, headers=headers)\n\n        if response.status_code == 200:\n            print(filename,'request success')\n            download_url = response.json()['url']\n            download_data[filename] = [download_url,os.path.join(dataset_path,filename),md5]\n        else:\n            print(f'request failed : {response.status_code}')\n            print(response.text)\n\n    print(\"Downloading files...\")\n    for output_name,(download_url,save_file,md5) in download_data.items():\n        save_file = download_file(download_url,save_file,md5)\n        download_data[output_name] = [download_url,save_file,md5]\n\n\n\n    def extract_tgz_to_original_folder(tgz_file_path):\n        original_folder = os.path.dirname(tgz_file_path)\n        print(f\"Extracting {tgz_file_path} to {original_folder}\")\n\n        with gzip.open(tgz_file_path, 'rb') as f_in:\n            with tarfile.open(fileobj=f_in, mode='r') as tar:\n                tar.extractall(original_folder)\n\n    def extract_tar_to_original_folder(tar_file_path):\n        original_folder = os.path.dirname(tar_file_path)\n        print(f\"Extracting {tar_file_path} to {original_folder}\")\n\n        with tarfile.open(tar_file_path, 'r') as tar:\n            tar.extractall(original_folder)\n\n\n    print(\"Extracting files...\")\n    for output_name,(download_url,save_file,md5) in download_data.items():\n        if output_name.endswith(\".tgz\"):\n            extract_tgz_to_original_folder(save_file)\n        elif output_name.endswith(\".tar\"):\n            extract_tar_to_original_folder(save_file)\n        else:\n            print(\"unknow file type\",output_name)\n\n    print(\"Done!\")\n\n\n\n    return output_dataset\n"
  },
  {
    "repo": "ajperry2/kubeflow_pipeline",
    "file_path": "kubeflow_pipeline/pipelines/hello_world/hello_world.py",
    "raw_url": "https://raw.githubusercontent.com/ajperry2/kubeflow_pipeline/main/kubeflow_pipeline/pipelines/hello_world/hello_world.py",
    "content": "import os\n\nfrom kfp import dsl\n\nfrom kubeflow_pipeline import components\n\n\n@dsl.pipeline\ndef hello_world(message: str) -> str:\n    hello_task = components.hello_world.say_hello(message=message)\n    hello_task.set_display_name(\"STEP 0: Hello World\")\n    return hello_task.output\n"
  },
  {
    "repo": "ajperry2/kubeflow_pipeline",
    "file_path": "kubeflow_pipeline/pipelines/nuscenes/download_nuscene.py",
    "raw_url": "https://raw.githubusercontent.com/ajperry2/kubeflow_pipeline/main/kubeflow_pipeline/pipelines/nuscenes/download_nuscene.py",
    "content": "import os\nfrom kfp import dsl\n\nfrom kubeflow_pipeline import components\n\n\n@dsl.pipeline\ndef download_nuscene(\n    nuscene_email: str,\n    nuscene_password: str,\n    region:str\n) -> dsl.Dataset:\n    download_task = components.nuscenes.download_nuscene(\n        nuscene_email=nuscene_email,\n        nuscene_password=nuscene_password,\n        region=region,\n    )\n    download_task.set_display_name(\"STEP 0: Download Data\")\n    return download_task.output\n"
  },
  {
    "repo": "viraj-s15/KubeflowAccuPred",
    "file_path": "training_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/viraj-s15/KubeflowAccuPred/master/training_pipeline.py",
    "content": "import kfp \nfrom kfp import dsl\nimport os\nfrom components import step_data_preprocessing,step_data_splitting,step_hyperparam_optim,step_model_testing\n\n@dsl.pipeline(\n    name='Customer Frequency Training Pipeline',\n    description='A kubernetes pipeline for training a customer freq prediction model'\n)\ndef customer_freq_training_pipeline():\n    data_preprocessing_task = step_data_preprocessing()\n    data_splitting_task = step_data_splitting().after(data_preprocessing_task)\n    hyperparam_optim_task = step_hyperparam_optim().after(data_splitting_task)\n    model_testing_task = step_model_testing().after(hyperparam_optim_task)\n    \ndirectory_path = \"compiled_pipelines/\"\n\nif not os.path.exists(directory_path):\n    try:\n        os.makedirs(directory_path) \n        print(f\"Directory '{directory_path}' created successfully.\")\n    except OSError as error:\n        print(f\"Failed to create directory '{directory_path}': {error}\")\nelse:\n    print(f\"Directory '{directory_path}' already exists.\")    \n    \n\nkfp.compiler.Compiler().compile(\n    pipeline_func=customer_freq_training_pipeline,\n    package_path='./compiled_pipelines/Customer_freq_training.yaml')"
  },
  {
    "repo": "viraj-s15/KubeflowAccuPred",
    "file_path": "training_pipeline_catboost.py",
    "raw_url": "https://raw.githubusercontent.com/viraj-s15/KubeflowAccuPred/master/training_pipeline_catboost.py",
    "content": "import kfp\nfrom kfp import dsl\nimport os\nfrom components import (\n    step_model_testing_catboost,\n    step_hyperparam_optim_catboost,\n    step_data_splitting_catboost,\n    step_data_preprocessing_catboost,\n)\n\n\n@dsl.pipeline(\n    name=\"Customer Frequency Training Pipeline\",\n    description=\"A kubernetes pipeline for training a customer freq prediction model\",\n)\ndef customer_freq_training_pipeline():\n    data_preprocessing_task = step_data_preprocessing_catboost()\n    data_splitting_task = step_data_splitting_catboost().after(data_preprocessing_task)\n    hyperparam_optim_task = step_hyperparam_optim_catboost().after(data_splitting_task)\n    model_testing_task = step_model_testing_catboost().after(hyperparam_optim_task)\n\n\ndirectory_path = \"compiled_pipelines/\"\n\nif not os.path.exists(directory_path):\n    try:\n        os.makedirs(directory_path)\n        print(f\"Directory '{directory_path}' created successfully.\")\n    except OSError as error:\n        print(f\"Failed to create directory '{directory_path}': {error}\")\nelse:\n    print(f\"Directory '{directory_path}' already exists.\")\n\n\nkfp.compiler.Compiler().compile(\n    pipeline_func=customer_freq_training_pipeline,\n    package_path=\"./compiled_pipelines/Customer_freq_training_catboost.yaml\",\n)\n"
  },
  {
    "repo": "omiearicent/kubeflow_pipeline",
    "file_path": "componentymltopipeline.py",
    "raw_url": "https://raw.githubusercontent.com/omiearicent/kubeflow_pipeline/master/componentymltopipeline.py",
    "content": "#!/usr/bin/env python\n# coding: utf-8\n\n# In[3]:\n\n\nimport kfp\n# Load the component by calling load_component_from_file or load_component_from_url\n# To load the component, the pipeline author only needs to have access to the component.yaml file.\n# The Kubernetes cluster executing the pipeline needs access to the container image specified in the component.\nmodel_generation_op = kfp.components.load_component_from_file('component1.yaml') \nprediction_op = kfp.components.load_component_from_file('component2.yaml')\n# dummy_op = kfp.components.load_component_from_url('http://....../component.yaml')\n\n# dummy_op is now a \"factory function\" that accepts the arguments for the component's inputs\n# and produces a task object (e.g. ContainerOp instance).\n# Inspect the dummy_op function in Jupyter Notebook by typing \"dummy_op(\" and pressing Shift+Tab\n# You can also get help by writing help(dummy_op) or dummy_op? or dummy_op??\n# The signature of the dummy_op function corresponds to the inputs section of the component.\n# Some tweaks are performed to make the signature valid and pythonic:\n# 1) All inputs with default values will come after the inputs without default values\n# 2) The input names are converted to pythonic names (spaces and symbols replaced\n#    with underscores and letters lowercased).\n\n# Define a pipeline and create a task from a component:\n@kfp.dsl.pipeline(name='tensorflow image classification', description='demo pipeline')\ndef model_generation_new1():\n    model_generation = model_generation_op( )\n    prdediction_part = prediction_op( ).after(model_generation)\n    # To access GCS, you must configure the container to have access to a\n    # GCS secret that grants required access to the bucket.\n    # The outputs of the dummy1_task can be referenced using the\n    # dummy1_task.outputs dictionary.\n    # ! The output names are converted to lowercased dashed names.\n\n    # Pass the outputs of the dummy1_task to some other component\n \n    # To access GCS, you must configure the container to have access to a\n    # GCS secret that grants required access to the bucket.\n\n\n# In[4]:\n\n\nimport kfp\nfrom kfp import compiler\nimport kfp.compiler as compiler\nimport kfp.components as comp\nimport kfp.dsl as dsl\nfrom kfp import gcp\npipeline_func = model_generation_new1\npipeline_filename = pipeline_func.__name__ + '.pipeline.zip'\n\ncompiler.Compiler().compile(model_generation_new1, \n                            pipeline_filename)\n\n\n\n\n\n\n\n"
  },
  {
    "repo": "slv-ai/kubeflow-pipelines",
    "file_path": "kubeflow-kserve/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/slv-ai/kubeflow-pipelines/main/kubeflow-kserve/pipeline.py",
    "content": "import kfp\nfrom kfp import dsl\nfrom kfp import compiler\nfrom typing import Dict,List\nimport json\nimport os\nfrom kfp.dsl import Input,Output,Dataset,Model,component\n\n#load dataset\n@dsl.component(base_image=\"python:3.9\")\ndef load_data(output_csv: Output[Dataset]):\n    import subprocess\n    subprocess.run([\"pip\",\"install\",\"pandas\",\"scikit-learn\"],check=True)\n    from sklearn.datasets import load_iris\n    import pandas as pd \n    iris=load_iris()\n    df=pd.DataFrame(data=iris.data,columns=iris.feature_names)\n    df['target']=iris.target\n\n    df.to_csv(output_csv.path,index=False)\n\n#preprocess data\n@dsl.component(base_image=\"python:3.9\")\ndef preprocess_data(input_csv: Input[Dataset],output_train: Output[Dataset],output_test: Output[Dataset],\n                    output_ytrain: Output[Dataset],output_ytest: Output[Dataset]):\n    import subprocess\n    subprocess.run([\"pip\",\"install\",\"pandas\",\"scikit-learn\"],check=True)\n    import pandas as pd\n    from sklearn.preprocessing import StandardScalar\n    from sklearn.model_selection import train_test_split\n    df=pd.read_csv(input_csv.path)\n\n    print(\"dataset shape :\",df.shape)\n    print(\"missing values in a dataset :\"df.isnull().sum())\n    if df.isnull().values().any():\n        print(\"missing values detected\")\n        df=df.dropna()\n    features=df.drop(columns=['target'])\n    target=df['target']\n    #standardize features\n    scalar=StandardScalar()\n    scaled_features=scalar.fit_transform(features)\n\n    #train-test split\n    X_train,X_test,y_train,y_test=train_test_split(scaled_features,target,test_size=0.2,random_state=42)\n    print(\"X_train :\",X_train.shape, \"X_test :\",X_test.shape)\n    print(\"y_train :\",y_train.shape, \"y_test :\",y_test.shape)\n\n    X_train_df=pd.DataFrame(X_train,columns=features.columns)\n    X_train_df.to_csv(output_train.path,index=False)\n\n    X_test_df=pd.DataFrame(X_test,columns=features.columns)\n    X_test_df.to_csv(output_test.path,index=False)\n\n    y_train_df=pd.DataFrame(y_train)\n    y_train_df.to_csv(output_ytrain.path,index=False)\n\n    y_test_df=pd.DataFrame(y_test)\n    y_test_df.to_csv(output_ytest.path,index=False)\n\n#train the model\n@dsl.component(\n    base_image=\"python:3.9\",\n    packages_to_install=[\n        \"pandas\",\n        \"scikit-learn\",\n        \"joblib\",\n        \"boto3\",\n        \"s3fs\"\n    ] \n)\ndef train_model(\n    train_data: Input[Dataset],\n    ytrain_data: Input[Dataset],\n    model_output: Output[Model],\n    aws_access_key_id: str,\n    aws_secret_access_key: str,\n    s3_bucket: str,\n    s3_key: str\n)-> str:\n    import pandas as pd\n    import sklearn.linear_model import LogisticRegression\n    from joblib import dump\n    import boto3\n    import os\n    from datetime import datetime\n    import json\n    #load training data\n    X_train=pd.read_csv(train_data.path)\n    y_train_df=pd.read_csv(ytrain_data.path)\n    model=LogisticRegression()\n    model.fit(X_train,y_train)\n    #save model\n    #save locally\n    local_path=model_output.path\n    dump(model,local_path)\n    print(f\"model saved local {local_path}\")\n\n    try:\n        s3_client=boto3.client(\n            's3',\n            aws_access_key_id=aws_access_key_id,\n            aws_secret_access_key=aws_secret_access_key\n        )\n        #upload to s3:\n        timestamp=datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        s3_path=f\"{s3_key}/model_{timestamp}.joblib\"\n        s3_client.upload_file(\n            local_path,\n            s3_bucket,\n            s3_path\n        )\n        print(f\"model uploaded to s3://{s3_bucket}/{s3_path}\")\n\n        # Create outputs directory if it doesn't exist\n        os.makedirs('/tmp/outputs', exist_ok=True)\n\n        # Save S3 path to metadata\n        metadata_path = '/tmp/outputs/output_metadata.json'\n        model_uri = f\"s3://{s3_bucket}/{s3_path}\"\n        with open(metadata_path, 'w') as f:\n            json.dump({\n                'model_s3_path': model_uri\n            }, f)\n        print(f\"Metadata saved to: {metadata_path}\")\n\n        print(f\"Returning model URI: {model_uri}\")\n        return model_uri  # Return the S3 URI as a string\n\n    except Exception as e:\n        print(f\"Error uploading to S3: {str(e)}\")\n        raise\n\n#evaluate model\n@dsl.component(base_image=\"python:3.9\")\ndef evaluate_model(test_data: Input[Dataset],ytest_data: Input[Dataset],model: Input[Model],metrics_output: Output[Dataset]):\n    import subprocess\n    subprocess.run([\"pip\",\"install\",\"pandas\",\"scikit-learn\",\"joblib\"],check=True)\n    import pandas as pd\n    from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n    from joblib import load\n    X_test=pd.read_csv(test_data.path)\n    y_test=pd.read_csv(ytest_data.path)\n    model=load(model.path)\n    #predict\n    y_pred=model.predict(X_test)\n    report=classification_report(y_test,y_pred,output_dict=True)\n    cm=confusion_matrix(y_test,y_pred)\n    accuracy=accuracy_score(y_test,y_pred)\n    #save metrics\n    metrics_path=metrics_output.path\n    with open(metrics_path,'w')as f_in:\n        f_in.write(str(report))\n        f_in.write(f\"Accuracy: {accuracy}\")\n\n#define pipeline\n@dsl.pipeline(name=\"ml-pipeline\")\ndef ml_pipeline(\n    aws_access_key_id: str = os.getenv('AWS_ACCESS_KEY_ID'),\n    aws_secret_access_key: str = os.getenv('AWS_SECRET_ACCESS_KEY'),\n    s3_bucket: str = \"kubeflow-projects\",\n    s3_key: str = \"models/iris\"\n):\n    load_op=load_data()\n    preprocess_op=preprocess_data(input_csv=load_op.outputs[\"output_csv\"])\n    train_op=train_model(\n        train_data=preprocess_op.outputs[\"output_train\"],\n        ytrain_data=preprocess_op.outputs[\"output_ytrain\"],\n        aws_access_key_id=aws_access_key_id,\n        aws_secret_access_key=aws_secret_access_key,\n        s3_bucket=s3_bucket,\n        s3_key=s3_key\n    )\n    evaluate_op=evaluate_model(\n        test_data=preprocess_op.outputs[\"output_test\"],\n        ytest_data=preprocess_op.outputs[\"output_ytest\"],\n        model=train_op.outputs[\"model_output\"]\n    )\n\n\nif __name__ == \"main\":\n    compiler.Compiler().compile(pipeline_func=ml_pipeline,package_path=\"kubeflow_pipeline.yaml\")\n\n\n\n\n\n\n\n\n"
  },
  {
    "repo": "slv-ai/kubeflow-pipelines",
    "file_path": "kubeflow-pipeline/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/slv-ai/kubeflow-pipelines/main/kubeflow-pipeline/pipeline.py",
    "content": "import kfp\nfrom kfp import dsl\nfrom kfp import compiler\nfrom typing import Dict,List\nfrom kfp.dsl import Input,Output,Dataset,Model,component\n\n#load dataset\n@dsl.component(base_image=\"python:3.9\")\ndef load_data(output_csv: Output[Dataset]):\n    import subprocess\n    subprocess.run([\"pip\",\"install\",\"pandas\",\"scikit-learn\"],check=True)\n    from sklearn.datasets import load_iris\n    import pandas as pd \n    iris=load_iris()\n    df=pd.DataFrame(data=iris.data,columns=iris.feature_names)\n    df['target']=iris.target\n\n    df.to_csv(output_csv.path,index=False)\n\n#preprocess data\n@dsl.component(base_image=\"python:3.9\")\ndef preprocess_data(input_csv: Input[Dataset],output_train: Output[Dataset],output_test: Output[Dataset],\n                    output_ytrain: Output[Dataset],output_ytest: Output[Dataset]):\n    import subprocess\n    subprocess.run([\"pip\",\"install\",\"pandas\",\"scikit-learn\"],check=True)\n    import pandas as pd\n    from sklearn.preprocessing import StandardScalar\n    from sklearn.model_selection import train_test_split\n    df=pd.read_csv(input_csv.path)\n\n    print(\"dataset shape :\",df.shape)\n    print(\"missing values in a dataset :\"df.isnull().sum())\n    if df.isnull().values().any():\n        print(\"missing values detected\")\n        df=df.dropna()\n    features=df.drop(columns=['target'])\n    target=df['target']\n    #standardize features\n    scalar=StandardScalar()\n    scaled_features=scalar.fit_transform(features)\n\n    #train-test split\n    X_train,X_test,y_train,y_test=train_test_split(scaled_features,target,test_size=0.2,random_state=42)\n    print(\"X_train :\",X_train.shape, \"X_test :\",X_test.shape)\n    print(\"y_train :\",y_train.shape, \"y_test :\",y_test.shape)\n\n    X_train_df=pd.DataFrame(X_train,columns=features.columns)\n    X_train_df.to_csv(output_train.path,index=False)\n\n    X_test_df=pd.DataFrame(X_test,columns=features.columns)\n    X_test_df.to_csv(output_test.path,index=False)\n\n    y_train_df=pd.DataFrame(y_train)\n    y_train_df.to_csv(output_ytrain.path,index=False)\n\n    y_test_df=pd.DataFrame(y_test)\n    y_test_df.to_csv(output_ytest.path,index=False)\n\n#train the model\n@dsl.component(base_image=\"python:3.9\")\ndef train_model(train_data: Input[Dataset],ytrain_data: Input[Dataset],model_output: Output[Model]):\n    import subprocess\n    subprocess.run([\"pip\",\"install\",\"pandas\",\"scikit-learn\",\"joblib\"],check=True)\n    import pandas as pd\n    import sklearn.linear_model import LogisticRegression\n    from joblib import dump\n    #load training data\n    X_train=pd.read_csv(train_data.path)\n    y_train_df=pd.read_csv(ytrain_data.path)\n    model=LogisticRegression()\n    model.fit(X_train,y_train)\n    #save model\n    dump(model,model_output.path)\n\n#evaluate model\n@dsl.component(base_image=\"python:3.9\")\ndef evaluate_model(test_data: Input[Dataset],ytest_data: Input[Dataset],model: Input[Model],metrics_output: Output[Dataset]):\n    import subprocess\n    subprocess.run([\"pip\",\"install\",\"pandas\",\"scikit-learn\",\"joblib\"],check=True)\n    import pandas as pd\n    from sklearn.metrics import classification_report,confusion_matrix\n    from joblib import load\n    X_test=pd.read_csv(test_data.path)\n    y_test=pd.read_csv(ytest_data.path)\n    model=load(model.path)\n    #predict\n    y_pred=model.predict(X_test)\n    report=classification_report(y_test,y_pred,output_dict=True)\n    cm=confusion_matrix(y_test,y_pred)\n    #save metrics\n    metrics_path=metrics_output.path\n    with open(metrics_path,'w')as f_in:\n        f_in.write(str(report))\n\n#define pipeline\n@dsl.pipeline(name=\"ml_pipeline\")\ndef ml_pipeline:\n    load_op=load_data()\n    preprocess_op=preprocess_data(input_csv=load_op.outputs[\"output_csv\"])\n    train_op=train_model(train_data=preprocess_op.outputs[\"output_train\"],ytrain_data=preprocess_op.outputs[\"output_ytrain\"])\n    evaluate_op=evaluate_model(test_data=preprocess_op.outputs[\"output_test\"],ytest_data=preprocess_op.outputs[\"output_ytest\"],model=train_op.outputs[\"model_output\"])\n\n\nif __name__ == \"main\":\n    compiler.Compiler().compile(pipeline_func=ml_pipeline,package_path=\"kubeflow_pipeline.yaml\")\n\n\n\n\n\n\n\n\n"
  },
  {
    "repo": "stackdemos/snehal-kubeflow-ml",
    "file_path": "components/golang/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/snehal-kubeflow-ml/master/components/golang/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef processing_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('GOLANG_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=[script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n"
  },
  {
    "repo": "stackdemos/snehal-kubeflow-ml",
    "file_path": "components/training/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/snehal-kubeflow-ml/master/components/training/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef training_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('TRAINING_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=['/usr/local/bin/python', script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n\n"
  },
  {
    "repo": "tam0201/kubeflow-pipeline",
    "file_path": "pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/tam0201/kubeflow-pipeline/main/pipeline.py",
    "content": "import kfp.dsl\nfrom kfp.components import ComponentStore\n\nfrom components.definitions import *\n\nstore = ComponentStore.default_store\npandas_transform_csv_op = store.load_component('pandas/Transform_DataFrame/in_CSV_format')\ndrop_header_op = store.load_component('tables/Remove_header')\n\n@ksp.dsl.pipeline(\n    name = 'Table Classification: $NAME',\n    description = '$DESCRIPTION'\n)\ndef pipeline(\n    s3_input_csv:str,\n    num_boost_found: int,\n    target: str,\n):\n    #load data from S3\n    input_csv=download_file_from_s3(s3_input_csv).output\n\n    #Use KFP UI for simple EDA\n    set_sweetviz(input_csv)\n\n    #Split test data\n    split_train_test=split_train_test_step(input_csv, test_size='your input here')\n    train_csv = split_train_test.output['output_csv_train']\n    test_csv = split_train_test.output['output_csv_test']\n\n    #Split val data\n    split_train_val=split_train_val_step(train_csv, val_size='your input here')\n    train_csv = split_train_val.output['output_csv_train']\n    val_csv = split_train_val.output['output_csv_test']\n\n    #preprocess all datasets\n    process_train - preprocessing_step(train_csv, target = target)\n    process_val - preprocessing_step(val_csv, target = target)\n    process_test - preprocessing_step(test_csv, target = target)\n\n    train_csv_processed = process_train.output\n    val_csv_processed = process_val.output\n    test_csv_processed = process_test.output\n\n    clf_path = train_classifier_step(train_csv_processed, val_csv_processed, num_boost_found, target_name = '').output\n\n    #Remove target from test data\n    x_test = pandas_transform_csv_op(table = test_csv_processed, transform_code = f'df = df.drop(\"{target}\", axis=1)').output\n    ).outputs\n\n    #Extract target from test data\n    y_test = pandas_transform_csv_op(table = test_csv_processed, transform_code = f'df[\"{target}\"] = df[\"{target}\"].astype(\"category\").cat.codes\\n'\n                                                                                    f'df = df[[\"target\"]]'\n    ).outputs\n    y_true_test = drop_header_op(y_test).output\n    y_pred_test = infer_classifier_step(model_path = clf_path, dataset_path = x_test).output\n\n    #Compute classification metrics\n    metrics = compute_classification_metrics(y_true_path = y_true_test, y_pred_path = y_pred_test, label_name = target).output\n\n    set_metrics(metrics)\n\n    #additional bells and whistles\n    process_train.set_display_name('Process: Train')\n    process_val.set_display_name('Process: Val')\n    process_test.set_display_name('Process: Test')\n\n    split_train_test.set_display_name('Split: Train/Test')\n    split_train_val.set_display_name('Split: Train/Val')\n\n"
  },
  {
    "repo": "bartosz-bok/kubeflow-pipelines",
    "file_path": "distributed-pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/bartosz-bok/kubeflow-pipelines/main/distributed-pipeline.py",
    "content": "import kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import component\nfrom kfp.v2.dsl import (\n    Input,\n    Output,\n    Artifact,\n    Dataset,\n    Metrics\n)\n\n@component(\n    packages_to_install=['pandas']\n)\ndef download_dataframe(url: str, output_csv: Output[Dataset]):\n  import pandas as pd\n\n  df_data = pd.read_csv(url)\n\n  df_data.to_csv(output_csv.path, index=False)\n\n@component(\n    packages_to_install=['pandas',\n                         'scikit-learn']\n)\ndef preprocessing(data_path: Input[Dataset], output_preprocessed_data_x_train: Output[Dataset],\n                                             output_preprocessed_data_x_val: Output[Dataset],\n                                             output_preprocessed_data_y_train: Output[Dataset],\n                                             output_preprocessed_data_y_val: Output[Dataset]):\n  import pandas as pd\n  from sklearn.model_selection import train_test_split\n\n  df_data = pd.read_csv(data_path.path)\n\n  df_data.drop(['Race'], axis=1, inplace=True)\n  df_data.replace(to_replace=['No','No, borderline diabetes', 'Yes', 'Yes (during pregnancy)'], value=[0, 0, 1, 1], inplace=True)\n  df_data['Sex'] = df_data['Sex'].map({'Female': 1, 'Male': 0})\n  df_data['AgeCategory'] = df_data['AgeCategory'].map({'18-24': 1,'25-29': 2,'30-34': 3, '35-39': 4,'40-44': 5,'45-49': 6,'50-54': 7,  '55-59': 8, '60-64': 9, '65-69': 10, '70-74': 11, '75-79': 12, '80 or older': 13})\n  df_data['GenHealth'] = df_data['GenHealth'].map({'Poor': 1, 'Fair': 2, 'Good': 3,'Very good': 4,'Excellent': 5})\n\n  X = df_data.drop(['HeartDisease'], axis=1)\n  y = df_data['HeartDisease']\n\n  X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n  \n  X_train.to_csv(output_preprocessed_data_x_train.path, index=False)\n  X_val.to_csv(output_preprocessed_data_x_val.path, index=False)\n  y_train.to_csv(output_preprocessed_data_y_train.path, index=False)\n  y_val.to_csv(output_preprocessed_data_y_val.path, index=False)\n\n@component(\n    packages_to_install=['pandas',\n                         'torch',\n                         'scikit-learn']\n)\ndef training_1st(num_epochs: int, preprocessed_data_path_x_train: Input[Dataset],\n                                  preprocessed_data_path_x_val: Input[Dataset],\n                                  preprocessed_data_path_y_train: Input[Dataset],\n                                  preprocessed_data_path_y_val: Input[Dataset],\n                                  output_accuracy: Output[Metrics]):\n  import torch\n  import torch.nn as nn\n  import torch.optim as optim\n  import pandas as pd\n  from sklearn.preprocessing import StandardScaler\n  from sklearn.metrics import accuracy_score\n\n  X_train = pd.read_csv(preprocessed_data_path_x_train.path)\n  X_val = pd.read_csv(preprocessed_data_path_x_val.path)\n  y_train = pd.read_csv(preprocessed_data_path_y_train.path)\n  y_val = pd.read_csv(preprocessed_data_path_y_val.path)\n\n  scaler = StandardScaler()\n  X_train = scaler.fit_transform(X_train)\n  X_val = scaler.transform(X_val)\n\n  X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n  y_train_tensor = torch.tensor(y_train.values.reshape(-1, 1), dtype=torch.float32)\n  X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n  y_val_tensor = torch.tensor(y_val.values.reshape(-1, 1), dtype=torch.float32)\n\n  # Definicja modelu\n  class RegressionModel(nn.Module):\n      def __init__(self):\n          super(RegressionModel, self).__init__()\n          self.fc1 = nn.Linear(16, 8)\n          self.fc2 = nn.Linear(8, 4)\n          self.fc3 = nn.Linear(4, 1)\n\n      def forward(self, x):\n          x = torch.relu(self.fc1(x))\n          x = torch.relu(self.fc2(x))\n          x = self.fc3(x)\n          return x\n\n  model = RegressionModel()\n\n  criterion = nn.MSELoss()\n  optimizer = optim.Adam(model.parameters(), lr=0.01)\n\n  # Trenowanie modelu\n  for epoch in range(num_epochs):\n      # Forward pass\n      outputs = model(X_train_tensor)\n      loss = criterion(outputs, y_train_tensor)\n\n      optimizer.zero_grad()\n      loss.backward()\n      optimizer.step()\n\n      with torch.no_grad():\n          val_outputs = model(X_val_tensor)\n          val_loss = criterion(val_outputs, y_val_tensor)\n          print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Validation Loss: {val_loss.item():.4f}')\n\n  with torch.no_grad():\n      val_predictions = model(X_val_tensor)\n      val_predictions = torch.round(val_predictions)\n      val_accuracy = accuracy_score(y_val_tensor.numpy(), val_predictions.numpy())\n      print(f'Validation Accuracy: {val_accuracy:.4f}')\n\n  output_accuracy.log_metric('accuracy', val_accuracy) \n\n@component(\n    packages_to_install=['pandas',\n                         'torch',\n                         'scikit-learn']\n)\ndef training_2nd(num_epochs: int, preprocessed_data_path_x_train: Input[Dataset],\n                                  preprocessed_data_path_x_val: Input[Dataset],\n                                  preprocessed_data_path_y_train: Input[Dataset],\n                                  preprocessed_data_path_y_val: Input[Dataset],\n                                  output_accuracy: Output[Metrics]):\n  import torch\n  import torch.nn as nn\n  import torch.optim as optim\n  import pandas as pd\n  from sklearn.preprocessing import StandardScaler\n  from sklearn.metrics import accuracy_score\n\n  X_train = pd.read_csv(preprocessed_data_path_x_train.path)\n  X_val = pd.read_csv(preprocessed_data_path_x_val.path)\n  y_train = pd.read_csv(preprocessed_data_path_y_train.path)\n  y_val = pd.read_csv(preprocessed_data_path_y_val.path)\n\n  scaler = StandardScaler()\n  X_train = scaler.fit_transform(X_train)\n  X_val = scaler.transform(X_val)\n\n  X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n  y_train_tensor = torch.tensor(y_train.values.reshape(-1, 1), dtype=torch.float32)\n  X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n  y_val_tensor = torch.tensor(y_val.values.reshape(-1, 1), dtype=torch.float32)\n\n  # Definicja modelu\n  class RegressionModel(nn.Module):\n      def __init__(self):\n          super(RegressionModel, self).__init__()\n          self.fc1 = nn.Linear(16, 16)\n          self.fc2 = nn.Linear(16, 8)\n          self.fc3 = nn.Linear(8, 8)\n          self.fc4 = nn.Linear(8, 1)\n\n      def forward(self, x):\n          x = torch.relu(self.fc1(x))\n          x = torch.relu(self.fc2(x))\n          x = torch.relu(self.fc3(x))\n          x = self.fc4(x)\n          return x\n\n  model = RegressionModel()\n\n  criterion = nn.MSELoss()\n  optimizer = optim.Adam(model.parameters(), lr=0.01)\n\n  # Trenowanie modelu\n  for epoch in range(num_epochs):\n      # Forward pass\n      outputs = model(X_train_tensor)\n      loss = criterion(outputs, y_train_tensor)\n\n      optimizer.zero_grad()\n      loss.backward()\n      optimizer.step()\n\n      with torch.no_grad():\n          val_outputs = model(X_val_tensor)\n          val_loss = criterion(val_outputs, y_val_tensor)\n          print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Validation Loss: {val_loss.item():.4f}')\n\n  with torch.no_grad():\n      val_predictions = model(X_val_tensor)\n      val_predictions = torch.round(val_predictions)\n      val_accuracy = accuracy_score(y_val_tensor.numpy(), val_predictions.numpy())\n      print(f'Validation Accuracy: {val_accuracy:.4f}')\n\n  output_accuracy.log_metric('accuracy', val_accuracy) \n\n@component()\ndef compare_accuracy(accuracy_1st: Input[Metrics],accuracy_2nd: Input[Metrics], better_model: Output[Metrics]):\n\n  accuracy_1st_result = accuracy_1st.metadata['accuracy']\n  accuracy_2nd_result = accuracy_2nd.metadata['accuracy']\n\n  print(f'Pierwszy model osiagnal wartosc {accuracy_1st_result}, a drugi {accuracy_2nd_result}.')\n\n  which_better = 0\n\n  if accuracy_1st_result > accuracy_2nd_result:\n    print('Pierwszy model osiagnal wieksza dokladnosc')\n    which_better = 1\n  elif accuracy_1st_result < accuracy_2nd_result:\n    print('Drugi model osiagnal wieksza dokladnosc')\n    which_better = 2\n  elif accuracy_1st_result == accuracy_2nd_result:\n    print('Obydwa modele osiagnely taka sama dokladnosc')\n  else:\n    print('Blad')\n\n  better_model.log_metric('better_model', which_better)\n\n# Define a pipeline and create a task from a component:\n@dsl.pipeline(\n    name='my-pipeline',\n    # You can optionally specify your own pipeline_root\n    # pipeline_root='gs://my-pipeline-root/example-pipeline',\n)\ndef my_pipeline(url: str, num_epochs_1st: int, num_epochs_2nd: int):\n  web_downloader_task = download_dataframe(url=url)\n  preprocessing_task = preprocessing(data_path=web_downloader_task.outputs['output_csv'])\n  training_task_1st = training_1st(num_epochs=num_epochs_1st, preprocessed_data_path_x_train=preprocessing_task.outputs['output_preprocessed_data_x_train'],\n                                                              preprocessed_data_path_x_val=preprocessing_task.outputs['output_preprocessed_data_x_val'],\n                                                              preprocessed_data_path_y_train=preprocessing_task.outputs['output_preprocessed_data_y_train'],\n                                                              preprocessed_data_path_y_val=preprocessing_task.outputs['output_preprocessed_data_y_val'])\n  training_task_2nd = training_2nd(num_epochs=num_epochs_2nd, preprocessed_data_path_x_train=preprocessing_task.outputs['output_preprocessed_data_x_train'],\n                                                              preprocessed_data_path_x_val=preprocessing_task.outputs['output_preprocessed_data_x_val'],\n                                                              preprocessed_data_path_y_train=preprocessing_task.outputs['output_preprocessed_data_y_train'],\n                                                              preprocessed_data_path_y_val=preprocessing_task.outputs['output_preprocessed_data_y_val'])\n  compare_task = compare_accuracy(accuracy_1st=training_task_1st.outputs['output_accuracy'], accuracy_2nd=training_task_2nd.outputs['output_accuracy'])\n\nkfp.compiler.Compiler(mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE).compile(\n    pipeline_func=my_pipeline,\n    package_path='pipeline_distributed_v1.yaml')"
  },
  {
    "repo": "bartosz-bok/kubeflow-pipelines",
    "file_path": "sequenced-pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/bartosz-bok/kubeflow-pipelines/main/sequenced-pipeline.py",
    "content": "import kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import component\nfrom kfp.v2.dsl import (\n    Input,\n    Output,\n    Artifact,\n    Dataset,\n    Metrics\n)\n\n@component(\n    packages_to_install=['pandas']\n)\ndef download_dataframe(url: str, output_csv: Output[Dataset]):\n  import pandas as pd\n\n  df_data = pd.read_csv(url)\n\n  df_data.to_csv(output_csv.path, index=False)\n\n@component(\n    packages_to_install=['pandas',\n                         'scikit-learn']\n)\ndef preprocessing(data_path: Input[Dataset], output_preprocessed_data_x_train: Output[Dataset],\n                                             output_preprocessed_data_x_val: Output[Dataset],\n                                             output_preprocessed_data_y_train: Output[Dataset],\n                                             output_preprocessed_data_y_val: Output[Dataset]):\n  import pandas as pd\n  from sklearn.model_selection import train_test_split\n\n  df_data = pd.read_csv(data_path.path)\n\n  df_data.drop(['Race'], axis=1, inplace=True)\n  df_data.replace(to_replace=['No','No, borderline diabetes', 'Yes', 'Yes (during pregnancy)'], value=[0, 0, 1, 1], inplace=True)\n  df_data['Sex'] = df_data['Sex'].map({'Female': 1, 'Male': 0})\n  df_data['AgeCategory'] = df_data['AgeCategory'].map({'18-24': 1,'25-29': 2,'30-34': 3, '35-39': 4,'40-44': 5,'45-49': 6,'50-54': 7,  '55-59': 8, '60-64': 9, '65-69': 10, '70-74': 11, '75-79': 12, '80 or older': 13})\n  df_data['GenHealth'] = df_data['GenHealth'].map({'Poor': 1, 'Fair': 2, 'Good': 3,'Very good': 4,'Excellent': 5})\n\n  X = df_data.drop(['HeartDisease'], axis=1)\n  y = df_data['HeartDisease']\n\n  X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n  \n  X_train.to_csv(output_preprocessed_data_x_train.path, index=False)\n  X_val.to_csv(output_preprocessed_data_x_val.path, index=False)\n  y_train.to_csv(output_preprocessed_data_y_train.path, index=False)\n  y_val.to_csv(output_preprocessed_data_y_val.path, index=False)\n\n@component(\n    packages_to_install=['pandas',\n                         'torch',\n                         'scikit-learn']\n)\ndef training_1st(num_epochs: int, preprocessed_data_path_x_train: Input[Dataset],\n                                  preprocessed_data_path_x_val: Input[Dataset],\n                                  preprocessed_data_path_y_train: Input[Dataset],\n                                  preprocessed_data_path_y_val: Input[Dataset],\n                                  output_preprocessed_data_x_train: Output[Dataset],\n                                  output_preprocessed_data_x_val: Output[Dataset],\n                                  output_preprocessed_data_y_train: Output[Dataset],\n                                  output_preprocessed_data_y_val: Output[Dataset],\n                                  output_accuracy: Output[Metrics]):\n  import torch\n  import torch.nn as nn\n  import torch.optim as optim\n  import pandas as pd\n  from sklearn.preprocessing import StandardScaler\n  from sklearn.metrics import accuracy_score\n\n  X_train = pd.read_csv(preprocessed_data_path_x_train.path)\n  X_val = pd.read_csv(preprocessed_data_path_x_val.path)\n  y_train = pd.read_csv(preprocessed_data_path_y_train.path)\n  y_val = pd.read_csv(preprocessed_data_path_y_val.path)\n\n  X_train.to_csv(output_preprocessed_data_x_train.path, index=False)\n  X_val.to_csv(output_preprocessed_data_x_val.path, index=False)\n  y_train.to_csv(output_preprocessed_data_y_train.path, index=False)\n  y_val.to_csv(output_preprocessed_data_y_val.path, index=False)\n\n  scaler = StandardScaler()\n  X_train = scaler.fit_transform(X_train)\n  X_val = scaler.transform(X_val)\n\n  X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n  y_train_tensor = torch.tensor(y_train.values.reshape(-1, 1), dtype=torch.float32)\n  X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n  y_val_tensor = torch.tensor(y_val.values.reshape(-1, 1), dtype=torch.float32)\n\n  # Definicja modelu\n  class RegressionModel(nn.Module):\n      def __init__(self):\n          super(RegressionModel, self).__init__()\n          self.fc1 = nn.Linear(16, 8)\n          self.fc2 = nn.Linear(8, 4)\n          self.fc3 = nn.Linear(4, 1)\n\n      def forward(self, x):\n          x = torch.relu(self.fc1(x))\n          x = torch.relu(self.fc2(x))\n          x = self.fc3(x)\n          return x\n\n  model = RegressionModel()\n\n  criterion = nn.MSELoss()\n  optimizer = optim.Adam(model.parameters(), lr=0.01)\n\n  # Trenowanie modelu\n  for epoch in range(num_epochs):\n      # Forward pass\n      outputs = model(X_train_tensor)\n      loss = criterion(outputs, y_train_tensor)\n\n      optimizer.zero_grad()\n      loss.backward()\n      optimizer.step()\n\n      with torch.no_grad():\n          val_outputs = model(X_val_tensor)\n          val_loss = criterion(val_outputs, y_val_tensor)\n          print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Validation Loss: {val_loss.item():.4f}')\n\n  with torch.no_grad():\n      val_predictions = model(X_val_tensor)\n      val_predictions = torch.round(val_predictions)\n      val_accuracy = accuracy_score(y_val_tensor.numpy(), val_predictions.numpy())\n      print(f'Validation Accuracy: {val_accuracy:.4f}')\n\n  output_accuracy.log_metric('accuracy', val_accuracy) \n\n@component(\n    packages_to_install=['pandas',\n                         'torch',\n                         'scikit-learn']\n)\ndef training_2nd(num_epochs: int, preprocessed_data_path_x_train: Input[Dataset],\n                                  preprocessed_data_path_x_val: Input[Dataset],\n                                  preprocessed_data_path_y_train: Input[Dataset],\n                                  preprocessed_data_path_y_val: Input[Dataset],\n                                  input_accuracy_1st: Input[Metrics],\n                                  output_accuracy_1st: Output[Metrics],\n                                  output_accuracy: Output[Metrics]):\n  import torch\n  import torch.nn as nn\n  import torch.optim as optim\n  import pandas as pd\n  from sklearn.preprocessing import StandardScaler\n  from sklearn.metrics import accuracy_score\n\n  X_train = pd.read_csv(preprocessed_data_path_x_train.path)\n  X_val = pd.read_csv(preprocessed_data_path_x_val.path)\n  y_train = pd.read_csv(preprocessed_data_path_y_train.path)\n  y_val = pd.read_csv(preprocessed_data_path_y_val.path)\n\n  scaler = StandardScaler()\n  X_train = scaler.fit_transform(X_train)\n  X_val = scaler.transform(X_val)\n\n  X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n  y_train_tensor = torch.tensor(y_train.values.reshape(-1, 1), dtype=torch.float32)\n  X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n  y_val_tensor = torch.tensor(y_val.values.reshape(-1, 1), dtype=torch.float32)\n\n  # Definicja modelu\n  class RegressionModel(nn.Module):\n      def __init__(self):\n          super(RegressionModel, self).__init__()\n          self.fc1 = nn.Linear(16, 16)\n          self.fc2 = nn.Linear(16, 8)\n          self.fc3 = nn.Linear(8, 8)\n          self.fc4 = nn.Linear(8, 1)\n\n      def forward(self, x):\n          x = torch.relu(self.fc1(x))\n          x = torch.relu(self.fc2(x))\n          x = torch.relu(self.fc3(x))\n          x = self.fc4(x)\n          return x\n\n  model = RegressionModel()\n\n  criterion = nn.MSELoss()\n  optimizer = optim.Adam(model.parameters(), lr=0.01)\n\n  # Trenowanie modelu\n  for epoch in range(num_epochs):\n      # Forward pass\n      outputs = model(X_train_tensor)\n      loss = criterion(outputs, y_train_tensor)\n\n      optimizer.zero_grad()\n      loss.backward()\n      optimizer.step()\n\n      with torch.no_grad():\n          val_outputs = model(X_val_tensor)\n          val_loss = criterion(val_outputs, y_val_tensor)\n          print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Validation Loss: {val_loss.item():.4f}')\n\n  with torch.no_grad():\n      val_predictions = model(X_val_tensor)\n      val_predictions = torch.round(val_predictions)\n      val_accuracy = accuracy_score(y_val_tensor.numpy(), val_predictions.numpy())\n      print(f'Validation Accuracy: {val_accuracy:.4f}')\n\n  output_accuracy.log_metric('accuracy', val_accuracy)\n\n  output_accuracy_1st.log_metric('accuracy', input_accuracy_1st.metadata['accuracy']) \n\n@component()\ndef compare_accuracy(accuracy_1st: Input[Metrics],accuracy_2nd: Input[Metrics], better_model: Output[Metrics]):\n\n  accuracy_1st_result = accuracy_1st.metadata['accuracy']\n  accuracy_2nd_result = accuracy_2nd.metadata['accuracy']\n\n  print(f'Pierwszy model osiagnal wartosc {accuracy_1st_result}, a drugi {accuracy_2nd_result}.')\n\n  which_better = 0\n\n  if accuracy_1st_result > accuracy_2nd_result:\n    print('Pierwszy model osiagnal wieksza dokladnosc')\n    which_better = 1\n  elif accuracy_1st_result < accuracy_2nd_result:\n    print('Drugi model osiagnal wieksza dokladnosc')\n    which_better = 2\n  elif accuracy_1st_result == accuracy_2nd_result:\n    print('Obydwa modele osiagnely taka sama dokladnosc')\n  else:\n    print('Blad')\n\n  better_model.log_metric('better_model', which_better)\n\n# Define a pipeline and create a task from a component:\n@dsl.pipeline(\n    name='my-pipeline',\n    # You can optionally specify your own pipeline_root\n    # pipeline_root='gs://my-pipeline-root/example-pipeline',\n)\ndef my_pipeline(url: str, num_epochs_1st: int, num_epochs_2nd: int):\n  web_downloader_task = download_dataframe(url=url)\n  preprocessing_task = preprocessing(data_path=web_downloader_task.outputs['output_csv'])\n  training_task_1st = training_1st(num_epochs=num_epochs_1st, preprocessed_data_path_x_train=preprocessing_task.outputs['output_preprocessed_data_x_train'],\n                                                              preprocessed_data_path_x_val=preprocessing_task.outputs['output_preprocessed_data_x_val'],\n                                                              preprocessed_data_path_y_train=preprocessing_task.outputs['output_preprocessed_data_y_train'],\n                                                              preprocessed_data_path_y_val=preprocessing_task.outputs['output_preprocessed_data_y_val'])\n  training_task_2nd = training_2nd(num_epochs=num_epochs_2nd, preprocessed_data_path_x_train=training_task_1st.outputs['output_preprocessed_data_x_train'],\n                                                              preprocessed_data_path_x_val=training_task_1st.outputs['output_preprocessed_data_x_val'],\n                                                              preprocessed_data_path_y_train=training_task_1st.outputs['output_preprocessed_data_y_train'],\n                                                              preprocessed_data_path_y_val=training_task_1st.outputs['output_preprocessed_data_y_val'],\n                                                              input_accuracy_1st = training_task_1st.outputs['output_accuracy'])\n  compare_task = compare_accuracy(accuracy_1st=training_task_2nd.outputs['output_accuracy_1st'], accuracy_2nd=training_task_2nd.outputs['output_accuracy'])\n\nkfp.compiler.Compiler(mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE).compile(\n    pipeline_func=my_pipeline,\n    package_path='pipeline_sequential_v1.yaml')"
  },
  {
    "repo": "akontaxakis/kubeflow_pipelines",
    "file_path": "samples/Full_experiment.py",
    "raw_url": "https://raw.githubusercontent.com/akontaxakis/kubeflow_pipelines/master/samples/Full_experiment.py",
    "content": "\nimport kfp\nfrom kfp import dsl\n\n\ndef A(TRAIN_SET_COUNT: int, TRAIN_SET_LIMIT: int,  TRAIN_INPUT_JSON: kfp.components.OutputPath(str)):\n    from random import randint\n    import json\n    TRAIN_INPUT = list()\n    for i in range(TRAIN_SET_COUNT):\n        a = randint(0, TRAIN_SET_LIMIT)\n        b = randint(0, TRAIN_SET_LIMIT)\n        c = randint(0, TRAIN_SET_LIMIT)\n        TRAIN_INPUT.append([a, b, c])\n    jsonString = json.dumps(TRAIN_INPUT)\n    with open(TRAIN_INPUT_JSON, 'w') as odd_writer:\n        odd_writer.write(jsonString)\n\n\nA_op = kfp.components.func_to_container_op(A)\n\ndef B(TRAIN_SET_COUNT: int, TRAIN_INPUT_JSON: kfp.components.InputPath(str),TRAIN_OUTPUT_JSON: kfp.components.OutputPath(str)):\n    \"\"\"Generate a random number between minimum and maximum (inclusive).\"\"\"\n    from random import randint\n    import json\n    TRAIN_OUTPUT = list()\n    with open(TRAIN_INPUT_JSON, 'r') as reader:\n        TRAIN_INPUT = json.loads(reader.readline())\n    for i in range(TRAIN_SET_COUNT):\n        a = TRAIN_INPUT.__getitem__(i)[0]\n        b = TRAIN_INPUT.__getitem__(i)[1]\n        c = TRAIN_INPUT.__getitem__(i)[2]\n        op = a + (2 * b) + (3 * c)\n        TRAIN_OUTPUT.append(op)\n    jsonString = json.dumps(TRAIN_OUTPUT)\n    with open(TRAIN_OUTPUT_JSON, 'w') as odd_writer:\n        odd_writer.write(jsonString)\n\nB_op = kfp.components.func_to_container_op(B)\n\n\ndef AB(TRAIN_SET_COUNT: int, TRAIN_SET_LIMIT: int,  TRAIN_INPUT_JSON: kfp.components.OutputPath(str), TRAIN_OUTPUT_JSON: kfp.components.OutputPath(str)):\n    \"\"\"Generate a random number between minimum and maximum (inclusive).\"\"\"\n    from random import randint\n    import json\n    TRAIN_INPUT = list()\n    TRAIN_OUTPUT = list()\n    for i in range(TRAIN_SET_COUNT):\n        a = randint(0, TRAIN_SET_LIMIT)\n        b = randint(0, TRAIN_SET_LIMIT)\n        c = randint(0, TRAIN_SET_LIMIT)\n        op = a + (2 * b) + (3 * c)\n        TRAIN_INPUT.append([a, b, c])\n        TRAIN_OUTPUT.append(op)\n    jsonString = json.dumps(TRAIN_INPUT)\n    with open(TRAIN_INPUT_JSON, 'w') as odd_writer:\n        odd_writer.write(jsonString)\n    jsonString = json.dumps(TRAIN_OUTPUT)\n    with open(TRAIN_OUTPUT_JSON, 'w') as odd_writer:\n        odd_writer.write(jsonString)\n\nAB_op = kfp.components.func_to_container_op(AB)\n\ndef ABC(TRAIN_SET_COUNT: int, TRAIN_SET_LIMIT: int, testD: kfp.components.OutputPath(str)):\n    \"\"\"Generate a random number between minimum and maximum (inclusive).\"\"\"\n    from random import randint\n    import json\n    TRAIN_INPUT = list()\n    TRAIN_OUTPUT = list()\n    for i in range(TRAIN_SET_COUNT):\n        a = randint(0, TRAIN_SET_LIMIT)\n        b = randint(0, TRAIN_SET_LIMIT)\n        c = randint(0, TRAIN_SET_LIMIT)\n        op = a + (2 * b) + (3 * c)\n        TRAIN_INPUT.append([a, b, c])\n        TRAIN_OUTPUT.append(op)\n\n\n    import sys\n    import subprocess\n\n    # implement pip as a subprocess:\n    subprocess.check_call([sys.executable, '-m', 'pip', 'install',\n                           'sklearn'])\n\n    import sklearn\n    from sklearn.linear_model import LinearRegression\n\n    predictor = LinearRegression(n_jobs=-1)\n    import json\n\n    predictor.fit(X=TRAIN_INPUT, y=TRAIN_OUTPUT)\n\n    X_TEST = [[10, 20, 30]]\n    outcome = predictor.predict(X=X_TEST)\n    coefficients = predictor.coef_\n    with open(testD, 'w') as odd_writer:\n        odd_writer.write('Outcome : {}\\nCoefficients : {}'.format(outcome, coefficients))\n\n\n\nABC_op = kfp.components.func_to_container_op(ABC)\n\ndef C(TRAIN_INPUT_JSON: str, TRAIN_OUTPUT_JSON: str, testD: kfp.components.OutputPath(str)):\n    \"\"\"Flip a coin and output heads or tails randomly.\"\"\"\n    import sys\n    import subprocess\n\n    # implement pip as a subprocess:\n    subprocess.check_call([sys.executable, '-m', 'pip', 'install',\n                           'sklearn'])\n\n    import sklearn\n    from sklearn.linear_model import LinearRegression\n    predictor = LinearRegression(n_jobs=-1)\n    import json\n    predictor.fit(X=json.loads(TRAIN_INPUT_JSON), y=json.loads(TRAIN_OUTPUT_JSON))\n\n    X_TEST = [[10, 20, 30]]\n    outcome = predictor.predict(X=X_TEST)\n    coefficients = predictor.coef_\n    with open(testD, 'w') as odd_writer:\n        odd_writer.write('Outcome! : {}\\nCoefficients : {}'.format(outcome, coefficients))\n\n\nC_op = kfp.components.func_to_container_op(C)\n\ndef D(testD: kfp.components.InputPath(str)):\n    \"\"\"Flip a coin and output heads or tails randomly.\"\"\"\n    print(\"!!!\"+testD)\n\n\nD_op = kfp.components.func_to_container_op(D)\n\ndef ABCD(TRAIN_SET_COUNT: int, TRAIN_SET_LIMIT: int) ->str:\n    \"\"\"Generate a random number between minimum and maximum (inclusive).\"\"\"\n    from random import randint\n    import json\n    TRAIN_INPUT = list()\n    TRAIN_OUTPUT = list()\n    for i in range(TRAIN_SET_COUNT):\n        a = randint(0, TRAIN_SET_LIMIT)\n        b = randint(0, TRAIN_SET_LIMIT)\n        c = randint(0, TRAIN_SET_LIMIT)\n        op = a + (2 * b) + (3 * c)\n        TRAIN_INPUT.append([a, b, c])\n        TRAIN_OUTPUT.append(op)\n\n\n    import sys\n    import subprocess\n\n    # implement pip as a subprocess:\n    subprocess.check_call([sys.executable, '-m', 'pip', 'install',\n                           'sklearn'])\n\n    import sklearn\n    from sklearn.linear_model import LinearRegression\n\n    predictor = LinearRegression(n_jobs=-1)\n    import json\n\n    predictor.fit(X=TRAIN_INPUT, y=TRAIN_OUTPUT)\n\n    X_TEST = [[10, 20, 30]]\n    outcome = predictor.predict(X=X_TEST)\n    coefficients = predictor.coef_\n\n    return '!!Outcome : {}\\nCoefficients : {}'.format(outcome, coefficients)\n\nABCD_op = kfp.components.func_to_container_op(ABCD)\n\n@dsl.pipeline(\n    name='No merging pipeline',\n    description='Shows the cost of running an unmerged pipeline'\n)\ndef noMerge_pipeline():\n    out1 = A_op(1000, 1000)\n    out2 = B_op(1000, out1.outputs['TRAIN_INPUT_JSON'])\n    out3 = C_op(out1.outputs['TRAIN_INPUT_JSON'], out2.outputs['TRAIN_OUTPUT_JSON'])\n    D_op(out3.outputs['testD'])\n\n\ndef halfMerge_pipeline():\n    out3 = ABC_op(1000, 1000)\n    D_op(out3.outputs['testD'])\n\ndef fullMerge_pipeline():\n    out3 = ABCD_op(1000, 1000)\n\n\n\nif __name__ == '__main__':\n\n    client = kfp.Client()\n    arguments = {}  # whatever makes sense for new version\n\n    client.create_run_from_pipeline_func(fullMerge_pipeline, arguments=arguments)\n"
  },
  {
    "repo": "akontaxakis/kubeflow_pipelines",
    "file_path": "samples/HalfMerge.py",
    "raw_url": "https://raw.githubusercontent.com/akontaxakis/kubeflow_pipelines/master/samples/HalfMerge.py",
    "content": "\n\nimport kfp\nfrom kfp import dsl\nfrom kfp.components import func_to_container_op, InputPath, OutputPath\n\n\ndef get_train_input(TRAIN_SET_COUNT: int, TRAIN_SET_LIMIT: int,  TRAIN_INPUT_JSON: kfp.components.OutputPath(str), TRAIN_OUTPUT_JSON: kfp.components.OutputPath(str)):\n    \"\"\"Generate a random number between minimum and maximum (inclusive).\"\"\"\n    from random import randint\n    import json\n    TRAIN_INPUT = list()\n    TRAIN_OUTPUT = list()\n    for i in range(TRAIN_SET_COUNT):\n        a = randint(0, TRAIN_SET_LIMIT)\n        b = randint(0, TRAIN_SET_LIMIT)\n        c = randint(0, TRAIN_SET_LIMIT)\n        op = a + (2 * b) + (3 * c)\n        TRAIN_INPUT.append([a, b, c])\n        TRAIN_OUTPUT.append(op)\n    jsonString = json.dumps(TRAIN_INPUT)\n    with open(TRAIN_INPUT_JSON, 'w') as odd_writer:\n        odd_writer.write(jsonString)\n    jsonString = json.dumps(TRAIN_OUTPUT)\n    with open(TRAIN_OUTPUT_JSON, 'w') as odd_writer:\n        odd_writer.write(jsonString)\n\nget_train_input_op = kfp.components.func_to_container_op(get_train_input)\n\n\ndef train_evaluation(k: int, TRAIN_INPUT_JSON: str, TRAIN_OUTPUT_JSON: str) -> str:\n    \"\"\"Flip a coin and output heads or tails randomly.\"\"\"\n    import sys\n    import subprocess\n    j = k\n    # implement pip as a subprocess:\n    subprocess.check_call([sys.executable, '-m', 'pip', 'install','sklearn'])\n\n    import sklearn\n    from sklearn.linear_model import LinearRegression\n    predictor = LinearRegression(n_jobs=-1)\n    import json\n    predictor.fit(X=json.loads(TRAIN_INPUT_JSON), y=json.loads(TRAIN_OUTPUT_JSON))\n\n    X_TEST = [[10, 20, 30]]\n    outcome = predictor.predict(X=X_TEST)\n    coefficients = predictor.coef_\n\n    return 'Outcome : {}\\nCoefficients : {}'.format(outcome, coefficients)\n\n\ntrain_evaluation_op = kfp.components.func_to_container_op(train_evaluation)\n\n\n@dsl.pipeline(\n    name='No merging pipeline',\n    description='Shows the cost of running an unmerged pipeline'\n)\ndef halfMerge_pipeline():\n    out1 = get_train_input_op(2000, 100)\n    #output = train_evaluation_op(-1, out1.outputs['TRAIN_INPUT_JSON'], out1.outputs['TRAIN_OUTPUT_JSON'])\n\n\n# Submit the pipeline for execution:\n# kfp.Client(host=kfp_endpoint).create_run_from_pipeline_func(flipcoin_pipeline, arguments={})\nif __name__ == '__main__':\n    # Compiling the pipeline\n\n    # pipeline_func = noMerge_pipeline\n    # pipeline_filename = pipeline_func.__name__ + '.pipeline.tar.gz'\n    # import kfp.compiler as compiler\n    # compiler.Compiler().compile(pipeline_func, pipeline_filename)\n\n    client = kfp.Client()\n    # Specify pipeline argument values\n    arguments = {}  # whatever makes sense for new version\n    # Submit a pipeline run\n    client.create_run_from_pipeline_func(halfMerge_pipeline, arguments=arguments)\n"
  },
  {
    "repo": "akontaxakis/kubeflow_pipelines",
    "file_path": "samples/Hopefully.py",
    "raw_url": "https://raw.githubusercontent.com/akontaxakis/kubeflow_pipelines/master/samples/Hopefully.py",
    "content": "\n\nimport kfp\nfrom kfp import dsl\nfrom kfp.components import func_to_container_op, InputPath, OutputPath\n\n\ndef get_train_input(TRAIN_SET_COUNT: int, TRAIN_SET_LIMIT: int,  TRAIN_INPUT_JSON: kfp.components.OutputPath(str)):\n    \"\"\"Generate a random number between minimum and maximum (inclusive).\"\"\"\n    from random import randint\n    import json\n    TRAIN_INPUT = list()\n    for i in range(TRAIN_SET_COUNT):\n        a = randint(0, TRAIN_SET_LIMIT)\n        b = randint(0, TRAIN_SET_LIMIT)\n        c = randint(0, TRAIN_SET_LIMIT)\n        TRAIN_INPUT.append([a, b, c])\n    jsonString = json.dumps(TRAIN_INPUT)\n    with open(TRAIN_INPUT_JSON, 'w') as odd_writer:\n        odd_writer.write(jsonString)\n\n\nget_train_input_op = kfp.components.func_to_container_op(get_train_input)\n\n\ndef get_train_output(TRAIN_SET_COUNT: int, TRAIN_INPUT_JSON: kfp.components.InputPath(str),TRAIN_OUTPUT_JSON: kfp.components.OutputPath(str)):\n    \"\"\"Generate a random number between minimum and maximum (inclusive).\"\"\"\n    from random import randint\n    import json\n    TRAIN_OUTPUT = list()\n    with open(TRAIN_INPUT_JSON, 'r') as reader:\n        TRAIN_INPUT = json.loads(reader.readline())\n    for i in range(TRAIN_SET_COUNT):\n        a = TRAIN_INPUT.__getitem__(i)[0]\n        b = TRAIN_INPUT.__getitem__(i)[1]\n        c = TRAIN_INPUT.__getitem__(i)[2]\n        op = a + (2 * b) + (3 * c)\n        TRAIN_OUTPUT.append(op)\n    jsonString = json.dumps(TRAIN_OUTPUT)\n    with open(TRAIN_OUTPUT_JSON, 'w') as odd_writer:\n        odd_writer.write(jsonString)\n\n\nget_train_output_op = kfp.components.func_to_container_op(get_train_output)\n\n\ndef train_evaluation(TRAIN_INPUT_JSON: str, TRAIN_OUTPUT_JSON: str, tests: kfp.components.OutputPath(str)):\n    \"\"\"Flip a coin and output heads or tails randomly.\"\"\"\n    import sys\n    import subprocess\n\n    # implement pip as a subprocess:\n    subprocess.check_call([sys.executable, '-m', 'pip', 'install',\n                           'sklearn'])\n\n    import sklearn\n    from sklearn.linear_model import LinearRegression\n    predictor = LinearRegression(n_jobs=-1)\n    import json\n    predictor.fit(X=json.loads(TRAIN_INPUT_JSON), y=json.loads(TRAIN_OUTPUT_JSON))\n\n    X_TEST = [[10, 20, 30]]\n    outcome = predictor.predict(X=X_TEST)\n    coefficients = predictor.coef_\n\n    return 'Outcome : {}\\nCoefficients : {}'.format(outcome, coefficients)\n\n\ntrain_evaluation_op = kfp.components.func_to_container_op(train_evaluation)\n\ndef print_fun(test: str):\n    \"\"\"Flip a coin and output heads or tails randomly.\"\"\"\n    print(test)\n\n\nprint_fun_op = kfp.components.func_to_container_op(print_fun)\n\n\n@dsl.pipeline(\n    name='No merging pipeline',\n    description='Shows the cost of running an unmerged pipeline'\n)\ndef noMerge_pipeline():\n    out1 = get_train_input_op(2000, 2000)\n\n# Submit the pipeline for execution:\n# kfp.Client(host=kfp_endpoint).create_run_from_pipeline_func(flipcoin_pipeline, arguments={})\nif __name__ == '__main__':\n    # Compiling the pipeline\n\n    # pipeline_func = noMerge_pipeline\n    # pipeline_filename = pipeline_func.__name__ + '.pipeline.tar.gz'\n    # import kfp.compiler as compiler\n    # compiler.Compiler().compile(pipeline_func, pipeline_filename)\n\n    client = kfp.Client()\n    # Specify pipeline argument values\n    arguments = {}  # whatever makes sense for new version\n    # Submit a pipeline run\n    client.create_run_from_pipeline_func(noMerge_pipeline, arguments=arguments)\n"
  },
  {
    "repo": "akontaxakis/kubeflow_pipelines",
    "file_path": "samples/book_example.py",
    "raw_url": "https://raw.githubusercontent.com/akontaxakis/kubeflow_pipelines/master/samples/book_example.py",
    "content": "import kfp\nfrom kfp import dsl\nfrom kfp.components import func_to_container_op\n\ndef add(a: float, b: float) -> float:\n    '''Calculates sum of two arguments'''\n    return a + b\n\n\nadd_op = kfp.components.func_to_container_op(add)\n\nfrom typing import NamedTuple\n\n\ndef my_divmod(dividend: float, divisor: float) -> NamedTuple('MyDivmodOutput', [('quotient', float), ('remainder', float)]):\n    '''Divides two numbers and calculate the quotient and remainder'''\n    # Imports inside a component function:\n    import numpy as np\n    # This function demonstrates how to use nested functions inside a\n    # component function:\n    def divmod_helper(dividend, divisor):\n        return np.divmod(dividend, divisor)\n\n    (quotient, remainder) = divmod_helper(dividend, divisor)\n    from collections import namedtuple\n    divmod_output = namedtuple('MyDivmodOutput', ['quotient', 'remainder'])\n    return divmod_output(quotient, remainder)\n\n\ndivmod_op = kfp.components.func_to_container_op(my_divmod, base_image='tensorflow/tensorflow:1.14.0-py3')\n\n\n@dsl.pipeline(\n    name='Calculation pipeline',\n    description='A toy pipeline that performs arithmetic calculations.'\n)\ndef calc_pipeline(\n        a='a',\n        b='7',\n        c='17',\n):\n    # Passing pipeline parameter and a constant value as operation arguments\n    add_task = add_op(a, 4)  # Returns a dsl.ContainerOp class instance.\n    # Passing a task output reference as operation arguments\n    # For an operation with a single return value, the output\n    # reference can be accessed using `task.output`\n    # or `task.outputs['output_name']` syntax\n    divmod_task = divmod_op(add_task.output, b)\n    # For an operation with multiple return values, the output references\n    # can be accessed using `task.outputs['output_name']` syntax\n    result_task = add_op(divmod_task.outputs['quotient'], c)\n\nif __name__ == '__main__':\n    client = kfp.Client()\n    # Specify pipeline argument values\n    arguments = {'a': '7', 'b': '8'} #whatever makes sense for new version\n    # Submit a pipeline run\n    client.create_run_from_pipeline_func(calc_pipeline, arguments=arguments)\n"
  },
  {
    "repo": "akontaxakis/kubeflow_pipelines",
    "file_path": "samples/merge.py",
    "raw_url": "https://raw.githubusercontent.com/akontaxakis/kubeflow_pipelines/master/samples/merge.py",
    "content": "\n\nimport kfp\nfrom kfp import dsl\nfrom kfp.components import func_to_container_op, InputPath, OutputPath\n\n\ndef get_train_input(TRAIN_SET_COUNT: int, TRAIN_SET_LIMIT: int):\n    \"\"\"Generate a random number between minimum and maximum (inclusive).\"\"\"\n    from random import randint\n    import json\n    TRAIN_SET_LIMIT = 1000\n    TRAIN_SET_COUNT = 100\n    import sys\n    import subprocess\n\n    # implement pip as a subprocess:\n    subprocess.check_call([sys.executable, '-m', 'pip', 'install',\n                           'sklearn'])\n    TRAIN_INPUT = list()\n    TRAIN_OUTPUT = list()\n    print(type(TRAIN_OUTPUT))\n    for i in range(TRAIN_SET_COUNT):\n        a = randint(0, TRAIN_SET_LIMIT)\n        b = randint(0, TRAIN_SET_LIMIT)\n        c = randint(0, TRAIN_SET_LIMIT)\n        op = a + (2 * b) + (3 * c)\n        TRAIN_INPUT.append([a, b, c])\n        TRAIN_OUTPUT.append(op)\n\n    jsonString = json.dumps(TRAIN_INPUT)\n    print(jsonString)\n\n    TRAIN_INPUT = json.loads(jsonString)\n    print(type(jsonString))\n    for i in range(TRAIN_SET_COUNT):\n        print(TRAIN_INPUT)\n    print(TRAIN_OUTPUT)\n    from sklearn.linear_model import LinearRegression\n\n    predictor = LinearRegression(n_jobs=-1)\n    predictor.fit(X=TRAIN_INPUT, y=TRAIN_OUTPUT)\n\n    print(type(predictor))\n    X_TEST = [[10, 20, 30]]\n    outcome = predictor.predict(X=X_TEST)\n    coefficients = predictor.coef_\n\n    print('Outcome : {}\\nCoefficients : {}'.format(outcome, coefficients))\n\n\nget_train_input_op = kfp.components.func_to_container_op(get_train_input)\n\n\ndef get_train_output(TRAIN_SET_COUNT: int, TRAIN_INPUT_JSON: kfp.components.InputPath(str),TRAIN_OUTPUT_JSON: kfp.components.OutputPath(str)):\n    \"\"\"Generate a random number between minimum and maximum (inclusive).\"\"\"\n    from random import randint\n    import json\n    TRAIN_OUTPUT = list()\n    with open(TRAIN_INPUT_JSON, 'r') as reader:\n        TRAIN_INPUT = json.loads(reader.readline())\n    for i in range(TRAIN_SET_COUNT):\n        a = TRAIN_INPUT.__getitem__(i)[0]\n        b = TRAIN_INPUT.__getitem__(i)[1]\n        c = TRAIN_INPUT.__getitem__(i)[2]\n        op = a + (2 * b) + (3 * c)\n        TRAIN_OUTPUT.append(op)\n    jsonString = json.dumps(TRAIN_OUTPUT)\n    with open(TRAIN_OUTPUT_JSON, 'w') as odd_writer:\n        odd_writer.write(jsonString)\n\n\n\nget_train_output_op = kfp.components.func_to_container_op(get_train_output)\n\n\ndef train_evaluation(TRAIN_INPUT_JSON: str, TRAIN_OUTPUT_JSON: str) -> str:\n    \"\"\"Flip a coin and output heads or tails randomly.\"\"\"\n    import sys\n    import subprocess\n\n    # implement pip as a subprocess:\n    subprocess.check_call([sys.executable, '-m', 'pip', 'install',\n                           'sklearn'])\n\n    import sklearn\n    from sklearn.linear_model import LinearRegression\n    predictor = LinearRegression(n_jobs=-1)\n    import json\n    predictor.fit(X=json.loads(TRAIN_INPUT_JSON), y=json.loads(TRAIN_OUTPUT_JSON))\n\n    X_TEST = [[10, 20, 30]]\n    outcome = predictor.predict(X=X_TEST)\n    coefficients = predictor.coef_\n\n    return 'Outcome : {}\\nCoefficients : {}'.format(outcome, coefficients)\n\n\ntrain_evaluation_op = kfp.components.func_to_container_op(train_evaluation)\n\n\n@dsl.pipeline(\n    name='No merging pipeline',\n    description='Shows the cost of running an unmerged pipeline'\n)\ndef Merge_pipeline():\n    out1 = get_train_input_op(1000, 10000)\n    #out2 = get_train_output_op(1000, out1.outputs['TRAIN_INPUT_JSON'])\n    #output = train_evaluation_op(out1.outputs['TRAIN_INPUT_JSON'], out2.outputs['TRAIN_OUTPUT_JSON'])\n\n\n# Submit the pipeline for execution:\n# kfp.Client(host=kfp_endpoint).create_run_from_pipeline_func(flipcoin_pipeline, arguments={})\nif __name__ == '__main__':\n    # Compiling the pipeline\n\n    # pipeline_func = noMerge_pipeline\n    # pipeline_filename = pipeline_func.__name__ + '.pipeline.tar.gz'\n    # import kfp.compiler as compiler\n    # compiler.Compiler().compile(pipeline_func, pipeline_filename)\n\n    client = kfp.Client()\n    # Specify pipeline argument values\n    arguments = {}  # whatever makes sense for new version\n    # Submit a pipeline run\n    client.create_run_from_pipeline_func(Merge_pipeline, arguments=arguments)\n"
  },
  {
    "repo": "akontaxakis/kubeflow_pipelines",
    "file_path": "samples/mnist_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/akontaxakis/kubeflow_pipelines/master/samples/mnist_pipeline.py",
    "content": "# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nKubeflow Pipelines MNIST example\nRun this script to compile pipeline\n\"\"\"\n\n\nimport kfp.dsl as dsl\nimport kfp.gcp as gcp\nimport kfp.onprem as onprem\n\nplatform = 'local'\n\n@dsl.pipeline(\n  name='MNIST',\n  description='A pipeline to train and serve the MNIST example.'\n)\ndef mnist_pipeline(model_export_dir='gs://your-bucket/export',\n                   train_steps='200',\n                   learning_rate='0.01',\n                   batch_size='100',\n                   pvc_name=''):\n  \"\"\"\n  Pipeline with three stages:\n    1. train an MNIST classifier\n    2. deploy a tf-serving instance to the cluster\n    3. deploy a web-ui to interact with it\n  \"\"\"\n  train = dsl.ContainerOp(\n      name='train',\n      image='gcr.io/kubeflow-examples/mnist/model:v20190304-v0.2-176-g15d997b',\n      arguments=[\n          \"/opt/model.py\",\n          \"--tf-export-dir\", model_export_dir,\n          \"--tf-train-steps\", train_steps,\n          \"--tf-batch-size\", batch_size,\n          \"--tf-learning-rate\", learning_rate\n          ]\n  )\n\n\n  serve_args = [\n      '--model-export-path', model_export_dir,\n      '--server-name', \"mnist-service\"\n  ]\n  if platform != 'GCP':\n    serve_args.extend([\n        '--cluster-name', \"mnist-pipeline\",\n        '--pvc-name', pvc_name\n    ])\n\n  serve = dsl.ContainerOp(\n      name='serve',\n      image='gcr.io/ml-pipeline/ml-pipeline-kubeflow-deployer:'\n            '7775692adf28d6f79098e76e839986c9ee55dd61',\n      arguments=serve_args\n  )\n  serve.after(train)\n\n\n  webui_args = [\n          '--image', 'gcr.io/kubeflow-examples/mnist/web-ui:'\n                     'v20190304-v0.2-176-g15d997b-pipelines',\n          '--name', 'web-ui',\n          '--container-port', '5000',\n          '--service-port', '80',\n          '--service-type', \"LoadBalancer\"\n  ]\n  if platform != 'GCP':\n    webui_args.extend([\n      '--cluster-name', \"mnist-pipeline\"\n    ])\n\n  web_ui = dsl.ContainerOp(\n      name='web-ui',\n      image='gcr.io/kubeflow-examples/mnist/deploy-service:latest',\n      arguments=webui_args\n  )\n  web_ui.after(serve)\n\n  steps = [train, serve, web_ui]\n  for step in steps:\n    if platform == 'GCP':\n      step.apply(gcp.use_gcp_secret('user-gcp-sa'))\n    else:\n      step.apply(onprem.mount_pvc(pvc_name, 'local-storage', '/mnt'))\n\nif __name__ == '__main__':\n  import kfp.compiler as compiler\n  compiler.Compiler().compile(mnist_pipeline, __file__ + '.tar.gz')"
  },
  {
    "repo": "akontaxakis/kubeflow_pipelines",
    "file_path": "samples/sendData.py",
    "raw_url": "https://raw.githubusercontent.com/akontaxakis/kubeflow_pipelines/master/samples/sendData.py",
    "content": "import kfp\nfrom kfp import dsl\n\ndef SendMsg(\n    send_msg: str = 'akash'\n):\n    return dsl.ContainerOp(\n        name = 'Print msg',\n        image = 'docker.io/akashdesarda/comp1:latest',\n        command = ['python', 'msg.py'],\n        arguments=[\n            '--msg', send_msg\n        ],\n        file_outputs={\n            'output': '/output.txt',\n        }\n    )\n\ndef GetMsg(\n    get_msg: str\n):\n    return dsl.ContainerOp(\n        name = 'Read msg from 1st component',\n        image = 'docker.io/akashdesarda/comp2:latest',\n        command = ['python', 'msg.py'],\n        arguments=[\n            '--msg', get_msg\n        ]\n    )\n\n@dsl.pipeline(\n    name = 'Pass parameter',\n    description = 'Passing para')\ndef  passing_parameter(send_msg):\n    comp1 = SendMsg(\"send_msg\")\n    comp2 = GetMsg(comp1.output)\n\n\nif __name__ == '__main__':\n  import kfp.compiler as compiler\n\n  client = kfp.Client()\n  arguments = {}\n  client.create_run_from_pipeline_func(passing_parameter, arguments=arguments)"
  },
  {
    "repo": "akontaxakis/kubeflow_pipelines",
    "file_path": "samples/testFun.py",
    "raw_url": "https://raw.githubusercontent.com/akontaxakis/kubeflow_pipelines/master/samples/testFun.py",
    "content": "import json\n\nimport kfp\nfrom kfp import dsl\nfrom kfp.components import func_to_container_op, InputPath, OutputPath\n\n\ndef get_train_input(TRAIN_SET_COUNT: int, TRAIN_SET_LIMIT: int) -> str:\n    \"\"\"Generate a random number between minimum and maximum (inclusive).\"\"\"\n    from random import randint\n    TRAIN_INPUT = list()\n    for i in range(TRAIN_SET_COUNT):\n        a = randint(0, TRAIN_SET_LIMIT)\n        b = randint(0, TRAIN_SET_LIMIT)\n        c = randint(0, TRAIN_SET_LIMIT)\n        TRAIN_INPUT.append([a, b, c])\n    jsonString = json.dumps(TRAIN_INPUT)\n    return '' + jsonString\n\n\nget_train_input_op = kfp.components.func_to_container_op(get_train_input)\n\n\ndef get_train_output(TRAIN_SET_COUNT: int, TRAIN_INPUT_JSON: str) -> str:\n    \"\"\"Generate a random number between minimum and maximum (inclusive).\"\"\"\n    from random import randint\n\n    TRAIN_OUTPUT = list()\n    TRAIN_INPUT = json.loads(TRAIN_INPUT_JSON)\n    for i in range(TRAIN_SET_COUNT):\n        a = TRAIN_INPUT.__getitem__(i)[0]\n        b = TRAIN_INPUT.__getitem__(i)[1]\n        c = TRAIN_INPUT.__getitem__(i)[2]\n        op = a + (2 * b) + (3 * c)\n        TRAIN_OUTPUT.append(op)\n\n    jsonString = json.dumps(TRAIN_OUTPUT)\n    return '' + jsonString\n\n\nget_train_output_op = kfp.components.func_to_container_op(get_train_output)\n\n\ndef train_evaluation(TRAIN_INPUT_JSON: str, TRAIN_OUTPUT_JSON: str) -> str:\n    \"\"\"Flip a coin and output heads or tails randomly.\"\"\"\n    from sklearn.linear_model import LinearRegression\n    predictor = LinearRegression(n_jobs=-1)\n    predictor.fit(X=json.loads(TRAIN_INPUT_JSON), y=json.loads(TRAIN_OUTPUT_JSON))\n\n    X_TEST = [[10, 20, 30]]\n    outcome = predictor.predict(X=X_TEST)\n    coefficients = predictor.coef_\n\n    return 'Outcome : {}\\nCoefficients : {}'.format(outcome, coefficients)\n\n\ntrain_evaluation_op = kfp.components.func_to_container_op(train_evaluation)\n\n\ndef print_fun(TRAIN_OUTPUT_JSON: str):\n    \"\"\"Flip a coin and output heads or tails randomly.\"\"\"\n    print(str)\n\n\nprint_fun_op = kfp.components.func_to_container_op(print_fun)\n\n\n@dsl.pipeline(\n    name='No merging pipeline',\n    description='Shows the cost of running an unmerged pipeline'\n)\ndef noMerge_pipeline():\n    out1 = get_train_input_op(100, 1000)\n    out2 = get_train_output_op(100, out1.outputs['TRAIN_INPUT_JSON'])\n    output = train_evaluation_op(out1.outputs['TRAIN_INPUT_JSON'], out2.outputs['TRAIN_OUTPUT_JSON'])\n    print_fun_op(output)\n\n# Submit the pipeline for execution:\n# kfp.Client(host=kfp_endpoint).create_run_from_pipeline_func(flipcoin_pipeline, arguments={})\nif __name__ == '__main__':\n    # Compiling the pipeline\n\n    # pipeline_func = noMerge_pipeline\n    # pipeline_filename = pipeline_func.__name__ + '.pipeline.tar.gz'\n    # import kfp.compiler as compiler\n    # compiler.Compiler().compile(pipeline_func, pipeline_filename)\n\n    client = kfp.Client()\n    # Specify pipeline argument values\n    arguments = {}  # whatever makes sense for new version\n    # Submit a pipeline run\n    client.create_run_from_pipeline_func(noMerge_pipeline, arguments=arguments)\n"
  },
  {
    "repo": "akontaxakis/kubeflow_pipelines",
    "file_path": "samples/test/after.py",
    "raw_url": "https://raw.githubusercontent.com/akontaxakis/kubeflow_pipelines/master/samples/test/after.py",
    "content": "# Copyright 2020 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom kfp import components\nfrom kfp import dsl\nfrom kfp import compiler\n\ncomponent_op = components.load_component_from_text(\"\"\"\nname: Print Text\ninputs:\n- {name: text, type: String}\nimplementation:\n  container:\n    image: alpine\n    command:\n    - sh\n    - -c\n    - |\n      set -e -x\n      echo \"$0\"\n    - {inputValue: text}\n\"\"\")\n\n\n@dsl.pipeline(name='pipeline-with-after')\ndef my_pipeline():\n    task1 = component_op(text='1st task')\n    task2 = component_op(text='2nd task').after(task1)\n    task3 = component_op(text='3rd task').after(task1, task2)\n\n\nif __name__ == '__main__':\n    compiler.Compiler().compile(\n        pipeline_func=my_pipeline,\n        pipeline_root='dummy_root',\n        output_path=__file__ + '.json')\n"
  },
  {
    "repo": "akontaxakis/kubeflow_pipelines",
    "file_path": "samples/test/after_test.py",
    "raw_url": "https://raw.githubusercontent.com/akontaxakis/kubeflow_pipelines/master/samples/test/after_test.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp.deprecated as kfp\nfrom .after import my_pipeline\nfrom kfp.samples.test.utils import run_pipeline_func, TestCase\n\nrun_pipeline_func([\n    TestCase(\n        pipeline_func=my_pipeline,\n        mode=kfp.dsl.PipelineExecutionMode.V2_ENGINE),\n])\n"
  },
  {
    "repo": "akontaxakis/kubeflow_pipelines",
    "file_path": "samples/test/cache_v2_compatible_test.py",
    "raw_url": "https://raw.githubusercontent.com/akontaxakis/kubeflow_pipelines/master/samples/test/cache_v2_compatible_test.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Two step v2-compatible pipeline.\"\"\"\n\n# %%\n\nfrom __future__ import annotations\n\nimport random\nimport string\nimport unittest\nimport functools\n\nimport kfp_server_api\n\nfrom .two_step import two_step_pipeline\nfrom kfp.samples.test.utils import run_pipeline_func, TestCase, KfpMlmdClient, KfpTask\nfrom ml_metadata.proto import Execution\n\n\ndef verify_tasks(t: unittest.TestCase, tasks: dict[str, KfpTask], task_state,\n                 uri: str, some_int: int):\n    task_names = [*tasks.keys()]\n    t.assertCountEqual(task_names, ['train-op', 'preprocess'], 'task names')\n\n    preprocess = tasks['preprocess']\n    train = tasks['train-op']\n\n    t.assertEqual(\n        {\n            'name': 'preprocess',\n            'inputs': {\n                'artifacts': [],\n                'parameters': {\n                    'some_int': some_int,\n                    'uri': uri\n                }\n            },\n            'outputs': {\n                'artifacts': [{\n                    'metadata': {\n                        'display_name': 'output_dataset_one',\n                    },\n                    'name': 'output_dataset_one',\n                    'type': 'system.Dataset'\n                }],\n                'parameters': {\n                    'output_parameter_one': some_int\n                }\n            },\n            'type': 'system.ContainerExecution',\n            'state': task_state,\n        }, preprocess.get_dict())\n    t.assertEqual(\n        {\n            'name': 'train-op',\n            'inputs': {\n                'artifacts': [{\n                    'metadata': {\n                        'display_name': 'output_dataset_one',\n                    },\n                    'name': 'dataset',\n                    'type': 'system.Dataset',\n                }],\n                'parameters': {\n                    'num_steps': some_int\n                }\n            },\n            'outputs': {\n                'artifacts': [{\n                    'metadata': {\n                        'display_name': 'model',\n                    },\n                    'name': 'model',\n                    'type': 'system.Model',\n                }],\n                'parameters': {}\n            },\n            'type': 'system.ContainerExecution',\n            'state': task_state,\n        }, train.get_dict())\n\n\ndef verify(run: kfp_server_api.ApiRun, mlmd_connection_config, uri: str,\n           some_int, state: int, **kwargs):\n    t = unittest.TestCase()\n    t.maxDiff = None  # we always want to see full diff\n    t.assertEqual(run.status, 'Succeeded')\n    client = KfpMlmdClient(mlmd_connection_config=mlmd_connection_config)\n    tasks = client.get_tasks(run_id=run.id)\n    verify_tasks(t, tasks, state, uri, some_int)\n\n\nif __name__ == '__main__':\n    letters = string.ascii_lowercase\n    random_uri = 'http://' + ''.join(random.choice(letters) for i in range(5))\n    random_int = random.randint(0, 10000)\n    # TODO: update to v2 engine test\n    # run_pipeline_func([\n    #     TestCase(\n    #         pipeline_func=two_step_pipeline,\n    #         arguments={\n    #             'uri': f'{random_uri}',\n    #             'some_int': f'{random_int}'\n    #         },\n    #         verify_func=functools.partial(\n    #             verify,\n    #             uri=random_uri,\n    #             some_int=random_int,\n    #             state=Execution.State.COMPLETE,\n    #         ),\n    #         mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE,\n    #         enable_caching=True\n    #     ),\n    # ]),\n    # run_pipeline_func([\n    #     TestCase(\n    #         pipeline_func=two_step_pipeline,\n    #         arguments={\n    #             'uri': f'{random_uri}',\n    #             'some_int': f'{random_int}'\n    #         },\n    #         verify_func=functools.partial(\n    #             verify,\n    #             uri=random_uri,\n    #             some_int=random_int,\n    #             state=Execution.State.CACHED\n    #         ),\n    #         mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE,\n    #         enable_caching=True\n    #     ),\n    # ])\n\n# %%\n"
  },
  {
    "repo": "akontaxakis/kubeflow_pipelines",
    "file_path": "samples/test/fail.py",
    "raw_url": "https://raw.githubusercontent.com/akontaxakis/kubeflow_pipelines/master/samples/test/fail.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Fail pipeline.\"\"\"\n\nfrom kfp.deprecated import components, dsl\n\n\ndef fail():\n    '''Fails'''\n    import sys\n    sys.exit(1)\n\n\nfail_op = components.create_component_from_func(\n    fail, base_image='alpine:latest')\n\n\n@dsl.pipeline(name='fail-pipeline')\ndef fail_pipeline():\n    fail_task = fail_op()\n"
  },
  {
    "repo": "akontaxakis/kubeflow_pipelines",
    "file_path": "samples/test/fail_parameter_value_missing.py",
    "raw_url": "https://raw.githubusercontent.com/akontaxakis/kubeflow_pipelines/master/samples/test/fail_parameter_value_missing.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom kfp.deprecated import dsl, components\n\necho = components.load_component_from_text(\n    \"\"\"\nname: Echo\ninputs:\n- {name: text, type: String}\nimplementation:\n  container:\n    image: alpine\n    command:\n    - echo\n    - {inputValue: text}\n\"\"\"\n)\n\n\n@dsl.pipeline(name='parameter_value_missing')\ndef pipeline(\n    parameter:\n    str  # parameter should be specified when submitting, but we are missing it in the test\n):\n    echo_op = echo(text=parameter)\n"
  },
  {
    "repo": "akontaxakis/kubeflow_pipelines",
    "file_path": "samples/test/fail_parameter_value_missing_test.py",
    "raw_url": "https://raw.githubusercontent.com/akontaxakis/kubeflow_pipelines/master/samples/test/fail_parameter_value_missing_test.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp.deprecated as kfp\nfrom .fail_parameter_value_missing import pipeline\nfrom kfp.samples.test.utils import run_pipeline_func, TestCase\n\n\ndef verify(run, run_id: str, **kwargs):\n    assert run.status == 'Succeeded'\n    # TODO(Bobgy): should a pipeline fail when it is missing a required input?\n    # assert run.status == 'Failed'\n\n\nrun_pipeline_func([\n    TestCase(\n        pipeline_func=pipeline,\n        verify_func=verify,\n        mode=kfp.dsl.PipelineExecutionMode.V1_LEGACY)\n])\n"
  },
  {
    "repo": "akontaxakis/kubeflow_pipelines",
    "file_path": "samples/test/fail_test.py",
    "raw_url": "https://raw.githubusercontent.com/akontaxakis/kubeflow_pipelines/master/samples/test/fail_test.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Fail pipeline.\"\"\"\n\nfrom __future__ import annotations\nimport unittest\nimport kfp.deprecated as kfp\nimport kfp_server_api\nfrom ml_metadata.proto import Execution\nfrom .fail import fail_pipeline\nfrom .fail_v2 import fail_pipeline as fail_v2_pipeline\nfrom kfp.samples.test.utils import TaskInputs, TaskOutputs, run_pipeline_func, TestCase, KfpTask\n\n\ndef verify(run, **kwargs):\n    assert run.status == 'Failed'\n\n\ndef verify_v2(t: unittest.TestCase, run: kfp_server_api.ApiRun,\n              tasks: dict[str, KfpTask], **kwargs):\n    t.assertEqual(run.status, 'Failed')\n    t.assertEqual(\n        {\n            'fail':\n                KfpTask(\n                    name='fail',\n                    type='system.ContainerExecution',\n                    # TODO(Bobgy): fix v2 engine to properly publish FAILED state.\n                    state=Execution.State.RUNNING,\n                    inputs=TaskInputs(parameters={}, artifacts=[]),\n                    outputs=TaskOutputs(parameters={}, artifacts=[]),\n                )\n        },\n        tasks,\n    )\n\n\nrun_pipeline_func([\n    TestCase(\n        pipeline_func=fail_v2_pipeline,\n        verify_func=verify_v2,\n        mode=kfp.dsl.PipelineExecutionMode.V2_ENGINE),\n    TestCase(\n        pipeline_func=fail_pipeline,\n        verify_func=verify,\n        mode=kfp.dsl.PipelineExecutionMode.V1_LEGACY),\n])\n"
  },
  {
    "repo": "akontaxakis/kubeflow_pipelines",
    "file_path": "samples/test/fail_v2.py",
    "raw_url": "https://raw.githubusercontent.com/akontaxakis/kubeflow_pipelines/master/samples/test/fail_v2.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Fail pipeline.\"\"\"\n\nfrom kfp import dsl\n\n\n@dsl.component\ndef fail():\n    '''Fails'''\n    import sys\n    sys.exit(1)\n\n\n@dsl.pipeline(name='fail-pipeline')\ndef fail_pipeline():\n    fail_task = fail()\n"
  },
  {
    "repo": "akontaxakis/kubeflow_pipelines",
    "file_path": "samples/test/legacy_data_passing.py",
    "raw_url": "https://raw.githubusercontent.com/akontaxakis/kubeflow_pipelines/master/samples/test/legacy_data_passing.py",
    "content": "# This pipeline demonstrates and verifies all ways of data passing supported by KFP.\n\n# %% [markdown]\n# KFP has simple data passing model. There are three different kinds of arguments and tw different ways to consume an input argument. All combinations are supported.\n\n# Any input can be consumed:\n# * As value (`inputValue` placeholder)\n# * As file (`inputPath` placeholder).\n\n# Input argument can come from:\n# * Constant value\n# * Pipeline parameter\n# * Upstream component output\n\n# Combining these options there are 6 end-to-end data passing cases, each of which works regardless of type:\n\n# 1. Pass constant value which gets consumed as value. (Common)\n# 2. Pass constant value which gets consumed as file. (Often used in test pipelines)\n# 3. Pass pipeline parameter that gets consumed as value. (Common)\n# 4. Pass pipeline parameter that gets consumed as file. (Rare. Sometimes used with JSON parameters)\n# 5. Pass task output that gets consumed as value. (Common)\n# 6. Pass task output that gets consumed as file. (Common)\n\n# The only restriction on types is that when both upstream output and downstream input have types, the types must match.\n\n# %%\nimport kfp.deprecated as kfp\nfrom kfp.deprecated.components import create_component_from_func, InputPath, OutputPath\n\n\n# Components\n# Produce\n@create_component_from_func\ndef produce_anything(data_path: OutputPath()):\n    with open(data_path, \"w\") as f:\n        f.write(\"produce_anything\")\n\n\n@create_component_from_func\ndef produce_something(data_path: OutputPath(\"Something\")):\n    with open(data_path, \"w\") as f:\n        f.write(\"produce_something\")\n\n\n@create_component_from_func\ndef produce_something2() -> 'Something':\n    return \"produce_something2\"\n\n\n@create_component_from_func\ndef produce_string() -> str:\n    return \"produce_string\"\n\n\n# Consume as value\n@create_component_from_func\ndef consume_anything_as_value(data):\n    print(\"consume_anything_as_value: \" + data)\n\n\n@create_component_from_func\ndef consume_something_as_value(data: \"Something\"):\n    print(\"consume_something_as_value: \" + data)\n\n\n@create_component_from_func\ndef consume_string_as_value(data: str):\n    print(\"consume_string_as_value: \" + data)\n\n\n# Consume as file\n@create_component_from_func\ndef consume_anything_as_file(data_path: InputPath()):\n    with open(data_path) as f:\n        print(\"consume_anything_as_file: \" + f.read())\n\n\n@create_component_from_func\ndef consume_something_as_file(data_path: InputPath('Something')):\n    with open(data_path) as f:\n        print(\"consume_something_as_file: \" + f.read())\n\n\n@create_component_from_func\ndef consume_string_as_file(data_path: InputPath(str)):\n    with open(data_path) as f:\n        print(\"consume_string_as_file: \" + f.read())\n\n\n# Pipeline\n@kfp.dsl.pipeline(name='data_passing_pipeline')\ndef data_passing_pipeline(\n    anything_param=\"anything_param\",\n    something_param: \"Something\" = \"something_param\",\n    string_param: str = \"string_param\",\n):\n    produced_anything = produce_anything().output\n    produced_something = produce_something().output\n    produced_string = produce_string().output\n\n    # Pass constant value; consume as value\n    consume_anything_as_value(\"constant\")\n    consume_something_as_value(\"constant\")\n    consume_string_as_value(\"constant\")\n\n    # Pass constant value; consume as file\n    consume_anything_as_file(\"constant\")\n    consume_something_as_file(\"constant\")\n    consume_string_as_file(\"constant\")\n\n    # Pass pipeline parameter; consume as value\n    consume_anything_as_value(anything_param)\n    consume_anything_as_value(something_param)\n    consume_anything_as_value(string_param)\n    consume_something_as_value(anything_param)\n    consume_something_as_value(something_param)\n    consume_string_as_value(anything_param)\n    consume_string_as_value(string_param)\n\n    # Pass pipeline parameter; consume as file\n    consume_anything_as_file(anything_param)\n    consume_anything_as_file(something_param)\n    consume_anything_as_file(string_param)\n    consume_something_as_file(anything_param)\n    consume_something_as_file(something_param)\n    consume_string_as_file(anything_param)\n    consume_string_as_file(string_param)\n\n    # Pass task output; consume as value\n    consume_anything_as_value(produced_anything)\n    consume_anything_as_value(produced_something)\n    consume_anything_as_value(produced_string)\n    consume_something_as_value(produced_anything)\n    consume_something_as_value(produced_something)\n    consume_string_as_value(produced_anything)\n    consume_string_as_value(produced_string)\n\n    # Pass task output; consume as file\n    consume_anything_as_file(produced_anything)\n    consume_anything_as_file(produced_something)\n    consume_anything_as_file(produced_string)\n    consume_something_as_file(produced_anything)\n    consume_something_as_file(produced_something)\n    consume_string_as_file(produced_anything)\n    consume_string_as_file(produced_string)\n\n\nif __name__ == \"__main__\":\n    kfp_endpoint = None\n    kfp.Client(host=kfp_endpoint).create_run_from_pipeline_func(\n        data_passing_pipeline, arguments={})\n"
  },
  {
    "repo": "akontaxakis/kubeflow_pipelines",
    "file_path": "samples/test/legacy_exit_handler.py",
    "raw_url": "https://raw.githubusercontent.com/akontaxakis/kubeflow_pipelines/master/samples/test/legacy_exit_handler.py",
    "content": "#!/usr/bin/env python3\n# Copyright 2019 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp.deprecated as kfp\nfrom kfp.deprecated import dsl, compiler\n\n\ndef gcs_download_op(url):\n    return dsl.ContainerOp(\n        name='GCS - Download',\n        image='google/cloud-sdk:279.0.0',\n        command=['sh', '-ce'],\n        arguments=['gsutil cp $0 $1 && cat $1', url, '/tmp/results.txt'],\n        file_outputs={\n            'data': '/tmp/results.txt',\n        }\n    )\n\n\ndef echo_op(text):\n    return dsl.ContainerOp(\n        name='echo',\n        image='library/bash:4.4.23',\n        command=['sh', '-cex'],\n        arguments=['echo \"$0\"', text],\n    )\n\n\n@dsl.pipeline(\n    name='Exit Handler',\n    description=\n    'Downloads a message and prints it. The exit handler will run after the pipeline finishes (successfully or not).'\n)\ndef download_and_print(url='gs://ml-pipeline/shakespeare1.txt'):\n    \"\"\"A sample pipeline showing exit handler.\"\"\"\n\n    exit_task = echo_op('exit!')\n\n    with dsl.ExitHandler(exit_task):\n        download_task = gcs_download_op(url)\n        echo_task = echo_op(download_task.output)\n\n\nif __name__ == '__main__':\n    compiler.Compiler().compile(download_and_print, __file__ + '.yaml')\n"
  },
  {
    "repo": "akontaxakis/kubeflow_pipelines",
    "file_path": "samples/test/legacy_exit_handler_test.py",
    "raw_url": "https://raw.githubusercontent.com/akontaxakis/kubeflow_pipelines/master/samples/test/legacy_exit_handler_test.py",
    "content": "#!/usr/bin/env python3\n# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp.deprecated as kfp\nfrom .legacy_exit_handler import download_and_print\nfrom kfp.samples.test.utils import run_pipeline_func, TestCase\n\nrun_pipeline_func([\n    # This sample is expected to not work on v2 compatible mode.\n    TestCase(\n        pipeline_func=download_and_print,\n        mode=kfp.dsl.PipelineExecutionMode.V1_LEGACY)\n])\n"
  },
  {
    "repo": "akontaxakis/kubeflow_pipelines",
    "file_path": "samples/test/lightweight_python_functions_v2_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/akontaxakis/kubeflow_pipelines/master/samples/test/lightweight_python_functions_v2_pipeline.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Sample pipeline for passing data in KFP v2.\"\"\"\nfrom typing import Dict, List\nimport os\n\nfrom kfp import dsl\nfrom kfp.dsl import Input, InputPath, Output, OutputPath, Dataset, Model, component\nimport kfp.compiler as compiler\n\n# In tests, we install a KFP package from the PR under test. Users should not\n# normally need to specify `kfp_package_path` in their component definitions.\n_KFP_PACKAGE_PATH = os.getenv('KFP_PACKAGE_PATH')\n\n\n@component(kfp_package_path=_KFP_PACKAGE_PATH)\ndef preprocess(\n    # An input parameter of type string.\n    message: str,\n    # Use Output[T] to get a metadata-rich handle to the output artifact\n    # of type `Dataset`.\n    output_dataset_one: Output[Dataset],\n    # A locally accessible filepath for another output artifact of type\n    # `Dataset`.\n    output_dataset_two_path: OutputPath('Dataset'),\n    # A locally accessible filepath for an output parameter of type string.\n    output_parameter_path: OutputPath(str),\n    # A locally accessible filepath for an output parameter of type bool.\n    output_bool_parameter_path: OutputPath(bool),\n    # A locally accessible filepath for an output parameter of type dict.\n    output_dict_parameter_path: OutputPath(Dict[str, int]),\n    # A locally accessible filepath for an output parameter of type list.\n    output_list_parameter_path: OutputPath(List[str]),\n    # An input message that defaults to empty.\n    empty_message: str = \"\",\n):\n    \"\"\"Dummy preprocessing step\"\"\"\n\n    # Use Dataset.path to access a local file path for writing.\n    # One can also use Dataset.uri to access the actual URI file path.\n    with open(output_dataset_one.path, 'w') as f:\n        f.write(message)\n\n    # OutputPath is used to just pass the local file path of the output artifact\n    # to the function.\n    with open(output_dataset_two_path, 'w') as f:\n        f.write(message)\n\n    with open(output_parameter_path, 'w') as f:\n        f.write(message)\n\n    with open(output_bool_parameter_path, 'w') as f:\n        f.write(\n            str(True)\n        )  # use either `str()` or `json.dumps()` for bool values.\n\n    import json\n    with open(output_dict_parameter_path, 'w') as f:\n        f.write(json.dumps({'A': 1, 'B': 2}))\n\n    with open(output_list_parameter_path, 'w') as f:\n        f.write(json.dumps(['a', 'b', 'c']))\n\n\n@component(kfp_package_path=_KFP_PACKAGE_PATH)\ndef train(\n    # Use InputPath to get a locally accessible path for the input artifact\n    # of type `Dataset`.\n    dataset_one_path: InputPath('Dataset'),\n    # Use Input[T] to get a metadata-rich handle to the input artifact\n    # of type `Dataset`.\n    dataset_two: Input[Dataset],\n    # An input parameter of type string.\n    message: str,\n    # Use Output[T] to get a metadata-rich handle to the output artifact\n    # of type `Dataset`.\n    model: Output[Model],\n    # An input parameter of type bool.\n    input_bool: bool,\n    # An input parameter of type dict.\n    input_dict: Dict[str, int],\n    # An input parameter of type List[str].\n    input_list: List[str],\n    # An input parameter of type int with a default value.\n    num_steps: int = 100,\n):\n    \"\"\"Dummy Training step\"\"\"\n    with open(dataset_one_path, 'r') as input_file:\n        dataset_one_contents = input_file.read()\n\n    with open(dataset_two.path, 'r') as input_file:\n        dataset_two_contents = input_file.read()\n\n    line = (\n        f'dataset_one_contents: {dataset_one_contents} || '\n        f'dataset_two_contents: {dataset_two_contents} || '\n        f'message: {message} || '\n        f'input_bool: {input_bool}, type {type(input_bool)} || '\n        f'input_dict: {input_dict}, type {type(input_dict)} || '\n        f'input_list: {input_list}, type {type(input_list)} \\n'\n    )\n\n    with open(model.path, 'w') as output_file:\n        for i in range(num_steps):\n            output_file.write('Step {}\\n{}\\n=====\\n'.format(i, line))\n\n    # model is an instance of Model artifact, which has a .metadata dictionary\n    # to store arbitrary metadata for the output artifact.\n    model.metadata['accuracy'] = 0.9\n\n\n@dsl.pipeline(name='my-test-pipeline-beta')\ndef pipeline(message: str = 'message'):\n    preprocess_task = preprocess(message=message)\n    train_task = train(\n        dataset_one_path=preprocess_task.outputs['output_dataset_one'],\n        dataset_two=preprocess_task.outputs['output_dataset_two_path'],\n        message=preprocess_task.outputs['output_parameter_path'],\n        input_bool=preprocess_task.outputs['output_bool_parameter_path'],\n        input_dict=preprocess_task.outputs['output_dict_parameter_path'],\n        input_list=preprocess_task.outputs['output_list_parameter_path'],\n    )\n\n\nif __name__ == '__main__':\n    compiler.Compiler().compile(\n        pipeline_func=pipeline, package_path=__file__.replace('.py', '.yaml')\n    )\n"
  },
  {
    "repo": "gerardfarm/Kubeflow-Pipelines",
    "file_path": "mnist.py",
    "raw_url": "https://raw.githubusercontent.com/gerardfarm/Kubeflow-Pipelines/main/mnist.py",
    "content": "\n\n\"\"\" To run this pipeline, put into your terminal:\n        dsl-compile --py utils.py --output pipeline.yaml\n\"\"\"\n\ndef unzip_func(bucket_name, zip_data_path_in_s3='mnist.zip',\n                            unzip_data_path_in_s3='dest/',\n                            downloaded_data_path_out='data/mnist.zip',\n                            unzip_downloaded_data_path='./unzipped_data',\n                            AWS_REGION='us-east-1'):\n    \"\"\" \n    Download zip data from s3, extract and then re-upload it to S3\n    Parameters:\n        - bucket_name : str, name of the bucket\n        - zip_data_path_in_s3: str, complete path of zip data on S3\n        - unzip_data_path_in_s3: str, path where you want to extract your data on S3\n        - downloaded_data_path_out: str, path where data is downloaded on PC\n        - unzip_downloaded_data_path: str, path to extract data\n    \"\"\"\n\n    # It is mandotory to put necessary libraries here\n    import os\n    import boto3\n    import zipfile\n\n    os.makedirs(\"data\", exist_ok=True)\n    os.makedirs(\"unzipped_data\", exist_ok=True)\n\n    # Access S3\n    conn_s3 = boto3.client('s3', region_name=AWS_REGION)\n    s3 = boto3.resource('s3', region_name=AWS_REGION)\n    my_bucket = s3.Bucket(bucket_name)\n\n    # Download data on your PC\n    obj = my_bucket.Object(zip_data_path_in_s3)\n    obj.download_file(Filename=downloaded_data_path_out)\n    \n    # Unzip downloaded data\n    with zipfile.ZipFile(downloaded_data_path_out, 'r') as file:\n        file.extractall(unzip_downloaded_data_path)\n            \n    # Upload unzipped data to your S3 Storage Bucket\n    for file in os.listdir(unzip_downloaded_data_path):\n        output_path = unzip_data_path_in_s3 + file\n        conn_s3.upload_file(os.path.join(unzip_downloaded_data_path, file), bucket_name, output_path)\n\n#unzip_func(bucket_name='ali-bucket-gerard')\n\nimport kfp\nimport boto3\n\nAWS_REGION='us-east-1'\nAWS_ACCOUNT_ID = boto3.client('sts').get_caller_identity().get('Account')\n\nREPO_NAME = 'hello-repository'\nBUCKET_NAME = 'ali-bucket-gerard'\nDOCKER_REGISTRY = '{}.dkr.ecr.{}.amazonaws.com/{}:latest'.format(AWS_ACCOUNT_ID, \n                                                            AWS_REGION, \n                                                            REPO_NAME)\n\nunzip_files_op = kfp.components.create_component_from_func(unzip_func, base_image=DOCKER_REGISTRY) \n\n@kfp.dsl.pipeline(\n    name='testing-s3-in-pipeline',\n    description='dowload zip mnist data and re-upload it on s3.'\n)\ndef unzip_and_read_pipeline(BUCKET_NAME='ali-bucket-gerard'):  \n    # Call the first OP\n    first_task = unzip_files_op(BUCKET_NAME)\n\nif __name__ == \"__main__\":\n    # execute only if run as a script\n    kfp.compiler.Compiler().compile(\n        pipeline_func=unzip_and_read_pipeline,\n        package_path='test-s3-pipeline.yaml')"
  },
  {
    "repo": "gerardfarm/Kubeflow-Pipelines",
    "file_path": "examples/hello_world.py",
    "raw_url": "https://raw.githubusercontent.com/gerardfarm/Kubeflow-Pipelines/main/examples/hello_world.py",
    "content": "import kfp\nimport boto3\n\n\nAWS_REGION='region-name'\nAWS_ACCOUNT_ID = boto3.client('sts').get_caller_identity().get('Account')\n\nREPO_NAME = 'repository-name'\nBUCKET_NAME = 'bucket-name'\nDOCKER_REGISTRY = '{}.dkr.ecr.{}.amazonaws.com/{}:latest'.format(AWS_ACCOUNT_ID, \n                                                            AWS_REGION, \n                                                            REPO_NAME)\n\ndef hello_world(text: str) -> str:\n    print(text)\n    return text\n\nhello_task = kfp.components.create_component_from_func(hello_world, \n                                                        base_image=DOCKER_REGISTRY) \n\n\n@kfp.dsl.pipeline(name='hello-world', description='A simple intro pipeline')\ndef pipeline_hello_world(text: str = 'hi there'):\n    \"\"\"Pipeline that passes small pipeline parameter string to consumer op.\"\"\"\n\n    consume_task = hello_task(text)\n\nif __name__ == \"__main__\":\n    # execute only if run as a script\n    kfp.compiler.Compiler().compile(\n        pipeline_func=pipeline_hello_world,\n        package_path='pipeline.yaml')\n"
  },
  {
    "repo": "gerardfarm/Kubeflow-Pipelines",
    "file_path": "examples/mnist.py",
    "raw_url": "https://raw.githubusercontent.com/gerardfarm/Kubeflow-Pipelines/main/examples/mnist.py",
    "content": "\n\n\"\"\" To run this pipeline, put into your terminal:\n        dsl-compile --py utils.py --output pipeline.yaml\n\"\"\"\n\ndef unzip_func(bucket_name, zip_data_path_in_s3='mnist.zip',\n                            unzip_data_path_in_s3='dest/',\n                            downloaded_data_path_out='data/mnist.zip',\n                            unzip_downloaded_data_path='./unzipped_data',\n                            AWS_REGION='region-name'):\n    \"\"\" \n    Download zip data from s3, extract and then re-upload it to S3\n    Parameters:\n        - bucket_name : str, name of the bucket\n        - zip_data_path_in_s3: str, complete path of zip data on S3\n        - unzip_data_path_in_s3: str, path where you want to extract your data on S3\n        - downloaded_data_path_out: str, path where data is downloaded on PC\n        - unzip_downloaded_data_path: str, path to extract data\n    \"\"\"\n\n    # It is mandotory to put necessary libraries here\n    import os\n    import boto3\n    import zipfile\n\n    os.makedirs(\"data\", exist_ok=True)\n    os.makedirs(\"unzipped_data\", exist_ok=True)\n\n    # Access S3\n    conn_s3 = boto3.client('s3', region_name=AWS_REGION)\n    s3 = boto3.resource('s3', region_name=AWS_REGION)\n    my_bucket = s3.Bucket(bucket_name)\n\n    # Download data on your PC\n    obj = my_bucket.Object(zip_data_path_in_s3)\n    obj.download_file(Filename=downloaded_data_path_out)\n    \n    # Unzip downloaded data\n    with zipfile.ZipFile(downloaded_data_path_out, 'r') as file:\n        file.extractall(unzip_downloaded_data_path)\n            \n    # Upload unzipped data to your S3 Storage Bucket\n    for file in os.listdir(unzip_downloaded_data_path):\n        output_path = unzip_data_path_in_s3 + file\n        conn_s3.upload_file(os.path.join(unzip_downloaded_data_path, file), bucket_name, output_path)\n\n\nimport kfp\nimport boto3\n\nAWS_REGION='region-name'\nAWS_ACCOUNT_ID = boto3.client('sts').get_caller_identity().get('Account')\n\nREPO_NAME = 'repository-name'\nBUCKET_NAME = 'bucket-name'\nDOCKER_REGISTRY = '{}.dkr.ecr.{}.amazonaws.com/{}:latest'.format(AWS_ACCOUNT_ID, \n                                                            AWS_REGION, \n                                                            REPO_NAME)\n\nunzip_files_op = kfp.components.create_component_from_func(unzip_func, base_image=DOCKER_REGISTRY) \n\n@kfp.dsl.pipeline(\n    name='testing-s3-in-pipeline',\n    description='dowload zip mnist data and re-upload it on s3.'\n)\ndef unzip_and_read_pipeline(BUCKET_NAME):  \n    # Call the first OP\n    first_task = unzip_files_op(BUCKET_NAME)\n\nif __name__ == \"__main__\":\n    # execute only if run as a script\n    kfp.compiler.Compiler().compile(\n        pipeline_func=unzip_and_read_pipeline,\n        package_path='test-s3-pipeline.yaml')"
  },
  {
    "repo": "gerardfarm/Kubeflow-Pipelines",
    "file_path": "examples/math_operations/main.py",
    "raw_url": "https://raw.githubusercontent.com/gerardfarm/Kubeflow-Pipelines/main/examples/math_operations/main.py",
    "content": "import kfp\nimport kfp.dsl as dsl\n\nfrom adding import add_op, divmod_op\n\n@dsl.pipeline(\n  name='Addition pipeline',\n  description='An example pipeline that performs addition calculations.'\n)\ndef add_pipeline(\n  a='1',\n  b='7',\n  c='3'\n):\n  # Passes a pipeline parameter and a constant value to the `add_op` factory\n  # function.\n  first_task = add_op(a, 4)\n  # Passes an output reference from `first_add_task` and a pipeline parameter\n  # to the `add_op` factory function. For operations with a single return\n  # value, the output reference can be accessed as `task.output` or\n  # `task.outputs['output_name']`.\n  second_task = add_op(first_task.output, b)\n\n  third_task = divmod_op(first_task.output, \n                              second_task.output)\n\n  result_task = add_op(third_task.outputs['quotient'], c)\n\nif __name__ == \"__main__\":\n    # execute only if run as a script\n    kfp.compiler.Compiler().compile(\n        pipeline_func=add_pipeline,\n        package_path='add_div_pipeline.yaml')"
  },
  {
    "repo": "gerardfarm/Kubeflow-Pipelines",
    "file_path": "examples/object-detection/main.py",
    "raw_url": "https://raw.githubusercontent.com/gerardfarm/Kubeflow-Pipelines/main/examples/object-detection/main.py",
    "content": "import kfp\nimport boto3\n\nfrom typing import NamedTuple\n\n\n# ================================================================\n#                         Upload dataset to S3\n# ================================================================\ndef Preprocessing() -> NamedTuple('My_Output',[('feedback', str)]):\n    \n    # You should upload your dataset to s3\n    \n    # import os\n    # import boto3\n\n    # conn_s3 = boto3.client('s3', region_name=AWS_REGION)\n    \n    # # Images names list\n    # filenames = os.listdir(data_path)\n\n    # # Upload all images to s3\n    # for filename in filenames:\n    #     conn_s3.upload_file(os.path.join(data_path, filename), \n    #                         bucket_name, \n    #                         os.path.join(output_path, filename))\n\n    from collections import namedtuple\n    feedback_msg = 'Done! Data are on S3.'\n    func_output = namedtuple('MyOutput', ['feedback'])\n    return func_output(feedback_msg)\n\n# ================================================================\n#             Object Detection Evaluation on CoCo dataset\n# ================================================================\ndef test_object_detection(msg, bucket_name, AWS_REGION):\n\n    import os\n    import boto3\n    import subprocess\n\n    subprocess.call(\"python3 test.py --data coco128.yaml --weights yolov5s.pt --img 640 --batch-size 2\", shell=True)\n\n    print(msg)\n    #print(os.listdir('runs/test/exp'))\n    \n    # Upload results to s3\n    conn_s3 = boto3.client('s3', region_name=AWS_REGION)\n    output_path = 'runs/test/exp/'\n\n    for filename in os.listdir(output_path):\n        path = os.path.join(output_path, filename)\n        conn_s3.upload_file(path, bucket_name, os.path.join(\"object-detection/results/\", filename))\n\n\n\n# Define registry\nAWS_REGION='region-name'\nAWS_ACCOUNT_ID = boto3.client('sts').get_caller_identity().get('Account')\n\nREPO_NAME = 'object-detection'\nBUCKET_NAME = 'bucket-name'\nTAG_NAME = 'latest'\nDOCKER_REGISTRY = '{}.dkr.ecr.{}.amazonaws.com/{}:{}'.format(\n                                                    AWS_ACCOUNT_ID, \n                                                    AWS_REGION, \n                                                    REPO_NAME,\n                                                    TAG_NAME\n                                                )\n\n# Create components\npreprocess_task = kfp.components.create_component_from_func(Preprocessing, \n                                                    base_image=DOCKER_REGISTRY,\n                                                  #  output_component_file = 'preprocessing.yaml'\n                                                )\n\nmain_task = kfp.components.create_component_from_func(test_object_detection, \n                                                    base_image=DOCKER_REGISTRY,\n                                                  #  output_component_file = 'preprocessing.yaml'\n                                                )\n\n# Create pipeline\n@kfp.dsl.pipeline(\n    name='Object Detection Algorithm', \n    description='Testing YOLOv5 on few images.'\n)\ndef object_detection_pipeline(bucket_name: str = BUCKET_NAME,\n                                AWS_REGION: str = 'us-east-1'\n                                ):\n\n    # Upload your dataset to s3\n    first_task = preprocess_task()\n    second_task = main_task(first_task.outputs['feedback'], bucket_name, AWS_REGION)\n\nif __name__ == \"__main__\":\n    # execute only if run as a script\n    kfp.compiler.Compiler().compile(\n        pipeline_func=object_detection_pipeline,\n        package_path='full-pipeline-object-detection.yaml')\n"
  },
  {
    "repo": "kikuriyou/Kubeflow-pipelines",
    "file_path": "lda/kfp_topic_test.py",
    "raw_url": "https://raw.githubusercontent.com/kikuriyou/Kubeflow-pipelines/master/lda/kfp_topic_test.py",
    "content": "#!/usr/bin/env python3\n\nfrom datetime import datetime, date, timedelta\nimport kfp.dsl as dsl\nimport kfp.compiler as compiler\n\nPROJECT_ID = 'project_id'\nBUCKET = 'bucket'\n\ndef preprocess_op(project: 'GcpProject', bucket, date, dict_file, dataset_file, tmp_dir,\n                  preprocess_output: 'GcsUri[Directory]', step_name='preprocess'):\n    return dsl.ContainerOp(\n        name = step_name,\n        image = 'gcr.io/{}/kfp/pre:latest'.format(PROJECT_ID),\n        arguments = [\n            '--project',      project,\n            '--bucket',       bucket,\n            '--date',         date,\n            '--dict_file',    dict_file,\n            '--dataset_file', dataset_file,\n            '--tmp_dir',      tmp_dir,\n            '--output',       preprocess_output\n        ],\n        file_outputs = {'preprocess': '/output.txt'}\n    )\n\n\ndef training_op(preprocess_output: 'GcsUri[Directory]', project: 'GcpProject', bucket, table, \n                prev_date, date, dict_file, dataset_file, learning_type, pipeline_version, tmp_dir, \n                training_output: 'GcsUri[Directory]', step_name='train'):\n    return dsl.ContainerOp(\n        name = step_name,\n        image = 'gcr.io/{}/kfp/train:latest'.format(PROJECT_ID,\n        arguments = [\n            '--preprocess_output', preprocess_output,\n            '--project',           project,\n            '--bucket',            bucket,\n            '--table',             table,\n            '--prev_date',         prev_date,\n            '--date',              date,\n            '--dict_file',         dict_file,\n            '--dataset_file',      dataset_file,\n            '--learning_type',     learning_type,\n            '--pipeline_version',  pipeline_version,\n            '--tmp_dir',           tmp_dir,\n            '--output',            training_output\n        ],\n        file_outputs = {'train': '/output.txt'}\n    )\n\n\ndef postprocess_op(training_output: 'GcsUri[Directory]', project: 'GcpProject', bucket, table, date, \n                   postprocess_output: 'GcsUri[Directory]', step_name='postprocess'):\n    return dsl.ContainerOp(\n        name = step_name,\n        image = 'gcr.io/{}/kfp/post:latest'.format(PROJECT_ID,\n        arguments = [\n            '--training_output', training_output,\n            '--project',         project,\n            '--bucket',          bucket,\n            '--table',           table,\n            '--date',            date,\n            '--output',          postprocess_output\n        ],\n        file_outputs = {'postprocess': '/output.txt'}\n    )\n\n\n@dsl.pipeline(\n    name='LDA pipeline',\n    description='LDA pipeline running on every Wednesday'\n)\ndef kubeflow_training(\n    output:        dsl.PipelineParam, \n    project:       dsl.PipelineParam,\n    bucket:        dsl.PipelineParam=dsl.PipelineParam(name='bucket',          value=bucket),\n    table:         dsl.PipelineParam=dsl.PipelineParam(name='table',           value='TOPIC_TRY'),\n    prev_date:     dsl.PipelineParam=dsl.PipelineParam(name='prev-date',       value=''),\n    date:          dsl.PipelineParam=dsl.PipelineParam(name='date',            value=''),\n    dict_file:     dsl.PipelineParam=dsl.PipelineParam(name='dictionary-file', value='dict'),\n    dataset_file:  dsl.PipelineParam=dsl.PipelineParam(name='dataset-file',    value='dataset'),\n    learning_type: dsl.PipelineParam=dsl.PipelineParam(name='learning-type',   value='update')):\n\n\n    # TODO: use the argo job name as the workflow\n    #workflow = '{{workflow.name}}'\n    pipeline_version = __file__\n\n    # Make pipeline\n    preprocess = preprocess_op(project, bucket, date, dict_file, dataset_file, '/tmp', output)\n    training = training_op(preprocess.output, project, bucket, table, prev_date, date, \n                           dict_file, dataset_file, learning_type, pipeline_version, '/tmp', output)\n    postprocess = postprocess_op(training.output, project, bucket, table, date, output)\n\n\nif __name__ == '__main__':\n    compiler.Compiler().compile(kubeflow_training, __file__ + '.tar.gz')\n\n"
  },
  {
    "repo": "terus-lim-df/kubeflow-pipeline",
    "file_path": "sample1__embulk_dag.py",
    "raw_url": "https://raw.githubusercontent.com/terus-lim-df/kubeflow-pipeline/main/sample1__embulk_dag.py",
    "content": "#!/usr/bin/env python3\n\nimport kfp\nfrom kfp.components import func_to_container_op, InputPath, OutputPath\n\n\nread_csv = kfp.components.load_component_from_url(\"https://raw.githubusercontent.com/terus-lim-df/kubeflow-pipeline/main/sample1__embulk_component.yaml\")\n\n\n@kfp.dsl.pipeline(\n  name='My pipeline',\n  description='My machine learning pipeline'\n)\ndef file_passing_pipelines(file_path):\n    \"\"\"Combining all pipelines together in a single pipeline\"\"\"\n    read_csv(file_path)\n\n\nif __name__ == '__main__':\n    # Compiling the pipeline\n    kfp.compiler.Compiler().compile(file_passing_pipelines, \"dags/\" + __file__.split(\"/\")[-1] + '.yaml')\n"
  },
  {
    "repo": "SWE-Gym-Raw/kubeflow__pipelines",
    "file_path": "samples/test/after.py",
    "raw_url": "https://raw.githubusercontent.com/SWE-Gym-Raw/kubeflow__pipelines/0.1.8/samples/test/after.py",
    "content": "# Copyright 2020 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom kfp import components\nfrom kfp.v2 import dsl\nimport kfp.v2.compiler as compiler\n\ncomponent_op = components.load_component_from_text(\n    \"\"\"\nname: Print Text\ninputs:\n- {name: text, type: String}\nimplementation:\n  container:\n    image: alpine\n    command:\n    - sh\n    - -c\n    - |\n      set -e -x\n      echo \"$0\"\n    - {inputValue: text}\n\"\"\"\n)\n\n\n@dsl.pipeline(name='pipeline-with-after')\ndef my_pipeline():\n    task1 = component_op(text='1st task')\n    task2 = component_op(text='2nd task').after(task1)\n    task3 = component_op(text='3rd task').after(task1, task2)\n\n\nif __name__ == '__main__':\n    compiler.Compiler().compile(\n        pipeline_func=my_pipeline,\n        pipeline_root='dummy_root',\n        output_path=__file__ + '.json'\n    )\n"
  },
  {
    "repo": "SWE-Gym-Raw/kubeflow__pipelines",
    "file_path": "samples/test/after_test.py",
    "raw_url": "https://raw.githubusercontent.com/SWE-Gym-Raw/kubeflow__pipelines/0.1.8/samples/test/after_test.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp\nfrom .after import my_pipeline\nfrom .util import run_pipeline_func, TestCase\n\nrun_pipeline_func([\n    TestCase(pipeline_func=my_pipeline),\n    TestCase(\n        pipeline_func=my_pipeline, mode=kfp.dsl.PipelineExecutionMode.V2_ENGINE\n    ),\n])\n"
  },
  {
    "repo": "SWE-Gym-Raw/kubeflow__pipelines",
    "file_path": "samples/test/cache_v2_test.py",
    "raw_url": "https://raw.githubusercontent.com/SWE-Gym-Raw/kubeflow__pipelines/0.1.8/samples/test/cache_v2_test.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Two step v2-compatible pipeline.\"\"\"\n\n# %%\n\nfrom __future__ import annotations\n\nimport random\nimport string\nimport unittest\nimport functools\n\nimport kfp\nimport kfp_server_api\n\nfrom .two_step import two_step_pipeline\nfrom .util import run_pipeline_func, TestCase, KfpMlmdClient, KfpTask\nfrom ml_metadata.proto import Execution\n\n\ndef verify_tasks(\n    t: unittest.TestCase, tasks: dict[str, KfpTask], task_state, uri: str,\n    some_int: int\n):\n    task_names = [*tasks.keys()]\n    t.assertCountEqual(task_names, ['train-op', 'preprocess'], 'task names')\n\n    preprocess = tasks['preprocess']\n    train = tasks['train-op']\n\n    t.assertEqual({\n        'name': 'preprocess',\n        'inputs': {\n            'artifacts': [],\n            'parameters': {\n                'some_int': some_int,\n                'uri': uri\n            }\n        },\n        'outputs': {\n            'artifacts': [{\n                'metadata': {\n                    'display_name': 'output_dataset_one',\n                },\n                'name': 'output_dataset_one',\n                'type': 'system.Dataset'\n            }],\n            'parameters': {\n                'output_parameter_one': some_int\n            }\n        },\n        'type': 'system.ContainerExecution',\n        'state': task_state,\n    }, preprocess.get_dict())\n    t.assertEqual({\n        'name': 'train-op',\n        'inputs': {\n            'artifacts': [{\n                'metadata': {\n                    'display_name': 'output_dataset_one',\n                },\n                'name': 'dataset',\n                'type': 'system.Dataset',\n            }],\n            'parameters': {\n                'num_steps': some_int\n            }\n        },\n        'outputs': {\n            'artifacts': [{\n                'metadata': {\n                    'display_name': 'model',\n                },\n                'name': 'model',\n                'type': 'system.Model',\n            }],\n            'parameters': {}\n        },\n        'type': 'system.ContainerExecution',\n        'state': task_state,\n    }, train.get_dict())\n\n\ndef verify(\n    run: kfp_server_api.ApiRun, mlmd_connection_config, uri: str, some_int,\n    state: int, **kwargs\n):\n    t = unittest.TestCase()\n    t.maxDiff = None  # we always want to see full diff\n    t.assertEqual(run.status, 'Succeeded')\n    client = KfpMlmdClient(mlmd_connection_config=mlmd_connection_config)\n    tasks = client.get_tasks(run_id=run.id)\n    verify_tasks(t, tasks, state, uri, some_int)\n\n\nif __name__ == '__main__':\n    letters = string.ascii_lowercase\n    random_uri = 'http://' + ''.join(random.choice(letters) for i in range(5))\n    random_int = random.randint(0, 10000)\n    run_pipeline_func([\n        TestCase(\n            pipeline_func=two_step_pipeline,\n            arguments={\n                'uri': f'{random_uri}',\n                'some_int': f'{random_int}'\n            },\n            verify_func=functools.partial(\n                verify,\n                uri=random_uri,\n                some_int=random_int,\n                state=Execution.State.COMPLETE,\n            ),\n            mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE,\n            enable_caching=True\n        ),\n    ]),\n    run_pipeline_func([\n        TestCase(\n            pipeline_func=two_step_pipeline,\n            arguments={\n                'uri': f'{random_uri}',\n                'some_int': f'{random_int}'\n            },\n            verify_func=functools.partial(\n                verify,\n                uri=random_uri,\n                some_int=random_int,\n                state=Execution.State.CACHED\n            ),\n            mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE,\n            enable_caching=True\n        ),\n    ])\n\n# %%\n"
  },
  {
    "repo": "SWE-Gym-Raw/kubeflow__pipelines",
    "file_path": "samples/test/fail.py",
    "raw_url": "https://raw.githubusercontent.com/SWE-Gym-Raw/kubeflow__pipelines/0.1.8/samples/test/fail.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Fail pipeline.\"\"\"\n\nfrom kfp import components, dsl\n\n\ndef fail():\n    '''Fails'''\n    import sys\n    sys.exit(1)\n\n\nfail_op = components.create_component_from_func(\n    fail, base_image='alpine:latest'\n)\n\n\n@dsl.pipeline(name='fail_pipeline')\ndef fail_pipeline():\n    fail_task = fail_op()\n"
  },
  {
    "repo": "SWE-Gym-Raw/kubeflow__pipelines",
    "file_path": "samples/test/fail_parameter_value_missing.py",
    "raw_url": "https://raw.githubusercontent.com/SWE-Gym-Raw/kubeflow__pipelines/0.1.8/samples/test/fail_parameter_value_missing.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom kfp import dsl, components\n\necho = components.load_component_from_text(\n    \"\"\"\nname: Echo\ninputs:\n- {name: text, type: String}\nimplementation:\n  container:\n    image: alpine\n    command:\n    - echo\n    - {inputValue: text}\n\"\"\"\n)\n\n\n@dsl.pipeline(name='parameter_value_missing')\ndef pipeline(\n    parameter:\n    str  # parameter should be specified when submitting, but we are missing it in the test\n):\n    echo_op = echo(text=parameter)\n"
  },
  {
    "repo": "SWE-Gym-Raw/kubeflow__pipelines",
    "file_path": "samples/test/legacy_data_passing.py",
    "raw_url": "https://raw.githubusercontent.com/SWE-Gym-Raw/kubeflow__pipelines/0.1.8/samples/test/legacy_data_passing.py",
    "content": "# This pipeline demonstrates and verifies all ways of data passing supported by KFP.\n\n# %% [markdown]\n# KFP has simple data passing model. There are three different kinds of arguments and tw different ways to consume an input argument. All combinations are supported.\n\n# Any input can be consumed:\n# * As value (`inputValue` placeholder)\n# * As file (`inputPath` placeholder).\n\n# Input argument can come from:\n# * Constant value\n# * Pipeline parameter\n# * Upstream component output\n\n# Combining these options there are 6 end-to-end data passing cases, each of which works regardless of type:\n\n# 1. Pass constant value which gets consumed as value. (Common)\n# 2. Pass constant value which gets consumed as file. (Often used in test pipelines)\n# 3. Pass pipeline parameter that gets consumed as value. (Common)\n# 4. Pass pipeline parameter that gets consumed as file. (Rare. Sometimes used with JSON parameters)\n# 5. Pass task output that gets consumed as value. (Common)\n# 6. Pass task output that gets consumed as file. (Common)\n\n# The only restriction on types is that when both upstream output and downstream input have types, the types must match.\n\n# %%\nimport kfp\nfrom kfp.components import create_component_from_func, InputPath, OutputPath\n\n\n# Components\n# Produce\n@create_component_from_func\ndef produce_anything(data_path: OutputPath()):\n    with open(data_path, \"w\") as f:\n        f.write(\"produce_anything\")\n\n\n@create_component_from_func\ndef produce_something(data_path: OutputPath(\"Something\")):\n    with open(data_path, \"w\") as f:\n        f.write(\"produce_something\")\n\n\n@create_component_from_func\ndef produce_something2() -> 'Something':\n    return \"produce_something2\"\n\n\n@create_component_from_func\ndef produce_string() -> str:\n    return \"produce_string\"\n\n\n# Consume as value\n@create_component_from_func\ndef consume_anything_as_value(data):\n    print(\"consume_anything_as_value: \" + data)\n\n\n@create_component_from_func\ndef consume_something_as_value(data: \"Something\"):\n    print(\"consume_something_as_value: \" + data)\n\n\n@create_component_from_func\ndef consume_string_as_value(data: str):\n    print(\"consume_string_as_value: \" + data)\n\n\n# Consume as file\n@create_component_from_func\ndef consume_anything_as_file(data_path: InputPath()):\n    with open(data_path) as f:\n        print(\"consume_anything_as_file: \" + f.read())\n\n\n@create_component_from_func\ndef consume_something_as_file(data_path: InputPath('Something')):\n    with open(data_path) as f:\n        print(\"consume_something_as_file: \" + f.read())\n\n\n@create_component_from_func\ndef consume_string_as_file(data_path: InputPath(str)):\n    with open(data_path) as f:\n        print(\"consume_string_as_file: \" + f.read())\n\n\n# Pipeline\n@kfp.dsl.pipeline(name='data_passing_pipeline')\ndef data_passing_pipeline(\n    anything_param=\"anything_param\",\n    something_param: \"Something\" = \"something_param\",\n    string_param: str = \"string_param\",\n):\n    produced_anything = produce_anything().output\n    produced_something = produce_something().output\n    produced_string = produce_string().output\n\n    # Pass constant value; consume as value\n    consume_anything_as_value(\"constant\")\n    consume_something_as_value(\"constant\")\n    consume_string_as_value(\"constant\")\n\n    # Pass constant value; consume as file\n    consume_anything_as_file(\"constant\")\n    consume_something_as_file(\"constant\")\n    consume_string_as_file(\"constant\")\n\n    # Pass pipeline parameter; consume as value\n    consume_anything_as_value(anything_param)\n    consume_anything_as_value(something_param)\n    consume_anything_as_value(string_param)\n    consume_something_as_value(anything_param)\n    consume_something_as_value(something_param)\n    consume_string_as_value(anything_param)\n    consume_string_as_value(string_param)\n\n    # Pass pipeline parameter; consume as file\n    consume_anything_as_file(anything_param)\n    consume_anything_as_file(something_param)\n    consume_anything_as_file(string_param)\n    consume_something_as_file(anything_param)\n    consume_something_as_file(something_param)\n    consume_string_as_file(anything_param)\n    consume_string_as_file(string_param)\n\n    # Pass task output; consume as value\n    consume_anything_as_value(produced_anything)\n    consume_anything_as_value(produced_something)\n    consume_anything_as_value(produced_string)\n    consume_something_as_value(produced_anything)\n    consume_something_as_value(produced_something)\n    consume_string_as_value(produced_anything)\n    consume_string_as_value(produced_string)\n\n    # Pass task output; consume as file\n    consume_anything_as_file(produced_anything)\n    consume_anything_as_file(produced_something)\n    consume_anything_as_file(produced_string)\n    consume_something_as_file(produced_anything)\n    consume_something_as_file(produced_something)\n    consume_string_as_file(produced_anything)\n    consume_string_as_file(produced_string)\n\n\nif __name__ == \"__main__\":\n    kfp_endpoint = None\n    kfp.Client(host=kfp_endpoint).create_run_from_pipeline_func(\n        data_passing_pipeline, arguments={})\n"
  },
  {
    "repo": "SWE-Gym-Raw/kubeflow__pipelines",
    "file_path": "samples/test/legacy_data_passing_test.py",
    "raw_url": "https://raw.githubusercontent.com/SWE-Gym-Raw/kubeflow__pipelines/0.1.8/samples/test/legacy_data_passing_test.py",
    "content": "from .data_passing import data_passing_pipeline\nfrom .util import run_pipeline_func, TestCase,\nfrom kfp.dsl import PipelineExecutionMode\n\nrun_pipeline_func([\n    TestCase(\n        pipeline_func=data_passing_pipeline,\n        mode=PipelineExecutionMode.V1_LEGACY,\n    )\n])\n"
  },
  {
    "repo": "SWE-Gym-Raw/kubeflow__pipelines",
    "file_path": "samples/test/legacy_exit_handler.py",
    "raw_url": "https://raw.githubusercontent.com/SWE-Gym-Raw/kubeflow__pipelines/0.1.8/samples/test/legacy_exit_handler.py",
    "content": "#!/usr/bin/env python3\n# Copyright 2019 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp\nfrom kfp import dsl\n\n\ndef gcs_download_op(url):\n    return dsl.ContainerOp(\n        name='GCS - Download',\n        image='google/cloud-sdk:279.0.0',\n        command=['sh', '-ce'],\n        arguments=['gsutil cp $0 $1 && cat $1', url, '/tmp/results.txt'],\n        file_outputs={\n            'data': '/tmp/results.txt',\n        }\n    )\n\n\ndef echo_op(text):\n    return dsl.ContainerOp(\n        name='echo',\n        image='library/bash:4.4.23',\n        command=['sh', '-cex'],\n        arguments=['echo \"$0\"', text],\n    )\n\n\n@dsl.pipeline(\n    name='Exit Handler',\n    description=\n    'Downloads a message and prints it. The exit handler will run after the pipeline finishes (successfully or not).'\n)\ndef download_and_print(url='gs://ml-pipeline/shakespeare1.txt'):\n    \"\"\"A sample pipeline showing exit handler.\"\"\"\n\n    exit_task = echo_op('exit!')\n\n    with dsl.ExitHandler(exit_task):\n        download_task = gcs_download_op(url)\n        echo_task = echo_op(download_task.output)\n\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(download_and_print, __file__ + '.yaml')\n"
  },
  {
    "repo": "SWE-Gym-Raw/kubeflow__pipelines",
    "file_path": "samples/test/legacy_exit_handler_test.py",
    "raw_url": "https://raw.githubusercontent.com/SWE-Gym-Raw/kubeflow__pipelines/0.1.8/samples/test/legacy_exit_handler_test.py",
    "content": "#!/usr/bin/env python3\n# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp\nfrom .legacy_exit_handler import download_and_print\nfrom .util import run_pipeline_func, TestCase\n\n\nrun_pipeline_func([\n    # This sample is expected to not work on v2 compatible mode.\n    TestCase(\n        pipeline_func=download_and_print,\n        mode=kfp.dsl.PipelineExecutionMode.V1_LEGACY\n    )\n])\n"
  },
  {
    "repo": "SWE-Gym-Raw/kubeflow__pipelines",
    "file_path": "samples/test/lightweight_python_functions_v2_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/SWE-Gym-Raw/kubeflow__pipelines/0.1.8/samples/test/lightweight_python_functions_v2_pipeline.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Sample pipeline for passing data in KFP v2.\"\"\"\nfrom typing import Dict, List\nimport os\n\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import Input, InputPath, Output, OutputPath, Dataset, Model, component\nimport kfp.v2.compiler as compiler\n\n# In tests, we install a KFP package from the PR under test. Users should not\n# normally need to specify `kfp_package_path` in their component definitions.\n_KFP_PACKAGE_PATH = os.getenv('KFP_PACKAGE_PATH')\n\n\n@component(kfp_package_path=_KFP_PACKAGE_PATH)\ndef preprocess(\n    # An input parameter of type string.\n    message: str,\n    # Use Output[T] to get a metadata-rich handle to the output artifact\n    # of type `Dataset`.\n    output_dataset_one: Output[Dataset],\n    # A locally accessible filepath for another output artifact of type\n    # `Dataset`.\n    output_dataset_two_path: OutputPath('Dataset'),\n    # A locally accessible filepath for an output parameter of type string.\n    output_parameter_path: OutputPath(str),\n    # A locally accessible filepath for an output parameter of type bool.\n    output_bool_parameter_path: OutputPath(bool),\n    # A locally accessible filepath for an output parameter of type dict.\n    output_dict_parameter_path: OutputPath(Dict[str, int]),\n    # A locally accessible filepath for an output parameter of type list.\n    output_list_parameter_path: OutputPath(List[str]),\n    # An input message that defaults to empty.\n    empty_message: str = \"\",\n):\n    \"\"\"Dummy preprocessing step\"\"\"\n\n    # Use Dataset.path to access a local file path for writing.\n    # One can also use Dataset.uri to access the actual URI file path.\n    with open(output_dataset_one.path, 'w') as f:\n        f.write(message)\n\n    # OutputPath is used to just pass the local file path of the output artifact\n    # to the function.\n    with open(output_dataset_two_path, 'w') as f:\n        f.write(message)\n\n    with open(output_parameter_path, 'w') as f:\n        f.write(message)\n\n    with open(output_bool_parameter_path, 'w') as f:\n        f.write(\n            str(True)\n        )  # use either `str()` or `json.dumps()` for bool values.\n\n    import json\n    with open(output_dict_parameter_path, 'w') as f:\n        f.write(json.dumps({'A': 1, 'B': 2}))\n\n    with open(output_list_parameter_path, 'w') as f:\n        f.write(json.dumps(['a', 'b', 'c']))\n\n\n@component(kfp_package_path=_KFP_PACKAGE_PATH)\ndef train(\n    # Use InputPath to get a locally accessible path for the input artifact\n    # of type `Dataset`.\n    dataset_one_path: InputPath('Dataset'),\n    # Use Input[T] to get a metadata-rich handle to the input artifact\n    # of type `Dataset`.\n    dataset_two: Input[Dataset],\n    # An input parameter of type string.\n    message: str,\n    # Use Output[T] to get a metadata-rich handle to the output artifact\n    # of type `Dataset`.\n    model: Output[Model],\n    # An input parameter of type bool.\n    input_bool: bool,\n    # An input parameter of type dict.\n    input_dict: Dict[str, int],\n    # An input parameter of type List[str].\n    input_list: List[str],\n    # An input parameter of type int with a default value.\n    num_steps: int = 100,\n):\n    \"\"\"Dummy Training step\"\"\"\n    with open(dataset_one_path, 'r') as input_file:\n        dataset_one_contents = input_file.read()\n\n    with open(dataset_two.path, 'r') as input_file:\n        dataset_two_contents = input_file.read()\n\n    line = (\n        f'dataset_one_contents: {dataset_one_contents} || '\n        f'dataset_two_contents: {dataset_two_contents} || '\n        f'message: {message} || '\n        f'input_bool: {input_bool}, type {type(input_bool)} || '\n        f'input_dict: {input_dict}, type {type(input_dict)} || '\n        f'input_list: {input_list}, type {type(input_list)} \\n'\n    )\n\n    with open(model.path, 'w') as output_file:\n        for i in range(num_steps):\n            output_file.write('Step {}\\n{}\\n=====\\n'.format(i, line))\n\n    # Use model.get() to get a Model artifact, which has a .metadata dictionary\n    # to store arbitrary metadata for the output artifact.\n    model.metadata['accuracy'] = 0.9\n\n\n@dsl.pipeline(name='my-test-pipeline-beta')\ndef pipeline(message: str = 'message'):\n    preprocess_task = preprocess(message=message)\n    train_task = train(\n        dataset_one_path=preprocess_task.outputs['output_dataset_one'],\n        dataset_two=preprocess_task.outputs['output_dataset_two_path'],\n        message=preprocess_task.outputs['output_parameter_path'],\n        input_bool=preprocess_task.outputs['output_bool_parameter_path'],\n        input_dict=preprocess_task.outputs['output_dict_parameter_path'],\n        input_list=preprocess_task.outputs['output_list_parameter_path'],\n    )\n\n\nif __name__ == '__main__':\n    compiler.Compiler().compile(\n        pipeline_func=pipeline, package_path=__file__.replace('.py', '.json')\n    )\n"
  },
  {
    "repo": "SWE-Gym-Raw/kubeflow__pipelines",
    "file_path": "samples/test/lightweight_python_functions_v2_pipeline_test.py",
    "raw_url": "https://raw.githubusercontent.com/SWE-Gym-Raw/kubeflow__pipelines/0.1.8/samples/test/lightweight_python_functions_v2_pipeline_test.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nfrom pprint import pprint\nimport kfp_server_api\nimport kfp.dsl as dsl\n\nfrom .lightweight_python_functions_v2_pipeline import pipeline\nfrom .util import run_pipeline_func, TestCase, KfpMlmdClient\nfrom ml_metadata.proto import Execution\n\n\ndef verify(run: kfp_server_api.ApiRun, mlmd_connection_config, **kwargs):\n    t = unittest.TestCase()\n    t.maxDiff = None  # we always want to see full diff\n    t.assertEqual(run.status, 'Succeeded')\n    client = KfpMlmdClient(mlmd_connection_config=mlmd_connection_config)\n    tasks = client.get_tasks(run_id=run.id)\n\n    task_names = [*tasks.keys()]\n    t.assertCountEqual(task_names, ['preprocess', 'train'], 'task names')\n    pprint(tasks)\n\n    preprocess = tasks['preprocess']\n    train = tasks['train']\n    pprint(preprocess.get_dict())\n    t.assertEqual(\n        {\n            'inputs': {\n                'artifacts': [],\n                'parameters': {\n                    'message': 'message',\n                    'empty_message': '',\n                }\n            },\n            'name': 'preprocess',\n            'outputs': {\n                'artifacts': [{\n                    'metadata': {\n                        'display_name': 'output_dataset_one'\n                    },\n                    'name': 'output_dataset_one',\n                    'type': 'system.Dataset'\n                }, {\n                    'metadata': {\n                        'display_name': 'output_dataset_two_path'\n                    },\n                    'name': 'output_dataset_two_path',\n                    'type': 'system.Dataset'\n                }],\n                'parameters': {\n                    'output_bool_parameter_path': 'True',\n                    'output_dict_parameter_path': '{\"A\": 1, \"B\": 2}',\n                    'output_list_parameter_path': '[\"a\", \"b\", \"c\"]',\n                    'output_parameter_path': 'message'\n                }\n            },\n            'type': 'system.ContainerExecution',\n            'state': Execution.State.COMPLETE,\n        },\n        preprocess.get_dict(),\n    )\n    t.assertEqual(\n        {\n            'inputs': {\n                'artifacts': [{\n                    'metadata': {\n                        'display_name': 'output_dataset_one'\n                    },\n                    'name': 'dataset_one_path',\n                    'type': 'system.Dataset'\n                }, {\n                    'metadata': {\n                        'display_name': 'output_dataset_two_path'\n                    },\n                    'name': 'dataset_two',\n                    'type': 'system.Dataset'\n                }],\n                'parameters': {\n                    'input_bool': 'True',\n                    'input_dict': '{\"A\": 1, \"B\": 2}',\n                    'input_list': '[\"a\", \"b\", \"c\"]',\n                    'message': 'message',\n                    'num_steps': 100,\n                }\n            },\n            'name': 'train',\n            'outputs': {\n                'artifacts': [{\n                    'metadata': {\n                        'display_name': 'model',\n                        'accuracy': 0.9,\n                    },\n                    'name': 'model',\n                    'type': 'system.Model'\n                }],\n                'parameters': {}\n            },\n            'type': 'system.ContainerExecution',\n            'state': Execution.State.COMPLETE,\n        },\n        train.get_dict(),\n    )\n\n\nrun_pipeline_func([\n    TestCase(\n        pipeline_func=pipeline,\n        verify_func=verify,\n        mode=dsl.PipelineExecutionMode.V2_COMPATIBLE\n    ),\n    TestCase(\n        pipeline_func=pipeline,\n        verify_func=verify,\n        mode=dsl.PipelineExecutionMode.V2_ENGINE\n    ),\n])\n"
  },
  {
    "repo": "SWE-Gym-Raw/kubeflow__pipelines",
    "file_path": "samples/test/lightweight_python_functions_v2_with_outputs.py",
    "raw_url": "https://raw.githubusercontent.com/SWE-Gym-Raw/kubeflow__pipelines/0.1.8/samples/test/lightweight_python_functions_v2_with_outputs.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Lightweight functions v2 with outputs.\"\"\"\nfrom typing import NamedTuple\nimport os\nfrom kfp import components, dsl\nfrom kfp.v2 import compiler\nfrom kfp.v2.dsl import Input, Dataset, Model, Metrics, component\n\n# In tests, we install a KFP package from the PR under test. Users should not\n# normally need to specify `kfp_package_path` in their component definitions.\n_KFP_PACKAGE_PATH = os.getenv('KFP_PACKAGE_PATH')\n\n\n@component(kfp_package_path=_KFP_PACKAGE_PATH)\ndef concat_message(first: str, second: str) -> str:\n    return first + second\n\n\n@component(kfp_package_path=_KFP_PACKAGE_PATH)\ndef add_numbers(first: int, second: int) -> int:\n    return first + second\n\n\n@component(kfp_package_path=_KFP_PACKAGE_PATH)\ndef output_artifact(number: int, message: str) -> Dataset:\n    result = [message for _ in range(number)]\n    return '\\n'.join(result)\n\n\n@component(kfp_package_path=_KFP_PACKAGE_PATH)\ndef output_named_tuple(\n    artifact: Input[Dataset]\n) -> NamedTuple(\n        'Outputs', [\n            ('scalar', str),\n            ('metrics', Metrics),\n            ('model', Model),\n        ]\n):\n    scalar = \"123\"\n\n    import json\n    metrics = json.dumps({\n        'metrics': [{\n            'name': 'accuracy',\n            'numberValue': 0.9,\n            'format': \"PERCENTAGE\",\n        }]\n    })\n\n    with open(artifact.path, 'r') as f:\n        artifact_contents = f.read()\n    model = \"Model contents: \" + artifact_contents\n\n    from collections import namedtuple\n    output = namedtuple('Outputs', ['scalar', 'metrics', 'model'])\n    return output(scalar, metrics, model)\n\n\n@dsl.pipeline(name='functions-with-outputs')\ndef pipeline(\n    first_message: str = 'first',\n    second_message: str = 'second',\n    first_number: int = 1,\n    second_number: int = 2,\n):\n    concat_task = concat_message(first=first_message, second=second_message)\n    add_numbers_task = add_numbers(first=first_number, second=second_number)\n    output_artifact_task = output_artifact(\n        number=add_numbers_task.output, message=concat_task.output\n    )\n    output_name_tuple = output_named_tuple(output_artifact_task.output)\n\n\nif __name__ == '__main__':\n    compiler.Compiler().compile(\n        pipeline_func=pipeline,\n        pipeline_root='dummy_root',\n        output_path=__file__ + '.json'\n    )\n"
  },
  {
    "repo": "SWE-Gym-Raw/kubeflow__pipelines",
    "file_path": "samples/test/lightweight_python_functions_v2_with_outputs_test.py",
    "raw_url": "https://raw.githubusercontent.com/SWE-Gym-Raw/kubeflow__pipelines/0.1.8/samples/test/lightweight_python_functions_v2_with_outputs_test.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom pprint import pprint\nimport unittest\nimport kfp\nimport kfp_server_api\nimport os\nfrom minio import Minio\n\nfrom .lightweight_python_functions_v2_with_outputs import pipeline\nfrom .util import KfpMlmdClient, run_pipeline_func, TestCase\n\n\ndef verify(run: kfp_server_api.ApiRun, mlmd_connection_config, **kwargs):\n    t = unittest.TestCase()\n    t.maxDiff = None  # we always want to see full diff\n    t.assertEqual(run.status, 'Succeeded')\n    client = KfpMlmdClient(mlmd_connection_config=mlmd_connection_config)\n    tasks = client.get_tasks(run_id=run.id)\n    pprint(tasks)\n\n    output_artifact = tasks['output-artifact']\n    output = [\n        a for a in output_artifact.outputs.artifacts if a.name == 'Output'\n    ][0]\n    pprint(output)\n\n    host = os.environ['MINIO_SERVICE_SERVICE_HOST']\n    port = os.environ['MINIO_SERVICE_SERVICE_PORT']\n    minio = Minio(\n        f'{host}:{port}',\n        access_key='minio',\n        secret_key='minio123',\n        secure=False\n    )\n    bucket, key = output.uri[len('minio://'):].split('/', 1)\n    print(f'bucket={bucket} key={key}')\n    response = minio.get_object(bucket, key)\n    data = response.read().decode('UTF-8')\n    t.assertEqual(data, 'firstsecond\\nfirstsecond\\nfirstsecond')\n\n\nrun_pipeline_func([\n    TestCase(\n        pipeline_func=pipeline,\n        mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE\n    ),\n    TestCase(\n        pipeline_func=pipeline,\n        mode=kfp.dsl.PipelineExecutionMode.V2_ENGINE,\n    ),\n    # Verify overriding pipeline root to MinIO\n    TestCase(\n        pipeline_func=pipeline,\n        verify_func=verify,\n        mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE,\n        arguments={\n            kfp.dsl.ROOT_PARAMETER_NAME: 'minio://mlpipeline/override/artifacts'\n        },\n    ),\n])\n"
  },
  {
    "repo": "SWE-Gym-Raw/kubeflow__pipelines",
    "file_path": "samples/test/metrics_visualization_v1.py",
    "raw_url": "https://raw.githubusercontent.com/SWE-Gym-Raw/kubeflow__pipelines/0.1.8/samples/test/metrics_visualization_v1.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp.dsl as dsl\nfrom ..core.visualization.confusion_matrix import confusion_visualization\nfrom ..core.visualization.html import html_visualization\nfrom ..core.visualization.markdown import markdown_visualization\nfrom ..core.visualization.roc import roc_visualization\nfrom ..core.visualization.table import table_visualization\n\n# Note: This test is to verify that visualization metrics on V1 is runnable by KFP.\n# However, this pipeline is only runnable on V1 mode, but not V2 compatible mode.\n\n@dsl.pipeline(\n    name='metrics-visualization-v1-pipeline')\ndef metrics_visualization_v1_pipeline():\n    confusion_visualization_task = confusion_visualization()\n    html_visualization_task = html_visualization(\"\")\n    markdown_visualization_task = markdown_visualization()\n    roc_visualization_task = roc_visualization()\n    table_visualization_task = table_visualization()\n"
  },
  {
    "repo": "SWE-Gym-Raw/kubeflow__pipelines",
    "file_path": "samples/test/metrics_visualization_v1_test.py",
    "raw_url": "https://raw.githubusercontent.com/SWE-Gym-Raw/kubeflow__pipelines/0.1.8/samples/test/metrics_visualization_v1_test.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp_server_api\nimport unittest\nfrom pprint import pprint\nfrom .metrics_visualization_v1 import metrics_visualization_v1_pipeline\nfrom .util import KfpMlmdClient, run_pipeline_func, TestCase\n\nimport kfp\nimport kfp_server_api\n\n\ndef verify(\n    run: kfp_server_api.ApiRun, mlmd_connection_config, argo_workflow_name: str,\n    **kwargs\n):\n    t = unittest.TestCase()\n    t.maxDiff = None  # we always want to see full diff\n    t.assertEqual(run.status, 'Succeeded')\n    client = KfpMlmdClient(mlmd_connection_config=mlmd_connection_config)\n    tasks = client.get_tasks(argo_workflow_name=argo_workflow_name)\n    pprint(tasks)\n\n    table_visualization = tasks['table-visualization']\n    markdown_visualization = tasks['markdown-visualization']\n    roc_visualization = tasks['roc-visualization']\n    html_visualization = tasks['html-visualization']\n    confusion_visualization = tasks['confusion-visualization']\n\n    t.assertEqual(table_visualization.get_dict()['outputs']['artifacts'][0]['name'],\n                  'mlpipeline_ui_metadata')\n    t.assertEqual(markdown_visualization.get_dict()['outputs']['artifacts'][0]['name'],\n                  'mlpipeline_ui_metadata')\n    t.assertEqual(roc_visualization.get_dict()['outputs']['artifacts'][0]['name'],\n                  'mlpipeline_ui_metadata')\n    t.assertEqual(html_visualization.get_dict()['outputs']['artifacts'][0]['name'],\n                  'mlpipeline_ui_metadata')\n    t.assertEqual(confusion_visualization.get_dict()['outputs']['artifacts'][0]['name'],\n                  'mlpipeline_ui_metadata')\n\n\nrun_pipeline_func([TestCase(pipeline_func=metrics_visualization_v1_pipeline,\n                            mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE\n                            )])\n"
  },
  {
    "repo": "SWE-Gym-Raw/kubeflow__pipelines",
    "file_path": "samples/test/metrics_visualization_v2.py",
    "raw_url": "https://raw.githubusercontent.com/SWE-Gym-Raw/kubeflow__pipelines/0.1.8/samples/test/metrics_visualization_v2.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport os\n\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import (\n    component,\n    Output,\n    ClassificationMetrics,\n    Metrics,\n    HTML,\n    Markdown\n)\n\n# In tests, we install a KFP package from the PR under test. Users should not\n# normally need to specify `kfp_package_path` in their component definitions.\n_KFP_PACKAGE_PATH = os.getenv('KFP_PACKAGE_PATH')\n\n\n@component(\n    packages_to_install=['sklearn'],\n    base_image='python:3.9',\n    kfp_package_path=_KFP_PACKAGE_PATH,\n)\ndef digit_classification(metrics: Output[Metrics]):\n    from sklearn import model_selection\n    from sklearn.linear_model import LogisticRegression\n    from sklearn import datasets\n    from sklearn.metrics import accuracy_score\n\n    # Load digits dataset\n    iris = datasets.load_iris()\n\n    # # Create feature matrix\n    X = iris.data\n\n    # Create target vector\n    y = iris.target\n\n    #test size\n    test_size = 0.33\n\n    seed = 7\n    #cross-validation settings\n    kfold = model_selection.KFold(n_splits=10, random_state=seed, shuffle=True)\n\n    #Model instance\n    model = LogisticRegression()\n    scoring = 'accuracy'\n    results = model_selection.cross_val_score(model, X, y, cv=kfold, scoring=scoring)\n\n    #split data\n    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=test_size, random_state=seed)\n    #fit model\n    model.fit(X_train, y_train)\n\n    #accuracy on test set\n    result = model.score(X_test, y_test)\n    metrics.log_metric('accuracy', (result*100.0))\n\n@component(\n    packages_to_install=['sklearn'],\n    base_image='python:3.9',\n    kfp_package_path=_KFP_PACKAGE_PATH,\n)\ndef wine_classification(metrics: Output[ClassificationMetrics]):\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.metrics import roc_curve\n    from sklearn.datasets import load_wine\n    from sklearn.model_selection import train_test_split, cross_val_predict\n\n    X, y = load_wine(return_X_y=True)\n    # Binary classification problem for label 1.\n    y = y == 1\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n    rfc = RandomForestClassifier(n_estimators=10, random_state=42)\n    rfc.fit(X_train, y_train)\n    y_scores = cross_val_predict(rfc, X_train, y_train, cv=3, method='predict_proba')\n    y_predict = cross_val_predict(rfc, X_train, y_train, cv=3, method='predict')\n    fpr, tpr, thresholds = roc_curve(y_true=y_train, y_score=y_scores[:,1], pos_label=True)\n    metrics.log_roc_curve(fpr, tpr, thresholds)\n\n@component(\n    packages_to_install=['sklearn'],\n    base_image='python:3.9',\n    kfp_package_path=_KFP_PACKAGE_PATH,\n)\ndef iris_sgdclassifier(test_samples_fraction: float, metrics: Output[ClassificationMetrics]):\n    from sklearn import datasets, model_selection\n    from sklearn.linear_model import SGDClassifier\n    from sklearn.metrics import confusion_matrix\n\n    iris_dataset = datasets.load_iris()\n    train_x, test_x, train_y, test_y = model_selection.train_test_split(\n        iris_dataset['data'], iris_dataset['target'], test_size=test_samples_fraction)\n\n\n    classifier = SGDClassifier()\n    classifier.fit(train_x, train_y)\n    predictions = model_selection.cross_val_predict(classifier, train_x, train_y, cv=3)\n    metrics.log_confusion_matrix(\n        ['Setosa', 'Versicolour', 'Virginica'],\n        confusion_matrix(train_y, predictions).tolist() # .tolist() to convert np array to list.\n    )\n\n@component(\n    kfp_package_path=_KFP_PACKAGE_PATH,\n)\ndef html_visualization(html_artifact: Output[HTML]):\n    html_content = '<!DOCTYPE html><html><body><h1>Hello world</h1></body></html>'\n    with open(html_artifact.path, 'w') as f:\n        f.write(html_content)\n\n@component(\n    kfp_package_path=_KFP_PACKAGE_PATH,\n)\ndef markdown_visualization(markdown_artifact: Output[Markdown]):\n    markdown_content = '## Hello world \\n\\n Markdown content'\n    with open(markdown_artifact.path, 'w') as f:\n        f.write(markdown_content)\n\n@dsl.pipeline(\n    name='metrics-visualization-pipeline')\ndef metrics_visualization_pipeline():\n    wine_classification_op = wine_classification()\n    iris_sgdclassifier_op = iris_sgdclassifier(test_samples_fraction=0.3)\n    digit_classification_op = digit_classification()\n    html_visualization_op = html_visualization()\n    markdown_visualization_op = markdown_visualization()\n"
  },
  {
    "repo": "SWE-Gym-Raw/kubeflow__pipelines",
    "file_path": "samples/test/metrics_visualization_v2_test.py",
    "raw_url": "https://raw.githubusercontent.com/SWE-Gym-Raw/kubeflow__pipelines/0.1.8/samples/test/metrics_visualization_v2_test.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport unittest.mock as mock\nfrom pprint import pprint\nimport kfp\nimport kfp_server_api\n\nfrom .metrics_visualization_v2 import metrics_visualization_pipeline\nfrom .util import run_pipeline_func, TestCase, KfpMlmdClient\nfrom ml_metadata.proto import Execution\n\n\ndef verify(run: kfp_server_api.ApiRun, mlmd_connection_config, **kwargs):\n    t = unittest.TestCase()\n    t.maxDiff = None  # we always want to see full diff\n    t.assertEqual(run.status, 'Succeeded')\n    client = KfpMlmdClient(mlmd_connection_config=mlmd_connection_config)\n    tasks = client.get_tasks(run_id=run.id)\n\n    task_names = [*tasks.keys()]\n    t.assertCountEqual(\n        task_names,\n        ['wine-classification', 'iris-sgdclassifier', 'digit-classification', 'html-visualization', 'markdown-visualization'],\n        'task names'\n    )\n\n    wine_classification = tasks['wine-classification']\n    iris_sgdclassifier = tasks['iris-sgdclassifier']\n    digit_classification = tasks['digit-classification']\n    html_visualization = tasks['html-visualization']\n    markdown_visualization = tasks['markdown-visualization']\n\n    pprint('======= wine classification task =======')\n    pprint(wine_classification.get_dict())\n    pprint('======= iris sgdclassifier task =======')\n    pprint(iris_sgdclassifier.get_dict())\n    pprint('======= digit classification task =======')\n    pprint(digit_classification.get_dict())\n    pprint('======= HTML task =======')\n    pprint(html_visualization.get_dict())\n    pprint('======= Markdown task =======')\n    pprint(markdown_visualization.get_dict())\n\n    t.assertEqual({\n        'inputs': {\n            'artifacts': [],\n            'parameters': {}\n        },\n        'name': 'wine-classification',\n        'outputs': {\n            'artifacts': [{\n                'metadata': {\n                    'display_name': 'metrics',\n                    'confidenceMetrics': {\n                        'list': [{\n                            'confidenceThreshold': 2.0,\n                            'falsePositiveRate': 0.0,\n                            'recall': 0.0\n                        }, {\n                            'confidenceThreshold': 1.0,\n                            'falsePositiveRate': 0.0,\n                            'recall': 0.33962264150943394\n                        }, {\n                            'confidenceThreshold': 0.9,\n                            'falsePositiveRate': 0.0,\n                            'recall': 0.6037735849056604\n                        }, {\n                            'confidenceThreshold': 0.8,\n                            'falsePositiveRate': 0.0,\n                            'recall': 0.8490566037735849\n                        }, {\n                            'confidenceThreshold': 0.6,\n                            'falsePositiveRate': 0.0,\n                            'recall': 0.8867924528301887\n                        }, {\n                            'confidenceThreshold': 0.5,\n                            'falsePositiveRate': 0.0125,\n                            'recall': 0.9245283018867925\n                        }, {\n                            'confidenceThreshold': 0.4,\n                            'falsePositiveRate': 0.075,\n                            'recall': 0.9622641509433962\n                        }, {\n                            'confidenceThreshold': 0.3,\n                            'falsePositiveRate': 0.0875,\n                            'recall': 1.0\n                        }, {\n                            'confidenceThreshold': 0.2,\n                            'falsePositiveRate': 0.2375,\n                            'recall': 1.0\n                        }, {\n                            'confidenceThreshold': 0.1,\n                            'falsePositiveRate': 0.475,\n                            'recall': 1.0\n                        }, {\n                            'confidenceThreshold': 0.0,\n                            'falsePositiveRate': 1.0,\n                            'recall': 1.0\n                        }]\n                    }\n                },\n                'name': 'metrics',\n                'type': 'system.ClassificationMetrics'\n            }],\n            'parameters': {}\n        },\n        'type': 'system.ContainerExecution',\n        'state': Execution.State.COMPLETE,\n    }, wine_classification.get_dict())\n    t.assertEqual(\n        {\n            'inputs': {\n                'artifacts': [],\n                'parameters': {\n                    'test_samples_fraction': 0.3\n                }\n            },\n            'name': 'iris-sgdclassifier',\n            'outputs': {\n                'artifacts': [{\n                    'metadata': {\n                        'display_name': 'metrics',\n                        'confusionMatrix': {'struct': {\n                            'annotationSpecs': [{\n                                'displayName': 'Setosa'\n                            }, {\n                                'displayName': 'Versicolour'\n                            }, {\n                                'displayName': 'Virginica'\n                            }],\n                            'rows': [{ # these numbers can be random during execution\n                                'row': [mock.ANY, mock.ANY, mock.ANY]\n                            }, {\n                                'row': [mock.ANY, mock.ANY, mock.ANY]\n                            }, {\n                                'row': [mock.ANY, mock.ANY, mock.ANY]\n                            }]\n                        }}\n                    },\n                    'name': 'metrics',\n                    'type': 'system.ClassificationMetrics'\n                }],\n                'parameters': {}\n            },\n            'type': 'system.ContainerExecution',\n            'state': Execution.State.COMPLETE,\n        }, iris_sgdclassifier.get_dict()\n    )\n    rows = iris_sgdclassifier.get_dict()['outputs']['artifacts'][0]['metadata'][\n        'confusionMatrix']['struct']['rows']\n    for i, row in enumerate(rows):\n        for j, item in enumerate(row['row']):\n            t.assertIsInstance(\n                item, float,\n                f'value of confusion matrix row {i}, col {j} is not a number'\n            )\n\n    t.assertEqual({\n        'inputs': {\n            'artifacts': [],\n            'parameters': {}\n        },\n        'name': 'digit-classification',\n        'outputs': {\n            'artifacts': [{\n                'metadata': {\n                    'display_name': 'metrics',\n                    'accuracy': 92.0,\n                },\n                'name': 'metrics',\n                'type': 'system.Metrics'\n            }],\n            'parameters': {}\n        },\n        'type': 'system.ContainerExecution',\n        'state': Execution.State.COMPLETE,\n    }, digit_classification.get_dict())\n\n    t.assertEqual({\n        'inputs': {\n            'artifacts': [], \n            'parameters': {}\n        },\n        'name': 'html-visualization',\n        'outputs': {\n            'artifacts': [{\n                'metadata': {\n                    'display_name': 'html_artifact'\n                },\n                'name': 'html_artifact',\n                'type': 'system.HTML'\n            }],\n            'parameters': {}\n        },\n        'state': Execution.State.COMPLETE,\n        'type': 'system.ContainerExecution'\n    }, html_visualization.get_dict())\n\n    t.assertEqual({\n        'inputs': {\n            'artifacts': [], \n            'parameters': {}\n        },\n        'name': 'markdown-visualization',\n        'outputs': {\n            'artifacts': [{\n                'metadata': {\n                    'display_name': 'markdown_artifact'\n                },\n                'name': 'markdown_artifact',\n                'type': 'system.Markdown'\n            }],\n            'parameters': {}\n        },\n        'state': Execution.State.COMPLETE,\n        'type': 'system.ContainerExecution'\n    }, markdown_visualization.get_dict())\n\n\nrun_pipeline_func([\n    TestCase(\n        pipeline_func=metrics_visualization_pipeline,\n        verify_func=verify,\n        mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE\n    ),\n    TestCase(\n        pipeline_func=metrics_visualization_pipeline,\n        verify_func=verify,\n        mode=kfp.dsl.PipelineExecutionMode.V2_ENGINE\n    ),\n])\n"
  },
  {
    "repo": "SWE-Gym-Raw/kubeflow__pipelines",
    "file_path": "samples/test/parameter_with_format.py",
    "raw_url": "https://raw.githubusercontent.com/SWE-Gym-Raw/kubeflow__pipelines/0.1.8/samples/test/parameter_with_format.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom kfp import components\nfrom kfp import dsl\n\n\n@components.create_component_from_func\ndef print_op(name: str) -> str:\n    print(name)\n    return name\n\n\n@dsl.pipeline(name='pipeline-with-pipelineparam-containing-format')\ndef my_pipeline(name: str = 'KFP'):\n    print_task = print_op('Hello {}'.format(name))\n    print_op('{}, again.'.format(print_task.output))\n"
  },
  {
    "repo": "SWE-Gym-Raw/kubeflow__pipelines",
    "file_path": "samples/test/parameter_with_format_test.py",
    "raw_url": "https://raw.githubusercontent.com/SWE-Gym-Raw/kubeflow__pipelines/0.1.8/samples/test/parameter_with_format_test.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp\nfrom .parameter_with_format import my_pipeline\nfrom .util import run_pipeline_func, TestCase\n\n\ndef verify(run, run_id: str, **kwargs):\n    assert run.status == 'Succeeded'\n    # TODO(Bobgy): verify output\n\n\nrun_pipeline_func([\n    TestCase(\n        pipeline_func=my_pipeline,\n        verify_func=verify,\n    ),\n    TestCase(\n        pipeline_func=my_pipeline,\n        verify_func=verify,\n        mode=kfp.dsl.PipelineExecutionMode.V2_ENGINE,\n    ),\n    TestCase(\n        pipeline_func=my_pipeline,\n        mode=kfp.dsl.PipelineExecutionMode.V1_LEGACY,\n    ),\n])\n"
  },
  {
    "repo": "SWE-Gym-Raw/kubeflow__pipelines",
    "file_path": "samples/test/placeholder_concat.py",
    "raw_url": "https://raw.githubusercontent.com/SWE-Gym-Raw/kubeflow__pipelines/0.1.8/samples/test/placeholder_concat.py",
    "content": "# Copyright 2020 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom kfp import components\nfrom kfp.v2 import dsl\nimport kfp.v2.compiler as compiler\n\ncomponent_op = components.load_component_from_text(\"\"\"\nname: Component with concat placeholder\ninputs:\n- {name: input_one, type: String}\n- {name: input_two, type: String}\nimplementation:\n  container:\n    image: gcr.io/google-containers/busybox\n    command:\n    - sh\n    - -ec\n    args:\n    - echo \"$0\" > /tmp/test && [[ \"$0\" == 'one+two=three' ]]\n    - concat: [{inputValue: input_one}, '+', {inputValue: input_two}, '=three']\n\"\"\")\n\n\n@dsl.pipeline(name='one-step-pipeline-with-concat-placeholder')\ndef pipeline_with_concat_placeholder():\n  component = component_op(input_one='one', input_two='two')\n"
  },
  {
    "repo": "gbhasin0828/Kubeflow_Pipeline",
    "file_path": "kubeflow_model_for_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/gbhasin0828/Kubeflow_Pipeline/main/kubeflow_model_for_pipeline.py",
    "content": "# -*- coding: utf-8 -*-\n\"\"\"KubeFlow_Model_For_Pipeline.ipynb\n\nAutomatically generated by Colab.\n\nOriginal file is located at\n    https://colab.research.google.com/drive/19_0dWxpGnnSyPMuxnk1bn8u74vZiKQBN\n\"\"\"\n\n!pip install kfp --upgrade\n\nimport kfp.components as compile\nimport kfp.dsl as dsl\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport os\nfrom sklearn.linear_model import ElasticNet, LogisticRegression\n\ndef download_dataset(url: str, output_path: str) -> pd.DataFrame:\n    import pandas as pd\n    import os\n\n    # Create the directory if it doesn't exist\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n\n    # Download the dataset\n    dataset = pd.read_csv(url)\n\n    # Save the dataset locally\n    dataset.to_csv(output_path, index=False)\n    print(f\"Data saved at {output_path}\")\n\n    # Return the DataFrame\n    return dataset\n\ndef preprocess_dataset(df: pd.DataFrame):\n  import pandas as pd\n  df.columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']\n  return df\n\ndef train_test_split(df: pd.DataFrame, output_dir : str) -> dict:\n  import pandas as pd\n  import numpy as np\n  from sklearn.model_selection import train_test_split\n  import os\n\n  os.makedirs(output_dir, exist_ok = True)\n\n  target_column = 'class'\n\n  X = df.loc[:, df.columns != target_column]\n  y = df.loc[:, df.columns == target_column]\n\n  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=47)\n\n  X_train_path = f'{output_dir}/X_train.npy'\n  X_test_path = f'{output_dir}/X_test.npy'\n  y_train_path = f'{output_dir}/y_train.npy'\n  y_test_path = f'{output_dir}/y_test.npy'\n\n  np.save(X_train_path, X_train)\n  np.save(X_test_path, X_test)\n  np.save(y_train_path, y_train)\n  np.save(y_test_path, y_test)\n\n  return {\n      \"X_train\" : X_train_path,\n      \"X_test\" : X_test_path,\n      \"y_train\" : y_train_path,\n      \"y_test\" : y_test_path\n  }\n\ndef train_model(path : dict, model_output_path):\n  import numpy as np\n  import pickle\n  from sklearn.linear_model import LogisticRegression\n  import os\n\n  X_train = np.load(path['X_train'], allow_pickle = True)\n  y_train = np.load(path['y_train'], allow_pickle = True)\n\n  classifier = LogisticRegression(max_iter=500)\n  classifier.fit(X_train, y_train)\n\n  os.makedirs(os.path.dirname(model_output_path), exist_ok = True)\n  with open(model_output_path, 'wb') as f:\n    pickle.dump(classifier, f)\n  print(\"Model Training Completed\")\n\nif __name__ == '__main__':\n\n  dataset_url = 'https://raw.githubusercontent.com/gbhasin0828/Kubeflow_Files/main/iris.csv'\n\n  output_path = 'data/final_df.csv'\n  output_dir = 'data/splits'\n  model_output_path = 'data/model.pkl'\n\n  df = download_dataset(dataset_url, output_path)\n\n  processed_df  = preprocess_dataset(df)\n\n  dict_data = train_test_split(df_processed, output_dir)\n\n  train_model(data_dict, model_output_path)\n\n\"\"\"# **Kubeflow Pipeline Work Starts - Look at the file Pipeline_For_Model.py**\"\"\"\n\n\n\n\n\n\n\n"
  },
  {
    "repo": "gbhasin0828/Kubeflow_Pipeline",
    "file_path": "kubeflow_pipeline_intermediate.py",
    "raw_url": "https://raw.githubusercontent.com/gbhasin0828/Kubeflow_Pipeline/main/kubeflow_pipeline_intermediate.py",
    "content": "# -*- coding: utf-8 -*-\n\"\"\"KubeFlow_Pipeline.ipynb\n\nAutomatically generated by Colab.\n\nOriginal file is located at\n    https://colab.research.google.com/drive/19_0dWxpGnnSyPMuxnk1bn8u74vZiKQBN\n\"\"\"\n\npip install kfp --upgrade\n\nimport kfp\n\nimport kfp.components as compile\nimport kfp.dsl as dsl\n\nfrom kfp.v2.dsl import component\n\n@component(\n    base_image='python:3.12',\n    packages_to_install=['pandas', 'numpy', 'scikit-learn']\n)\n\ndef prepare_data(output_path: str):\n    # Save to the mounted volume\n    import pandas as pd\n    import os\n\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n    df = pd.read_csv(\"https://raw.githubusercontent.com/TripathiAshutosh/dataset/main/iris.csv\")\n    df.to_csv(output_path, index=False)\n    print(f\"Data saved at {output_path}\")\n\n@component(\n    base_image='python:3.12',\n    packages_to_install=['pandas', 'numpy', 'scikit-learn']\n)\n\ndef train_test_split(input_path: str, output_dir: str) -> dict:\n    import pandas as pd\n    import numpy as np\n    from sklearn.model_selection import train_test_split\n    import os\n\n    os.makedirs(output_dir, exist_ok=True)\n    final_data = pd.read_csv(input_path)\n\n    target_column = 'class'\n\n    X = final_data.loc[:, final_data.columns != target_column]\n    y = final_data.loc[:, final_data.columns == target_column]\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=47)\n\n    X_train_path = f'{output_dir}/X_train.npy'\n    X_test_path = f'{output_dir}/X_test.npy'\n    y_train_path = f'{output_dir}/y_train.npy'\n    y_test_path = f'{output_dir}/y_test.npy'\n\n    np.save(X_train_path, X_train)\n    np.save(X_test_path, X_test)\n    np.save(y_train_path, y_train)\n    np.save(y_test_path, y_test)\n\n    return {\n        \"X_train\": X_train_path,\n        \"X_test\": X_test_path,\n        \"y_train\": y_train_path,\n        \"y_test\": y_test_path\n    }\n\n@component(\n    base_image='python:3.12',\n    packages_to_install=['pandas', 'numpy', 'scikit-learn']\n)\n\n\ndef training_basic_classifier(X_train_path: str, y_train_path: str, model_output_path: str):\n    import numpy as np\n    import pickle\n    from sklearn.linear_model import LogisticRegression\n    import os\n\n    X_train = np.load(X_train_path, allow_pickle=True)\n    y_train = np.load(y_train_path, allow_pickle=True)\n\n    classifier = LogisticRegression(max_iter=500)\n    classifier.fit(X_train, y_train)\n\n    os.makedirs(os.path.dirname(model_output_path), exist_ok=True)\n    with open(model_output_path, 'wb') as f:\n        pickle.dump(classifier, f)\n    print(\"Model Training Completed\")\n\n@component(\n    base_image='python:3.12',\n    packages_to_install=['pandas', 'numpy', 'scikit-learn']\n)\n\ndef predict_on_test_data(model_path: str, X_test_path: str, y_pred_path: str):\n    import numpy as np\n    import pickle\n\n    with open(model_path, 'rb') as f:\n        model = pickle.load(f)\n\n    X_test = np.load(X_test_path, allow_pickle=True)\n    y_pred = model.predict(X_test)\n    np.save(y_pred_path, y_pred)\n    print(\"Predictions saved!\")\n\n\"\"\"# **Kubeflow Pipeline Work Starts**\"\"\"\n\nfrom kfp.v2.dsl import pipeline\nfrom kfp.v2 import compiler\n\n@pipeline(\n    name='prepare_data_pipeline',\n    description='A pipeline to prepare data and save it to a CSV file'\n)\n\ndef create_step_prepare_data(output_path: str = 'data/final_df.csv'):\n    # Step 1: Call the prepare_data component\n    prepare_data_op = prepare_data(output_path=output_path)\n\n# Compile the pipeline\ncompiler.Compiler().compile(\n    pipeline_func=create_step_prepare_data,\n    package_path='prepare_data_pipeline.yaml'\n)\n\n@pipeline(\n    name='train_test_split',\n    description='A pipeline to prepare data for training & testing'\n)\n\ndef create_step_train_test_split(input_path: str = 'data/final_df.csv', output_dir: str = 'data'):\n    \"\"\"\n    A pipeline step that splits the data into training and testing sets.\n\n    Args:\n        input_path (str): Path to the input dataset.\n        output_dir (str): Directory where the split data will be saved.\n    \"\"\"\n    # Call the train_test_split component\n    train_test_split_op = train_test_split(\n        input_path=input_path,\n        output_dir=output_dir\n    )\n\n# Compile the pipeline\ncompiler.Compiler().compile(\n    pipeline_func=create_step_train_test_split,\n    package_path='train_test_split_pipeline.yaml'\n)\n\nfrom kfp.v2.dsl import pipeline\n\n@pipeline(\n    name='training_basic_classifier',\n    description='A pipeline to prepare data classifier for training'\n)\ndef create_step_training_basic_classifier(X_train_path: str, y_train_path: str, model_output_path: str = 'data/model.pkl'):\n    \"\"\"\n    Pipeline step to train a basic classifier.\n\n    Args:\n        X_train_path (str): Path to the training features file.\n        y_train_path (str): Path to the training labels file.\n        model_output_path (str): Path to save the trained model.\n    \"\"\"\n    # Call the training_basic_classifier component\n    training_basic_classifier_op = training_basic_classifier(\n        X_train_path=X_train_path,\n        y_train_path=y_train_path,\n        model_output_path=model_output_path\n    )\n\n# Compile the pipeline\ncompiler.Compiler().compile(\n    pipeline_func=create_step_training_basic_classifier,\n    package_path='training_basic_classifier_pipeline.yaml'\n)\n\nfrom kfp.v2.dsl import pipeline\n\n@pipeline(\n    name='predict_on_test_data',\n    description='A pipeline to perform predictions'\n)\n\ndef create_step_predict_on_test_data(model_path: str, X_test_path: str, y_pred_path: str = 'data/y_pred.npy'):\n    \"\"\"\n    Pipeline step to perform predictions on test data.\n\n    Args:\n        model_path (str): Path to the trained model file.\n        X_test_path (str): Path to the test features file.\n        y_pred_path (str): Path to save the predictions.\n    \"\"\"\n    # Call the predict_on_test_data component\n    predict_on_test_data_op = predict_on_test_data(\n        model_path=model_path,\n        X_test_path=X_test_path,\n        y_pred_path=y_pred_path\n    )\n\n# Compile the pipeline\ncompiler.Compiler().compile(\n    pipeline_func=create_step_predict_on_test_data,\n    package_path='predict_on_test_data_pipeline.yaml'\n)\n\nfrom kfp.v2.dsl import pipeline, component\nimport os\n\n@pipeline(\n    name='iris-classifier-pipeline',\n    description='Pipeline for IRIS classification using a shared volume'\n)\ndef iris_classifier_pipeline(data_path: str = '/mnt/data/final_df.csv'):\n    \"\"\"\n    Defines the IRIS classification pipeline using a shared persistent volume.\n\n    Args:\n        data_path (str): Path to save the prepared dataset in the shared volume.\n    \"\"\"\n    # Step 1: Prepare Data\n    prepare_data_task = prepare_data(\n        output_path=data_path\n    ).set_volume_mounts([\n        kfp.dsl.VolumeMount(\n            name='iris-data-pvc',  # Name of the PVC created earlier\n            mount_path='/mnt/data'\n        )\n    ])\n\n    # Step 2: Train-Test Split\n    split_data_task = train_test_split(\n        input_path=data_path,\n        output_dir='/mnt/data/splits'\n    ).set_volume_mounts([\n        kfp.dsl.VolumeMount(\n            name='iris-data-pvc',\n            mount_path='/mnt/data'\n        )\n    ])\n\n    # Step 3: Train Classifier\n    train_model_task = training_basic_classifier(\n        X_train_path='/mnt/data/splits/X_train.npy',\n        y_train_path='/mnt/data/splits/y_train.npy',\n        model_output_path='/mnt/data/model.pkl'\n    ).set_volume_mounts([\n        kfp.dsl.VolumeMount(\n            name='iris-data-pvc',\n            mount_path='/mnt/data'\n        )\n    ])\n\n    # Step 4: Predict on Test Data\n    predict_task = predict_on_test_data(\n        model_path='/mnt/data/model.pkl',\n        X_test_path='/mnt/data/splits/X_test.npy',\n        y_pred_path='/mnt/data/y_pred.npy'\n    ).set_volume_mounts([\n        kfp.dsl.VolumeMount(\n            name='iris-data-pvc',\n            mount_path='/mnt/data'\n        )\n    ])"
  },
  {
    "repo": "gbhasin0828/Kubeflow_Pipeline",
    "file_path": "pipeline_for_model.py",
    "raw_url": "https://raw.githubusercontent.com/gbhasin0828/Kubeflow_Pipeline/main/pipeline_for_model.py",
    "content": "# -*- coding: utf-8 -*-\n\"\"\"Pipeline_For_Model.ipynb\n\nAutomatically generated by Colab.\n\nOriginal file is located at\n    https://colab.research.google.com/drive/1QEyX82Jm4xwSVGeY5G0Tcyb-qvxVUSU8\n\"\"\"\n\n!pip install kfp --upgrade\n!pip install Docker\n\nfrom kfp import local, dsl\nimport kfp.components as compile\nimport kfp.dsl as dsl\n\nimport pandas as pd\nimport os\nimport requests\n\nlocal.init(local.DockerRunner())\n\n@dsl.component(\n    base_image = 'python:3.12',\n    output_component_file='download_dataset_component.yaml')\ndef download_dataset(url: str, dataset: dsl.Output[dsl.Dataset]) :\n    import pandas as pd\n    import os\n\n    # Create the directory if it doesn't exist\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n\n    # Download the dataset\n    df = pd.read_csv(url)\n\n    # Save the dataset locally\n    df.to_csv(dataset.path, index=False)\n    print(f\"Data saved at {dataset.path}\")\n\n    # Return the DataFrame\n    return dataset.path\n\n@dsl.component(base_image = 'python:3.12', output_component_file = 'preprocess_dataset_component.yaml')\ndef preprocess_dataset(dataset: dsl.Input[dsl.Dataset], preprocessed_dataset: dsl.Output[dsl.Dataset]):\n  import pandas as pd\n  dataset.columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']\n  dataset.to_csv(preprocessed_dataset.path, index = False)\n  return preprocessed_dataset.path\n\n@dsl.component(\n    base_image = 'python:3.12',\n    output_component_file = 'train_test_split.yaml')\ndef train_test_split(\n    dataframe: dsl.Input[dsl.Dataset],\n    X_train: dsl.Output[dsl.Dataset],\n    X_test : dsl.Output[dsl.Dataset],\n    y_train: dsl.Output[dsl.Dataset],\n    y_test: dsl.Output[dsl.Dataset]\n    ):\n  import pandas as pd\n  import numpy as np\n  from sklearn.model_selection import train_test_split\n  import os\n\n  df = pd.read_csv(dataframe.path)\n\n\n  target_column = 'class'\n\n  X = df.loc[:, df.columns != target_column]\n  y = df.loc[:, df.columns == target_column]\n\n  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=47)\n\n  np.save(X_train.path, X_train)\n  np.save(X_test.path, X_test)\n  np.save(y_train.path, y_train)\n  np.save(y_test.path, y_test)\n\n@dsl.component(\n    base_image = 'python: 3.12',\n    output_component_file= 'train_model.yaml'\n)\ndef train_model(\n    X_train: dsl.Input[dsl.Dataset],\n    y_train: dsl.Input[dsl.Dataset],\n    model: dsl.Output[dsl.Model]\n):\n  import numpy as np\n  import pickle\n  from sklearn.linear_model import LogisticRegression\n  import os\n\n  X_train = np.load(path['X_train'], allow_pickle = True)\n  y_train = np.load(path['y_train'], allow_pickle = True)\n\n  classifier = LogisticRegression(max_iter=500)\n  classifier.fit(X_train, y_train)\n\n  os.makedirs(os.path.dirname(model_output_path), exist_ok = True)\n  with open(model.path, 'wb') as f:\n    pickle.dump(classifier, f)\n  print(\"Model Training Completed\")\n\n@dsl.pipeline\ndef mlops_pipeline(url : str, output_path: str):\n  download_dataset_task = download_dataset(url = url)\n  preprocess_dataset_task = preprocess_dataset(dataset = download_dataset_task.outputs[\"dataset\"])\n  train_test_split_task = train_test_split(dataframe = preprocess_dataset_task.outputs['preprocessed_dataset'] )\n  train_model_task = train_model(X_train = train_test_split_task.outputs['X_train'],\n                                 y_train = train_test_split_task.outputs['y_train'])\n\nif __name__ == '__main__':\n  dataset_url = 'https://raw.githubusercontent.com/gbhasin0828/Kubeflow_Files/main/iris.csv'\n  output_path = 'data/final_df.csv'\n  model_output_path = 'data/model.pkl'\n\n# Compile the pipeline\nfrom kfp.v2 import compiler\n\npipeline_file = 'mlops_complete_pipeline_file.yaml'\n\ncompiler.Compiler().compile(\n    pipeline_func = mlops_pipeline,\n    package_path= pipeline_file\n)"
  },
  {
    "repo": "Jabor047/Kubeflow-Pipelines",
    "file_path": "lightweight_pipeline/telco_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/Jabor047/Kubeflow-Pipelines/main/lightweight_pipeline/telco_pipeline.py",
    "content": "import kfp\nfrom kfp import dsl\nfrom typing import NamedTuple\n\ndef download_data() -> str:\n    import pandas as pd\n    from requests import get\n    import io\n\n    url = \"https://storage.googleapis.com/<GCS Bucket>/data/telco.csv\"\n    s = get(url).content\n    df = pd.read_csv(io.StringIO(s.decode(\"utf-8\")))\n\n    data_path = \"/data/telco.csv\"\n    df.to_csv(data_path)\n\n    return data_path\n\n\ndef clean_data(input_path: str) -> str:\n    import pandas as pd\n\n    df = pd.read_csv(input_path)\n    df_fill_nan = df.drop([\"Bearer Id\", \"IMSI\", \"MSISDN/Number\", \"IMEI\", \"Start\", \"End\", \"Last Location Name\",\n                           \"Handset Manufacturer\", \"Handset Type\"], axis=1)\n\n    # fill all the null values with the mean of the column they occur in\n    df_fill_nan = df_fill_nan.apply(lambda x: x.fillna(x.median()), axis=1)\n\n    # replace the columns where values have been filled in\n    for col in df_fill_nan.columns.to_list():\n        df[col] = df_fill_nan[col]\n\n    clean_data_path = \"/data/clean_telco.csv\"\n    df.to_csv(clean_data_path)\n\n    return clean_data_path\n\ndef feature_prep(input_path: str) -> NamedTuple(\"feature_paths\", [(\"exp_path\", str), (\"eng_path\", str)]):\n    import pandas as pd\n\n    # this sums up all the values in a dataframe col\n    def sum_agg(dataframe: pd.DataFrame, col: str) -> pd.DataFrame:\n        s = dataframe.groupby(\"MSISDN/Number\")[col].agg(\"sum\").sort_values(ascending=False)\n        df = pd.DataFrame({\"MSISDN/Number\": s.index, col: s.values})\n\n        return df\n\n    df = pd.read_csv(input_path)\n\n    # combine both the download and upload data cols in bytes\n    df[\"Total traffic\"] = df[\"Total DL (Bytes)\"] + df[\"Total UL (Bytes)\"]\n\n    # get total duration and traffic for each use and merhe them into one dataframe\n    df_dur_ms = sum_agg(df, \"Dur. (ms)\")\n    df_total_bytes = sum_agg(df, \"Total traffic\")\n    df_engagement = pd.merge(df_dur_ms, df_total_bytes, on=\"MSISDN/Number\")\n\n    # get the number of sessions each user has\n    session_series = df.groupby(\"MSISDN/Number\")[\"Dur. (ms)\"].count().sort_values(ascending=False)\n    df_sess_freq = pd.DataFrame({\"MSISDN/Number\": session_series.index, \"sessions freq\": session_series.values})\n\n    # merge the engagement dataframe and sessions frequency dataframe\n    df_engagement = pd.merge(df_engagement, df_sess_freq, on=\"MSISDN/Number\")\n\n    # Summing the Uploads and Downloads columns to get the Total data columns\n    df[\"Avg RTT\"] = df[\"Avg RTT DL (ms)\"] + df[\"Avg RTT UL (ms)\"]\n    df[\"Avg Bearer TP (kbps)\"] = df[\"Avg Bearer TP DL (kbps)\"] + df[\"Avg Bearer TP UL (kbps)\"]\n    df[\"TCP Retrans. Vol (Bytes)\"] = df[\"TCP DL Retrans. Vol (Bytes)\"] + df[\"TCP UL Retrans. Vol (Bytes)\"]\n\n    # select the required columns for experience analysis\n    df_experience = df[[\"MSISDN/Number\", \"Avg RTT\", \"Avg Bearer TP (kbps)\", \"TCP Retrans. Vol (Bytes)\"]]\n\n    df_exp_path = \"/data/experience.csv\"\n    df_eng_path = \"/data/engagement.csv\"\n\n    df_engagement.to_csv(df_eng_path)\n    df_experience.to_csv(df_exp_path)\n\n    # convert the feature paths to a named tuple\n    from collections import namedtuple\n    feature_paths = namedtuple(\"feature_paths\", [\"exp_path\", \"eng_path\"])\n    return feature_paths(df_eng_path, df_eng_path)\n\ndef find_eng_and_exp_score(exp_path: str, eng_path: str) -> str:\n    import joblib\n    import json\n    import pandas as pd\n    import numpy as np\n    from sklearn.preprocessing import normalize\n    from sklearn.cluster import KMeans\n    from google.cloud import storage\n    from requests import get\n\n    url = \"https://storage.googleapis.com/<GCS Bucket>/kubeflow-tutorials-340813-d338387fc0f4.json\"\n    s = get(url).content\n\n    serv_acc_json = json.loads(s)\n    with open(\"/data/service_account.json\", 'w') as f:\n        json.dump(serv_acc_json, f)\n\n    storage_client = storage.Client.from_service_account_json(\"/data/service_account.json\")\n    bucket = storage_client.bucket(\"<GCS Bucket>\")\n    eng_blob = bucket.blob(\"models/sklearn/engagement/001/model.pkl\")\n    exp_blob = bucket.blob(\"models/sklearn/experience/001/model.pkl\")\n\n    df_experience = pd.read_csv(exp_path)\n    df_engagement = pd.read_csv(eng_path)\n\n    # normalize the columns to be entered in to the clustering algo\n    def normalize_df(df: pd.DataFrame) -> pd.DataFrame:\n        df.drop(\"MSISDN/Number\", axis=1, inplace=True)\n        for col in df.columns.to_list():\n            df[col] = np.transpose(normalize([np.array(df[col])]))\n\n        return df\n\n    df_experience_norm = normalize_df(df_experience.copy())\n    df_engagement_norm = normalize_df(df_engagement.copy())\n\n    # cluster the engagement and experience into 3 clusters each\n    engagement_kmeans = KMeans(n_clusters=3, random_state=42).fit(df_engagement_norm)\n    experience_kmeans = KMeans(n_clusters=3, random_state=42).fit(df_experience_norm)\n\n    joblib.dump(engagement_kmeans, \"/data/engagement_kmeans.pkl\")\n    joblib.dump(experience_kmeans, \"/data/experience_kmeans.pkl\")\n    eng_blob.upload_from_filename(\"/data/engagement_kmeans.pkl\")\n    exp_blob.upload_from_filename(\"/data/experience_kmeans.pkl\")\n\n    least_eng_cluster = engagement_kmeans.cluster_centers_[0]\n    least_exp_cluster = experience_kmeans.cluster_centers_[0]\n\n    # finding the eculidean distance between each data point and the least engaged cluster\n    engagement_score = []\n    for row in df_engagement_norm.to_numpy():\n        eng_score = np.linalg.norm(row - least_eng_cluster)\n        engagement_score.append(eng_score)\n\n    # finding the eculidean distance between each data point and the least experience cluster\n    experience_score = []\n    for row in df_experience_norm.to_numpy():\n        exp_score = np.linalg.norm(row - least_exp_cluster)\n        experience_score.append(exp_score)\n\n    # add the engagement and experience scores to normalized dataframes\n    df_engagement_norm[\"Engagement Score\"] = np.transpose(np.array(engagement_score))\n    df_experience_norm[\"Experience Score\"] = np.transpose(np.array(experience_score))\n\n    # add the unique identifier for merging\n    df_experience_norm[\"MSISDN/Number\"] = df_experience[\"MSISDN/Number\"]\n    df_engagement_norm[\"MSISDN/Number\"] = df_engagement[\"MSISDN/Number\"]\n\n    # merging the two dataframes\n    df = pd.merge(df_engagement_norm, df_experience_norm, on=\"MSISDN/Number\")\n\n    sat_data_path = \"/data/satisfication.csv\"\n    df.to_csv(sat_data_path)\n\n    return sat_data_path\n\ndef find_satisfaction(input_path: str):\n    import joblib\n    import json\n    import pandas as pd\n    import numpy as np\n    from sklearn.cluster import KMeans\n    from google.cloud import storage\n    from requests import get\n\n    url = \"https://storage.googleapis.com/<GCS Bucket>/kubeflow-tutorials-340813-d338387fc0f4.json\"\n    s = get(url).content\n\n    serv_acc_json = json.loads(s)\n    with open(\"/data/service_account.json\", 'w') as f:\n        json.dump(serv_acc_json, f)\n\n    storage_client = storage.Client.from_service_account_json(\"/data/service_account.json\")\n    bucket = storage_client.bucket(\"<GCS Bucket>\")\n    blob = bucket.blob(\"models/sklearn/satisfaction/001/model.pkl\")\n\n    df = pd.read_csv(input_path)\n    satisfaction_kmeans = KMeans(n_clusters=2, random_state=42).fit(df[[\"Engagement Score\", \"Experience Score\"]])\n    df[\"Satisfaction\"] = np.transpose(satisfaction_kmeans.labels_)\n    joblib.dump(satisfaction_kmeans, \"/data/satisfaction_kmeans.pkl\")\n    blob.upload_from_filename(\"/data/satisfaction_kmeans.pkl\")\n\n    print(df[[\"MSISDN/Number\", \"Satisfaction\"]])\n\n@dsl.pipeline(name=\"telco_pipeline\",\n              description=\"lightweight component Telco pipeline for the presentation\")\ndef telco_pipeline():\n\n    data_op = dsl.VolumeOp(name=\"create-pvc\",\n                           resource_name=\"data-volume\",\n                           size=\"2Gi\",\n                           modes=dsl.VOLUME_MODE_RWO)\n\n    download_data_op = kfp.components.func_to_container_op(download_data,\n                                                           packages_to_install=[\"pandas\", \"requests\"])\n    clean_data_op = kfp.components.func_to_container_op(clean_data, packages_to_install=[\"pandas\"])\n    feature_prep_op = kfp.components.func_to_container_op(feature_prep, packages_to_install=[\"pandas\"])\n    find_eng_and_exp_score_op = kfp.components.func_to_container_op(find_eng_and_exp_score,\n                                                                    packages_to_install=[\"pandas\", \"numpy\",\n                                                                                         \"scikit-learn\",\n                                                                                         \"google-cloud-storage\",\n                                                                                         \"requests\"])\n    find_satisfaction_op = kfp.components.func_to_container_op(find_satisfaction,\n                                                               packages_to_install=[\"pandas\", \"numpy\",\n                                                                                    \"scikit-learn\",\n                                                                                    \"google-cloud-storage\",\n                                                                                    \"requests\"])\n\n    step1 = download_data_op().add_pvolumes({\"/data\": data_op.volume})\n    step2 = clean_data_op(step1.output).add_pvolumes({\"/data\": data_op.volume})\n    step3 = feature_prep_op(step2.output).add_pvolumes({\"/data\": data_op.volume})\n    step4 = find_eng_and_exp_score_op(step3.outputs[\"exp_path\"], step3.outputs[\"eng_path\"])\\\n        .add_pvolumes({\"/data\": data_op.volume})\n    step5 = find_satisfaction_op(step4.output).add_pvolumes({\"/data\": data_op.volume})\n\n    kf_serve = kfp.components.load_component_from_url(\"https://raw.githubusercontent.com/kubeflow/pipelines/master/comp\"\n                                                      \"onents/kubeflow/kfserving/component.yaml\")\n    kf_serve_op = kf_serve(\n        action=\"apply\",\n        model_uri=\"gs://<GCS Bucket>/models/sklearn/satisfaction\",\n        model_name=\"satisfactionkmeans\",\n        namespace=\"gkkarobia\",\n        framework=\"sklearn\",\n        watch_timeout=\"300\"\n    )\n    kf_serve_op.after(step5)\n\nkfp.compiler.Compiler().compile(telco_pipeline, \"telco_pipeline.zip\")\n\nkubeflow_gateway_endpoint = \"localhost:7777\"\nauthservice_session_cookie = \"MTY1Mjg2MTY4MnxOd3dBTkZnM1dVdFVRMFEwTkVWR1dVWlZWbEpYVmxBMVNrUlpTRXhTUkZoV05ETkVTalpaUWtaU\"\n\"E5GaExXbFZKVlU0MlNWZE9XbEU9fCF_YdjoAAYppXzd0de2fRiN9xrLret1r5AhyO25J0XO\"\nnamespace = \"gkkarobia\"\n\nclient = kfp.Client(f\"http://{kubeflow_gateway_endpoint}/pipeline\",\n                    cookies=f\"authservice_session={authservice_session_cookie}\")\n\nexperiment = client.create_experiment(\"telco\", namespace=namespace)\nprint(client.list_experiments(namespace=namespace))\n\nrun = client.run_pipeline(experiment_id=experiment.id, job_name=\"telco_pipeline\",\n                          pipeline_package_path=\"telco_pipeline.zip\")\n"
  },
  {
    "repo": "qiuosier/kubeflow_pipelines",
    "file_path": "kf_sklearn_iris.py",
    "raw_url": "https://raw.githubusercontent.com/qiuosier/kubeflow_pipelines/main/kf_sklearn_iris.py",
    "content": "from pickle import load\nimport kfp\nfrom kfp import dsl\nfrom kfp.components import func_to_container_op, InputPath, OutputPath\n\n\ndef load_and_split(\n    train_x_path: OutputPath(str), \n    test_x_path: OutputPath(str),\n    train_y_path: OutputPath(str), \n    test_y_path: OutputPath(str)\n):\n    import numpy\n    from sklearn import datasets\n    from sklearn.model_selection import train_test_split\n\n    iris_x, iris_y = datasets.load_iris(return_X_y=True)\n    train_x, test_x, train_y , test_y = train_test_split(iris_x, iris_y, test_size=0.2)\n    print(f\"Training data size - X: {train_x.size}, y: {train_y.size}\")\n    print(f\"Testing data size - X: {test_x.size}, y: {test_y.size}\")\n\n    with open(train_x_path, 'wb') as f:\n        numpy.save(f, train_x)\n    with open(test_x_path, 'wb') as f:\n        numpy.save(f, test_x)\n    with open(train_y_path, 'wb') as f:\n        numpy.save(f, train_y)\n    with open(test_y_path, 'wb') as f:\n        numpy.save(f, test_y)\n\n\ndef train_knn(train_x_path: InputPath(), train_y_path: InputPath(), model_path: OutputPath()):\n    import numpy\n    import pickle\n    from sklearn.neighbors import KNeighborsClassifier\n    train_x = numpy.load(train_x_path)\n    train_y = numpy.load(train_y_path)\n    print(f\"Training data size - X: {train_x.size}, y: {train_y.size}\")\n    knn = KNeighborsClassifier()\n    knn.fit(train_x, train_y)\n    with open(model_path, 'wb') as f:\n        pickle.dump(knn, f)\n\n\ndef test_knn(test_x_path: InputPath(), test_y_path: InputPath(), model_path: InputPath()):\n    import numpy\n    import pickle\n    from sklearn.metrics import classification_report\n    test_x = numpy.load(test_x_path)\n    test_y = numpy.load(test_y_path)\n    print(f\"Testing data size - X: {test_x.size}, y: {test_y.size}\")\n    with open(model_path, 'rb') as f:\n        knn = pickle.load(f)\n    pred_y = knn.predict(test_x)\n    print(classification_report(test_y, pred_y))\n\n\ndef train_test_knn():\n    load_op = func_to_container_op(load_and_split, base_image='qiuosier/sklearn')\n    train_op = func_to_container_op(train_knn, base_image='qiuosier/sklearn')\n    test_op = func_to_container_op(test_knn, base_image='qiuosier/sklearn')\n    \n    load_task = load_op()\n    train_task = train_op(load_task.outputs['train_x'], load_task.outputs['train_y'])\n    test_task = test_op(load_task.outputs['test_x'], load_task.outputs['test_y'], train_task.output)\n\n\n@dsl.pipeline(name='sklearn-iris', description='Classifying Iris data with KNN.')\ndef kf_pipeline():\n    train_test_knn()\n\n\n# if __name__ == '__main__':\n#     # Compile the pipeline\n#     kfp.compiler.Compiler().compile(kf_pipeline, __file__ + '.yaml')\n"
  },
  {
    "repo": "qiuosier/kubeflow_pipelines",
    "file_path": "kf_sklearn_iris_with_condition.py",
    "raw_url": "https://raw.githubusercontent.com/qiuosier/kubeflow_pipelines/main/kf_sklearn_iris_with_condition.py",
    "content": "import kfp\nfrom kfp import dsl\nfrom kfp.components import func_to_container_op, InputPath, OutputPath\n\n\ndef load_data(x_path: OutputPath(str), y_path: OutputPath(str)):\n    import numpy\n    from sklearn import datasets\n    iris_x, iris_y = datasets.load_iris(return_X_y=True)\n    with open(x_path, 'wb') as f:\n        numpy.save(f, iris_x)\n    with open(y_path, 'wb') as f:\n        numpy.save(f, iris_y)\n\ndef split_data(\n    x_path: InputPath(), \n    y_path: InputPath(),\n    train_x_path: OutputPath(str), \n    test_x_path: OutputPath(str),\n    train_y_path: OutputPath(str), \n    test_y_path: OutputPath(str),\n):\n    import numpy\n    from sklearn.model_selection import train_test_split\n    iris_x = numpy.load(x_path)\n    iris_y = numpy.load(y_path)\n    train_x, test_x, train_y , test_y = train_test_split(iris_x, iris_y, test_size=0.2)\n    print(f\"Training data size - X: {train_x.size}, y: {train_y.size}\")\n    print(f\"Testing data size - X: {test_x.size}, y: {test_y.size}\")\n\n    with open(train_x_path, 'wb') as f:\n        numpy.save(f, train_x)\n    with open(test_x_path, 'wb') as f:\n        numpy.save(f, test_x)\n    with open(train_y_path, 'wb') as f:\n        numpy.save(f, train_y)\n    with open(test_y_path, 'wb') as f:\n        numpy.save(f, test_y)\n\n\ndef train_knn(train_x_path: InputPath(), train_y_path: InputPath(), model_path: OutputPath()):\n    import numpy\n    import pickle\n    from sklearn.neighbors import KNeighborsClassifier\n    train_x = numpy.load(train_x_path)\n    train_y = numpy.load(train_y_path)\n    print(f\"Training data size - X: {train_x.size}, y: {train_y.size}\")\n    knn = KNeighborsClassifier()\n    knn.fit(train_x, train_y)\n    with open(model_path, 'wb') as f:\n        pickle.dump(knn, f)\n\ndef train_logistics(train_x_path: InputPath(), train_y_path: InputPath(), model_path: OutputPath()):\n    import numpy\n    import pickle\n    import random\n    from sklearn.linear_model import LogisticRegression\n    p = random.random()\n    print(p)\n    if p > 0.5:\n        raise Exception()\n    train_x = numpy.load(train_x_path)\n    train_y = numpy.load(train_y_path)\n    print(f\"Training data size - X: {train_x.size}, y: {train_y.size}\")\n    knn = LogisticRegression()\n    knn.fit(train_x, train_y)\n    with open(model_path, 'wb') as f:\n        pickle.dump(knn, f)\n\n\ndef test_model(test_x_path: InputPath(), test_y_path: InputPath(), model_path: InputPath()):\n    import numpy\n    import pickle\n    import random\n    from sklearn.metrics import classification_report\n    p = random.random()\n    print(p)\n    if p > 0.5:\n        raise Exception()\n    test_x = numpy.load(test_x_path)\n    test_y = numpy.load(test_y_path)\n    print(f\"Testing data size - X: {test_x.size}, y: {test_y.size}\")\n    with open(model_path, 'rb') as f:\n        model = pickle.load(f)\n    pred_y = model.predict(test_x)\n    print(classification_report(test_y, pred_y))\n\n@func_to_container_op\ndef select_classifier(minimum: int, maximum: int) -> int:\n    \"\"\"Generate a random number between minimum and maximum (inclusive).\"\"\"\n    import random\n    result = random.randint(minimum, maximum)\n    print(result)\n    return result\n\n@func_to_container_op\ndef fail_op(message):\n    \"\"\"Fails.\"\"\"\n    import sys\n    print(message)    \n    sys.exit(1)\n\ndef train_test_knn():\n\n    load_op = func_to_container_op(load_data, base_image='qiuosier/sklearn')\n    split_op = func_to_container_op(split_data, base_image='qiuosier/sklearn')\n    test_op = func_to_container_op(test_model, base_image='qiuosier/sklearn')\n    train_knn_op = func_to_container_op(train_knn, base_image='qiuosier/sklearn')\n    train_logistics_op = func_to_container_op(train_logistics, base_image='qiuosier/sklearn')\n    \n    load_task = load_op()\n\n    split_task = split_op(load_task.outputs['x'], load_task.outputs['y'])\n    split_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n\n    p = select_classifier(0, 9)\n    p.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n\n    with dsl.Condition(p.output >= 5):\n        train_task = train_knn_op(split_task.outputs['train_x'], split_task.outputs['train_y'])\n        test_op(split_task.outputs['test_x'], split_task.outputs['test_y'], train_task.output)\n    with dsl.Condition(p.output < 5):\n        train_task = train_logistics_op(split_task.outputs['train_x'], split_task.outputs['train_y'])\n        test_op(split_task.outputs['test_x'], split_task.outputs['test_y'], train_task.output)\n\n\n@dsl.pipeline(name='sklearn-iris', description='Classifying Iris data with KNN.')\ndef kf_pipeline():\n    train_test_knn()\n\n\n# if __name__ == '__main__':\n#     # Compile the pipeline\n#     kfp.compiler.Compiler().compile(kf_pipeline, __file__ + '.yaml')\n"
  },
  {
    "repo": "tiktakdad/kubeflow_pipeline",
    "file_path": "iris/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/tiktakdad/kubeflow_pipeline/master/iris/pipeline.py",
    "content": "import kfp\nimport kfp.components as comp\nfrom kubernetes.client.models import V1SecurityContext\nfrom kfp import dsl\n@dsl.pipeline(\n    name='tiktakdad-iris',\n    description='tiktakdad iris test'\n)\n\ndef soojin_pipeline():\n    user = 1000\n    add_p = dsl.ContainerOp(\n        name=\"load iris data pipeline\",\n        image=\"tiktakdad/iris-preprocessing@sha256:9afcd31ddd4f9068f09201669915911ca9e4504f6f993f514d26418b6627959a\",\n        arguments=[\n            '--data_path', './Iris.csv'\n        ],\n        file_outputs={'iris' : '/iris.csv'}\n    )\n    add_p.set_security_context(V1SecurityContext(run_as_user=user, run_as_group=0))\n\n    ml = dsl.ContainerOp(\n        name=\"training pipeline\",\n        image=\"tiktakdad/iris-training@sha256:5a316f66ad586d9494bc9b2b70225948337cbb1d1341e92fb8e83d70ff3f92d9\",\n        arguments=[\n            '--data', add_p.outputs['iris']\n        ]\n    )\n    ml.set_security_context(V1SecurityContext(run_as_user=user, run_as_group=0))\n\n    ml.after(add_p)\n    \nif __name__ == \"__main__\":\n    import kfp.compiler as compiler\n    compiler.Compiler().compile(test_pipeline, __file__ + \".tar.gz\")\n"
  },
  {
    "repo": "tiktakdad/kubeflow_pipeline",
    "file_path": "iris/pipeline_test.py",
    "raw_url": "https://raw.githubusercontent.com/tiktakdad/kubeflow_pipeline/master/iris/pipeline_test.py",
    "content": "from kfp import dsl\nfrom kubernetes.client.models import V1EnvVar, V1SecretKeySelector\n@dsl.pipeline(\n    name='foo',\n    description='hello world')\ndef foo_pipeline(tag: str, pull_image_policy: str):\n  # any attributes can be parameterized (both serialized string or actual PipelineParam)\n  op = dsl.ContainerOp(name='foo',\n                      image='busybox:%s' % tag,\n                      # pass in init_container list\n                      init_containers=[dsl.UserContainer('print', 'busybox:latest', command='echo \"hello\"')],\n                      # pass in sidecars list\n                      sidecars=[dsl.Sidecar('print', 'busybox:latest', command='echo \"hello\"')],\n                      # pass in k8s container kwargs\n                      container_kwargs={'env': [V1EnvVar('foo', 'bar')]},\n  )\n  # set `imagePullPolicy` property for `container` with `PipelineParam`\n  op.container.set_image_pull_policy(pull_image_policy)\n  # add sidecar with parameterized image tag\n  # sidecar follows the argo sidecar swagger spec\n  op.add_sidecar(dsl.Sidecar('redis', 'redis:%s' % tag).set_image_pull_policy('Always'))\n"
  },
  {
    "repo": "tiktakdad/kubeflow_pipeline",
    "file_path": "metrics_evaluation_and_check_condition/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/tiktakdad/kubeflow_pipeline/master/metrics_evaluation_and_check_condition/pipeline.py",
    "content": "import kfp\nimport kfp.components as comp\nfrom kfp import dsl\n\ndef print_op(msg):\n    \"\"\"Print a message.\"\"\"\n    return dsl.ContainerOp(\n        name='Print',\n        image='alpine:3.6',\n        command=['echo', msg],\n)\n\n@dsl.pipeline(\n    name='soojin-iris',\n    description='soojin iris test'\n)\n\ndef soojin_pipeline():\n    add_p = dsl.ContainerOp(\n        name=\"load iris data pipeline\",\n        image=\"lsjsj92/soojin-iris-load:0.7\",\n        arguments=[\n            '--data_path', './Iris.csv'\n        ],\n        file_outputs={'iris' : '/iris.csv'}\n    )\n\n    train_and_eval = dsl.ContainerOp(\n        name=\"training pipeline\",\n        image=\"lsjsj92/soojin-iris-train_and_eval:0.4\",\n        arguments=[\n            '--data', add_p.outputs['iris']\n        ],\n        file_outputs={\n            'accuracy' : '/accuracy.json',\n            'mlpipeline-metrics' : '/mlpipeline-metrics.json'\n        }\n    )\n\n    train_and_eval.after(add_p)\n    baseline = 0.7\n    with dsl.Condition(train_and_eval.outputs['accuracy'] > baseline ) as check_condition:\n        print_op(f\"accuracy\ub294 {train_and_eval.outputs['accuracy']}\ub85c accuracy baseline\uc778 {baseline}\ubcf4\ub2e4 \ud07d\ub2c8\ub2e4!\")\n    \n    with dsl.Condition(train_and_eval.outputs['accuracy'] < baseline) as check_condition:\n        print_op(f\"accuracy\ub294 {train_and_eval.outputs['accuracy']}\ub85c accuracy baseline\uc778 {baseline}\ubcf4\ub2e4 \uc791\uc2b5\ub2c8\ub2e4.\")\n    \nif __name__ == \"__main__\":\n    import kfp.compiler as compiler\n    compiler.Compiler().compile(soojin_pipeline, __file__ + \".tar.gz\")\n"
  },
  {
    "repo": "tiktakdad/kubeflow_pipeline",
    "file_path": "titanic/pipelines.py",
    "raw_url": "https://raw.githubusercontent.com/tiktakdad/kubeflow_pipeline/master/titanic/pipelines.py",
    "content": "import kfp\nimport kfp.components as comp\nfrom kfp import dsl\nfrom kfp import compiler\n\n@dsl.pipeline(\n    name='soojin-kubeflow-titanic',\n    description = \"This is a pipeline of titanic made by soojin(lsjsj92.tistory.com)\"\n)\ndef titanic_pipeline(\n    BUCKETNAME,\n    ACCESSKEY,\n    SECRETKEY,\n    REGIONNAME,\n    ORIKEY,\n    SAVEKEY,\n    MODELKEY):\n    \n    step_1_preprocessing = dsl.ContainerOp(\n        name = 'cleaning titanic data',\n        image = 'lsjsj92/titanic_preprocessing:latest',\n        arguments=[\n            '--bucket_name' , BUCKETNAME,\n            '--ACCESSKEY' , ACCESSKEY,\n            '--SECRETKEY' , SECRETKEY,\n            '--region_name' , REGIONNAME,\n            '--data_key' , ORIKEY,\n            '--save_key' , SAVEKEY\n        ]\n    )\n\n    step_2_training = dsl.ContainerOp(\n        name = 'training model',\n        image = 'lsjsj92/titanic_modeling:latest',\n        arguments=[\n            '--bucket_name' , BUCKETNAME,\n            '--ACCESSKEY', ACCESSKEY,\n            '--SECRETKEY', SECRETKEY,\n            '--region_name' , REGIONNAME,\n            '--model_key' , MODELKEY,\n            '--data' , SAVEKEY\n        ]\n    )\n\n    step_2_training.after(step_1_preprocessing)\n\ncompiler.Compiler().compile(titanic_pipeline, 'soojin_titanic.tar.gz')"
  },
  {
    "repo": "swe-train/kubeflow__pipelines",
    "file_path": "samples/test/after.py",
    "raw_url": "https://raw.githubusercontent.com/swe-train/kubeflow__pipelines/0.1.8/samples/test/after.py",
    "content": "# Copyright 2020 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom kfp import components\nfrom kfp.v2 import dsl\nimport kfp.v2.compiler as compiler\n\ncomponent_op = components.load_component_from_text(\n    \"\"\"\nname: Print Text\ninputs:\n- {name: text, type: String}\nimplementation:\n  container:\n    image: alpine\n    command:\n    - sh\n    - -c\n    - |\n      set -e -x\n      echo \"$0\"\n    - {inputValue: text}\n\"\"\"\n)\n\n\n@dsl.pipeline(name='pipeline-with-after')\ndef my_pipeline():\n    task1 = component_op(text='1st task')\n    task2 = component_op(text='2nd task').after(task1)\n    task3 = component_op(text='3rd task').after(task1, task2)\n\n\nif __name__ == '__main__':\n    compiler.Compiler().compile(\n        pipeline_func=my_pipeline,\n        pipeline_root='dummy_root',\n        output_path=__file__ + '.json'\n    )\n"
  },
  {
    "repo": "swe-train/kubeflow__pipelines",
    "file_path": "samples/test/after_test.py",
    "raw_url": "https://raw.githubusercontent.com/swe-train/kubeflow__pipelines/0.1.8/samples/test/after_test.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp\nfrom .after import my_pipeline\nfrom .util import run_pipeline_func, TestCase\n\nrun_pipeline_func([\n    TestCase(pipeline_func=my_pipeline),\n    TestCase(\n        pipeline_func=my_pipeline, mode=kfp.dsl.PipelineExecutionMode.V2_ENGINE\n    ),\n])\n"
  },
  {
    "repo": "swe-train/kubeflow__pipelines",
    "file_path": "samples/test/cache_v2_test.py",
    "raw_url": "https://raw.githubusercontent.com/swe-train/kubeflow__pipelines/0.1.8/samples/test/cache_v2_test.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Two step v2-compatible pipeline.\"\"\"\n\n# %%\n\nfrom __future__ import annotations\n\nimport random\nimport string\nimport unittest\nimport functools\n\nimport kfp\nimport kfp_server_api\n\nfrom .two_step import two_step_pipeline\nfrom .util import run_pipeline_func, TestCase, KfpMlmdClient, KfpTask\nfrom ml_metadata.proto import Execution\n\n\ndef verify_tasks(\n    t: unittest.TestCase, tasks: dict[str, KfpTask], task_state, uri: str,\n    some_int: int\n):\n    task_names = [*tasks.keys()]\n    t.assertCountEqual(task_names, ['train-op', 'preprocess'], 'task names')\n\n    preprocess = tasks['preprocess']\n    train = tasks['train-op']\n\n    t.assertEqual({\n        'name': 'preprocess',\n        'inputs': {\n            'artifacts': [],\n            'parameters': {\n                'some_int': some_int,\n                'uri': uri\n            }\n        },\n        'outputs': {\n            'artifacts': [{\n                'metadata': {\n                    'display_name': 'output_dataset_one',\n                },\n                'name': 'output_dataset_one',\n                'type': 'system.Dataset'\n            }],\n            'parameters': {\n                'output_parameter_one': some_int\n            }\n        },\n        'type': 'system.ContainerExecution',\n        'state': task_state,\n    }, preprocess.get_dict())\n    t.assertEqual({\n        'name': 'train-op',\n        'inputs': {\n            'artifacts': [{\n                'metadata': {\n                    'display_name': 'output_dataset_one',\n                },\n                'name': 'dataset',\n                'type': 'system.Dataset',\n            }],\n            'parameters': {\n                'num_steps': some_int\n            }\n        },\n        'outputs': {\n            'artifacts': [{\n                'metadata': {\n                    'display_name': 'model',\n                },\n                'name': 'model',\n                'type': 'system.Model',\n            }],\n            'parameters': {}\n        },\n        'type': 'system.ContainerExecution',\n        'state': task_state,\n    }, train.get_dict())\n\n\ndef verify(\n    run: kfp_server_api.ApiRun, mlmd_connection_config, uri: str, some_int,\n    state: int, **kwargs\n):\n    t = unittest.TestCase()\n    t.maxDiff = None  # we always want to see full diff\n    t.assertEqual(run.status, 'Succeeded')\n    client = KfpMlmdClient(mlmd_connection_config=mlmd_connection_config)\n    tasks = client.get_tasks(run_id=run.id)\n    verify_tasks(t, tasks, state, uri, some_int)\n\n\nif __name__ == '__main__':\n    letters = string.ascii_lowercase\n    random_uri = 'http://' + ''.join(random.choice(letters) for i in range(5))\n    random_int = random.randint(0, 10000)\n    run_pipeline_func([\n        TestCase(\n            pipeline_func=two_step_pipeline,\n            arguments={\n                'uri': f'{random_uri}',\n                'some_int': f'{random_int}'\n            },\n            verify_func=functools.partial(\n                verify,\n                uri=random_uri,\n                some_int=random_int,\n                state=Execution.State.COMPLETE,\n            ),\n            mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE,\n            enable_caching=True\n        ),\n    ]),\n    run_pipeline_func([\n        TestCase(\n            pipeline_func=two_step_pipeline,\n            arguments={\n                'uri': f'{random_uri}',\n                'some_int': f'{random_int}'\n            },\n            verify_func=functools.partial(\n                verify,\n                uri=random_uri,\n                some_int=random_int,\n                state=Execution.State.CACHED\n            ),\n            mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE,\n            enable_caching=True\n        ),\n    ])\n\n# %%\n"
  },
  {
    "repo": "swe-train/kubeflow__pipelines",
    "file_path": "samples/test/fail.py",
    "raw_url": "https://raw.githubusercontent.com/swe-train/kubeflow__pipelines/0.1.8/samples/test/fail.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Fail pipeline.\"\"\"\n\nfrom kfp import components, dsl\n\n\ndef fail():\n    '''Fails'''\n    import sys\n    sys.exit(1)\n\n\nfail_op = components.create_component_from_func(\n    fail, base_image='alpine:latest'\n)\n\n\n@dsl.pipeline(name='fail_pipeline')\ndef fail_pipeline():\n    fail_task = fail_op()\n"
  },
  {
    "repo": "swe-train/kubeflow__pipelines",
    "file_path": "samples/test/fail_parameter_value_missing.py",
    "raw_url": "https://raw.githubusercontent.com/swe-train/kubeflow__pipelines/0.1.8/samples/test/fail_parameter_value_missing.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom kfp import dsl, components\n\necho = components.load_component_from_text(\n    \"\"\"\nname: Echo\ninputs:\n- {name: text, type: String}\nimplementation:\n  container:\n    image: alpine\n    command:\n    - echo\n    - {inputValue: text}\n\"\"\"\n)\n\n\n@dsl.pipeline(name='parameter_value_missing')\ndef pipeline(\n    parameter:\n    str  # parameter should be specified when submitting, but we are missing it in the test\n):\n    echo_op = echo(text=parameter)\n"
  },
  {
    "repo": "swe-train/kubeflow__pipelines",
    "file_path": "samples/test/legacy_data_passing.py",
    "raw_url": "https://raw.githubusercontent.com/swe-train/kubeflow__pipelines/0.1.8/samples/test/legacy_data_passing.py",
    "content": "# This pipeline demonstrates and verifies all ways of data passing supported by KFP.\n\n# %% [markdown]\n# KFP has simple data passing model. There are three different kinds of arguments and tw different ways to consume an input argument. All combinations are supported.\n\n# Any input can be consumed:\n# * As value (`inputValue` placeholder)\n# * As file (`inputPath` placeholder).\n\n# Input argument can come from:\n# * Constant value\n# * Pipeline parameter\n# * Upstream component output\n\n# Combining these options there are 6 end-to-end data passing cases, each of which works regardless of type:\n\n# 1. Pass constant value which gets consumed as value. (Common)\n# 2. Pass constant value which gets consumed as file. (Often used in test pipelines)\n# 3. Pass pipeline parameter that gets consumed as value. (Common)\n# 4. Pass pipeline parameter that gets consumed as file. (Rare. Sometimes used with JSON parameters)\n# 5. Pass task output that gets consumed as value. (Common)\n# 6. Pass task output that gets consumed as file. (Common)\n\n# The only restriction on types is that when both upstream output and downstream input have types, the types must match.\n\n# %%\nimport kfp\nfrom kfp.components import create_component_from_func, InputPath, OutputPath\n\n\n# Components\n# Produce\n@create_component_from_func\ndef produce_anything(data_path: OutputPath()):\n    with open(data_path, \"w\") as f:\n        f.write(\"produce_anything\")\n\n\n@create_component_from_func\ndef produce_something(data_path: OutputPath(\"Something\")):\n    with open(data_path, \"w\") as f:\n        f.write(\"produce_something\")\n\n\n@create_component_from_func\ndef produce_something2() -> 'Something':\n    return \"produce_something2\"\n\n\n@create_component_from_func\ndef produce_string() -> str:\n    return \"produce_string\"\n\n\n# Consume as value\n@create_component_from_func\ndef consume_anything_as_value(data):\n    print(\"consume_anything_as_value: \" + data)\n\n\n@create_component_from_func\ndef consume_something_as_value(data: \"Something\"):\n    print(\"consume_something_as_value: \" + data)\n\n\n@create_component_from_func\ndef consume_string_as_value(data: str):\n    print(\"consume_string_as_value: \" + data)\n\n\n# Consume as file\n@create_component_from_func\ndef consume_anything_as_file(data_path: InputPath()):\n    with open(data_path) as f:\n        print(\"consume_anything_as_file: \" + f.read())\n\n\n@create_component_from_func\ndef consume_something_as_file(data_path: InputPath('Something')):\n    with open(data_path) as f:\n        print(\"consume_something_as_file: \" + f.read())\n\n\n@create_component_from_func\ndef consume_string_as_file(data_path: InputPath(str)):\n    with open(data_path) as f:\n        print(\"consume_string_as_file: \" + f.read())\n\n\n# Pipeline\n@kfp.dsl.pipeline(name='data_passing_pipeline')\ndef data_passing_pipeline(\n    anything_param=\"anything_param\",\n    something_param: \"Something\" = \"something_param\",\n    string_param: str = \"string_param\",\n):\n    produced_anything = produce_anything().output\n    produced_something = produce_something().output\n    produced_string = produce_string().output\n\n    # Pass constant value; consume as value\n    consume_anything_as_value(\"constant\")\n    consume_something_as_value(\"constant\")\n    consume_string_as_value(\"constant\")\n\n    # Pass constant value; consume as file\n    consume_anything_as_file(\"constant\")\n    consume_something_as_file(\"constant\")\n    consume_string_as_file(\"constant\")\n\n    # Pass pipeline parameter; consume as value\n    consume_anything_as_value(anything_param)\n    consume_anything_as_value(something_param)\n    consume_anything_as_value(string_param)\n    consume_something_as_value(anything_param)\n    consume_something_as_value(something_param)\n    consume_string_as_value(anything_param)\n    consume_string_as_value(string_param)\n\n    # Pass pipeline parameter; consume as file\n    consume_anything_as_file(anything_param)\n    consume_anything_as_file(something_param)\n    consume_anything_as_file(string_param)\n    consume_something_as_file(anything_param)\n    consume_something_as_file(something_param)\n    consume_string_as_file(anything_param)\n    consume_string_as_file(string_param)\n\n    # Pass task output; consume as value\n    consume_anything_as_value(produced_anything)\n    consume_anything_as_value(produced_something)\n    consume_anything_as_value(produced_string)\n    consume_something_as_value(produced_anything)\n    consume_something_as_value(produced_something)\n    consume_string_as_value(produced_anything)\n    consume_string_as_value(produced_string)\n\n    # Pass task output; consume as file\n    consume_anything_as_file(produced_anything)\n    consume_anything_as_file(produced_something)\n    consume_anything_as_file(produced_string)\n    consume_something_as_file(produced_anything)\n    consume_something_as_file(produced_something)\n    consume_string_as_file(produced_anything)\n    consume_string_as_file(produced_string)\n\n\nif __name__ == \"__main__\":\n    kfp_endpoint = None\n    kfp.Client(host=kfp_endpoint).create_run_from_pipeline_func(\n        data_passing_pipeline, arguments={})\n"
  },
  {
    "repo": "swe-train/kubeflow__pipelines",
    "file_path": "samples/test/legacy_data_passing_test.py",
    "raw_url": "https://raw.githubusercontent.com/swe-train/kubeflow__pipelines/0.1.8/samples/test/legacy_data_passing_test.py",
    "content": "from .data_passing import data_passing_pipeline\nfrom .util import run_pipeline_func, TestCase,\nfrom kfp.dsl import PipelineExecutionMode\n\nrun_pipeline_func([\n    TestCase(\n        pipeline_func=data_passing_pipeline,\n        mode=PipelineExecutionMode.V1_LEGACY,\n    )\n])\n"
  },
  {
    "repo": "swe-train/kubeflow__pipelines",
    "file_path": "samples/test/legacy_exit_handler.py",
    "raw_url": "https://raw.githubusercontent.com/swe-train/kubeflow__pipelines/0.1.8/samples/test/legacy_exit_handler.py",
    "content": "#!/usr/bin/env python3\n# Copyright 2019 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp\nfrom kfp import dsl\n\n\ndef gcs_download_op(url):\n    return dsl.ContainerOp(\n        name='GCS - Download',\n        image='google/cloud-sdk:279.0.0',\n        command=['sh', '-ce'],\n        arguments=['gsutil cp $0 $1 && cat $1', url, '/tmp/results.txt'],\n        file_outputs={\n            'data': '/tmp/results.txt',\n        }\n    )\n\n\ndef echo_op(text):\n    return dsl.ContainerOp(\n        name='echo',\n        image='library/bash:4.4.23',\n        command=['sh', '-cex'],\n        arguments=['echo \"$0\"', text],\n    )\n\n\n@dsl.pipeline(\n    name='Exit Handler',\n    description=\n    'Downloads a message and prints it. The exit handler will run after the pipeline finishes (successfully or not).'\n)\ndef download_and_print(url='gs://ml-pipeline/shakespeare1.txt'):\n    \"\"\"A sample pipeline showing exit handler.\"\"\"\n\n    exit_task = echo_op('exit!')\n\n    with dsl.ExitHandler(exit_task):\n        download_task = gcs_download_op(url)\n        echo_task = echo_op(download_task.output)\n\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(download_and_print, __file__ + '.yaml')\n"
  },
  {
    "repo": "swe-train/kubeflow__pipelines",
    "file_path": "samples/test/legacy_exit_handler_test.py",
    "raw_url": "https://raw.githubusercontent.com/swe-train/kubeflow__pipelines/0.1.8/samples/test/legacy_exit_handler_test.py",
    "content": "#!/usr/bin/env python3\n# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp\nfrom .legacy_exit_handler import download_and_print\nfrom .util import run_pipeline_func, TestCase\n\n\nrun_pipeline_func([\n    # This sample is expected to not work on v2 compatible mode.\n    TestCase(\n        pipeline_func=download_and_print,\n        mode=kfp.dsl.PipelineExecutionMode.V1_LEGACY\n    )\n])\n"
  },
  {
    "repo": "swe-train/kubeflow__pipelines",
    "file_path": "samples/test/lightweight_python_functions_v2_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/swe-train/kubeflow__pipelines/0.1.8/samples/test/lightweight_python_functions_v2_pipeline.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Sample pipeline for passing data in KFP v2.\"\"\"\nfrom typing import Dict, List\nimport os\n\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import Input, InputPath, Output, OutputPath, Dataset, Model, component\nimport kfp.v2.compiler as compiler\n\n# In tests, we install a KFP package from the PR under test. Users should not\n# normally need to specify `kfp_package_path` in their component definitions.\n_KFP_PACKAGE_PATH = os.getenv('KFP_PACKAGE_PATH')\n\n\n@component(kfp_package_path=_KFP_PACKAGE_PATH)\ndef preprocess(\n    # An input parameter of type string.\n    message: str,\n    # Use Output[T] to get a metadata-rich handle to the output artifact\n    # of type `Dataset`.\n    output_dataset_one: Output[Dataset],\n    # A locally accessible filepath for another output artifact of type\n    # `Dataset`.\n    output_dataset_two_path: OutputPath('Dataset'),\n    # A locally accessible filepath for an output parameter of type string.\n    output_parameter_path: OutputPath(str),\n    # A locally accessible filepath for an output parameter of type bool.\n    output_bool_parameter_path: OutputPath(bool),\n    # A locally accessible filepath for an output parameter of type dict.\n    output_dict_parameter_path: OutputPath(Dict[str, int]),\n    # A locally accessible filepath for an output parameter of type list.\n    output_list_parameter_path: OutputPath(List[str]),\n    # An input message that defaults to empty.\n    empty_message: str = \"\",\n):\n    \"\"\"Dummy preprocessing step\"\"\"\n\n    # Use Dataset.path to access a local file path for writing.\n    # One can also use Dataset.uri to access the actual URI file path.\n    with open(output_dataset_one.path, 'w') as f:\n        f.write(message)\n\n    # OutputPath is used to just pass the local file path of the output artifact\n    # to the function.\n    with open(output_dataset_two_path, 'w') as f:\n        f.write(message)\n\n    with open(output_parameter_path, 'w') as f:\n        f.write(message)\n\n    with open(output_bool_parameter_path, 'w') as f:\n        f.write(\n            str(True)\n        )  # use either `str()` or `json.dumps()` for bool values.\n\n    import json\n    with open(output_dict_parameter_path, 'w') as f:\n        f.write(json.dumps({'A': 1, 'B': 2}))\n\n    with open(output_list_parameter_path, 'w') as f:\n        f.write(json.dumps(['a', 'b', 'c']))\n\n\n@component(kfp_package_path=_KFP_PACKAGE_PATH)\ndef train(\n    # Use InputPath to get a locally accessible path for the input artifact\n    # of type `Dataset`.\n    dataset_one_path: InputPath('Dataset'),\n    # Use Input[T] to get a metadata-rich handle to the input artifact\n    # of type `Dataset`.\n    dataset_two: Input[Dataset],\n    # An input parameter of type string.\n    message: str,\n    # Use Output[T] to get a metadata-rich handle to the output artifact\n    # of type `Dataset`.\n    model: Output[Model],\n    # An input parameter of type bool.\n    input_bool: bool,\n    # An input parameter of type dict.\n    input_dict: Dict[str, int],\n    # An input parameter of type List[str].\n    input_list: List[str],\n    # An input parameter of type int with a default value.\n    num_steps: int = 100,\n):\n    \"\"\"Dummy Training step\"\"\"\n    with open(dataset_one_path, 'r') as input_file:\n        dataset_one_contents = input_file.read()\n\n    with open(dataset_two.path, 'r') as input_file:\n        dataset_two_contents = input_file.read()\n\n    line = (\n        f'dataset_one_contents: {dataset_one_contents} || '\n        f'dataset_two_contents: {dataset_two_contents} || '\n        f'message: {message} || '\n        f'input_bool: {input_bool}, type {type(input_bool)} || '\n        f'input_dict: {input_dict}, type {type(input_dict)} || '\n        f'input_list: {input_list}, type {type(input_list)} \\n'\n    )\n\n    with open(model.path, 'w') as output_file:\n        for i in range(num_steps):\n            output_file.write('Step {}\\n{}\\n=====\\n'.format(i, line))\n\n    # Use model.get() to get a Model artifact, which has a .metadata dictionary\n    # to store arbitrary metadata for the output artifact.\n    model.metadata['accuracy'] = 0.9\n\n\n@dsl.pipeline(name='my-test-pipeline-beta')\ndef pipeline(message: str = 'message'):\n    preprocess_task = preprocess(message=message)\n    train_task = train(\n        dataset_one_path=preprocess_task.outputs['output_dataset_one'],\n        dataset_two=preprocess_task.outputs['output_dataset_two_path'],\n        message=preprocess_task.outputs['output_parameter_path'],\n        input_bool=preprocess_task.outputs['output_bool_parameter_path'],\n        input_dict=preprocess_task.outputs['output_dict_parameter_path'],\n        input_list=preprocess_task.outputs['output_list_parameter_path'],\n    )\n\n\nif __name__ == '__main__':\n    compiler.Compiler().compile(\n        pipeline_func=pipeline, package_path=__file__.replace('.py', '.json')\n    )\n"
  },
  {
    "repo": "swe-train/kubeflow__pipelines",
    "file_path": "samples/test/lightweight_python_functions_v2_pipeline_test.py",
    "raw_url": "https://raw.githubusercontent.com/swe-train/kubeflow__pipelines/0.1.8/samples/test/lightweight_python_functions_v2_pipeline_test.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nfrom pprint import pprint\nimport kfp_server_api\nimport kfp.dsl as dsl\n\nfrom .lightweight_python_functions_v2_pipeline import pipeline\nfrom .util import run_pipeline_func, TestCase, KfpMlmdClient\nfrom ml_metadata.proto import Execution\n\n\ndef verify(run: kfp_server_api.ApiRun, mlmd_connection_config, **kwargs):\n    t = unittest.TestCase()\n    t.maxDiff = None  # we always want to see full diff\n    t.assertEqual(run.status, 'Succeeded')\n    client = KfpMlmdClient(mlmd_connection_config=mlmd_connection_config)\n    tasks = client.get_tasks(run_id=run.id)\n\n    task_names = [*tasks.keys()]\n    t.assertCountEqual(task_names, ['preprocess', 'train'], 'task names')\n    pprint(tasks)\n\n    preprocess = tasks['preprocess']\n    train = tasks['train']\n    pprint(preprocess.get_dict())\n    t.assertEqual(\n        {\n            'inputs': {\n                'artifacts': [],\n                'parameters': {\n                    'message': 'message',\n                    'empty_message': '',\n                }\n            },\n            'name': 'preprocess',\n            'outputs': {\n                'artifacts': [{\n                    'metadata': {\n                        'display_name': 'output_dataset_one'\n                    },\n                    'name': 'output_dataset_one',\n                    'type': 'system.Dataset'\n                }, {\n                    'metadata': {\n                        'display_name': 'output_dataset_two_path'\n                    },\n                    'name': 'output_dataset_two_path',\n                    'type': 'system.Dataset'\n                }],\n                'parameters': {\n                    'output_bool_parameter_path': 'True',\n                    'output_dict_parameter_path': '{\"A\": 1, \"B\": 2}',\n                    'output_list_parameter_path': '[\"a\", \"b\", \"c\"]',\n                    'output_parameter_path': 'message'\n                }\n            },\n            'type': 'system.ContainerExecution',\n            'state': Execution.State.COMPLETE,\n        },\n        preprocess.get_dict(),\n    )\n    t.assertEqual(\n        {\n            'inputs': {\n                'artifacts': [{\n                    'metadata': {\n                        'display_name': 'output_dataset_one'\n                    },\n                    'name': 'dataset_one_path',\n                    'type': 'system.Dataset'\n                }, {\n                    'metadata': {\n                        'display_name': 'output_dataset_two_path'\n                    },\n                    'name': 'dataset_two',\n                    'type': 'system.Dataset'\n                }],\n                'parameters': {\n                    'input_bool': 'True',\n                    'input_dict': '{\"A\": 1, \"B\": 2}',\n                    'input_list': '[\"a\", \"b\", \"c\"]',\n                    'message': 'message',\n                    'num_steps': 100,\n                }\n            },\n            'name': 'train',\n            'outputs': {\n                'artifacts': [{\n                    'metadata': {\n                        'display_name': 'model',\n                        'accuracy': 0.9,\n                    },\n                    'name': 'model',\n                    'type': 'system.Model'\n                }],\n                'parameters': {}\n            },\n            'type': 'system.ContainerExecution',\n            'state': Execution.State.COMPLETE,\n        },\n        train.get_dict(),\n    )\n\n\nrun_pipeline_func([\n    TestCase(\n        pipeline_func=pipeline,\n        verify_func=verify,\n        mode=dsl.PipelineExecutionMode.V2_COMPATIBLE\n    ),\n    TestCase(\n        pipeline_func=pipeline,\n        verify_func=verify,\n        mode=dsl.PipelineExecutionMode.V2_ENGINE\n    ),\n])\n"
  },
  {
    "repo": "swe-train/kubeflow__pipelines",
    "file_path": "samples/test/lightweight_python_functions_v2_with_outputs.py",
    "raw_url": "https://raw.githubusercontent.com/swe-train/kubeflow__pipelines/0.1.8/samples/test/lightweight_python_functions_v2_with_outputs.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Lightweight functions v2 with outputs.\"\"\"\nfrom typing import NamedTuple\nimport os\nfrom kfp import components, dsl\nfrom kfp.v2 import compiler\nfrom kfp.v2.dsl import Input, Dataset, Model, Metrics, component\n\n# In tests, we install a KFP package from the PR under test. Users should not\n# normally need to specify `kfp_package_path` in their component definitions.\n_KFP_PACKAGE_PATH = os.getenv('KFP_PACKAGE_PATH')\n\n\n@component(kfp_package_path=_KFP_PACKAGE_PATH)\ndef concat_message(first: str, second: str) -> str:\n    return first + second\n\n\n@component(kfp_package_path=_KFP_PACKAGE_PATH)\ndef add_numbers(first: int, second: int) -> int:\n    return first + second\n\n\n@component(kfp_package_path=_KFP_PACKAGE_PATH)\ndef output_artifact(number: int, message: str) -> Dataset:\n    result = [message for _ in range(number)]\n    return '\\n'.join(result)\n\n\n@component(kfp_package_path=_KFP_PACKAGE_PATH)\ndef output_named_tuple(\n    artifact: Input[Dataset]\n) -> NamedTuple(\n        'Outputs', [\n            ('scalar', str),\n            ('metrics', Metrics),\n            ('model', Model),\n        ]\n):\n    scalar = \"123\"\n\n    import json\n    metrics = json.dumps({\n        'metrics': [{\n            'name': 'accuracy',\n            'numberValue': 0.9,\n            'format': \"PERCENTAGE\",\n        }]\n    })\n\n    with open(artifact.path, 'r') as f:\n        artifact_contents = f.read()\n    model = \"Model contents: \" + artifact_contents\n\n    from collections import namedtuple\n    output = namedtuple('Outputs', ['scalar', 'metrics', 'model'])\n    return output(scalar, metrics, model)\n\n\n@dsl.pipeline(name='functions-with-outputs')\ndef pipeline(\n    first_message: str = 'first',\n    second_message: str = 'second',\n    first_number: int = 1,\n    second_number: int = 2,\n):\n    concat_task = concat_message(first=first_message, second=second_message)\n    add_numbers_task = add_numbers(first=first_number, second=second_number)\n    output_artifact_task = output_artifact(\n        number=add_numbers_task.output, message=concat_task.output\n    )\n    output_name_tuple = output_named_tuple(output_artifact_task.output)\n\n\nif __name__ == '__main__':\n    compiler.Compiler().compile(\n        pipeline_func=pipeline,\n        pipeline_root='dummy_root',\n        output_path=__file__ + '.json'\n    )\n"
  },
  {
    "repo": "swe-train/kubeflow__pipelines",
    "file_path": "samples/test/lightweight_python_functions_v2_with_outputs_test.py",
    "raw_url": "https://raw.githubusercontent.com/swe-train/kubeflow__pipelines/0.1.8/samples/test/lightweight_python_functions_v2_with_outputs_test.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom pprint import pprint\nimport unittest\nimport kfp\nimport kfp_server_api\nimport os\nfrom minio import Minio\n\nfrom .lightweight_python_functions_v2_with_outputs import pipeline\nfrom .util import KfpMlmdClient, run_pipeline_func, TestCase\n\n\ndef verify(run: kfp_server_api.ApiRun, mlmd_connection_config, **kwargs):\n    t = unittest.TestCase()\n    t.maxDiff = None  # we always want to see full diff\n    t.assertEqual(run.status, 'Succeeded')\n    client = KfpMlmdClient(mlmd_connection_config=mlmd_connection_config)\n    tasks = client.get_tasks(run_id=run.id)\n    pprint(tasks)\n\n    output_artifact = tasks['output-artifact']\n    output = [\n        a for a in output_artifact.outputs.artifacts if a.name == 'Output'\n    ][0]\n    pprint(output)\n\n    host = os.environ['MINIO_SERVICE_SERVICE_HOST']\n    port = os.environ['MINIO_SERVICE_SERVICE_PORT']\n    minio = Minio(\n        f'{host}:{port}',\n        access_key='minio',\n        secret_key='minio123',\n        secure=False\n    )\n    bucket, key = output.uri[len('minio://'):].split('/', 1)\n    print(f'bucket={bucket} key={key}')\n    response = minio.get_object(bucket, key)\n    data = response.read().decode('UTF-8')\n    t.assertEqual(data, 'firstsecond\\nfirstsecond\\nfirstsecond')\n\n\nrun_pipeline_func([\n    TestCase(\n        pipeline_func=pipeline,\n        mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE\n    ),\n    TestCase(\n        pipeline_func=pipeline,\n        mode=kfp.dsl.PipelineExecutionMode.V2_ENGINE,\n    ),\n    # Verify overriding pipeline root to MinIO\n    TestCase(\n        pipeline_func=pipeline,\n        verify_func=verify,\n        mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE,\n        arguments={\n            kfp.dsl.ROOT_PARAMETER_NAME: 'minio://mlpipeline/override/artifacts'\n        },\n    ),\n])\n"
  },
  {
    "repo": "swe-train/kubeflow__pipelines",
    "file_path": "samples/test/metrics_visualization_v1.py",
    "raw_url": "https://raw.githubusercontent.com/swe-train/kubeflow__pipelines/0.1.8/samples/test/metrics_visualization_v1.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp.dsl as dsl\nfrom ..core.visualization.confusion_matrix import confusion_visualization\nfrom ..core.visualization.html import html_visualization\nfrom ..core.visualization.markdown import markdown_visualization\nfrom ..core.visualization.roc import roc_visualization\nfrom ..core.visualization.table import table_visualization\n\n# Note: This test is to verify that visualization metrics on V1 is runnable by KFP.\n# However, this pipeline is only runnable on V1 mode, but not V2 compatible mode.\n\n@dsl.pipeline(\n    name='metrics-visualization-v1-pipeline')\ndef metrics_visualization_v1_pipeline():\n    confusion_visualization_task = confusion_visualization()\n    html_visualization_task = html_visualization(\"\")\n    markdown_visualization_task = markdown_visualization()\n    roc_visualization_task = roc_visualization()\n    table_visualization_task = table_visualization()\n"
  },
  {
    "repo": "swe-train/kubeflow__pipelines",
    "file_path": "samples/test/metrics_visualization_v1_test.py",
    "raw_url": "https://raw.githubusercontent.com/swe-train/kubeflow__pipelines/0.1.8/samples/test/metrics_visualization_v1_test.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp_server_api\nimport unittest\nfrom pprint import pprint\nfrom .metrics_visualization_v1 import metrics_visualization_v1_pipeline\nfrom .util import KfpMlmdClient, run_pipeline_func, TestCase\n\nimport kfp\nimport kfp_server_api\n\n\ndef verify(\n    run: kfp_server_api.ApiRun, mlmd_connection_config, argo_workflow_name: str,\n    **kwargs\n):\n    t = unittest.TestCase()\n    t.maxDiff = None  # we always want to see full diff\n    t.assertEqual(run.status, 'Succeeded')\n    client = KfpMlmdClient(mlmd_connection_config=mlmd_connection_config)\n    tasks = client.get_tasks(argo_workflow_name=argo_workflow_name)\n    pprint(tasks)\n\n    table_visualization = tasks['table-visualization']\n    markdown_visualization = tasks['markdown-visualization']\n    roc_visualization = tasks['roc-visualization']\n    html_visualization = tasks['html-visualization']\n    confusion_visualization = tasks['confusion-visualization']\n\n    t.assertEqual(table_visualization.get_dict()['outputs']['artifacts'][0]['name'],\n                  'mlpipeline_ui_metadata')\n    t.assertEqual(markdown_visualization.get_dict()['outputs']['artifacts'][0]['name'],\n                  'mlpipeline_ui_metadata')\n    t.assertEqual(roc_visualization.get_dict()['outputs']['artifacts'][0]['name'],\n                  'mlpipeline_ui_metadata')\n    t.assertEqual(html_visualization.get_dict()['outputs']['artifacts'][0]['name'],\n                  'mlpipeline_ui_metadata')\n    t.assertEqual(confusion_visualization.get_dict()['outputs']['artifacts'][0]['name'],\n                  'mlpipeline_ui_metadata')\n\n\nrun_pipeline_func([TestCase(pipeline_func=metrics_visualization_v1_pipeline,\n                            mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE\n                            )])\n"
  },
  {
    "repo": "swe-train/kubeflow__pipelines",
    "file_path": "samples/test/metrics_visualization_v2.py",
    "raw_url": "https://raw.githubusercontent.com/swe-train/kubeflow__pipelines/0.1.8/samples/test/metrics_visualization_v2.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport os\n\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import (\n    component,\n    Output,\n    ClassificationMetrics,\n    Metrics,\n    HTML,\n    Markdown\n)\n\n# In tests, we install a KFP package from the PR under test. Users should not\n# normally need to specify `kfp_package_path` in their component definitions.\n_KFP_PACKAGE_PATH = os.getenv('KFP_PACKAGE_PATH')\n\n\n@component(\n    packages_to_install=['sklearn'],\n    base_image='python:3.9',\n    kfp_package_path=_KFP_PACKAGE_PATH,\n)\ndef digit_classification(metrics: Output[Metrics]):\n    from sklearn import model_selection\n    from sklearn.linear_model import LogisticRegression\n    from sklearn import datasets\n    from sklearn.metrics import accuracy_score\n\n    # Load digits dataset\n    iris = datasets.load_iris()\n\n    # # Create feature matrix\n    X = iris.data\n\n    # Create target vector\n    y = iris.target\n\n    #test size\n    test_size = 0.33\n\n    seed = 7\n    #cross-validation settings\n    kfold = model_selection.KFold(n_splits=10, random_state=seed, shuffle=True)\n\n    #Model instance\n    model = LogisticRegression()\n    scoring = 'accuracy'\n    results = model_selection.cross_val_score(model, X, y, cv=kfold, scoring=scoring)\n\n    #split data\n    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=test_size, random_state=seed)\n    #fit model\n    model.fit(X_train, y_train)\n\n    #accuracy on test set\n    result = model.score(X_test, y_test)\n    metrics.log_metric('accuracy', (result*100.0))\n\n@component(\n    packages_to_install=['sklearn'],\n    base_image='python:3.9',\n    kfp_package_path=_KFP_PACKAGE_PATH,\n)\ndef wine_classification(metrics: Output[ClassificationMetrics]):\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.metrics import roc_curve\n    from sklearn.datasets import load_wine\n    from sklearn.model_selection import train_test_split, cross_val_predict\n\n    X, y = load_wine(return_X_y=True)\n    # Binary classification problem for label 1.\n    y = y == 1\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n    rfc = RandomForestClassifier(n_estimators=10, random_state=42)\n    rfc.fit(X_train, y_train)\n    y_scores = cross_val_predict(rfc, X_train, y_train, cv=3, method='predict_proba')\n    y_predict = cross_val_predict(rfc, X_train, y_train, cv=3, method='predict')\n    fpr, tpr, thresholds = roc_curve(y_true=y_train, y_score=y_scores[:,1], pos_label=True)\n    metrics.log_roc_curve(fpr, tpr, thresholds)\n\n@component(\n    packages_to_install=['sklearn'],\n    base_image='python:3.9',\n    kfp_package_path=_KFP_PACKAGE_PATH,\n)\ndef iris_sgdclassifier(test_samples_fraction: float, metrics: Output[ClassificationMetrics]):\n    from sklearn import datasets, model_selection\n    from sklearn.linear_model import SGDClassifier\n    from sklearn.metrics import confusion_matrix\n\n    iris_dataset = datasets.load_iris()\n    train_x, test_x, train_y, test_y = model_selection.train_test_split(\n        iris_dataset['data'], iris_dataset['target'], test_size=test_samples_fraction)\n\n\n    classifier = SGDClassifier()\n    classifier.fit(train_x, train_y)\n    predictions = model_selection.cross_val_predict(classifier, train_x, train_y, cv=3)\n    metrics.log_confusion_matrix(\n        ['Setosa', 'Versicolour', 'Virginica'],\n        confusion_matrix(train_y, predictions).tolist() # .tolist() to convert np array to list.\n    )\n\n@component(\n    kfp_package_path=_KFP_PACKAGE_PATH,\n)\ndef html_visualization(html_artifact: Output[HTML]):\n    html_content = '<!DOCTYPE html><html><body><h1>Hello world</h1></body></html>'\n    with open(html_artifact.path, 'w') as f:\n        f.write(html_content)\n\n@component(\n    kfp_package_path=_KFP_PACKAGE_PATH,\n)\ndef markdown_visualization(markdown_artifact: Output[Markdown]):\n    markdown_content = '## Hello world \\n\\n Markdown content'\n    with open(markdown_artifact.path, 'w') as f:\n        f.write(markdown_content)\n\n@dsl.pipeline(\n    name='metrics-visualization-pipeline')\ndef metrics_visualization_pipeline():\n    wine_classification_op = wine_classification()\n    iris_sgdclassifier_op = iris_sgdclassifier(test_samples_fraction=0.3)\n    digit_classification_op = digit_classification()\n    html_visualization_op = html_visualization()\n    markdown_visualization_op = markdown_visualization()\n"
  },
  {
    "repo": "swe-train/kubeflow__pipelines",
    "file_path": "samples/test/metrics_visualization_v2_test.py",
    "raw_url": "https://raw.githubusercontent.com/swe-train/kubeflow__pipelines/0.1.8/samples/test/metrics_visualization_v2_test.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport unittest.mock as mock\nfrom pprint import pprint\nimport kfp\nimport kfp_server_api\n\nfrom .metrics_visualization_v2 import metrics_visualization_pipeline\nfrom .util import run_pipeline_func, TestCase, KfpMlmdClient\nfrom ml_metadata.proto import Execution\n\n\ndef verify(run: kfp_server_api.ApiRun, mlmd_connection_config, **kwargs):\n    t = unittest.TestCase()\n    t.maxDiff = None  # we always want to see full diff\n    t.assertEqual(run.status, 'Succeeded')\n    client = KfpMlmdClient(mlmd_connection_config=mlmd_connection_config)\n    tasks = client.get_tasks(run_id=run.id)\n\n    task_names = [*tasks.keys()]\n    t.assertCountEqual(\n        task_names,\n        ['wine-classification', 'iris-sgdclassifier', 'digit-classification', 'html-visualization', 'markdown-visualization'],\n        'task names'\n    )\n\n    wine_classification = tasks['wine-classification']\n    iris_sgdclassifier = tasks['iris-sgdclassifier']\n    digit_classification = tasks['digit-classification']\n    html_visualization = tasks['html-visualization']\n    markdown_visualization = tasks['markdown-visualization']\n\n    pprint('======= wine classification task =======')\n    pprint(wine_classification.get_dict())\n    pprint('======= iris sgdclassifier task =======')\n    pprint(iris_sgdclassifier.get_dict())\n    pprint('======= digit classification task =======')\n    pprint(digit_classification.get_dict())\n    pprint('======= HTML task =======')\n    pprint(html_visualization.get_dict())\n    pprint('======= Markdown task =======')\n    pprint(markdown_visualization.get_dict())\n\n    t.assertEqual({\n        'inputs': {\n            'artifacts': [],\n            'parameters': {}\n        },\n        'name': 'wine-classification',\n        'outputs': {\n            'artifacts': [{\n                'metadata': {\n                    'display_name': 'metrics',\n                    'confidenceMetrics': {\n                        'list': [{\n                            'confidenceThreshold': 2.0,\n                            'falsePositiveRate': 0.0,\n                            'recall': 0.0\n                        }, {\n                            'confidenceThreshold': 1.0,\n                            'falsePositiveRate': 0.0,\n                            'recall': 0.33962264150943394\n                        }, {\n                            'confidenceThreshold': 0.9,\n                            'falsePositiveRate': 0.0,\n                            'recall': 0.6037735849056604\n                        }, {\n                            'confidenceThreshold': 0.8,\n                            'falsePositiveRate': 0.0,\n                            'recall': 0.8490566037735849\n                        }, {\n                            'confidenceThreshold': 0.6,\n                            'falsePositiveRate': 0.0,\n                            'recall': 0.8867924528301887\n                        }, {\n                            'confidenceThreshold': 0.5,\n                            'falsePositiveRate': 0.0125,\n                            'recall': 0.9245283018867925\n                        }, {\n                            'confidenceThreshold': 0.4,\n                            'falsePositiveRate': 0.075,\n                            'recall': 0.9622641509433962\n                        }, {\n                            'confidenceThreshold': 0.3,\n                            'falsePositiveRate': 0.0875,\n                            'recall': 1.0\n                        }, {\n                            'confidenceThreshold': 0.2,\n                            'falsePositiveRate': 0.2375,\n                            'recall': 1.0\n                        }, {\n                            'confidenceThreshold': 0.1,\n                            'falsePositiveRate': 0.475,\n                            'recall': 1.0\n                        }, {\n                            'confidenceThreshold': 0.0,\n                            'falsePositiveRate': 1.0,\n                            'recall': 1.0\n                        }]\n                    }\n                },\n                'name': 'metrics',\n                'type': 'system.ClassificationMetrics'\n            }],\n            'parameters': {}\n        },\n        'type': 'system.ContainerExecution',\n        'state': Execution.State.COMPLETE,\n    }, wine_classification.get_dict())\n    t.assertEqual(\n        {\n            'inputs': {\n                'artifacts': [],\n                'parameters': {\n                    'test_samples_fraction': 0.3\n                }\n            },\n            'name': 'iris-sgdclassifier',\n            'outputs': {\n                'artifacts': [{\n                    'metadata': {\n                        'display_name': 'metrics',\n                        'confusionMatrix': {'struct': {\n                            'annotationSpecs': [{\n                                'displayName': 'Setosa'\n                            }, {\n                                'displayName': 'Versicolour'\n                            }, {\n                                'displayName': 'Virginica'\n                            }],\n                            'rows': [{ # these numbers can be random during execution\n                                'row': [mock.ANY, mock.ANY, mock.ANY]\n                            }, {\n                                'row': [mock.ANY, mock.ANY, mock.ANY]\n                            }, {\n                                'row': [mock.ANY, mock.ANY, mock.ANY]\n                            }]\n                        }}\n                    },\n                    'name': 'metrics',\n                    'type': 'system.ClassificationMetrics'\n                }],\n                'parameters': {}\n            },\n            'type': 'system.ContainerExecution',\n            'state': Execution.State.COMPLETE,\n        }, iris_sgdclassifier.get_dict()\n    )\n    rows = iris_sgdclassifier.get_dict()['outputs']['artifacts'][0]['metadata'][\n        'confusionMatrix']['struct']['rows']\n    for i, row in enumerate(rows):\n        for j, item in enumerate(row['row']):\n            t.assertIsInstance(\n                item, float,\n                f'value of confusion matrix row {i}, col {j} is not a number'\n            )\n\n    t.assertEqual({\n        'inputs': {\n            'artifacts': [],\n            'parameters': {}\n        },\n        'name': 'digit-classification',\n        'outputs': {\n            'artifacts': [{\n                'metadata': {\n                    'display_name': 'metrics',\n                    'accuracy': 92.0,\n                },\n                'name': 'metrics',\n                'type': 'system.Metrics'\n            }],\n            'parameters': {}\n        },\n        'type': 'system.ContainerExecution',\n        'state': Execution.State.COMPLETE,\n    }, digit_classification.get_dict())\n\n    t.assertEqual({\n        'inputs': {\n            'artifacts': [], \n            'parameters': {}\n        },\n        'name': 'html-visualization',\n        'outputs': {\n            'artifacts': [{\n                'metadata': {\n                    'display_name': 'html_artifact'\n                },\n                'name': 'html_artifact',\n                'type': 'system.HTML'\n            }],\n            'parameters': {}\n        },\n        'state': Execution.State.COMPLETE,\n        'type': 'system.ContainerExecution'\n    }, html_visualization.get_dict())\n\n    t.assertEqual({\n        'inputs': {\n            'artifacts': [], \n            'parameters': {}\n        },\n        'name': 'markdown-visualization',\n        'outputs': {\n            'artifacts': [{\n                'metadata': {\n                    'display_name': 'markdown_artifact'\n                },\n                'name': 'markdown_artifact',\n                'type': 'system.Markdown'\n            }],\n            'parameters': {}\n        },\n        'state': Execution.State.COMPLETE,\n        'type': 'system.ContainerExecution'\n    }, markdown_visualization.get_dict())\n\n\nrun_pipeline_func([\n    TestCase(\n        pipeline_func=metrics_visualization_pipeline,\n        verify_func=verify,\n        mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE\n    ),\n    TestCase(\n        pipeline_func=metrics_visualization_pipeline,\n        verify_func=verify,\n        mode=kfp.dsl.PipelineExecutionMode.V2_ENGINE\n    ),\n])\n"
  },
  {
    "repo": "swe-train/kubeflow__pipelines",
    "file_path": "samples/test/parameter_with_format.py",
    "raw_url": "https://raw.githubusercontent.com/swe-train/kubeflow__pipelines/0.1.8/samples/test/parameter_with_format.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom kfp import components\nfrom kfp import dsl\n\n\n@components.create_component_from_func\ndef print_op(name: str) -> str:\n    print(name)\n    return name\n\n\n@dsl.pipeline(name='pipeline-with-pipelineparam-containing-format')\ndef my_pipeline(name: str = 'KFP'):\n    print_task = print_op('Hello {}'.format(name))\n    print_op('{}, again.'.format(print_task.output))\n"
  },
  {
    "repo": "swe-train/kubeflow__pipelines",
    "file_path": "samples/test/parameter_with_format_test.py",
    "raw_url": "https://raw.githubusercontent.com/swe-train/kubeflow__pipelines/0.1.8/samples/test/parameter_with_format_test.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp\nfrom .parameter_with_format import my_pipeline\nfrom .util import run_pipeline_func, TestCase\n\n\ndef verify(run, run_id: str, **kwargs):\n    assert run.status == 'Succeeded'\n    # TODO(Bobgy): verify output\n\n\nrun_pipeline_func([\n    TestCase(\n        pipeline_func=my_pipeline,\n        verify_func=verify,\n    ),\n    TestCase(\n        pipeline_func=my_pipeline,\n        verify_func=verify,\n        mode=kfp.dsl.PipelineExecutionMode.V2_ENGINE,\n    ),\n    TestCase(\n        pipeline_func=my_pipeline,\n        mode=kfp.dsl.PipelineExecutionMode.V1_LEGACY,\n    ),\n])\n"
  },
  {
    "repo": "swe-train/kubeflow__pipelines",
    "file_path": "samples/test/placeholder_concat.py",
    "raw_url": "https://raw.githubusercontent.com/swe-train/kubeflow__pipelines/0.1.8/samples/test/placeholder_concat.py",
    "content": "# Copyright 2020 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom kfp import components\nfrom kfp.v2 import dsl\nimport kfp.v2.compiler as compiler\n\ncomponent_op = components.load_component_from_text(\"\"\"\nname: Component with concat placeholder\ninputs:\n- {name: input_one, type: String}\n- {name: input_two, type: String}\nimplementation:\n  container:\n    image: gcr.io/google-containers/busybox\n    command:\n    - sh\n    - -ec\n    args:\n    - echo \"$0\" > /tmp/test && [[ \"$0\" == 'one+two=three' ]]\n    - concat: [{inputValue: input_one}, '+', {inputValue: input_two}, '=three']\n\"\"\")\n\n\n@dsl.pipeline(name='one-step-pipeline-with-concat-placeholder')\ndef pipeline_with_concat_placeholder():\n  component = component_op(input_one='one', input_two='two')\n"
  },
  {
    "repo": "smk508/kung-fu-pipelines",
    "file_path": "kungfupipelines/workflow.py",
    "raw_url": "https://raw.githubusercontent.com/smk508/kung-fu-pipelines/master/kungfupipelines/workflow.py",
    "content": "import abc\nimport kfp\nfrom kfp import components, dsl, gcp\nimport wrapt\nfrom typing import Callable, List\nfrom kungfupipelines.step import Step\nfrom kungfupipelines.cli import StepSwitch\n\ndef make_sequence(ops: List[dsl.ContainerOp]) -> None:\n    \"\"\" \n    Links a sequence of pipeline operations so that they are configured\n    to take place one after another.\n    Args:\n        ops - list[dsl.ContainerOp]\n    \"\"\"\n    l = len(ops)\n    if l <= 1:\n        return\n    i = 0\n    before = ops[i]\n    for op in ops[1:]:\n        op.after(before)\n        before = op\n    \nclass Workflow(abc.ABC):\n    \"\"\"\n    A Workflow abstracts the connectivity structure between pipeline steps. Individual steps\n    can be provided to a Workflow, and they will then be compiled together so that they happen\n    in a predefined sequence specified by that Workflow. This makes is easy to have 'standardized'\n    pipeline structures in which you can simply swap out individual steps as needed.\n    For example, a machine learning workflow might have slots for dataset preprocessing, training,\n    hyperparameter optimization, etc. You can provide specific pipeline steps to fill in those slots\n    and generate the pipeline spec without having to rewrite the connectivity structure each time.\n    \"\"\"\n    @abc.abstractmethod\n    def compile(self, image:str, script_path:str) -> Callable:\n        \"\"\" \n        Generates a kfp Pipeline spec which can be used to generate an \n        Argo workflow.yaml\n        Args:\n            image: The uri for the image to use.\n            script_path: The path that your script is located in in the image.\n        \"\"\"\n        pass\n\n    def generate_yaml(self, filename, *compile_args):\n        \"\"\"\n        Generates an argo workflow.yaml spec which can be used to submit this\n        workflow to Argo / Kubeflow.\n        \"\"\"\n        pipeline = self.compile(*compile_args)\n        kfp.compiler.Compiler().compile(pipeline, filename)\n\nclass BasicMLWorkflow(Workflow):\n    \"\"\"\n    This specifies a simple pipeline for machiine learning. It consists of the following steps \n    taking place one after another:\n    1) Create/download/acquire the master dataset\n    2) Perform a train/test split\n    3) Apply any preprocessing logic\n    4) Train your model\n    5) Apply any post processing logic using the test set, including computing accuracy, ROC, etc.\n\n    Args:\n        name: Name to use for the compiled pipeline\n        image: Docker container URI containing all of the scripts\n        script_path: Path to script that runs the steps\n        make_dataset: The Step to use for making the dataset\n        train_test_split: The Step to use for train_test_split\n        preprocess: The Step to use for preprocessing\n        train: The Step to use for model training\n        postprocess_ops: A list of Steps to perform post-training operations with\n        description: An optional string describing your pipeline\n    \"\"\"\n    def __init__(\n        self,\n        name: str,\n        image: str,\n        script_path: str,\n        make_dataset: Step,\n        train_test_split: Step,\n        preprocess: Step,\n        train: Step,\n        postprocess_ops: List[Step] = [],\n        description: str = None,\n    ):\n        self.make_dataset = make_dataset\n        self.train_test_split = train_test_split\n        self.preprocess = preprocess\n        self.train = train\n        self.postprocess_ops = postprocess_ops\n        self.image = image\n        self.script_path = script_path\n\n    def compile(self):\n        \n        def pipeline(*args, **kwargs):\n\n            make_dataset_op = self.make_dataset.dslContainerOp(self.image, self.script_path, **kwargs)\n            train_test_split_op = self.train_test_split.dslContainerOp(self.image, self.script_path, **kwargs)\n            train_op = self.train.dslContainerOp(\n                self.image, self.script_path, **kwargs\n                ).set_gpu_limit(1).add_toleration({\n                    'key': 'nvidia.com/gpu',\n                    'operator': 'Equal',\n                    'value': 'present',\n                    'effect': 'NoSchedule'\n                    })\n            postprocess_ops = [\n                pp.dslContainerOp(self.image, self.script_path, **kwargs)\n                for pp in self.postprocess_ops\n            ]\n\n            make_sequence([make_dataset_op, train_test_split_op, train_op])\n            for pp in postprocess_ops:\n                pp.after(train_op)\n\n        return pipeline\n\nclass SequentialWorkflow(Workflow):\n    \n    def __init__(self, name: str, steps: List[Step]):\n        self.steps = steps\n        self.step_switch = StepSwitch(name, steps)\n\n    def compile(self, image: str):\n\n        def pipeline(*args, **kwargs):\n            ops = []\n            for step in self.steps:\n                op = step.dslContainerOp(image)\n            \n            make_sequence(ops)\n\n        return pipeline\n\n\ndef Pipeline(pipeline_func: Callable, name:str, description:str=''): # NOTE: This does not work\n\n    @dsl.pipeline(name=name, description=description)\n    @wrapt.decorator\n    def wrapper(wrapped, instance, args, kwargs):\n        return pipeline_func(*args, **kwargs)\n    \n    return wrapper \n"
  },
  {
    "repo": "duttab49/kubeflow-pipelines",
    "file_path": "add_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/duttab49/kubeflow-pipelines/main/add_pipeline.py",
    "content": "from asyncio.windows_events import NULL\r\nimport kfp.dsl as dsl\r\nimport kfp.compiler as compiler\r\nimport kfp.components as comp\r\n\r\n\r\ndef add(a: float, b: float) -> float:\r\n  '''Calculates sum of two arguments'''\r\n  return a + b\r\n\r\nadd_op = comp.create_component_from_func(\r\n    add, output_component_file='add_component.yaml')\r\n\r\ndef echo_out(result: float) -> NULL:\r\n    print(\"The result is :\", result)\r\n\r\necho_op = comp.create_component_from_func(echo_out)\r\n\r\n\r\n@dsl.pipeline(\r\n    name='add_pipeline',\r\n    description='sample pipeline to add two numbers and then print the result.'\r\n)\r\n\r\ndef add_pipeline(\r\n  a='1',\r\n  b='7',\r\n):\r\n  # Passes a pipeline parameter and a constant value to the `add_op` factory\r\n  # function.\r\n  first_add_task = add_op(a, 4)\r\n  # Passes an output reference from `first_add_task` and a pipeline parameter\r\n  # to the `add_op` factory function. For operations with a single return\r\n  # value, the output reference can be accessed as `task.output` or\r\n  # `task.outputs['output_name']`.\r\n  second_add_task = add_op(first_add_task.output, b)\r\n\r\n  # Print the results\r\n  third_echo_task = echo_op(second_add_task.output)\r\n\r\nif __name__ == '__main__':\r\n    compiler.Compiler().compile(add_pipeline, __file__ + '.yaml')"
  },
  {
    "repo": "dhiebtarak/kubeflow_pipeline",
    "file_path": "kubeflow_pipeline_test.py",
    "raw_url": "https://raw.githubusercontent.com/dhiebtarak/kubeflow_pipeline/master/kubeflow_pipeline_test.py",
    "content": "from typing import Dict, List\nfrom kfp import dsl\nfrom kfp import compiler\nimport kfp\nfrom client_manager import KFPClientManager\nfrom kfp.dsl import Input, Output, Dataset, Model, component\n# Preprocessing Function for Prediction \ndef preprocess_input(raw_input: str) -> str:\n    \"\"\"\n    Preprocess raw input (e.g., TrdCaptRpt string) to match input_message_clean format.\n    Args:\n        raw_input (str): Raw input string, e.g., concatenated TrdCaptRpt messages.\n    Returns:\n        str: Cleaned input string ready for model prediction.\n    \"\"\"\n    import re\n    # Remove XML-like tags and special characters\n    pattern = r'[<\"/]'\n    cleaned_text = re.sub(pattern, '', raw_input)\n    cleaned_text = cleaned_text.replace('>', ' ')\n    # Remove extra whitespace\n    cleaned_text = ' '.join(cleaned_text.split())\n    return cleaned_text\n\n# Step 1: Generate Dataset\n@component(base_image=\"tarakdhieb7/kubeflow-testing:latest\")\ndef generate_data(output_csv: Output[Dataset], execution_id: str = \"\"):\n    import subprocess\n    subprocess.run([\"pip\", \"install\", \"minio\"], check=True)\n    import random\n    import pandas as pd\n    import numpy as np\n    from minio import Minio\n    from minio.error import S3Error\n    import os\n    import uuid\n\n    # Initialize minio client\n    minio_client = Minio(\n        \"10.106.67.253:9000\",\n        access_key=\"minio\",  # Replace with your minio access key\n        secret_key=\"minio123\",  # Replace with your minio secret key\n        secure=False\n    )\n\n    # Ensure bucket exists\n    bucket_name = \"mlpipeline\"\n    if not minio_client.bucket_exists(bucket_name):\n        minio_client.make_bucket(bucket_name)\n\n    def generate_full_service_wf():\n        rptid = random.randint(1000, 400000)\n        trdid = random.randint(20281004800, 40281004800)\n        trdtyp = random.choice([0, 1, 2, 3, 11, 15, 20, 29, 31, 45])\n        mtchid = random.randint(20281001777, 40281001777)\n        pxtyp = random.choice([2, 10])\n        lastqty = random.randint(10, 100)\n        lastpx = random.choice([100, 370, 876, 250.45, 300.9, 540.698, 1050, 3590.59])\n        trdt = random.choice([\"2022-12-17\", \"2022-12-21\", \"2023-01-02\", \"2023-01-05\"])\n        bizdt = random.choice([\"2023-01-06\", \"2023-01-11\", \"2023-01-20\", \"2023-02-05\", \"2023-02-10\"])\n        t = random.choice([\"2023-02-15\", \"2023-02-21\", \"2023-03-20\", \"2023-03-30\"])\n        time1 = random.choice(['T05:17:30', 'T10:18:15', 'T12:25:13', 'T16:40:55'])\n        r1 = random.choice(['.100', '.230', '.326'])\n        r2 = random.choice(['.350', '.470', '.768'])\n        tz1 = random.choice(['-5:00', '-6:00', '-7:00'])\n        txntm = t + time1 + r1 + tz1\n        time2 = random.choice(['T17:30:55.600', 'T19:33:05.333', 'T22:03:12.436'])\n        snt1 = t + time1 + r2 + tz1\n        snt2 = t + time2 + '+01:00'\n        snt3 = t + 'T00:12:55.138' + tz1\n        snt4 = t + 'T00:12:55.160' + tz1\n        sym = random.choice([\"BUI\", \"ABP\", \"TMP\"])\n        id5 = random.choice(['B', 'W', 'AS', 'LM', 'BUI', 'IP0'])\n        my = random.choice([\"2023-05\", \"2023-07\", \"2023-10\"])\n        mmy = my.replace('-', '')\n        matdt = my + '-' + str(random.randint(1, 28))\n        inptsrc = random.choice(['MA', 'EL', 'SY'])\n        clordid = random.choice(['123', '134', '145', '146ZBT', '245BTN', '987NL', 'OrdAT'])\n        tid = random.choice(['ABCD', 'D210', 'B509', 'HBPL', 'TL98'])\n        id2 = random.choice(['Emira-115', 'Ahlem-110', 'Karim-200', 'Mohamed-300', 'acc-AT', 'bl-NL', 'Mariem', 'ahmed'])\n        typ = random.randint(20, 30)\n        id3 = random.choice([\"FIRMACT1\", \"AKLPM9\", \"MPLAMP6\", \"00120007\", \"Hassen\", \"Ahlem\", 'arij'])\n        custcpcty1 = random.randint(1, 4)\n        custcpcty2 = random.randint(1, 4)\n        ex = random.choice(['BTNL', 'SNTL', 'HDPL', 'XMGE'])\n        mult = random.choice([0.1, 0.2, 0.3, 0.4, 0.9, 1000, 2000, 5000, 300, 200, 50, 70])\n        pos1 = random.choice(['Y', 'N'])\n        pos3 = random.choice(['Y', 'N'])\n        cfi = random.choice(['FCCPXX', 'APYBKL', 'MKLPON', 'FCAPSX'])\n        pos2 = random.choice(['O', 'C'])\n        side = random.randint(1, 2)\n        id26 = random.randint(1, 2)\n        mlrty = random.randint(1, 3)\n        alin = random.randint(0, 1)\n        inpdev = random.choice(['PORTAL', 'EXCHANGE', 'API', 'CLEARING'])\n        id4 = random.randint(100, 500)\n        id12 = random.choice(['100', '200', '400', 'A123', 'B115'])\n        cfi2 = random.choice(['FCCPXX', 'APYBKL', 'MKLPON', 'FCAPSX'])\n        return (\n            f\"\"\"<TrdCaptRpt RptID=\"{rptid}\" TrdID=\"{trdid}\" TransTyp=\"0\" RptTyp=\"2\" TrdTyp=\"{trdtyp}\" MtchID=\"{mtchid}\" PxTyp=\"{pxtyp}\" LastQty=\"{lastqty}\" LastPx=\"{lastpx}\" TrdDt=\"{trdt}\" BizDt=\"{bizdt}\" TxnTm=\"{txntm}\"><Hdr SID=\"MGEX\" TID=\"{tid}\" PosDup=\"{pos1}\" PosRsnd=\"{pos3}\" Snt=\"{snt1}\"></Hdr>\n        <Instrmt Sym=\"{sym}\" ID=\"{id5}\" Src=\"H\" CFI=\"{cfi}\" MMY=\"{mmy}\" MatDt=\"{matdt}\" Mult=\"{mult}\" Exch=\"{ex}\"></Instrmt><RptSide Side=\"{side}\" Ccy=\"USD\" InptSrc=\"{inptsrc}\" InptDev=\"{inpdev}\" CustCpcty=\"{custcpcty1}\" PosEfct=\"{pos2}\" ClOrdID=\"{clordid}\" MLegRptTyp=\"{mlrty}\" AllocInd=\"{alin}\"><Pty ID=\"MGEX\" R=\"21\"></Pty><Pty ID=\"210\" R=\"1\"></Pty><Pty ID=\"{id4}\" R=\"4\"></Pty>\n        <Pty ID=\"{id2}\" R=\"24\"><Sub ID=\"{id26}\" Typ=\"26\"></Sub></Pty><Pty ID=\"{id12}\" R=\"12\"></Pty><TrdRegTS TS=\"{txntm}\" Typ=\"1\">\n        </TrdRegTS></RptSide></TrdCaptRpt>\"\"\".replace('\\n', ''),\n            f\"\"\"<TrdCaptRpt RptID=\"{trdid}\" TrdDt=\"{trdt}\" BizDt=\"{bizdt}\" TxnTm=\"{txntm}\" TrdID=\"{trdid}\" TransTyp=\"2\" RptTyp=\"0\" LastQty=\"{float(lastqty)}\" LastPx=\"{float(lastpx)}\"><Hdr Snt=\"{snt2}\" TID=\"MGEX\" SID=\"{tid}\"/><Instrmt Exch=\"{ex}\" ID=\"{id5}\" MMY=\"{mmy}\" CFI=\"{cfi2}\"/><RptSide Side=\"{side}\" CustCpcty=\"{custcpcty2}\">\n        <Pty ID=\"210\" R=\"1\"/><Pty ID=\"{id3}\" R=\"24\"><Sub ID=\"{id26}\" Typ=\"26\"/></Pty></RptSide></TrdCaptRpt>\"\"\".replace('\\n', ''),\n            f\"\"\"<TrdCaptRpt RptID=\"580198\" TrdID=\"{trdid}\" TransTyp=\"2\" RptTyp=\"2\" TrdTyp=\"{trdtyp}\" MtchID=\"{mtchid}\" PxTyp=\"{pxtyp}\" LastQty=\"{lastqty}\" LastPx=\"{lastpx}\" TrdDt=\"{trdt}\" BizDt=\"{bizdt}\" TxnTm=\"{snt3}\"><Hdr SID=\"MGEX\" TID=\"{tid}\" PosDup=\"{pos1}\" PosRsnd=\"{pos3}\" Snt=\"{snt4}\"></Hdr>\n        <Instrmt Sym=\"{sym}\" ID=\"{id5}\" Src=\"H\" CFI=\"{cfi}\" MMY=\"{mmy}\" MatDt=\"{matdt}\" Mult=\"{mult}\" Exch=\"{ex}\"></Instrmt><RptSide Side=\"{side}\" Ccy=\"USD\" InptSrc=\"{inptsrc}\" InptDev=\"{inpdev}\" CustCpcty=\"{custcpcty2}\" PosEfct=\"{pos2}\" ClOrdID=\"{clordid}\" MLegRptTyp=\"{mlrty}\" AllocInd=\"{alin}\"><Pty ID=\"MGEX\" R=\"21\"></Pty><Pty ID=\"210\" R=\"1\"></Pty><Pty ID=\"{id4}\" R=\"4\"></Pty>\n        <Pty ID=\"{id3}\" R=\"24\"><Sub ID=\"{id26}\" Typ=\"26\"></Sub></Pty><Pty ID=\"{id12}\" R=\"12\"></Pty><TrdRegTS TS=\"{txntm}\" Typ=\"1\"></TrdRegTS></RptSide>\n        </TrdCaptRpt>\"\"\".replace('\\n', '')\n        )\n\n    dataset = [generate_full_service_wf() for _ in range(500)]\n    ccp_new_trade_messages = [message[0] for message in dataset]\n    sgw_fs_operation_messages = [message[1] for message in dataset]\n    ccp_response_messages = [message[2] for message in dataset]\n    df = pd.DataFrame({\n        'Executed Trade': ccp_new_trade_messages,\n        'Full Service Operation': sgw_fs_operation_messages,\n        'CCP Response': ccp_response_messages\n    })\n    df['input_message'] = df['Executed Trade'] + ' ' + df['Full Service Operation']\n    df.drop(['Executed Trade', 'Full Service Operation'], axis=1, inplace=True)\n    df.to_csv(output_csv.path, index=False)\n    print(output_csv.path)\n    # Upload to minio\n    execution_id = execution_id if execution_id else str(uuid.uuid4())\n    object_name = f\"{execution_id}/generate_data/output.csv\"\n    try:\n        minio_client.fput_object(bucket_name, object_name, output_csv.path)\n        print(f\"Uploaded {object_name} to minio bucket {bucket_name}\")\n    except S3Error as e:\n        print(f\"Failed to upload to minio: {e}\")\n\n# Step 2: Preprocess Data\n@component(base_image=\"tarakdhieb7/kubeflow-testing:latest\")\ndef preprocess_data(input_csv: Input[Dataset], output_train: Output[Dataset], output_val: Output[Dataset], execution_id: str = \"\"):\n    import subprocess\n    subprocess.run([\"pip\", \"install\", \"minio\"], check=True)\n    import re\n    import pandas as pd\n    from minio import Minio\n    from minio.error import S3Error\n    import uuid\n\n    # Initialize minio client\n    minio_client = Minio(\n        \"10.106.67.253:9000\",\n        access_key=\"minio\",  # Replace with your minio access key\n        secret_key=\"minio123\",  # Replace with your minio secret key\n        secure=False\n    )\n\n    # Ensure bucket exists\n    bucket_name = \"mlpipeline\"\n    if not minio_client.bucket_exists(bucket_name):\n        minio_client.make_bucket(bucket_name)\n\n    df = pd.read_csv(input_csv.path)\n    def clean_text(input_message):\n        pattern = r'[<\"/]'\n        cleaned_text = re.sub(pattern, '', input_message)\n        cleaned_text = cleaned_text.replace('>', ' ')\n        cleaned_text = ' '.join(cleaned_text.split())\n        return cleaned_text\n\n    df['input_message_clean'] = df['input_message'].apply(clean_text)\n    df.drop(['input_message'], axis=1, inplace=True)\n    train_data = df.iloc[:int(len(df) * 0.9)]\n    val_data = df.iloc[int(len(df) * 0.9):]\n    print(\"Shapes after train-val split:\")\n    print(\"train_df:\", train_data.shape)\n    print(\"val_df:\", val_data.shape)\n    train_data.to_csv(output_train.path, index=False)\n    val_data.to_csv(output_val.path, index=False)\n    print(\"Preprocessed data saved to:\", output_train.path, output_val.path)\n\n    # Upload to minio\n    execution_id = execution_id if execution_id else str(uuid.uuid4())\n    for output, name in [(output_train, \"output_train.csv\"), (output_val, \"output_val.csv\")]:\n        object_name = f\"{execution_id}/preprocess_data/{name}\"\n        try:\n            minio_client.fput_object(bucket_name, object_name, output.path)\n            print(f\"Uploaded {object_name} to minio bucket {bucket_name}\")\n        except S3Error as e:\n            print(f\"Failed to upload to minio: {e}\")\n\n# Step 3: Train Model\n@component(base_image=\"tarakdhieb7/kubeflow-testing:latest\")\ndef train_model(train_data: Input[Dataset], val_data: Input[Dataset], model_output: Output[Model], execution_id: str = \"\"):\n    import subprocess\n    subprocess.run([\"pip\", \"install\", \"minio\"], check=True)\n\n    import torch\n    import pandas as pd\n    from transformers import GPT2Tokenizer, AutoModelForCausalLM\n    from torch.utils.data import Dataset, DataLoader\n    import time\n    import os\n    from minio import Minio\n    from minio.error import S3Error\n    import uuid\n\n    # Initialize minio client\n    minio_client = Minio(\n        \"10.106.67.253:9000\",\n        access_key=\"minio\",  # Replace with your minio access key\n        secret_key=\"minio123\",  # Replace with your minio secret key\n        secure=False\n    )\n\n    # Ensure bucket exists\n    bucket_name = \"mlpipeline\"\n    if not minio_client.bucket_exists(bucket_name):\n        minio_client.make_bucket(bucket_name)\n\n    os.environ['CURL_CA_BUNDLE'] = ''\n    os.environ['REQUESTS_CA_BUNDLE'] = ''\n\n    class CustomDataset(Dataset):\n        def __init__(self, data, tokenizer, max_length):\n            self.data = data\n            self.tokenizer = tokenizer\n            self.max_length = max_length\n\n        def __len__(self):\n            return len(self.data)\n\n        def __getitem__(self, index):\n            input_seq = self.data.iloc[index]['input_message_clean']\n            target_seq = self.data.iloc[index]['CCP Response']\n            concat_seq = f\"{input_seq} {target_seq}\"\n            tokenized_seq = self.tokenizer.encode(\n                concat_seq,\n                return_tensors='pt',\n                max_length=self.max_length,\n                truncation=True,\n                padding='max_length'\n            )\n            return tokenized_seq.squeeze(0)\n\n    # Load data\n    train_df = pd.read_csv(train_data.path)\n    val_df = pd.read_csv(val_data.path)\n    print(\"Shape of train_df:\", train_df.shape)\n    print(\"Shape of val_df:\", val_df.shape)\n\n    # Initialize tokenizer and model\n    tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2', low_cpu_mem_usage=True)\n    tokenizer.pad_token = tokenizer.eos_token\n    model = AutoModelForCausalLM.from_pretrained('distilgpt2')\n\n    # Create datasets\n    max_length = 512  # Reduced for CPU\n    batch_size = 1\n    train_dataset = CustomDataset(train_df, tokenizer, max_length)\n    val_dataset = CustomDataset(val_df, tokenizer, max_length)\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n\n    # Move to device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n    model.to(device)\n\n    # Train\n    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n    epochs = 1\n    start_time = time.time()\n    model.train()\n    total_loss = 0\n    for i, batch in enumerate(train_loader):\n        input_ids = batch.to(device)\n        outputs = model(input_ids=input_ids, labels=input_ids)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        total_loss += loss.item()\n        print(f\"Batch {i+1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n    avg_loss = total_loss / len(train_loader)\n    print(f\"Average Training Loss: {avg_loss:.4f}\")\n    print(f\"Training Time: {(time.time() - start_time)/60:.2f} minutes\")\n\n    # Save model and tokenizer\n    model.save_pretrained(model_output.path)\n    tokenizer.save_pretrained(model_output.path)\n\n    # Upload model files to minio\n    execution_id = execution_id if execution_id else str(uuid.uuid4())\n    model_files = [\n        \"config.json\",\n        \"pytorch_model.bin\",\n        \"vocab.json\",\n        \"merges.txt\",\n        \"tokenizer_config.json\",\n        \"special_tokens_map.json\"\n    ]\n    for file in model_files:\n        file_path = os.path.join(model_output.path, file)\n        if os.path.exists(file_path):\n            object_name = f\"{execution_id}/train_model/{file}\"\n            try:\n                minio_client.fput_object(bucket_name, object_name, file_path)\n                print(f\"Uploaded {object_name} to minio bucket {bucket_name}\")\n            except S3Error as e:\n                print(f\"Failed to upload to minio: {e}\")\n\n@component(base_image=\"tarakdhieb7/kubeflow-testing:latest\")\ndef train_model_distributed(train_data: Input[Dataset], val_data: Input[Dataset], model_output: Output[Model], execution_id: str = \"\"):\n    import subprocess\n    subprocess.run([\"pip\", \"install\", \"minio\"], check=True)\n    import os\n    import torch\n    import torch.distributed as dist\n    import pandas as pd\n    from torch.utils.data import Dataset, DataLoader\n    from transformers import GPT2Tokenizer, AutoModelForCausalLM\n    from torch.nn.parallel import DistributedDataParallel as DDP\n    import logging\n    import time\n    import sys\n    from minio import Minio\n    from minio.error import S3Error\n    import uuid\n\n    # Initialize minio client\n    minio_client = Minio(\n        \"10.106.67.253:9000\",\n        access_key=\"minio\",  # Replace with your minio access key\n        secret_key=\"minio123\",  # Replace with your minio secret key\n        secure=False\n    )\n\n    # Ensure bucket exists\n    bucket_name = \"mlpipeline\"\n    if not minio_client.bucket_exists(bucket_name):\n        minio_client.make_bucket(bucket_name)\n\n# Set up logging\n    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n    logger = logging.getLogger(__name__)\n    logger.info(f\"Starting train_model_distributed, execution_id: {execution_id}\")\n    logger.info(f\"Model output path: {model_output.path}\")\n\n    os.environ['CURL_CA_BUNDLE'] = ''\n    os.environ['REQUESTS_CA_BUNDLE'] = ''\n\n    class CustomDataset(Dataset):\n        def __init__(self, data, tokenizer, max_length):\n            self.data = data\n            self.tokenizer = tokenizer\n            self.max_length = max_length\n\n        def __len__(self):\n            return len(self.data)\n\n        def __getitem__(self, index):\n            input_seq = self.data.iloc[index]['input_message_clean']\n            target_seq = self.data.iloc[index]['CCP Response']\n            concat_seq = f\"{input_seq} {target_seq}\"\n            tokenized_seq = self.tokenizer.encode(\n                concat_seq,\n                return_tensors='pt',\n                max_length=self.max_length,\n                truncation=True,\n                padding='max_length'\n            )\n            return tokenized_seq.squeeze(0)\n\n    def setup_ddp(rank, world_size):\n        try:\n            os.environ['MASTER_ADDR'] = 'localhost'\n            os.environ['MASTER_PORT'] = '12345'\n            dist.init_process_group(backend='gloo', rank=rank, world_size=world_size)\n            logger.info(f\"DDP initialized for rank {rank}/{world_size}\")\n        except Exception as e:\n            logger.error(f\"DDP setup failed: {e}\")\n            raise\n\n    def train(rank, world_size, train_loader, model, optimizer, device, epochs):\n        try:\n            setup_ddp(rank, world_size)\n            model = model.to(device)\n            model = DDP(model, device_ids=None)\n            model.train()\n            for epoch in range(epochs):\n                total_loss = 0\n                for i, batch in enumerate(train_loader):\n                    input_ids = batch.to(device)\n                    outputs = model(input_ids=input_ids, labels=input_ids)\n                    loss = outputs.loss\n                    loss.backward()\n                    optimizer.step()\n                    optimizer.zero_grad()\n                    total_loss += loss.item()\n                    if rank == 0:\n                        logger.info(f\"Epoch {epoch+1}/{epochs}, Batch {i+1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n                avg_loss = total_loss / len(train_loader)\n                if rank == 0:\n                    logger.info(f\"Epoch {epoch+1}/{epochs}, Average Loss: {avg_loss:.4f}\")\n            dist.destroy_process_group()\n        except Exception as e:\n            logger.error(f\"Training failed on rank {rank}: {e}\")\n            dist.destroy_process_group()\n            raise\n\n    # Function to initialize the dataset and model (called in each process)\n    def initialize_training_components():\n        # Load data\n        train_df = pd.read_csv(train_data.path)\n        logger.info(f\"Shape of train_df: {train_df.shape}\")\n\n        # Initialize tokenizer and model\n        tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2', low_cpu_mem_usage=True)\n        tokenizer.pad_token = tokenizer.eos_token\n        model = AutoModelForCausalLM.from_pretrained('distilgpt2')\n\n        # Create dataset\n        max_length = 512\n        batch_size = 2\n        train_dataset = CustomDataset(train_df, tokenizer, max_length)\n        train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\n        train_loader = DataLoader(\n            train_dataset,\n            batch_size=batch_size,\n            shuffle=False,\n            sampler=train_sampler\n        )\n\n        # Training setup\n        device = torch.device('cpu')\n        optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n        epochs = 1\n\n        return train_loader, model, optimizer, device, epochs, tokenizer\n\n    def distributed_training():\n        try:\n            # Get rank and world_size from environment (set by torchrun)\n            rank = int(os.environ[\"RANK\"])\n            world_size = int(os.environ[\"WORLD_SIZE\"])\n            \n            start_time = time.time()\n            logger.info(f\"Starting distributed training with rank={rank}, world_size={world_size}\")\n\n            # Initialize training components in each process\n            train_loader, model, optimizer, device, epochs, tokenizer = initialize_training_components()\n\n            # Run the training\n            train(rank, world_size, train_loader, model, optimizer, device, epochs)\n\n            # Save model and tokenizer (only rank 0)\n            if rank == 0:\n                model.module.save_pretrained(model_output.path)\n                model.metadata['framework'] = 'transformers'\n                tokenizer.save_pretrained(model_output.path)\n                logger.info(f\"Model saved to {model_output.path}\")\n\n                # Upload model files to minio\n                execution_id = execution_id if execution_id else str(uuid.uuid4())\n                model_files = [\n                    \"config.json\",\n                    \"pytorch_model.bin\",\n                    \"vocab.json\",\n                    \"merges.txt\",\n                    \"tokenizer_config.json\",\n                    \"special_tokens_map.json\"\n                ]\n                for file in model_files:\n                    file_path = os.path.join(model_output.path, file)\n                    if os.path.exists(file_path):\n                        object_name = f\"{execution_id}/train_model_distributed/{file}\"\n                        try:\n                            minio_client.fput_object(bucket_name, object_name, file_path)\n                            logger.info(f\"Uploaded {object_name} to minio bucket {bucket_name}\")\n                        except S3Error as e:\n                            logger.error(f\"Failed to upload to minio: {e}\")\n            logger.info(f\"Training Time: {(time.time() - start_time)/60:.2f} minutes\")\n        except Exception as e:\n            logger.error(f\"Distributed training failed: {e}\")\n            raise\n\n    if __name__ == \"__main__\":\n        # Since this is a Kubeflow component, we'll assume it's invoked with torchrun\n        # Example command (not run here, but for reference):\n        # torchrun --nproc_per_node=3 --master_addr=localhost --master_port=12345 this_script.py\n        distributed_training()\n\n# Step 4: Evaluate Model\n@component(base_image=\"tarakdhieb7/kubeflow-testing:latest\")\ndef evaluate_model(val_data: Input[Dataset], model: Input[Model], metrics_output: Output[Dataset], execution_id: str = \"\"):\n    import subprocess\n    subprocess.run([\"pip\", \"install\", \"minio\"], check=True)\n    import torch\n    import pandas as pd\n    from transformers import GPT2Tokenizer, AutoModelForCausalLM\n    from torch.utils.data import Dataset, DataLoader\n    import time\n    from minio import Minio\n    from minio.error import S3Error\n    import uuid\n\n    # Initialize minio client\n    minio_client = Minio(\n        \"10.106.67.253:9000\",\n        access_key=\"minio\",  # Replace with your minio access key\n        secret_key=\"minio123\",  # Replace with your minio secret key\n        secure=False\n    )\n\n    # Ensure bucket exists\n    bucket_name = \"mlpipeline\"\n    if not minio_client.bucket_exists(bucket_name):\n        minio_client.make_bucket(bucket_name)\n\n    class CustomDataset(Dataset):\n        def __init__(self, data, tokenizer, max_length):\n            self.data = data\n            self.tokenizer = tokenizer\n            self.max_length = max_length\n\n        def __len__(self):\n            return len(self.data)\n\n        def __getitem__(self, index):\n            input_seq = self.data.iloc[index]['input_message_clean']\n            target_seq = self.data.iloc[index]['CCP Response']\n            concat_seq = f\"{input_seq} {target_seq}\"\n            tokenized_seq = self.tokenizer.encode(\n                concat_seq,\n                return_tensors='pt',\n                max_length=self.max_length,\n                truncation=True,\n                padding='max_length'\n            )\n            return tokenized_seq.squeeze(0)\n\n    # Load data\n    val_df = pd.read_csv(val_data.path)\n    print(\"Shape of val_df:\", val_df.shape)\n\n    # Load tokenizer and model\n    tokenizer = GPT2Tokenizer.from_pretrained(model.path)\n    model = AutoModelForCausalLM.from_pretrained(model.path)\n    max_length = 512\n    val_dataset = CustomDataset(val_df, tokenizer, max_length)\n    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=True)\n\n    # Evaluate\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n    model.eval()\n    total_loss = 0\n    start_time = time.time()\n    with torch.no_grad():\n        for batch in val_loader:\n            input_ids = batch.to(device)\n            outputs = model(input_ids=input_ids, labels=input_ids)\n            total_loss += outputs.loss.item()\n    avg_loss = total_loss / len(val_loader)\n    print(f\"Average Validation Loss: {avg_loss:.4f}\")\n    print(f\"Validation Time: {(time.time() - start_time)/60:.2f} minutes\")\n\n    # Save metrics\n    with open(metrics_output.path, 'w') as f:\n        f.write(f\"Average Validation Loss: {avg_loss:.4f}\")\n\n    # Upload to minio\n    execution_id = execution_id if execution_id else str(uuid.uuid4())\n    object_name = f\"{execution_id}/evaluate_model/metrics.txt\"\n    try:\n        minio_client.fput_object(bucket_name, object_name, metrics_output.path)\n        print(f\"Uploaded {object_name} to minio bucket {bucket_name}\")\n    except S3Error as e:\n        print(f\"Failed to upload to minio: {e}\")\n\n# Step 5: Predict\n@component(base_image=\"tarakdhieb7/kubeflow-testing:latest\")\ndef predict(val_data: Input[Dataset], model: Input[Model], prediction_output: Output[Dataset], execution_id: str = \"\"):\n    import subprocess\n    subprocess.run([\"pip\", \"install\", \"minio\"], check=True)\n    import torch\n    import pandas as pd\n    from transformers import GPT2Tokenizer, AutoModelForCausalLM\n    import time\n    from minio import Minio\n    from minio.error import S3Error\n    import uuid\n\n    # Initialize minio client\n    minio_client = Minio(\n        \"10.106.67.253:9000\",\n        access_key=\"minio\",  # Replace with your minio access key\n        secret_key=\"minio123\",  # Replace with your minio secret key\n        secure=False\n    )\n\n    # Ensure bucket exists\n    bucket_name = \"mlpipeline\"\n    if not minio_client.bucket_exists(bucket_name):\n        minio_client.make_bucket(bucket_name)\n\n    def preprocess_input(raw_input: str) -> str:\n        import re\n        pattern = r'[<\"/]'\n        cleaned_text = re.sub(pattern, '', raw_input)\n        cleaned_text = cleaned_text.replace('>', ' ')\n        cleaned_text = ' '.join(cleaned_text.split())\n        return cleaned_text\n\n    def predict_single(input_message: str, model, tokenizer, device, max_length=512):\n        model.eval()\n        cleaned_input = preprocess_input(input_message)\n        tokenized_input = tokenizer.encode(\n            cleaned_input,\n            return_tensors='pt',\n            max_length=max_length,\n            truncation=True\n        )\n        tokenized_input = tokenized_input.to(device)\n        with torch.no_grad():\n            outputs = model.generate(\n                input_ids=tokenized_input,\n                max_length=max_length,\n                num_return_sequences=1,\n                pad_token_id=tokenizer.eos_token_id\n            )\n        decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        predicted_output = decoded_output[len(cleaned_input):].strip()\n        return predicted_output\n\n    # Load data\n    val_df = pd.read_csv(val_data.path)\n    print(\"Shape of val_df:\", val_df.shape)\n\n    # Load tokenizer and model\n    tokenizer = GPT2Tokenizer.from_pretrained(model.path)\n    model = AutoModelForCausalLM.from_pretrained(model.path)\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n  \n    # Predict on sample input\n    sample_input = val_df.iloc[0]['input_message_clean']  # Use first validation input\n    start_time = time.time()\n    predicted_output = predict_single(sample_input, model, tokenizer, device)\n    print(f\"Sample Input: {sample_input[:100]}...\")\n    print(f\"Predicted Output: {predicted_output}\")\n    print(f\"Prediction Time: {(time.time() - start_time):.2f} seconds\")\n\n    # Save prediction\n    with open(prediction_output.path, 'w') as f:\n        f.write(f\"Input: {sample_input}\\nPrediction: {predicted_output}\")\n    print(f\"model saved to: {model.path}\")\n    # Upload to minio\n    execution_id = execution_id if execution_id else str(uuid.uuid4())\n    object_name = f\"{execution_id}/predict/prediction.txt\"\n    try:\n        minio_client.fput_object(bucket_name, object_name, prediction_output.path)\n        print(f\"Uploaded {object_name} to minio bucket {bucket_name}\")\n    except S3Error as e:\n        print(f\"Failed to upload to minio: {e}\")\n\n@component(base_image=\"tarakdhieb7/kubeflow-testing:latest\")\ndef upload_model(model: Output[Model]):\n    import subprocess\n    subprocess.run([\"pip\", \"install\", \"minio\"], check=True)\n    import torch\n    import pandas as pd\n    from transformers import GPT2Tokenizer, AutoModelForCausalLM\n    import time\n    from minio import Minio\n    from minio.error import S3Error\n\n    # Initialize minio client\n    minio_client = Minio(\n        \"10.106.67.253:9000\",\n        access_key=\"minio\",  # Replace with your minio access key\n        secret_key=\"minio123\",  # Replace with your minio secret key\n        secure=False\n    )\n    # Ensure bucket exists\n    bucket_name = \"mlpipeline\"\n    if not minio_client.bucket_exists(bucket_name):\n        minio_client.make_bucket(bucket_name)\n    # Upload model files to minio\n    \n# Define the Pipeline\n@dsl.pipeline(name=\"ml-pipeline\")\ndef ml_pipeline():\n    import uuid\n    unique_suffix = str(uuid.uuid4())\n    execution_id = str(uuid.uuid4())  # Generate a unique execution ID for this pipeline run\n    # Step 1: Generate Dataset\n    load_op = generate_data(execution_id=execution_id)\n    # Step 2: Preprocess Data\n    preprocess_op = preprocess_data(input_csv=load_op.outputs[\"output_csv\"], execution_id=execution_id)\n\n    # Step 3: Train Model\n#    train_op = train_model(\n#      train_data=preprocess_op.outputs[\"output_train\"],\n#       val_data=preprocess_op.outputs[\"output_val\"]\n#    )\n\n    train_op = train_model(\n        train_data=preprocess_op.outputs[\"output_train\"],\n        val_data=preprocess_op.outputs[\"output_val\"],\n        execution_id=execution_id\n    )\n    #train_op.set_caching_options(enable_caching=False)\n    # Step 4: Evaluate Model\n    evaluate_op = evaluate_model(\n        val_data=preprocess_op.outputs[\"output_val\"],\n        model=train_op.outputs[\"model_output\"],\n        execution_id=execution_id\n    )\n\n    # Step 5: Predict\n    predict_op = predict(\n        val_data=preprocess_op.outputs[\"output_val\"],\n        model=train_op.outputs[\"model_output\"],\n        execution_id=execution_id\n    )\n\nif __name__ == \"__main__\":\n    import argparse\n    from kfp import Client, compiler\n    import kfp_server_api.exceptions as kfp_exceptions\n    import uuid\n\n    # Parse command-line arguments\n    parser = argparse.ArgumentParser(description=\"Run and upload Kubeflow pipeline\")\n    parser.add_argument(\n        \"--pipeline-exists\",\n        action=\"store_true\",\n        help=\"Set if the pipeline 'demo_test_distributed' already exists\"\n    )\n    parser.add_argument(\n        \"--version\",\n        type=float,\n        default=0.1,\n        help=\"Version number for the pipeline (required if --pipeline-exists, e.g., 0.2)\"\n    )\n    args = parser.parse_args()\n\n    \n    name_space = \"kubeflow-user-example-com\"  # Specify the namespace\n    endpoint = \"http://localhost:8080/pipeline\"  # Specify the endpoint\n    # initialize a KFPClientManager\n    kfp_client_manager = KFPClientManager(\n    api_url=endpoint,\n    skip_tls_verify=True,\n\n    dex_username=\"user@example.com\",\n    dex_password=\"12341234\",\n\n    # can be 'ldap' or 'local' depending on your Dex configuration\n    dex_auth_type=\"local\",\n)\n    client = kfp_client_manager.create_kfp_client()\n\n    \n    # Run pipeline with unique run_id to force fresh execution\n    try:\n        unique_run_id = f\"run-{uuid.uuid4()}\"\n        run_result = client.create_run_from_pipeline_func(\n            ml_pipeline,\n            arguments={},\n            experiment_name=\"test\",\n            run_name=unique_run_id,\n            namespace=\"kubeflow-user-example-com\",  # Specify the namespace\n            service_account=\"pipeline-runner\"  # Specify the service account\n        )\n        print(f\"Started run: {run_result.run_id}\")\n        print(f\"Run details: {endpoint}/#/runs/details/{run_result.run_id}\")\n    except Exception as e:\n        print(f\"Failed to start run: {e}\")\n        exit(1)\n\n    # Compile pipeline\n    pipeline_name = \"demo_test\"\n    yaml_file = \"kubeflow_pipeline_demo.yaml\"\n    try:\n        compiler.Compiler().compile(pipeline_func=ml_pipeline, package_path=yaml_file)\n        print(f\"Compiled pipeline to {yaml_file}\")\n    except Exception as e:\n        print(f\"Failed to compile pipeline: {e}\")\n        exit(1)\n\n    # Upload pipeline based on --pipeline-exists flag\n    try:\n        if args.pipeline_exists:\n            if not args.version:\n                print(\"Error: --version required when --pipeline-exists is set\")\n                exit(1)\n            print(f\"Uploading version {args.version} for existing pipeline {pipeline_name}\")\n            try:\n                client.upload_pipeline_version(\n                    pipeline_package_path=yaml_file,\n                    pipeline_version_name=str(args.version),\n                    pipeline_name=pipeline_name,\n                    description=\"just for testing\"\n                )\n            except kfp_exceptions.ApiException as api_err:\n                if api_err.status == 404:  # Pipeline doesn't exist\n                    print(f\"Pipeline {pipeline_name} not found, uploading as new pipeline\")\n                    client.upload_pipeline(\n                        pipeline_package_path=yaml_file,\n                        pipeline_name=pipeline_name,\n                        description=\"just for testing\"\n                    )\n                else:\n                    raise api_err\n        else:\n            print(f\"Uploading new pipeline {pipeline_name}\")\n            client.upload_pipeline(\n                pipeline_package_path=yaml_file,\n                pipeline_name=pipeline_name,\n                description=\"just for testing\"\n            )\n    except Exception as e:\n        print(f\"Failed to upload pipeline: {e}\")\n        exit(1)"
  },
  {
    "repo": "stackdemos/kubeflow-acm-ecr",
    "file_path": "components/golang/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/kubeflow-acm-ecr/master/components/golang/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef training_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('TRAINING_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=['/usr/local/bin/python', script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n\n"
  },
  {
    "repo": "stackdemos/kubeflow-acm-ecr",
    "file_path": "components/training/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/kubeflow-acm-ecr/master/components/training/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef training_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('TRAINING_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=['/usr/local/bin/python', script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n\n"
  },
  {
    "repo": "stackdemos/s3-kubeflow-asi",
    "file_path": "components/golang/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/s3-kubeflow-asi/master/components/golang/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef processing_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('GOLANG_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=[script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n"
  },
  {
    "repo": "stackdemos/s3-kubeflow-asi",
    "file_path": "components/training/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/s3-kubeflow-asi/master/components/training/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef training_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('TRAINING_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=['/usr/local/bin/python', script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n\n"
  },
  {
    "repo": "antongollbo123/kfp_utils",
    "file_path": "local_pipeline2.py",
    "raw_url": "https://raw.githubusercontent.com/antongollbo123/kfp_utils/main/local_pipeline2.py",
    "content": "import json\n\nfrom google.cloud import storage  # type:ignore\nfrom google.oauth2 import service_account  # type:ignore\n\n# from google.cloud import storage  #type:ignore\n# from google.oauth2 import service_account  #type:ignore\nfrom kfp import compiler, dsl, local  # type:ignore\nfrom kfp.dsl import Artifact, Output  # type:ignore\n\nlocal.init(runner=local.DockerRunner())\n\n\n@dsl.component(base_image=\"custom_python:latest\", packages_to_install=[\"numpy\"])\ndef upload_to_gcs(\n    bucket_name: str,\n    source_file_name: str,\n    destination_blob_name: str,\n    out_artifact: Output[Artifact],\n) -> str:\n    import logging\n\n    import numpy as np\n    from google.cloud import storage  # type:ignore\n    from google.oauth2 import service_account  # type:ignore\n\n    logging.basicConfig(level=logging.INFO)\n    logger = logging.getLogger(__name__)\n    try:\n        credentials = service_account.Credentials.from_service_account_file(\n            \"credentials.json\"\n        )\n        storage_client = storage.Client(credentials=credentials)\n        bucket = storage_client.bucket(bucket_name)\n        logger.info(np.abs(1))\n        blob = bucket.blob(destination_blob_name)\n        blob.upload_from_filename(source_file_name)\n        print(\n            f\"File {source_file_name} uploaded to {destination_blob_name} in bucket {bucket_name}\"\n        )\n        #out_artifact.metadata[\"test\"] = \"test123\"\n        logger.info(\"Success!\")\n    except Exception as e:\n        logger.error(f\"Error has occured: {e}\")\n    return \"hello\"\n\n\n@dsl.component(base_image=\"python:3.8\")\ndef add(a: int, b: int, out_artifact: Output[Artifact]):\n    import json\n    import logging\n\n    logging.basicConfig(level=logging.INFO)\n    logger = logging.getLogger(__name__)\n    result = json.dumps(a + b)\n    logger.info(f\"Hello from inside the pipeline, with message {a}, {b}\")\n\n    with open(out_artifact.path, \"w\") as f:\n        f.write(result)\n\n    out_artifact.metadata[\"operation\"] = \"addition\"\n    out_artifact.metadata[\"test\"] = \"test123\"\n\n\"\"\"\ntask = upload_to_gcs(\n    bucket_name=\"new_bucket123123213\",\n    source_file_name=\"sample_file.txt\",\n    destination_blob_name=\"sample_file.txt\",\n)  # type:ignore\n\"\"\"\n@dsl.pipeline()\ndef gcs_pipeline() -> None:\n    upload_to_gcs(bucket_name=\"new_bucket123123213\", \n                         source_file_name=\"sample_file.txt\",\n                         destination_blob_name=\"sample_file.txt\"\n                         ) # type:ignore\n\n\n\npipeline_file = \"test_pipeline.yaml\"\ncompiler.Compiler().compile(\n    gcs_pipeline,  # type: ignore\n    pipeline_file,\n)\n\n# can read artifact contents\n#with open(task.outputs[\"out_artifact\"].path) as f:\n#    contents = f.read()\n"
  },
  {
    "repo": "antongollbo123/kfp_utils",
    "file_path": "test_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/antongollbo123/kfp_utils/main/test_pipeline.py",
    "content": "\n\nimport kfp  #type:ignore\nfrom kfp import compiler, dsl, local\n\nlocal.init(runner=local.DockerRunner())\n\n@dsl.component(base_image=\"custom_python:latest\")\ndef print_hello(message: str) -> None:\n    import logging\n    logging.basicConfig(level=logging.INFO)\n    logger = logging.getLogger(__name__)\n    logger.info(f\"Hello from inside the pipeline, with message {message}\")\n    #return message\n\n@dsl.pipeline\ndef logging_pipeline(message: str) -> None:\n    \"\"\"My test pipeline\"\"\"\n    print_hello(message=message) # type: ignore\n\npipeline = logging_pipeline(message=\"hello from the other side\")\n#pipeline_file = \"logging_pipeline.yaml\"\n#compiler.Compiler().compile(logging_pipeline,  # type: ignore\n#                            pipeline_file)\n\n#client = kfp.Client()\n#client.create_run_from_pipeline_package(pipeline_file=pipeline_file,\n#                                        arguments={},\n#                                        experiment_name=\"Logging pipeline experiment\",\n#                                        namespace=None, )"
  },
  {
    "repo": "antongollbo123/kfp_utils",
    "file_path": "local_run/custom_python_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/antongollbo123/kfp_utils/main/local_run/custom_python_pipeline.py",
    "content": "import json\nfrom typing import Any, List\n\nimport pandas as pd  # type:ignore\nimport pyarrow  # type:ignore\nfrom google.cloud import storage  # type:ignore\nfrom google.oauth2 import service_account  # type:ignore\n\n# from google.cloud import storage  #type:ignore\n# from google.oauth2 import service_account  #type:ignore\nfrom kfp import compiler, dsl, local  # type:ignore\nfrom kfp.dsl import Artifact, Dataset, Input, Output #type:ignore\nimport trace\n\nlocal.init(runner=local.DockerRunner())\n\n\n@dsl.component(\n    base_image=\"custom_python:latest\",\n    packages_to_install=[\"numpy\", \"pyarrow\", \"pandas\"],\n)\ndef read_from_gcs(\n    bucket_name: str, blob_name: str, output_dataset: Output[Dataset]\n) -> None:\n    import logging\n    import os\n\n    import pandas as pd  # type:ignore\n    import pyarrow.parquet as pq  # type:ignore\n    from google.cloud import storage  # type:ignore\n\n    from google.oauth2 import service_account  # type:ignore\n\n    logging.basicConfig(level=logging.INFO)\n    logger = logging.getLogger(__name__)\n    try:\n        os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"credentials.json\"\n        gcs_path = f\"gs://{bucket_name}/{blob_name}\"\n        table = pq.read_table(gcs_path)\n        logger.info(f\"Succesfully read {table}\")\n        local_path = \"output_file.parquet\"\n        output_dataset.name = \"output_file\"\n\n        df = table.to_pandas()\n        new_df = df.drop(columns=[\"Gender\"])\n        new_df.to_parquet(local_path)\n\n        client = storage.Client()\n        bucket = client.bucket(bucket_name+2)\n        blob = bucket.blob(f\"{output_dataset.name}.parquet\")\n        blob.upload_from_filename(local_path)\n\n        os.remove(local_path)\n        output_dataset.path = f\"gs://{bucket_name}/{output_dataset.name}.parquet\"\n        \n        logger.info(f\"Succesfully transformed to pandas df: {output_dataset.path},  \")\n    except Exception as e:\n        logger.error(f\"Error has occured: {e}\")\n        logger.exception(e)\n\n\n@dsl.component(\n    base_image=\"custom_python:latest\", packages_to_install=[\"pandas\", \"pyarrow\"]\n)\ndef do_df_operation(input_dataset: Input[Dataset]) -> str:\n    import logging\n    import os\n\n    import pandas as pd\n    import pyarrow.parquet as pq\n\n    logging.basicConfig(level=logging.INFO)\n    logger = logging.getLogger()\n\n    try:\n        logger.info(input_dataset.uri)\n        os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"credentials.json\"\n        table = pq.read_table(input_dataset.uri)\n        logger.info(f\"Succesfully read {table}\")\n        logger.info(f\"Path is equal to: {input_dataset.path}, type: {type(input_dataset.path)}\")\n        logger.info(f\"URI is equal to: {input_dataset.uri}, type: {type(input_dataset.uri)}\")\n        logger.info(f\"Metadata is equal to: {input_dataset.metadata}, type: {type(input_dataset.metadata)}\")\n        logger.info(f\"Name is equal to: {input_dataset.name}, type: {type(input_dataset.name)}\")\n        df = table.to_pandas()\n        return_val = df.iloc[0].Name\n    except Exception as e:\n        logger.error(f\"Error {e} caught...\")\n    return return_val\n\n\n@dsl.pipeline\ndef df_pass_pipeline(bucket_name: str, blob_name: str) -> None:\n    task1 = read_from_gcs(bucket_name=bucket_name, blob_name=blob_name)  # type:ignore\n    task2 = do_df_operation(input_dataset=task1.outputs[\"output_dataset\"])\n\n\npipeline = df_pass_pipeline(\n    bucket_name=\"new_bucket123123213\", blob_name=\"example_file.parquet\"\n)"
  },
  {
    "repo": "antongollbo123/kfp_utils",
    "file_path": "local_run/custom_python_pipeline_multi.py",
    "raw_url": "https://raw.githubusercontent.com/antongollbo123/kfp_utils/main/local_run/custom_python_pipeline_multi.py",
    "content": "import json\nfrom typing import Any, List\nimport logging\n\nimport pandas as pd  # type:ignore\nimport pyarrow  # type:ignore\nfrom google.cloud import storage  # type:ignore\nfrom google.oauth2 import service_account  # type:ignore\n\n# from google.cloud import storage  #type:ignore\n# from google.oauth2 import service_account  #type:ignore\nfrom kfp import compiler, dsl, local  # type:ignore\nfrom kfp.dsl import Artifact, Dataset, Input, Output, OutputPath  # type:ignore\n\nlocal.init(runner=local.DockerRunner())\n\n\n@dsl.component(\n    base_image=\"python:3.11\",\n    packages_to_install=[\"numpy\", \"pyarrow\", \"pandas\"],\n)\ndef save_df(output_dataset: Output[Dataset]\n) -> Dataset:\n    \n    import logging\n    import os\n\n    import pandas as pd  # type:ignore\n    import pyarrow.parquet as pq  # type:ignore\n\n    logging.basicConfig(level=logging.INFO)\n    logger = logging.getLogger(__name__)\n\n    data1 = {'Name': ['Alice', 'Bob', 'Charlie'],\n         'Age': [25, 30, 35],\n         'City': ['New York', 'Los Angeles', 'Chicago']}\n    df1 = pd.DataFrame(data1)\n\n    # Creating the second example DataFrame\n    data2 = {'Product': ['Laptop', 'Phone', 'Tablet'],\n            'Price': [1200, 800, 500],\n            'Stock': [10, 20, 15]}\n    df2 = pd.DataFrame(data2)\n    \n    df1.to_csv(output_dataset.path, header=True, index=False)\n    df2.to_csv(output_dataset.path, header=True, index=False)\n    \n    logger.info(f\"Succesfully saved df to disk at location {output_dataset.path}!\")\n\n@dsl.component(\n    base_image=\"python:3.11\",\n    packages_to_install=[\"numpy\", \"pyarrow\", \"pandas\"],\n)\ndef read_df(input_dataset: Input[Dataset]) -> None:\n    import pandas as pd\n    import logging\n    df1  = pd.read_csv(input_dataset.path)\n    return_val = f\"DF_SHAPE: {df1.shape[0]}\"\n    logger = logging.getLogger(__name__)\n    logging.basicConfig(level=logging.INFO)\n    logger.info(input_dataset.path)\n    logger.info(input_dataset.uri)\n\n\n@dsl.pipeline\ndef df_pass_pipeline() -> None:\n    task1 = save_df()  # type:ignore\n    task2 = read_df(input_dataset=task1.outputs[\"output_dataset\"])\n\npipe_df = df_pass_pipeline()\n"
  },
  {
    "repo": "antongollbo123/kfp_utils",
    "file_path": "local_run/gcp_client_dependency_injection.py",
    "raw_url": "https://raw.githubusercontent.com/antongollbo123/kfp_utils/main/local_run/gcp_client_dependency_injection.py",
    "content": "import json\nfrom typing import Any, List\nimport logging\n\nimport pandas as pd  # type:ignore\nimport pyarrow  # type:ignore\nfrom google.cloud import storage, client  # type:ignore\nfrom google.oauth2 import service_account  # type:ignore\n\n# from google.cloud import storage  #type:ignore\n# from google.oauth2 import service_account  #type:ignore\nfrom kfp import compiler, dsl, local  # type:ignore\nfrom kfp.dsl import Artifact, Dataset, Input, Output, OutputPath  # type:ignore\nfrom google.cloud.storage import Client\n\n@dsl.component(\n    base_image=\"python:3.11\",\n    packages_to_install=[\"numpy\", \"google\"],\n)\ndef component_inject_client(client: Client) -> list:\n    from google.cloud.storage import Client\n    bucket = client.bucket(bucket_name=\"anton-test-bucket123\")\n    blob_list = bucket.list_blobs()\n    blob_name_list = []\n    for blob in blob_list:\n        blob_name_list.append(blob.name)\n    return blob_list\n\n\n\n\n@dsl.pipeline\ndef dep_injection_pipeline() -> None:\n    client = storage.Client.from_service_account_json(\"credentials.json\")\n    output_list = component_inject_client(client=client)\n    print(output_list)\n\npipe = dep_injection_pipeline()"
  },
  {
    "repo": "antongollbo123/kfp_utils",
    "file_path": "local_run/get_gcs_object_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/antongollbo123/kfp_utils/main/local_run/get_gcs_object_pipeline.py",
    "content": "import json\n\nfrom google.cloud import storage  # type:ignore\nfrom google.oauth2 import service_account  # type:ignore\n\n# from google.cloud import storage  #type:ignore\n# from google.oauth2 import service_account  #type:ignore\nfrom kfp import compiler, dsl, local  # type:ignore\nfrom kfp.dsl import Artifact, Output  # type:ignore\nimport jsonargparse\n\nlocal.init(runner=local.DockerRunner())\n\n\n@dsl.component(\n    base_image=\"custom_python:latest\",\n    packages_to_install=[\"numpy\", \"pandas\", \"pyarrow\"],\n)\ndef get_gcs_object(\n    bucket_name: str\n) -> list:\n    \"\"\"\n    Custom Python image has 'google' package installed, but not numpy.\n    Further, GCP credentials are mounted into this image.\n    \"\"\"\n    import logging\n    from google.cloud import storage  # type:ignore\n    logging.basicConfig(level=logging.INFO)\n    logger = logging.getLogger(__name__)\n    try:\n        bucket_name = \"anton-test-bucket123\"\n        client = storage.Client.from_service_account_json(\"credentials.json\")\n        bucket = client.bucket(bucket_name=bucket_name)\n        blob_list = bucket.list_blobs()\n        blob_name_list = []\n        for blob in blob_list:\n            blob_name_list.append(f\"{blob.name}\")\n\n    except Exception as e:\n        logger.error(f\"Error has occured: {e}\")\n    return blob_name_list\n\n@dsl.component(\n    base_image=\"custom_python:latest\",\n    packages_to_install=[\"numpy\", \"pandas\", \"pyarrow\"],\n)\n\ndef print_gcs_objects(blob_list: list, bucket_name: str) -> None:\n    import pandas as pd\n    from google.cloud import storage\n    from io import BytesIO\n    client = storage.Client.from_service_account_json(\"credentials.json\")\n    bucket = client.bucket(bucket_name=bucket_name)\n    for blob in blob_list: \n        blob = bucket.get_blob(blob)\n        content = blob.download_as_string()\n        print(content)\n\n#@dsl.pipeline\ndef pipeline_orchestrator(bucket_name: str, \n                          pipeline_name: str,\n                          upload_path: str\n                          ) -> None:\n    print(pipeline_name)\n    if pipeline_name == \"mega_pipeline\":\n        print(\"true\")\n        @dsl.pipeline\n        def mega_pipeline() -> None:\n            task_1 = get_gcs_object(bucket_name=bucket_name)\n            print(task_1.output)\n            task_2 = print_gcs_objects(blob_list=task_1.output, bucket_name=bucket_name)\n        def mega_pipeline2() -> None:\n            task_1 = get_gcs_object(bucket_name=bucket_name)\n        pipeline = mega_pipeline()\n\nif __name__ == \"__main__\":\n    jsonargparse.CLI(pipeline_orchestrator, as_positional=False)\n    #pipe = gcs_obj_pipeline(bucket_name=\"anton-test-bucket123\")\n"
  },
  {
    "repo": "antongollbo123/kfp_utils",
    "file_path": "local_run/import_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/antongollbo123/kfp_utils/main/local_run/import_pipeline.py",
    "content": "from number_sum_pipeline import number_sum_pipeline\nfrom kfp.dsl import pipeline # type:ignore\nfrom kfp import dsl, local  # type:ignore\nfrom google.cloud import bigquery \nlocal.init(runner=local.SubprocessRunner())\n\n\n@pipeline\ndef double_pipeline() -> None:\n    pipeline1 = number_sum_pipeline(x=1, y=2, z=3)\n    pipeline2 = number_sum_pipeline(x=2, y=3, z=4).after(pipeline1)\n\n\npipe = double_pipeline()"
  },
  {
    "repo": "antongollbo123/kfp_utils",
    "file_path": "local_run/number_sum_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/antongollbo123/kfp_utils/main/local_run/number_sum_pipeline.py",
    "content": "from kfp import dsl, local  # type:ignore\n\nlocal.init(runner=local.SubprocessRunner())\n\n\n@dsl.component\ndef add(a: int, b: int) -> int:\n    import logging\n\n    logging.basicConfig(level=logging.INFO)\n    logger = logging.getLogger(__name__)\n    logger.info(f\"Hello from inside the pipeline, with message {a}, {b}\")\n    return a + b\n\n@dsl.component\ndef add_2(a: int, b: int) -> list:\n    return [a,b]\n\n@dsl.component\ndef add_3(a: list) -> int:\n    import logging\n    logging.basicConfig(level=logging.INFO)\n    logger = logging.getLogger(__name__)\n    sum_ = 0\n    for i in a:\n        logger.info(f\"Iterating through list, with element {i}\")\n        sum_ += i\n    sum_ = int(sum_)\n    return sum_\n\n@dsl.component\ndef add_4(a: list) -> list:\n    str_list = []\n    for i in a:\n        str_list.append(str(i))\n    return str_list\n\n\n# or run it in a pipeline\n@dsl.pipeline\ndef number_sum_pipeline(x: int, y: int, z: int) -> list:\n    t1 = add(a=x, b=y)  # type:ignore\n    t2 = add(a=t1.output, b=z)  # type:ignore\n    t3 = add_2(a=t2.output, b=t2.output)\n    #t4 = add_3(a=t3.output)\n    t4 = add_4(a=t3.output)\n    return t4.output"
  },
  {
    "repo": "antongollbo123/kfp_utils",
    "file_path": "local_run/test_import.py",
    "raw_url": "https://raw.githubusercontent.com/antongollbo123/kfp_utils/main/local_run/test_import.py",
    "content": "from get_gcs_object_pipeline import get_gcs_object, print_gcs_objects\nimport kfp #type:ignore\nfrom kfp import dsl\n\n@dsl.pipeline\ndef mega_pipeline() -> None:\n    task_1 = get_gcs_object(bucket_name=\"anton-test-bucket123\")\n    task_2 = print_gcs_objects(blob_list=task_1.output)\n\npipe = mega_pipeline()"
  },
  {
    "repo": "antongollbo123/kfp_utils",
    "file_path": "local_testing/testing_mock_example.py",
    "raw_url": "https://raw.githubusercontent.com/antongollbo123/kfp_utils/main/local_testing/testing_mock_example.py",
    "content": "from kfp.dsl import component, pipeline  # type:ignore\nimport typing\nfrom kfp.dsl import Dataset, Output, Input, component, Model  # type:ignore\nimport pytest\nimport pandas as pd  # type:ignore\n\n\ndef make_test_artifact(artifact_type):\n    class TestArtifact(artifact_type):  # type: ignore\n        def _get_path(self):\n            return super()._get_path() or self.uri\n\n    return TestArtifact\n\n\n@pytest.fixture(scope=\"session\", name=\"input_data\")\ndef input_dataset_artifact(tmp_path_factory):\n    temp_dir = tmp_path_factory.mktemp(\"artifact_store\")\n    uri = str(temp_dir / \"input.csv\")\n    df = pd.DataFrame({\"text\": [\"Hello world\", \"Goodbye world\"]})\n    df.to_csv(uri)\n    output_dataset = make_test_artifact(Dataset)(uri=uri)\n    return output_dataset\n\n\n@pytest.fixture(scope=\"session\", name=\"output_data\")\ndef output_dataset_artifact(tmp_path_factory):\n    temp_dir = tmp_path_factory.mktemp(\"artifact_store\")\n    uri = str(temp_dir / \"output.csv\")\n    output_dataset = make_test_artifact(Dataset)(uri=uri)\n    return output_dataset\n\n\n@component(\n    packages_to_install=[\n        \"pandas==1.3.4\",\n        \"numpy==1.23.2\",\n    ],\n    base_image=\"python:3.10.5-slim-bullseye\",\n)\ndef clean_text_data(input_art: Input[Dataset], output_art: Output[Dataset]) -> None:\n    import pandas as pd\n    import numpy as np\n\n    df = pd.read_csv(input_art.path)\n\n    df[\"text\"] = df[\"text\"].str.lower()\n    df[\"text\"] = df[\"text\"].str.replace(r\"[^\\w\\s]\", \"\")\n    df[\"text\"] = df[\"text\"].str.replace(r\"\\d+\", \"\")\n    df[\"text\"] = df[\"text\"].str.replace(r\"\\s+\", \" \")\n    df[\"text\"] = df[\"text\"].str.strip()\n\n    df.to_csv(output_art.path, index=False)\n\n\ndef test_clean_text_data_cleans_text_data(input_data, output_data):\n    clean_text_data.python_func(\n        input_art=input_data, output_art=output_data)\n    df = pd.read_csv(output_data.path)\n    breakpoint()\n    assert df['text'].str.contains('hello world').any()\n    assert df['text'].str.contains('goodbye world').any()"
  },
  {
    "repo": "talaman/kubeflow-ml-pipeline",
    "file_path": "mlp-example/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/talaman/kubeflow-ml-pipeline/main/mlp-example/pipeline.py",
    "content": "\n\nfrom typing import Dict, List\n\nfrom kfp import dsl\nfrom kfp.dsl import Input, InputPath, Output, OutputPath, Dataset, Model, component, HTML\nimport kfp\n\n\n@component(packages_to_install=['scikit-learn','pandas'])\ndef exctract_data(\n    dataset: Output[Dataset],\n):\n    from sklearn import datasets\n    data = datasets.load_iris(as_frame=True).frame\n    with open(dataset.path, 'w') as f:\n        f.write(data.to_csv(index=False))\n\n@component(packages_to_install=['evidently'],base_image='python:3.10')\ndef data_validation(\n    dataset: Input[Dataset],\n    tests: Output[HTML],\n    reports: Output[HTML],\n):\n    from evidently.test_suite import TestSuite\n    from evidently.test_preset import DataQualityTestPreset\n    from evidently.test_preset import DataStabilityTestPreset\n    from evidently.report import Report\n    from evidently.metric_preset import DataQualityPreset\n    from evidently.metric_preset import DataDriftPreset\n    import pandas as pd\n    \n    with open(dataset.path, 'r') as input_file:\n        data = pd.read_csv(input_file)\n\n    tests_suite= TestSuite(tests=[\n        DataStabilityTestPreset(),\n        DataQualityTestPreset()\n    ])\n    tests_suite.run(current_data=data.iloc[:60], reference_data=data.iloc[60:], column_mapping=None)\n    tests_suite.save_html(tests.path)\n\n\n    reports_suite = Report(metrics=[\n        DataQualityPreset(),\n        DataDriftPreset()\n    ])\n\n    reports_suite.run(current_data=data.iloc[:60], reference_data=data.iloc[60:], column_mapping=None)\n    reports_suite.save_html(reports.path)\n\n\n@component(packages_to_install=['scikit-learn', 'pandas'], base_image='python:3.9')\ndef data_preparation(\n    dataset: Input[Dataset],\n    dataset_transformed: Output[Dataset],\n):\n    import pandas as pd\n    from sklearn.preprocessing import MinMaxScaler, OneHotEncoder    \n\n    NUMERIC_FEATURE_KEYS = ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n    LABEL_KEY = 'target'\n\n    def preprocessing_fn(inputs):\n        \"\"\"Preprocess input columns into transformed columns.\"\"\"\n        outputs = inputs.copy()\n\n        # Scale numeric features to range [0, 1]\n        scaler = MinMaxScaler()\n        outputs[NUMERIC_FEATURE_KEYS] = scaler.fit_transform(outputs[NUMERIC_FEATURE_KEYS])\n\n        # One-hot encode the label column\n        encoder = OneHotEncoder()\n        outputs[LABEL_KEY] = encoder.fit_transform(outputs[LABEL_KEY].values.reshape(-1, 1)).toarray()\n\n        return outputs\n\n    with open(dataset.path, 'r') as input_file:\n        data = pd.read_csv(input_file)\n\n    data_transformed = preprocessing_fn(data)\n\n    with open(dataset_transformed.path, 'w') as f:\n        f.write(data_transformed.to_csv(index=False))\n\n@component(packages_to_install=['pandas', 'scikit-learn'],base_image='python:3.9')\ndef split_data(dataset: Input[Dataset], dataset_train: Output[Dataset], dataset_validation: Output[Dataset], dataset_test: Output[Dataset]):\n    import pandas as pd\n    from sklearn.model_selection import train_test_split\n    with open(dataset.path, 'r') as input_file:\n        data = pd.read_csv(input_file)\n\n    train, test = train_test_split(data, test_size=0.2)\n    train, validation = train_test_split(train, test_size=0.2)\n\n    with open(dataset_train.path, 'w') as f:\n        f.write(train.to_csv(index=False))\n    with open(dataset_validation.path, 'w') as f:\n        f.write(validation.to_csv(index=False))\n    with open(dataset_test.path, 'w') as f:\n        f.write(test.to_csv(index=False))\n\n@component(packages_to_install=['tensorflow', 'pandas', 'joblib'\n                                ],base_image='python:3.9')\ndef train_model(\n    dataset_train: Input[Dataset],\n    dataset_validation: Input[Dataset],\n    model_artifact: Output[Model],\n    epochs: int = 10,\n):\n    import pandas as pd\n    import tensorflow as tf\n    from tensorflow import keras\n    from tensorflow.keras import layers\n    def create_model():\n        tf_model = keras.Sequential([\n            layers.Dense(64, activation='relu', input_shape=(4,)),\n            layers.Dense(64, activation='relu'),\n            layers.Dense(3, activation='softmax')\n        ])\n        return tf_model\n\n\n    with open(dataset_train.path, 'r') as train_file:\n        train_data = pd.read_csv(train_file)\n    with open(dataset_validation.path, 'r') as validation_file:\n        validation_data = pd.read_csv(validation_file)\n\n    # Preprocess the data\n    train_features = train_data.drop('target', axis=1)\n    train_labels = train_data['target']\n    validation_features = validation_data.drop('target', axis=1)\n    validation_labels = validation_data['target']\n\n    # Create the model\n    tf_model = create_model()\n\n    # Compile the model\n    tf_model.compile(optimizer='adam',\n                    loss='sparse_categorical_crossentropy',\n                    metrics=['accuracy'])\n\n    # Train the model\n    tf_model.fit(train_features, train_labels, epochs=epochs, validation_data=(validation_features, validation_labels))\n\n    # Evaluate the model\n    r = tf_model.evaluate(validation_features, validation_labels)\n    print(\"Result:\",r)\n\n    # Save the model\n    tf_model.save( \"model_artifact.keras\")\n\n    # Copy the model to the output path\n    import shutil\n    shutil.move(\"model_artifact.keras\", model_artifact.path)\n\n\n\n        \n\n\n@component(packages_to_install=['tensorflow', 'pandas'],base_image='python:3.9')\ndef validate_model(model_artifact: Input[Model], dataset_test: Input[Dataset]):\n    import pandas as pd\n    import tensorflow as tf\n\n    # Copy the model to the current directory\n    import shutil\n    shutil.copy(model_artifact.path, \"model_artifact.keras\")\n\n    # Load the model\n    tf_model = tf.keras.models.load_model(\"model_artifact.keras\")\n\n    with open(dataset_test.path, 'r') as test_file:\n        test_data = pd.read_csv(test_file)\n\n    # Preprocess the data\n    test_features = test_data.drop('target', axis=1)\n    test_labels = test_data['target']\n\n    # Evaluate the model\n    r = tf_model.evaluate(test_features, test_labels)\n    print(r)\n\n\n\n@dsl.pipeline(pipeline_root='', name='mlp-example-pipeline')\ndef ml_pipeline(message: str = 'message'):\n    exctract_data_task = exctract_data()\n    data_validation_task = data_validation(dataset=exctract_data_task.outputs['dataset'])\n    data_preparation_task = data_preparation(dataset=exctract_data_task.outputs['dataset']).after(data_validation_task)\n    split_data_task = split_data(dataset=data_preparation_task.outputs['dataset_transformed'])\n    train_model_task = train_model(epochs=30, dataset_train=split_data_task.outputs['dataset_train'], dataset_validation=split_data_task.outputs['dataset_validation'])\n    validate_model_task = validate_model(model_artifact=train_model_task.outputs['model_artifact'], dataset_test=split_data_task.outputs['dataset_test'])\n\n\n\ndef run_pipeline():\n    host = \"http://localhost:9000/pipeline\"\n    \n    # Compile and run the pipeline\n    kfp.compiler.Compiler().compile(ml_pipeline, 'mlp-example.yaml')\n    kfp.Client(host=host).create_run_from_pipeline_func(ml_pipeline, arguments={})\n\nif __name__ == '__main__':\n    run_pipeline()"
  },
  {
    "repo": "zinzinhust96/kubeflow_pipeline_examples",
    "file_path": "build_pipeline/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/zinzinhust96/kubeflow_pipeline_examples/master/build_pipeline/pipeline.py",
    "content": "import kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import component\nfrom kfp.v2.dsl import (\n    Input,\n    Output,\n    Artifact,\n    Dataset,\n)\n\n@component(\n    packages_to_install=['pandas==1.1.4'],\n    output_component_file='component.yaml'\n)\ndef merge_csv(tar_data: Input[Artifact], output_csv: Output[Dataset]):\n    import glob\n    import pandas as pd\n    import tarfile\n\n    tarfile.open(name=tar_data.path, mode=\"r|gz\").extractall('data')\n    df = pd.concat(\n        [pd.read_csv(csv_file, header=None)\n        for csv_file in glob.glob('data/*.csv')])\n    df.to_csv(output_csv.path, index=False, header=False)\n\nweb_downloader_op = kfp.components.load_component_from_url(\n    'https://raw.githubusercontent.com/kubeflow/pipelines/master/components/web/Download/component-sdk-v2.yaml')\n\n# Define a pipeline and create a task from a component:\n@dsl.pipeline(\n    name='build_pipeline_example',\n    # You can optionally specify your own pipeline_root\n    # pipeline_root='gs://my-pipeline-root/example-pipeline',\n)\ndef build_pipeline(url: str):\n    web_downloader_task = web_downloader_op(url=url)\n    print('web_downloader_task: ', web_downloader_task)\n    print('web_downloader_task output: ', web_downloader_task.outputs['data'])\n    merge_csv_task = merge_csv(tar_data=web_downloader_task.outputs['data'])\n    print('merge_csv_task: ', merge_csv_task)\n    print('merge_csv_task.outputs: ', merge_csv_task.outputs)\n    # The outputs of the merge_csv_task can be referenced using the\n    # merge_csv_task.outputs dictionary: merge_csv_task.outputs['output_csv']\n\nclient = kfp.Client(\"http://localhost:8080/pipeline\")\nclient.create_run_from_pipeline_func(\n    build_pipeline,\n    mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE,\n    # You can optionally override your pipeline_root when submitting the run too:\n    # pipeline_root='gs://my-pipeline-root/example-pipeline',\n    arguments={\n        'url': 'https://storage.googleapis.com/ml-pipeline-playground/iris-csv-files.tar.gz'\n    })"
  },
  {
    "repo": "zinzinhust96/kubeflow_pipeline_examples",
    "file_path": "kubeflow_pipeline_sample/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/zinzinhust96/kubeflow_pipeline_examples/master/kubeflow_pipeline_sample/pipeline.py",
    "content": "import kfp\nimport kfp.components as comp\nfrom kfp.v2.dsl import component\nfrom kubernetes import client as k8s_client\nfrom kfp.v2.dsl import (\n    Input,\n    Output,\n    Artifact,\n    Dataset,\n)\n\ncreate_step_get_lines = comp.load_component_from_file('./component.yaml')\n\n# create_step_get_lines is a \"factory function\" that accepts the arguments\n# for the component's inputs and output paths and returns a pipeline step\n# (ContainerOp instance).\n#\n# To inspect the get_lines_op function in Jupyter Notebook, enter\n# \"get_lines_op(\" in a cell and press Shift+Tab.\n# You can also get help by entering `help(get_lines_op)`, `get_lines_op?`,\n# or `get_lines_op??`.\n\n@component\ndef print_op(input_txt: Input[Artifact]):\n    with open(input_txt.path, 'r') as fr:\n        for line in fr.readlines():\n            print(line.strip())\n\n# Define your pipeline\n@kfp.dsl.pipeline(\n    name=\"example-pipeline\",\n)\ndef my_pipeline():\n    get_lines_step = create_step_get_lines(\n        # Input name \"Input 1\" is converted to pythonic parameter name \"input_1\"\n        input_1='/data_processing/input.txt',\n        parameter_1='5',\n        ).add_volume(k8s_client.V1Volume(name='data-processing',\n                                         host_path=k8s_client.V1HostPathVolumeSource(path='/home/docker/data_processing'))) \\\n        .add_volume_mount(k8s_client.V1VolumeMount(mount_path='/data_processing',\n                                                   name='data-processing'))\n\n    print('write output at: ', get_lines_step.outputs, type(get_lines_step.outputs['output_1']))\n    check_output_step = print_op(get_lines_step.outputs['output_1'])\n\n\n# def my_pipeline():\n#     get_lines_step = create_step_get_lines(\n#         # Input name \"Input 1\" is converted to pythonic parameter name \"input_1\"\n#         input_1='one\\ntwo\\nthree\\nfour\\nfive\\nsix\\nseven\\neight\\nnine\\nten',\n#         parameter_1='5',\n#     )\n\n# If you run this command on a Jupyter notebook running on Kubeflow,\n# you can exclude the host parameter.\n# client = kfp.Client()\nclient = kfp.Client(host='http://localhost:8080/pipeline')\n\n# Compile, upload, and submit this pipeline for execution.\nclient.create_run_from_pipeline_func(my_pipeline, arguments={},\n    mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE)\n"
  },
  {
    "repo": "zinzinhust96/kubeflow_pipeline_examples",
    "file_path": "train_classifier/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/zinzinhust96/kubeflow_pipeline_examples/master/train_classifier/pipeline.py",
    "content": "import os\nfrom pathlib import Path\nimport requests\nfrom kubernetes import client as k8s_client\n\nimport kfp\n\ncomponent_url_prefix = '/data_processing/'\ntest_data_url_prefix = component_url_prefix + 'testdata'\n\n#Prepare input/output paths and data\n# input_data_gcs_dir = 'gs://<my bucket>/<path>/'\n# output_data_gcs_dir = 'gs://<my bucket>/<path>/'\n\n#Downloading the training set (to upload to GCS later)\n# training_set_features_local_path = os.path.join('.', 'training_set_features.tsv')\n# training_set_labels_local_path = os.path.join('.', 'training_set_labels.tsv')\n\n# training_set_features_url = test_data_url_prefix + '/training_set_features.tsv'\n# training_set_labels_url = test_data_url_prefix + '/training_set_labels.tsv'\n\ntraining_set_features_local_path = os.path.join(test_data_url_prefix, 'training_set_features.tsv')\ntraining_set_labels_local_path = os.path.join(test_data_url_prefix, 'training_set_labels.tsv')\n\n# Path(training_set_features_local_path).write_bytes(requests.get(training_set_features_url).content)\n# Path(training_set_labels_local_path).write_bytes(requests.get(training_set_labels_url).content)\n\n#Uploading the data to GCS where it can be read by the trainer\n# training_set_features_gcs_path = os.path.join(input_data_gcs_dir, 'training_set_features.tsv')\n# training_set_labels_gcs_path = os.path.join(input_data_gcs_dir, 'training_set_labels.tsv')\n\n# gfile.Copy(training_set_features_local_path, training_set_features_gcs_path)\n# gfile.Copy(training_set_labels_local_path, training_set_labels_gcs_path)\n\n# output_model_uri_template = os.path.join(output_data_gcs_dir, kfp.dsl.EXECUTION_ID_PLACEHOLDER, 'output_model_uri', 'data')\n# output_model_local_template = component_url_prefix + 'tests/output/trained'\n\n# xor_model_config = requests.get('https://raw.githubusercontent.com/kubeflow/pipelines/master/components/sample/keras/train_classifier/tests/testdata/' + 'model_config.json').content\n\nxor_model_config = Path('./tests/testdata').joinpath('model_config.json').read_text()\n# print('xor_model_config: ', type(xor_model_config), xor_model_config)\n\n\n#Load the component\ntrain_op = kfp.components.load_component_from_file('./component.yaml')\nkeras_convert_hdf5_model_to_tf_saved_model_op = kfp.components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/51e49282d9511e4b72736c12dc66e37486849c6e/components/_converters/KerasModelHdf5/to_TensorflowSavedModel/component.yaml')\n\n#Use the component as part of the pipeline\n@kfp.dsl.pipeline(name='Test keras/train_classifier', description='Pipeline to test keras/train_classifier component')\ndef pipeline_to_test_keras_train_classifier():\n    train_task = train_op(\n        training_set_features_path=training_set_features_local_path,\n        training_set_labels_path=training_set_labels_local_path,\n        model_config=xor_model_config,\n        number_of_classes=2,\n        number_of_epochs=10,\n        batch_size=32,\n    ).add_volume(k8s_client.V1Volume(name='data-processing',\n                                     host_path=k8s_client.V1HostPathVolumeSource(path='/home/docker/data_processing'))) \\\n    .add_volume_mount(k8s_client.V1VolumeMount(mount_path='/data_processing',\n                                               name='data-processing'))\n\n    keras_model_in_tf_format = keras_convert_hdf5_model_to_tf_saved_model_op(\n        model=train_task.outputs['output_model_uri'],\n    ).output\n    #Use train_task.outputs['output_model_uri'] to obtain the reference to the trained model URI that can be a passed to other pipeline tasks (e.g. for prediction or analysis)\n\n\n# If you run this command on a Jupyter notebook running on Kubeflow,\n# you can exclude the host parameter.\n# client = kfp.Client()\nclient = kfp.Client(host='http://localhost:8080/pipeline')\n\n# Compile, upload, and submit this pipeline for execution.\nclient.create_run_from_pipeline_func(pipeline_to_test_keras_train_classifier, arguments={},\n    mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE)"
  },
  {
    "repo": "ima-tester/sample-kubeflow-app-001",
    "file_path": "components/golang/component.py",
    "raw_url": "https://raw.githubusercontent.com/ima-tester/sample-kubeflow-app-001/master/components/golang/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef training_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('TRAINING_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=['/usr/local/bin/python', script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n\n"
  },
  {
    "repo": "ima-tester/sample-kubeflow-app-001",
    "file_path": "components/training/component.py",
    "raw_url": "https://raw.githubusercontent.com/ima-tester/sample-kubeflow-app-001/master/components/training/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef training_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('TRAINING_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=['/usr/local/bin/python', script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n\n"
  },
  {
    "repo": "davide-belfiori/kubeflow_test_pipelines",
    "file_path": "simple_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/davide-belfiori/kubeflow_test_pipelines/master/simple_pipeline.py",
    "content": "\"\"\"\n    End-to-End Kubeflow Pipeline, from data loading to model serving.\n\"\"\"\n\n# TODOs: \n#   - add minio_access_key and minio_secret_key as parameter for all components\n\n# --------------\n# --- IMPORT ---\n# --------------\n\nimport sys\nfrom kfp import dsl, compiler, components\nfrom typing import NamedTuple\n\n# -----------------\n# --- FUNCTIONS ---\n# -----------------\n\ndef load_dataset(m: float = 0.5,\n                 q: float = 1.0,\n                 noise_mean: float = 0,\n                 noise_scale: float = 0.1,\n                 test_size: float = 0.3,\n                 random_state: int = 123,\n                 sep: str = ',',\n                 minio_endpoint: str = \"minio-service.kubeflow\",\n                 bucket_name: str = \"datasets\",\n                 update_dataset: bool = False):\n    \"\"\"\n    Function for data loading.\n    \"\"\"\n    # >>> IMPORT\n    import pandas as pd\n    import numpy as np\n    from sklearn.model_selection import train_test_split\n    from minio import Minio\n\n    print(\"--- Loading dataset ---\")\n\n    # >>> CONNECT TO MINIO\n    minio_client = Minio(endpoint=minio_endpoint+\":9000\",\n                         access_key=\"minio\",\n                         secret_key=\"minio123\",\n                         secure=False)\n    \n    if not minio_client.bucket_exists(bucket_name=bucket_name):\n        print(\"--- Creating bucket {} ---\".format(bucket_name))\n        minio_client.make_bucket(bucket_name=bucket_name)\n\n    obj_names = list(map(lambda obj: obj.object_name, minio_client.list_objects(bucket_name=bucket_name)))\n    if not update_dataset and \"simple_pipeline_dataset/\" in obj_names:   \n        # >>> DOWNLOAD DATASET FROM MINIO\n        minio_client.fget_object(bucket_name=bucket_name,\n                                    object_name=\"simple_pipeline_dataset/train_data.csv\",\n                                    file_path=\"data/train_data.csv\")\n        minio_client.fget_object(bucket_name=bucket_name,\n                                    object_name=\"simple_pipeline_dataset/test_data.csv\",\n                                    file_path=\"data/test_data.csv\")\n        print(\"--- Dataset found ---\")\n        return\n    else:\n        print(\"--- Creating Dataset ---\")\n        # >>> CREATE DATA\n        X = np.linspace(-10, 10, 100)\n        Y = m * X + q\n        rs = np.random.RandomState(np.random.MT19937(seed = random_state))\n        Y += rs.normal(loc = noise_mean, scale = noise_scale, size = Y.shape)\n\n        # >>> TRAIN-TEST SPLIT\n        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, shuffle=True, random_state=random_state)\n        train_df = pd.DataFrame({\"X\": X_train, \"Y\": Y_train})\n        test_df = pd.DataFrame({\"X\": X_test, \"Y\": Y_test})\n\n        # >>> SAVE DATA LOCALLY\n        train_df.to_csv(f\"data/train_data.csv\", sep=sep, index=False)\n        test_df.to_csv(f\"data/test_data.csv\", sep=sep, index=False)\n\n        # >>> UPLOAD TO MINIO\n        minio_client.fput_object(bucket_name = bucket_name, \n                                 object_name = \"simple_pipeline_dataset/train_data.csv\",\n                                 file_path = \"data/train_data.csv\")\n        minio_client.fput_object(bucket_name = bucket_name, \n                                 object_name = \"simple_pipeline_dataset/test_data.csv\",\n                                 file_path = \"data/test_data.csv\")\n\n        print(\"--- Data csv saved to MinIO location simple_pipeline_dataset/ ---\")\n\ndef print_data_desc(sep: str = ',', \n                    minio_endpoint: str = \"minio-service.kubeflow\", \n                    bucket_name: str = \"datasets\") -> NamedTuple('PrintDataOutput', [('mlpipeline_ui_metadata', 'UI_metadata')]):\n    \"\"\"\n    Print Dataset description.\n    \"\"\"\n    # >>> IMPORT\n    import pandas as pd\n    from minio import Minio\n    from collections import namedtuple\n    import json\n\n    # >>> CONNECT TO MINIO\n    minio_client = Minio(endpoint=minio_endpoint+\":9000\",\n                         access_key=\"minio\",\n                         secret_key=\"minio123\",\n                         secure=False)\n    if not minio_client.bucket_exists(bucket_name=bucket_name):\n        raise ValueError(\"{} bucket does not exists.\".format(bucket_name))\n\n    # >>> DOWNLOAD DATASET FROM MINIO\n    try:\n        minio_client.fget_object(bucket_name=bucket_name,\n                                    object_name=\"simple_pipeline_dataset/train_data.csv\",\n                                    file_path=\"data/train_data.csv\")\n        minio_client.fget_object(bucket_name=bucket_name,\n                                    object_name=\"simple_pipeline_dataset/test_data.csv\",\n                                    file_path=\"data/test_data.csv\")\n    except:\n        raise RuntimeError(\"Error while loading dataset.\")\n    \n    # >>> READ DATASET\n    train_df = pd.read_csv(f\"data/train_data.csv\", sep=sep)\n    test_df = pd.read_csv(f\"data/test_data.csv\", sep=sep)\n    \n    # >>> PRINT DESCRIPTION\n    print(\"--- TRAIN SET ---\")\n    print(\"> Info :\")\n    print(train_df.info())\n    print(\"> Description :\")\n    print(train_df.describe())\n\n    print(\"--- TEST SET ---\")\n    print(\"> Info :\")\n    print(test_df.info())\n    print(\"> Description :\")\n    print(test_df.describe())\n\n    train_head = train_df.head().to_csv(index = False, header = False)\n    test_head = test_df.head().to_csv(index = False, header = False)\n\n    metadata = {\n        'outputs' : [{\n            'type': 'table',\n            'format': 'csv',\n            'storage': 'inline',\n            'header': train_df.columns.to_list(),\n            'source': train_head\n        },\n        {\n            'type': 'table',\n            'format': 'csv',\n            'storage': 'inline',\n            'header': test_df.columns.to_list(),\n            'source': test_head\n        }]\n    }\n    output = namedtuple('PrintDataOutput', ['mlpipeline_ui_metadata'])\n    return output(json.dumps(metadata))\n\ndef train_model(sep: str = ',',\n                save_model: bool = True,\n                model_name: str = \"simple_pipeline_model\",\n                minio_endpoint: str = \"minio-service.kubeflow\",\n                data_bucket: str = \"datasets\",\n                model_bucket: str = \"models\") -> NamedTuple('TrainModelOutput', [('mlpipeline_ui_metadata', 'UI_metadata')]):\n    \"\"\"\n    Train a Linear Regression model.\n    \"\"\"\n    # >>> IMPORT\n    import pandas as pd\n    from sklearn.linear_model import LinearRegression\n    from minio import Minio\n    import joblib\n    import plotly.graph_objects as go\n    from collections import namedtuple\n    import json\n\n    # >>> CONNECT TO MINIO\n    minio_client = Minio(endpoint=minio_endpoint+\":9000\",\n                         access_key=\"minio\",\n                         secret_key=\"minio123\",\n                         secure=False)\n    if not minio_client.bucket_exists(bucket_name=data_bucket):\n        raise ValueError(\"{} bucket does not exists.\".format(data_bucket))\n\n    # >>> DOWNLOAD DATASET FROM MINIO\n    try:\n        minio_client.fget_object(bucket_name=data_bucket,\n                                    object_name=\"simple_pipeline_dataset/train_data.csv\",\n                                    file_path=\"data/train_data.csv\")\n    except:\n        raise RuntimeError(\"Error while loading dataset.\")\n\n    # >>> SPLIT DATA\n    train_df = pd.read_csv(f\"data/train_data.csv\", sep=sep)\n    X_train = train_df.loc[:,\"X\"].values.reshape(-1, 1)\n    Y_train = train_df.loc[:,\"Y\"].values.reshape(-1, 1)\n\n    # >>> TRAIN MODEL\n    model = LinearRegression()\n    model.fit(X=X_train, y=Y_train)\n\n    # >>> SAVE MODEL\n    if save_model:\n        # Dump model\n        model_filename = \"data/\" + model_name + \".joblib\"\n        joblib.dump(model, filename=model_filename)\n        # Upload to MinIO\n        if not minio_client.bucket_exists(bucket_name=model_bucket):\n            print(\"--- Creating bucket {} ---\".format(model_bucket))\n            minio_client.make_bucket(bucket_name=model_bucket)\n        try:\n            minio_client.fput_object(bucket_name = model_bucket, \n                                    object_name = \"simple_pipeline/\"+ model_name + \".joblib\",\n                                    file_path = model_filename)\n        except:\n            raise RuntimeError(\"Error while uploading model.\") \n        print(\"--- Model saved to MinIO location simple_pipeline/ ---\")\n\n    # >>> VISUALIZE MODEL\n    score = model.score(X_train, Y_train)\n    Y_pred = model.predict(X_train)\n    fig = go.Figure()\n    fig.add_trace(go.Scatter(x=X_train[:,0], y=Y_train[:,0], mode=\"markers\", name=\"train data\"))\n    fig.add_trace(go.Scatter(x=X_train[:,0], y=Y_pred[:,0], mode=\"lines\", name=\"model\"))\n    fig.update_layout({\n        'title': \"Model on train set (R2: {:.2f})\".format(score),\n        'xaxis_title': \"X\",\n        'yaxis_title': \"Y\"\n    })\n    fig.write_html(\"data/train_plot.html\")\n    minio_client.fput_object(bucket_name = model_bucket, \n                             object_name = \"simple_pipeline/train_plot.html\",\n                             file_path = \"data/train_plot.html\")\n    metadata = {\n        'outputs' : [{\n            'source': \"minio://{}/simple_pipeline/train_plot.html\".format(model_bucket),\n            'type': 'web-app',\n    }]}\n    output = namedtuple('TrainModelOutput', ['mlpipeline_ui_metadata'])\n    return output(json.dumps(metadata))\n\ndef eval_model(minio_endpoint: str = \"minio-service.kubeflow\",\n               data_bucket: str = \"datasets\",\n               sep: str = ',',\n               model_bucket: str = \"models\",\n               model_name: str = \"simple_pipeline_model\") -> NamedTuple('EvalModelOutput', [('mlpipeline_ui_metadata', 'UI_metadata'), ('mlpipeline_metrics', 'Metrics')]):\n    \"\"\"\n    Evaluate model on test set.\n    \"\"\"\n    # >>> IMPORT\n    from minio import Minio\n    import pandas as pd\n    import joblib\n    import plotly.graph_objects as go\n    from collections import namedtuple\n    import json\n\n    # >>> CONNECT TO MINIO\n    minio_client = Minio(endpoint=minio_endpoint+\":9000\",\n                         access_key=\"minio\",\n                         secret_key=\"minio123\",\n                         secure=False)\n    if not minio_client.bucket_exists(bucket_name=data_bucket):\n        raise ValueError(\"{} bucket does not exists.\".format(data_bucket))\n    if not minio_client.bucket_exists(bucket_name=model_bucket):\n        raise ValueError(\"{} bucket does not exists.\".format(model_bucket))\n\n    # >>> DOWNLOAD DATASET AND MODEL FROM MINIO\n    try:\n        minio_client.fget_object(bucket_name=data_bucket,\n                                 object_name=\"simple_pipeline_dataset/test_data.csv\",\n                                 file_path=\"data/test_data.csv\")\n        minio_client.fget_object(bucket_name=model_bucket,\n                                 object_name=\"simple_pipeline/\"+ model_name + \".joblib\",\n                                 file_path=\"data/\" + model_name + \".joblib\")\n    except:\n        raise RuntimeError(\"Error while loading dataset and model.\")\n\n    # >>> SPLIT DATA\n    test_df = pd.read_csv(f\"data/test_data.csv\", sep=sep)\n    X_test = test_df.loc[:,\"X\"].values.reshape(-1, 1)\n    Y_test = test_df.loc[:,\"Y\"].values.reshape(-1, 1)\n\n    # >>> LOAD MODEL\n    model = joblib.load(filename=\"data/\" + model_name + \".joblib\")\n\n    # >>> EVAL MODEL\n    score = model.score(X=X_test, y=Y_test)\n    Y_pred = model.predict(X_test)\n    fig = go.Figure()\n    fig.add_trace(go.Scatter(x=X_test[:,0], y=Y_test[:,0], mode=\"markers\", name=\"test data\"))\n    fig.add_trace(go.Scatter(x=X_test[:,0], y=Y_pred[:,0], mode=\"lines\", name=\"model\"))\n    fig.update_layout({\n        'title': \"Model on test set (R2: {:.2f})\".format(score),\n        'xaxis_title': \"X\",\n        'yaxis_title': \"Y\"\n    })\n    fig.write_html(\"data/test_plot.html\")\n    minio_client.fput_object(bucket_name = model_bucket, \n                             object_name = \"simple_pipeline/test_plot.html\",\n                             file_path = \"data/test_plot.html\")\n    metadata = {\n        'outputs' : [{\n            'source': \"minio://{}/simple_pipeline/test_plot.html\".format(model_bucket),\n            'type': 'web-app',\n    }]}\n\n    # >>> LOG METRICS\n    print(\"Score on test set: {:.2f} %\".format(score * 100))\n    metrics = {\n      'metrics': [{\n          'name': 'test-r2-score',\n          'numberValue':  float(score),\n          'format' : \"PERCENTAGE\"\n        }]}\n    \n    output = namedtuple('EvalModelOutput', ['mlpipeline_ui_metadat', 'mlpipeline_metrics'])\n    return output(json.dumps(metadata), json.dumps(metrics))\n\ndef serve_model(service_name: str = \"simple-serving\",\n                add_service_version: bool = True,\n                namespace: str = None,\n                kserve_version: str = \"v1beta1\",\n                model_bucket: str = \"models\",\n                model_name: str = \"simple_pipeline_model\",\n                service_account_name: str = \"kserve-service-credentials\",\n                update_credentials: bool = False,\n                minio_endpoint: str = \"minio-service.kubeflow\",\n                minio_access_key: str = \"minio\",\n                minio_secret_key: str = \"minio123\",\n                access_key_name: str = None,\n                secret_key_name: str = None,\n                profile: str = \"default\",\n                use_https: bool = False,\n                verify_ssl: bool = False):\n    \"\"\"\n    Create an InferenceService for the trained model.\n    \"\"\"\n    # >>> IMPORT\n    from kserve import (\n        KServeClient, \n        utils, \n        constants, \n        V1beta1SKLearnSpec, \n        V1beta1PredictorSpec,\n        V1beta1InferenceServiceSpec,\n        V1beta1InferenceService\n    )\n    from kubernetes import client\n    import base64\n    import datetime\n\n    KServe = KServeClient()\n    if namespace == None:\n        namespace = utils.get_default_target_namespace()\n\n    # >>> MINIO CREDENTIALS\n    if update_credentials:\n        # Encode access and secret key in base64\n        b64_access_key = base64.b64encode(minio_access_key.encode(\"ascii\"))\n        b64_access_key = b64_access_key.decode(\"ascii\")\n        b64_secret = base64.b64encode(minio_secret_key.encode(\"ascii\"))\n        b64_secret = b64_secret.decode(\"ascii\")\n        # Write credentials\n        if access_key_name == None or access_key_name == \"\":\n            access_key_name = constants.S3_ACCESS_KEY_ID_DEFAULT_NAME\n        if secret_key_name == None or secret_key_name == \"\":\n            secret_key_name = constants.S3_SECRET_ACCESS_KEY_DEFAULT_NAME\n        credentials = \"[{profile}]\\n{access_key_name}={access_key}\\n{secret_key_name}={secret_key}\".format(\n            profile = profile,\n            access_key_name = access_key_name,\n            access_key = b64_access_key, \n            secret_key_name = secret_key_name,\n            secret_key = b64_secret)\n        with open(\"data/credentials\", \"wt\") as cred_file:\n            cred_file.write(credentials)\n        KServe.set_credentials(storage_type='S3',\n                                namespace=namespace,\n                                credentials_file='data/credentials',\n                                service_account=service_account_name,\n                                s3_profile=profile,\n                                s3_endpoint=minio_endpoint+\":9000\",\n                                s3_use_https='1' if use_https else '0',\n                                s3_verify_ssl='1' if verify_ssl else '0')\n        \n    # >>> INFERENCE SERVICE OPTIONS\n    api_version = constants.KSERVE_GROUP + '/' + kserve_version\n    storage_uri = \"s3://{}/simple_pipeline/{}.joblib\".format(model_bucket, model_name)\n    if add_service_version:\n        service_name = service_name + \"-\" + datetime.datetime.now().strftime(\"%Y-%m-%d--%H-%M-%S\")\n\n    # >>> INFERENCE SERVICE DEFINITION\n    metadata = client.V1ObjectMeta(name = service_name, namespace = namespace)\n    sklearn_spec = V1beta1SKLearnSpec(storage_uri = storage_uri)\n    predictor = V1beta1PredictorSpec(sklearn = sklearn_spec, service_account_name = service_account_name)\n    spec = V1beta1InferenceServiceSpec(predictor = predictor)\n    isvc = V1beta1InferenceService(api_version = api_version,\n                                   kind = constants.KSERVE_KIND,\n                                   metadata = metadata,\n                                   spec = spec)\n    \n    # >>> INFERENCE SERVICE CREATION\n    info = KServe.create(isvc)\n    print(info)\n\n# ----------------\n# --- PIPELINE ---\n# ----------------\n\n# > COMPONENTS\nload_dataset_component = components.create_component_from_func(\n    func = load_dataset,\n    base_image = \"python:3.8\",\n    packages_to_install = [\"pandas==2.0.0\", \n                            \"numpy==1.24.2\", \n                            \"scikit-learn==1.2.2\",\n                            \"minio==7.1.14\"]\n)\nprint_data_desc_component = components.create_component_from_func(\n    func = print_data_desc,\n    base_image = \"python:3.8\",\n    packages_to_install = [\"pandas==2.0.0\", \n                           \"minio==7.1.14\"]\n)\ntrain_model_component = components.create_component_from_func(\n    func = train_model,\n    base_image = \"python:3.8\",\n    packages_to_install = [\"pandas==2.0.0\", \n                           \"numpy==1.24.2\", \n                           \"scikit-learn==1.2.2\", \n                           \"plotly==5.14.1\",\n                           \"minio==7.1.14\",\n                           \"joblib==1.2.0\"] \n)\neval_model_component = components.create_component_from_func(\n    func = eval_model,\n    base_image = \"python:3.8\",\n    packages_to_install = [\"pandas==2.0.0\",\n                           \"plotly==5.14.1\",\n                           \"minio==7.1.14\",\n                           \"scikit-learn==1.2.2\",\n                           \"joblib==1.2.0\"]\n)\nserve_model_component = components.create_component_from_func(\n    func = serve_model,\n    base_image = \"python:3.8\",\n    packages_to_install = [\"kserve==0.10.1\",\n                           \"kubernetes==25.3.0\"]\n)\n\n# > PIPELINE\n@dsl.pipeline(\n        name=\"simple-pipeline\",\n        description=\"Simple pipeline example.\"\n)\ndef simple_pipeline(m: float = 0.5,\n                    q: float = 1.0,\n                    noise_mean: float = 0,\n                    noise_scale: float = 0.1,\n                    test_size: float = 0.3,\n                    random_state: int = 123,\n                    update_dataset: bool = False,\n                    sep: str = ',',\n                    save_model: bool = True,\n                    model_name: str = \"simple_pipeline_model\",\n                    minio_endpoint: str = \"minio-service.kubeflow\",\n                    data_bucket: str = \"datasets\",\n                    model_bucket: str = \"models\",\n                    inference_service_name: str = \"simple-serving\",\n                    add_inference_service_version: bool = True,\n                    namespace: str = None,\n                    kserve_version: str = \"v1beta1\",\n                    service_account_name: str = \"kserve-service-credentials\",\n                    update_credentials: bool = False,\n                    access_key_name: str = None,\n                    secret_key_name: str = None,\n                    profile: str = \"default\",\n                    use_https: bool = False,\n                    verify_ssl: bool = False):\n    \"\"\"\n    Pipeline function\n    \"\"\"\n    # >>> Create a volume for this pipeline\n    vop = dsl.VolumeOp(name = \"simple_pipeline_volume\", \n                        resource_name = \"simple_pipeline_volume\",\n                        size = \"1Gi\",\n                        modes = dsl.VOLUME_MODE_RWO)\n    # >>> Run tasks\n    load_data_task = load_dataset_component(m = m, \n                                            q = q,\n                                            noise_mean = noise_mean,\n                                            noise_scale = noise_scale,\n                                            test_size = test_size,\n                                            random_state = random_state,\n                                            sep = sep,\n                                            minio_endpoint = minio_endpoint,\n                                            bucket_name = data_bucket,\n                                            update_dataset = update_dataset).add_pvolumes({\"/data\": vop.volume})\n    print_desc_task = print_data_desc_component(sep = sep,\n                                                minio_endpoint = minio_endpoint,\n                                                bucket_name = data_bucket).add_pvolumes({\"/data\": vop.volume}).after(load_data_task)\n    train_model_task = train_model_component(sep = sep,\n                                            save_model = save_model,\n                                            model_name = model_name,\n                                            minio_endpoint = minio_endpoint,\n                                            data_bucket = data_bucket,\n                                            model_bucket = model_bucket).add_pvolumes({\"/data\": vop.volume}).after(load_data_task)\n    eval_model_task = eval_model_component(sep = sep,\n                                            model_name = model_name,\n                                            minio_endpoint = minio_endpoint,\n                                            data_bucket = data_bucket,\n                                            model_bucket = model_bucket).add_pvolumes({\"/data\": vop.volume}).after(train_model_task)\n    serve_model_task = serve_model_component(service_name = inference_service_name,\n                                             add_service_version = add_inference_service_version,\n                                             namespace = namespace,\n                                             kserve_version = kserve_version,\n                                             model_bucket = model_bucket,\n                                             model_name = model_name,\n                                             service_account_name = service_account_name,\n                                             update_credentials = update_credentials,\n                                             minio_endpoint = minio_endpoint,\n                                             minio_access_key = \"minio\",\n                                             minio_secret_key = \"minio123\",\n                                             access_key_name = access_key_name,\n                                             secret_key_name = secret_key_name,\n                                             profile = profile,\n                                             use_https = use_https,\n                                             verify_ssl = verify_ssl).add_pvolumes({\"/data\": vop.volume}).after(eval_model_task)\n    # >>> Disable caching\n    load_data_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n    print_desc_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n    train_model_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n    eval_model_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n    serve_model_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n    \n\nif __name__ == \"__main__\":\n    a_count = len(sys.argv)\n    if a_count > 1:\n        mode = sys.argv[1]\n    else:\n        mode = \"test\"\n    if mode == \"test\":\n        # >>> Run test\n        minio_endpoint = \"127.0.0.1\"\n        load_dataset(minio_endpoint = minio_endpoint)\n        print_data_desc(minio_endpoint = minio_endpoint)\n        train_model(minio_endpoint = minio_endpoint)\n        eval_model(minio_endpoint = minio_endpoint)\n        # serve_model(minio_endpoint = minio_endpoint)\n    elif mode == \"compile\":\n        # >>> Compile pipeline\n        compiler.Compiler().compile(\n            pipeline_func=simple_pipeline,\n            package_path='pipelines/SimplePipeline.yaml'\n        )\n    else:\n        print(\"USAGE: {} [OPTION]\".format(sys.argv[0]))\n        print(\"OPTIONS:\\n \\ttest: Run test.\\n \\tcompile Compile pipeline\")\n"
  },
  {
    "repo": "davide-belfiori/kubeflow_test_pipelines",
    "file_path": "test_minio_connection.py",
    "raw_url": "https://raw.githubusercontent.com/davide-belfiori/kubeflow_test_pipelines/master/test_minio_connection.py",
    "content": "\"\"\"\n    Kubeflow Pipeline for MinIO connection test.\n\"\"\"\n\n# --------------\n# --- IMPORT ---\n# --------------\n\nimport kfp\nimport sys\n\n# -----------------\n# --- CONSTANTS ---\n# -----------------\n\nPIPELINE_NAME = \"minio-connection-test\"\nPIPELINE_DESC = \"MinIO connection test pipeline\"\nPIPELINE_FILENAME = \"MinIOConnectionTest\"\n\n# ------------------\n# --- CONN. TEST ---\n# ------------------\n\ndef test_connection(endpoint_addr: str = \"minio-service.kubeflow.svc.cluster.local\",\n                    endpoint_port: str = \"9000\",\n                    access_key: str = \"minio\",\n                    secret_key: str = \"minio123\",\n                    secure: bool = False,\n                    remove_tmp_bucket: bool = True):\n    # > IMPORT\n    from minio import Minio\n    from datetime import datetime\n    # > MINIO CLIENT\n    minio_client = Minio(\n        endpoint = endpoint_addr + \":\" + endpoint_port,\n        access_key = access_key,\n        secret_key = secret_key,\n        secure = secure\n    )\n    # > CREATE AN EMPTY BUCKET\n    dt = datetime.now().strftime(\"%m%d%Y-%H%M%S\")\n    bucket_name = \"tmp-bucket-\"+dt\n    print(\"--- Creating bucket {} ---\".format(bucket_name))\n    minio_client.make_bucket(bucket_name=bucket_name)\n    # > REMOVE BUCKET\n    if remove_tmp_bucket:\n        print(\"--- Removing bucket {} ---\".format(bucket_name))\n        minio_client.remove_bucket(bucket_name=bucket_name)\n\n# ----------------\n# --- PIPELINE ---\n# ----------------\n\n# > DEFINE COMPONENTS\ntest_connection_component = kfp.components.create_component_from_func(\n    func = test_connection,\n    base_image = \"python:3.8\",\n    packages_to_install = [\"minio==7.1.14\"]\n)\n\n# > PIPELINE\n@kfp.dsl.pipeline(\n    name = PIPELINE_NAME,\n    description = PIPELINE_DESC\n)\ndef test_connection_pipeline(endpoint_addr: str = \"minio-service.kubeflow.svc.cluster.local\",\n                             endpoint_port: str = \"9000\",\n                             access_key: str = \"minio\",\n                             secret_key: str = \"minio123\",\n                             secure: bool = False,\n                             remove_tmp_bucket: bool = True):\n    # > RUN TASKS\n    test_connection_task = test_connection_component(endpoint_addr = endpoint_addr,\n                                                     endpoint_port = endpoint_port,\n                                                     access_key = access_key,\n                                                     secret_key = secret_key,\n                                                     secure = secure,\n                                                     remove_tmp_bucket = remove_tmp_bucket)\n    # > DISABLE CACHING\n    test_connection_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n\nif __name__ == \"__main__\":\n    a_count = len(sys.argv)\n    if a_count > 1:\n        mode = sys.argv[1]\n    else:\n        mode = \"test\"\n    if mode == \"test\":\n        # >>> Run test\n        minio_endpoint = \"127.0.0.1\"\n        test_connection(endpoint_addr=minio_endpoint)\n    elif mode == \"compile\":\n        # >>> Compile pipeline\n        kfp.compiler.Compiler().compile(\n            pipeline_func=test_connection_pipeline,\n            package_path='pipelines/' + PIPELINE_FILENAME + '.yaml'\n        )\n    else:\n        print(\"USAGE: {} [OPTION]\".format(sys.argv[0]))\n        print(\"OPTIONS:\\n \\ttest: Run test.\\n \\tcompile Compile pipeline\")"
  },
  {
    "repo": "ahmedyass/mnist-kubeflow-pipeline",
    "file_path": "components.py",
    "raw_url": "https://raw.githubusercontent.com/ahmedyass/mnist-kubeflow-pipeline/main/components.py",
    "content": "# data_processing.py\nfrom kfp.dsl import component, Output, Dataset\n\n@component(\n        packages_to_install=['tensorflow==2.4.0', 'numpy']\n)\ndef preprocess_data(\n    training_data_output: Output[Dataset],\n    test_data_output: Output[Dataset]\n):\n    import tensorflow as tf\n    from tensorflow.keras.datasets import mnist\n    import numpy as np\n    import os\n\n    # Load dataset\n    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n\n    # Normalize data\n    x_train, x_test = x_train / 255.0, x_test / 255.0\n\n    # Save preprocessed data to the component's output paths\n    np.savez_compressed(training_data_output.path, x_train=x_train, y_train=y_train)\n    np.savez_compressed(test_data_output.path, x_test=x_test, y_test=y_test)\n\n# model_training.py\nfrom kfp.v2.dsl import component, Input, Output, Model, Dataset\n\n@component(\n        packages_to_install=['tensorflow==2.4.0', 'numpy']\n)\ndef train_model(\n    training_data: Input[Dataset],\n    model_output: Output[Model]\n):\n    import tensorflow as tf\n    from tensorflow.keras.models import Sequential\n    from tensorflow.keras.layers import Dense, Flatten\n    from tensorflow.keras.utils import to_categorical\n    import numpy as np\n\n    # Load preprocessed training data\n    with np.load(training_data.path) as data:\n        x_train = data['x_train']\n        y_train = data['y_train']\n    y_train = to_categorical(y_train)\n\n    # Define and compile the model\n    model = Sequential([\n        Flatten(input_shape=(28, 28)),\n        Dense(128, activation='relu'),\n        Dense(10, activation='softmax')\n    ])\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n    # Train the model\n    model.fit(x_train, y_train, epochs=5)\n\n    # Save the trained model\n    model.save(model_output.path)\n\n# model_evaluation.py\nfrom kfp.dsl import component, Input, Model\n\n@component(\n        packages_to_install=['tensorflow==2.4.0']\n)\ndef evaluate_model(\n    model_input: Input[Model]\n    ):\n    import tensorflow as tf\n    from tensorflow.keras.datasets import mnist\n    from tensorflow.keras.utils import to_categorical\n\n    # Load and preprocess test dataset\n    (_, _), (x_test, y_test) = mnist.load_data()\n    x_test = x_test / 255.0\n    y_test = to_categorical(y_test)\n\n    # Load the model from the input path provided by Kubeflow\n    model = tf.keras.models.load_model(model_input.path)\n\n    # Evaluate the model\n    loss, accuracy = model.evaluate(x_test, y_test)\n    print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")\n\n# pipeline.py\nfrom kfp.dsl import pipeline\nimport kfp.compiler as compiler\nfrom data_loading import preprocess_data\nfrom model_evaluation import evaluate_model\nfrom model_training import train_model\n\n@pipeline(\n  name='mnist-classification-pipeline',\n  description='A pipeline that processes MNIST data and trains a model.'\n)\ndef mnist_pipeline():\n    preprocess_task = preprocess_data()\n    train_task = train_model(\n        training_data=preprocess_task.outputs['training_data_output']\n    )\n    evaluate_task = evaluate_model(\n        model_input=train_task.outputs['model_output']\n    )\n    \n# Compile the pipeline\ncompiler.Compiler().compile(\n    pipeline_func=mnist_pipeline,\n    package_path='../pipeline/mnist_pipeline.yaml'\n)\n"
  },
  {
    "repo": "ahmedyass/mnist-kubeflow-pipeline",
    "file_path": "pipeline_def.py",
    "raw_url": "https://raw.githubusercontent.com/ahmedyass/mnist-kubeflow-pipeline/main/pipeline_def.py",
    "content": "from kfp.dsl import pipeline\nimport kfp.compiler as compiler\nfrom data_loading import load_dataset\nfrom data_processing import preprocessing\nfrom model_building import model_building\nfrom model_training import model_training\n\n@pipeline(\n    name='mnist-classifier-dev',\n    description='Detect digits'\n)\ndef mnist_pipeline(hyperparameters: dict):\n    load_task = load_dataset()\n    preprocess_task = preprocessing(\n        x_train_artifact = load_task.outputs[\"x_train_artifact\"],\n        x_test_artifact = load_task.outputs[\"x_test_artifact\"]\n    )\n\n    model_building_task = model_building()\n\n    training_task = model_training(\n        ml_model = model_building_task.outputs[\"ml_model\"],\n        x_train_processed = preprocess_task.outputs[\"x_train_processed\"],\n        x_test_processed = preprocess_task.outputs[\"x_test_processed\"],\n        y_train_artifact = load_task.outputs[\"y_train_artifact\"],\n        y_test_artifact = load_task.outputs[\"y_test_artifact\"],\n        hyperparameters = hyperparameters\n    )\n\n    \n# Compile the pipeline\ncompiler.Compiler().compile(\n    pipeline_func=mnist_pipeline,\n    package_path='output/mnist_pipeline.yaml'\n)\n"
  },
  {
    "repo": "ahmedyass/mnist-kubeflow-pipeline",
    "file_path": "data_loading/__init__.py",
    "raw_url": "https://raw.githubusercontent.com/ahmedyass/mnist-kubeflow-pipeline/main/data_loading/__init__.py",
    "content": "from kfp.dsl import component, Output, Dataset\n\n@component(\n        base_image=\"tensorflow/tensorflow\"\n)\ndef load_dataset(x_train_artifact: Output[Dataset],\n                 x_test_artifact: Output[Dataset],\n                 y_train_artifact: Output[Dataset],\n                 y_test_artifact: Output[Dataset]\n    ):\n    '''\n    get dataset from Keras and load it separating input from output and train from test\n    '''\n    import numpy as np\n    from tensorflow import keras\n    import os\n   \n    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n    \n    np.save(\"/tmp/x_train.npy\",x_train)\n    os.rename(\"/tmp/x_train.npy\", x_train_artifact.path)\n    \n    np.save(\"/tmp/y_train.npy\",y_train)\n    os.rename(\"/tmp/y_train.npy\", y_train_artifact.path)\n    \n    np.save(\"/tmp/x_test.npy\",x_test)\n    os.rename(\"/tmp/x_test.npy\", x_test_artifact.path)\n    \n    np.save(\"/tmp/y_test.npy\",y_test)\n    os.rename(\"/tmp/y_test.npy\", y_test_artifact.path)\n"
  },
  {
    "repo": "ahmedyass/mnist-kubeflow-pipeline",
    "file_path": "data_processing/__init__.py",
    "raw_url": "https://raw.githubusercontent.com/ahmedyass/mnist-kubeflow-pipeline/main/data_processing/__init__.py",
    "content": "from kfp.dsl import component, Input, Output, Dataset, Metrics\n\n@component(\n        packages_to_install['numpy']\n)\ndef preprocessing(metrics : Output[Metrics], x_train_processed : Output[Dataset], x_test_processed: Output[Dataset],\n                  x_train_artifact: Input[Dataset], x_test_artifact: Input[Dataset]):\n    ''' \n    just reshape and normalize data\n    '''\n    import numpy as np\n    import os\n    \n    # load data artifact store\n    x_train = np.load(x_train_artifact.path) \n    x_test = np.load(x_test_artifact.path)\n    \n    # reshaping the data\n    # reshaping pixels in a 28x28px image with greyscale, canal = 1. This is needed for the Keras API\n    x_train = x_train.reshape(-1,28,28,1)\n    x_test = x_test.reshape(-1,28,28,1)\n    # normalizing the data\n    # each pixel has a value between 0-255. Here we divide by 255, to get values from 0-1\n    x_train = x_train / 255\n    x_test = x_test / 255\n    \n    #logging metrics using Kubeflow Artifacts\n    metrics.log_metric(\"Len x_train\", x_train.shape[0])\n    metrics.log_metric(\"Len y_train\", x_test.shape[0])\n   \n    \n    # save feuture in artifact store\n    np.save(\"tmp/x_train.npy\",x_train)\n    os.rename(\"tmp/x_train.npy\", x_train_processed.path)\n    \n    np.save(\"tmp/x_test.npy\",x_test)\n    os.rename(\"tmp/x_test.npy\", x_test_processed.path)"
  },
  {
    "repo": "ahmedyass/mnist-kubeflow-pipeline",
    "file_path": "model_building/__init__.py",
    "raw_url": "https://raw.githubusercontent.com/ahmedyass/mnist-kubeflow-pipeline/main/model_building/__init__.py",
    "content": "import os\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import component, Input, Output, Dataset, Model, Metrics, ClassificationMetrics\n\n@component(base_image=\"tensorflow/tensorflow\")\ndef model_building(ml_model : Output[Model]):\n    '''\n    Define the model architecture\n    This way it's more simple to change the model architecture and all the steps and indipendent\n    '''\n    from tensorflow import keras\n    import tensorflow as tf\n    import os\n    \n    #model definition\n    model = keras.models.Sequential()\n    model.add(keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=(28,28,1)))\n    model.add(keras.layers.MaxPool2D(2, 2))\n\n    model.add(keras.layers.Flatten())\n    model.add(keras.layers.Dense(64, activation='relu'))\n    model.add(keras.layers.Dense(32, activation='relu'))\n\n    model.add(keras.layers.Dense(10, activation='softmax'))\n    \n    #saving model\n    model.save(ml_model.path)"
  },
  {
    "repo": "ahmedyass/mnist-kubeflow-pipeline",
    "file_path": "model_evaluation/__init__.py",
    "raw_url": "https://raw.githubusercontent.com/ahmedyass/mnist-kubeflow-pipeline/main/model_evaluation/__init__.py",
    "content": "from kfp.dsl import component, Input, Model\n\n@component(\n        packages_to_install=['tensorflow==2.4.0']\n)\ndef evaluate_model(\n    model_input: Input[Model]\n    ):\n    import tensorflow as tf\n    from tensorflow.keras.datasets import mnist\n    from tensorflow.keras.utils import to_categorical\n\n    # Load and preprocess test dataset\n    (_, _), (x_test, y_test) = mnist.load_data()\n    x_test = x_test / 255.0\n    y_test = to_categorical(y_test)\n\n    # Load the model from the input path provided by Kubeflow\n    model = tf.keras.models.load_model(model_input.path)\n\n    # Evaluate the model\n    loss, accuracy = model.evaluate(x_test, y_test)\n    print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")\n"
  },
  {
    "repo": "ahmedyass/mnist-kubeflow-pipeline",
    "file_path": "model_training/__init__.py",
    "raw_url": "https://raw.githubusercontent.com/ahmedyass/mnist-kubeflow-pipeline/main/model_training/__init__.py",
    "content": "import os\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import component, Input, Output, Dataset, Model, Metrics, ClassificationMetrics\n\n@component(base_image=\"tensorflow/tensorflow\", packages_to_install=['scikit-learn'])\ndef model_training(\n    ml_model : Input[Model],\n    x_train_processed : Input[Dataset], x_test_processed: Input[Dataset],\n    y_train_artifact : Input[Dataset], y_test_artifact :Input[Dataset],\n    hyperparameters : dict, \n    metrics: Output[Metrics], classification_metrics: Output[ClassificationMetrics], model_trained: Output[Model]\n    ):\n    \"\"\"\n    Build the model with Keras API\n    Export model metrics\n    \"\"\"\n    from tensorflow import keras\n    import tensorflow as tf\n    import numpy as np\n    import os\n    import glob\n    from sklearn.metrics import confusion_matrix\n    \n    #load dataset\n    x_train = np.load(x_train_processed.path)\n    x_test = np.load(x_test_processed.path)\n    y_train = np.load(y_train_artifact.path)\n    y_test = np.load(y_test_artifact.path)\n    \n    #load model structure\n    model = keras.models.load_model(ml_model.path)\n    \n    #reading best hyperparameters from katib\n    lr=float(hyperparameters[\"lr\"])\n    no_epochs = int(hyperparameters[\"num_epochs\"])\n    \n    #compile the model - we want to have a binary outcome\n    model.compile(tf.keras.optimizers.SGD(learning_rate=lr),\n              loss=\"sparse_categorical_crossentropy\",\n              metrics=['accuracy'])\n\n    \n    #fit the model and return the history while training\n    history = model.fit(\n      x=x_train,\n      y=y_train,\n      epochs=no_epochs,\n      batch_size=20,\n    )\n\n     \n    # Test the model against the test dataset\n    # Returns the loss value & metrics values for the model in test mode.\n    model_loss, model_accuracy = model.evaluate(x=x_test,y=y_test)\n    \n    #build a confusione matrix\n    y_predict = model.predict(x=x_test)\n    y_predict = np.argmax(y_predict, axis=1)\n    cmatrix = confusion_matrix(y_test, y_predict)\n    cmatrix = cmatrix.tolist()\n    numbers_list = ['0','1','2','3','4','5','6','7','8','9']\n    #log confusione matrix\n    classification_metrics.log_confusion_matrix(numbers_list,cmatrix)\n  \n    #Kubeflox metrics export\n    metrics.log_metric(\"Test loss\", model_loss)\n    metrics.log_metric(\"Test accuracy\", model_accuracy)\n    \n    #adding /1/ subfolder for TFServing and saving model to artifact store\n    model_trained.uri = model_trained.uri + '/1/'\n    keras.models.save_model(model,model_trained.path)"
  },
  {
    "repo": "MouadE0/Kubeflow-Regression-Pipeline",
    "file_path": "pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/MouadE0/Kubeflow-Regression-Pipeline/main/pipeline.py",
    "content": "import kfp\nfrom kfp import dsl\nfrom kfp.components import func_to_container_op\n# import matplotlib.pyplot as plt\n\n\n@func_to_container_op\ndef show_results(score_rf_reg: float, score_hist_gradient_boosting: float, score_decision_tree: float\n, score_elastic_net: float, score_lasso: float, \n    score_ridge: float, score_polynomial : float\n) -> None:\n    # Given the outputs from decision_tree and logistic regression components\n    # the results are shown.\n    print(f\"Random Forest regression (accuracy): {score_rf_reg}\")\n    print(f\"Hist Gradient Boosting regression (accuracy): {score_hist_gradient_boosting}\")\n    print(f\"Decision Tree regression (accuracy): {score_decision_tree}\")\n    print(f\"Elastic Net regression (accuracy): {score_elastic_net}\")\n    print(f\"Lasso regression (accuracy): {score_lasso}\")\n    print(f\"Ridge regression (accuracy): {score_ridge}\")\n    print(f\"Polynomial regression (accuracy): {score_polynomial}\")\n    # Best Model\n    best_model = max(score_rf_reg, score_hist_gradient_boosting, score_decision_tree, score_elastic_net)\n    # Switch Case for best model\n    if best_model == score_rf_reg:\n        print(\"Best Model is Random Forest\" + str(best_model))\n    elif best_model == score_hist_gradient_boosting:\n        print(\"Best Model is Hist Gradient Boosting\" + str(best_model))\n    elif best_model == score_decision_tree:\n        print(\"Best Model is Decision Tree\" + str(best_model))\n    elif best_model == score_elastic_net:\n        print(\"Best Model is Elastic Net\" + str(best_model))\n    elif best_model == score_lasso:\n        print(\"Best Model is Lasso\" + str(best_model))\n    elif best_model == score_polynomial:\n        print(\"Best Model is Polynomial\" + str(best_model))\n    elif best_model == score_ridge:\n        print(\"Best Model is Ridge\" + str(best_model))\n    else:\n        print(\"No Best Model\")\n\n    # Print Comparative Graph\n    # plt.bar([\"Linear Regression\", \"Random Forest\", \n    #         \"Hist Gradient Boosting\", \n    #         \"Decision Tree\", \"Elastic Net\", \"Lasso\", \n    #         \"Polynomial\", \"Ridge\"],\n    #         [score_lin_reg, score_rf_reg, score_hist_gradient_boosting, score_decision_tree, \n    #         score_elastic_net, score_lasso, score_polynomial, score_ridge])\n    # plt.title(\"Comparative Graph\")\n    # plt.xlabel(\"Models\")\n    # plt.show()\n\n\n\n@dsl.pipeline(\n    name=\"Pipeline\",\n    description=\"Applies Preprocess, Linear and Random Forest Regression problem.\",\n)\ndef test_pipeline():\n    # # Loads the yaml manifest for each component\n    # preprocess_clean = kfp.components.load_component_from_file(\n    #     \"preprocess_clean/preprocess_clean.yaml\"\n    # )\n    # preprocess_split = kfp.components.load_component_from_file(\n    #     \"preprocess_split/preprocess_split.yaml\"\n    # )\n\n    # # Loads the yaml manifest for each component\n    preprocess = kfp.components.load_component_from_file(\n        \"preprocess/preprocess.yaml\"\n    )\n    rf_regression = kfp.components.load_component_from_file(\n        \"rf_regression/rf_regression.yaml\"\n    )\n    hist_gradient_boosting = kfp.components.load_component_from_file(\n        \"hist_gradient_boosting_regression/hist_gradient_boosting_regression.yaml\"\n    )\n    decision_tree = kfp.components.load_component_from_file(\n        \"decision_tree_regression/decision_tree_regression.yaml\"\n    )\n    elastic_net = kfp.components.load_component_from_file(\n        \"elastic_net_regression/elastic_net_regression.yaml\"\n    )\n    lasso = kfp.components.load_component_from_file(\n        \"lasso_regression/lasso_regression.yaml\"\n    )\n    ridge = kfp.components.load_component_from_file(\n        \"ridge_regression/ridge_regression.yaml\"\n    )\n    polynomial = kfp.components.load_component_from_file(\n        \"polynomial_regression/polynomial_regression.yaml\"\n    )\n\n    # Preprocess data \n    preprocess_task = preprocess()\n    rf_regression_task = rf_regression(preprocess_task.output)\n    hist_gradient_boosting_task = hist_gradient_boosting(preprocess_task.output)\n    decision_tree = decision_tree(preprocess_task.output)\n    elastic_net = elastic_net(preprocess_task.output)\n    lasso = lasso(preprocess_task.output)\n    ridge = ridge(preprocess_task.output)\n    polynomial = polynomial(preprocess_task.output)\n\n    # the component \"show_results\" is called to print the results.\n    show_results(rf_regression_task.output, \n                hist_gradient_boosting_task.output, decision_tree.output, \n                elastic_net.output, lasso.output, ridge.output, polynomial.output)\n\n\nif __name__ == \"__main__\":\n    kfp.compiler.Compiler().compile(test_pipeline, \"pipeline.yaml\")\n"
  },
  {
    "repo": "tanzumlai/sample-kubeflow-pipeline",
    "file_path": "app/main.py",
    "raw_url": "https://raw.githubusercontent.com/tanzumlai/sample-kubeflow-pipeline/main/app/main.py",
    "content": "import kfp.dsl as dsl\nfrom kfp import Client\nimport logging\nimport warnings\nfrom . import utils\nimport datetime\n\nlogging.basicConfig()\nlogging.getLogger().setLevel(logging.INFO)\nwarnings.filterwarnings('ignore')\n\n\n@dsl.pipeline(\n    name='cifar_cnn_kfp',\n    description='Pipeline which trains and serves a CNN model using Keras.'\n)\ndef cifar_pipeline():\n\n    # Upload Dataset\n    upload_dataset = dsl.ContainerOp(\n        name='upload_dataset',\n        image='oawofolu/ml-image-processor:latest',\n        command=\"python\",\n        arguments=[\n            \"/app/main.py\",\n            \"mlflow_entry=upload_dataset\",\n            f\"mlflow_stage={utils.get_env_var('MLFLOW_STAGE')}\",\n            f\"git_repo={utils.get_env_var('GIT_REPO')}\",\n            f\"experiment_name={utils.get_env_var('EXPERIMENT_NAME')}\",\n            f\"environment_name={utils.get_env_var('ENVIRONMENT_NAME')}\",\n            f\"mlflow_s3_uri={utils.get_cmd_arg('mlflow_s3_uri') or utils.get_env_var('MLFLOW_S3_ENDPOINT_URL')}\",\n            f\"mlflow_tracking_uri={utils.get_cmd_arg('mlflow_tracking_uri') or utils.get_env_var('MLFLOW_TRACKING_URI')}\",\n            f\"default_token={datetime.datetime.now()}\"\n        ]\n    )\n\n    # Train Model\n    train_model = dsl.ContainerOp(\n        name='train_model',\n        image='oawofolu/ml-image-processor:latest',\n        command=\"python\",\n        arguments=[\n            \"/app/main.py\",\n            \"mlflow_entry=train_model\",\n            f\"mlflow_stage={utils.get_env_var('MLFLOW_STAGE')}\",\n            f\"git_repo={utils.get_env_var('GIT_REPO')}\",\n            f\"experiment_name={utils.get_env_var('EXPERIMENT_NAME')}\",\n            f\"environment_name={utils.get_env_var('ENVIRONMENT_NAME')}\",\n            f\"mlflow_s3_uri={utils.get_cmd_arg('mlflow_s3_uri') or utils.get_env_var('MLFLOW_S3_ENDPOINT_URL')}\",\n            f\"mlflow_tracking_uri={utils.get_cmd_arg('mlflow_tracking_uri') or utils.get_env_var('MLFLOW_TRACKING_URI')}\",\n            f\"default_token={datetime.datetime.now()}\"\n        ]\n    )\n    train_model.after(upload_dataset)\n\n    # Evaluate Model\n    evaluate_model = dsl.ContainerOp(\n        name='evaluate_model',\n        image='oawofolu/ml-image-processor:latest',\n        command=\"python\",\n        arguments=[\n            \"/app/main.py\",\n            \"mlflow_entry=evaluate_model\",\n            f\"mlflow_stage={utils.get_env_var('MLFLOW_STAGE')}\",\n            f\"git_repo={utils.get_env_var('GIT_REPO')}\",\n            f\"experiment_name={utils.get_env_var('EXPERIMENT_NAME')}\",\n            f\"environment_name={utils.get_env_var('ENVIRONMENT_NAME')}\",\n            f\"mlflow_s3_uri={utils.get_cmd_arg('mlflow_s3_uri') or utils.get_env_var('MLFLOW_S3_ENDPOINT_URL')}\",\n            f\"mlflow_tracking_uri={utils.get_cmd_arg('mlflow_tracking_uri') or utils.get_env_var('MLFLOW_TRACKING_URI')}\",\n            f\"default_token={datetime.datetime.now()}\"\n        ]\n    )\n    evaluate_model.after(train_model)\n\n    # Promote Model to Staging\n    promote_model_to_staging = dsl.ContainerOp(\n        name='promote_model_to_staging',\n        image='oawofolu/ml-image-processor:latest',\n        command=\"python\",\n        arguments=[\n            \"/app/main.py\",\n            \"mlflow_entry=promote_model_to_staging\",\n            f\"mlflow_stage={utils.get_env_var('MLFLOW_STAGE')}\",\n            f\"git_repo={utils.get_env_var('GIT_REPO')}\",\n            f\"experiment_name={utils.get_env_var('EXPERIMENT_NAME')}\",\n            f\"environment_name={utils.get_env_var('ENVIRONMENT_NAME')}\",\n            f\"mlflow_s3_uri={utils.get_cmd_arg('mlflow_s3_uri') or utils.get_env_var('MLFLOW_S3_ENDPOINT_URL')}\",\n            f\"mlflow_tracking_uri={utils.get_cmd_arg('mlflow_tracking_uri') or utils.get_env_var('MLFLOW_TRACKING_URI')}\",\n            f\"default_token={datetime.datetime.now()}\"\n        ]\n    )\n    promote_model_to_staging.after(evaluate_model)\n\n\nif __name__ == '__main__':\n    logging.info(f\"Compiling Kubeflow pipeline...host={utils.get_env_var('KUBEFLOW_PIPELINES_HOST')}\")\n    import kfp.compiler as compiler\n    compiler.Compiler().compile(cifar_pipeline, __file__ + '.zip')\n\n    logging.info(\"Generating new experiment run...\")\n    client = Client(host=f'{utils.get_env_var(\"KUBEFLOW_PIPELINES_HOST\")}')\n    cifar_experiment = client.create_experiment(name=utils.get_env_var('EXPERIMENT_NAME'))\n    cifar_run = client.run_pipeline(cifar_experiment.id, 'cifar-pipeline', __file__ + '.zip')\n    logging.info(\"Run completed.\")\n"
  },
  {
    "repo": "Our-Glass/kubeflow_pipelines_example",
    "file_path": "pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/Our-Glass/kubeflow_pipelines_example/main/pipeline.py",
    "content": "import kfp\r\nimport kfp.dsl as dsl\r\nfrom kubernetes import client as k8s\r\nfrom kubernetes import client as k8s_client\r\n\r\nEXPERIMENT_NAME = 'Sentiment Analysis Pipeline'\r\nIMAGE_USERNAME = <your docker hub username>\r\n\r\nclient = kfp.Client()\r\n\r\n@dsl.pipeline(\r\n    name='Sentiment Analysis Pipeline',\r\n    description='A pipeline to analyze sentiment of BTS videos'\r\n)\r\ndef sentiment_analysis_pipeline(\r\n\r\n    data_extraction_image=f'{IMAGE_USERNAME}/dataextraction:latest',\r\n    preprocessing_image=f'{IMAGE_USERNAME}/preprocessing:latest',\r\n    data_file='/mnt/bts.data',\r\n    train_file='/mnt/bts_train.data',\r\n    test_file='/mnt/bts_test.data',\r\n    validation_file='/mnt/bts_validation.data',\r\n    preprocess_file='/mnt/bts_preprocessed.data',\r\n    split_size=0.2,\r\n\r\n):\r\n\r\n    vop = dsl.VolumeOp(\r\n        name=\"create_pvc\",\r\n        resource_name=\"bts-pvc\",\r\n        modes=dsl.VOLUME_MODE_RWO,\r\n        size=\"1Gi\"\r\n    )\r\n\r\n    data_extraction_op = dsl.ContainerOp(\r\n            name='data_extraction',\r\n            image=data_extraction_image,\r\n            command=['python'],\r\n            arguments=['/app/dataextraction.py',\"--data_file\", data_file,],\r\n            pvolumes={\"/mnt\": vop.volume}\r\n        )\r\n\r\n    preprocessing_op = dsl.ContainerOp(\r\n            name='preprocessing',\r\n            image=preprocessing_image,\r\n            command=['python'],\r\n            arguments=[\r\n                '/app/preprocessing.py',\r\n                '--data_file', data_file,\r\n                '--preprocess_file', preprocess_file,\r\n                '--train_file', train_file,\r\n                '--test_file', test_file,\r\n                '--validation_file', validation_file,\r\n                '--split_size', split_size,\r\n                ],\r\n            pvolumes={\"/mnt\": data_extraction_op.pvolume}\r\n        )\r\n\r\n\r\nif __name__ == '__main__':\r\n    import kfp.compiler as compiler\r\n    pipeline_func = sentiment_analysis_pipeline\r\n    pipeline_filename = pipeline_func.__name__ + '.pipeline.zip'\r\n    compiler.Compiler().compile(pipeline_func, pipeline_filename)\r\n\r\n    try:\r\n        experiment = client.create_experiment(EXPERIMENT_NAME)\r\n    except:\r\n        experiment = client.create_experiment(EXPERIMENT_NAME)\r\n\r\n\r\n    arguments = {}\r\n    run_name = pipeline_func.__name__ + ' sentiment_run'\r\n    run_result = client.run_pipeline(experiment.id, run_name, pipeline_filename, arguments)\r\n\r\n    print(run_result)\r\n    print(run_name)\r\n    print(pipeline_filename)\r\n    print(arguments)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n            \r\n"
  },
  {
    "repo": "Our-Glass/kubeflow_pipelines_example",
    "file_path": "example_custom_module/pipeline_contenarized.py",
    "raw_url": "https://raw.githubusercontent.com/Our-Glass/kubeflow_pipelines_example/main/example_custom_module/pipeline_contenarized.py",
    "content": "import kfp\nfrom kfp.v2 import dsl\n# Load components\nfrom components.load_data.load_data import load_data\nfrom components.print_meta.print_meta import print_meta\nfrom components.split_data_custom.split_data_custom import split_data_custom as split_data\n\n# Declare pipeline\n@dsl.pipeline(name='pipeline artifcats')\ndef pipeline_custom_module_artifacts() -> str:\n        task_load_data = load_data()\n        task_split_data = split_data( x=task_load_data.outputs['X'], y=task_load_data.outputs['y'])\n        task_split_data.enable_caching = False\n        # task_print = print_meta(\n        #         x_train=task_split_data.outputs['x_train_out'],\n        #         x_test=task_split_data.outputs['x_test_out'])\n        return \"task_print.output\"\n\n\nNAME_EXPERIMENT = '02_pipeline_artifacts'\nOUTPUT_FILE = '02_pipeline_artifacts.zip'\n\n# Compile\nkfp.compiler.Compiler(mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE).compile(\n        pipeline_func=pipeline_custom_module_artifacts,\n        package_path=OUTPUT_FILE)\n        \n\n# Instance client\nclient = kfp.Client()\n\n# Create experiment\nmy_exp = client.create_experiment(name=NAME_EXPERIMENT)\n\n# Run pipeline\nmy_run = client.run_pipeline(\n        experiment_id = my_exp.id,\n        job_name = 'pipeline_custom_module_artifacts',\n        pipeline_package_path = OUTPUT_FILE\n)\n\"\"\" for running\npython contenerized_examples/02_pipeline_custom_component2/pipeline.py\n\"\"\""
  },
  {
    "repo": "Our-Glass/kubeflow_pipelines_example",
    "file_path": "example_custom_module/pipeline_lightweight.py",
    "raw_url": "https://raw.githubusercontent.com/Our-Glass/kubeflow_pipelines_example/main/example_custom_module/pipeline_lightweight.py",
    "content": "import kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import Input, Output, Dataset, Model\n\n\n@dsl.component(packages_to_install=['pandas', 'scikit-learn'])\ndef load_data(\n        X: Output[Dataset],\n        y: Output[Dataset]):\n        import pandas as pd\n        from sklearn.datasets import load_iris\n        \n        iris = load_iris()\n        X_data = pd.DataFrame(iris.data, columns=iris.feature_names)\n        y_data = pd.Series(iris.target)\n        \n        # transform series to dataframe and add column 'y'\n        y_data = pd.DataFrame(y_data, columns=['y'])\n        \n        X_data.to_csv(X.path)\n        y_data.to_csv(y.path)\n\n\n# split_data_light --> lightweigth component with custom python model\n@dsl.component(base_image='mevo12318/base_im:latest', packages_to_install=['pandas', 'scikit-learn'])\ndef split_data_light(\n        x: Input[Dataset],\n        y: Input[Dataset],\n        x_train_out: Output[Dataset],\n        x_test_out: Output[Dataset]):\n\n        # Import libraries\n        import pandas as pd\n        from src.utils import parse_df\n        from sklearn.model_selection import train_test_split\n        \n        # Load data from input\n        with open(x.path, \"rb\") as f:\n                X_data = pd.read_csv(f)\n        with open(y.path, \"rb\") as f:\n                y_data = pd.read_csv(f)\n\n        print(f\"type(x) = {type(X_data)}\")\n        print(f\"type(y) = {type(y_data)}\")\n                \n        # Split data\n        x_train, x_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=42)\n        \n        # Parse data\n        x_train, x_test = parse_df(x_train, x_test, y_train, y_test)\n        \n        # save data\n        with open(x_train_out.path, \"wb\") as f:\n                x_train.to_csv(f)\n        with open(x_test_out.path, \"wb\") as f:\n                x_test.to_csv(f)\n\n\n@dsl.component(packages_to_install=['pandas'])\ndef print_meta(\n        x_train: Input[Dataset],\n        x_test: Input[Dataset],\n        ) -> str:\n        import pandas as pd\n        \n\n        # Load data from input\n        with open(x_train.path, \"rb\") as f:\n                X_train = pd.read_csv(f)\n        with open(x_test.path, \"rb\") as f:\n                X_test = pd.read_csv(f)\n        \n        len_X_train = len(X_train)\n        path_X_train = x_train.path\n        uri_X_train = x_train.uri\n        \n        len_X_test = len(X_test)\n        path_X_test = x_test.path\n        uri_X_test = x_test.uri\n        \n        return f\"\"\"\n        len_X_train: {len_X_train}\n        path_X_train: {path_X_train}\n        uri_X_train: {uri_X_train}\n        \n        len_X_test: {len_X_test}\n        path_X_test: {path_X_test}\n        uri_X_test: {uri_X_test}\n        \"\"\"\n\n\n@dsl.component(packages_to_install=['pandas', 'scikit-learn', 'pickle5'])\ndef train_model(\n        x_train: Input[Dataset],\n        model_out: Output[Model]):\n        import pickle\n        import pandas as pd\n        from sklearn.linear_model import LinearRegression\n\n        # Load data from input\n        with open(x_train.path, \"rb\") as f:\n                X_train = pd.read_csv(f)\n        \n        # Train model\n        y_train = X_train['y']\n        X_train = X_train.drop(['y'], axis=1)\n        \n        # create model for classification iris dataset\n        model = LinearRegression()\n        model.fit(X_train, y_train)\n        \n        # save model\n        with open(model_out.path, \"wb\") as f:\n                pickle.dump(model, f)\n\n\n@dsl.component(packages_to_install=['pandas', 'pickle5'])\ndef test_model(\n        x_test: Input[Dataset],\n        model_in: Input[Model]) -> str:\n        import pickle\n        import pandas as pd\n\n        # Load data from input\n        with open(x_test.path, \"rb\") as f:\n                X_test = pd.read_csv(f)\n        \n        # Load model\n        with open(model_in.path, \"rb\") as f:\n                model = pickle.load(f)\n        \n        # Test model\n        y_test = X_test['y']\n        X_test = X_test.drop(['y'], axis=1)\n        \n        # Get metrics\n        score = model.score(X_test, y_test)\n        \n        return f\"score: {score}\"\n        \n\n# Declare pipeline\n@dsl.pipeline(name='pipeline artifcats')\ndef pipeline_custom_module_artifacts_light() -> str:\n        task_load_data = load_data()\n        task_split_data = split_data_light( x=task_load_data.outputs['X'], y=task_load_data.outputs['y'])\n        task_print = print_meta(\n                x_train=task_split_data.outputs['x_train_out'],\n                x_test=task_split_data.outputs['x_test_out'])        \n        task_train_model = train_model( x_train=task_split_data.outputs['x_train_out'])\n        task_test_model = test_model( x_test=task_split_data.outputs['x_test_out'], model_in=task_train_model.outputs['model_out'])\n\n        return task_print.output\n\n\nNAME_EXPERIMENT = '02_pipeline_artifacts_light'\nOUTPUT_FILE = '02_pipeline_artifacts_light.zip'\n\n# Compile\nkfp.compiler.Compiler(mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE).compile(\n        pipeline_func=pipeline_custom_module_artifacts_light,\n        package_path=OUTPUT_FILE)\n        \n\n# Instance client\nclient = kfp.Client()\n\n# Create experiment\nmy_exp = client.create_experiment(name=NAME_EXPERIMENT)\n\n# Run pipeline\nmy_run = client.run_pipeline(\n        experiment_id = my_exp.id,\n        job_name = 'pipeline_custom_module_artifacts_light',\n        pipeline_package_path = OUTPUT_FILE\n)\n\"\"\" for running\npython contenerized_examples/02_pipeline_custom_component2/pipeline.py\n\"\"\""
  },
  {
    "repo": "Our-Glass/kubeflow_pipelines_example",
    "file_path": "kfpv2/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/Our-Glass/kubeflow_pipelines_example/main/kfpv2/pipeline.py",
    "content": "\r\nfrom kfp.v2 import dsl\r\nfrom kubernetes import client as k8s\r\nfrom kubernetes import client as k8s_client\r\nfrom components.preprocessing.preprocessing import training_data_processing\r\nfrom components.dataextraction.dataextraction import get_data\r\nfrom components.train_linear.train_linear import train_linear_model\r\nfrom components.train_logistic.train_logistic import train_logistic_model\r\n\r\n\r\n# create a breast cancer pipeline\r\n@dsl.pipeline(\r\n    name='Breast Cancer Pipeline',\r\n    description='A pipeline to predict breast cancer'\r\n)\r\ndef breast_cancer_pipeline(\r\n    split_size: float = 0.2\r\n):\r\n    # get data\r\n    get_data_task = get_data()\r\n    # split data\r\n    split_data_task = training_data_processing(dataset_input=get_data_task.outputs['dataset'],split_size=split_size)\r\n    train_linear_model_task = train_linear_model(dataset=split_data_task.outputs['dataset'])\r\n    train_logistic_model_task = train_logistic_model(dataset=split_data_task.outputs['dataset'])\r\n    # TODO: add a component to evaluate the model\r\n\r\nif __name__ == '__main__':\r\n    import kfp\r\n    EXPERIMENT_NAME = 'brest_cancer'\r\n    pipelineGzFile = 'breast_cancer.zip'\r\n\r\n    kfp.compiler.Compiler(mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE).compile(\r\n        pipeline_func=breast_cancer_pipeline,\r\n        package_path=pipelineGzFile)\r\n\r\n\r\n\r\n# run the pipeline\r\n\r\n    client = kfp.Client()\r\n    my_exp = client.create_experiment(\r\n        name=EXPERIMENT_NAME\r\n    )\r\n    my_run = client.run_pipeline(\r\n        my_exp.id, \r\n        'breast_cancer_pipeline', \r\n        pipelineGzFile,\r\n        #enable_caching=False\r\n    )"
  },
  {
    "repo": "mouachan/kubeflow-pipeline-project",
    "file_path": "pipeline/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/mouachan/kubeflow-pipeline-project/main/pipeline/pipeline.py",
    "content": "from kfp import dsl\n\n@dsl.pipeline(\n    name='My Kubeflow Pipeline',\n    description='A pipeline that orchestrates an R script and a Python script.'\n)\ndef my_pipeline():\n    # Define the R script component\n    r_script_op = dsl.ContainerOp(\n        name='run-r-script',\n        image='your-r-image:latest',  # Replace with your R Docker image\n        command=['Rscript', '/scripts/script.R'],\n        file_outputs={\n            'output': '/output/output.txt'  # Adjust as necessary\n        }\n    )\n    \n    # Define the Python script component\n    python_script_op = dsl.ContainerOp(\n        name='run-python-script',\n        image='your-python-image:latest',  # Replace with your Python Docker image\n        command=['python', '/scripts/script.py'],\n        arguments=['--input', r_script_op.outputs['output']],\n        file_outputs={\n            'output': '/output/output.txt'  # Adjust as necessary\n        }\n    )\n\n    # Set dependencies\n    python_script_op.after(r_script_op)\n\nif __name__ == '__main__':\n    import kfp.compiler as compiler\n    compiler.Compiler().compile(my_pipeline, 'pipeline.yaml')"
  },
  {
    "repo": "chuyangzh/kubeflow-spark-pipeline",
    "file_path": "kubeflow-pipeline/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/chuyangzh/kubeflow-spark-pipeline/main/kubeflow-pipeline/pipeline.py",
    "content": "import kfp\nfrom kfp import dsl\nfrom kfp.components import load_component_from_text\n\n# Define the Spark component\nspark_component = load_component_from_text(\"\"\"\nname: Spark Weather Job\ndescription: Runs the Spark job to process weather data.\nimplementation:\n    container:\n        image: spark-weather-job:latest\n        command:\n        - spark-submit\n        - /app/spark-job.py\n\"\"\")\n\n@dsl.pipeline(\n    name=\"Spark Weather Pipeline\",\n    description=\"A pipeline that runs a Spark job to process weather data.\"\n)\ndef spark_weather_pipeline():\n    # Add the Spark component to the pipeline\n    spark_task = spark_component()\n\n# Compile the pipeline for v1 compatibility\nif __name__ == \"__main__\":\n    kfp.compiler.Compiler().compile(\n        pipeline_func=spark_weather_pipeline,\n        package_path=\"kubeflow-pipeline/spark_weather_pipeline.yaml\"\n    )\n"
  },
  {
    "repo": "j-cunanan/kubeflow-pipelines-TFOD",
    "file_path": "license.py",
    "raw_url": "https://raw.githubusercontent.com/j-cunanan/kubeflow-pipelines-TFOD/master/license.py",
    "content": "import json\n\nimport kfp\nfrom kfp.components import func_to_container_op, InputPath, OutputPath\nfrom kfp.components import load_component_from_file, load_component_from_url\n\n\n@func_to_container_op\ndef dl_pipeline_config(\n    pipeline_config: OutputPath(),\n):\n    import subprocess\n    import sys\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"requests\"])\n    import requests\n        \n    with open(pipeline_config, 'wb') as file:\n        r = requests.get(\"https://www.dropbox.com/s/ftl82cdyf5twgev/licence_plate.config?dl=1\", allow_redirects=True)\n        file.write(r.content)\n\n@func_to_container_op\ndef list_dir_files_python_op(input_dir_path: InputPath()):\n    import os\n    dir_items = os.listdir(input_dir_path)\n    for dir_item in dir_items:\n        print(dir_item)\n\n@func_to_container_op\ndef read_files_python_op(input_dir_path: InputPath()):\n    with open(input_dir_path, 'r') as f:\n        print(f.read())\n\ntrain_eval_op = load_component_from_file(\"train_eval/component.yaml\")\ntfrecordgen_op = load_component_from_file(\"TFRecordsGen/component.yaml\")\nloadweights_op = load_component_from_file(\"LoadWeights/component.yaml\")\ntfserving_op = load_component_from_file(\"TFServing/component.yaml\")\n\n@kfp.dsl.pipeline(name='First Pipeline', description='describe this')\ndef my_pipeline(\n    model_name: str = 'model',\n    num_train_steps: int =100,\n                data_url='https://www.dropbox.com/s/gx9zmtlkjlfg1m5/license.zip?dl=1',\n                converter_script_url='https://www.dropbox.com/s/j18c859mqkzs52o/create_licence_plate_tf_record.py?dl=1',\n                pbtxt_url='https://www.dropbox.com/s/jy7bzzgeax9b95t/licence_plate_label_map.pbtxt?dl=1',\n                weights_url='http://download.tensorflow.org/models/object_detection/tf2/20200711/efficientdet_d0_coco17_tpu-32.tar.gz',\n                num_shards: int = 1):\n\n    dl_task = dl_pipeline_config()\n    loadweights_task = loadweights_op(weights_url=weights_url)\n    conversion_task = tfrecordgen_op(data_url=data_url,\n                                     converter_script_url=converter_script_url,\n                                     pbtxt_url=pbtxt_url,\n                                     num_shards=num_shards)\n\n    list_dir_files_python_op(conversion_task.outputs['output_dir'])\n\n    modelling_task = train_eval_op(pipeline_config=dl_task.output,\n                                   record_summaries=False,\n                                   label_map=conversion_task.outputs['label_map'],\n                                   data=conversion_task.outputs['output_dir'],\n                                   pretrained_weights=loadweights_task.output,\n                                   num_train_steps=num_train_steps,\n                                   model=model_name\n                                  )\n\n    modelling_task.container.set_gpu_limit(1)\n\n    list_dir_files_python_op(modelling_task.outputs['model_dir'])\n    list_dir_files_python_op(modelling_task.outputs['export_dir'])\n    \n    tfserving_task = tfserving_op(model_name=model_name,\n                                 export_dir=modelling_task.outputs['export_dir'])\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(my_pipeline, 'my_pipeline.tar.gz')\n    "
  },
  {
    "repo": "Minseojeonn/Kubeflow_capstone_pipeline",
    "file_path": "Pipeline/pipe/tmp.py",
    "raw_url": "https://raw.githubusercontent.com/Minseojeonn/Kubeflow_capstone_pipeline/master/Pipeline/pipe/tmp.py",
    "content": "import kfp\nimport kfp.components as comp\nfrom kfp import dsl\nfrom kfp import onprem\n@dsl.pipeline(\n    name='ms_min',\n    description='minseo'\n)\n\ndef create_inference_model():\n    kserve_op = comp.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/'\n                                               'master/components/kserve/component.yaml')\n    \n    isvc_yaml = '''\n                apiVersion: \"serving.kserve.io/v1beta1\"\n                kind: \"InferenceService\"\n                metadata:\n                    name: \"torchserve-temp\"\n                    namespace: \"kubeflow-user-example-com\"\n                spec:\n                    predictor:\n                        serviceAccountName: 'sa'\n                        pytorch:\n                            storageUri: s3://efficientcluster\n                            resources:\n                                limits:\n                                    cpu: \"5\"\n                '''\n    \n    return kserve_op(action=\"apply\",\n              inferenceservice_yaml=isvc_yaml\n              )    \n\ndef ms_min_pipeline():\n\n    start = dsl.ContainerOp(\n        name=\"Start\",\n        command=[\"/bin/sh\"],\n        image=\"python:3.9\"\n       \n    )\n    \n    end = dsl.ContainerOp(\n        name=\"END\",\n        command=[\"/bin/sh\"],\n        image=\"python:3.9\"\n    )\n\n\n    add_p = dsl.ContainerOp(\n        name=\"load_data\",\n        image=\"raichal2000/capstonepipe:4\",\n        command=[\"python\",\"1_videos_to_img_with_extract_face_and_crop.py\"],\n        arguments=[\n            '--video','/data/src', '--output', '/data/faces'\n        ]\n    ).apply(onprem.mount_pvc(\"data\", volume_name=\"data\", volume_mount_path=\"/data\"))\n\n    ml = dsl.ContainerOp(\n        name=\"training pipeline\",\n        image=\"raichal2000/capstonepipe:8\",\n        command=[\"python\",\"2_train_efficient_vit.py\"],\n        arguments=[\n            '--path', '/data/faces', '--savepath', '/train/pth_saves'\n        ]\n    ).set_gpu_limit(1).apply(onprem.mount_pvc(\"data\", volume_name=\"data\", volume_mount_path=\"/data\")).apply(onprem.mount_pvc(\"train\", volume_name=\"train\", volume_mount_path=\"/train\"))\n\n    mar = dsl.ContainerOp(\n        name=\"Creating Marfile\",\n        command=[\"/bin/sh\"],\n        image=\"python:3.9\",\n        arguments=[\n            \"-c\",\n            \"cd /mar/efficient_vit_mar; pip install torchserve torch-model-archiver torch-workflow-archiver; torch-model-archiver --model-name efficient --version 1.0 --serialized-file /train/efficient_vit.pth --extra-files ./src/efficient_vit.py,./src/architecture.yaml,./src/temp2.png,./src/utils.py,./src/tester.py,./src/albu.py  --handler ./src/handler.py --requirements-file ./src/requirements.txt\"\n        ],  # pip install => create mar file => make model_store folder => mv marfile to model_store\n    ).apply(onprem.mount_pvc(\"mar\", volume_name=\"mar\", volume_mount_path=\"/mar\")).apply(onprem.mount_pvc(\"train\", volume_name=\"train\", volume_mount_path=\"/train\"))\n    \n    add_p.after(start)\n    ml.after(add_p)\n    mar.after(ml)\n    inference_model = create_inference_model()\n    inference_model.apply(onprem.mount_pvc(pvc_name=\"mar\", volume_name=\"mar\", volume_mount_path=\"/mar\"))\n    inference_model.after(mar)\n    end.after(inference_model)\n    \n    \nif __name__ == \"__main__\":\n    import kfp.compiler as compiler\n    compiler.Compiler().compile(ms_min_pipeline, __file__ + \".tar.gz\")"
  },
  {
    "repo": "Louis5499/Kubeflow-mnist-pipeline",
    "file_path": "tfJob_kfServing_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/Louis5499/Kubeflow-mnist-pipeline/master/tfJob_kfServing_pipeline.py",
    "content": "import json\nfrom kfp import components\nimport kfp.dsl as dsl\n\n@dsl.pipeline(\n    name=\"Launch kubeflow tfjob & kfserving template\",\n    description=\"An example to launch tfjob.\"\n)\ndef mnist_pipeline(\n        name=\"mnist\",\n        namespace=\"kubeflow\",\n        workerNum=2,\n        deleteAfterDone=False):\n    tfjob_launcher_op = components.load_component_from_file(\"./tfJobComponent.yaml\")\n    kfserving_op = components.load_component_from_file(\"./kfServingComponent.yaml\")\n    duplicated_gs_deletion_op = components.load_component_from_file(\"./duplicatedGsDeleteComponent.yaml\")\n    bucket = \"kf_second_test\"\n    \n    chief = {\n      \"replicas\": 1,\n      \"template\": {\n        \"metadata\": {\n          \"annotations\": {\n            \"sidecar.istio.io/inject\": \"false\"\n          }\n        },\n        \"spec\": {\n          \"containers\": [\n            {\n              \"command\": [\n                \"/usr/bin/python\",\n                \"/opt/model.py\",\n                \"--tf-model-dir=$(modelDir)\",\n                \"--tf-export-dir=$(exportDir)\",\n                \"--tf-train-steps=$(trainSteps)\",\n                \"--tf-batch-size=$(batchSize)\",\n                \"--tf-learning-rate=$(learningRate)\"\n              ],\n              \"env\": [\n                {\n                  \"name\": \"modelDir\",\n                  \"value\": f\"gs://{bucket}/my-model\"\n                },\n                {\n                  \"name\": \"exportDir\",\n                  \"value\": f\"gs://{bucket}/my-model/export\"\n                },\n                {\n                  \"name\": \"trainSteps\",\n                  \"value\": \"200\"\n                },\n                {\n                  \"name\": \"batchSize\",\n                  \"value\": \"100\"\n                },\n                {\n                  \"name\": \"learningRate\",\n                  \"value\": \"0.01\"\n                }\n              ],\n              \"image\": \"gcr.io/kubeflow-examples/mnist/model:build-1202842504546750464\",\n              \"name\": \"tensorflow\",\n              \"workingDir\": \"/opt\"\n            }\n          ],\n          \"restartPolicy\": \"OnFailure\",\n          \"serviceAccount\": \"k8s-sa\"\n        }\n      }\n    }\n    worker = {}\n    if workerNum > 0:\n      worker = {\n        \"replicas\": workerNum,\n        \"template\": {\n          \"metadata\": {\n            \"annotations\": {\n              \"sidecar.istio.io/inject\": \"false\"\n            }\n          },\n          \"spec\": {\n            \"containers\": [\n              {\n                \"command\": [\n                  \"/usr/bin/python\",\n                  \"/opt/model.py\",\n                  \"--tf-model-dir=$(modelDir)\",\n                  \"--tf-export-dir=$(exportDir)\",\n                  \"--tf-train-steps=$(trainSteps)\",\n                  \"--tf-batch-size=$(batchSize)\",\n                  \"--tf-learning-rate=$(learningRate)\"\n                ],\n                \"env\": [\n                  {\n                    \"name\": \"modelDir\",\n                    \"value\": f\"gs://{bucket}/my-model\"\n                  },\n                  {\n                    \"name\": \"exportDir\",\n                    \"value\": f\"gs://{bucket}/my-model/export\"\n                  },\n                  {\n                    \"name\": \"trainSteps\",\n                    \"value\": \"200\"\n                  },\n                  {\n                    \"name\": \"batchSize\",\n                    \"value\": \"100\"\n                  },\n                  {\n                    \"name\": \"learningRate\",\n                    \"value\": \"0.01\"\n                  }\n                ],\n                \"image\": \"gcr.io/kubeflow-examples/mnist/model:build-1202842504546750464\",\n                \"name\": \"tensorflow\",\n                \"workingDir\": \"/opt\"\n              }\n            ],\n            \"restartPolicy\": \"OnFailure\",\n            \"serviceAccount\": \"k8s-sa\"\n          }\n        }\n      }\n\n    ps = {\n      \"replicas\": 1,\n      \"template\": {\n        \"metadata\": {\n          \"annotations\": {\n            \"sidecar.istio.io/inject\": \"false\"\n          }\n        },\n        \"spec\": {\n          \"containers\": [\n            {\n              \"command\": [\n                \"/usr/bin/python\",\n                \"/opt/model.py\",\n                \"--tf-model-dir=$(modelDir)\",\n                \"--tf-export-dir=$(exportDir)\",\n                \"--tf-train-steps=$(trainSteps)\",\n                \"--tf-batch-size=$(batchSize)\",\n                \"--tf-learning-rate=$(learningRate)\"\n              ],\n              \"env\": [\n                {\n                  \"name\": \"modelDir\",\n                  \"value\": f\"gs://{bucket}/my-model\"\n                },\n                {\n                  \"name\": \"exportDir\",\n                  \"value\": f\"gs://{bucket}/my-model/export\"\n                },\n                {\n                  \"name\": \"trainSteps\",\n                  \"value\": \"200\"\n                },\n                {\n                  \"name\": \"batchSize\",\n                  \"value\": \"100\"\n                },\n                {\n                  \"name\": \"learningRate\",\n                  \"value\": \"0.01\"\n                }\n              ],\n              \"image\": \"gcr.io/kubeflow-examples/mnist/model:build-1202842504546750464\",\n              \"name\": \"tensorflow\",\n              \"workingDir\": \"/opt\"\n            }\n          ],\n          \"restartPolicy\": \"OnFailure\",\n          \"serviceAccount\": \"k8s-sa\"\n        }\n      }\n    }\n    tfJobLauncher = tfjob_launcher_op(\n      name=name,\n      namespace=namespace,\n      worker_spec=worker,\n      chief_spec=chief,\n      ps_spec=ps,\n      delete_finished_tfjob=deleteAfterDone\n    )\n\n    duplicatedGsDirDeletion = duplicated_gs_deletion_op(\n      bucket_name=bucket\n    ).after(tfJobLauncher)\n\n    kfserving_op(\n      name=\"kfserving\",\n      default_model_uri=f\"gs://{bucket}/my-model/export\",\n      model_name=\"main\",\n      transformer_custom_image=\"jackfantasy/image-transformer:v1\"\n    ).after(duplicatedGsDirDeletion)\n\nif __name__ == \"__main__\":\n    import kfp.compiler as compiler\n    compiler.Compiler().compile(mnist_pipeline, __file__ + \".tar.gz\")\n"
  },
  {
    "repo": "iqer/kubeflow_demo_pipeline",
    "file_path": "demo_pipeline1/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/iqer/kubeflow_demo_pipeline/main/demo_pipeline1/pipeline.py",
    "content": "import kfp\nfrom kfp import dsl\n\n\ndef preprocess_op():\n    return dsl.ContainerOp(\n        name='Preprocess Data',\n        image='shanau2/tf_pipeline_preprocess:v0.2',\n        arguments=[\n        ],\n        file_outputs={\n            'x_train': '/tmp/data/x_train.pickle',\n            'x_test': '/tmp/data/x_test.pickle',\n            'y_train': '/tmp/data/y_train.pickle',\n            'y_test': '/tmp/data/y_test.pickle',\n            'model_dir': 'tmp/data'\n        }\n    )\n\n\ndef train_op(x_train, y_train, model_dir):\n    return dsl.ContainerOp(\n        name='Train Model',\n        image='shanau2/tf_pipeline_train:v0.1',\n        arguments=[\n            '--x-train', x_train,\n            '--y_train', y_train,\n            '--model_dir', model_dir\n        ],\n        file_outputs={\n            'model_dir': model_dir,\n        }\n    )\n\n\n# def test_op(x_test, y_test, model_dir, output_dir):\n#     return dsl.ContainerOp(\n#         name='Test Model',\n#         image='shanau2/boston_pipeline_test:v3',\n#         arguments=[\n#             '--x_test', x_test,\n#             '--y_test', y_test,\n#             '--model_dir', model_dir,\n#             '--output-dir', output_dir,\n#         ],\n#         file_outputs={\n#             'output_dir': output_dir,\n#         }\n#     )\n\n\n# def deploy(output_dir):\n#     return dsl.ContainerOp(\n#         name='Deploy Model',\n#         image='shanau2/tf_pipeline_deploy_model:v0.1',\n#         arguments=[\n#             '--output_dir', output_dir\n#         ]\n#     )\n\n\n@dsl.pipeline(\n    name='Fashion MNIST Training Pipeline',\n    description='Fashion MNIST Training Pipeline to be executed on KubeFlow.'\n)\ndef training_pipeline():\n    _preprocess_op = preprocess_op()\n\n    _train_op = train_op(\n        dsl.InputArgumentPath(_preprocess_op.outputs['x_train']),\n        dsl.InputArgumentPath(_preprocess_op.outputs['y_train']),\n        dsl.InputArgumentPath(_preprocess_op.outputs['model_dir']),\n    ).after(_preprocess_op)\n\n    # _test_op = test_op(\n    #     dsl.InputArgumentPath(_preprocess_op.outputs['x_test']),\n    #     dsl.InputArgumentPath(_preprocess_op.outputs['y_test']),\n    #     dsl.InputArgumentPath(_train_op.outputs['model_dir'])\n    # ).after(_train_op)\n\n    # deploy_model_op(\n    #     dsl.InputArgumentPath(_train_op.outputs['model_dir'])\n    # ).after(_test_op)\n\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(training_pipeline, 'demo.tar.gz')\n\n"
  },
  {
    "repo": "iqer/kubeflow_demo_pipeline",
    "file_path": "demo_pipeline3/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/iqer/kubeflow_demo_pipeline/main/demo_pipeline3/pipeline.py",
    "content": "import kfp\nfrom kfp import dsl\n\n\ndef demo_op():\n    return dsl.ContainerOp(name='demo', image='demo:v0.0.1')\n\n\n@dsl.pipeline(\n    name='demo',\n    description='test'\n)\ndef pipeline():\n    op = demo_op()\n\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(pipeline, __file__ + '.zip')\n"
  },
  {
    "repo": "sagravat/kubeflow-pipelines-examples",
    "file_path": "chest-xray/pipelines/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/sagravat/kubeflow-pipelines-examples/master/chest-xray/pipelines/pipeline.py",
    "content": "#!/usr/bin/env python3\n# Copyright 2018 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport kfp.dsl as dsl\n\nclass ObjectDict(dict):\n  def __getattr__(self, name):\n    if name in self:\n      return self[name]\n    else:\n      raise AttributeError(\"No such attribute: \" + name)\n\n\n@dsl.pipeline(\n  name='my-pipeline',\n  description='preprocess pipeline'\n)\ndef run(\n    project,\n    bucket,\n    input_dir,\n    output_dir,\n    labels_file,\n    mode='cloud'\n):\n  \"\"\"Pipeline to train babyweight model\"\"\"\n  start_step = 1\n\n  # Step 1: create training dataset using Apache Beam on Cloud Dataflow\n  if start_step <= 1:\n    preprocess = dsl.ContainerOp(\n      name='preprocess',\n      # image needs to be a compile-time string\n      image='gcr.io/{}/dicom-preprocess:latest'.format(project),\n      arguments=[\n        '--input_dir', input_dir,\n        '--output_dir', output_dir,\n        '--labels_file', labels_file,\n        '--project', project,\n        '--mode', mode,\n      ],\n      file_outputs={'bucket': '/output.txt'}\n    )\n  else:\n    preprocess = ObjectDict({\n      'outputs': {\n        'bucket': bucket\n      }\n    })\n\n\n\nif __name__ == '__main__':\n  import kfp.compiler as compiler\n  import sys\n  if len(sys.argv) != 2:\n    print(\"Usage: pipeline pipeline-output-name\")\n    sys.exit(-1)\n  \n  filename = sys.argv[1]\n  compiler.Compiler().compile(run, filename)\n"
  },
  {
    "repo": "sagravat/kubeflow-pipelines-examples",
    "file_path": "chest-xray/pipelines/train_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/sagravat/kubeflow-pipelines-examples/master/chest-xray/pipelines/train_pipeline.py",
    "content": "#!/usr/bin/env python3\n# Copyright 2018 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport kfp.dsl as dsl\n\nclass ObjectDict(dict):\n  def __getattr__(self, name):\n    if name in self:\n      return self[name]\n    else:\n      raise AttributeError(\"No such attribute: \" + name)\n\n\n@dsl.pipeline(\n  name='train-pipeline',\n  description='train pipeline'\n)\ndef run(\n    project,\n    bucket,\n    train_file,\n    eval_file\n):\n  \"\"\"Pipeline to train babyweight model\"\"\"\n  start_step = 1\n\n  # Step 1: create training dataset using Apache Beam on Cloud Dataflow\n  if start_step <= 1:\n    train = dsl.ContainerOp(\n      name='train',\n      # image needs to be a compile-time string\n      image='gcr.io/{}/chest-xray-xfer-learning-train:latest'.format(project),\n      arguments=[\n        '--train_file', train_file,\n        '--eval_file', eval_file,\n      ],\n      file_outputs={'bucket': '/output.txt'}\n    )\n    train.set_gpu_limit(4)\n  else:\n    train = ObjectDict({\n      'outputs': {\n        'bucket': bucket\n      }\n    })\n\n\n\nif __name__ == '__main__':\n  import kfp.compiler as compiler\n  compiler.Compiler().compile(run, __file__ + '.tar.gz')\n"
  },
  {
    "repo": "luizrennocosta/kubeflow-ml-pipeline",
    "file_path": "src/deep_model_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/luizrennocosta/kubeflow-ml-pipeline/main/src/deep_model_pipeline.py",
    "content": "import kfp.dsl as dsl\n\n\n@dsl.pipeline(\n  name='NLP',\n  description='A pipeline demonstrating reproducible steps for NLP'\n)\ndef nlp_pipeline(\n        csv_url=\"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_multilingual_UK_v1_00.tsv.gz\",\n        embed_weights_url=\"http://nlp.stanford.edu/data/glove.42B.300d.zip\",\n        features_column=\"review_body\",\n        labels_column=\"product_category\",\n        raw_text_path='/mnt/text.data',\n        labels_path='/mnt/labels.data',\n        data_folder='/mnt/data',\n        clean_text_path='/mnt/clean.data',\n        tokens_path='/mnt/tokens.data',\n        tfidf_vectors_path='/mnt/tfidf.data',\n        model_prediction_path='/mnt/predicted_train.data',\n        tfidf_model_path='/mnt/tfidf.model',\n        word_index_path='/mnt/word_index.data',\n        embedded_matrix_path='/mnt/embedded_matrix.data',\n        pre_embedded_weights='/mnt/data/glove.42B.300d.txt',\n        train_ratio=0.98,\n        validation_ratio=0.01,\n        test_ratio=0.01,\n        num_words=20000,\n        sentence_max_length=50,\n        deep_model='/mnt/deep_model.model',\n        batch_size='100'):\n    \"\"\"\n    Pipeline\n    \"\"\"\n    vop = dsl.VolumeOp(\n      name='my-pvc',\n      resource_name=\"my-pvc\",\n      modes=[\"ReadWriteMany\"],\n      size=\"30Gi\"\n    )\n\n    download_step = dsl.ContainerOp(\n        name='data_downloader',\n        image='docker.io/cyferino/component-kubeflow:0.0.14',\n        command=\"python\",\n        arguments=[\n            \"/src/pipeline_steps/data_downloader/pipeline_step.py\",\n            \"--labels-path\", labels_path,\n            \"--data-folder\", data_folder,\n            \"--features-path\", raw_text_path,\n            \"--csv-url\", csv_url,\n            \"--features-column\", features_column,\n            \"--labels-column\", labels_column\n        ],\n        pvolumes={\"/mnt\": vop.volume}\n    )\n\n    download_embed_step = dsl.ContainerOp(\n        name='embed_weights_downloader',\n        image='docker.io/cyferino/component-kubeflow:0.0.14',\n        command=\"python\",\n        arguments=[\n            \"/src/pipeline_steps/weights_downloader/pipeline_step.py\",\n            \"--url\", embed_weights_url\n        ],\n        pvolumes={\"/mnt\": vop.volume}\n    )\n\n    clean_step = dsl.ContainerOp(\n        name='clean_text',\n        image='docker.io/cyferino/component-kubeflow:0.0.14',\n        command=\"python\",\n        arguments=[\n            \"/src/pipeline_steps/clean_text/pipeline_step.py\",\n            \"--in-path\", raw_text_path,\n            \"--out-path\", clean_text_path,\n        ],\n        pvolumes={\"/mnt\": vop.volume}\n    ).after(download_step)\n\n    data_split_step = dsl.ContainerOp(\n        name='data_splitter',\n        image='docker.io/cyferino/component-kubeflow:0.0.14',\n        command=\"python\",\n        arguments=[\n            \"/src/pipeline_steps/train_val_test/pipeline_step.py\",\n            \"--in-path\", clean_text_path,\n            \"--labels-path\", labels_path,\n            \"--out-folder\", data_folder,\n            \"--train-ratio\", train_ratio,\n            \"--validation-ratio\", validation_ratio,\n            \"--test-ratio\", test_ratio\n        ],\n        pvolumes={\"/mnt\": vop.volume}\n    ).after(clean_step)\n\n    tokenize_step = dsl.ContainerOp(\n        name='tokenize',\n        image='docker.io/cyferino/component-kubeflow:0.0.14',\n        command=\"python\",\n        arguments=[\n            \"/src/pipeline_steps/tokenize/pipeline_step.py\",\n            \"--in-path\", \"/mnt/data/train.data\",\n            \"--out-path\", tokens_path,\n            \"--word-index-path\", word_index_path,\n            \"--tokenizer-path\", \"/mnt/tokenizer.model\",\n            \"--action\", \"train\",\n            \"--num-words\", num_words,\n            \"--max-length\", sentence_max_length,\n        ],\n        pvolumes={\"/mnt\": vop.volume}\n    ).after(data_split_step)\n\n    tokenize_step_val = dsl.ContainerOp(\n        name='val_tokenize',\n        image='docker.io/cyferino/component-kubeflow:0.0.14',\n        command=\"python\",\n        arguments=[\n            \"/src/pipeline_steps/tokenize/pipeline_step.py\",\n            \"--in-path\", \"/mnt/data/val.data\",\n            \"--out-path\", \"/mnt/data/tokenized_val.data\",\n            \"--word-index-path\", word_index_path,\n            \"--tokenizer-path\", \"/mnt/tokenizer.model\",\n            \"--action\", \"predict\",\n            \"--num-words\", num_words,\n            \"--max-length\", sentence_max_length,\n        ],\n        pvolumes={\"/mnt\": vop.volume}\n    ).after(tokenize_step)\n\n    tokenize_step_test = dsl.ContainerOp(\n        name='test_tokenize',\n        image='docker.io/cyferino/component-kubeflow:0.0.14',\n        command=\"python\",\n        arguments=[\n            \"/src/pipeline_steps/tokenize/pipeline_step.py\",\n            \"--in-path\", \"/mnt/data/test.data\",\n            \"--out-path\", \"/mnt/data/tokenized_test.data\",\n            \"--word-index-path\", word_index_path,\n            \"--tokenizer-path\", \"/mnt/tokenizer.model\",\n            \"--action\", \"predict\",\n            \"--num-words\", num_words,\n            \"--max-length\", sentence_max_length,\n        ],\n        pvolumes={\"/mnt\": vop.volume}\n    ).after(tokenize_step)\n\n    embedding_step = dsl.ContainerOp(\n        name='embedder',\n        image='docker.io/cyferino/component-kubeflow:0.0.14',\n        command=\"python\",\n        arguments=[\n            \"/src/pipeline_steps/embedder/pipeline_step.py\",\n            \"--in-path\", word_index_path,\n            \"--out-path\", embedded_matrix_path,\n            \"--glove-file\", pre_embedded_weights,\n        ],\n        pvolumes={\"/mnt\": vop.volume}\n    ).after(tokenize_step, download_embed_step)\n\n    train_step = dsl.ContainerOp(\n        name='predictor',\n        image='docker.io/cyferino/component-kubeflow:0.0.14',\n        command=\"python\",\n        arguments=[\n            \"/src/pipeline_steps/deep_classifier/pipeline_step.py\",\n            \"--in-path\", tokens_path,\n            \"--embed-weight\", embedded_matrix_path,\n            \"--out-path\", model_prediction_path,\n            \"--action\", \"train\",\n            \"--model-path\", deep_model,\n            \"--epochs\", 20,\n            \"--batch-size\", 1024,\n            \"--optimizer\", \"rmsprop\",\n            \"--metrics\", [\"acc\"]\n        ],\n        pvolumes={\"/mnt\": vop.volume}\n    ).after(embedding_step)\n\n    predict_val = dsl.ContainerOp(\n        name='val_predictor',\n        image='docker.io/cyferino/component-kubeflow:0.0.14',\n        command=\"python\",\n        arguments=[\n            \"/src/pipeline_steps/deep_classifier/pipeline_step.py\",\n            \"--in-path\", \"/mnt/data/tokenized_val.data\",\n            \"--embed-weight\", embedded_matrix_path,\n            \"--out-path\", '/mnt/predicted_val.data',\n            \"--action\", \"predict\",\n            \"--model-path\", deep_model,\n        ],\n        pvolumes={\"/mnt\": vop.volume}\n    ).after(train_step, tokenize_step_val)\n\n    predict_test = dsl.ContainerOp(\n        name='test_predictor',\n        image='docker.io/cyferino/component-kubeflow:0.0.14',\n        command=\"python\",\n        arguments=[\n            \"/src/pipeline_steps/deep_classifier/pipeline_step.py\",\n            \"--in-path\", \"/mnt/data/tokenized_test.data\",\n            \"--embed-weight\", embedded_matrix_path,\n            \"--out-path\", '/mnt/predicted_test.data',\n            \"--action\", \"predict\",\n            \"--model-path\", deep_model,\n        ],\n        pvolumes={\"/mnt\": vop.volume}\n    ).after(train_step, tokenize_step_test)\n\n    evaluate_model = dsl.ContainerOp(\n        name='model_evaluator',\n        image='docker.io/cyferino/component-kubeflow:0.0.14',\n        command=\"python\",\n        arguments=[\n            \"/src/pipeline_steps/deep_classifier/pipeline_step.py\",\n            \"--data-folder\", \"/mnt/data\",\n            \"--predicted-train-data\", '/mnt/predicted_train.data',\n            \"--predicted-val-data\", '/mnt/predicted_val.data',\n            \"--predicted-test-data\", '/mnt/predicted_test.data'\n        ],\n        pvolumes={\"/mnt\": vop.volume},\n        file_outputs={\n            'mlpipeline-metrics': '/mlpipeline-metrics.json',\n            'mlpipeline-ui-data': '/mlpipeline-ui-data.json'\n        },\n        output_artifact_paths={\n            'mlpipeline-metrics': '/mlpipeline-metrics.json',\n            'mlpipeline-ui-data': '/mlpipeline-ui-data.json'\n        }\n    ).after(train_step, predict_val, predict_test)\n\n\nif __name__ == '__main__':\n    import kfp.compiler as compiler\n    compiler.Compiler().compile(nlp_pipeline, __file__ + '.tar.gz')"
  },
  {
    "repo": "luizrennocosta/kubeflow-ml-pipeline",
    "file_path": "src/lr_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/luizrennocosta/kubeflow-ml-pipeline/main/src/lr_pipeline.py",
    "content": "import kfp.dsl as dsl\nimport yaml\n\n\n@dsl.pipeline(\n  name='NLP',\n  description='A pipeline demonstrating reproducible steps for NLP using logistic regression'\n)\ndef nlp_pipeline(\n        csv_url=\"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_multilingual_UK_v1_00.tsv.gz\",\n        embed_weights_url=\"http://nlp.stanford.edu/data/glove.42B.300d.zip\",\n        features_column=\"review_body\",\n        labels_column=\"product_category\",\n        raw_text_path='/mnt/text.data',\n        labels_path='/mnt/labels.data',\n        data_folder='/mnt/data',\n        clean_text_path='/mnt/clean.data',\n        tokens_path='/mnt/tokens.data',\n        tfidf_vectors_path='/mnt/tfidf.data',\n        model_prediction_path='/mnt/predicted_train.data',\n        tfidf_model_path='/mnt/tfidf.model',\n        word_index_path='/mnt/word_index.data',\n        embedded_matrix_path='/mnt/embedded_matrix.data',\n        pre_embedded_weights='/mnt/data/glove.42B.300d.txt',\n        train_ratio=0.98,\n        validation_ratio=0.01,\n        test_ratio=0.01,\n        num_words=20000,\n        sentence_max_length=50,\n        deep_model='/mnt/deep_model.model',\n        batch_size='100'):\n    \"\"\"\n    Pipeline\n    \"\"\"\n    vop = dsl.VolumeOp(\n      name='lr-pvc',\n      resource_name=\"lr-pvc\",\n      modes=[\"ReadWriteMany\"],\n      size=\"30Gi\"\n    )\n\n    download_step = dsl.ContainerOp(\n        name='data_downloader',\n        image='docker.io/cyferino/component-kubeflow:0.0.14',\n        command=\"python\",\n        arguments=[\n            \"/src/pipeline_steps/data_downloader/pipeline_step.py\",\n            \"--labels-path\", labels_path,\n            \"--data-folder\", data_folder,\n            \"--features-path\", raw_text_path,\n            \"--csv-url\", csv_url,\n            \"--features-column\", features_column,\n            \"--labels-column\", labels_column\n        ],\n        pvolumes={\"/mnt\": vop.volume}\n    )\n\n    clean_step = dsl.ContainerOp(\n        name='clean_text',\n        image='docker.io/cyferino/component-kubeflow:0.0.14',\n        command=\"python\",\n        arguments=[\n            \"/src/pipeline_steps/clean_text/pipeline_step.py\",\n            \"--in-path\", raw_text_path,\n            \"--out-path\", clean_text_path,\n        ],\n        pvolumes={\"/mnt\": vop.volume}\n    ).after(download_step)\n\n    data_split_step = dsl.ContainerOp(\n        name='data_splitter',\n        image='docker.io/cyferino/component-kubeflow:0.0.14',\n        command=\"python\",\n        arguments=[\n            \"/src/pipeline_steps/train_val_test/pipeline_step.py\",\n            \"--in-path\", clean_text_path,\n            \"--labels-path\", labels_path,\n            \"--out-folder\", data_folder,\n            \"--train-ratio\", train_ratio,\n            \"--validation-ratio\", validation_ratio,\n            \"--test-ratio\", test_ratio\n        ],\n        pvolumes={\"/mnt\": vop.volume}\n    ).after(clean_step)\n\n    tfidf_step = dsl.ContainerOp(\n        name='tfidf',\n        image='docker.io/cyferino/component-kubeflow:0.0.14',\n        command=\"python\",\n        arguments=[\n            \"/src/pipeline_steps/tfidf_vectorizer/pipeline_step.py\",\n            \"--in-path\", \"/mnt/data/train.data\",\n            \"--out-path\", tokens_path,\n            \"--model-path\", \"/mnt/tfidf.model\",\n            \"--action\", \"train\",\n            \"--ngram-range\", 2,\n            \"--max-features\", 1000,\n        ],\n        pvolumes={\"/mnt\": vop.volume}\n    ).after(data_split_step)\n\n    tfidf_step_val = dsl.ContainerOp(\n        name='tfidf_val',\n        image='docker.io/cyferino/component-kubeflow:0.0.14',\n        command=\"python\",\n        arguments=[\n            \"/src/pipeline_steps/tfidf_vectorizer/pipeline_step.py\",\n            \"--in-path\", \"/mnt/data/val.data\",\n            \"--out-path\", \"/mnt/data/tokenized_val.data\",\n            \"--model-path\", \"/mnt/tfidf.model\",\n            \"--action\", \"predict\",\n        ],\n        pvolumes={\"/mnt\": vop.volume}\n    ).after(tfidf_step)\n\n    tfidf_step_test = dsl.ContainerOp(\n        name='tfidf_test',\n        image='docker.io/cyferino/component-kubeflow:0.0.14',\n        command=\"python\",\n        arguments=[\n            \"/src/pipeline_steps/tfidf_vectorizer/pipeline_step.py\",\n            \"--in-path\", \"/mnt/data/test.data\",\n            \"--out-path\", \"/mnt/data/tokenized_test.data\",\n            \"--model-path\", \"/mnt/tfidf.model\",\n            \"--action\", \"predict\",\n        ],\n        pvolumes={\"/mnt\": vop.volume}\n    ).after(tfidf_step)\n\n    train_step = dsl.ContainerOp(\n        name='predictor',\n        image='docker.io/cyferino/component-kubeflow:0.0.14',\n        command=\"python\",\n        arguments=[\n            \"/src/pipeline_steps/lr_text_model/pipeline_step.py\",\n            \"--in-path\", tokens_path,\n            \"--out-path\", model_prediction_path,\n            \"--model-path\", '/mnt/lr_text.model',\n            \"--action\", \"train\",\n            \"--c-param\", 0.1,\n        ],\n        pvolumes={\"/mnt\": vop.volume}\n    ).after(tfidf_step)\n\n    predict_val = dsl.ContainerOp(\n        name='val_predictor',\n        image='docker.io/cyferino/component-kubeflow:0.0.14',\n        command=\"python\",\n        arguments=[\n            \"/src/pipeline_steps/lr_text_model/pipeline_step.py\",\n            \"--in-path\", \"/mnt/data/tokenized_val.data\",\n            \"--out-path\", '/mnt/predicted_val.data',\n            \"--model-path\", '/mnt/lr_text.model',\n            \"--action\", \"predict\",\n        ],\n        pvolumes={\"/mnt\": vop.volume}\n    ).after(train_step, tfidf_step_val)\n\n    predict_test = dsl.ContainerOp(\n        name='test_predictor',\n        image='docker.io/cyferino/component-kubeflow:0.0.14',\n        command=\"python\",\n        arguments=[\n            \"/src/pipeline_steps/lr_text_model/pipeline_step.py\",\n            \"--in-path\", \"/mnt/data/tokenized_test.data\",\n            \"--out-path\", '/mnt/predicted_test.data',\n            \"--model-path\", '/mnt/lr_text.model',\n            \"--action\", \"predict\",\n        ],\n        pvolumes={\"/mnt\": vop.volume}\n    ).after(train_step, tfidf_step_test)\n\n    evaluate_model = dsl.ContainerOp(\n        name='model_evaluator',\n        image='docker.io/cyferino/component-kubeflow:0.0.14',\n        command=\"python\",\n        arguments=[\n            \"/src/pipeline_steps/evaluate_model/pipeline_step.py\",\n            \"--data-folder\", \"/mnt/data\",\n            \"--predicted-train-data\", '/mnt/predicted_train.data',\n            \"--predicted-val-data\", '/mnt/predicted_val.data',\n            \"--predicted-test-data\", '/mnt/predicted_test.data'\n        ],\n        pvolumes={\"/mnt\": vop.volume},\n        file_outputs={'mlpipeline-metrics': '/mlpipeline-metrics.json',\n            'mlpipeline-ui-metadata': '/mlpipeline-ui-metadata.json'},\n        output_artifact_paths={'mlpipeline-metrics': '/mlpipeline-metrics.json',\n            'mlpipeline-ui-metadata': '/mlpipeline-ui-metadata.json'}\n    ).after(train_step, predict_val, predict_test)\n\n    seldon_config = yaml.load(open(\"seldon_production_pipeline.yaml\"))\n\n    deploy_step = dsl.ResourceOp(\n        name=\"seldondeploy\",\n        k8s_resource=seldon_config,\n        attribute_outputs={\"name\": \"{.metadata.name}\"})\n\n    deploy_step.after(train_step)\n\n\nif __name__ == '__main__':\n    import kfp.compiler as compiler\n    compiler.Compiler().compile(nlp_pipeline, __file__ + '_lr.tar.gz')"
  },
  {
    "repo": "Here2ServeU/kubeflow-ml-pipeline",
    "file_path": "pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/Here2ServeU/kubeflow-ml-pipeline/main/pipeline.py",
    "content": "import kfp\nfrom kfp import dsl\nfrom kfp.components import load_component_from_file\n\n@dsl.pipeline(name=\"Kubeflow ML Pipeline\", description=\"End-to-end ML pipeline\")\ndef pipeline():\n    data_load = load_component_from_file(\"components/data_load.py\")()\n    preprocess = load_component_from_file(\"components/preprocess.py\")(data_load.output)\n    train = load_component_from_file(\"components/train.py\")(preprocess.output)\n    evaluate = load_component_from_file(\"components/evaluate.py\")(preprocess.output, train.output)\n    deploy = load_component_from_file(\"components/deploy.py\")(evaluate.output)\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(pipeline, 'pipeline.yaml')\n\n"
  },
  {
    "repo": "javierdarksoul/Kubeflow-pipelines-tutorial",
    "file_path": "tutorial-1/sum_mul_paralell_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/javierdarksoul/Kubeflow-pipelines-tutorial/main/tutorial-1/sum_mul_paralell_pipeline.py",
    "content": "import kfp\nimport kfp.dsl as dsl\nfrom kfp.v2.dsl import component\nfrom kfp import compiler\n@component\ndef add(a: float, b: float) -> float:\n  return a + b\n\n@component\ndef mul(a: float, b: float) -> float:\n  return a * b\n\n\n@dsl.pipeline(\n  name='add-mul-pipeline',\n  description='An example pipeline that performs addition calculations.',\n  # pipeline_root='gs://my-pipeline-root/example-pipeline'\n)\ndef my_pipeline(a: float = 1, b: float = 7, c: float = 2.3):\n  add_task_1 = add(a, b)\n  add_task_4 = add(10, 10)\n  mul_task_1 = mul(add_task_1.output, c)\n  add_task_2 = add(add_task_1.output, mul_task_1.output)\n  mul_task_2 = mul(add_task_2.output, mul_task_1.output)\n\n\n\n\ncompiler.Compiler(mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE).compile(pipeline_func=my_pipeline, package_path='pipeline.yaml')\n\nclient = kfp.Client()\n# run the pipeline in v2 compatibility mode\nclient.create_run_from_pipeline_func(\n    my_pipeline,\n    arguments={'a': 7, 'b': 8, 'c': 3.2},\n    mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE,\n)"
  },
  {
    "repo": "javierdarksoul/Kubeflow-pipelines-tutorial",
    "file_path": "tutorial-1/sum_mul_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/javierdarksoul/Kubeflow-pipelines-tutorial/main/tutorial-1/sum_mul_pipeline.py",
    "content": "import kfp\nimport kfp.dsl as dsl\nfrom kfp.v2.dsl import component\nfrom kfp import compiler\n@component\ndef add(a: float, b: float) -> float:\n  return a + b\n\n@component\ndef mul(a: float, b: float) -> float:\n  return a * b\n\n\n@dsl.pipeline(\n  name='add-mul-pipeline',\n  description='An example pipeline that performs addition calculations.',\n  # pipeline_root='gs://my-pipeline-root/example-pipeline'\n)\ndef my_pipeline(a: float = 1, b: float = 7, c: float = 2.3):\n  add_task = add(a, b)\n  mul_task = mul(add_task.output, c)\n\n\n\n\ncompiler.Compiler(mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE).compile(pipeline_func=my_pipeline, package_path='pipeline.yaml')\n\nclient = kfp.Client()\n# run the pipeline in v2 compatibility mode\nclient.create_run_from_pipeline_func(\n    my_pipeline,\n    arguments={'a': 7, 'b': 8, 'c': 3.2},\n    mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE,\n)"
  },
  {
    "repo": "javierdarksoul/Kubeflow-pipelines-tutorial",
    "file_path": "tutorial-1/sum_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/javierdarksoul/Kubeflow-pipelines-tutorial/main/tutorial-1/sum_pipeline.py",
    "content": "import kfp\nimport kfp.dsl as dsl\nfrom kfp.v2.dsl import component\nfrom kfp import compiler\n@component\ndef add(a: float, b: float) -> float:\n  return a + b\n\n@dsl.pipeline(\n  name='addition-pipeline',\n  description='An example pipeline that performs addition calculations.',\n  # pipeline_root='gs://my-pipeline-root/example-pipeline'\n)\ndef add_pipeline(a: float = 1, b: float = 7):\n  add_task = add(a, b)\n\n\ncompiler.Compiler(mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE).compile(pipeline_func=add_pipeline, package_path='pipeline.yaml')\n\nclient = kfp.Client()\n# run the pipeline in v2 compatibility mode\nclient.create_run_from_pipeline_func(\n    add_pipeline,\n    arguments={'a': 7, 'b': 8},\n    mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE,\n)"
  },
  {
    "repo": "javierdarksoul/Kubeflow-pipelines-tutorial",
    "file_path": "tutorial-2/main.py",
    "raw_url": "https://raw.githubusercontent.com/javierdarksoul/Kubeflow-pipelines-tutorial/main/tutorial-2/main.py",
    "content": "from components.dataset import load_dataset\nimport kfp.dsl as dsl\nfrom kfp.v2 import compiler\nimport kfp\n@dsl.pipeline(\n  name='nn-pipeline',\n  description='un ejemplo de una pipeline completa de un modelo',\n)\ndef nnpipeline(datatype: str):\n  dataset= load_dataset(datatype)\n\n\n\ncompiler.Compiler().compile(pipeline_func=nnpipeline, package_path='pipeline.json')\n\nclient = kfp.Client()\nclient.create_run_from_pipeline_func(\n    nnpipeline,\n    arguments={'datatype': \"mnist\"},\n    mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE,\n)"
  },
  {
    "repo": "javierdarksoul/Kubeflow-pipelines-tutorial",
    "file_path": "tutorial-3/main.py",
    "raw_url": "https://raw.githubusercontent.com/javierdarksoul/Kubeflow-pipelines-tutorial/main/tutorial-3/main.py",
    "content": "from components.dataset import load_dataset\nfrom components.build_src import build_src\nfrom components.loads import loads\nimport kfp.dsl as dsl\nfrom kfp.v2 import compiler\nimport kfp\n@dsl.pipeline(\n  name='nn-pipeline',\n  description='un ejemplo de una pipeline completa de un modelo',\n)\ndef nnpipeline(datatype: str):\n  dataset= load_dataset(datatype)\n  src = build_src()\n  load_task= loads(src.outputs[\"source\"],dataset.outputs[\"dataset\"])\n \n  \n\n\n\ncompiler.Compiler().compile(pipeline_func=nnpipeline, package_path='pipeline.json')\n\nclient = kfp.Client()\nclient.create_run_from_pipeline_func(\n    nnpipeline,\n    arguments={'datatype': \"fmnist\"},\n    mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE,\n)"
  },
  {
    "repo": "javierdarksoul/Kubeflow-pipelines-tutorial",
    "file_path": "tutorial-4/main.py",
    "raw_url": "https://raw.githubusercontent.com/javierdarksoul/Kubeflow-pipelines-tutorial/main/tutorial-4/main.py",
    "content": "import kfp.dsl as dsl\nfrom kfp.v2 import compiler\nimport kfp\nfrom secret import access_secret_version\nfrom kfp.v2.dsl import component\nfrom kfp.v2.dsl import (\n    Output,\n    Input,\n    Artifact,\n)\nimport json\n\n#secret = projects/209915815446/secrets/vertex-ai-secret/versions/1\n\ncredentials=access_secret_version(\"209915815446\",\"vertex-ai-secret\", \"1\")\nget_model=kfp.components.load_component_from_file(\"components/get_model_component.yaml\")\nget_data=kfp.components.load_component_from_file(\"components/get_data_component.yaml\")\n\n@component(\n    packages_to_install=['torch','torchvision'],\n    base_image='python:3.8',\n)\ndef loads(dataset: Input[Artifact],source: Input[Artifact]):\n    import pickle\n    import torch\n    import tarfile\n    tarfile.open(name=source.path, mode=\"r\").extractall('.')\n    from src.nn import my_nn\n    with open(dataset.path,'rb') as file:\n        dataloaders = pickle.load(file)\n    train_loader=dataloaders[\"train_loader\"]\n    valid_loader=dataloaders[\"valid_loader\"]\n    print(train_loader)\n    for image,label in train_loader:\n        print(image.shape)\n        break\n\n@dsl.pipeline(\n  name='nn-pipeline',\n  description='un ejemplo de una pipeline completa de un modelo',\n)\ndef nnpipeline():\n  src = get_model(githubpath='https://github.com/javierdarksoul/src_test.git')\n  data = get_data(githubpath=' https://github.com/javierdarksoul/data_test.git',folder =\"FashionMNIST\",credentials=credentials)\n  load_task= loads(data.outputs['trainloader'],src.outputs['output1path'])\n\ncompiler.Compiler().compile(pipeline_func=nnpipeline, package_path='pipeline.json')\n\nclient = kfp.Client()\nclient.create_run_from_pipeline_func(\n    nnpipeline,\n    arguments={},\n    mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE,\n)\n"
  },
  {
    "repo": "javierdarksoul/Kubeflow-pipelines-tutorial",
    "file_path": "tutorial-5/main.py",
    "raw_url": "https://raw.githubusercontent.com/javierdarksoul/Kubeflow-pipelines-tutorial/main/tutorial-5/main.py",
    "content": "import kfp.dsl as dsl\nfrom kfp.v2 import compiler\nimport kfp\n\nfrom components.pythoncomponents.train import train\nfrom components.pythoncomponents.test import test\nget_model=kfp.components.load_component_from_file(\"components/yaml-components/get_model_component.yaml\")\nget_data=kfp.components.load_component_from_file(\"components/yaml-components/get_data_component.yaml\")\n\n\n@dsl.pipeline(\n  name='nn-pipeline',\n  description='un ejemplo de una pipeline completa de un modelo',\n)\ndef nnpipeline():\n  src = get_model(githubpath='https://github.com/javierdarksoul/src_test.git')\n  data = get_data(githubpath=' https://github.com/javierdarksoul/data_test.git',folder =\"FashionMNIST\")\n  train_task= train(src.outputs[\"output1path\"], data.outputs[\"trainloader\"]) \n  test(src.outputs[\"output1path\"],train_task.outputs[\"weights\"], data.outputs[\"testloader\"])\n  #load_task= loads(src.outputs['output1path'])\n\ncompiler.Compiler().compile(pipeline_func=nnpipeline, package_path='pipeline.json')\n\nclient = kfp.Client()\nclient.create_run_from_pipeline_func(\n    nnpipeline,\n    arguments={},\n    mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE,\n)\n"
  },
  {
    "repo": "javierdarksoul/Kubeflow-pipelines-tutorial",
    "file_path": "example/train_nn/main.py",
    "raw_url": "https://raw.githubusercontent.com/javierdarksoul/Kubeflow-pipelines-tutorial/main/example/train_nn/main.py",
    "content": "import kfp.dsl as dsl\nfrom kfp.v2 import compiler\nimport kfp\nimport google.cloud.aiplatform as aip\nimport os\nimport google.auth\nfrom google_cloud_pipeline_components import aiplatform as gcc_aip\nfrom components.pythoncomponents.train import train\nfrom components.pythoncomponents.test import test\nget_model=kfp.components.load_component_from_file(\"components/yaml-components/get_model_component.yaml\")\nget_data=kfp.components.load_component_from_file(\"components/yaml-components/get_data_component.yaml\")\nmar_comp=kfp.components.load_component_from_file(\"components/yaml-components/component_mar.yaml\")\nbuild_torchserve=kfp.components.load_component_from_file(\"components/yaml-components/build_torchserve_component.yaml\")\n\n@dsl.pipeline(\n  name='nn-pipeline',\n  description='un ejemplo de una pipeline completa de un modelo',\n)\ndef nnpipeline():\n  project_id = \"zippedi-project-01\"\n  region= \"us-central1\"\n  src_repo = 'https://github.com/javierdarksoul/src_test.git'\n  data_repo = 'https://github.com/javierdarksoul/data_test.git'\n  dataset = \"FashionMNIST\"\n  model_name= \"example-model\"\n  artifact_folder = \"us-docker.pkg.dev/zippedi-project-01/kubeflow-components/\"\n  tensorboard_route = \"gs://example-vertex-ai/tensorboard_runs/example001/\"\n  src = get_model(githubpath=src_repo)\n  data = get_data(githubpath=data_repo,folder =dataset)\n  train_task= train(src.outputs[\"output1path\"], data.outputs[\"trainloader\"],tensorboard_route,True).add_node_selector_constraint('cloud.google.com/gke-accelerator','nvidia-tesla-t4').set_gpu_limit(1)\n  test_task=test(src.outputs[\"output1path\"],train_task.outputs[\"weights\"], data.outputs[\"testloader\"])\n  test_task.set_caching_options(False)\n\n  with dsl.Condition(test_task.outputs['output']>0.8):\n\n    mar_comp_task=mar_comp(src.outputs[\"output1path\"],model_name,train_task.outputs[\"weights\"] )\n    task=build_torchserve(model_name,mar_comp_task.outputs[\"output_1\"],artifact_folder,project_id)\n    task.set_caching_options(False)\n    model_upload_op = gcc_aip.ModelUploadOp(\n        project=project_id,\n        display_name=model_name,\n        serving_container_image_uri=task.outputs['output_1'],\n        serving_container_predict_route=\"/predictions/{}\".format(model_name),\n        serving_container_health_route=\"/ping\",\n        location=region,\n        serving_container_ports=[{\"containerPort\": 7080}]      \n    )\n    model_upload_op.set_caching_options(False)\n    \n  # gcc_aip.EndpointCreateOp.component_spec.implementation.container.image = \"gcr.io/ml-pipeline/google-cloud-pipeline-components:0.1.7\"\n    endpoint_create_op = gcc_aip.EndpointCreateOp(\n          project=project_id,\n          display_name=model_name+\"-endpoint\",\n          location=region\n      ).set_caching_options(False)\n    \n    #gcc_aip.ModelDeployOp.component_spec.implementation.container.image = \"gcr.io/ml-pipeline/google-cloud-pipeline-components:0.1.7\"\n    model_deploy_op = gcc_aip.ModelDeployOp(\n          #project=project_id,\n          endpoint=endpoint_create_op.outputs[\"endpoint\"],\n          model=model_upload_op.outputs[\"model\"],\n          deployed_model_display_name=model_name,\n          dedicated_resources_machine_type=\"n1-standard-4\",\n          dedicated_resources_min_replica_count=1,\n          #accelerator_type='NVIDIA_TESLA_P100',  # CHANGE THIS as necessary\n          #accelerator_count=1        \n      )\n    \n    \ncompiler.Compiler().compile(pipeline_func=nnpipeline, package_path='pipeline.json')\n\nimport google.auth\n\ncred = google.auth.default() #google.auth.load_credentials_from_file(\"../../../credentials/credentials.json\")\njob = aip.PipelineJob(\n    display_name=\"train_nn_pipeline\",\n    template_path=\"pipeline.json\",\n    pipeline_root=\"gs://example-vertex-ai\",\n    credentials=cred[0],\n    project\t= \"zippedi-project-01\",\n    location = \"us-central1\",\n    parameter_values={\n    }\n)\n#print(job)\njob.submit()\nos.system(\"rm pipeline.json\")\n"
  },
  {
    "repo": "javierdarksoul/Kubeflow-pipelines-tutorial",
    "file_path": "tutorial-6/sum_mul/sum_mul_paralell_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/javierdarksoul/Kubeflow-pipelines-tutorial/main/tutorial-6/sum_mul/sum_mul_paralell_pipeline.py",
    "content": "import kfp\nimport kfp.dsl as dsl\nfrom kfp.v2.dsl import component\nfrom kfp.v2 import compiler\nimport google.cloud.aiplatform as aip\nimport os\n@component\ndef add(a: float, b: float) -> float:\n  return a + b\n\n@component\ndef mul(a: float, b: float) -> float:\n  return a * b\n\n\n@dsl.pipeline(\n  name='add-mul-pipeline',\n  description='An example pipeline that performs addition calculations.',\n  # pipeline_root='gs://my-pipeline-root/example-pipeline'\n)\ndef my_pipeline(a: float = 1, b: float = 7, c: float = 2.3):\n  add_task_1 = add(a, b)\n  mul_task_1 = mul(add_task_1.output, c)\n  add_task_2 = add(add_task_1.output, mul_task_1.output)\n  mul_task_2 = mul(add_task_2.output, mul_task_1.output)\n\n\n\n\ncompiler.Compiler().compile(pipeline_func=my_pipeline, package_path='sum_mul_paralell.json')\n\nimport google.auth\ncred =  google.auth.default()  #google.auth.load_credentials_from_file(\"../../../credentials/credentials.json\")\n\njob = aip.PipelineJob(\n    display_name=\"sum_mul_paralell\",\n    template_path=\"sum_mul_paralell.json\",\n    pipeline_root= \"gs://example-vertex-ai\",\n    credentials=cred[0],\n    project\t= \"zippedi-project-01\",\n    location = \"us-central1\",\n    parameter_values={\n        'a': 1,\n        'b': 2,\n        'c': 3\n    }\n)\njob.submit()\nos.system(\"rm sum_mul_paralell.json\")\n"
  },
  {
    "repo": "javierdarksoul/Kubeflow-pipelines-tutorial",
    "file_path": "tutorial-6/train_nn/main.py",
    "raw_url": "https://raw.githubusercontent.com/javierdarksoul/Kubeflow-pipelines-tutorial/main/tutorial-6/train_nn/main.py",
    "content": "import kfp.dsl as dsl\nfrom kfp.v2 import compiler\nimport kfp\nimport google.cloud.aiplatform as aip\nimport os\nfrom components.pythoncomponents.train import train\nfrom components.pythoncomponents.test import test\nget_model=kfp.components.load_component_from_file(\"components/yaml-components/get_model_component.yaml\")\nget_data=kfp.components.load_component_from_file(\"components/yaml-components/get_data_component.yaml\")\n\n\n@dsl.pipeline(\n  name='nn-pipeline',\n  description='un ejemplo de una pipeline completa de un modelo',\n)\ndef nnpipeline():\n  src = get_model(githubpath='https://github.com/javierdarksoul/src_test.git')\n  data = get_data(githubpath=' https://github.com/javierdarksoul/data_test.git',folder =\"FashionMNIST\")\n  train_task= train(src.outputs[\"output1path\"], data.outputs[\"trainloader\"])#.add_node_selector_constraint('cloud.google.com/gke-accelerator','NVIDIA_TESLA_P100').set_gpu_limit(1))\n  test(src.outputs[\"output1path\"],train_task.outputs[\"weights\"], data.outputs[\"testloader\"])\n \n\ncompiler.Compiler().compile(pipeline_func=nnpipeline, package_path='pipeline.json')\n\nimport google.auth\ncred = google.auth.default() #google.auth.load_credentials_from_file(\"../../../credentials/credentials.json\")\njob = aip.PipelineJob(\n    display_name=\"train_nn_pipeline\",\n    template_path=\"pipeline.json\",\n    pipeline_root=\"gs://example-vertex-ai\",\n    credentials=cred[0],\n    project\t= \"zippedi-project-01\",\n    location = \"us-central1\",\n    parameter_values={\n    }\n)\n#print(job)\njob.submit()\nos.system(\"rm pipeline.json\")\n"
  },
  {
    "repo": "THEANMOZHI/kubeflow-iris-pipeline",
    "file_path": "iris.py",
    "raw_url": "https://raw.githubusercontent.com/THEANMOZHI/kubeflow-iris-pipeline/main/iris.py",
    "content": "# kfp\r\nimport kfp\r\nfrom kfp import dsl\r\nfrom kfp.v2 import compiler\r\nfrom kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\r\n                        OutputPath, ClassificationMetrics, Metrics, component)\r\nfrom kfp.v2.google.client import AIPlatformClient\r\n\r\n# gcp\r\nfrom google.cloud import aiplatform\r\nfrom google_cloud_pipeline_components import aiplatform as gcc_aip\r\nfrom google.cloud import storage\r\nfrom googleapiclient import discovery\r\nfrom oauth2client.client import GoogleCredentials\r\n\r\n# i/o\r\nfrom typing import NamedTuple\r\nfrom io import StringIO\r\nimport os\r\n\r\n# pandas & sklearn\r\nimport pandas as pd\r\nimport sklearn\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.svm import SVC\r\nfrom sklearn.metrics import accuracy_score\r\n\r\n\r\n# definites\r\nPROJECT_ID=\"kedro-kubeflow-334417\"\r\nBUCKET_NAME=\"gs://diab-gsbucket\"\r\nREGION=\"us-central1\"\r\nPIPELINE_NAME = \"diabetes_pipeline\"\r\nPIPELINE_ROOT = f\"{BUCKET_NAME}/pipeline_root/\"\r\nPIPELINE_ROOT\r\n\r\n\r\n# data component\r\n@component(base_image='python:3.7',\r\n           packages_to_install=['numpy', 'pandas', 'google-cloud-storage==1.43.0', 'google-cloud-aiplatform==1.0.0',\r\n                         'kubeflow-metadata', 'scikit-learn', 'gcsfs==2021.11.1'])\r\ndef data_component(bucket: str, value: float, marker: int) -> int:\r\n    import kfp\r\n    import pandas as pd\r\n    import sklearn\r\n    from sklearn.model_selection import train_test_split\r\n    from kfp.v2.google.client import AIPlatformClient\r\n    from google.cloud import storage\r\n    \r\n    # read data from gcs bucket\r\n    data = pd.read_csv('gs://iris-kfp/iris.csv', index_col=False) \r\n    \r\n    # data preprocessing\r\n    # normalizing data\r\n    df = data\r\n    for column in df.columns:\r\n        df[column] = (df[column] - df[column].mean()) / df[column].std() \r\n    data = df\r\n    # dependent and independent data sets\r\n    train_data = data.drop('class',axis=1)\r\n    test_data = data['class']    \r\n    \r\n    # test-train  data split\r\n    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(train_data, test_data, test_size = value, random_state=42)\r\n    X_train = X_train.to_csv()\r\n    X_test = X_test.to_csv()\r\n    y_train = y_train.to_csv()\r\n    y_test = y_test.to_csv()\r\n    \r\n    # storage client\r\n    storage_client = storage.Client()\r\n    bucket = storage_client.get_bucket('iris-kfp')\r\n    # blobs\r\n    d1 = bucket.blob('X_train.csv')\r\n    d2 = bucket.blob('X_test.csv')\r\n    d3 = bucket.blob('y_train.csv')\r\n    d4 = bucket.blob('y_test.csv')\r\n    \r\n    # uploading train-test datasets into gcs bucket\r\n    d1.upload_from_string(f'{X_train}.csv', 'text/csv')\r\n    d2.upload_from_string(f'{X_test}.csv', 'text/csv')\r\n    d3.upload_from_string(f'{y_train}.csv', 'text/csv')\r\n    d4.upload_from_string(f'{y_test}.csv', 'text/csv')\r\n    \r\n    # setting flag\r\n    df1 = pd.read_csv(\"gs://iris-kfp/X_train.csv\", index_col=0)\r\n    df2 = pd.read_csv(\"gs://iris-kfp/X_test.csv\", index_col=0)\r\n    df3 = pd.read_csv(\"gs://iris-kfp/y_train.csv\", index_col=0)\r\n    df4 = pd.read_csv(\"gs://iris-kfp/y_test.csv\", index_col=0)\r\n    \r\n    if(df1.empty == True and df2.empty == True and df3.empty == True and df4.empty == True):\r\n        marker = 1\r\n    else:\r\n        marker = 0\r\n\r\n    return marker\r\n\r\n\r\n\r\n# model component\r\n@component(base_image='python:3.7',\r\n        packages_to_install=['numpy', 'pandas', 'google-cloud-storage==1.43.0', 'google-cloud-aiplatform==1.0.0',\r\n                            'kubeflow-metadata', 'scikit-learn', 'gcsfs==2021.11.1'])\r\ndef model_component(bucket:str, xtrain:str, ytrain:str, xtest:str, ytest:str) -> float:\r\n    import pandas as pd    \r\n    from sklearn.ensemble import RandomForestClassifier\r\n    from sklearn.metrics import accuracy_score\r\n    \r\n    # read test-train data sets from GCS bucket\r\n    X_train = pd.read_csv(f'{bucket}/{xtrain}.csv', sep=\",\")\r\n    y_train = pd.read_csv(f'{bucket}/{ytrain}.csv', sep=\",\")\r\n    X_test = pd.read_csv(f'{bucket}/{xtest}.csv', sep=\",\")\r\n    y_test = pd.read_csv(f'{bucket}/{ytest}.csv', sep=\",\")    \r\n        \r\n    # train model\r\n    model = RandomForestClassifier(max_depth=2, random_state=3)\r\n    model.fit(X_train, y_train)\r\n    predictions = model.predict(X_test)\r\n\r\n    # find accuracy\r\n    accuracy = accuracy_score(y_test, predictions)\r\n    \r\n    return accuracy\r\n\r\n\r\n# when accuracy >= threshold\r\n@component(base_image='python:3.7',\r\n           packages_to_install=['numpy', 'pandas', 'google-cloud-storage==1.43.0', 'google-cloud-aiplatform==1.0.0',\r\n                         'kubeflow-metadata', 'scikit-learn'])\r\ndef true_component(accuracy:float) -> None:\r\n    print(f'Yes!! {accuracy} is the Accuracy and its greater than the threshold')\r\n\r\n\r\n# when accuracy < thrershold\r\n@component(base_image='python:3.7',\r\n           packages_to_install=['numpy', 'pandas', 'google-cloud-storage==1.43.0', 'google-cloud-aiplatform==1.0.0',\r\n                         'kubeflow-metadata', 'scikit-learn'])\r\ndef false_component(accuracy:float) -> None:\r\n    print(f'No. {accuracy} is the Accuracy and its smaller than the threshold')\r\n\r\n\r\n# pipeline\r\n@kfp.dsl.pipeline(name = \"iris-pipeline\",\r\n                  pipeline_root = PIPELINE_ROOT)\r\ndef iris_pipeline(\r\n    display_name: str=\"iris-kfp\",\r\n    project: str = PROJECT_ID,\r\n    gcp_region: str = \"us-central1\",\r\n    api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",\r\n    marker: int = 0,\r\n    test_train_split_ratio: float = 0.3,\r\n    accuracy_threshold: float = 0.5,\r\n    bucket: str = \"gs://iris-kfp\"\r\n) -> None:\r\n        \r\n    # initiating data component\r\n    data_op = data_component(bucket, test_train_split_ratio, marker)\r\n\r\n    # initiating model component\r\n    with dsl.Condition(data_op.output == 1):\r\n        model_op = model_component(bucket, \"X_train\", \"y_train\", \"X_test\", \"y_test\")\r\n    \r\n        with dsl.Condition(model_op.output >= accuracy_threshold, name=\"accuracy>=50\"):\r\n            true_component(model_op.output)\r\n        with dsl.Condition(model_op.output < accuracy_threshold, name=\"accuracy<50\"):\r\n            false_component(model_op.output)\r\n\r\n\r\ncompiler.Compiler().compile(\r\n    pipeline_func=iris_pipeline, package_path=\"iris_kfp.json\"\r\n)\r\n\r\napi_client = AIPlatformClient(\r\n    project_id=PROJECT_ID,\r\n    region=REGION,\r\n)\r\n\r\nresponse = api_client.create_run_from_job_spec(\r\n    \"iris_kfp.json\", pipeline_root=PIPELINE_ROOT,\r\n)"
  },
  {
    "repo": "agapebondservant/kubeflow-pipelines-accelerator",
    "file_path": "app/main.py",
    "raw_url": "https://raw.githubusercontent.com/agapebondservant/kubeflow-pipelines-accelerator/main/app/main.py",
    "content": "import kfp.dsl as dsl\nfrom kfp import Client\nimport logging\nimport warnings\nimport utils\nimport datetime\n\nlogging.basicConfig()\nlogging.getLogger().setLevel(logging.INFO)\nwarnings.filterwarnings('ignore')\n\n\n@dsl.pipeline(\n    name='cifar_cnn_kfp',\n    description='Pipeline which trains and serves a CNN model using Keras.'\n)\ndef cifar_pipeline():\n\n    # Upload Dataset\n    upload_dataset = dsl.ContainerOp(\n        name='upload_dataset',\n        image='oawofolu/ml-image-processor:latest',\n        command=\"python\",\n        arguments=[\n            \"/app/main.py\",\n            \"mlflow_entry=upload_dataset\",\n            f\"mlflow_stage={utils.get_env_var('MLFLOW_STAGE')}\",\n            f\"git_repo={utils.get_env_var('GIT_REPO')}\",\n            f\"experiment_name={utils.get_env_var('EXPERIMENT_NAME')}\",\n            f\"environment_name={utils.get_env_var('ENVIRONMENT_NAME')}\",\n            f\"default_token={datetime.datetime.now()}\"\n        ]\n    )\n\n    # Train Model\n    train_model = dsl.ContainerOp(\n        name='train_model',\n        image='oawofolu/ml-image-processor:latest',\n        command=\"python\",\n        arguments=[\n            \"/app/main.py\",\n            \"mlflow_entry=train_model\",\n            f\"mlflow_stage={utils.get_env_var('MLFLOW_STAGE')}\",\n            f\"git_repo={utils.get_env_var('GIT_REPO')}\",\n            f\"experiment_name={utils.get_env_var('EXPERIMENT_NAME')}\",\n            f\"environment_name={utils.get_env_var('ENVIRONMENT_NAME')}\",\n            f\"default_token={datetime.datetime.now()}\"\n        ]\n    )\n    train_model.after(upload_dataset)\n\n    # Evaluate Model\n    evaluate_model = dsl.ContainerOp(\n        name='evaluate_model',\n        image='oawofolu/ml-image-processor:latest',\n        command=\"python\",\n        arguments=[\n            \"/app/main.py\",\n            \"mlflow_entry=evaluate_model\",\n            f\"mlflow_stage={utils.get_env_var('MLFLOW_STAGE')}\",\n            f\"git_repo={utils.get_env_var('GIT_REPO')}\",\n            f\"experiment_name={utils.get_env_var('EXPERIMENT_NAME')}\",\n            f\"environment_name={utils.get_env_var('ENVIRONMENT_NAME')}\",\n            f\"default_token={datetime.datetime.now()}\"\n        ]\n    )\n    evaluate_model.after(train_model)\n\n    # Promote Model to Staging\n    promote_model_to_staging = dsl.ContainerOp(\n        name='promote_model_to_staging',\n        image='oawofolu/ml-image-processor:latest',\n        command=\"python\",\n        arguments=[\n            \"/app/main.py\",\n            \"mlflow_entry=promote_model_to_staging\",\n            f\"mlflow_stage={utils.get_env_var('MLFLOW_STAGE')}\",\n            f\"git_repo={utils.get_env_var('GIT_REPO')}\",\n            f\"experiment_name={utils.get_env_var('EXPERIMENT_NAME')}\",\n            f\"environment_name={utils.get_env_var('ENVIRONMENT_NAME')}\",\n            f\"default_token={datetime.datetime.now()}\"\n        ]\n    )\n    promote_model_to_staging.after(evaluate_model)\n\n\nif __name__ == '__main__':\n    logging.info(f\"Compiling Kubeflow pipeline...host={utils.get_env_var('KUBEFLOW_PIPELINES_HOST')}\")\n    import kfp.compiler as compiler\n    compiler.Compiler().compile(cifar_pipeline, __file__ + '.zip')\n\n    logging.info(\"Generating new experiment run...\")\n    client = Client(host=f'{utils.get_env_var(\"KUBEFLOW_PIPELINES_HOST\")}')\n    cifar_experiment = client.create_experiment(name=utils.get_env_var('EXPERIMENT_NAME'))\n    cifar_run = client.run_pipeline(cifar_experiment.id, 'cifar-pipeline', __file__ + '.zip')\n    logging.info(\"Run completed.\")\n"
  },
  {
    "repo": "himanshusharma89/kubeflow-ml-pipeline",
    "file_path": "pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/himanshusharma89/kubeflow-ml-pipeline/main/pipeline.py",
    "content": "# pipeline.py\nfrom kfp import dsl, compiler\nfrom components.data_acquisition import acquire_dataset\nfrom components.feature_preparation import prepare_features\nfrom components.model_development import develop_model\nfrom components.performance_assessment import assess_performance\n\n@dsl.pipeline(name=\"iris-classification-pipeline\")\ndef classification_pipeline():\n    \"\"\"Orchestrate the end-to-end classification pipeline.\"\"\"\n    # Data acquisition\n    data_op = acquire_dataset()\n    \n    # Feature preparation\n    prep_op = prepare_features(raw_dataset=data_op.outputs[\"dataset_output\"])\n    \n    # Model development\n    model_op = develop_model(\n        training_features=prep_op.outputs[\"training_features\"],\n        training_labels=prep_op.outputs[\"training_labels\"]\n    )\n    \n    # Performance assessment - Fixed the output reference\n    assess_op = assess_performance(\n        testing_features=prep_op.outputs[\"testing_features\"],\n        testing_labels=prep_op.outputs[\"testing_labels\"],  # This was the issue\n        trained_model=model_op.outputs[\"model_artifact\"]\n    )\n\nif __name__ == \"__main__\":\n    compiler.Compiler().compile(\n        pipeline_func=classification_pipeline,\n        package_path=\"iris_pipeline.yaml\"\n    )"
  },
  {
    "repo": "himanshusharma89/kubeflow-ml-pipeline",
    "file_path": "components/data_acquisition.py",
    "raw_url": "https://raw.githubusercontent.com/himanshusharma89/kubeflow-ml-pipeline/main/components/data_acquisition.py",
    "content": "from kfp import dsl\nfrom kfp.dsl import Output, Dataset, component\n\n@dsl.component(base_image=\"python:3.9\")\ndef acquire_dataset(dataset_output: Output[Dataset]):\n    \"\"\"Acquire and prepare the initial dataset.\"\"\"\n    import subprocess\n    subprocess.run([\"pip\", \"install\", \"pandas\", \"scikit-learn\"], check=True)\n    \n    from sklearn.datasets import load_iris\n    import pandas as pd\n    \n    raw_data = load_iris()\n    dataset = pd.DataFrame(\n        raw_data.data,\n        columns=[name.replace(' ', '_').lower() for name in raw_data.feature_names]\n    )\n    dataset['species_class'] = raw_data.target\n    \n    dataset.to_csv(dataset_output.path, index=False)\n"
  },
  {
    "repo": "himanshusharma89/kubeflow-ml-pipeline",
    "file_path": "components/feature_preparation.py",
    "raw_url": "https://raw.githubusercontent.com/himanshusharma89/kubeflow-ml-pipeline/main/components/feature_preparation.py",
    "content": "from kfp import dsl\nfrom kfp.dsl import Input, Output, Dataset, component\n\n@dsl.component(base_image=\"python:3.9\")\ndef prepare_features(\n    raw_dataset: Input[Dataset],\n    training_features: Output[Dataset],\n    testing_features: Output[Dataset],\n    training_labels: Output[Dataset],\n    testing_labels: Output[Dataset]\n):\n    \"\"\"Transform and split the dataset for modeling.\"\"\"\n    import subprocess\n    subprocess.run([\"pip\", \"install\", \"pandas\", \"scikit-learn\"], check=True)\n    \n    import pandas as pd\n    import numpy as np\n    from sklearn.preprocessing import RobustScaler\n    from sklearn.model_selection import train_test_split\n    \n    dataset = pd.read_csv(raw_dataset.path)\n    assert dataset.notna().all().all(), \"Dataset contains missing values\"\n    \n    features = dataset.drop(columns=['species_class'])\n    target = dataset['species_class']\n    \n    feature_transformer = RobustScaler()\n    normalized_features = feature_transformer.fit_transform(features)\n    \n    X_train, X_test, y_train, y_test = train_test_split(\n        normalized_features, \n        target,\n        test_size=0.25,\n        random_state=42,\n        stratify=target\n    )\n    \n    train_df = pd.DataFrame(X_train, columns=features.columns)\n    test_df = pd.DataFrame(X_test, columns=features.columns)\n    train_labels_df = pd.DataFrame(y_train, columns=['species_class'])\n    test_labels_df = pd.DataFrame(y_test, columns=['species_class'])\n    \n    train_df.to_csv(training_features.path, index=False)\n    test_df.to_csv(testing_features.path, index=False)\n    train_labels_df.to_csv(training_labels.path, index=False)\n    test_labels_df.to_csv(testing_labels.path, index=False)\n"
  },
  {
    "repo": "himanshusharma89/kubeflow-ml-pipeline",
    "file_path": "components/model_development.py",
    "raw_url": "https://raw.githubusercontent.com/himanshusharma89/kubeflow-ml-pipeline/main/components/model_development.py",
    "content": "from kfp import dsl\nfrom kfp.dsl import Input, Output, Dataset, Model, component\n\n@dsl.component(base_image=\"python:3.9\")\ndef develop_model(\n    training_features: Input[Dataset],\n    training_labels: Input[Dataset],\n    model_artifact: Output[Model]\n):\n    \"\"\"Build and train the classification model.\"\"\"\n    import subprocess\n    subprocess.run([\"pip\", \"install\", \"pandas\", \"scikit-learn\", \"joblib\"], check=True)\n    \n    import pandas as pd\n    from sklearn.linear_model import LogisticRegression\n    from joblib import dump\n    \n    X = pd.read_csv(training_features.path)\n    y = pd.read_csv(training_labels.path)['species_class']\n    \n    classifier = LogisticRegression(\n        class_weight='balanced',\n        max_iter=1000,\n        random_state=42,\n        multi_class='multinomial'\n    )\n    classifier.fit(X, y)\n    \n    dump(classifier, model_artifact.path)"
  },
  {
    "repo": "himanshusharma89/kubeflow-ml-pipeline",
    "file_path": "components/performance_assessment.py",
    "raw_url": "https://raw.githubusercontent.com/himanshusharma89/kubeflow-ml-pipeline/main/components/performance_assessment.py",
    "content": "from kfp import dsl\nfrom kfp.dsl import Input, Output, Dataset, Model, component\n\n@dsl.component(base_image=\"python:3.9\")\ndef assess_performance(\n    testing_features: Input[Dataset],\n    testing_labels: Input[Dataset],\n    trained_model: Input[Model],\n    performance_metrics: Output[Dataset]\n):\n    \"\"\"Evaluate model performance and generate visualization.\"\"\"\n    import subprocess\n    subprocess.run([\"pip\", \"install\", \"pandas\", \"scikit-learn\", \"seaborn\", \"joblib\"], check=True)\n    \n    import pandas as pd\n    import seaborn as sns\n    import matplotlib.pyplot as plt\n    from sklearn.metrics import classification_report, confusion_matrix\n    from joblib import load\n    \n    X_test = pd.read_csv(testing_features.path)\n    y_true = pd.read_csv(testing_labels.path)['species_class']\n    classifier = load(trained_model.path)\n    \n    y_pred = classifier.predict(X_test)\n    \n    metrics = classification_report(y_true, y_pred, output_dict=True)\n    conf_matrix = confusion_matrix(y_true, y_pred)\n    \n    plt.figure(figsize=(10, 8))\n    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='YlOrRd')\n    plt.title('Confusion Matrix Heatmap')\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Actual Class')\n    \n    results = {\n        'metrics': metrics,\n        'confusion_matrix': conf_matrix.tolist()\n    }\n    pd.DataFrame([results]).to_json(performance_metrics.path)\n"
  },
  {
    "repo": "Louis5499/DRAGON-Kubeflow",
    "file_path": "tf_job_dragon_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/Louis5499/DRAGON-Kubeflow/master/tf_job_dragon_pipeline.py",
    "content": "import json\nfrom kfp import components\nimport kfp.dsl as dsl\n\n@dsl.pipeline(\n    name=\"Launch kubeflow tfjob & kfserving template\",\n    description=\"An example to launch tfjob.\"\n)\ndef mnist_pipeline(\n        name=\"mnist\",\n        namespace=\"kubeflow\",\n        workerNum=2,\n        deleteAfterDone=False):\n    tfjob_launcher_op = components.load_component_from_file(\"./tfJobComponent.yaml\")\n    bucket = \"kf_second_test\"\n    \n    chief = {}\n    worker = {}\n    if workerNum > 0:\n      worker = {\n        \"replicas\": 1,\n        \"restartPolicy\": \"OnFailure\",\n        \"template\": {\n          \"spec\": {\n            \"terminationGracePeriodSeconds\": 0,\n            \"containers\": [\n              {\n                \"args\": [\n                  \"curl -s http://140.114.78.229/web/mnist-new.py | python3 -\"\n                ],\n                \"env\": [\n                  {\n                    \"name\": \"global_steps\",\n                    \"value\": \"100000\"\n                  }\n                ],\n                \"command\": [\n                  \"/bin/bash\",\n                  \"-c\"\n                ],\n                \"image\": \"ncy9371/tensorflow:1.15.2-py3-noavx\",\n                \"name\": \"tensorflow\",\n                \"ports\": [\n                  {\n                    \"containerPort\": 2222,\n                    \"name\": \"tfjob-port\"\n                  }\n                ],\n                \"resources\": {\n                  \"requests\": {\n                    \"cpu\": \"1\",\n                    \"memory\": \"2Gi\"\n                  }\n                }\n              }\n            ]\n          }\n        }\n      }\n\n    ps = {\n        \"replicas\": 1,\n        \"restartPolicy\": \"OnFailure\",\n        \"template\": {\n            \"spec\": {\n            \"terminationGracePeriodSeconds\": 0,\n            \"containers\": [\n                {\n                \"args\": [\n                    \"curl -s http://140.114.78.229/web/mnist-new.py | python3 -\"\n                ],\n                \"env\": [\n                    {\n                    \"name\": \"global_steps\",\n                    \"value\": \"100000\"\n                    }\n                ],\n                \"command\": [\n                    \"/bin/bash\",\n                    \"-c\"\n                ],\n                \"image\": \"ncy9371/tensorflow:1.15.2-py3-noavx\",\n                \"name\": \"tensorflow\",\n                \"ports\": [\n                    {\n                    \"containerPort\": 2222,\n                    \"name\": \"tfjob-port\"\n                    }\n                ]\n                }\n            ]\n            }\n        }\n    }\n    tfJobLauncher = tfjob_launcher_op(\n      name=name,\n      namespace=namespace,\n      worker_spec=worker,\n      chief_spec=chief,\n      ps_spec=ps,\n      delete_finished_tfjob=deleteAfterDone\n    )\n\nif __name__ == \"__main__\":\n    import kfp.compiler as compiler\n    compiler.Compiler().compile(mnist_pipeline, __file__ + \".tar.gz\")\n"
  },
  {
    "repo": "dhodun/kubeflow-pipeline-examples",
    "file_path": "mask_rcnn/mask_rcnn.py",
    "raw_url": "https://raw.githubusercontent.com/dhodun/kubeflow-pipeline-examples/master/mask_rcnn/mask_rcnn.py",
    "content": "#!/usr/bin/env python3\n# Copyright 2018 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport kfp.dsl as dsl\nfrom kfp.gcp import use_tpu\n\ndef export_op_fn(name, arguments):\n  op = dsl.ContainerOp(\n    name=name,\n    image='gcr.io/dhodun1/export-mask-rcnn-saved:latest',\n    arguments=arguments,\n    file_outputs={'export_dir': '/output.txt'}\n  )\n  return op\n\n\n@dsl.pipeline(\n  name='mask_rcnn',\n  description='Preprocess COCO and train Mask RCNN Model'\n)\ndef train_and_deploy(\n        project='dhodun1',\n        bucket='gs://maskrcnn-kfp',\n        #TODO: non-camel-case was conflicting with the use_tpu op modifier\n):\n  usetpu = True\n  istest = True\n\n  \"\"\"Pipeline to train Mask RCNN\"\"\"\n  start_step = 1\n\n  if start_step <= 1:\n    preprocess_coco = dsl.ContainerOp(\n      name='preprocess_coco',\n      # image needs to be compile-time string\n      image='gcr.io/dhodun1/preprocess-coco:latest',\n      arguments=[bucket],\n      file_outputs={'coco_dir': '/output.txt'}\n    )\n    preprocess_coco.container.set_cpu_request('8')\n    preprocess_coco.container.set_memory_request('30G')\n\n  if start_step <=2:\n    train_mask_rcnn = dsl.ContainerOp(\n      name='train_mask_rcnn_tpu',\n      # image needs to be a compile-time string\n      image='gcr.io/dhodun1/train-mask-rcnn:latest',\n      arguments=[bucket,\n                 preprocess_coco.outputs['coco_dir'],\n                 str(usetpu),\n                 str(istest)],\n      file_outputs={'model_dir': '/model_dir.txt',\n                    'mAP_box': '/map_box.txt',\n                    'mAP_segm': '/map_segm.txt'}\n    )\n    train_mask_rcnn.after(preprocess_coco)\n    train_mask_rcnn.container.set_cpu_request('8')\n    train_mask_rcnn.container.set_memory_request('30G')\n    #train_mask_rcnn_tpu.container.set_pull_image_policy('Always')\n    if usetpu:\n      train_mask_rcnn.apply(use_tpu(tpu_cores=8, tpu_resource='v3', tf_version='1.12'))\n      # note needed now that i've consolidated TPU\n      #train_mask_rcnn.container.image='gcr.io/dhodun1/train-mask-rcnn-tpu:latest'\n\n\n  if start_step <=3:\n    export_model_jpeg = export_op_fn(name='export_model_jpeg',\n                                     arguments=['jpeg',\n                                                train_mask_rcnn.outputs['model_dir'],\n                                                train_mask_rcnn.outputs['model_dir']]\n    )\n    export_model_jpeg.after(train_mask_rcnn)\n\n    export_model_tensor = export_op_fn(name='export_model_tensor',\n                                     arguments=['tensor',\n                                                train_mask_rcnn.outputs['model_dir'],\n                                                train_mask_rcnn.outputs['model_dir']]\n                                     )\n    export_model_tensor.after(train_mask_rcnn)\n\n\n\n\n\n\n\"\"\"\n  if start_step <= 2:\n    download_and_preprocess = dsl.ContainerOp(\n      name='download_and_preprocess',\n      # image needs to be a compile-time string\n      image='gcr.io/dhodun1/tpu-test',\n      arguments=[\n        '--project', project,\n        '--mode', 'cloud',\n        '--bucket', bucket,\n        '--start_year', startYear\n      ],\n      file_outputs={'result': '/output.txt'}\n    )\n  else:\n    preprocess = ObjectDict({\n      'outputs': {\n        'bucket': bucket\n      }\n    })\n  download_and_preprocess.apply(use_tpu(tpu_cores=8, tpu_resource='v2', tf_version='1.11'))\n  download_and_preprocess.set_gpu_limit(8, vendor='nvidia')\n\n\n  # Step 1: Download COCO dataset and transform to TFRecords\n  if start_step <= 1:\n    download_and_preprocess = dsl.ContainerOp(\n      name='download_and_preprocess',\n      # image needs to be a compile-time string\n      image='gcr.io/tensorflow/tpu-models:r1.11',\n      command = '''          - /bin/bash\n          - -c\n          - >\n            cd /tensorflow_tpu_models/tools/datasets &&\n            bash download_and_preprocess_coco.sh /scratch-dir &&\n            gsutil -m cp /scratch-dir/*.tfrecord ${DATA_BUCKET}/coco &&\n            gsutil cp /scratch-dir/raw-data/annotations/*.json ${DATA_BUCKET}/coco''',\n      arguments=[\n        '--project', project,\n        '--mode', 'cloud',\n        '--bucket', bucket,\n        '--start_year', startYear\n      ],\n      file_outputs={'bucket': '/output.txt'}\n    )\n    #download_and_preprocess.set_memory_request('2G')\n    download_and_preprocess.add_env_variable([])\n  else:\n    preprocess = ObjectDict({\n      'outputs': {\n        'bucket': bucket\n      }\n    })\n  # Step 2: Do hyperparameter tuning of the model on Cloud ML Engine\n  if start_step <= 2:\n    hparam_train = dsl.ContainerOp(\n      name='hypertrain',\n      # image needs to be a compile-time string\n      image='gcr.io/cloud-training-demos/babyweight-pipeline-hypertrain:latest',\n      arguments=[\n        preprocess.outputs['bucket']\n      ],\n      file_outputs={'jobname': '/output.txt'}\n    )\n  else:\n    hparam_train = ObjectDict({\n      'outputs': {\n        'jobname': 'babyweight_181008_210829'\n      }\n    })\n\n  # Step 3: Train the model some more, but on the pipelines cluster itself\n  if start_step <= 3:\n    train_tuned = dsl.ContainerOp(\n      name='traintuned',\n      # image needs to be a compile-time string\n      image='gcr.io/cloud-training-demos/babyweight-pipeline-traintuned-trainer:latest',\n      #image='gcr.io/cloud-training-demos/babyweight-pipeline-traintuned-trainer@sha256:3d73c805430a16d0675aeafa9819d6d2cfbad0f0f34cff5fb9ed4e24493bc9a8',\n      arguments=[\n        hparam_train.outputs['jobname'],\n        bucket\n      ],\n      file_outputs={'train': '/output.txt'}\n    )\n    train_tuned.set_memory_request('2G')\n    train_tuned.set_cpu_request('1')\n  else:\n    train_tuned = ObjectDict({\n        'outputs': {\n          'train': 'gs://cloud-training-demos-ml/babyweight/hyperparam/15'\n        }\n    })\n\n  # Step 4: Deploy the trained model to Cloud ML Engine\n  if start_step <= 4:\n    deploy_cmle = dsl.ContainerOp(\n      name='deploycmle',\n      # image needs to be a compile-time string\n      image='gcr.io/cloud-training-demos/babyweight-pipeline-deploycmle:latest',\n      arguments=[\n        train_tuned.outputs['train'],  # modeldir\n        'babyweight',\n        'mlp'\n      ],\n      file_outputs={\n        'model': '/model.txt',\n        'version': '/version.txt'\n      }\n    )\n  else:\n    deploy_cmle = ObjectDict({\n      'outputs': {\n        'model': 'babyweight',\n        'version': 'mlp'\n      }\n    })\n\n  # Step 5: Deploy the trained model to AppEngine\n  if start_step <= 5:\n    deploy_cmle = dsl.ContainerOp(\n      name='deployapp',\n      # image needs to be a compile-time string\n      image='gcr.io/cloud-training-demos/babyweight-pipeline-deployapp:latest',\n      arguments=[\n        deploy_cmle.outputs['model'],\n        deploy_cmle.outputs['version']\n      ],\n      file_outputs={\n        'appurl': '/appurl.txt'\n      }\n    )\n  else:\n    deploy_cmle = ObjectDict({\n      'outputs': {\n        'appurl': 'https://cloud-training-demos.appspot.com/'\n      }\n    })\n\"\"\"\n\nif __name__ == '__main__':\n  import kfp.compiler as compiler\n\n  compiler.Compiler().compile(train_and_deploy, __file__ + '.tgz')\n"
  },
  {
    "repo": "dhodun/kubeflow-pipeline-examples",
    "file_path": "mask_rcnn/mask_trt_example.py",
    "raw_url": "https://raw.githubusercontent.com/dhodun/kubeflow-pipeline-examples/master/mask_rcnn/mask_trt_example.py",
    "content": "#!/usr/bin/env python3\n# Copyright 2018 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport kfp.dsl as dsl\n\nclass ObjectDict(dict):\n  def __getattr__(self, name):\n    if name in self:\n      return self[name]\n    else:\n      raise AttributeError(\"No such attribute: \" + name)\n\nfrom kfp.gcp import use_tpu\n\n@dsl.pipeline(\n  name='mask_rcnn_trt_full',\n  description='Preprocess COCO and train Mask RCNN Model'\n)\ndef train_and_deploy(\n    project=dsl.PipelineParam(name='project', value='dhodun1'),\n    bucket=dsl.PipelineParam(name='bucket', value='gs://dhodun1-central1'),\n    startYear=dsl.PipelineParam(name='startYear', value='2000')\n):\n  \"\"\"Pipeline to train Mask RCNN\"\"\"\n\n  reprocess_coco = dsl.ContainerOp(\n    name='preprocess_coco',\n    # image needs to be compile-time string\n    image='gcr.io/dhodun1/preprocess-coco:latest',\n    arguments=[\n      bucket,\n    ],\n    file_outputs={'bucket': '/output.txt'}\n  )\n\n  if start_step <= 1:\n    preprocess_coco = dsl.ContainerOp(\n      name='preprocess_coco',\n      # image needs to be compile-time string\n      image='gcr.io/dhodun1/preprocess-coco:latest',\n      arguments=[\n        bucket,\n      ],\n      file_outputs={'bucket': '/output.txt'}\n    )\n    preprocess_coco.set_cpu_request('8')\n    preprocess_coco.set_memory_request('30G')\n  else:\n    preprocess_coco = ObjectDict({\n      'outputs': {\n        'bucket': bucket\n      }\n    })\n\n  if start_step <=2:\n    train_mask_rcnn = dsl.ContainerOp(\n      name='train_mask_rcnn_tpu',\n      # image needs to be a compile-time string\n      image='gcr.io/dhodun1/train-mask-rcnn',\n      arguments=[\n        bucket,\n      ],\n      #file_outputs={'results': '/output.txt'}\n    )\n    train_mask_rcnn.apply(use_tpu(tpu_cores=8, tpu_resource='v3', tf_version='1.12'))\n    train_mask_rcnn.set_cpu_request('8')\n    train_mask_rcnn.set_memory_request('30G')\n\"\"\"\n  if start_step <= 2:\n    download_and_preprocess = dsl.ContainerOp(\n      name='download_and_preprocess',\n      # image needs to be a compile-time string\n      image='gcr.io/dhodun1/tpu-test',\n      arguments=[\n        '--project', project,\n        '--mode', 'cloud',\n        '--bucket', bucket,\n        '--start_year', startYear\n      ],\n      file_outputs={'result': '/output.txt'}\n    )\n  else:\n    preprocess = ObjectDict({\n      'outputs': {\n        'bucket': bucket\n      }\n    })\n  download_and_preprocess.apply(use_tpu(tpu_cores=8, tpu_resource='v2', tf_version='1.11'))\n  download_and_preprocess.set_gpu_limit(8, vendor='nvidia')\n\n\n  # Step 1: Download COCO dataset and transform to TFRecords\n  if start_step <= 1:\n    download_and_preprocess = dsl.ContainerOp(\n      name='download_and_preprocess',\n      # image needs to be a compile-time string\n      image='gcr.io/tensorflow/tpu-models:r1.11',\n      command = '''          - /bin/bash\n          - -c\n          - >\n            cd /tensorflow_tpu_models/tools/datasets &&\n            bash download_and_preprocess_coco.sh /scratch-dir &&\n            gsutil -m cp /scratch-dir/*.tfrecord ${DATA_BUCKET}/coco &&\n            gsutil cp /scratch-dir/raw-data/annotations/*.json ${DATA_BUCKET}/coco''',\n      arguments=[\n        '--project', project,\n        '--mode', 'cloud',\n        '--bucket', bucket,\n        '--start_year', startYear\n      ],\n      file_outputs={'bucket': '/output.txt'}\n    )\n    #download_and_preprocess.set_memory_request('2G')\n    download_and_preprocess.add_env_variable([])\n  else:\n    preprocess = ObjectDict({\n      'outputs': {\n        'bucket': bucket\n      }\n    })\n  # Step 2: Do hyperparameter tuning of the model on Cloud ML Engine\n  if start_step <= 2:\n    hparam_train = dsl.ContainerOp(\n      name='hypertrain',\n      # image needs to be a compile-time string\n      image='gcr.io/cloud-training-demos/babyweight-pipeline-hypertrain:latest',\n      arguments=[\n        preprocess.outputs['bucket']\n      ],\n      file_outputs={'jobname': '/output.txt'}\n    )\n  else:\n    hparam_train = ObjectDict({\n      'outputs': {\n        'jobname': 'babyweight_181008_210829'\n      }\n    })\n\n  # Step 3: Train the model some more, but on the pipelines cluster itself\n  if start_step <= 3:\n    train_tuned = dsl.ContainerOp(\n      name='traintuned',\n      # image needs to be a compile-time string\n      image='gcr.io/cloud-training-demos/babyweight-pipeline-traintuned-trainer:latest',\n      #image='gcr.io/cloud-training-demos/babyweight-pipeline-traintuned-trainer@sha256:3d73c805430a16d0675aeafa9819d6d2cfbad0f0f34cff5fb9ed4e24493bc9a8',\n      arguments=[\n        hparam_train.outputs['jobname'],\n        bucket\n      ],\n      file_outputs={'train': '/output.txt'}\n    )\n    train_tuned.set_memory_request('2G')\n    train_tuned.set_cpu_request('1')\n  else:\n    train_tuned = ObjectDict({\n        'outputs': {\n          'train': 'gs://cloud-training-demos-ml/babyweight/hyperparam/15'\n        }\n    })\n\n  # Step 4: Deploy the trained model to Cloud ML Engine\n  if start_step <= 4:\n    deploy_cmle = dsl.ContainerOp(\n      name='deploycmle',\n      # image needs to be a compile-time string\n      image='gcr.io/cloud-training-demos/babyweight-pipeline-deploycmle:latest',\n      arguments=[\n        train_tuned.outputs['train'],  # modeldir\n        'babyweight',\n        'mlp'\n      ],\n      file_outputs={\n        'model': '/model.txt',\n        'version': '/version.txt'\n      }\n    )\n  else:\n    deploy_cmle = ObjectDict({\n      'outputs': {\n        'model': 'babyweight',\n        'version': 'mlp'\n      }\n    })\n\n  # Step 5: Deploy the trained model to AppEngine\n  if start_step <= 5:\n    deploy_cmle = dsl.ContainerOp(\n      name='deployapp',\n      # image needs to be a compile-time string\n      image='gcr.io/cloud-training-demos/babyweight-pipeline-deployapp:latest',\n      arguments=[\n        deploy_cmle.outputs['model'],\n        deploy_cmle.outputs['version']\n      ],\n      file_outputs={\n        'appurl': '/appurl.txt'\n      }\n    )\n  else:\n    deploy_cmle = ObjectDict({\n      'outputs': {\n        'appurl': 'https://cloud-training-demos.appspot.com/'\n      }\n    })\n\"\"\"\n\nif __name__ == '__main__':\n  import kfp.compiler as compiler\n\n  compiler._compile(train_and_deploy, __file__ + '.tgz')\n"
  },
  {
    "repo": "dhodun/kubeflow-pipeline-examples",
    "file_path": "mask_rcnn/mlp_babyweight.bak.py",
    "raw_url": "https://raw.githubusercontent.com/dhodun/kubeflow-pipeline-examples/master/mask_rcnn/mlp_babyweight.bak.py",
    "content": "#!/usr/bin/env python3\n# Copyright 2018 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport kfp.dsl as dsl\n\nclass ObjectDict(dict):\n  def __getattr__(self, name):\n    if name in self:\n      return self[name]\n    else:\n      raise AttributeError(\"No such attribute: \" + name)\n\n\n@dsl.pipeline(\n  name='babyweight',\n  description='Train Babyweight model'\n)\ndef train_and_deploy(\n    project=dsl.PipelineParam(name='project', value='cloud-training-demos'),\n    bucket=dsl.PipelineParam(name='bucket', value='cloud-training-demos-ml'),\n    startYear=dsl.PipelineParam(name='startYear', value='2000')\n):\n  \"\"\"Pipeline to train babyweight model\"\"\"\n  start_step = 1\n\n  # Step 1: create training dataset using Apache Beam on Cloud Dataflow\n  if start_step <= 1:\n    preprocess = dsl.ContainerOp(\n      name='preprocess',\n      # image needs to be a compile-time string\n      image='gcr.io/cloud-training-demos/babyweight-pipeline-bqtocsv:latest',\n      arguments=[\n        '--project', project,\n        '--mode', 'cloud',\n        '--bucket', bucket,\n        '--start_year', startYear\n      ],\n      file_outputs={'bucket': '/output.txt'}\n    )\n  else:\n    preprocess = ObjectDict({\n      'outputs': {\n        'bucket': bucket\n      }\n    })\n\n  # Step 2: Do hyperparameter tuning of the model on Cloud ML Engine\n  if start_step <= 2:\n    hparam_train = dsl.ContainerOp(\n      name='hypertrain',\n      # image needs to be a compile-time string\n      image='gcr.io/cloud-training-demos/babyweight-pipeline-hypertrain:latest',\n      arguments=[\n        preprocess.outputs['bucket']\n      ],\n      file_outputs={'jobname': '/output.txt'}\n    )\n  else:\n    hparam_train = ObjectDict({\n      'outputs': {\n        'jobname': 'babyweight_181008_210829'\n      }\n    })\n\n  # Step 3: Train the model some more, but on the pipelines cluster itself\n  if start_step <= 3:\n    train_tuned = dsl.ContainerOp(\n      name='traintuned',\n      # image needs to be a compile-time string\n      image='gcr.io/cloud-training-demos/babyweight-pipeline-traintuned-trainer:latest',\n      #image='gcr.io/cloud-training-demos/babyweight-pipeline-traintuned-trainer@sha256:3d73c805430a16d0675aeafa9819d6d2cfbad0f0f34cff5fb9ed4e24493bc9a8',\n      arguments=[\n        hparam_train.outputs['jobname'],\n        bucket\n      ],\n      file_outputs={'train': '/output.txt'}\n    )\n    train_tuned.set_memory_request('2G')\n    train_tuned.set_cpu_request('1')\n  else:\n    train_tuned = ObjectDict({\n        'outputs': {\n          'train': 'gs://cloud-training-demos-ml/babyweight/hyperparam/15'\n        }\n    })\n\n\n  # Step 4: Deploy the trained model to Cloud ML Engine\n  if start_step <= 4:\n    deploy_cmle = dsl.ContainerOp(\n      name='deploycmle',\n      # image needs to be a compile-time string\n      image='gcr.io/cloud-training-demos/babyweight-pipeline-deploycmle:latest',\n      arguments=[\n        train_tuned.outputs['train'],  # modeldir\n        'babyweight',\n        'mlp'\n      ],\n      file_outputs={\n        'model': '/model.txt',\n        'version': '/version.txt'\n      }\n    )\n  else:\n    deploy_cmle = ObjectDict({\n      'outputs': {\n        'model': 'babyweight',\n        'version': 'mlp'\n      }\n    })\n\n  # Step 5: Deploy the trained model to AppEngine\n  if start_step <= 5:\n    deploy_cmle = dsl.ContainerOp(\n      name='deployapp',\n      # image needs to be a compile-time string\n      image='gcr.io/cloud-training-demos/babyweight-pipeline-deployapp:latest',\n      arguments=[\n        deploy_cmle.outputs['model'],\n        deploy_cmle.outputs['version']\n      ],\n      file_outputs={\n        'appurl': '/appurl.txt'\n      }\n    )\n  else:\n    deploy_cmle = ObjectDict({\n      'outputs': {\n        'appurl': 'https://cloud-training-demos.appspot.com/'\n      }\n    })\n\n\nif __name__ == '__main__':\n  import kfp.compiler as compiler\n\n  compiler._compile(train_and_deploy, __file__ + '.tgz')\n"
  },
  {
    "repo": "dhodun/kubeflow-pipeline-examples",
    "file_path": "mask_rcnn/test_client.py",
    "raw_url": "https://raw.githubusercontent.com/dhodun/kubeflow-pipeline-examples/master/mask_rcnn/test_client.py",
    "content": "import kfp.components as comp\n\nEXPERIMENT_NAME='lightweight python components'\n\n#Define a Python function\ndef add(a: float, b: float) -> float:\n   '''Calculates sum of two arguments'''\n   return a + b\n\nadd_op = comp.func_to_container_op(add)\n\n# Advanced function\n# Demonstrates imports, helper functions and multiple outputs\nfrom typing import NamedTuple\n\n\ndef my_divmod(dividend: float, divisor: float, output_dir: str = './') -> NamedTuple('MyDivmodOutput',\n                                                                                     [('quotient', float),\n                                                                                      ('remainder', float)]):\n    '''Divides two numbers and calculate  the quotient and remainder'''\n    # Imports inside a component function:\n    import numpy as np\n\n    # This function demonstrates how to use nested functions inside a component function:\n    def divmod_helper(dividend, divisor):\n        return np.divmod(dividend, divisor)\n\n    (quotient, remainder) = divmod_helper(dividend, divisor)\n\n    from tensorflow.python.lib.io import file_io\n    import json\n\n    # Exports a sample tensorboard:\n    metadata = {\n        'outputs': [{\n            'type': 'tensorboard',\n            'source': 'gs://ml-pipeline-dataset/tensorboard-train',\n        }]\n    }\n    with open(output_dir + 'mlpipeline-ui-metadata.json', 'w') as f:\n        json.dump(metadata, f)\n\n    # Exports two sample metrics:\n    metrics = {\n        'metrics': [{\n            'name': 'quotient',\n            'numberValue': float(quotient),\n        }, {\n            'name': 'remainder',\n            'numberValue': float(remainder),\n        }]}\n\n    with file_io.FileIO(output_dir + 'mlpipeline-metrics.json', 'w') as f:\n        json.dump(metrics, f)\n\n    from collections import namedtuple\n    divmod_output = namedtuple('MyDivmodOutput', ['quotient', 'remainder'])\n    return divmod_output(quotient, remainder)\n\nmy_divmod(100, 7)\n\ndivmod_op = comp.func_to_container_op(my_divmod, base_image='tensorflow/tensorflow:1.11.0-py3')\n\nimport kfp.dsl as dsl\n\n\n@dsl.pipeline(\n    name='Calculation pipeline',\n    description='A toy pipeline that performs arithmetic calculations.'\n)\ndef calc_pipeline(\n        a='a',\n        b='7',\n        c='17',\n):\n    # Passing pipeline parameter and a constant value as operation arguments\n    add_task = add_op(a, 4)  # Returns a dsl.ContainerOp class instance.\n\n    # Passing a task output reference as operation arguments\n    # For an operation with a single return value, the output reference can be accessed using `task.output` or `task.outputs['output_name']` syntax\n    divmod_task = divmod_op(add_task.output, b, '/')\n\n    # For an operation with a multiple return values, the output references can be accessed using `task.outputs['output_name']` syntax\n    result_task = add_op(divmod_task.outputs['quotient'], c)\n\n\npipeline_func = calc_pipeline\npipeline_filename = pipeline_func.__name__ + '.pipeline.zip'\nimport kfp.compiler as compiler\ncompiler.Compiler().compile(pipeline_func, pipeline_filename)\n\n\n#Specify pipeline argument values\narguments = {'a': '7', 'b': '8'}\n\n#Get or create an experiment and submit a pipeline run\nimport kfp\nclient = kfp.Client('127.0.0.1:8085/pipeline')\nexperiment = client.create_experiment(EXPERIMENT_NAME)\n\n#Submit a pipeline run\nrun_name = pipeline_func.__name__ + ' run'\nrun_result = client.run_pipeline(experiment.id, run_name, pipeline_filename, arguments)\n\n#vvvvvvvvv This link leads to the run information page. (Note: There is a bug in JupyterLab that modifies the URL and makes the link stop working)"
  },
  {
    "repo": "ventus550/kubeflow-pipelines-project",
    "file_path": "pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/ventus550/kubeflow-pipelines-project/master/pipeline.py",
    "content": "# +\nfrom google.cloud import aiplatform\nfrom kfp.registry import RegistryClient\nfrom kfp import compiler, dsl\nfrom kfp.dsl import (\n    Dataset,\n    Input,\n    Model,\n    If,\n    Else,\n)\n\nimport google_cloud_pipeline_components.v1.custom_job.utils as custom_job\nimport components\nfrom src.secrets import configs\n\n# -\n\n# ## Static Configuration\n\nTENSORBOARD_ID = str(configs.tensorboard)\nTRAIN_LOCATION = configs.location\nACCELERATOR_TYPE = \"NVIDIA_TESLA_T4\"\n\n# ## Pipeline definition\n\n# +\ntensorboard = aiplatform.Tensorboard(\n    location=TRAIN_LOCATION, project=configs.project, tensorboard_name=TENSORBOARD_ID\n)\n\ntraining_config = dict(\n    component_spec=components.train_model,\n    display_name=configs.model,\n    tensorboard=tensorboard.resource_name,\n    base_output_directory=configs.pipeline_directory,\n    service_account=configs.service_account,\n)\n\ngpu = dict(accelerator_type=ACCELERATOR_TYPE)\n\n\n@dsl.pipeline(\n    pipeline_root=configs.pipeline_directory,\n    name=configs.model,\n)\ndef pipeline(\n    dataset: str = f\"{configs.data_directory}/words.npz\",\n    epochs: int = 10,\n    upload: bool = False,\n    upload_threshold: float = 0.0,\n    accelerator: bool = False,\n    pretrained_model: Input[Model] = None,\n):\n    importer = dsl.importer(\n        artifact_uri=dataset,\n        artifact_class=Dataset,\n        reimport=False,\n    )\n\n    data = components.split_data(ratio=0.1, dataset=importer.output)\n    train = data.outputs[\"train\"]\n    test = data.outputs[\"test\"]\n\n    train_model_gpu_op = custom_job.create_custom_training_job_op_from_component(\n        **(training_config | gpu)\n    )\n    train_model_cpu_op = custom_job.create_custom_training_job_op_from_component(\n        **training_config\n    )\n\n    def branch(train_model_op):\n        # replace with OneOf once https://github.com/kubeflow/pipelines/issues/10271 is resolved\n        model = train_model_op.outputs[\"trained_model\"]\n        components.shap_explainer(rows=4, cols=5, dataset=test, model=model)\n        metric = components.metrics(dataset=test, model=model).outputs[\"Output\"]\n        with If(upload and metric > upload_threshold, name=\"metric > threshold\"):\n            components.upload_model(model=model)\n\n    with If(accelerator == True, name=\"gpu\"):\n        train_model_gpu = train_model_gpu_op(\n            epochs=epochs, dataset=train, location=TRAIN_LOCATION\n        )\n        branch(train_model_gpu)\n    with Else(name=\"cpu\"):\n        train_model_cpu = train_model_cpu_op(\n            epochs=epochs, dataset=train, location=TRAIN_LOCATION\n        )\n        branch(train_model_cpu)\n\n\n# -\n\n\n# ## Compilation and upload\n\nclient = RegistryClient(host=configs.artifactory)\ncompiler.Compiler().compile(pipeline_func=pipeline, package_path=configs.pipeline_name)\nclient.upload_pipeline(file_name=configs.pipeline_name, tags=[\"latest\"])\n"
  },
  {
    "repo": "ventus550/kubeflow-pipelines-project",
    "file_path": "components/_metrics.py",
    "raw_url": "https://raw.githubusercontent.com/ventus550/kubeflow-pipelines-project/master/components/_metrics.py",
    "content": "from kfp.dsl import component, Dataset, Input, Output, Model, Markdown\nfrom src.secrets import configs\n\n\n@component(base_image=configs.keras_image, packages_to_install=[\"seaborn\"])\ndef metrics(\n    dataset: Input[Dataset],\n    model: Input[Model],\n    edit_distance_histogram: Output[Markdown],\n    predictions: Output[Markdown],\n) -> int:\n    from src import aitoolkit\n    from src.utils import capture_image\n    import seaborn\n    import numpy\n\n    X, Y = numpy.load(dataset.path + \".npz\").values()\n    X, Y = aitoolkit.format_data(X, Y, aitoolkit.characters)\n    model = aitoolkit.load(model.path)\n    batch = aitoolkit.batch_prediction(model, X, Y)\n\n    edit_distance_values = numpy.array([sample.value for sample in batch])\n    seaborn.histplot(edit_distance_values, bins=10, kde=True, alpha=0.6)\n    open(edit_distance_histogram.path, \"w\").write(f\"![Image]({capture_image()})\")\n\n    batch.show()\n    open(predictions.path, \"w\").write(f\"![Image]({capture_image()})\")\n\n    average_edit_distance = numpy.mean(edit_distance_values)\n    print(\"Average edit distance:\", average_edit_distance)\n    return int(average_edit_distance)\n"
  },
  {
    "repo": "ventus550/kubeflow-pipelines-project",
    "file_path": "components/_shap_explainer.py",
    "raw_url": "https://raw.githubusercontent.com/ventus550/kubeflow-pipelines-project/master/components/_shap_explainer.py",
    "content": "from kfp.dsl import component, Dataset, Input, Output, Model, Markdown\nfrom src.secrets import configs\n\n\n@component(base_image=configs.keras_image, packages_to_install=[\"shap==0.44\"])\ndef shap_explainer(\n    rows: int,\n    cols: int,\n    dataset: Input[Dataset],\n    model: Input[Model],\n    explanation: Output[Markdown],\n):\n    import cv2\n    import shap\n    import numpy\n    import keras\n    import tensorflow as tf\n\n    from src import aitoolkit\n    from src.utils import capture_image\n\n    characters = aitoolkit.characters\n    X, Y = numpy.load(dataset.path + \".npz\").values()\n    X, Y = aitoolkit.format_data(X, Y, characters)\n    model = aitoolkit.load(model.path)\n\n    # repurpose model for \"classification\"\n    classifier = keras.models.Model(\n        model.input, tf.reduce_max(model.output, axis=1)[:, : len(characters)]\n    )\n\n    masker = shap.maskers.Image(\"inpaint_telea\", X[0].shape)\n    explainer = shap.Explainer(classifier, masker, output_names=list(characters))\n    indices = numpy.random.randint(len(X), size=rows)\n    shap_values = explainer(\n        X[indices],\n        max_evals=100,\n        batch_size=1000,\n        outputs=shap.Explanation.argsort.flip[:cols],\n    )\n\n    labels = shap_values.output_names\n    if len(numpy.shape(shap_values.output_names)) == 1:\n        # shap is an awful library and thus requires awful fixes\n        labels = numpy.reshape(labels * rows, (rows, cols))\n\n    shap.image_plot(shap_values, labels=labels)\n    open(explanation.path, \"w\").write(f\"![Image]({capture_image()})\")\n"
  },
  {
    "repo": "ventus550/kubeflow-pipelines-project",
    "file_path": "components/_split_data.py",
    "raw_url": "https://raw.githubusercontent.com/ventus550/kubeflow-pipelines-project/master/components/_split_data.py",
    "content": "from kfp.dsl import component, Dataset, Input, Output\nfrom src.secrets import configs\n\n\n@component(base_image=configs.keras_image)\ndef split_data(\n    ratio: float, dataset: Input[Dataset], train: Output[Dataset], test: Output[Dataset]\n):\n    import numpy\n\n    X, Y = numpy.load(dataset.path).values()\n    test_size = int(ratio * len(X))\n\n    train_set = (X[test_size:], Y[test_size:])\n    test_set = (X[:test_size], Y[:test_size])\n\n    numpy.savez(train.path, *train_set)\n    numpy.savez(test.path, *test_set)\n"
  },
  {
    "repo": "ventus550/kubeflow-pipelines-project",
    "file_path": "components/_train_model.py",
    "raw_url": "https://raw.githubusercontent.com/ventus550/kubeflow-pipelines-project/master/components/_train_model.py",
    "content": "from kfp.dsl import component, Dataset, Input, Output, Model\nfrom src.secrets import configs\n\n\n@component(\n    base_image=configs.keras_image,\n    packages_to_install=[\"google-cloud-secret-manager>=2.17.0\"],\n)\ndef train_model(epochs: int, dataset: Input[Dataset], trained_model: Output[Model]):\n    import numpy\n    import keras\n    import os\n\n    from keras.layers import (\n        Conv2D,\n        BatchNormalization,\n        MaxPooling2D,\n        Dropout,\n        Flatten,\n        Dense,\n        Activation,\n        Permute,\n        Reshape,\n        Bidirectional,\n        LSTM,\n    )\n    from keras import callbacks\n    import tensorflow as tf\n    from contextlib import suppress\n    from src import aitoolkit\n    from src.secrets import configs\n\n    image_width = 128\n    image_height = 32\n\n    X, Y = numpy.load(dataset.path + \".npz\").values()\n    X, Y = aitoolkit.format_data(X, Y, aitoolkit.characters)\n    X_test, Y_test = X[:1000], Y[:1000]\n    X_train, Y_train = X[1000:], Y[1000:]\n\n    tensorboard = callbacks.TensorBoard(\n        # vertex-provided directory for logs\n        log_dir=os.environ[\"AIP_TENSORBOARD_LOG_DIR\"],\n        histogram_freq=1,\n    )\n    checkpoints = callbacks.ModelCheckpoint(\n        monitor=\"edit_distance\",\n        filepath=\"./weights.h5\",\n        save_weights_only=True,\n        save_best_only=True,\n        mode=\"min\",\n    )\n    plateau = callbacks.ReduceLROnPlateau(\n        monitor=\"edit_distance\", patience=10, verbose=1, factor=0.90\n    )\n\n    namespace = tf.Graph()\n\n    def Convolutions(filters, kernel=5):\n        return keras.models.Sequential(\n            [\n                Conv2D(filters, kernel, padding=\"same\", kernel_initializer=\"he_normal\"),\n                BatchNormalization(),\n                Activation(\"relu\"),\n            ],\n            name=namespace.unique_name(\"convolutions\"),\n        )\n\n    def build_model():\n        input = keras.Input(shape=(image_height, image_width, 1), name=\"images\")\n\n        x = Convolutions(128, 3)(input)\n        x = MaxPooling2D((2, 2))(x)\n\n        x = Convolutions(64, 3)(x)\n        x = MaxPooling2D((2, 2))(x)\n\n        x = Permute([2, 1, 3])(x)\n        x = Reshape([image_width // 4, -1])(x)\n\n        x = Dense(64, activation=\"relu\")(x)\n        x = Dropout(0.2)(x)\n\n        x = Bidirectional(LSTM(128, return_sequences=True, dropout=0.25))(x)\n        x = Bidirectional(LSTM(64, return_sequences=True, dropout=0.25))(x)\n\n        # Reserve two extra tokens for the ctc_loss\n        x = Dense(\n            len(aitoolkit.characters) + 2, activation=\"softmax\", name=\"predictions\"\n        )(x)\n\n        model = keras.models.Model(inputs=input, outputs=x, name=configs.model)\n        model.compile(\n            optimizer=\"adam\", loss=aitoolkit.ctc_loss, metrics=[aitoolkit.edit_distance]\n        )\n        return model\n\n    model = build_model()\n    model.summary()\n\n    model.fit(\n        X_train,\n        Y_train,\n        validation_split=0.1,\n        epochs=epochs,\n        callbacks=[tensorboard, checkpoints, plateau],\n    )\n\n    # load from last checkpoint\n    with suppress(FileNotFoundError):\n        model.load_weights(\"weights.h5\")\n\n    meta = {\n        \"epochs\": epochs,\n        \"characters\": \"\".join(aitoolkit.characters),\n        \"padded\": -1,\n        \"parameters\": model.count_params(),\n    }\n\n    aitoolkit.save(model, trained_model.path, metadata=meta)\n"
  },
  {
    "repo": "ventus550/kubeflow-pipelines-project",
    "file_path": "components/_upload_model.py",
    "raw_url": "https://raw.githubusercontent.com/ventus550/kubeflow-pipelines-project/master/components/_upload_model.py",
    "content": "from kfp.dsl import component, Dataset, Input, Output, Model, Markdown\nfrom src.secrets import configs\n\n\n@component(\n    base_image=configs.keras_image,\n    packages_to_install=[\"google-cloud-secret-manager>=2.17.0\"],\n)\ndef upload_model(\n    model: Input[Model],\n    container_image: str = \"europe-docker.pkg.dev/vertex-ai-restricted/prediction/tf_opt-cpu.2-15:latest\",\n):\n    import keras\n    from google.cloud import aiplatform\n    from src.secrets import configs\n    from src import aitoolkit\n\n    path = model.path\n    model = aitoolkit.load(path)\n    model = keras.Model(model.input, model.output)\n    model.save(path, save_format=\"tf\")\n\n    print(\"uploading to model registry\")\n    model = aiplatform.Model.upload(\n        display_name=configs.model,\n        artifact_uri=path,\n        serving_container_image_uri=container_image,\n        location=configs.location,\n        project=configs.project,\n        staging_bucket=configs.bucket,\n    )\n"
  },
  {
    "repo": "Michallote/kubeflow-from-pipeline",
    "file_path": "generated_functions.py",
    "raw_url": "https://raw.githubusercontent.com/Michallote/kubeflow-from-pipeline/main/generated_functions.py",
    "content": "\nfrom typing import Any, NamedTuple\nimport kfp.dsl as dsl\nfrom collections import namedtuple\nfrom kfp.dsl import Input, Output, Dataset, component\nfrom consts import MLFLOW_IMAGE\n\n@component(base_image=MLFLOW_IMAGE)\ndef hello_project_raw_catalog_segmentation_tyrany(a_dataset: Input[Dataset], b_dataset: Input[Dataset], catalog_dataset: Output[Dataset], segmentation_dataset: Output[Dataset], tyrany_dataset: Output[Dataset]) -> None:\n    # Imports\n    from kubeflow_from_pipeline.databases import gs_read_auto\n    from kubeflow_from_pipeline.databases import gs_store_auto\n    from hello import hello_project\n    # Body\n    # Inputs Loading\n    a = gs_read_auto(a_dataset.path)\n    b = gs_read_auto(b_dataset.path)\n    # Function Call\n    catalog, segmentation, tyrany = hello_project(a, b, sql_file='data.sql', n_iqr=1.5)\n    # Outputs Packing\n    gs_store_auto(catalog, catalog_dataset.path)\n    gs_store_auto(segmentation, segmentation_dataset.path)\n    gs_store_auto(tyrany, tyrany_dataset.path)"
  },
  {
    "repo": "Michallote/kubeflow-from-pipeline",
    "file_path": "main_legacy.py",
    "raw_url": "https://raw.githubusercontent.com/Michallote/kubeflow-from-pipeline/main/main_legacy.py",
    "content": "from collections import namedtuple\nfrom typing import Any, Callable, Iterable, NamedTuple, Optional\n\nfrom jinja2 import Template\n\nfrom hello import hello_project\nfrom kubeflow_from_pipeline.databases import gs_read_auto, load_gs_data\n\nGLOBAL_IMPORTS = \"\"\"\nfrom typing import Any, NamedTuple\nimport kfp.dsl as dsl\nfrom collections import namedtuple\nfrom kfp.dsl import Input, Output, Dataset, component\nfrom consts import MLFLOW_IMAGE\n\"\"\"\n\n\nTUPLE_FUNCTION_TEMPLATE = Template(\n    \"\"\"\n{{ decorator }}\ndef {{ function_name }}({{ parameters|join(', ') }}) -> {{ output_type_hint }}:\n    # Imports\n    {{ imports|join('\\n')|indent(4) }}\n    # Body\n    {{ body|indent(4) }}\n\n\"\"\"\n)\n\nDECLARATIVE_FUNCTION_TEMPLATE = Template(\n    \"\"\"\n{{ decorator }}\ndef {{ function_name }}({{ parameters|join(', ') }}):\n    # Imports\n    {{ imports|join('\\n')|indent(4) }}\n    # Body\n    {{ body|indent(4) }}\n\"\"\"\n)\n\n\ndef make_input(params: Iterable) -> list[str]:\n    return list(map(lambda x: f\"{x}: Input[Dataset]\", params))\n\n\ndef create_function_code(\n    func_name: str,\n    params: list[str],\n    body: str,\n    imports: list[str],\n    output_type_hint: str = \"Any\",\n    decorator: Optional[str] = None,\n) -> str:\n\n    if decorator is None:\n        decorator = \"\"\n\n    code = TUPLE_FUNCTION_TEMPLATE.render(\n        function_name=func_name,\n        parameters=params,\n        output_type_hint=output_type_hint,\n        imports=imports,\n        body=body,\n        decorator=decorator,\n    )\n\n    return code\n\n\ndef create_import_statement(function: Callable) -> str:\n    return f\"from {function.__module__} import {function.__name__}\"\n\n\ndef create_function_call(\n    function_name: str, args: list[str], kwargs: dict[str, Any], outputs: list[str]\n) -> str:\n\n    function_arguments = args + [f\"{k}={repr(v)}\" for k, v in kwargs.items()]\n\n    if outputs:\n        function_outputs = \", \".join(outputs) + \" = \"\n    else:\n        function_outputs = \"\"\n\n    return f\"{function_outputs}{function_name}({', '.join(function_arguments)})\"\n\n\ndef create_return_statement(outputs: list[str]) -> tuple[str, str]:\n\n    type_statement = \", \".join(map(lambda x: f\"{x}=dsl.Dataset\", outputs))\n\n    output_asignment = \", \".join(map(lambda x: f\"{x}={x}\", outputs))\n\n    output_statement = (\n        f'outputs = NamedTuple(\"outputs\", {type_statement})\\n'\n        f\"return outputs({output_asignment})\"\n    )\n    output_type_hint = f'NamedTuple(\"outputs\", {type_statement})'\n\n    return output_statement, output_type_hint\n\n\ndef create_function_from_step(\n    name,\n    function: Callable,\n    inputs: Optional[list[str]] = None,\n    outputs: Optional[list[str]] = None,\n    kwargs: Optional[dict] = None,\n    decorator: Optional[str] = None,\n) -> str:\n\n    if inputs is None:\n        inputs = []\n\n    if outputs is None:\n        outputs = []\n\n    if kwargs is None:\n        kwargs = {}\n\n    component_name = \"_\".join(([function.__name__, name] + outputs))\n\n    params = list(map(lambda x: f\"{x}_dataset\", inputs))\n\n    imports = []\n\n    if inputs:\n        imports.append(create_import_statement(gs_read_auto))\n\n    imports.append(create_import_statement(function))\n\n    loading_lines = [\n        f\"{inpt} = gs_read_auto({param}.path)\" for param, inpt in zip(params, inputs)\n    ]\n\n    loading_statement = \"\\n\".join(loading_lines)\n\n    function_call = create_function_call(\n        function_name=function.__name__, args=inputs, kwargs=kwargs, outputs=outputs\n    )\n\n    output_statement, output_type_hint = create_return_statement(outputs=outputs)\n\n    body = \"\\n\".join(\n        [\n            \"# Inputs Loading\",\n            loading_statement,\n            \"# Function Call\",\n            function_call,\n            \"# Outputs Packing\",\n            output_statement,\n        ]\n    )\n\n    component_code = create_function_code(\n        component_name,\n        make_input(params),\n        body,\n        imports,\n        output_type_hint,\n        decorator=\"@component(base_image=MLFLOW_IMAGE)\",\n    )\n\n    return component_code\n\n\nstep = {\n    \"name\": \"raw\",\n    \"function\": hello_project,\n    \"inputs\": [\"a\", \"b\"],\n    \"outputs\": [\"catalog\"],\n    \"kwargs\": {\"sql_file\": \"data.sql\", \"n_iqr\": 1.5},\n}\n\nfunc_name = \"multiply_numbers\"\nparams = [\n    \"a\",\n    \"b\",\n]\nbody = \"\"\"\n\ncatalog_full = combine_catalog_sources(a, b)\n\noutputs = namedtuple(\"outputs\", catalog_full=dsl.Dataset)\nreturn outputs(catalog_full=catalog_full)\"\"\"\nimports = [\n    \"from src.databases.load_gs_data import gs_read_df\",\n    \"from src.databases.data_collection import combine_catalog_sources\",\n]\noutput_type_hint = 'NamedTuple(\"outputs\", catalog_full=dsl.Dataset)'\n\ndecorator = \"@component(base_image=MLFLOW_IMAGE)\"\n\ncode = GLOBAL_IMPORTS\ncode += create_function_code(func_name, params, body, imports, output_type_hint)\ncode += create_function_code(\n    func_name, make_input(params), body, imports, decorator=decorator\n)\n\ncode += create_function_from_step(**step)\n\nwith open(\"generated_functions.py\", \"w\") as f:\n    f.write(code)\n"
  },
  {
    "repo": "Michallote/kubeflow-from-pipeline",
    "file_path": "kubeflow_from_pipeline/kfp_generator_declarative.py",
    "raw_url": "https://raw.githubusercontent.com/Michallote/kubeflow-from-pipeline/main/kubeflow_from_pipeline/kfp_generator_declarative.py",
    "content": "from collections import namedtuple\nfrom typing import Any, Callable, Iterable, NamedTuple, Optional\n\nfrom jinja2 import Template\n\nfrom hello import hello_project\nfrom kubeflow_from_pipeline.databases import gs_read_auto, gs_store_auto, load_gs_data\n\nGLOBAL_IMPORTS = \"\"\"\nfrom typing import Any, NamedTuple\nimport kfp.dsl as dsl\nfrom collections import namedtuple\nfrom kfp.dsl import Input, Output, Dataset, component\nfrom consts import MLFLOW_IMAGE\n\"\"\"\n\nDECLARATIVE_FUNCTION_TEMPLATE = Template(\n    \"\"\"\n{{ decorator }}\ndef {{ function_name }}({{ parameters|join(', ') }}) -> {{ output_type_hint }}:\n    # Imports\n    {{ imports|join('\\n')|indent(4) }}\n    # Body\n    {{ body|indent(4) }}\n\"\"\"\n)\n\n\ndef make_input(params: Iterable) -> list[str]:\n    return list(map(lambda x: f\"{x}: Input[Dataset]\", params))\n\n\ndef make_output(params: Iterable) -> list[str]:\n    return list(map(lambda x: f\"{x}: Output[Dataset]\", params))\n\n\ndef create_function_code(\n    func_name: str,\n    params: list[str],\n    body: str,\n    imports: list[str],\n    output_type_hint: str = \"Any\",\n    decorator: Optional[str] = None,\n) -> str:\n\n    if decorator is None:\n        decorator = \"\"\n\n    code = DECLARATIVE_FUNCTION_TEMPLATE.render(\n        function_name=func_name,\n        parameters=params,\n        output_type_hint=output_type_hint,\n        imports=imports,\n        body=body,\n        decorator=decorator,\n    )\n\n    return code\n\n\ndef create_import_statement(function: Callable) -> str:\n    return f\"from {function.__module__} import {function.__name__}\"\n\n\ndef create_function_call(\n    function_name: str, args: list[str], kwargs: dict[str, Any], outputs: list[str]\n) -> str:\n\n    function_arguments = args + [f\"{k}={repr(v)}\" for k, v in kwargs.items()]\n\n    if outputs:\n        function_outputs = \", \".join(outputs) + \" = \"\n    else:\n        function_outputs = \"\"\n\n    return f\"{function_outputs}{function_name}({', '.join(function_arguments)})\"\n\n\ndef create_function_from_step(\n    name,\n    function: Callable,\n    inputs: Optional[list[str]] = None,\n    outputs: Optional[list[str]] = None,\n    kwargs: Optional[dict] = None,\n    decorator: Optional[str] = None,\n) -> str:\n\n    if inputs is None:\n        inputs = []\n\n    if outputs is None:\n        outputs = []\n\n    if kwargs is None:\n        kwargs = {}\n\n    component_name = \"_\".join(([function.__name__, name] + outputs))\n\n    imports = []\n\n    if inputs:\n        imports.append(create_import_statement(gs_read_auto))\n\n    if outputs:\n        imports.append(create_import_statement(gs_store_auto))\n\n    imports.append(create_import_statement(function))\n\n    input_params = list(map(lambda x: f\"{x}_dataset\", inputs))\n    loading_lines = [\n        f\"{inpt} = gs_read_auto({param}.path)\"\n        for param, inpt in zip(input_params, inputs)\n    ]\n\n    loading_statement = \"\\n\".join(loading_lines)\n\n    function_call = create_function_call(\n        function_name=function.__name__, args=inputs, kwargs=kwargs, outputs=outputs\n    )\n\n    output_params = list(map(lambda x: f\"{x}_dataset\", outputs))\n\n    output_lines = [\n        f\"gs_store_auto({outpt}, {param}.path)\"\n        for outpt, param in zip(outputs, output_params)\n    ]\n\n    output_statement = \"\\n\".join(output_lines)\n\n    body = \"\\n\".join(\n        [\n            \"# Inputs Loading\",\n            loading_statement,\n            \"# Function Call\",\n            function_call,\n            \"# Outputs Packing\",\n            output_statement,\n        ]\n    )\n\n    output_type_hint = \"None\"\n\n    component_params = make_input(input_params) + make_output(output_params)\n\n    component_code = create_function_code(\n        component_name,\n        component_params,\n        body,\n        imports,\n        output_type_hint,\n        decorator=decorator,\n    )\n\n    return component_code\n\n\nif __name__ == \"__main__\":\n\n    step = {\n        \"name\": \"raw\",\n        \"function\": hello_project,\n        \"inputs\": [\"a\", \"b\"],\n        \"outputs\": [\"catalog\"],\n        \"kwargs\": {\"sql_file\": \"data.sql\", \"n_iqr\": 1.5},\n    }\n\n    func_name = \"multiply_numbers\"\n    params = [\n        \"a\",\n        \"b\",\n    ]\n    body = \"\"\"\n\n    catalog_full = combine_catalog_sources(a, b)\n\n    outputs = namedtuple(\"outputs\", catalog_full=dsl.Dataset)\n    return outputs(catalog_full=catalog_full)\"\"\"\n    imports = [\n        \"from src.databases.load_gs_data import gs_read_df\",\n        \"from src.databases.data_collection import combine_catalog_sources\",\n    ]\n    output_type_hint = 'NamedTuple(\"outputs\", catalog_full=dsl.Dataset)'\n\n    decorator = \"@component(base_image=MLFLOW_IMAGE)\"\n\n    code = GLOBAL_IMPORTS\n    code += create_function_code(func_name, params, body, imports, output_type_hint)\n    code += create_function_code(\n        func_name, make_input(params), body, imports, decorator=decorator\n    )\n    name = step[\"name\"]\n    function: Callable = step[\"function\"]\n    inputs: list = step[\"inputs\"]\n    outputs: list = step[\"outputs\"]\n    kwargs: dict = step[\"kwargs\"]\n\n    code += create_function_from_step(\n        name,\n        function,\n        inputs,\n        outputs,\n        kwargs,\n        decorator=\"@component(base_image=MLFLOW_IMAGE)\",\n    )\n\n    with open(\"generated_functions.py\", \"w\") as f:\n        f.write(code)\n"
  },
  {
    "repo": "Michallote/kubeflow-from-pipeline",
    "file_path": "kubeflow_from_pipeline/kfp_generator_namedtuple.py",
    "raw_url": "https://raw.githubusercontent.com/Michallote/kubeflow-from-pipeline/main/kubeflow_from_pipeline/kfp_generator_namedtuple.py",
    "content": "from collections import namedtuple\nfrom typing import Any, Callable, Iterable, NamedTuple, Optional\n\nfrom jinja2 import Template\n\nfrom kubeflow_from_pipeline.databases import gs_read_auto, load_gs_data\n\nGLOBAL_IMPORTS = \"\"\"\nfrom typing import Any, NamedTuple\nimport kfp.dsl as dsl\nfrom collections import namedtuple\nfrom kfp.dsl import Input, Output, Dataset, component\nfrom consts import MLFLOW_IMAGE\n\"\"\"\n\n\nTUPLE_FUNCTION_TEMPLATE = Template(\n    \"\"\"\n{{ decorator }}\ndef {{ function_name }}({{ parameters|join(', ') }}) -> {{ output_type_hint }}:\n    # Imports\n    {{ imports|join('\\n')|indent(4) }}\n    # Body\n    {{ body|indent(4) }}\n\n\"\"\"\n)\n\nDECLARATIVE_FUNCTION_TEMPLATE = Template(\n    \"\"\"\n{{ decorator }}\ndef {{ function_name }}({{ parameters|join(', ') }}):\n    # Imports\n    {{ imports|join('\\n')|indent(4) }}\n    # Body\n    {{ body|indent(4) }}\n\"\"\"\n)\n\n\ndef make_input(params: Iterable) -> list[str]:\n    return list(map(lambda x: f\"{x}: Input[Dataset]\", params))\n\n\ndef create_function_code(\n    func_name: str,\n    params: list[str],\n    body: str,\n    imports: list[str],\n    output_type_hint: str = \"Any\",\n    decorator: Optional[str] = None,\n) -> str:\n\n    if decorator is None:\n        decorator = \"\"\n\n    code = TUPLE_FUNCTION_TEMPLATE.render(\n        function_name=func_name,\n        parameters=params,\n        output_type_hint=output_type_hint,\n        imports=imports,\n        body=body,\n        decorator=decorator,\n    )\n\n    return code\n\n\ndef create_import_statement(function: Callable) -> str:\n    return f\"from {function.__module__} import {function.__name__}\"\n\n\ndef create_function_call(\n    function_name: str, args: list[str], kwargs: dict[str, Any], outputs: list[str]\n) -> str:\n\n    function_arguments = args + [f\"{k}={repr(v)}\" for k, v in kwargs.items()]\n\n    if outputs:\n        function_outputs = \", \".join(outputs) + \" = \"\n    else:\n        function_outputs = \"\"\n\n    return f\"{function_outputs}{function_name}({', '.join(function_arguments)})\"\n\n\ndef create_return_statement(outputs: list[str]) -> tuple[str, str]:\n\n    type_statement = \", \".join(map(lambda x: f\"{x}=dsl.Dataset\", outputs))\n\n    output_asignment = \", \".join(map(lambda x: f\"{x}={x}\", outputs))\n\n    output_statement = (\n        f'outputs = NamedTuple(\"outputs\", {type_statement})\\n'\n        f\"return outputs({output_asignment})\"\n    )\n    output_type_hint = f'NamedTuple(\"outputs\", {type_statement})'\n\n    return output_statement, output_type_hint\n\n\ndef create_function_from_step(\n    name,\n    function: Callable,\n    inputs: Optional[list[str]] = None,\n    outputs: Optional[list[str]] = None,\n    kwargs: Optional[dict] = None,\n    decorator: Optional[str] = None,\n) -> str:\n\n    if inputs is None:\n        inputs = []\n\n    if outputs is None:\n        outputs = []\n\n    if kwargs is None:\n        kwargs = {}\n\n    component_name = \"_\".join(([function.__name__, name] + outputs))\n\n    params = list(map(lambda x: f\"{x}_dataset\", inputs))\n\n    imports = []\n\n    if inputs:\n        imports.append(create_import_statement(gs_read_auto))\n\n    imports.append(create_import_statement(function))\n\n    loading_lines = [\n        f\"{inpt} = gs_read_auto({param}.path)\" for param, inpt in zip(params, inputs)\n    ]\n\n    loading_statement = \"\\n\".join(loading_lines)\n\n    function_call = create_function_call(\n        function_name=function.__name__, args=inputs, kwargs=kwargs, outputs=outputs\n    )\n\n    output_statement, output_type_hint = create_return_statement(outputs=outputs)\n\n    body = \"\\n\".join(\n        [\n            \"# Inputs Loading\",\n            loading_statement,\n            \"# Function Call\",\n            function_call,\n            \"# Outputs Packing\",\n            output_statement,\n        ]\n    )\n\n    component_code = create_function_code(\n        component_name,\n        make_input(params),\n        body,\n        imports,\n        output_type_hint,\n        decorator=\"@component(base_image=MLFLOW_IMAGE)\",\n    )\n\n    return component_code\n"
  },
  {
    "repo": "kashiftriffort/kubeflow-pipelines-master",
    "file_path": "boston_housing/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/kashiftriffort/kubeflow-pipelines-master/main/boston_housing/pipeline.py",
    "content": "import kfp\nfrom kfp import dsl\n\ndef preprocess_op():\n\n    return dsl.ContainerOp(\n        name='Preprocess Data',\n        image='gnovack/boston_pipeline_preprocessing:latest',\n        arguments=[],\n        file_outputs={\n            'x_train': '/app/x_train.npy',\n            'x_test': '/app/x_test.npy',\n            'y_train': '/app/y_train.npy',\n            'y_test': '/app/y_test.npy',\n        }\n    )\n\ndef train_op(x_train, y_train):\n\n    return dsl.ContainerOp(\n        name='Train Model',\n        image='gnovack/boston_pipeline_train:latest',\n        arguments=[\n            '--x_train', x_train,\n            '--y_train', y_train\n        ],\n        file_outputs={\n            'model': '/app/model.pkl'\n        }\n    )\n\ndef test_op(x_test, y_test, model):\n\n    return dsl.ContainerOp(\n        name='Test Model',\n        image='gnovack/boston_pipeline_test:latest',\n        arguments=[\n            '--x_test', x_test,\n            '--y_test', y_test,\n            '--model', model\n        ],\n        file_outputs={\n            'mean_squared_error': '/app/output.txt'\n        }\n    )\n\ndef deploy_model_op(model):\n\n    return dsl.ContainerOp(\n        name='Deploy Model',\n        image='gnovack/boston_pipeline_deploy_model:latest',\n        arguments=[\n            '--model', model\n        ]\n    )\n\n@dsl.pipeline(\n   name='Boston Housing Pipeline',\n   description='An example pipeline that trains and logs a regression model.'\n)\n\ndef boston_pipeline():\n    _preprocess_op = preprocess_op()\n    \n    _train_op = train_op(\n        dsl.InputArgumentPath(_preprocess_op.outputs['x_train']),\n        dsl.InputArgumentPath(_preprocess_op.outputs['y_train'])\n    ).after(_preprocess_op)\n\n    _test_op = test_op(\n        dsl.InputArgumentPath(_preprocess_op.outputs['x_test']),\n        dsl.InputArgumentPath(_preprocess_op.outputs['y_test']),\n        dsl.InputArgumentPath(_train_op.outputs['model'])\n    ).after(_train_op)\n\n    deploy_model_op(\n        dsl.InputArgumentPath(_train_op.outputs['model'])\n    ).after(_test_op)\n\nclient = kfp.Client()\nclient.create_run_from_pipeline_func(boston_pipeline, arguments={})\n"
  },
  {
    "repo": "ygeat7/kubeflow_pipeline_myxgb",
    "file_path": "pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/ygeat7/kubeflow_pipeline_myxgb/main/pipeline.py",
    "content": "import kfp\nimport kfp.components as comp\nfrom kfp import dsl\n\n@dsl.pipeline(\n    name='xgb-pipeline-katib',\n    description='xgb pipeline with katib'\n)\n\ndef xgb_pipeline_katib(building_num: int=1):\n    \n    xgb_pvc = dsl.PipelineVolume('xgb-pvc')\n    exp_pvc = dsl.PipelineVolume('exp-pvc')\n    data_pvc = dsl.PipelineVolume('data-pvc')\n    \n    data_load = dsl.ContainerOp(\n        name='data load',\n        image='kubeflow-registry.default.svc.cluster.local:30000/xgb_data_load:5.0',\n        command=['python', 'data_load.py'],\n        arguments=[\n            '--num', str(building_num)\n        ],\n        file_outputs={'dataset' : '/dataset.csv'}\n    )\n    \n    data_split = dsl.ContainerOp(\n        name='data split',\n        image='kubeflow-registry.default.svc.cluster.local:30000/data_split:5.5',\n        arguments=[\n            '--dataset', dsl.InputArgumentPath(data_load.outputs['dataset']),\n            '--num', str(building_num)\n        ],\n        command=['python', 'data_split.py'],\n        file_outputs={'xtrain' : '/xtrain.csv',\n                      'xtest' : '/xtest.csv',\n                      'ytrain' : '/ytrain.csv',\n                      'ytest' : '/ytest.csv',\n                      'expyaml' : '/test.yaml'},\n        pvolumes={'/app/exp': exp_pvc,\n\t\t  '/app/data': data_pvc}\n    )\n\n    katib_create_exp = dsl.ContainerOp(\n        name='create katib experiment',\n        image='kubeflow-registry.default.svc.cluster.local:30000/create_exp:5.1',\n        arguments=[\n            '--expyaml', dsl.InputArgumentPath(data_split.outputs['expyaml'])\n        ],\n        command=['python', 'create_exp.py'],\n    )\n\n    get_best_params = dsl.ContainerOp(\n        name='get best params',\n        image='kubeflow-registry.default.svc.cluster.local:30000/get_bp:5.3',\n        arguments=[\n            '--expyaml', dsl.InputArgumentPath(data_split.outputs['expyaml'])\n        ],\n        command=['python', 'get_bp.py'],\n\tfile_outputs={'best_params' : '/best_params.csv'}\n    )\n\n    train = dsl.ContainerOp(\n        name='xgb train',\n        image='kubeflow-registry.default.svc.cluster.local:30000/xgb_train:5.1',\n        arguments=[\n            '--xtrain',  dsl.InputArgumentPath(data_split.outputs['xtrain']),\n            '--ytrain',  dsl.InputArgumentPath(data_split.outputs['ytrain']),\n            '--best_params', dsl.InputArgumentPath(get_best_params.outputs['best_params'])\n        ],\n        command=['python', 'xgb_train.py'],\n        file_outputs={'model' : '/xgb_model.model'}\n    )\n    \n    test = dsl.ContainerOp(\n        name='test and evaluation',\n        image='kubeflow-registry.default.svc.cluster.local:30000/test_eval:5.1',\n        arguments=[\n            '--xtest',  dsl.InputArgumentPath(data_split.outputs['xtest']),\n            '--ytest',  dsl.InputArgumentPath(data_split.outputs['ytest']),\n            '--model',  dsl.InputArgumentPath(train.outputs['model']),\n            '--num', str(building_num)\n        ],\n        command=['python', 'test_eval.py'],\n        pvolumes={'/app/model': xgb_pvc},\n        file_outputs={'mlpipeline-metrics' : '/mlpipeline-metrics.json'}\n    )\n    \n    data_split.after(data_load)\n    katib_create_exp.after(data_split)\n    get_best_params.after(katib_create_exp)\n    train.after(data_split)\n    test.after(train)\n\nif __name__ == \"__main__\":\n    import kfp.compiler as compiler\n    compiler.Compiler().compile(xgb_pipeline_katib, __file__ + \".tar.gz\")\n"
  },
  {
    "repo": "Nannakaroliina/kubeflow-pipeline-demo",
    "file_path": "pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/Nannakaroliina/kubeflow-pipeline-demo/main/pipeline.py",
    "content": "import kfp\nfrom kfp import dsl\n\n\n# Build ContainerOps of Docker images with needed arguments for data/model access and file_output definitions.\n# TODO Fix the FutureWarning regarding ContainerOp\n\ndef preprocess_op():\n    return dsl.ContainerOp(\n        name='Preprocess Data',\n        image='nannakaroliina/kubeflow_pipeline_demo_preprocessing:latest',\n        arguments=[],\n        file_outputs={\n            'x_train': '/app/x_train.npy',\n            'x_test': '/app/x_test.npy',\n            'y_train': '/app/y_train.npy',\n            'y_test': '/app/y_test.npy',\n        }\n    )\n\n\ndef train_op(x_train, y_train):\n    return dsl.ContainerOp(\n        name='Train Model',\n        image='nannakaroliina/kubeflow_pipeline_demo_train:latest',\n        arguments=[\n            '--x_train', x_train,\n            '--y_train', y_train\n        ],\n        file_outputs={\n            'model': '/app/model.plk'\n        }\n    )\n\n\ndef predict_op(x_test, y_test, model):\n    return dsl.ContainerOp(\n        name='Test Model',\n        image='nannakaroliina/kubeflow_pipeline_demo_predict:latest',\n        arguments=[\n            '--x_test', x_test,\n            '--y_test', y_test,\n            '--model', model\n        ],\n        file_outputs={\n            'f1_score': '/app/results.txt'\n        }\n    )\n\n\n@dsl.pipeline(\n    name='Kubeflow pipeline demo',\n    description='Kubeflow pipeline demo with simple model training'\n)\ndef kubeflow_demo_pipeline():\n    _preprocess_op = preprocess_op()\n\n    _train_op = train_op(\n        dsl.InputArgumentPath(_preprocess_op.outputs['x_train']),\n        dsl.InputArgumentPath(_preprocess_op.outputs['y_train'])\n    ).after(_preprocess_op)\n\n    _test_op = predict_op(\n        dsl.InputArgumentPath(_preprocess_op.outputs['x_test']),\n        dsl.InputArgumentPath(_preprocess_op.outputs['y_test']),\n        dsl.InputArgumentPath(_train_op.outputs['model'])\n    ).after(_train_op)\n\n\nif __name__ == '__main__':\n    # Build a pipeline yaml file to be uploaded to Kubeflow Pipeline UI\n    # TODO implement local run option without manual pipeline creation\n    kfp.compiler.Compiler().compile(kubeflow_demo_pipeline, 'pipeline.yaml')\n"
  },
  {
    "repo": "nevrets/kubeflow-pipelines-example",
    "file_path": "01-hello-world/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/nevrets/kubeflow-pipelines-example/main/01-hello-world/pipeline.py",
    "content": "from kfp import dsl\nfrom kfp import compiler\n\n@dsl.component\ndef say_hello(name: str) -> str:\n    hello_text = f'Hello, {name}!'\n    print(hello_text)\n    return hello_text\n\n@dsl.pipeline\ndef hello_pipeline(recipient: str) -> str:\n    hello_task = say_hello(name=recipient)\n    return hello_task.output\n\n\n\nif __name__ == '__main__':\n    compiler.Compiler().compile(hello_pipeline, 'pipeline.yaml')"
  },
  {
    "repo": "nevrets/kubeflow-pipelines-example",
    "file_path": "02-Iris/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/nevrets/kubeflow-pipelines-example/main/02-Iris/pipeline.py",
    "content": "import kfp\nfrom kfp import dsl\nfrom config import CFG\n\n\n@dsl.pipeline(\n    name='nevret-iris',\n    description='nevret iris test'\n)\ndef iris_pipeline():\n    add_p = dsl.ContainerOp(\n        name=\"load iris data pipeline\",\n        image=\"nevret/nevret-iris-preprocessing:0.5\",\n        command=['python', 'load_data.py'],\n        arguments=[\n            '--data_path', './Iris.csv'\n        ],\n        file_outputs={'iris' : '/iris.csv'}\n    )\n    \n    ml = dsl.ContainerOp(\n        name=\"training pipeline\",\n        image=\"nevret/nevret-iris-training:0.5\",\n        command=['python', 'train.py'],\n        arguments=[\n            '--data', add_p.outputs['iris']\n        ]\n    )\n\n    ml.after(add_p)\n    \n    \n    \nif __name__ == \"__main__\":\n    import kfp.compiler as compiler\n    compiler.Compiler().compile(iris_pipeline, \"02-Iris/pipeline.yaml\")\n    \n    import kfp\n    import requests\n\n    USERNAME = CFG.kf_username\n    PASSWORD = CFG.kf_password\n    NAMESPACE = CFG.kf_namespace\n    HOST = CFG.kf_host\n\n    session = requests.Session()\n    response = session.get(HOST)\n\n    headers = {\n        \"Content-Type\": \"application/x-www-form-urlencoded\",\n    }\n\n    data = {\"login\": USERNAME, \"password\": PASSWORD}\n    session.post(response.url, headers=headers, data=data)\n    session_cookie = session.cookies.get_dict()[\"authservice_session\"]    \n\n    \n    # Submit the pipeline for execution\n    pipeline_func = iris_pipeline\n    client = kfp.Client(\n        host=f\"{HOST}/pipeline\",\n        namespace=f\"{NAMESPACE}\",\n        cookies=f\"authservice_session={session_cookie}\",\n    )\n    print(client.list_pipelines())\n    \n    run = client.create_run_from_pipeline_func(pipeline_func, arguments={})\n"
  },
  {
    "repo": "nevrets/kubeflow-pipelines-example",
    "file_path": "03-ML-model-project/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/nevrets/kubeflow-pipelines-example/main/03-ML-model-project/pipeline.py",
    "content": "import kfp\nfrom kfp import dsl\nfrom kfp.components import func_to_container_op\nfrom config import CFG\n@func_to_container_op\ndef show_results(decision_tree:float, \n                 logistic_regression:float, \n                 svm:float, \n                 naive_bayes:float, \n                 xgb:float) -> None:\n    \n    # Given the outputs from decision_tree, logistic regression, svm, naive_bayes, xgboost components\n    print(f\"Decision tree (accuracy): {decision_tree}\")\n    print(f\"Logistic regression (accuracy): {logistic_regression}\")\n    print(f\"SVM (SVC) (accuracy): {svm}\")\n    print(f\"Naive Bayes (Gaussian) (accuracy): {naive_bayes}\")\n    print(f\"XGBoost (accuracy): {xgb}\")\n\n\n@dsl.pipeline(name='ML Models Pipeline', description='Applies Decision Tree, Logistic Regression, SVM, Naive Bayes, XGBoost for classification problem.')\ndef ml_models_pipeline():\n    # Loads the yaml manifest for each component\n    download = kfp.components.load_component_from_file('/data/nevret/kubeflow_pipelines/03-ML-model-project/01-download_data/download_data.yaml')\n    decision_tree = kfp.components.load_component_from_file('/data/nevret/kubeflow_pipelines/03-ML-model-project/02-decision_tree/decision_tree.yaml')\n    logistic_regression = kfp.components.load_component_from_file('/data/nevret/kubeflow_pipelines/03-ML-model-project/03-logistic_regression/logistic_regression.yaml')\n    svm = kfp.components.load_component_from_file('/data/nevret/kubeflow_pipelines/03-ML-model-project/04-svm/svm.yaml')\n    naive_bayes = kfp.components.load_component_from_file('/data/nevret/kubeflow_pipelines/03-ML-model-project/05-naive_bayes/naive_bayes.yaml')\n    xgb = kfp.components.load_component_from_file('/data/nevret/kubeflow_pipelines/03-ML-model-project/06-xgb/xgb.yaml')\n    \n    # Run download_data task\n    download_task = download()\n    \n    # Run ML models tasks with input data\n    decision_tree_task = decision_tree(download_task.output)\n    logistic_regression_task = logistic_regression(download_task.output)\n    svm_task = svm(download_task.output)\n    naive_bayes_task = naive_bayes(download_task.output)\n    xgb_task = xgb(download_task.output)\n    \n    # Given the outputs from ML models tasks\n    # the component \"show_results\" is called to print the results.\n    show_results(decision_tree_task.output, logistic_regression_task.output, svm_task.output, naive_bayes_task.output, xgb_task.output)\n\n\n\nif __name__ == '__main__':\n    # ml_models_pipeline()\n    kfp.compiler.Compiler().compile(ml_models_pipeline, \"03-ML-model-project/ML-model-pipeline.yaml\")\n    \n    import requests\n\n    USERNAME = CFG.kf_username\n    PASSWORD = CFG.kf_password\n    NAMESPACE = CFG.kf_namespace\n    HOST = CFG.kf_host\n    \n    session = requests.Session()\n    response = session.get(HOST)\n\n    headers = {\n        \"Content-Type\": \"application/x-www-form-urlencoded\",\n    }\n\n    data = {\"login\": USERNAME, \"password\": PASSWORD}\n    session.post(response.url, headers=headers, data=data)\n    session_cookie = session.cookies.get_dict()[\"authservice_session\"]    \n\n    \n    # Submit the pipeline for execution\n    pipeline_func = ml_models_pipeline\n    client = kfp.Client(\n        host=f\"{HOST}/pipeline\",\n        namespace=f\"{NAMESPACE}\",\n        cookies=f\"authservice_session={session_cookie}\",\n    )\n    \n    print(client.list_pipelines())\n    \n    run = client.create_run_from_pipeline_func(pipeline_func, arguments={})\n    \n    print('')"
  },
  {
    "repo": "nevrets/kubeflow-pipelines-example",
    "file_path": "06-MLOps-example/iris-train-pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/nevrets/kubeflow-pipelines-example/main/06-MLOps-example/iris-train-pipeline.py",
    "content": "import os\nimport kfp\nfrom kfp import compiler, dsl\n\nfrom utils import add_nfs_volume, add_configmap, KubeflowClient, add_sharedmemory\nfrom config import CFG\n\n\n@kfp.dsl.pipeline(\n    name=\"IRIS Model Training Pipeline\",\n    description=\"\",\n)\ndef iris_train_pipeline(\n    lakefs_date: str = \"2025-04-08-104352/\",\n    random_state: int = 42,\n    max_iter: int = 1000,\n    multi_class: str = \"multinomial\",\n):\n    # ----- Temporary volume create ----- #\n    vop = dsl.VolumeOp(\n        name=\"Temporary volume create\",\n        storage_class=\"k8s-nfs\",\n        resource_name=\"tmp-volume\",\n        modes=dsl.VOLUME_MODE_RWM,     # ReadWriteMany\n        size=\"1Gi\",\n    ).add_pod_annotation(name=\"pipelines.kubeflow.org/max_cache_staleness\", value=\"P0D\")\n\n\n    download_datasets_op = (\n        dsl.ContainerOp(\n            name=\"Download Datasets\",\n            image=CFG.hb_iris_demo_function_image,\n            command=[\"python\", \"lakefs_download.py\"],\n            arguments=[\n                \"--root\", \"/mnt/preprocessed\",\n                \"--repo\", \"demo-iris\",\n                \"--date\", lakefs_date,\n            ],\n            pvolumes={\"/mnt\": vop.volume},\n        )\n        .add_pod_annotation(name=\"pipelines.kubeflow.org/max_cache_staleness\", value=\"P0D\")\n        .after(vop)\n    )\n\n\n    train_model_op = (\n        dsl.ContainerOp(\n            name=\"Train Model\",\n            image=CFG.hb_iris_demo_model_image,\n            command=[\"python\", \"train.py\"],\n            arguments=[\n                \"--data_path\", \"/mnt/preprocessed\", \n                \"--random_state\", random_state,\n                \"--max_iter\", max_iter,\n                \"--multi_class\", multi_class,\n            ],\n            pvolumes={\"/mnt\": vop.volume},\n        )\n        .add_pod_annotation(name=\"pipelines.kubeflow.org/max_cache_staleness\", value=\"P0D\")\n        .after(download_datasets_op)\n    )\n    \n    \n\n\nif __name__ == \"__main__\":\n    # ----- build ----- #\n    pipeconf = kfp.dsl.PipelineConf()\n\n    file_name = os.path.splitext(os.path.basename(__file__))[0]\n    compiler.Compiler().compile(iris_train_pipeline, f\"{file_name}.tar.gz\", pipeline_conf=pipeconf)\n\n    try:\n        client = KubeflowClient(\n            endpoint=CFG.kf_host,\n            username=CFG.kf_username,\n            password=CFG.kf_password,\n            namespace=CFG.kf_namespace,\n        )\n        res = client.upload_pipeline(f\"{file_name}.tar.gz\", \"iris-train-pipeline\")\n        print(\"\uc5c5\ub85c\ub4dc \uacb0\uacfc:\", res)\n    except Exception as e:\n        print(\"\uc5d0\ub7ec \ubc1c\uc0dd:\", str(e))\n        \n    print(\"\ud30c\uc774\ud504\ub77c\uc778 \ubaa9\ub85d:\", client.client.list_pipelines())"
  },
  {
    "repo": "ithingv/kubeflow_pipeline_mnist",
    "file_path": "add_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/ithingv/kubeflow_pipeline_mnist/master/add_pipeline.py",
    "content": "import kfp\nimport kfp.dsl as dsl\nfrom kfp.v2.dsl import component\n\n@component\ndef add(a: float, b: float) -> float:\n    \"\"\"Calculates sum of two arguments\"\"\"\n    return a + b\n\n@dsl.pipeline(\n    name=\"addition-pipeline\",\n    description=\"An example pipeline that performs addition calculations.\",\n    # pipeline_root='gs://my-pipeline-root/example-pipeline'\n)\ndef add_pipeline(a: float = 1, b: float = 7):\n    add_task = add(a, b)\n\nclient = kfp.Client()\nclient.create_run_from_pipeline_func(\n    add_pipeline,\n    arguments={\"a\": 7, \"b\": 8},\n    mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE,\n)"
  },
  {
    "repo": "ithingv/kubeflow_pipeline_mnist",
    "file_path": "mnist_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/ithingv/kubeflow_pipeline_mnist/master/mnist_pipeline.py",
    "content": "import kfp\nimport kfp.dsl as dsl\nfrom kfp.v2.dsl import component, Input, Output, Dataset, Model, Metrics\n\n@component(\n    packages_to_install=[\"tensorflow\", \"numpy\"]\n)\ndef load_data(output_dataset: Output[Dataset]):\n    print(\"data loading...\")\n    import tensorflow as tf\n    import numpy as np\n\n    mnist = tf.keras.datasets.mnist\n    (train_x, train_y), (test_x, test_y) = mnist.load_data()\n\n    with open(output_dataset.path, \"wb\") as f:\n        np.savez(f, train_x=train_x, train_y=train_y, test_x=test_x, test_y=test_y)\n    print(f\"Saved raw data on : {output_dataset.path}\")\n\n@component(\n    packages_to_install=[\"numpy\"]\n)\ndef preprocessing(input_dataset: Input[Dataset], output_dataset: Output[Dataset]):\n    print(\"Preprocessing...\")\n    import numpy as np\n\n    with open(input_dataset.path, \"rb\") as f:\n        mnist = np.load(f)\n        train_x, train_y = mnist[\"train_x\"], mnist[\"train_y\"]\n        test_x, test_y = mnist[\"test_x\"], mnist[\"test_y\"]\n\n    train_x = train_x / 255.0\n    test_x = test_x / 255.0\n\n    with open(output_dataset.path, \"wb\") as f:\n        np.savez(f, train_x=train_x, train_y=train_y, test_x=test_x, test_y=test_y)\n    print(f\"Saved preproceesed data on : {output_dataset.path}\")\n\n@component(\n    packages_to_install=[\"tensorflow\", \"numpy\"]\n)\ndef train(\n    dataset: Input[Dataset], output_model: Output[Model], metrics: Output[Metrics]\n):\n    print(\"training...\")\n    import tensorflow as tf\n    import numpy as np\n\n    with open(dataset.path, \"rb\") as f:\n        mnist = np.load(f)\n        train_x, train_y = mnist[\"train_x\"], mnist[\"train_y\"]\n        test_x, test_y = mnist[\"test_x\"], mnist[\"test_y\"]\n    print(f\"train x shape: {train_x.shape}\")\n    print(f\"train y shape: {train_y.shape}\")\n    print(f\"test x shape: {test_x.shape}\")\n    print(f\"test y shape: {test_y.shape}\")\n    model = tf.keras.Sequential(\n        [\n            tf.keras.layers.Flatten(input_shape=(28, 28)),\n            tf.keras.layers.Dense(128, activation=\"relu\"),\n            tf.keras.layers.Dense(10, activation=\"softmax\"),\n        ]\n    )\n    model.compile(\n        loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"acc\"]\n    )\n    model.fit(train_x, train_y)\n    loss, acc = model.evaluate(test_x, test_y)\n\n    metrics.log_metric(\"accuracy\", (acc * 100.0))\n    metrics.log_metric(\"loss\", loss)\n    metrics.log_metric(\"framework\", \"Tensorflow\")\n    metrics.log_metric(\"Model\", \"LinearModel\")\n    metrics.log_metric(\"dataset_size\", len(train_x))\n\n    model.save(output_model.path)\n\n@dsl.pipeline(\n    name=\"mnist-pipeline\", description=\"An example pipeline that mnist training.\"\n)\ndef mnist_pipeline():\n    load_data_task = load_data()\n    print(\"outputs: \", load_data_task.output)\n    preprocessing_task = preprocessing(load_data_task.outputs[\"output_dataset\"])\n    train_task = train(preprocessing_task.outputs[\"output_dataset\"])\n\nclient = kfp.Client()\nclient.create_run_from_pipeline_func(\n    mnist_pipeline, arguments={}, mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE,\n)"
  },
  {
    "repo": "raldebsi/Kubeflow-Experimental-Pipelines",
    "file_path": "src/pipelines/add_randoms.py",
    "raw_url": "https://raw.githubusercontent.com/raldebsi/Kubeflow-Experimental-Pipelines/master/src/pipelines/add_randoms.py",
    "content": "from collections import namedtuple\nfrom typing import NamedTuple\nimport kfp\nimport kfp.components as comp\nimport kfp.compiler\n\n\ndef add_random(num : int)-> NamedTuple(\n\t'Outputs',\n\t[\n\t\t('num', int),\n\t]):\n\tfrom collections import namedtuple\n\n\texample_output = namedtuple(\n\t\t'Outputs',\n\t\t['num'])\n\treturn example_output(num + 3)\n\treturn num + 3\n\ndef add_number(num : int)-> NamedTuple(\n\t'Outputs',\n\t[\n\t\t('num', int),\n\t]):\n\tfrom collections import namedtuple\n\texample_output = namedtuple(\n\t\t'Outputs',\n\t\t['num'])\n\treturn example_output(num + 5)\n\ndef mult_int(a : int, b : int)-> NamedTuple(\n\t'Outputs',\n\t[\n\t\t('num', int),\n\t]):\n\tfrom collections import namedtuple\n\texample_output = namedtuple(\n\t\t'Outputs',\n\t\t['num'])\n\treturn example_output(a * b)\n\n@kfp.dsl.pipeline(\n\tname='Add Random ',\n\tdescription='Add Random'\n)\ndef main_pipeline(a : int, b : int):\n\tadd_random_comp = comp.create_component_from_func(\n\t\tadd_random\n\t)\n \n\tadd_random_task1 = add_random_comp(a)\n\tadd_random_task2 = add_random_comp(b)\n\t\n\tmult_int_comp = comp.create_component_from_func(\n\t\tmult_int\n\t)\n\t# print(add_random_task1.outputs)\n\tmult_int_task = mult_int_comp(add_random_task1.outputs['num'], add_random_task2.outputs['num'])\n\tprint(mult_int_task.outputs['num'])\n \nif __name__ == \"__main__\":\n\tkfp.compiler.Compiler().compile(main_pipeline, 'random_pipeline.yaml')\n"
  },
  {
    "repo": "raldebsi/Kubeflow-Experimental-Pipelines",
    "file_path": "src/pipelines/common_utils.py",
    "raw_url": "https://raw.githubusercontent.com/raldebsi/Kubeflow-Experimental-Pipelines/master/src/pipelines/common_utils.py",
    "content": "import uuid\r\n\r\nimport yaml\r\nimport kfp\r\nfrom kfp import dsl\r\nfrom kfp.dsl._pipeline_param import sanitize_k8s_name\r\nfrom kfp.dsl._pipeline_volume import PipelineVolume\r\n\r\ndef generate_random_hex_service(randomize_service_suffix=True):\r\n    def generate_random(randomize_service_suffix: bool) -> str:\r\n        if not randomize_service_suffix: # No need to generate random\r\n            return \"\"\r\n        \r\n        from uuid import uuid4\r\n        return uuid4().hex[:7]\r\n    \r\n    return cacheless_task(kfp.components.func_to_container_op(generate_random)(randomize_service_suffix)).output\r\n\r\ndef sanitize_service(string: str, randomize_service_suffix):\r\n    def sanitize_service_name(name: str, extra: str) -> str:\r\n        print(\"Sanitizing\", name)\r\n        if extra:\r\n            name += '-' + extra[:5]\r\n        s = name.lower().replace(\" \", \"_\").replace(\".\", \"-\").replace(\"_\", \"-\")\r\n        print(\"Sanitized\", s)\r\n        return s\r\n    random_string = generate_random_hex_service(randomize_service_suffix)\r\n    return kfp.components.func_to_container_op(sanitize_service_name)(string, random_string).output\r\n\r\ndef spec_from_file_format(yaml_file, **kwargs):\r\n    with open(yaml_file, 'r') as f:\r\n        component_spec = f.read()\r\n        for k, v in kwargs.items():\r\n            component_spec = component_spec.replace('{' + k + '}', str(v))\r\n        return yaml.safe_load(component_spec)\r\n\r\ndef get_volume_by_name(name, unique_name = \"\") -> PipelineVolume:\r\n    # Get volume    \r\n    name = str(name)\r\n    if not unique_name:\r\n        is_pipeline_name = name.startswith('{{') and name.endswith('}}')\r\n        volume_name = sanitize_k8s_name(name) if not is_pipeline_name else name\r\n        unique_volume_name = \"%s-%s\" % (volume_name, uuid.uuid4().hex[:7])\r\n    else:\r\n        unique_volume_name = unique_name\r\n    mount_vol = PipelineVolume(\r\n        name=unique_volume_name, # Parameter name, if found then it will be reused. Therefore it should be unique.\r\n        pvc=name,\r\n        volume=None, # type: ignore \r\n    )\r\n\r\n    return mount_vol\r\n\r\ndef get_or_create_pvc(name: str, size_: str, resource: str, randomize: bool = False, mode=dsl.VOLUME_MODE_RWO):\r\n    if not resource and not randomize:\r\n        raise ValueError(\"Either resource or randomize should be provided\")\r\n    return dsl.VolumeOp(\r\n        name=name,  # Operation Unique Name\r\n                    # If operation exists then it will be reused.\r\n                    # If operation does not exist then it will be created and the name will be resource_name below.\r\n        size=size_,\r\n        modes=mode,\r\n        resource_name=resource, # PVC Name, will be {pipeline_name}-{id}-resource if name does not exist and generate_unique_name is True.\r\n        # volume_name=\"ridhwan-personal-volume\", # Volume Name, do not use as it will cause the volume to not be found\r\n        # data_source=PipelineParam(name=\"data_source\"), # This way it will create a volume snapshot from param\r\n        generate_unique_name=randomize,\r\n    )\r\n\r\ndef add_pvolumes_func(pvolumes):\r\n    # kfp.dsl._container_op.ContainerOp\r\n    return lambda func: func.add_pvolumes(pvolumes)\r\n\r\ndef number_to_base_26(number):\r\n    \"\"\"\r\n        Converts a number to base 26 then to alphabet, 0 -> a, 25 -> z, 26 -> aa\r\n    \"\"\"\r\n    if number < 26:\r\n        return chr(ord('a') + number)\r\n    else:\r\n        return number_to_base_26(number // 26 - 1) + chr(ord('a') + number % 26)\r\n\r\ndef setup_volume(volume_name, *mount_points):\r\n    if len(mount_points) == 1:\r\n        mount_vol = get_volume_by_name(volume_name, \"volume-bind\")\r\n        return add_pvolumes_func({mount_points[0]: mount_vol})\r\n    else:\r\n        mount_dict = {}\r\n        for i, mount_point in enumerate(mount_points):\r\n            mount_vol = get_volume_by_name(volume_name, \"volume-bind-%s\" % number_to_base_26(i))\r\n            mount_dict[mount_point] = mount_vol\r\n\r\n    return add_pvolumes_func(mount_dict)\r\n\r\ndef cacheless_task(task):\r\n    task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\r\n    return task # To allow chaining"
  },
  {
    "repo": "raldebsi/Kubeflow-Experimental-Pipelines",
    "file_path": "src/pipelines/kube_exp_ridhwan.py",
    "raw_url": "https://raw.githubusercontent.com/raldebsi/Kubeflow-Experimental-Pipelines/master/src/pipelines/kube_exp_ridhwan.py",
    "content": "from typing import NamedTuple, List\nimport kfp\nimport kfp.compiler\nimport kfp.components\n\ndef wget_data(url: str, output_path: str) -> None:\n    \"\"\"\n    Download data from a URL.\n    \"\"\"\n    import wget\n    import os\n    \n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n    print(\"Downloading {} to {}\".format(url, output_path))\n    data = wget.download(url, output_path)\n    if not data:\n        raise ValueError(\"Failed to download data from {}\".format(url))\n    \n    return\n\ndef validate_data(data_paths: List[str], data_root: str = \"/data\"):\n    \"\"\"\n    Validate data.\n    \"\"\"\n    import os\n\n    for data_path in data_paths:\n        if not data_path.startswith(data_root):\n            data_path = os.path.join(data_root, data_path)\n        if not os.path.exists(data_path):\n            raise ValueError(\"Data path {} does not exist\".format(data_path))\n        if not os.path.isfile(data_path):\n            raise ValueError(\"Data path {} is not a file\".format(data_path))\n        print(\"Data {} is valid\".format(data_path))\n\n    return\n\n\n@kfp.dsl.pipeline(\n    name=\"Kube Lablelizer Experiment\",\n    description=\"Experiment using KubeFlow to run a ML Train & Test pipeline\",\n)\ndef kube_exp_ridhwan(train_source: str, valid_source: str, test_data: kfp.components.InputPath(\"txt\")):\n    from kfp import dsl\n\n    print(\"Starting Pipeline Generation\")\n    print(\"Source Params:\")\n    print(\"Train Source: {}\".format(train_source))\n    print(\"Valid Source: {}\".format(valid_source))\n    print(\"Test data: {}\".format(test_data))\n\n    mount_vol = dsl.VolumeOp(\n        name=\"Mount Volume\",\n        resource_name=\"kube-mount-ridhwan\",\n        size=\"4Gi\",\n        modes=dsl.VOLUME_MODE_RWO,\n    )\n\n    data_store_path = \"/data\"\n    pvolumes = {data_store_path: mount_vol.volume}\n\n    # print(\"Volume:\", mount_vol)\n    # print(\"Volume Mount:\", pvolumes)\n\n    volumetrize = lambda func: func.add_pvolumes(pvolumes)\n\n    print(\"Generating Components\")\n    retreive_data_component = kfp.components.create_component_from_func(wget_data, base_image=\"python:slim-buster\", packages_to_install=[\"wget\"])\n    get_train_task = volumetrize(retreive_data_component(train_source, \"{root}/train.json\".format(root=data_store_path)))\n    get_valid_task = volumetrize(retreive_data_component(valid_source, \"{root}/dev.json\".format(root=data_store_path)))\n\n    validate_data_component = kfp.components.create_component_from_func(validate_data, base_image=\"python:slim-buster\")\n    validation_task = validate_data_component([\"/data/train.json\", \"/data/dev.json\"], \"/data\")\n    validation_task = volumetrize(validation_task)\n    validation_task.after(get_train_task, get_valid_task)\n\n\n    print(\"Pipeline Created\")\n\nif __name__ == \"__main__\":\n    kfp.compiler.Compiler().compile(kube_exp_ridhwan, __file__.rsplit(\".\", 1)[0] + \".yaml\")"
  },
  {
    "repo": "raldebsi/Kubeflow-Experimental-Pipelines",
    "file_path": "src/pipelines/test_upload.py",
    "raw_url": "https://raw.githubusercontent.com/raldebsi/Kubeflow-Experimental-Pipelines/master/src/pipelines/test_upload.py",
    "content": "from typing import NamedTuple\nimport kfp\nimport kfp.compiler\nimport kfp.components\n\ndef receive_and_print_data(bin_input: kfp.components.InputPath(\"DataInput\")) -> NamedTuple(\n    'Outputs', [\n        (\"my_out_data\", kfp.components.OutputBinaryFile) # ?\n    ]\n):\n    print(\"Bin_Input: \", bin_input)\n\n    with open(bin_input, encoding='utf-8') as f:\n        print(\"Content:\", f.read())\n    return \"This is my output\"\n\ndef receive_and_print_any(input):\n    print(\"input\", input)\n    import os\n    if os.path.exists(input):\n        print(\"Is File\")\n        with open(input, encoding='utf-8') as f:\n            print(\"Data:\", f.read())\n\n@kfp.dsl.pipeline(\n    \"Test File Upload\",\n    \"Testing uploading files from ui\",\n)\ndef pipeline(test_data_2: kfp.components.InputPath(\"DataInput\")):\n    print(test_data_2)\n\n    comp = kfp.components.create_component_from_func(\n        receive_and_print_data,\n    )\n    comp2 = kfp.components.create_component_from_func(receive_and_print_any)\n\n    task = comp(test_data_2)\n    task2 = comp2(task.outputs[\"my_out_data\"])\n"
  },
  {
    "repo": "raldebsi/Kubeflow-Experimental-Pipelines",
    "file_path": "src/pipelines/end_to_end/just_write.py",
    "raw_url": "https://raw.githubusercontent.com/raldebsi/Kubeflow-Experimental-Pipelines/master/src/pipelines/end_to_end/just_write.py",
    "content": "import kfp\r\nfrom src.pipelines.common_utils import get_volume_by_name, add_pvolumes_func \r\n\r\ndef write_me(to_write: str):\r\n    with open(\"/store/my_file.txt\", \"w\") as f:\r\n        f.write(to_write)\r\n\r\n\r\n@kfp.dsl.pipeline(\r\n    \"Just Write\"\r\n)\r\ndef pipeline(volume_name: str, to_write: str):\r\n    mount_vol = get_volume_by_name(volume_name, \"my-bind-volume\")\r\n    mount_dict = {\"/store\": mount_vol}\r\n    volumetrize = add_pvolumes_func(mount_dict)\r\n\r\n    write_me_op = kfp.components.func_to_container_op(write_me)\r\n    write_me_task = volumetrize(write_me_op(to_write))"
  },
  {
    "repo": "raldebsi/Kubeflow-Experimental-Pipelines",
    "file_path": "src/pipelines/end_to_end/mnist_experiment.py",
    "raw_url": "https://raw.githubusercontent.com/raldebsi/Kubeflow-Experimental-Pipelines/master/src/pipelines/end_to_end/mnist_experiment.py",
    "content": "import kfp\r\nfrom kfp import components, dsl\r\nfrom kfp.components import func_to_container_op\r\nfrom kubeflow.katib import (ApiClient, V1beta1AlgorithmSpec,\r\n                            V1beta1ExperimentSpec, V1beta1FeasibleSpace,\r\n                            V1beta1ObjectiveSpec, V1beta1ParameterSpec,\r\n                            V1beta1TrialParameterSpec, V1beta1TrialTemplate)\r\nimport yaml\r\n\r\nfrom src.pipelines.common_utils import get_or_create_pvc, spec_from_file_format\r\nimport json\r\n\r\ndef katib_experiment_factory(experiment, namespace, steps):\r\n    max_trial_count = 5\r\n    max_failed_trial_count = 3\r\n    parallel_trial_count = 2\r\n\r\n    objective = V1beta1ObjectiveSpec( # Objective is to minimize the loss\r\n        type=\"minimize\",\r\n        goal=0.001,\r\n        objective_metric_name=\"loss\"\r\n    )\r\n\r\n    algorithm = V1beta1AlgorithmSpec(\r\n        algorithm_name=\"random\"\r\n    )\r\n\r\n    param_learning_rate = V1beta1ParameterSpec(\r\n            name=\"learning_rate\",\r\n            parameter_type=\"double\",\r\n            feasible_space=V1beta1FeasibleSpace(\r\n                min=0.01,\r\n                max=0.05,\r\n        )\r\n    )\r\n    param_learning_rate_spec = V1beta1TrialParameterSpec(\r\n        name=\"learningRate\",\r\n        description=\"Learning rate for the training model\",\r\n        reference=\"learning_rate\",\r\n    )\r\n\r\n    param_batch_size = V1beta1ParameterSpec(\r\n        name=\"batch_size\",\r\n        parameter_type=\"int\",\r\n        feasible_space=V1beta1FeasibleSpace(\r\n            min=\"80\",\r\n            max=\"100\",\r\n        )        \r\n    )\r\n    param_batch_size_spec = V1beta1TrialParameterSpec(\r\n        name=\"batchSize\",\r\n        description=\"Batch size for the training model\",\r\n        reference=\"batch_size\",\r\n    )\r\n\r\n    params = [\r\n        param_learning_rate,\r\n        param_batch_size        \r\n    ]\r\n\r\n    trial_spec = spec_from_file_format(\"src/pipelines/yamls/Specs/TFJob.yaml\", trainStepsParamVal=steps)\r\n    trial_template = V1beta1TrialTemplate(\r\n        primary_container_name=\"tensorflow\",\r\n        trial_parameters=[\r\n            param_learning_rate_spec,\r\n            param_batch_size_spec\r\n        ],\r\n        trial_spec=trial_spec,\r\n    )\r\n\r\n    experiment_spec = V1beta1ExperimentSpec(\r\n        max_trial_count=max_trial_count,\r\n        max_failed_trial_count=max_failed_trial_count,\r\n        parallel_trial_count=parallel_trial_count,\r\n        objective=objective,\r\n        algorithm=algorithm,\r\n        parameters=params,\r\n        trial_template=trial_template,\r\n    )\r\n\r\n    katib_op = components.load_component_from_file(\"src/pipelines/yamls/Components/katib_launcher.yaml\")\r\n    katib_task = katib_op(\r\n        experiment_name=experiment,\r\n        experiment_namespace=namespace,\r\n        experiment_spec=ApiClient().sanitize_for_serialization(experiment_spec),\r\n        experiment_timeout_minutes=60,\r\n        delete_finished_experiment=False\r\n    )\r\n\r\n    return katib_task\r\n\r\ndef convert_hyperparams(hyperparams) -> str:\r\n    import json\r\n    results = json.loads(hyperparams)\r\n    print(\"Hyperparams FineTuned: \", results)\r\n    best_params = []\r\n    for param in results[\"currentOptimalTrial\"][\"parameterAssignments\"]:\r\n        if param[\"name\"] == \"learning_rate\":\r\n            best_params.append(\"--tf-learning-rate={}\".format(param[\"value\"]))\r\n        elif param[\"name\"] == \"batch_size\":\r\n            best_params.append(\"--tf-batch-size={}\".format(param[\"value\"]))\r\n    print(\"Best Params\", best_params)\r\n    return \" \".join(best_params)\r\n\r\ndef create_tfjob_task(job_name, job_namespace, steps, hyperparams, mount_name):\r\n    tfjob_chief_spec = spec_from_file_format(\"src/pipelines/yamls/Specs/TFJobChief.yaml\",\r\n        trainingStepsParamVal=steps,\r\n        bestHPsParamVal=hyperparams,\r\n        volumeResourceName=mount_name\r\n    )\r\n    tfjob_worker_spec = spec_from_file_format(\"src/pipelines/yamls/Specs/TFJobWorker.yaml\",\r\n        trainingStepsParamVal=steps,\r\n        bestHPsParamVal=hyperparams,\r\n    )\r\n\r\n    tfjob_op = components.load_component_from_file(\"src/pipelines/yamls/Components/tfjob_launcher.yaml\")\r\n\r\n    tfjob_task = tfjob_op(\r\n        name=job_name,\r\n        namespace=job_namespace,\r\n        chief_spec=json.dumps(tfjob_chief_spec),\r\n        worker_spec=json.dumps(tfjob_worker_spec),\r\n        tfjob_timeout_minutes=60,\r\n        delete_finished_tfjob=False\r\n    )\r\n\r\n    return tfjob_task\r\n\r\ndef create_serve_task(model_name, model_namespace, mount_name):\r\n    infer_service = spec_from_file_format(\r\n        \"src/pipelines/yamls/Specs/KFServe.yaml\",\r\n        apiVersion=\"serving.kubeflow.org/v1beta1\",\r\n        modelName=model_name,\r\n        modelNamespace=model_namespace,\r\n        volumeResourceName=mount_name,\r\n    )\r\n\r\n    serve_op = components.load_component_from_file(\"src/pipelines/yamls/Components/kserve_launcher.yaml\")\r\n    serve_task = serve_op(\r\n        action=\"apply\",\r\n        inferenceservice_yaml=yaml.dump(infer_service),\r\n    )\r\n    \r\n    return serve_task\r\n\r\n\r\n@dsl.pipeline(\r\n    name=\"MNIST E2E Test\",\r\n    description=\"Testin MNIST end to end pipeline\"\r\n)\r\ndef pipeline(name=\"mnist-e2e-test-ridhwan\", training_steps=\"200\"):\r\n    name = \"{{workflow.name}}-%s\" % name\r\n    namespace=\"{{workflow.namespace}}\"\r\n    katib_task = katib_experiment_factory(name, namespace, training_steps)\r\n\r\n    volume_pvc = get_or_create_pvc(\"Create Ridhwan Volumes Simple\", \"4Gi\", \"ridhwan-pvc-mount-four\")\r\n    volume_name = volume_pvc.outputs[\"name\"]\r\n\r\n    # Convert Params\r\n    convert_params_op = func_to_container_op(convert_hyperparams)\r\n    convert_params_task = convert_params_op(katib_task.output)\r\n\r\n    # TFJob\r\n    tfjob_task = create_tfjob_task(name, namespace, training_steps, convert_params_task.output, volume_name)\r\n\r\n\r\n    # Serve\r\n    serve_task = create_serve_task(name, namespace, volume_name).after(tfjob_task)\r\n"
  },
  {
    "repo": "raldebsi/Kubeflow-Experimental-Pipelines",
    "file_path": "src/pipelines/end_to_end/mnist_simple.py",
    "raw_url": "https://raw.githubusercontent.com/raldebsi/Kubeflow-Experimental-Pipelines/master/src/pipelines/end_to_end/mnist_simple.py",
    "content": "from kserve import KServeClient\r\nimport kfp\r\nfrom kfp import components, dsl\r\nfrom kfp.components import func_to_container_op\r\nfrom kubeflow.katib import (ApiClient, V1beta1AlgorithmSpec,\r\n                            V1beta1ExperimentSpec, V1beta1FeasibleSpace,\r\n                            V1beta1ObjectiveSpec, V1beta1ParameterSpec,\r\n                            V1beta1TrialParameterSpec, V1beta1TrialTemplate)\r\nimport yaml\r\n\r\nfrom src.pipelines.common_utils import get_or_create_pvc, spec_from_file_format\r\nimport json\r\n\r\ndef katib_experiment_factory(experiment, namespace, steps):\r\n    max_trial_count = 5\r\n    max_failed_trial_count = 3\r\n    parallel_trial_count = 2\r\n\r\n    objective = V1beta1ObjectiveSpec( # Objective is to minimize the loss\r\n        type=\"minimize\",\r\n        goal=0.001,\r\n        objective_metric_name=\"loss\"\r\n    )\r\n\r\n    algorithm = V1beta1AlgorithmSpec(\r\n        algorithm_name=\"random\"\r\n    )\r\n\r\n    param_learning_rate = V1beta1ParameterSpec(\r\n            name=\"learning_rate\",\r\n            parameter_type=\"double\",\r\n            feasible_space=V1beta1FeasibleSpace(\r\n                min=0.01,\r\n                max=0.05,\r\n        )\r\n    )\r\n    param_learning_rate_spec = V1beta1TrialParameterSpec(\r\n        name=\"learningRate\",\r\n        description=\"Learning rate for the training model\",\r\n        reference=\"learning_rate\",\r\n    )\r\n\r\n    param_batch_size = V1beta1ParameterSpec(\r\n        name=\"batch_size\",\r\n        parameter_type=\"int\",\r\n        feasible_space=V1beta1FeasibleSpace(\r\n            min=\"80\",\r\n            max=\"100\",\r\n        )        \r\n    )\r\n    param_batch_size_spec = V1beta1TrialParameterSpec(\r\n        name=\"batchSize\",\r\n        description=\"Batch size for the training model\",\r\n        reference=\"batch_size\",\r\n    )\r\n\r\n    params = [\r\n        param_learning_rate,\r\n        param_batch_size        \r\n    ]\r\n\r\n    trial_spec = spec_from_file_format(\"src/pipelines/yamls/Specs/TFJob.yaml\", trainStepsParamVal=steps)\r\n    trial_template = V1beta1TrialTemplate(\r\n        primary_container_name=\"tensorflow\",\r\n        trial_parameters=[\r\n            param_learning_rate_spec,\r\n            param_batch_size_spec\r\n        ],\r\n        trial_spec=trial_spec,\r\n    )\r\n\r\n    experiment_spec = V1beta1ExperimentSpec(\r\n        max_trial_count=max_trial_count,\r\n        max_failed_trial_count=max_failed_trial_count,\r\n        parallel_trial_count=parallel_trial_count,\r\n        objective=objective,\r\n        algorithm=algorithm,\r\n        parameters=params,\r\n        trial_template=trial_template,\r\n    )\r\n\r\n    katib_op = components.load_component_from_file(\"src/pipelines/yamls/Components/katib_launcher.yaml\")\r\n    katib_task = katib_op(\r\n        experiment_name=experiment,\r\n        experiment_namespace=namespace,\r\n        experiment_spec=ApiClient().sanitize_for_serialization(experiment_spec),\r\n        experiment_timeout_minutes=60,\r\n        delete_finished_experiment=False\r\n    )\r\n\r\n    return katib_task\r\n\r\ndef convert_hyperparams(hyperparams) -> str:\r\n    import json\r\n    results = json.loads(hyperparams)\r\n    print(\"Hyperparams FineTuned: \", results)\r\n    best_params = []\r\n    for param in results[\"currentOptimalTrial\"][\"parameterAssignments\"]:\r\n        if param[\"name\"] == \"learning_rate\":\r\n            best_params.append(\"--tf-learning-rate={}\".format(param[\"value\"]))\r\n        elif param[\"name\"] == \"batch_size\":\r\n            best_params.append(\"--tf-batch-size={}\".format(param[\"value\"]))\r\n    print(\"Best Params\", best_params)\r\n    return \" \".join(best_params)\r\n\r\ndef create_tfjob_task(job_name, job_namespace, steps, mount_name, hyperparams):\r\n    tfjob_chief_spec = spec_from_file_format(\"src/pipelines/yamls/Specs/TFJobChief.yaml\",\r\n        trainingStepsParamVal=steps,\r\n        bestHPsParamVal=hyperparams,\r\n        volumeResourceName=mount_name\r\n    )\r\n    tfjob_worker_spec = spec_from_file_format(\"src/pipelines/yamls/Specs/TFJobWorker.yaml\",\r\n        trainingStepsParamVal=steps,\r\n        bestHPsParamVal=hyperparams,\r\n    )\r\n\r\n    tfjob_op = components.load_component_from_file(\"src/pipelines/yamls/Components/tfjob_launcher.yaml\")\r\n\r\n    tfjob_task = tfjob_op(\r\n        name=job_name,\r\n        namespace=job_namespace,\r\n        chief_spec=json.dumps(tfjob_chief_spec),\r\n        worker_spec=json.dumps(tfjob_worker_spec),\r\n        tfjob_timeout_minutes=60,\r\n        delete_finished_tfjob=False\r\n    )\r\n\r\n    return tfjob_task\r\n\r\ndef create_serve_task(model_name, model_namespace, mount_name):\r\n    infer_service = spec_from_file_format(\r\n        \"src/pipelines/yamls/Specs/KFServe.yaml\",\r\n        apiVersion=\"kserve3/beta4\",\r\n        modelName=model_name,\r\n        modelNamespace=model_namespace,\r\n        volumeResourceName=mount_name,\r\n    )\r\n\r\n    serve_op = components.load_component_from_file(\"src/pipelines/yamls/Components/kserve_launcher.yaml\")\r\n    serve_task = serve_op(\r\n        action=\"apply\",\r\n        inferenceservice_yaml=yaml.dump(infer_service),\r\n    )\r\n    \r\n    return serve_task\r\n\r\n\r\n@dsl.pipeline(\r\n    name=\"MNIST E2E Test\",\r\n    description=\"Testin MNIST end to end pipeline\"\r\n)\r\ndef pipeline(name=\"mnist-e2e-test-ridhwan\", training_steps=\"2\"):\r\n    name = \"{{workflow.name}}-%s\" % name\r\n    namespace=\"{{workflow.namespace}}\"\r\n\r\n    volume_pvc = get_or_create_pvc(\"Create Ridhwan Volumes Simple\", \"4Gi\", \"ridhwan-pvc-mount-four\")\r\n    volume_name = volume_pvc.outputs[\"name\"]\r\n\r\n    # Convert Params\r\n    # convert_params_op = func_to_container_op(convert_hyperparams)\r\n    # convert_params_task = convert_params_op()\r\n\r\n    # TFJob\r\n    tfjob_task = create_tfjob_task(name, namespace, training_steps, volume_name, \"--tf-learning-rate=0.03369294498160041 --tf-batch-size=93\")\r\n\r\n\r\n    # Serve\r\n    serve_task = create_serve_task(name, namespace, volume_name).after(tfjob_task)\r\n"
  },
  {
    "repo": "raldebsi/Kubeflow-Experimental-Pipelines",
    "file_path": "src/pipelines/end_to_end/serve_mnist_premade.py",
    "raw_url": "https://raw.githubusercontent.com/raldebsi/Kubeflow-Experimental-Pipelines/master/src/pipelines/end_to_end/serve_mnist_premade.py",
    "content": "import kfp\nimport yaml\nfrom kfp import components\nfrom src.pipelines.common_utils import cacheless_task, spec_from_file_format\n\n\ndef create_serve_task():\n   \n    infer_service = spec_from_file_format(\n        \"src/pipelines/yamls/Specs/KServe_MNIST.yaml\",\n        modelNamespace=\"{{workflow.namespace}}\",\n    )\n\n    serve_op = components.load_component_from_file(\"src/pipelines/yamls/Components/kserve_launcher.yaml\")\n    serve_task = serve_op(\n        action=\"apply\",\n        inferenceservice_yaml=yaml.dump(infer_service), \n    ) \n    \n    return serve_task\n\n@kfp.dsl.pipeline(\n    name=\"MNIST Example Server\",\n    description=\"Serves the public MNIST torchserve example\"\n)\ndef pipeline(): \n    serve_task = create_serve_task()\n    cacheless_task(serve_task)\n"
  },
  {
    "repo": "raldebsi/Kubeflow-Experimental-Pipelines",
    "file_path": "src/pipelines/end_to_end/serve_pt.py",
    "raw_url": "https://raw.githubusercontent.com/raldebsi/Kubeflow-Experimental-Pipelines/master/src/pipelines/end_to_end/serve_pt.py",
    "content": "from typing import NamedTuple\n\nimport kfp\nimport yaml\nfrom kfp import components\nfrom kfp.components import func_to_container_op\nfrom src.pipelines.common_utils import (add_pvolumes_func, cacheless_task, generate_random_hex_service, get_volume_by_name, sanitize_service, setup_volume,\n                                        spec_from_file_format)\n\n\ndef create_serve_task(dataset_name: str, experiment_name: str, mount_name: str, randomize_service_suffix: bool, use_seed: bool):\n    # gen_name_comp = func_to_container_op(generate_inference_service_name)\n    # gen_name_task = gen_name_comp(dataset_name, experiment_name)\n\n    sane_service_name = sanitize_service(experiment_name, randomize_service_suffix)\n    randomSeed = generate_random_hex_service(use_seed)\n    \n    infer_service = spec_from_file_format(\n        \"src/pipelines/yamls/Specs/KServe_HF.yaml\",\n        modelNamespace=\"{{workflow.namespace}}\",\n        serviceName=sane_service_name,\n        volumeResourceName=mount_name,\n        experimentName=experiment_name,\n        datasetName=dataset_name,\n        randomSeed=randomSeed, # Prevent reuse of revision\n    )\n\n    serve_op = components.load_component_from_file(\"src/pipelines/yamls/Components/kserve_launcher.yaml\")\n    serve_task = serve_op(\n        action=\"apply\", \n        inferenceservice_yaml=yaml.dump(infer_service),\n        # model_name=sane_service_name,\n        # model_uri=\"pvc://{}/{}\".format(mount_name, experiment_name),\n        # framework=\"pytorch\",\n        # namespace=\"{{workflow.namespace}}\",\n        # enable_istio_sidecar=False,\n    )\n    \n    return serve_task\n\n\ndef generate_inference_service_name(dataset_name, experiment_name) -> str:\n    return \"{}-{}\".format(dataset_name, experiment_name).lower().replace('_', '-')\n \nclass OHandler(NamedTuple):\n    temp_path: str\n    handler_path: str\n    requirements_path: str\n\ndef create_handler(handler_code: str, experiment_name: str, root_path: str, additional_requirements: list) -> OHandler:\n    import os\n\n    temp_path = os.path.join(root_path, experiment_name, \"handler\")\n    os.makedirs(temp_path, exist_ok=True)\n    handler = os.path.join(temp_path, \"handler.py\")\n    requirements = os.path.join(temp_path, \"requirements.txt\")\n\n    print(\"Temp Path\", temp_path)\n    print(\"Handler\", handler)\n    print(\"Requirements\", requirements)\n\n    with open(handler, \"w\", encoding='utf-8') as f:\n        f.write(handler_code)\n    \n    with open(requirements, \"w\", encoding='utf-8') as f:\n        for r in additional_requirements:\n            f.write(\"{}\\n\".format(r))\n\n    return temp_path, handler, requirements # type: ignore\n\ndef create_mar_config(experiment_name: str, root_path: str, mar_file: str, model_name: str = \"\", model_version: str = \"1.0\", # Pipeline Config\n        threads_count: int = 4, job_queue_size: int = 10, install_dependencies: bool = True, is_default: bool = True, # Config Config\n        workers_count: int = 1, workers_max: int = 5, batch_size: int = 1, timeout: int = 120, # Model Config\n) -> None:\n    import os\n    import json\n\n    mar_file = mar_file if mar_file.endswith(\".mar\") else mar_file + \".mar\"\n    model_name = model_name or mar_file.rsplit(\".\", 1)[0] # If Empty replace with mar file\n    model_name = model_name.replace(\"-\", \"\").lower()\n\n    experiment_path = os.path.join(root_path, experiment_name)\n    print(\"Experiment Path is\", experiment_path)\n    model_store = os.path.join(experiment_path, \"model-store\")\n    config_dir = os.path.join(experiment_path, \"config\")\n    config_path = os.path.join(config_dir, \"config.properties\")\n\n    previous_models = {}\n\n    os.makedirs(config_dir, exist_ok=True)\n\n    if os.path.isfile(config_path):\n        with open(config_path, \"r\", encoding='utf-8') as f:\n            for line in f:\n                line = line.strip()\n                if line[0] == \"#\":\n                    continue\n                key, val = line.split(\"=\")\n                if key == \"model_snapshot\":\n                    previous_models = json.loads(val.strip())\n    if previous_models:\n        print(\"Found Previous Snapshot\")\n        print(previous_models)\n    # mar_files = [ file.rsplit(\".\", 1)[0]\n    #     for file in os.listdir(model_store)\n    #     if file.lower().endswith(\".mar\")\n    # ]\n\n    if model_version in previous_models.get(\"models\", {}).get(model_name, {}):\n        replace_mode = True\n    else:\n        replace_mode = False\n\n    model_snapshot = {\n        \"name\": \"startup.cfg\",\n        \"modelCount\": previous_models.get(\"modelCount\", 0) + (1 if not replace_mode else 0),\n        \"models\": previous_models.get(\"models\", {}) \n    }\n\n    model_dict = model_snapshot[\"models\"].setdefault(model_name, {})\n    if is_default:\n        print(\"Setting Default Model to\", model_version)\n        for model_version_dict in model_snapshot[\"models\"][model_name].values():\n            model_version_dict[\"defaultVersion\"] = False # Reset all to false since I am the new default\n    else:\n        print(\"Default model unchanged\")\n    \n    model_dict[model_version] = {\n        \"defaultVersion\": is_default,\n        \"marName\": mar_file,\n        \"minWorkers\": workers_count,\n        \"maxWorkers\": workers_max,\n        \"batchSize\": batch_size,\n        \"maxBatchDelay\": 5000,\n        \"responseTimeout\": timeout,\n    }\n\n    config_spec = {\n        \"inference_address\": \"http://0.0.0.0:8085\",\n        \"management_address\": \"http://0.0.0.0:8085\",\n        \"metrics_address\": \"http://0.0.0.0:8082\",\n        \"enable_metrics_api\": True,\n        \"metrics_format\": \"prometheus\",\n        \"number_of_netty_threads\": threads_count,\n        \"job_queue_size\": job_queue_size,\n        \"model_store\": \"/mnt/models/model-store\",\n        \"model_snapshot\": json.dumps(model_snapshot),\n        \"install_py_dep_per_model\": install_dependencies,\n    }\n\n    print(\"Saving config to\", config_path)\n    with open(config_path, \"w\", encoding='utf-8') as f:\n        for key, val in config_spec.items():\n            if isinstance(val, (set, tuple)):\n                val = list(val)\n            if isinstance(val, (bool, dict, list)):\n                val = json.dumps(val)\n            s = \"{}={}\".format(key, val)\n            print(s)\n            f.write(s + \"\\n\")\n        f.write(\"# AutoGenerated by KFP.\" \"{{workflow.name}}\")\n    \n\n\n\ndef move_mar_model(model_name: str, experiment_name: str, root_path: str, temp_path: str) -> None: # Can be replaced with text component\n    import os\n    import shutil\n\n    mar_name = \"{}.mar\".format(model_name)\n    model_store_path = os.path.join(root_path, experiment_name, \"model-store\")\n    mar_model = os.path.join(temp_path, mar_name)\n    \n    if os.path.exists(mar_model):\n        os.makedirs(model_store_path, exist_ok=True)\n        move_path = os.path.join(root_path, model_store_path)\n        file_path = os.path.join(move_path, mar_name)\n        if os.path.exists(file_path):\n            os.remove(file_path)\n        shutil.move(mar_model, move_path)\n    else:\n        raise Exception(\"Model not found at\" + mar_model)        \n    \n    \nclass OMarFiles(NamedTuple):\n    model_path: str\n    extra_files: str\n\ndef get_mar_required_files(experiment_name: str, dataset_name: str, root_path: str) -> OMarFiles:\n    import os\n    model_folder = os.path.join(root_path, experiment_name, \"outputs\", dataset_name)\n    model = os.path.join(model_folder, \"pytorch_model.bin\")\n    vocab = os.path.join(model_folder, \"vocab.txt\")\n    config = os.path.join(model_folder, \"config.json\")\n\n    return model, \"{},{}\".format(vocab, config) # type: ignore\n\n@kfp.dsl.pipeline(\n    name=\"End to End Hugging Face Topic Classifier - Serving\",\n    description=\"The Serving part of the E2E HF Topic Classifier - DEBUG ONLY\"\n)\ndef pipeline(experiment_name: str, volume_name: str, dataset_name: str,\n    model_version: str = \"1.0\", randomize_service_suffix: bool = False, use_seed: bool = True, additional_requirements: list = [\"anltk\",\"torchserve\",\"transformers\",],\n    model_serve_name: str = \"\", model_serve_threads_count: int = 4, model_serve_queue_size: int = 10,\n    model_serve_install_dependencies: bool = True, model_serve_is_default: bool = True, model_serve_workers: int = 1, model_serve_workers_max: int = 5,\n    model_serve_batch_size: int = 1, model_serve_timeout: int = 120,\n): \n    mount_dir = \"/store\"\n    volumetrize = setup_volume(volume_name, mount_dir)\n\n    # Convert this to a utility function\n    handler_import_path = \"src/handlers/topic_class.py\"\n    handler_code = open(handler_import_path, encoding='utf-8').read() \n\n    handler_op = components.func_to_container_op(create_handler)\n    handler_task = handler_op(handler_code, experiment_name, mount_dir, additional_requirements)\n    handler_task = volumetrize(handler_task) # To allow saving\n\n    get_mar_op = components.func_to_container_op(get_mar_required_files)\n    get_mar_task = get_mar_op(experiment_name, dataset_name, mount_dir)\n\n    model_path = get_mar_task.outputs[\"model_path\"]\n    extra_files = get_mar_task.outputs[\"extra_files\"]\n    model_name = dataset_name\n\n    handler_path = handler_task.outputs[\"handler_path\"]\n    requirements_path = handler_task.outputs[\"requirements_path\"]\n    out_temp_path = handler_task.outputs[\"temp_path\"]\n\n    mar_convert_op = components.load_component_from_file(\"src/pipelines/yamls/Components/hf_make_mar_file.yaml\")\n    mar_convert_task = mar_convert_op(\n        model_name = model_name,\n        model_version = model_version,\n        export_path = out_temp_path,\n        model_file = model_path,\n        extra_files = extra_files,\n        handler_file = handler_path,\n        requirements_file = requirements_path, \n    )\n    mar_convert_task = volumetrize(mar_convert_task)\n    cacheless_task(mar_convert_task)\n\n    move_model_op = components.func_to_container_op(move_mar_model)\n    move_model_task = move_model_op(model_name, experiment_name, mount_dir, out_temp_path)\n    move_model_task = volumetrize(move_model_task)\n    move_model_task = move_model_task.after(mar_convert_task).after(get_mar_task)\n    cacheless_task(move_model_task)\n \n    # Create Config\n    create_config_op = components.func_to_container_op(create_mar_config)\n    create_config_task = create_config_op(\n        experiment_name, mount_dir, dataset_name,\n        model_serve_name, model_version, model_serve_threads_count, model_serve_queue_size,\n        model_serve_install_dependencies, model_serve_is_default, model_serve_workers, model_serve_workers_max,\n        model_serve_batch_size, model_serve_timeout\n    )\n    create_config_task = volumetrize(create_config_task)\n\n\n    serve_task = create_serve_task(dataset_name, experiment_name, volume_name, randomize_service_suffix, use_seed)\n    serve_task = volumetrize(serve_task)\n    serve_task = serve_task.after(move_model_task).after(create_config_task)\n    cacheless_task(serve_task)\n"
  },
  {
    "repo": "raldebsi/Kubeflow-Experimental-Pipelines",
    "file_path": "src/pipelines/end_to_end/topic_class.py",
    "raw_url": "https://raw.githubusercontent.com/raldebsi/Kubeflow-Experimental-Pipelines/master/src/pipelines/end_to_end/topic_class.py",
    "content": "import json\nfrom typing import NamedTuple, Optional\n\nimport kfp\nimport yaml\nfrom kfp import components\nfrom kfp.components import func_to_container_op\nfrom src.pipelines.common_utils import (add_pvolumes_func, get_volume_by_name,\n                                        spec_from_file_format)\n\n\ndef get_run_args(dataset_name, has_test, seq_len, batch_size_dev, learn_rate, epochs, seed, experiment_name) -> NamedTuple(\n    \"Outputs\",\n    [\n        (\"train_args\", dict),\n        (\"extra_args\", dict),\n    ]\n):\n    import json\n    import os\n    args = {\"extra\": {}}\n\n    args[\"save_steps\"] = 1000\n    args[\"extra\"][\"overwrite_output_dir\"] = \"\"\n\n    model_name_or_path = \"CAMeL-Lab/bert-base-arabic-camelbert-msa-sixteenth\"\n    args[\"model_name_or_path\"] = model_name_or_path\n    \n    train_file = '/store/datasets/{}/train.json'.format(dataset_name)\n    args[\"train_file\"] = train_file\n    args[\"extra\"][\"do_train\"] = \"\"\n    \n    \n    valid_file = '/store/datasets/{}/valid.json'.format(dataset_name)\n    args[\"extra\"][\"validation_file\"] = valid_file\n    args[\"extra\"][\"do_eval\"] = \"\"\n\n    if has_test == True or (isinstance(has_test, str) and has_test.lower() in \"true1yes\"):\n        test_file = '/store/datasets/{}/test.json'.format(dataset_name)\n        args[\"extra\"][\"test_file\"] = test_file\n        args[\"extra\"][\"do_predict\"] = \"\"\n        \n    \n    args[\"max_seq_length\"] = seq_len\n    \n    args[\"per_device_train_batch_size\"] = batch_size_dev\n    \n    args[\"learning_rate\"] = learn_rate\n    \n    args[\"num_train_epochs\"] = epochs\n    \n    output_dir = '/store/{}/outputs/{}'.format(experiment_name, dataset_name)\n    os.makedirs(output_dir, exist_ok=True)\n    args[\"output_dir\"] = output_dir\n    \n    if seed:\n        args[\"extra\"][\"seed\"] = hash(seed) # Hash of int is the same as int, hash of str is int\n\n    # write the args to a file\n    with open(os.path.join(output_dir, \"{}-{}-best-hps.json\".format(experiment_name, dataset_name)), \"w\") as f:\n        json.dump(args, f)\n\n\n    print(\"Args:\")\n    print(args)\n\n    # convert args to string\n    # return \" \".join(\"--{} {}\".format(k, v) for k, v in args.items())\n\n    return args, args.pop(\"extra\")\n\ndef create_tfjob_task(job_name, hyperparams, mount_name):\n    tfjob_chief_spec = spec_from_file_format(\"src/pipelines/yamls/Specs/HFChief.yaml\",\n        trainParamValues=hyperparams,\n        volumeResourceName=mount_name,\n    )\n    tfjob_worker_spec = spec_from_file_format(\"src/pipelines/yamls/Specs/HFWorker.yaml\",\n        trainParamValues=hyperparams,\n    )\n\n    tfjob_op = components.load_component_from_file(\"src/pipelines/yamls/Components/tfjob_launcher.yaml\")\n\n    tfjob_task = tfjob_op(\n        name=str(job_name) + \"-{{workflow.name}}\",\n        namespace=\"{{workflow.namespace}}\",\n        chief_spec=json.dumps(tfjob_chief_spec),\n        worker_spec=json.dumps(tfjob_worker_spec),\n        tfjob_timeout_minutes=60,\n        delete_finished_tfjob=False\n    )\n\n    return tfjob_task\n\n\n\ndef hf_task(args: str, is_print=False):\n    file = \"src/pipelines/yamls/Components/hf_trainer_internal.yaml\"\n    if is_print:\n        with open(file, \"r\", encoding=\"utf-8\") as f:\n            hfjob_op = yaml.safe_load(f)\n            commands: list = hfjob_op[\"implementation\"][\"container\"][\"args\"]\n            name: str = hfjob_op[\"name\"]\n            description = hfjob_op[\"description\"]\n            first_arg = \"echo \" + commands.pop(0)\n            commands.insert(0, first_arg)\n            description = \"Prints the expected arguments and commands for \" + name\n            name = \"Print \" + name\n            hfjob_op[\"implementation\"][\"container\"][\"args\"] = commands\n            hfjob_op[\"name\"] = name\n            hfjob_op[\"description\"] = description\n            hfjob_op = components.load_component_from_text(yaml.dump(hfjob_op))\n    else:\n        hfjob_op = components.load_component_from_file(file)\n\n    return hfjob_op(params=args)\n\n\ndef convert_run_args_to_str(train_args: dict, extra_args: dict) -> str:\n    print(\"Args\")\n    print(train_args)\n    print(\"Extra Args\")\n    print(extra_args)\n    e_args = \" \".join(\"--{} {}\".format(k, v) if v != \"\" else \"--\" + k for k, v in extra_args.items())\n    p_args = \" \".join(\"--{} {}\".format(k, v) if v != \"\" else \"--\" + k for k, v in train_args.items())\n    print(\"Formatted\")\n    print(p_args)\n    print(e_args)\n\n    return \"{} {}\".format(p_args, e_args)\n\n\ndef convert_run_args(args: dict, extra_args: dict) -> NamedTuple(\n    \"Outputs\",\n    [\n        (\"model\", str),\n        (\"tf\", str),\n        (\"seq\", int),\n        (\"batch\", int),\n        (\"lr\", float),\n        (\"epoch\", int),\n        (\"out\", str),\n        (\"save\", int),\n        (\"extra_args\", str)\n    ]\n    ):\n    print(\"Input Dict\")\n    print(args)\n    return \\\n        str     (args[\"model_name_or_path\"]),           \\\n        str     (args[\"train_file\"]),                   \\\n        int     (args[\"max_seq_length\"]),               \\\n        int     (args[\"per_device_train_batch_size\"]),  \\\n        float   (args[\"learning_rate\"]),                \\\n        int     (args[\"num_train_epochs\"]),             \\\n        str     (args[\"output_dir\"]),                   \\\n        int     (args[\"save_steps\"]),                   \\\n        \" \".join(\"--{} {}\".format(k, v) if v != \"\" else \"--\" + k for k, v in extra_args.items())\n\ndef create_serve_task(model_name, experiment_name, mount_name):\n    model_namespace = \"{{workflow.namespace}}\"\n    model_name = \"{}_{}\".format(model_name, experiment_name)\n\n    infer_service = spec_from_file_format(\n        \"src/pipelines/yamls/Specs/KFServe.yaml\",\n        apiVersion=\"kserve3/beta4\",\n        modelName=model_name,\n        modelNamespace=model_namespace,\n        volumeResourceName=mount_name,\n    )\n\n    serve_op = components.load_component_from_file(\"src/pipelines/yamls/Components/kserve_launcher.yaml\")\n    serve_task = serve_op(\n        action=\"apply\",\n        inferenceservice_yaml=yaml.dump(infer_service),\n    )\n    \n    return serve_task\n\n@kfp.dsl.pipeline(\n    name=\"End to End Hugging Face Topic Classifier\",\n    description=\"End to End Topic Classiciation using HuggingFace Framework and CamelBert model\"\n)\ndef pipeline(experiment_name: str, volume_name: str,\n                dataset_name: str, has_test: bool = False,\n                max_sequence_length: int = 512, device_batch_size: int = 8,\n                learning_rate: float = 3e-5, epochs: int = 5, seed: Optional[int] = None):\n \n    mount_vol = get_volume_by_name(volume_name, \"volume-bind\")\n    mount_dict = {\"/store\": mount_vol}\n    volumetrize = add_pvolumes_func(mount_dict)\n\n    \n    get_args_comp = func_to_container_op(get_run_args) # Simulate Katib\n    get_args_task = get_args_comp(dataset_name, has_test, max_sequence_length, device_batch_size, learning_rate, epochs, seed, experiment_name)\n    get_args_task = volumetrize(get_args_task)\n\n    convert_args_comp = func_to_container_op(convert_run_args_to_str)\n    convert_args_task = convert_args_comp(\n        get_args_task.outputs[\"train_args\"],\n        get_args_task.outputs[\"extra_args\"],\n    )\n\n\n    train_params = convert_args_task.output\n\n    print_train_task =  hf_task(\n        train_params,\n        True\n    )\n\n    train_task = hf_task(train_params)\n    train_task = volumetrize(train_task)\n\n\n    # serve_task = create_serve_task(dataset_name, experiment_name, volume_name).after(train_task)\n"
  },
  {
    "repo": "raldebsi/Kubeflow-Experimental-Pipelines",
    "file_path": "src/pipelines/volume_test/read.py",
    "raw_url": "https://raw.githubusercontent.com/raldebsi/Kubeflow-Experimental-Pipelines/master/src/pipelines/volume_test/read.py",
    "content": "import kfp\r\nimport kfp.compiler\r\nimport kfp.components\r\nfrom kfp.dsl._pipeline_param import sanitize_k8s_name\r\nfrom kfp.dsl._pipeline_volume import PipelineVolume\r\nfrom src.pipelines.common_utils import get_volume_by_name, add_pvolumes_func\r\n\r\n\r\ndef read_file(path: str):\r\n    import os\r\n    full_path = os.path.join(path, \"file.txt\")\r\n    print(\"Full path is {}\".format(full_path))\r\n    print(\"Current path is {}\".format(os.getcwd()))\r\n    with open(full_path, \"r\", encoding='utf-8') as f:\r\n        data = f.read()\r\n        print(data)\r\n        return data\r\n\r\n@kfp.dsl.pipeline(\r\n    name=\"Shared Volume Pipeline\",\r\n    description=\"Test reading from a consistent volume\",\r\n)\r\ndef read_volume_pipeline(data_path: str):\r\n    # mount_vol = kfp.dsl.VolumeOp(\r\n    #     name=\"Read Ridhwan Volumes\",\r\n    #     size=\"1Gi\",\r\n    #     modes=kfp.dsl.VOLUME_MODE_RWO,\r\n    #     resource_name=\"ridhwan-pvc-mount\", # PVC Name, will be {pipeline_name}-{id}-resource if name does not exist and generate_unique_name is True.\r\n    #     volume_name=\"volumesecond\",\r\n    #     generate_unique_name=False,\r\n    # )\r\n\r\n    mount_vol = get_volume_by_name(\"ridhwan-pvc-mount\")\r\n    data_store_path = data_path\r\n    pvolumes = {data_store_path: mount_vol}\r\n    volumetrize = add_pvolumes_func(pvolumes)\r\n\r\n    # Components\r\n    read_volume_component = kfp.components.create_component_from_func(read_file)\r\n    # Tasks\r\n    read_task = volumetrize(read_volume_component(data_store_path))\r\n"
  },
  {
    "repo": "raldebsi/Kubeflow-Experimental-Pipelines",
    "file_path": "src/pipelines/volume_test/write.py",
    "raw_url": "https://raw.githubusercontent.com/raldebsi/Kubeflow-Experimental-Pipelines/master/src/pipelines/volume_test/write.py",
    "content": "from typing import NamedTuple, List\r\nimport kfp\r\nimport kfp.compiler\r\nimport kfp.components\r\n\r\ndef write_file(contents: str, path: str):\r\n    import os\r\n    os.makedirs(path, exist_ok=True)\r\n    full_path = os.path.join(path, \"file.txt\")\r\n    with open(full_path, \"w\", encoding='utf-8') as f:\r\n        f.write(contents)\r\n\r\ndef read_file(path: str):\r\n    import os\r\n    full_path = os.path.join(path, \"file.txt\")\r\n    with open(full_path, \"r\", encoding='utf-8') as f:\r\n        return f.read()\r\n\r\n@kfp.dsl.pipeline(\r\n    name=\"Writing Into PVC Test\",\r\n    description=\"Test writing to a consistent volume\",\r\n)\r\ndef write_volume_pipeline():\r\n    from kfp import dsl\r\n\r\n    mount_vol = dsl.VolumeOp(\r\n        name=\"Create Ridhwan Volumes\", # Volume Unique Name, will be used as identifier to operation.\r\n                                               # If operation exists then it will be reused.\r\n                                               # If operation does not exist then it will be created and the name will be resource_name below.\r\n        size=\"1Gi\",\r\n        modes=dsl.VOLUME_MODE_RWO,\r\n        resource_name=\"ridhwan-pvc-mount\", # PVC Name, will be {pipeline_name}-{id}-resource if name does not exist and generate_unique_name is True.\r\n        # volume_name=\"ridhwan-personal-volume\", # Volume Name, do not use as it will cause the volume to not be found\r\n        # data_source=PipelineParam(name=\"data_source\"), # This way it will create a volume snapshot from param\r\n        generate_unique_name=False,\r\n    )\r\n\r\n    data_store_path = \"/data\"\r\n    pvolumes = {data_store_path: mount_vol.volume}\r\n    volumetrize = lambda func: func.add_pvolumes(pvolumes)\r\n\r\n    # Components\r\n    write_volume_component = kfp.components.create_component_from_func(write_file)\r\n    read_volume_component = kfp.components.create_component_from_func(read_file)\r\n    # Tasks\r\n    write_task = volumetrize(write_volume_component(\"This is a file created by the pipeline into {}\".format(\"/data\"), data_store_path))\r\n    read_task = volumetrize(read_volume_component(data_store_path))\r\n    read_task.after(write_task)"
  },
  {
    "repo": "markbarna/kubeflow-pipelines-poc",
    "file_path": "main.py",
    "raw_url": "https://raw.githubusercontent.com/markbarna/kubeflow-pipelines-poc/master/main.py",
    "content": "from kfp import components, dsl, compiler, Client\n\nfrom components.fetch_data import fetch_data, split_data\nfrom components.model import train\nfrom utils.git import create_version_name\n\nBASE_IMAGE = 'dabarnyarddawg/kf-pipelines-base-images:latest'\nCLIENT = 'http://127.0.0.1:8080'\nPIPELINE_NAME = 'cancer-classifier'\nEXPERIMENT_NAME = 'cancer_detection'\n\nfetch_data_op = components.create_component_from_func(fetch_data, base_image=BASE_IMAGE)\nsplit_data_op = components.create_component_from_func(split_data, base_image=BASE_IMAGE)\ntrain_op = components.create_component_from_func(train, base_image=BASE_IMAGE)\n\n\n@dsl.pipeline(name=PIPELINE_NAME, description='test classifier pipeline with breast cancer dataset')\ndef pipeline(test_size: float = 0.2):\n    fetch_data_task = fetch_data_op()\n    split_data_task = split_data_op(x=fetch_data_task.outputs['x'], y=fetch_data_task.outputs['y'], test_size=test_size)\n    # TODO: train model(s) (with tuning) in parallel?\n    train_task = train_op(x=split_data_task.outputs['x_train'], y=split_data_task.outputs['y_train'])\n    # TODO: batch predictions (move to separate pipeline)\n    # TODO: serve model\n\n\nif __name__ == '__main__':\n    client = Client(host=CLIENT)\n    client.create_run_from_pipeline_func(\n        pipeline, arguments={}, run_name=create_version_name(), experiment_name=f'{EXPERIMENT_NAME}_dev'\n    )\n    # TODO: github action to compile & deploy pipeline on release\n    # TODO: unit tests on commit\n    # TODO: connect artifacts to local storage mount\n"
  },
  {
    "repo": "hafizurcse/azure-kubeflow-pipeline",
    "file_path": "code/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/hafizurcse/azure-kubeflow-pipeline/main/code/pipeline.py",
    "content": "import kfp.dsl as dsl\r\nfrom kubernetes import client as k8s_client\r\n\r\n\r\n@dsl.pipeline(\r\n    name='Tacos vs. Burritos',\r\n    description='Simple TF CNN for binary classifier between burritos and tacos'\r\n)\r\ndef tacosandburritos_train(\r\n    tenant_id,\r\n    service_principal_id,\r\n    service_principal_password,\r\n    subscription_id,\r\n    resource_group,\r\n    workspace,\r\n    persistent_volume_name='azure',\r\n    persistent_volume_path='/mnt/azure',\r\n    data_download='https://aiadvocate.blob.core.windows.net/public/tacodata.zip',\r\n    epochs=5,\r\n    batch=32,\r\n    learning_rate=0.0001,\r\n    imagetag='latest',\r\n    model_name='tacosandburritos',\r\n    profile_name='tacoprofile'\r\n):\r\n\r\n    operations = {}\r\n    image_size = 160\r\n    training_folder = 'train'\r\n    training_dataset = 'train.txt'\r\n    model_folder = 'model'\r\n\r\n    # preprocess data\r\n    operations['preprocess'] = dsl.ContainerOp(\r\n        name='preprocess',\r\n        image='insert your image here',\r\n        command=['python'],\r\n        arguments=[\r\n            '/scripts/data.py',\r\n            '--base_path', persistent_volume_path,\r\n            '--data', training_folder,\r\n            '--target', training_dataset,\r\n            '--img_size', image_size,\r\n            '--zipfile', data_download\r\n        ]\r\n    )\r\n\r\n    #train\r\n    operations['training'] = dsl.ContainerOp(\r\n        name='training',\r\n        image='insert your image here',\r\n        command=['python'],\r\n        arguments=[\r\n            '/scripts/train.py',\r\n            '--base_path', persistent_volume_path,\r\n            '--data', training_folder, \r\n            '--epochs', epochs, \r\n            '--batch', batch, \r\n            '--image_size', image_size, \r\n            '--lr', learning_rate, \r\n            '--outputs', model_folder, \r\n            '--dataset', training_dataset\r\n        ]\r\n    )\r\n    operations['training'].after(operations['preprocess'])\r\n\r\n    # register model\r\n    operations['register'] = dsl.ContainerOp(\r\n        name='register',\r\n        image='insert your image here',\r\n        command=['python'],\r\n        arguments=[\r\n            '/scripts/register.py',\r\n            '--base_path', persistent_volume_path,\r\n            '--model', 'latest.h5',\r\n            '--model_name', model_name,\r\n            '--tenant_id', tenant_id,\r\n            '--service_principal_id', service_principal_id,\r\n            '--service_principal_password', service_principal_password,\r\n            '--subscription_id', subscription_id,\r\n            '--resource_group', resource_group,\r\n            '--workspace', workspace\r\n        ]\r\n    )\r\n    operations['register'].after(operations['training'])\r\n\r\n    operations['profile'] = dsl.ContainerOp(\r\n        name='profile',\r\n        image='insert your image here',\r\n        command=['sh'],\r\n        arguments=[\r\n            '/scripts/profile.sh',\r\n            '-n', profile_name,\r\n            '-m', model_name,\r\n            '-i', '/scripts/inferenceconfig.json',\r\n            '-d', '{\"image\":\"https://www.exploreveg.org/files/2015/05/sofritas-burrito.jpeg\"}',\r\n            '-t', tenant_id,\r\n            '-r', resource_group,\r\n            '-w', workspace,\r\n            '-s', service_principal_id,\r\n            '-p', service_principal_password,\r\n            '-u', subscription_id,\r\n            '-b', persistent_volume_path\r\n        ]\r\n    )\r\n    operations['profile'].after(operations['register'])\r\n\r\n    operations['deploy'] = dsl.ContainerOp(\r\n        name='deploy',\r\n        image='insert your image here',\r\n        command=['sh'],\r\n        arguments=[\r\n            '/scripts/deploy.sh',\r\n            '-n', model_name,\r\n            '-m', model_name,\r\n            '-i', '/scripts/inferenceconfig.json',\r\n            '-d', '/scripts/deploymentconfig.json',\r\n            '-t', tenant_id,\r\n            '-r', resource_group,\r\n            '-w', workspace,\r\n            '-s', service_principal_id,\r\n            '-p', service_principal_password,\r\n            '-u', subscription_id,\r\n            '-b', persistent_volume_path\r\n        ]\r\n    )\r\n    operations['deploy'].after(operations['profile'])\r\n\r\n    for _, op in operations.items():\r\n        op.container.set_image_pull_policy(\"Always\")\r\n        op.add_volume(\r\n            k8s_client.V1Volume(\r\n                name='azure',\r\n                persistent_volume_claim=k8s_client.V1PersistentVolumeClaimVolumeSource(\r\n                    claim_name='azure-managed-disk')\r\n                )\r\n            ).add_volume_mount(k8s_client.V1VolumeMount(\r\n                mount_path='/mnt/azure', \r\n                name='azure')\r\n            )\r\n\r\n\r\nif __name__ == '__main__':\r\n   import kfp.compiler as compiler\r\n   compiler.Compiler().compile(tacosandburritos_train, __file__ + '.tar.gz')\r\n"
  },
  {
    "repo": "fontaine-raphael/kubeflow",
    "file_path": "pipelines/nyc_taxi.py",
    "raw_url": "https://raw.githubusercontent.com/fontaine-raphael/kubeflow/master/pipelines/nyc_taxi.py",
    "content": "import kfp.compiler as compiler\nimport kfp.components as comp\nimport kfp.dsl as dsl\nimport kfp.gcp as gcp\nimport os\n\n\nextract_path = '/Users/fontaine/projects/kubeflow/components/yellow-taxis-nyc/extract'\npreprocessing_path = '/Users/fontaine/projects/kubeflow/components/yellow-taxis-nyc/pre-processing'\n\nextract_op = comp.load_component_from_file(os.path.join(extract_path, 'component.yaml'))\npreprocessing_op = comp.load_component_from_file(os.path.join(preprocessing_path, 'component.yaml'))\n\n@dsl.pipeline(name='NYC Yellow Taxi Fare Predict', description='Pipeline to predict the fare amount of NYC Yellow Cab.')\ndef nyc_taxi_pipeline(\n    project='kubeflow-xyz',\n    dataset='yellow_taxi',\n    bucket='gs://yellow-taxi-nyc',\n    start_date='2015-01-01',\n    end_date='2015-01-05'\n):\n    extract = extract_op(\n        project=project,\n        dataset=dataset,\n        bucket=bucket,\n        start_date=start_date,\n        end_date=end_date\n    ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n\n    preprocessing = preprocessing_op(\n        project=project,\n        staging_bucket=extract.outputs['staging_bucket']\n    ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n\n# Compile\npipeline_func = nyc_taxi_pipeline\npipeline_filename = pipeline_func.__name__ + \".tar.gz\"\ncompiler.Compiler().compile(pipeline_func, pipeline_filename)\nprint(pipeline_filename)\n"
  },
  {
    "repo": "rahuja23/Blogpost_kubeflow_pipeline",
    "file_path": "Pipeline/kfp_v2.py",
    "raw_url": "https://raw.githubusercontent.com/rahuja23/Blogpost_kubeflow_pipeline/master/Pipeline/kfp_v2.py",
    "content": "import kfp\nimport kfp.dsl as dsl\nfrom kfp.v2.dsl import (\n    component,\n    Input,\n    Output,\n    Dataset,\n    Artifact,\n    InputPath,\n    OutputPath,\n)\nfrom kfp import compiler\nimport subprocess\nfrom kfp.aws import use_aws_secret\nfrom typing import NamedTuple\n\n@component(base_image=\"racahu23/blog_mlflow:1\", packages_to_install=['mlflow'])\ndef mlflow_setup_experiment(tracking_uri:str)->NamedTuple('Outputs',[(\"exp_id\", str), (\"run_id\", str)]):\n    import main\n    op = main.main(tracking_uri)\n    return op\n@component(base_image=\"racahu23/preprocess:blog\",  packages_to_install=['mlflow', 'boto3'])\ndef twitter_download_preprocess(information: Output[Artifact],  experiment_id: str, run_id: str, tracking_uri: str):\n    from main import twitter_sample_download_and_preprocess\n    args={\n        \"experiment_id\": experiment_id,\n        \"run_id\": run_id,\n        \"tracking_uri\": tracking_uri,\n        \"log_folder\": information.path\n    }\n    twitter_sample_download_and_preprocess(args)\n\n@component(base_image=\"racahu23/numpy:blog_final\",  packages_to_install=['mlflow', 'boto3'])\ndef numpy_process(information: Input[Artifact], information_output: Output[Artifact],  experiment_id: str, run_id: str,\n                  tracking_uri: str):\n    from main import numpy_process\n    args = {\n        \"experiment_id\": experiment_id,\n        \"run_id\": run_id,\n        \"tracking_uri\": tracking_uri,\n        \"log_folder\": information.path,\n        \"output_dir\": information_output.path\n           }\n    op = numpy_process(args)\n    return op\n@component(base_image=\"racahu23/scikit:3\", packages_to_install=['boto3'])\ndef sklearn_logistic(information_input: Input[Artifact], experiment_id: str,\n                     tracking_uri: str, run_id: str,  sklearn_output: Output[Artifact]):\n    from main import sklearn_logistic\n    args = {\n        \"experiment_id\": experiment_id,\n        \"tracking_uri\": tracking_uri,\n        \"log_folder\":  information_input.path,\n        \"output_dir\": sklearn_output.path,\n        \"run_id\": run_id\n    }\n    op = sklearn_logistic(args)\n@component(base_image=\"racahu23/logistic:2\",   packages_to_install=['mlflow', 'boto3'])\ndef logistic_op(sklearn_input:Input[Artifact], logistic_output: Output[Artifact],  experiment_id: str, run_id: str,\n                tracking_uri: str):\n    from main import logistic\n    args = {\n        \"experiment_id\": experiment_id,\n        \"run_id\": run_id,\n        \"tracking_uri\": tracking_uri,\n        \"log_folder\": sklearn_input.path,\n        \"output_dir\": logistic_output.path\n    }\n    op = logistic(args)\n@component(base_image=\"racahu23/torch:1\",  packages_to_install=['mlflow', 'boto3'])\ndef torch_op(logistic_input:Input[Artifact], torch_output: Output[Artifact],\n             experiment_id: str, run_id: str, tracking_uri: str\n             ):\n    from main import torch_process_logistic\n    args = {\n        \"experiment_id\": experiment_id,\n        \"run_id\": run_id,\n        \"tracking_uri\": tracking_uri,\n        \"log_folder\": logistic_input.path,\n        \"output_dir\": torch_output.path\n    }\n    op = torch_process_logistic(args)\n\n@component(base_image=\"racahu23/svm:1\",  packages_to_install=['mlflow', 'boto3'])\ndef svm_op(svm_input: Input[Artifact], svm_output: Output[Artifact],\n           experiment_id: str, run_id: str, tracking_uri: str\n           ):\n    from main import svm_process\n    args = {\n        \"experiment_id\": experiment_id,\n        \"run_id\": run_id,\n        \"tracking_uri\": tracking_uri,\n        \"log_folder\": svm_input.path,\n        \"output_dir\": svm_output.path\n    }\n    op = svm_process(args)\n\n@component(base_image=\"racahu23/register:4\",  packages_to_install=['mlflow', 'python-dotenv'])\ndef register_op( run_id: str, tracking_uri: str):\n    from main import register_model\n    args = {\n        \"run_id\": run_id,\n        \"tracking_uri\": tracking_uri,\n    }\n    op = register_model(args)\n\nif __name__ ==\"__main__\":\n    @dsl.pipeline(\n        name='Twitter nltk pipeline',\n        description='Writing code by the other way.'\n    )\n    def pipeline(mlflow_uri: str):\n        pvc_name = \"twitter-5000\"\n        \"\"\"\n        vop = dsl.VolumeOp(\n            name=pvc_name,\n            resource_name=\"twitter-5000\",\n            size=\"1Gi\",\n            modes=dsl.VOLUME_MODE_RWM\n        )\n        \"\"\"\n        op_mlflow = mlflow_setup_experiment(tracking_uri=mlflow_uri)\n        download_task= twitter_download_preprocess(experiment_id=op_mlflow.outputs[\"exp_id\"], run_id=op_mlflow.outputs[\"run_id\"], tracking_uri=mlflow_uri)\n        numpy_task = numpy_process(information=download_task.outputs['information'], experiment_id=op_mlflow.outputs[\"exp_id\"], run_id=op_mlflow.outputs[\"run_id\"]\n                                   , tracking_uri=mlflow_uri).after(\n            download_task)\n\n\n        sklearn_task= sklearn_logistic(information_input=numpy_task.outputs[\"information_output\"], experiment_id=op_mlflow.outputs[\"exp_id\"], tracking_uri=mlflow_uri, run_id=op_mlflow.outputs[\"run_id\"]).after(numpy_task)\n\n\n        logistic_task= logistic_op(sklearn_input=numpy_task.outputs[\"information_output\"], experiment_id=op_mlflow.outputs[\"exp_id\"], run_id=op_mlflow.outputs[\"run_id\"]\n                                   , tracking_uri=mlflow_uri).after(sklearn_task)\n\n\n        torch_task = torch_op(logistic_input=numpy_task.outputs[\"information_output\"], experiment_id=op_mlflow.outputs[\"exp_id\"], run_id=op_mlflow.outputs[\"run_id\"]\n                                   , tracking_uri=mlflow_uri ).after(logistic_task)\n\n        svm_task = svm_op(svm_input=numpy_task.outputs[\"information_output\"], experiment_id=op_mlflow.outputs[\"exp_id\"], run_id=op_mlflow.outputs[\"run_id\"]\n                                   , tracking_uri=mlflow_uri).after(torch_task)\n        register_task = register_op(tracking_uri=mlflow_uri, run_id=op_mlflow.outputs[\"run_id\"]).after(svm_task)\n    client = kfp.Client(namespace=\"kubeflow\", host=\"http://localhost:8080\")\n    client.create_run_from_pipeline_func(pipeline,\n                                         arguments={\"mlflow_uri\": \"http://mlflow.use-case.svc.cluster.local:5000\"},\n                                         mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE,\n                                         enable_caching=False)"
  },
  {
    "repo": "imsazzad/kubeflow-piplines-ml",
    "file_path": "app/kfp_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/imsazzad/kubeflow-piplines-ml/dev/app/kfp_pipeline.py",
    "content": "import kfp\nfrom kfp import dsl\nfrom app.preprocess_data.kf_preprocess_component import preprocess_op\nfrom app.train.kf_train_component import train_op\nfrom app.test.kf_test_component import test_op\nfrom app.deploy.kf_deploy_component import deploy_model_op\n\n@dsl.pipeline(\n    name='Boston Housing Pipeline',\n    description='An example pipeline.'\n)\ndef boston_pipeline():\n    _preprocess_op = preprocess_op()\n\n    _train_op = train_op(\n        dsl.InputArgumentPath(_preprocess_op.outputs['x_train']),\n        dsl.InputArgumentPath(_preprocess_op.outputs['y_train'])\n    ).after(_preprocess_op)\n\n    _test_op = test_op(\n        dsl.InputArgumentPath(_preprocess_op.outputs['x_test']),\n        dsl.InputArgumentPath(_preprocess_op.outputs['y_test']),\n        dsl.InputArgumentPath(_train_op.outputs['model'])\n    ).after(_train_op)\n\n    deploy_model_op(\n        dsl.InputArgumentPath(_train_op.outputs['model'])\n    ).after(_test_op)\n\n\n\nhost='http://kf-centraldashboard.k8sdev.infolytx.tech/pipeline/'\nnamespace='sazzad'\n\n\nclient = kfp.Client(host=host, namespace=namespace)\nclient.create_run_from_pipeline_func(boston_pipeline, arguments={}, experiment_name= \"Boston Housing Pipeline Experiment\")\n"
  },
  {
    "repo": "sfujiwara/kfpc",
    "file_path": "examples/simple.py",
    "raw_url": "https://raw.githubusercontent.com/sfujiwara/kfpc/main/examples/simple.py",
    "content": "import argparse\nfrom kfp.v2 import compiler\nfrom google.cloud import aiplatform\nimport kfpc\nimport kfp.dsl\n\n\ndef parse():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--project\", type=str, required=True)\n    args = parser.parse_args()\n    return args\n\n\n@kfp.dsl.pipeline(name=\"simple\")\ndef pipeline_fn(project: str):\n    query_select1_task = kfpc.bigquery.Query(name=\"select-1\").task(\n        query=\"SELECT 1\",\n        job_project=project,\n        destination_project=project,\n        destination_dataset=\"sandbox\",\n        destination_table=\"select1\",\n    )\n\n    query_select2_task = kfpc.bigquery.Query(name=\"select-2\").task(\n        query=\"SELECT 2\",\n        job_project=project,\n        destination_project=project,\n        destination_dataset=\"sandbox\",\n        destination_table=\"select2\",\n    )\n\n    query_select3_task = kfpc.bigquery.Query(name=\"select-3\").task(\n        query=\"SELECT 3\",\n        job_project=project,\n        destination_project=project,\n        destination_dataset=\"sandbox\",\n        destination_table=\"select3\",\n        depend_on=[\n            query_select1_task.destination_table,\n            query_select2_task.destination_table,\n        ],\n    )\n\n    extract_task = kfpc.bigquery.ExtractArtifact(name=\"extract\").task(\n        job_project=project,\n        source_table_artifact=query_select3_task.destination_table,\n    )\n\n    load_artifact_task = kfpc.bigquery.Load(name=\"load\").task(\n        job_project=project,\n        source_artifact=extract_task.output_files,\n        destination_project=project,\n        destination_dataset=\"sandbox\",\n        destination_table=\"load\",\n        schema=[{\"name\": \"f0_\", \"type\": \"INTEGER\"}],\n        source_uri_suffix=\"data-*.jsonl\",\n    )\n\n\ndef main():\n    args = parse()\n    project = args.project\n\n    compiler.Compiler().compile(pipeline_func=pipeline_fn, package_path=\"pipeline.yaml\")\n    job = aiplatform.PipelineJob(\n        project=project,\n        display_name=\"simple\",\n        enable_caching=False,\n        template_path=\"pipeline.yaml\",\n        parameter_values={\"project\": project},\n        pipeline_root=f\"gs://{project}-vertex-ai/pipeline-root\",\n    )\n    job.submit()\n\n\nif __name__ == \"__main__\":\n    main()\n"
  },
  {
    "repo": "dhirajpatra/kubeflow-demo",
    "file_path": "pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/dhirajpatra/kubeflow-demo/main/pipeline.py",
    "content": "from kfp import dsl\nfrom kfp.dsl import container_component, Output, OutputPath, Input, InputPath, pipeline, Artifact\n\n\n@container_component\ndef preprocess_op(output_data: Output[Artifact]):\n    return dsl.ContainerSpec(\n        image='dhirajpatra/kfp-components:latest',\n        command=['python', 'preprocess.py'],\n        args=[],\n        output_artifacts={'output_data': output_data}\n    )\n\n\n@container_component\ndef train_op(input_data: Input[Artifact]):\n    return dsl.ContainerSpec(\n        image='dhirajpatra/kfp-components:latest',\n        command=['python', 'train.py'],\n        args=[],\n        input_artifacts={'input_data': input_data}\n    )\n\n\n@pipeline(name='iris-classifier-pipeline')\ndef iris_pipeline():\n    preprocess = preprocess_op()\n    train = train_op(input_data=preprocess.outputs['output_data'])\n"
  },
  {
    "repo": "secrettoad/kubeflow",
    "file_path": "multifamily_pricing/main.py",
    "raw_url": "https://raw.githubusercontent.com/secrettoad/kubeflow/main/multifamily_pricing/main.py",
    "content": "from google.cloud import aiplatform\nfrom kfp import dsl\nfrom kfp.v2 import compiler\nfrom kfp.v2.dsl import component, Input, Output, Dataset, Model, Artifact\nimport functions_framework\nimport tempfile\n\n\n@component(packages_to_install=['dask[dataframe]', 'pyarrow', 'gcsfs'], base_image='python:3.7')\ndef ingest_component(df_uri: str, df: Output[Dataset]):\n    import dask.dataframe as dd\n    _df = dd.read_csv(df_uri).set_index('Unnamed: 0')\n    _df.to_parquet(df.uri)\n\n\n@component(packages_to_install=['dask[dataframe]', 'xgboost', 'scikit-learn', 'pyarrow', 'gcsfs'], base_image='python:3.7')\ndef train_component(df_train: Input[Dataset], model: Output[Model], params: Output[Artifact]):\n    import xgboost as xgb\n    import dask.dataframe as dd\n    import gcsfs\n    import pickle\n    import json\n\n    fs = gcsfs.GCSFileSystem()\n    _df_train = dd.read_parquet(df_train.uri).compute()\n    X = _df_train[['latitude', 'longitude', 'property_type', 'sqft', 'beds', 'baths', 'days_since_2014']]\n    y = _df_train['trans_log_price']\n    ##TODO add capability for distributed cluster on compute engine\n    ##TODO add hyperparameter tuning\n    ##TODO invert control of parameters\n    _params = {\"params\": {\"objective\": \"reg:squarederror\", \"random_state\": 111, \"monotonic_constraints\": {\"sqft\": 1}}}\n    _model = xgb.XGBRegressor(**_params['params'])\n    _model.fit(X, y)\n    model.uri = model.uri + '.pkl'\n\n    with fs.open(model.uri, 'wb') as f:\n        pickle.dump(_model, f)\n\n    with fs.open(params.uri, 'w') as f:\n        json.dump(_params, f)\n\n\n@component(packages_to_install=['dask[dataframe]', 'xgboost', 'scikit-learn', 'pyarrow', 'gcsfs'], base_image='python:3.7')\ndef predict_component(df_predict: Input[Dataset], model: Input[Model], y_hat: Output[Dataset]):\n    import dask.dataframe as dd\n    import pickle\n    import gcsfs\n\n    fs = gcsfs.GCSFileSystem()\n\n    _df_predict = dd.read_parquet(df_predict.uri).compute()\n\n    X = _df_predict[['latitude','longitude','property_type','sqft','beds','baths','days_since_2014']]\n\n    with fs.open(model.uri, 'rb') as f:\n        _model = pickle.load(f)\n\n    _df_predict['y_hat'] = _model.predict(X)\n\n    dd.from_pandas(_df_predict, chunksize=1000000).to_parquet(y_hat.uri)\n\n\n@component(packages_to_install=['dask[dataframe]', 'pyarrow', 'gcsfs',], base_image='python:3.7')\ndef validate_component(y_hat: Input[Dataset], y_hat_test: Input[Dataset], metrics: Output[Artifact]):\n    import dask.dataframe as dd\n    import gcsfs\n    import json\n    import numpy as np\n\n    fs = gcsfs.GCSFileSystem()\n\n    _y_hat = dd.read_parquet(y_hat.uri).compute()\n    _y_hat_test = dd.read_parquet(y_hat_test.uri).compute()\n\n    def inv_zscore_log_price(y, mean, std):\n        return np.exp((y * std) + mean)\n\n    def median_absolute_percentage_error(actual, predicted):\n        return np.median((np.abs(actual - predicted) / actual)) * 100\n\n    def moments(s):\n        return s.apply(np.log).mean(), s.apply(np.log).std()\n\n\n    _metrics = {}\n    _metrics['training_performance'] = median_absolute_percentage_error(inv_zscore_log_price(_y_hat['y_hat'], *moments(_y_hat['price'])),\n                                           inv_zscore_log_price(_y_hat['trans_log_price'], *moments(_y_hat['price'])))\n    _metrics['test_performance'] = median_absolute_percentage_error(inv_zscore_log_price(_y_hat_test['y_hat'], *moments(_y_hat['price'])),\n                                           inv_zscore_log_price(_y_hat_test['trans_log_price'], *moments(_y_hat['price'])))\n\n    with fs.open(metrics.uri, 'w') as f:\n        json.dump(_metrics, f)\n\n\n@component(packages_to_install=['dask[dataframe]', 'pyarrow', 'gcsfs', 'google-cloud-aiplatform'], base_image='python:3.7')\ndef deployment_component(model: Input[Model], metrics: Input[Artifact], vertex_endpoint: Output[Artifact], vertex_model: Output[Model]):\n    from google.cloud import aiplatform\n    import gcsfs\n    import json\n    aiplatform.init(project='demos-362417')\n\n    fs = gcsfs.GCSFileSystem()\n    with fs.open(metrics.uri, 'rb') as f:\n        _metrics = json.load(f)\n\n    ##TODO compare against current version performance\n    ##TODO log performance metrics to metadata\n    if _metrics['test_performance'] > 0:\n\n        _model = aiplatform.Model.upload(\n            display_name=\"multifamily_demo_model\",\n            artifact_uri='/'.join(model.uri.split('/')[:-1]),\n            serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest\",\n        )\n\n        endpoint = _model.deploy(machine_type=\"n1-standard-8\")\n        vertex_endpoint.uri = endpoint.resource_name\n        vertex_model.uri = _model.resource_name\n\n\n@dsl.pipeline(\n    name=\"demo-pipeline\",\n    description=\"demo\",\n    pipeline_root='gs://coysu-demo-pipelines/multifamily-pricing',\n)\ndef pipeline():\n    train_data = ingest_component(df_uri='gs://coysu-demo-datasets/multifamily_pricing/train/*.csv')\n    test_data = ingest_component(df_uri='gs://coysu-demo-datasets/multifamily_pricing/test/*.csv')\n    model = train_component(train_data.outputs['df'])\n    insample_preds = predict_component(train_data.outputs['df'], model.outputs['model'])\n    test_preds = predict_component(test_data.outputs['df'], model.outputs['model'])\n    metrics = validate_component(insample_preds.outputs['y_hat'], test_preds.outputs['y_hat'])\n    deployed_model = deployment_component(model.outputs['model'], metrics.outputs['metrics'])\n\n    ###todo start here - make serving pipeline and add business logic - add splitting to train pipeline\n\n\n# Triggered by a change in a storage bucket\n@functions_framework.cloud_event\ndef run_pricing_pipeline(event=None):\n    template_path = tempfile.gettempdir() + '/pipeline.json'\n    compiler.Compiler().compile(pipeline_func=pipeline, package_path=template_path)\n\n    aiplatform.init(project='demos-362417', staging_bucket=\"gs://coysu-demo-pipelines/multifamily_pricing/staging\")\n\n    job = aiplatform.PipelineJob(\n        display_name='multifamily_demo',\n        template_path=template_path,\n        pipeline_root='gs://coysu-demo-pipelines/multifamily_pricing'\n    )\n\n    job.run(sync=False)\n\n"
  },
  {
    "repo": "romanzdk/-test-kubeflow",
    "file_path": "hello_kf.py",
    "raw_url": "https://raw.githubusercontent.com/romanzdk/-test-kubeflow/main/hello_kf.py",
    "content": "# Copyright 2019 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp\nfrom kfp import dsl\n\ndef echo_op():\n    return dsl.ContainerOp(\n        name='echo',\n        image='library/bash:4.4.23',\n        command=['sh', '-c'],\n        arguments=['echo \"hello world\"']\n    )\n\n@dsl.pipeline(\n    name='my-first-pipeline',\n    description='A hello world pipeline.'\n)\ndef hello_world_pipeline():\n    echo_task = echo_op()\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(hello_world_pipeline, __file__ + '.yaml')"
  },
  {
    "repo": "Natrofl/kubeflow-linear-pipline",
    "file_path": "convert.py",
    "raw_url": "https://raw.githubusercontent.com/Natrofl/kubeflow-linear-pipline/main/convert.py",
    "content": "import kfp\nimport kfp.dsl as dsl\nfrom kfp.components import func_to_container_op\n\n@func_to_container_op\ndef show_results(linear_regression : float) -> None:\n\n    print(f\"Linear regression (accuracy): {linear_regression}\")\n\n@dsl.pipeline(name='Boston Housing Pipeline', description='Test kubeflow pipline for Boston Housing dataset .')\ndef first_pipeline():\n\n    download = kfp.components.load_component_from_file('download_data/download_data.yaml')\n    linear_regression = kfp.components.load_component_from_file('linear_regression/linear_regression.yaml')\n\n    download_task = download()\n\n    linear_regression_task = linear_regression(download_task.output)\n\n    show_results(linear_regression_task.output)\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(first_pipeline, 'LinearPipline.yaml')\n"
  },
  {
    "repo": "TassaraR/vertex-kubeflow-pipelines-tutorial",
    "file_path": "model/pipelines/container-component-pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/TassaraR/vertex-kubeflow-pipelines-tutorial/main/model/pipelines/container-component-pipeline.py",
    "content": "import os\nfrom kfp.dsl import (\n    container_component,\n    ContainerSpec,\n    Input,\n    Output,\n    Dataset,\n    Artifact,\n    Model,\n    pipeline,\n)\n\nPIPELINE_NAME = \"containerized-penguins-pipeline\"\nBASE_BUCKET = \"gs://kfp-tutorial-b0e2a25a\"\nPIPELINE_ROOT = os.path.join(BASE_BUCKET, PIPELINE_NAME)\n\n\n@container_component\ndef preprocessing_component(\n    project_id: str,\n    input_path: str,\n    num_cols: str,\n    cat_cols: str,\n    label_col: str,\n    output_train_path: Output[Dataset],\n    output_test_path: Output[Dataset],\n    output_encoder_path: Output[Artifact],\n):\n    return ContainerSpec(\n        image=f\"us-central1-docker.pkg.dev/{project_id}/pipeline-tutorial/preprocessing:latest\",  # noqa: E501\n        command=[\"python\", \"runner.py\"],\n        args=[\n            \"--input-path\",\n            input_path,\n            \"--num-cols\",\n            num_cols,\n            \"--cat-cols\",\n            cat_cols,\n            \"--label-col\",\n            label_col,\n            \"--output-train-path\",\n            output_train_path.uri,\n            \"--output-test-path\",\n            output_test_path.uri,\n            \"--output-encoder-path\",\n            output_encoder_path.uri,\n        ],\n    )\n\n\n@container_component\ndef training_component(\n    project_id: str,\n    num_cols: str,\n    cat_cols: str,\n    label_col: str,\n    input_train: Input[Dataset],\n    input_test: Input[Dataset],\n    encoder_path: Input[Artifact],\n    output_model: Output[Model],\n):\n    return ContainerSpec(\n        image=f\"us-central1-docker.pkg.dev/{project_id}/pipeline-tutorial/training:latest\",\n        command=[\"python\", \"runner.py\"],\n        args=[\n            \"--input-train\",\n            input_train.uri,\n            \"--input-test\",\n            input_test.uri,\n            \"--num-cols\",\n            num_cols,\n            \"--cat-cols\",\n            cat_cols,\n            \"--label-col\",\n            label_col,\n            \"--encoder-path\",\n            encoder_path.uri,\n            \"--output-model\",\n            output_model.uri,\n        ],\n    )\n\n\n@pipeline(\n    name=PIPELINE_NAME,\n    description=\"Penguins tutorial pipeline\",\n    pipeline_root=PIPELINE_ROOT,\n)\ndef penguins_pipeline(\n    project_id: str,\n    input_path: str,\n    num_cols: str,\n    cat_cols: str,\n    label_col: str,\n):\n    preproc_step = preprocessing_component(\n        project_id=project_id,\n        input_path=input_path,\n        num_cols=num_cols,\n        cat_cols=cat_cols,\n        label_col=label_col,\n    )\n\n    train_step = training_component(  # noqa: F841\n        project_id=project_id,\n        input_train=preproc_step.outputs[\"output_train_path\"],\n        input_test=preproc_step.outputs[\"output_test_path\"],\n        encoder_path=preproc_step.outputs[\"output_encoder_path\"],\n        num_cols=num_cols,\n        cat_cols=cat_cols,\n        label_col=label_col,\n    )\n"
  },
  {
    "repo": "TassaraR/vertex-kubeflow-pipelines-tutorial",
    "file_path": "model/pipelines/lightweight-component-pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/TassaraR/vertex-kubeflow-pipelines-tutorial/main/model/pipelines/lightweight-component-pipeline.py",
    "content": "import os\nfrom kfp.dsl import (\n    component,\n    Input,\n    Output,\n    Dataset,\n    Artifact,\n    Model,\n    Metrics,\n    ClassificationMetrics,\n    pipeline,\n)\n\n\nPIPELINE_NAME = \"lightweight-penguins-pipeline\"\nBASE_BUCKET = \"gs://kfp-tutorial-b0e2a25a\"\nPIPELINE_ROOT = os.path.join(BASE_BUCKET, PIPELINE_NAME)\n\nREPO = \"us-central1-docker.pkg.dev\"\nPROJECT = \"tassarar-ml\"\nBASE_PATH = \"pipeline-tutorial\"\nIMG_BASE_PATH = os.path.join(REPO, PROJECT, BASE_PATH)\n\n\n@component(base_image=os.path.join(IMG_BASE_PATH, \"preprocessing:latest\"))\ndef preprocessing_component(\n    input_path: str,\n    num_cols: str,\n    cat_cols: str,\n    label_col: str,\n    output_train_path: Output[Dataset],\n    output_test_path: Output[Dataset],\n    output_encoder_path: Output[Artifact],\n):\n    import joblib\n    import pandas as pd\n    from prepare_data import PreprocessData\n\n    input_data = pd.read_csv(input_path)\n    parse_cat_cols = cat_cols.split(\",\")\n    parse_num_cols = num_cols.split(\",\")\n\n    preproc = PreprocessData(\n        input_data=input_data,\n        categorical_cols=parse_cat_cols,\n        numerical_cols=parse_num_cols,\n        label_col=label_col,\n    )\n    preproc.build()\n\n    ds_train, y_train = preproc.get_train_set()\n    ds_train[label_col] = y_train\n    ds_train.to_csv(output_train_path.path, index=False)\n\n    ds_test, y_test = preproc.get_test_set()\n    ds_test[label_col] = y_test\n    ds_test.to_csv(output_test_path.path, index=False)\n\n    joblib.dump(preproc.get_encoder(), output_encoder_path.path)\n\n    output_train_path.metadata[\"shape\"] = str(ds_train.shape)\n    output_train_path.metadata[\"columns\"] = ds_train.columns.tolist()\n    output_test_path.metadata[\"shape\"] = str(ds_test.shape)\n    output_test_path.metadata[\"columns\"] = ds_test.columns.tolist()\n    output_encoder_path.metadata[\"params\"] = str(preproc.get_encoder().classes_)\n\n\n@component(base_image=os.path.join(IMG_BASE_PATH, \"training:latest\"))\ndef training_component(\n    num_cols: str,\n    cat_cols: str,\n    label_col: str,\n    input_train: Input[Dataset],\n    input_test: Input[Dataset],\n    encoder_path: Input[Artifact],\n    output_model: Output[Model],\n    eval_metrics: Output[Metrics],\n    confusion_matrix: Output[ClassificationMetrics],\n):\n    import joblib\n    import pandas as pd\n    from trainer import create_pipeline, evaluate_model\n\n    encoder = joblib.load(encoder_path.path)\n\n    train = pd.read_csv(input_train.path)\n    train_set = train.drop(columns=[label_col])\n    train_label = train[label_col].to_numpy()\n\n    test = pd.read_csv(input_test.path)\n    test_set = test.drop(columns=[label_col])\n    test_label = test[label_col].to_numpy()\n\n    parse_cat_cols = cat_cols.split(\",\")\n    parse_num_cols = num_cols.split(\",\")\n\n    clf = create_pipeline(categorical_cols=parse_cat_cols, numeric_cols=parse_num_cols)\n\n    clf.fit(train_set, train_label)\n\n    metrics = evaluate_model(\n        model=clf, eval_set=test_set, eval_y=test_label, encoder=encoder\n    )\n\n    joblib.dump(clf, output_model.path)\n\n    # Kubeflow artifact's metrics & metadata\n    output_model.metadata[\"params\"] = str(clf.get_params())\n\n    eval_metrics.log_metric(\"accuracy\", metrics[\"metrics\"][\"accuracy\"])\n    for model_class, class_metrics in metrics[\"metrics\"].items():\n        if isinstance(class_metrics, dict):\n            for metric, score in class_metrics.items():\n                eval_metrics.log_metric(f\"{model_class} - {metric}\", score)\n\n    conf_matrix = metrics[\"confusion-matrix\"]\n    conf_matrix_labels = [lbl.split(\"_\")[1] for lbl in conf_matrix.index]\n\n    confusion_matrix.log_confusion_matrix(\n        conf_matrix_labels, conf_matrix.T.to_numpy().tolist()\n    )\n\n\n@pipeline(\n    name=PIPELINE_NAME,\n    description=\"Penguins tutorial pipeline\",\n    pipeline_root=PIPELINE_ROOT,\n)\ndef penguins_pipeline(\n    input_path: str,\n    num_cols: str,\n    cat_cols: str,\n    label_col: str,\n):\n\n    preproc_step = preprocessing_component(\n        input_path=input_path,\n        num_cols=num_cols,\n        cat_cols=cat_cols,\n        label_col=label_col,\n    )\n\n    train_step = training_component(  # noqa: F841\n        input_train=preproc_step.outputs[\"output_train_path\"],\n        input_test=preproc_step.outputs[\"output_test_path\"],\n        encoder_path=preproc_step.outputs[\"output_encoder_path\"],\n        num_cols=num_cols,\n        cat_cols=cat_cols,\n        label_col=label_col,\n    )\n"
  },
  {
    "repo": "stackdemos/centralstate-edu",
    "file_path": "components/golang/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/centralstate-edu/master/components/golang/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef processing_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('GOLANG_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=[script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n"
  },
  {
    "repo": "stackdemos/centralstate-edu",
    "file_path": "components/training/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/centralstate-edu/master/components/training/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef training_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('TRAINING_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=['/usr/local/bin/python', script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n\n"
  },
  {
    "repo": "HibernationNo1/project_4_kubeflow_pipeline",
    "file_path": "pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/HibernationNo1/project_4_kubeflow_pipeline/master/pipeline.py",
    "content": "\nimport kfp\nimport kfp.dsl as dsl\n \nimport argparse\n\nfrom component.record.record_op import record_op\nfrom component.train.train_op import train_op\nfrom component.evaluate.evaluate_op import evaluate_op\nfrom component.test.test_op import test_op\n\n\nfrom pipeline_config import set_config, CONFIGS\nfrom pipeline_utils import (connet_client, get_experiment, run_pipeline, upload_pipeline, set_intput_papams, \n                            kfb_print)\n\n\nfrom kubernetes.client.models import V1EnvVar, V1EnvVarSource, V1SecretKeySelector\nfrom kubernetes.client import V1Volume, V1EmptyDirVolumeSource\n\nSECRETS = dict()\n\n@dsl.pipeline(name=\"hibernation_project\")\ndef project_pipeline(run_flag: dict, \n                     input_cfg: dict):           \n\n    # persistance volume\n    pvc_cfg = CONFIGS['pipeline'].kbf.volume.pvc\n    pvc_volume = dsl.VolumeOp(name= pvc_cfg.name,\n                       resource_name= pvc_cfg.resource_name,\n                       modes= pvc_cfg.mode,\n                       storage_class = pvc_cfg.storage_class,\n                       size= pvc_cfg.size)\n    \n    \n    # for allocate shared memory\n    shm_volume_cfg = CONFIGS['pipeline'].kbf.volume.share_memory\n    shm_volume = dsl.PipelineVolume(\n        volume=V1Volume(\n            name= shm_volume_cfg.name,\n            empty_dir=V1EmptyDirVolumeSource(medium=shm_volume_cfg.medium))\n        )  \n\n    \n    # set secrets\n    client_sc_name = \"client-secrets\"\n    for secrets_cate, secrets_cfg in SECRETS.items():\n        for key in secrets_cfg:\n            SECRETS[secrets_cate][key] = V1EnvVar(name=key, value_from=V1EnvVarSource(secret_key_ref=V1SecretKeySelector(name=client_sc_name, key=key)))\n       \n    _record_op = record_op(input_cfg, run_flag) \\\n        .add_env_variable(SECRETS['gs'][\"type\"]) \\\n        .add_env_variable(SECRETS['gs'][\"project_id\"]) \\\n        .add_env_variable(SECRETS['gs'][\"private_key_id\"]) \\\n        .add_env_variable(SECRETS['gs'][\"private_key\"]) \\\n        .add_env_variable(SECRETS['gs'][\"client_email\"]) \\\n        .add_env_variable(SECRETS['gs'][\"client_id\"]) \\\n        .add_env_variable(SECRETS['gs'][\"auth_uri\"]) \\\n        .add_env_variable(SECRETS['gs'][\"token_uri\"]) \\\n        .add_env_variable(SECRETS['gs'][\"auth_provider_x509_cert_url\"]) \\\n        .add_env_variable(SECRETS['gs'][\"client_x509_cert_url\"]) \\\n        .add_env_variable(SECRETS['db'][\"password\"]) \\\n        .add_env_variable(SECRETS['db'][\"host\"]) \\\n        .add_env_variable(SECRETS['db'][\"port\"]) \\\n        .add_pvolumes({pvc_cfg.mount_path: pvc_volume.volume})\n    \n    _train_op = train_op(input_cfg, _record_op.outputs['run_flag'])\\\n        .add_env_variable(SECRETS['gs'][\"type\"]) \\\n        .add_env_variable(SECRETS['gs'][\"project_id\"]) \\\n        .add_env_variable(SECRETS['gs'][\"private_key_id\"]) \\\n        .add_env_variable(SECRETS['gs'][\"private_key\"]) \\\n        .add_env_variable(SECRETS['gs'][\"client_email\"]) \\\n        .add_env_variable(SECRETS['gs'][\"client_id\"]) \\\n        .add_env_variable(SECRETS['gs'][\"auth_uri\"]) \\\n        .add_env_variable(SECRETS['gs'][\"token_uri\"]) \\\n        .add_env_variable(SECRETS['gs'][\"auth_provider_x509_cert_url\"]) \\\n        .add_env_variable(SECRETS['gs'][\"client_x509_cert_url\"]) \\\n        .add_env_variable(SECRETS['db'][\"password\"]) \\\n        .add_env_variable(SECRETS['db'][\"host\"]) \\\n        .add_env_variable(SECRETS['db'][\"port\"]) \\\n        .add_pvolumes({shm_volume_cfg.path: shm_volume})\\\n        .add_pvolumes({pvc_cfg.mount_path: pvc_volume.volume})\n\n    _evaluate_op = evaluate_op(input_cfg, _train_op.outputs['run_flag'])\\\n        .add_env_variable(SECRETS['gs'][\"type\"]) \\\n        .add_env_variable(SECRETS['gs'][\"project_id\"]) \\\n        .add_env_variable(SECRETS['gs'][\"private_key_id\"]) \\\n        .add_env_variable(SECRETS['gs'][\"private_key\"]) \\\n        .add_env_variable(SECRETS['gs'][\"client_email\"]) \\\n        .add_env_variable(SECRETS['gs'][\"client_id\"]) \\\n        .add_env_variable(SECRETS['gs'][\"auth_uri\"]) \\\n        .add_env_variable(SECRETS['gs'][\"token_uri\"]) \\\n        .add_env_variable(SECRETS['gs'][\"auth_provider_x509_cert_url\"]) \\\n        .add_env_variable(SECRETS['gs'][\"client_x509_cert_url\"]) \\\n        .add_env_variable(SECRETS['db'][\"password\"]) \\\n        .add_env_variable(SECRETS['db'][\"host\"]) \\\n        .add_env_variable(SECRETS['db'][\"port\"]) \\\n        .add_pvolumes({shm_volume_cfg.path: shm_volume})\\\n        .add_pvolumes({pvc_cfg.mount_path: pvc_volume.volume})\n\n    _test_op = test_op(input_cfg, _evaluate_op.outputs['run_flag'])\\\n        .add_env_variable(SECRETS['gs'][\"type\"]) \\\n        .add_env_variable(SECRETS['gs'][\"project_id\"]) \\\n        .add_env_variable(SECRETS['gs'][\"private_key_id\"]) \\\n        .add_env_variable(SECRETS['gs'][\"private_key\"]) \\\n        .add_env_variable(SECRETS['gs'][\"client_email\"]) \\\n        .add_env_variable(SECRETS['gs'][\"client_id\"]) \\\n        .add_env_variable(SECRETS['gs'][\"auth_uri\"]) \\\n        .add_env_variable(SECRETS['gs'][\"token_uri\"]) \\\n        .add_env_variable(SECRETS['gs'][\"auth_provider_x509_cert_url\"]) \\\n        .add_env_variable(SECRETS['gs'][\"client_x509_cert_url\"]) \\\n        .add_env_variable(SECRETS['db'][\"password\"]) \\\n        .add_env_variable(SECRETS['db'][\"host\"]) \\\n        .add_env_variable(SECRETS['db'][\"port\"]) \\\n        .add_pvolumes({shm_volume_cfg.path: shm_volume})\\\n        .add_pvolumes({pvc_cfg.mount_path: pvc_volume.volume})\n\n\ndef _parse_args():\n    \n    parser = argparse.ArgumentParser()    \n    parser.add_argument(\"--cfg_pipeline\", help=\"name of config file which for pipeline\")       \n    parser.add_argument(\"--cfg_train\", help=\"name of config file which for training\")                           # TODO: rename\n    parser.add_argument(\"--cfg_record\", help=\"name of config file which for record\") \n    parser.add_argument(\"--cfg_test\", help=\"name of config file which for test\")  \n    parser.add_argument(\"--cfg_eval\", help=\"name of config file which for evaluate\") \n    \n    kbf_parser = parser.add_argument_group('kubeflow')\n    kbf_parser.add_argument(\"--dashboard_pw\", type = str , help=\"password of kubeflow dashboard\")        \n    kbf_parser.add_argument(\"--pipeline_v\", type = str, help=\"version of pipeline\")                    \n    kbf_parser.add_argument(\"--pipeline_n\", type = str, help=\"name of pipeline\")    \n    kbf_parser.add_argument(\"--experiment_n\", type = str, help=\"name of experiment\") \n    kbf_parser.add_argument(\"--run_n\", type = str, help=\"name of run\") \n\n    kbf_parser.add_argument(\"--katib\", action = 'store_true',  help=\"If run for experiment\") \n    # gs_parser = parser.add_argument_group('google_storage')\n\n\n    db_parser = parser.add_argument_group('database')\n    db_parser.add_argument('--name_db', type = str, help = 'Database name to connect to database')\n    db_parser.add_argument('--user_db', type = str, help = 'User name to connect to database')\n    \n    \n    train_parser = parser.add_argument_group('train')\n    train_parser.add_argument(\"--model\", type = str, choices = ['MaskRCNN'],\n                              help=\"Name of the model to be trained\") \n    train_parser.add_argument(\"--epoch\", type = int, help=\"epoch for training\") \n    train_parser.add_argument(\"--lr\", type = str, choices = ['0.0001', '0.0005', '0.001', '0.00005', '0.00001', '0.000005'],\n                              help=\"Regular learning rate\")             # why str?: To get linear value with katib.\n    train_parser.add_argument(\"--swin_drop_rate\", type = str, choices = ['0.0', '0.1', '0.2', '0.3', '0.4'],\n                               help=\"drop_rate of swin transformar\")         # why str?: To get linear value with katib.\n    train_parser.add_argument(\"--swin_window_size\", type = str, choices = ['5', '7', '9', '11', '13'],\n                              help=\"window_size of swin transformar\")     # why str?: To get linear value with katib.\n    train_parser.add_argument(\"--swin_mlp_ratio\", type = str, choices = ['3', '4', '5'],\n                              help=\"mlp_ratio of swin transformar\")         # why str?: To get linear value with katib.\n    \n    \n    \n    test_parser = parser.add_argument_group('test')\n    test_parser.add_argument(\"--model_path\", type = str, help = \"Path of trained model(.pth format)\")\n    \n    \n    swin_parser = parser.add_argument_group('SwinTransformer')\n    swin_parser.add_argument('--pm_dilation', type = int, help= \"dilation of SwinTransformer.PatchMerging\") \n    swin_parser.add_argument('--drop_rate', type = float, help= \"drop_rate of SwinTransformer\") \n    swin_parser.add_argument('--drop_path_rate', type = float, help= \"drop_path_rate of SwinTransformer\") \n    swin_parser.add_argument('--attn_drop_rate', type = float, help= \"attn_drop_rate of SwinTransformer.SwinBlockSequence.ShiftWindowMSA.WindowMSA\") \n    \n    args = parser.parse_args()\n    input_args = vars(args)     # TODO delete\n    \n    return args, input_args\n\n\n\n\nif __name__==\"__main__\":      \n    args, input_args = _parse_args()  \n    set_config(args)\n\n    \n    SECRETS['gs'] = dict(CONFIGS['pipeline'].secrets.gs)  \n    SECRETS['db'] = dict(CONFIGS['pipeline'].secrets.db)  \n    \n\n    cfg_pipeline = CONFIGS.get('pipeline', None)\n    kfb_print(\"connet to kubeflow client\")\n    client = connet_client(cfg_pipeline.kbf.dashboard)  \n        \n    kfb_print(\"compile pipeline\")             \n    kfp.compiler.Compiler().compile(\n        project_pipeline,\n        f\"./{cfg_pipeline.kbf.pipeline.pac}\"\n        )\n    \n    # get experiment id by create experiment or load experiment info\n    kfb_print(\"get experiment\")\n    experiment_id = get_experiment(client, cfg_pipeline)          \n\n    # get experiment id by create pipeline or updata pipeline version\n    pipeline_id = upload_pipeline(client, cfg_pipeline.kbf.pipeline)     \n     \n    params = set_intput_papams() \n\n    run_pipeline(client, cfg_pipeline.kbf, experiment_id, pipeline_id, params)\n    \n    \n    \n        \n\n"
  },
  {
    "repo": "HibernationNo1/project_4_kubeflow_pipeline",
    "file_path": "pipeline_config.py",
    "raw_url": "https://raw.githubusercontent.com/HibernationNo1/project_4_kubeflow_pipeline/master/pipeline_config.py",
    "content": "import os, os.path as osp\n\nfrom pipeline_base_config import Config\n\nCONFIGS = dict()     # parameters for pipeline run  \nMAP_CONFIG = \"config/map.py\"\n\ndef set_cfg_pipeline(args, cfg):\n    if args.pipeline_n is not None: cfg.kbf.pipeline.name = args.pipeline_n\n    cfg.kbf.pipeline.version =  args.pipeline_v\n    if args.experiment_n is not None: cfg.kbf.experiment.name = args.experiment_n\n    if args.run_n is not None: cfg.kbf.run.name = args.run_n    \n    cfg.kbf.dashboard.pw =  args.dashboard_pw \n    \n    if cfg.kbf.volume.get(\"pvc\", None) is not None:\n        import kfp.dsl as dsl\n        mode = cfg.kbf.volume.pvc.mode\n        if mode == 'VOLUME_MODE_RWO':\n            cfg.kbf.volume.pvc.mode = dsl.VOLUME_MODE_RWO\n        elif mode == 'VOLUME_MODE_RWM':\n            cfg.kbf.volume.pvc.mode = dsl.VOLUME_MODE_RWM\n        elif mode == 'VOLUME_MODE_ROM':\n            cfg.kbf.volume.pvc.mode = dsl.VOLUME_MODE_ROM\n            \n       \n            \ndef comman_set(cfg):\n    if CONFIGS['pipeline'] is not None:     # when run only by kubeflow pipeline\n        if CONFIGS['pipeline'].kbf.volume.get(\"pvc\", None) is not None:\n            cfg.path.volume = CONFIGS['pipeline'].kbf.volume.pvc.mount_path\n        \n        \ndef set_cfg_record(args, cfg):\n    assert cfg.dvc.record.train == cfg.record.train_dataset\n    assert cfg.dvc.record.val == cfg.record.val_dataset\n    \n    comman_set(cfg)\n    \n     \ndef set_cfg_train(args, cfg):\n    # set config of model to training \n    assert args.model, f\"Model to be trained must be specified!!\\n\"\\\n        f\"add `--model` option when entering the command.\"  \n    \n    if args.katib:\n        cfg.katib = True\n\n        # set 0 when running for katib or in pod (not have enough shared memory) \n        cfg.data.workers_per_gpu = 0\n\n    comman_set(cfg)\n    \n    map_cfg = Config.fromfile(MAP_CONFIG)\n    models_cfg_path = osp.join(os.getcwd(), \n                                map_cfg.dir.config.name, \n                                map_cfg.dir.config.models)       \n                             \n    \n    model_cfg = Config.fromfile(osp.join(models_cfg_path, f\"{args.model}.py\")) \n\n    for key, item in cfg.model.get(args.model).items():\n        sub_cfg = Config.fromfile(osp.join(models_cfg_path, key, f\"{item}.py\"))\n        if key == 'backbone':\n            model_cfg.model.backbone = sub_cfg.get(key)\n            \n        if key == 'neck':\n            model_cfg.model.neck = sub_cfg.get(key)\n    cfg.model = model_cfg.model\n    \n    if args.name_db is not None: cfg.db.db = args.name_db \n    if args.user_db is not None: cfg.db.user = args.user_db \n    \n    if args.epoch is not None: cfg.max_epochs = args.epoch\n\n    if args.lr is not None: cfg.optimizer.lr = float(args.lr)\n   \n    if cfg.model.backbone.type == \"SwinTransformer\":\n        if args.swin_drop_rate is not None : \n            cfg.model.backbone.drop_rate = float(args.swin_drop_rate)\n            assert 0.<=cfg.model.backbone.drop_rate and cfg.model.backbone.drop_rate < 0.999\n        if args.swin_window_size is not None : \n            cfg.model.backbone.window_size = int(args.swin_window_size)\n            assert cfg.model.backbone.window_size in [3, 5, 7, 9, 11, 13, 15]\n        if args.swin_mlp_ratio is not None : \n            cfg.model.backbone.mlp_ratio = int(args.swin_mlp_ratio)\n            assert cfg.model.backbone.mlp_ratio in [i for i in range(10)] \n         \n\n    # If get dataset with dvc, load the paths from the database.\n    # And all paths were set by dvc config\n    if cfg.get('dvc', None) is not None:\n        if args.cfg_pipeline is not None:\n            cfg.data.train.data_root = cfg.data.val.data_root = osp.join(cfg.git.dataset.repo,\n                                                                         cfg.dvc.record.dir,\n                                                                         cfg.dvc.category)\n            \n            \n            cfg.data.train.ann_file = osp.join(cfg.data.train.data_root, cfg.dvc.record.train)\n            cfg.data.val.ann_file = osp.join(cfg.data.val.data_root, cfg.dvc.record.val)  \n        else:\n            cfg.pop('dvc')\n  \n    if args.pm_dilation is not None: cfg.model.backbone.pm_dilation = args.pm_dilation\n    if args.drop_rate is not None: cfg.model.backbone.drop_rate = args.drop_rate\n    if args.drop_path_rate is not None: cfg.model.backbone.drop_path_rate = args.drop_path_rate\n    if args.attn_drop_rate is not None: cfg.model.backbone.attn_drop_rate = args.attn_drop_rate    \n    \n    if CONFIGS['pipeline'] is not None:     # when run only by kubeflow pipeline\n        if CONFIGS['pipeline'].kbf.volume.get('pvc', None) is not None:\n            for i, hook_cfg in enumerate(cfg.hook_configs):\n                if hook_cfg.type == \"TensorBoard_Hook\":\n                    cfg.hook_configs[i].pvc_dir = osp.join(CONFIGS['pipeline'].kbf.volume.pvc.mount_path,\n                                                          cfg.hook_configs[i].pvc_dir)\n                    break\n\n\ndef set_cfg_test(args, cfg):\n    if args.model_path is not None: cfg.model_path = args.model_path  \n    \n    print(f\"test: {cfg.get('data_root', None)}\")\n    \n\ndef sef_cfg_evaluate(args, cfg):\n    if args.model_path is not None: cfg.model_path = args.model_path\n\n    # If get dataset with dvc, load the paths from the database.\n    # And all paths were set by dvc config\n    if cfg.get('dvc', None) is not None:\n        if args.cfg_pipeline is not None:\n            # why set `cfg.data.train.data_root` even unused in `evaluate_op.py`?\n            # To prevent conflicts between configs in `combine_config`\n            cfg.data.train.data_root = cfg.data.val.data_root = osp.join(cfg.git.dataset.repo,\n                                                                         cfg.dvc.record.dir,\n                                                                         cfg.dvc.category)\n            \n            \n            cfg.data.train.ann_file = osp.join(cfg.data.train.data_root, cfg.dvc.record.train)\n            cfg.data.val.ann_file = osp.join(cfg.data.val.data_root, cfg.dvc.record.val) \n        else:\n            cfg.pop('dvc')\n    \n    \n\n\nCONFIG_SET_FUNCTION = dict(\n    pipeline = set_cfg_pipeline,\n    record = set_cfg_record,\n    train = set_cfg_train,\n    test = set_cfg_test,\n    evaluate = sef_cfg_evaluate\n)\n\n\ndef set_config(args):\n    \"\"\" \n        cfg arguments determines which component be run.\n        Components that matching cfg arguments which got `None` are excluded from the pipeline.\n        cfg arguments: is chooses in [args.cfg_train, args.cfg_record]\n    Args:\n        args : argparse\n    \"\"\"\n\n    if args.katib:\n        if args.model is None:\n            args.model = 'MaskRCNN' \n        if args.cfg_train is None:\n            args.cfg_train = 'config/train_cfg.py'\n\n \n    if (args.cfg_pipeline is not None) and (args.pipeline_v is not None) and (args.dashboard_pw is not None):\n        print(\"Run with kubeflow pipeline\")\n        CONFIGS['pipeline'] = args.cfg_pipeline\n        \n    elif (args.cfg_pipeline is None) and (args.pipeline_v is None) and (args.dashboard_pw is None):\n        print(f\"Run without kubeflow pipleine\")\n        CONFIGS['pipeline'] = None\n    else:\n        raise ValueError(f\"To run in pipeline of kubeflow, config, version and password of pipeline must be set.\\n\"\\\n                         f\"add options --cfg_pipeline, --pipeline_v, --dashboard_pw\")\n           \n    CONFIGS['train'] = args.cfg_train\n    CONFIGS['record'] = args.cfg_record\n    CONFIGS['test'] = args.cfg_test\n    CONFIGS['evaluate'] = args.cfg_eval \n\n    for key, func in CONFIG_SET_FUNCTION.items(): \n        if CONFIGS[key] is not None:\n            # Assign config only included in args \n            config =  Config.fromfile(CONFIGS[key])\n            func(args, config)\n        else: config = None\n        # CONFIGS[key] = False or Config\n        # if False, components matching the key will be passed from the pipeline.\n        # >>    example\n        # >>    CONFIGS[record] = False\n        # >>    `record_op` component will be passed from the pipeline.\n        CONFIGS[key] = config\n\n\n"
  },
  {
    "repo": "tanle2694/kubeflow-resnet-tf-pipeline",
    "file_path": "pipeline/src/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/tanle2694/kubeflow-resnet-tf-pipeline/master/pipeline/src/pipeline.py",
    "content": "# Copyright (c) 2019, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport kfp.dsl as dsl\nimport datetime\nimport os\nfrom kubernetes import client as k8s_client\n\n\n# Modify image='<image>' in each op to match IMAGE in the build.sh of its corresponding component\n\ndef PreprocessOp(name, input_dir, output_dir):\n    return dsl.ContainerOp(\n        name=name,\n        image='preprocess-image',\n        arguments=[\n            '--input_dir', input_dir,\n            '--output_dir', output_dir,\n        ],\n        file_outputs={'output': '/output.txt'}\n    )\n\n\ndef TrainOp(name, input_dir, output_dir, model_name, model_version, epochs):\n    return dsl.ContainerOp(\n        name=name,\n        image='train-image',\n        arguments=[\n            '--input_dir', input_dir,\n            '--output_dir', output_dir,\n            '--model_name', model_name,\n            '--model_version', model_version,\n            '--epochs', epochs\n        ],\n        file_outputs={'output': '/output.txt'}\n    )\n\n\ndef InferenceServerLauncherOp(name, input_dir, trtserver_name):\n    return dsl.ContainerOp(\n        name=name,\n        image='inference-server-launcher-image',\n        arguments=[\n            '--trtserver_name', trtserver_name,\n            '--model_path', input_dir,\n        ],\n        file_outputs={'output': '/output.txt'}\n    )\n\n\ndef WebappLauncherOp(name, trtserver_name, model_name, model_version, webapp_prefix, webapp_port):\n    return dsl.ContainerOp(\n        name=name,\n        image='webapp-launcher-image',\n        arguments=[\n            '--workflow_name', '{{workflow.name}}',\n            '--trtserver_name', trtserver_name,\n            '--model_name', model_name,\n            '--model_version', str(model_version),\n            '--webapp_prefix', webapp_prefix,\n            '--webapp_port', str(webapp_port)\n        ],\n        file_outputs={}\n    )\n\n\n@dsl.pipeline(\n    name='resnet_cifar10_pipeline',\n    description='Demonstrate an end-to-end training & serving pipeline using ResNet and CIFAR-10'\n)\ndef resnet_pipeline(\n    raw_data_dir='/mnt/workspace/raw_data',\n    processed_data_dir='/mnt/workspace/processed_data',\n    model_dir='/mnt/workspace/saved_model',\n    epochs=50,\n    trtserver_name='trtis',\n    model_name='resnet_graphdef',\n    model_version=1,\n    webapp_prefix='webapp',\n    webapp_port=80\n):\n\n    persistent_volume_name = 'nvidia-workspace'\n    persistent_volume_path = '/mnt/workspace'\n\n    op_dict = {}\n\n    op_dict['preprocess'] = PreprocessOp(\n        'preprocess', raw_data_dir, processed_data_dir)\n\n    op_dict['train'] = TrainOp(\n        'train', op_dict['preprocess'].output, model_dir, model_name, model_version, epochs)\n    op_dict['train'].execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n    op_dict['deploy_inference_server'] = InferenceServerLauncherOp(\n        'deploy_inference_server', op_dict['train'].output, trtserver_name)\n\n    op_dict['deploy_webapp'] = WebappLauncherOp(\n        'deploy_webapp', op_dict['deploy_inference_server'].output, model_name, model_version, webapp_prefix, webapp_port)\n\n    for _, container_op in op_dict.items():\n        container_op.add_volume(k8s_client.V1Volume(\n            host_path=k8s_client.V1HostPathVolumeSource(\n                path=persistent_volume_path),\n            name=persistent_volume_name))\n        container_op.add_volume_mount(k8s_client.V1VolumeMount(\n            mount_path=persistent_volume_path,\n            name=persistent_volume_name))\n\n\nif __name__ == '__main__':\n    import kfp.compiler as compiler\n    compiler.Compiler().compile(resnet_pipeline, __file__ + '.tar.gz')\n"
  },
  {
    "repo": "gnanimail/kubeflow-pipeline-aircraft-emission",
    "file_path": "pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/gnanimail/kubeflow-pipeline-aircraft-emission/main/pipeline.py",
    "content": "import kfp\nfrom kfp import dsl\nfrom kfp import compiler\n\ndef preprocess_op():\n\n    return dsl.ContainerOp(\n        name='Preprocess Data',\n        image='gcr.io/psychic-kite-329307/ae_pipeline_preprocessing',\n        arguments=[],\n        file_outputs={\n            'x_train': '/app/x_train.npy',\n            'x_test': '/app/x_test.npy',\n            'y_train': '/app/y_train.npy',\n            'y_test': '/app/y_test.npy',\n        }\n    )\n\ndef train_op(x_train, y_train):\n\n    return dsl.ContainerOp(\n        name='Train Model',\n        image='gcr.io/psychic-kite-329307/ae_pipeline_train',\n        arguments=[\n            '--x_train', x_train,\n            '--y_train', y_train\n        ],\n        file_outputs={\n            'model': '/app/model.pkl'\n        }\n    )\n\ndef test_op(x_test, y_test, model):\n\n    return dsl.ContainerOp(\n        name='Test Model',\n        image='gcr.io/psychic-kite-329307/ae_pipeline_test',\n        arguments=[\n            '--x_test', x_test,\n            '--y_test', y_test,\n            '--model', model\n        ],\n        file_outputs={}\n    )\n\ndef deploy_model_op(model):\n\n    return dsl.ContainerOp(\n        name='Deploy Model',\n        image='gcr.io/psychic-kite-329307/ae_pipeline_api',\n        arguments=[\n            '--model', model\n        ]\n    )\n\n@dsl.pipeline(\n   name='Aircraft Emission',\n   description='An example pipeline that trains and logs a regression model.'\n)\ndef aircraft_emission_pipeline():\n    _preprocess_op = preprocess_op()\n    \n    _train_op = train_op(\n        dsl.InputArgumentPath(_preprocess_op.outputs['x_train']),\n        dsl.InputArgumentPath(_preprocess_op.outputs['y_train'])\n    ).after(_preprocess_op)\n\n    _test_op = test_op(\n        dsl.InputArgumentPath(_preprocess_op.outputs['x_test']),\n        dsl.InputArgumentPath(_preprocess_op.outputs['y_test']),\n        dsl.InputArgumentPath(_train_op.outputs['model'])\n    ).after(_train_op)\n\n    deploy_model_op(\n        dsl.InputArgumentPath(_train_op.outputs['model'])\n    ).after(_test_op)\n\n#client = kfp.Client()\n#client.create_run_from_pipeline_func(boston_pipeline, arguments={})\ncompiler.Compiler().compile(aircraft_emission_pipeline, 'aircraft_emission_pipeline.yaml')\n"
  },
  {
    "repo": "Deeksha-5/Kubeflow-Model-Train-Pipeline",
    "file_path": "autoML.py",
    "raw_url": "https://raw.githubusercontent.com/Deeksha-5/Kubeflow-Model-Train-Pipeline/main/autoML.py",
    "content": "import kfp\nfrom kfp import dsl\nfrom kfp.components import func_to_container_op\nfrom google.oauth2 import service_account\n\ncredentials = service_account.Credentials.from_service_account_file('./pipeline-key.json')\n\nPROJECT_ID = 'MY_PROJECT_ID'\nCOMPUTE_REGION = 'MY_COMPUTE_REGION'\n\n\n@func_to_container_op\ndef create_dataset(Dname):\n    import subprocess\n\n    def install(name):\n        subprocess.call(['pip', 'install', name])\n    install('google-cloud-automl')\n    import google.cloud.automl as automl\n    client = automl.AutoMlClient(credentials=credentials)\n    project_location = client.location_path(PROJECT_ID, COMPUTE_REGION)\n    DATASET_NAME = Dname\n    dataset_metadata = {}\n    my_dataset = {\"display_name\": DATASET_NAME, \"image_object_detection_dataset_metadata\": dataset_metadata, }\n    response = client.create_dataset(project_location, my_dataset)\n    print(\"Dataset name: {}\".format(response.name))\n    print(\"Dataset id: {}\".format(response.name.split(\"/\")[-1]))\n    print(\"Dataset display name: {}\".format(response.display_name))\n    print(\"Image classification dataset metadata:\")\n    print(\"\\t{}\".format(response.image_classification_dataset_metadata))\n    print(\"Dataset example count: {}\".format(response.example_count))\n    dataset_id = response.name.split(\"/\")[-1]\n    print(type(dataset_id))\n    return dataset_id\n\n\n@func_to_container_op\ndef import_items(Id, url):\n    import subprocess\n\n    def install(name):\n        subprocess.call(['pip', 'install', name])\n    install('google-cloud-automl')\n    import google.cloud.automl as automl\n    client = automl.AutoMlClient(credentials=credentials)\n    dataset_full_id = client.dataset_path(PROJECT_ID, COMPUTE_REGION, Id)\n    CSV_DATASET = url\n    input_config = {\"gcs_source\": {\"input_uris\": [CSV_DATASET]}}\n    response = client.import_data(dataset_full_id, input_config)\n    print(\"Data imported. {}\".format(response.result()))\n\n\n@func_to_container_op\ndef train_model(NAME, did):\n    import subprocess\n\n    def install(name):\n        subprocess.call(['pip', 'install', name])\n    install('google-cloud-automl')\n    import google.cloud.automl as automl\n    client = automl.AutoMlClient(credentials=credentials)\n    project_location = client.location_path(PROJECT_ID, COMPUTE_REGION)\n    MODEL_NAME = NAME\n    my_model = {\"display_name\": MODEL_NAME, \"dataset_id\": did, \"image_object_detection_model_metadata\": {}}\n    response = client.create_model(project_location, my_model)\n    print(\"Training operation name: {}\".format(response.operation.name))\n    print(\"Training done. {}\".format(response.result()))\n    model_id = response.result().name.split(\"/\")[-1]\n    print(model_id)\n\n\n@dsl.pipeline(\n    name='AutoML Vision pipeline',\n    description='A pipeline with AutoML Image Classification model training \\\n    steps.'\n)\ndef sequential_pipeline(dname='NewDataset', url='gs://ml-pipeline/test.csv', mname='NewModel'):\n    did = create_dataset(dname)\n    import_items(did, url)\n    train_model(mname, did)\n\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(sequential_pipeline, __file__ + '.tar.gz')\n"
  },
  {
    "repo": "PascalSchroederDE/kf-sample-pipeline",
    "file_path": "ml_test_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/PascalSchroederDE/kf-sample-pipeline/master/ml_test_pipeline.py",
    "content": "import kfp\nfrom kfp import components\nfrom kfp import dsl\nfrom kfp import onprem\n\n\ndef train_op(epochs, validations, workers, pvc_path, trainset, input, filenames, target,train_size, learn_rate, output):\n  return dsl.ContainerOp(\n    name='Train',\n    image='pascalschroeder/ml-train-test',\n    arguments=[\n      '--epochs', epochs,\n      '--validations', validations,\n      '--workers', workers,\n      '--pvc_path', pvc_path,\n      '--trainset', trainset,\n      '--input', input,\n      '--filenames', filenames,\n      '--target', target,\n      '--train_size', train_size,\n      '--learn_rate', learn_rate,\n      '--output', output\n    ],\n    file_outputs={\n        'output': '/output.txt'\n     } \n  )\n\ndef load_op(workers, pvc_path, testset, input, filenames, target, model, result):\n  return dsl.ContainerOp(\n    name='Load',\n    image='pascalschroeder/ml-load-test',\n    arguments=[\n      '--workers', workers,\n      '--pvc_path', pvc_path,\n      '--testset', testset,\n      '--input', input,\n      '--filenames', filenames,\n      '--target', target,\n      '--model', model,\n      '--output', result\n    ],\n  )\n\n\n@dsl.pipeline(\n  name='ML Test Pipeline',\n  description='Test'\n)\ndef train_pipeline(output=\"/mnt/model.h5\", result=\"/mnt/results.txt\", pvc_name=\"train-vol\", pvc_path=\"/mnt\", epochs=30, validations=10, trainset='/cut', testset='/cut', input='/train.csv', filenames='id', target='has_scratch', train_size=0.8, learn_rate=0.0001, workers=2):\n  train = train_op(epochs, validations, workers,  pvc_path, trainset, input, filenames, target, train_size, learn_rate, output).apply(onprem.mount_pvc(\"train-vol\", 'local-storage', \"/mnt\"))\n  load = load_op(workers, pvc_path, testset, input, filenames, target, train.outputs['output'], result).apply(onprem.mount_pvc(\"train-vol\", 'local-storage', \"/mnt\"))\n\nif __name__ == '__main__':\n  kfp.compiler.Compiler().compile(train_pipeline, __file__ + '.tar.gz')"
  },
  {
    "repo": "PascalSchroederDE/kf-sample-pipeline",
    "file_path": "pipeline-fashion-mnist.py",
    "raw_url": "https://raw.githubusercontent.com/PascalSchroederDE/kf-sample-pipeline/master/pipeline-fashion-mnist.py",
    "content": "import kfp\nfrom kfp import components\nfrom kfp import dsl\nfrom kfp import onprem\n\ndef data_prep_op(dataset_location, output):\n  return dsl.ContainerOp(\n    name='Data preparation',\n    image='pascalschroeder/kf-dataprep',\n    arguments=[\n      '--dataset_location', dataset_location,\n      '--output', output\n    ],\n    file_outputs={\n      'output': '/prepdf_output.txt'\n    }\n  )\n\ndef feature_eng_op(prep_dataset_location, output):\n  return dsl.ContainerOp(\n    name='Feature Engineering',\n    image='pascalschroeder/kf-featureeng',\n    arguments=[\n      '--dataset_location', prep_dataset_location,\n      '--output', output\n    ],\n    file_outputs={\n      'output': '/findf_output.txt'\n    }\n  )\n\ndef data_split_op(impr_dataset_location, test_size, random_state, output_train_img, output_train_label, output_test_img, output_test_label):\n  return dsl.ContainerOp(\n    name='Data Split',\n    image='pascalschroeder/kf-datasplit',\n    arguments=[\n      '--dataset_location', impr_dataset_location,\n      '--test_size', test_size,\n      '--random_state', random_state,\n      '--output_train_img', output_train_img,\n      '--output_train_label', output_train_label,\n      '--output_test_img', output_test_img,\n      '--output_test_label', output_test_label\n    ],\n    file_outputs={\n      'train_img': '/trainimg.txt',\n      'train_label': '/trainlabel.txt',\n      'test_img': '/testimg.txt',\n      'test_label': '/testlabel.txt',\n    }\n  )\n\ndef model_build_op(input_shape_height, input_shape_width, num_units, num_outputs, activation_l2, activation_l3, optimizer, loss, metrics, model_output):\n  return dsl.ContainerOp(\n    name='Model building',\n    image='pascalschroeder/kf-modelbuild',\n    arguments=[\n      '--input_shape_height', input_shape_height,\n      '--input_shape_width', input_shape_width,\n      '--num_units', num_units,\n      '--num_outputs', num_outputs,\n      '--activation_l2', activation_l2,\n      '--activation_l3', activation_l3,\n      '--optimizer', optimizer,\n      '--loss', loss,\n      '--metrics', metrics,\n      '--output', model_output\n    ],\n    file_outputs={\n      'output_model_loc': '/model.txt',\n    }\n  )\n\ndef model_download_op(input_shape_height, input_shape_width, model_output):\n  return dsl.ContainerOp(\n    name='Model download',\n    image='pascalschroeder/kf-modeldownload',\n    arguments=[\n      '--input_shape_height', input_shape_height,\n      '--input_shape_width', input_shape_width,\n      '--output', model_output\n    ],\n    file_outputs={\n      'output_model_loc': '/model.txt',\n    }\n  )\n\ndef model_train_op(input_train_img, input_train_label, input_shape_height, input_shape_width, model_location, epochs, model_output):\n  return dsl.ContainerOp(\n    name='Model training',\n    image='pascalschroeder/kf-modeltrain',\n    arguments=[\n      '--input_train_img', input_train_img,\n      '--input_train_label', input_train_label,\n      '--input_shape_height', input_shape_height,\n      '--input_shape_width', input_shape_width,\n      '--model_location', model_location,\n      '--epochs', epochs,\n      '--output', model_output\n    ],\n    file_outputs={\n      'output_model_loc': '/trained_model.txt',\n    }\n  )\n\ndef model_eval_op(input_test_img, input_test_label, input_shape_height, input_shape_width, model_location, result_output):\n  return dsl.ContainerOp(\n    name='Model evaluation',\n    image='pascalschroeder/kf-modeleval',\n    arguments=[\n      '--input_test_img', input_test_img,\n      '--input_test_label', input_test_label,\n      '--input_shape_height', input_shape_height,\n      '--input_shape_width', input_shape_width,\n      '--model_location', model_location,\n      '--output', result_output\n    ],\n    output_artifact_paths={\n            'mlpipeline-ui-metadata': '/mlpipeline-ui-metadata.json',\n    },\n  )\n\n@dsl.pipeline(\n  name='ML Test Pipeline',\n  description='Test'\n)\n\ndef pipeline(dataset_location='/mnt/data/manipulated_fashion_mnist.csv', test_size=0.3, random_state=42, input_shape_height=28, input_shape_width=28, use_pretrained_model='False', model_units_num=128, model_outputs_num=10, model_activation_func_layer2='relu', model_activation_func_layer3='softmax', optimizer='adam', loss='binary_crossentropy', metrics='accuracy', num_epochs=10, location_prepared_dataset='/mnt/data/prep_fashion_mnist.csv', location_improved_dataset='/mnt/data/impr_fasion_mnist.csv', location_training_images='/mnt/data/train_img.csv', location_training_labels='/mnt/data/train_labels.csv', location_test_images='/mnt/data/test_img.csv', location_test_labels='/mnt/data/test_labels.csv', location_base_model='/mnt/model/base_model.h5', location_trained_model='/mnt/model/trained_model.h5'):\n  data_preparation = data_prep_op(dataset_location, location_prepared_dataset).apply(onprem.mount_pvc(\"fashion-mnist-vol\", 'local-storage', \"/mnt\"))\n  feature_engineering = feature_eng_op(data_preparation.outputs['output'], location_improved_dataset).apply(onprem.mount_pvc(\"fashion-mnist-vol\", 'local-storage', \"/mnt\"))\n  data_split = data_split_op(feature_engineering.outputs['output'], test_size, random_state, location_training_images, location_training_labels, location_test_images, location_test_labels).apply(onprem.mount_pvc(\"fashion-mnist-vol\", 'local-storage', \"/mnt\"))\n  \n  with dsl.Condition(use_pretrained_model == 'True'):\n    model_building = model_download_op(input_shape_height, input_shape_width, location_base_model).apply(onprem.mount_pvc(\"fashion-mnist-vol\", 'local-storage', \"/mnt\"))\n    model_training = model_train_op(data_split.outputs['train_img'], data_split.outputs['train_label'], input_shape_height, input_shape_width, model_building.outputs['output_model_loc'], num_epochs, location_trained_model).apply(onprem.mount_pvc(\"fashion-mnist-vol\", 'local-storage', \"/mnt\"))\n    model_evaluation = model_eval_op(data_split.outputs['test_img'], data_split.outputs['test_label'], input_shape_height, input_shape_width, model_training.outputs['output_model_loc'], '/mlpipeline-ui-metadata.json').apply(onprem.mount_pvc(\"fashion-mnist-vol\", 'local-storage', \"/mnt\"))\n\n\n  with dsl.Condition(use_pretrained_model == 'False'):\n    model_building = model_build_op(input_shape_height, input_shape_width, model_units_num, model_outputs_num, model_activation_func_layer2, model_activation_func_layer3, optimizer, loss, metrics, location_base_model).apply(onprem.mount_pvc(\"fashion-mnist-vol\", 'local-storage', \"/mnt\"))\n    model_training = model_train_op(data_split.outputs['train_img'], data_split.outputs['train_label'], input_shape_height, input_shape_width, model_building.outputs['output_model_loc'], num_epochs, location_trained_model).apply(onprem.mount_pvc(\"fashion-mnist-vol\", 'local-storage', \"/mnt\"))\n    model_evaluation = model_eval_op(data_split.outputs['test_img'], data_split.outputs['test_label'], input_shape_height, input_shape_width, model_training.outputs['output_model_loc'], '/mlpipeline-ui-metadata.json').apply(onprem.mount_pvc(\"fashion-mnist-vol\", 'local-storage', \"/mnt\"))\n\n\n\nif __name__ == '__main__':\n  kfp.compiler.Compiler().compile(pipeline, __file__ + '.tar.gz')"
  },
  {
    "repo": "stackdemos/got-image-classification",
    "file_path": "components/golang/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/got-image-classification/master/components/golang/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef processing_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('GOLANG_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=[script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n"
  },
  {
    "repo": "stackdemos/got-image-classification",
    "file_path": "components/training/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/got-image-classification/master/components/training/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef training_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('TRAINING_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=['/usr/local/bin/python', script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n\n"
  },
  {
    "repo": "drbwa/kfp-hello-world",
    "file_path": "kfp_hello_world/kfp_hello_world.py",
    "raw_url": "https://raw.githubusercontent.com/drbwa/kfp-hello-world/main/kfp_hello_world/kfp_hello_world.py",
    "content": "from kfp import dsl\nfrom kfp import compiler\nfrom kfp import Client\n\n# One component used in a simple pipeline. Compiles pipeline to YAML and creates \n# a run from the pipeline YAML.\n\n@dsl.component\ndef say_hello(name: str) -> str:\n    hello_text = f\"Hello, {name}!\"\n    print(hello_text)\n    return hello_text\n\n\n@dsl.pipeline\ndef hello_pipeline(recipient: str) -> str:\n    hello_task = say_hello(name=recipient)\n    return hello_task.output\n\n\ndef compile(pipeline_name: str):\n    compiler.Compiler().compile(hello_pipeline, pipeline_name)\n\n\ndef run(endpoint: str, pipeline_name: str) -> str:\n    client = Client(host=endpoint)\n    run = client.create_run_from_pipeline_package(\n        pipeline_name, \n        arguments={\n            \"recipient\": \"World\",\n        },\n    )\n    url = f\"{endpoint}/#/runs/details/{run.run_id}\"\n    return url\n\n\ndef main():\n    pipeline_name = \"pipeline.yaml\"\n    endpoint = \"http://localhost:8080\"\n    compile(pipeline_name)\n    url = run(endpoint, pipeline_name)\n    print(url)\n\n\nif __name__ == \"__main__\":\n    main()\n"
  },
  {
    "repo": "drbwa/kfp-hello-world",
    "file_path": "kfp_hello_world/kfp_hello_world2.py",
    "raw_url": "https://raw.githubusercontent.com/drbwa/kfp-hello-world/main/kfp_hello_world/kfp_hello_world2.py",
    "content": "from kfp import dsl\nfrom kfp import Client\n\n# Simple pipeline that chains to invocations of a single component to each\n# other. Does not produce KFP IR through compilation and instead creates a run\n# directly from the pipeline function defined here.\n\n@dsl.component\ndef addition_component(num1: int, num2: int) -> int:\n    return num1 + num2\n\n@dsl.pipeline(name='addition-pipeline')\ndef my_pipeline(a: int, b: int, c: int = 10):\n    add_task_1 = addition_component(num1=a, num2=b)\n    add_task_2 = addition_component(num1=add_task_1.output, num2=c)\n\ndef run(endpoint: str, pipeline) -> str:\n    client = Client(host=endpoint)\n    run = client.create_run_from_pipeline_func(\n        pipeline, \n        arguments={\n            \"a\": 1,\n            \"b\": 2\n        },\n    )\n    url = f\"{endpoint}/#/runs/details/{run.run_id}\"\n    return url\n\ndef main():\n    endpoint = \"http://localhost:8080\"\n    pipeline = my_pipeline\n    url = run(endpoint, pipeline)\n    print(url)\n\nif __name__ == \"__main__\":\n    main()"
  },
  {
    "repo": "hh10/Minimal-Kubeflow-Pipeline-Template",
    "file_path": "kf_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/hh10/Minimal-Kubeflow-Pipeline-Template/main/kf_pipeline.py",
    "content": "import os\nimport sys\nimport kfp\nfrom kfp import dsl\nfrom kubernetes.client.models import V1Volume, V1VolumeMount\n\n_IMAGE = \"eu.gcr.io/grand-kingdom-352313/test_proj_image2\"  # change to your project/image path\n_PERSISTENT_VOL_CLAIM_PATH = \"/mnt/pvolume\"\n\n\ndef create_volume(size: str, vol_res_name: str = '', datasource_res_name: str = '', last_op=None):\n    assert not vol_res_name or not datasource_res_name, print(\"Provide either 'vol_res_name' for new volume creation or 'datasource_res_name' for cloning it, not both\")\n\n    vname, data_source, access_mode = f\"shared-volume-{size}\", None, dsl.VOLUME_MODE_RWO\n    if datasource_res_name:\n        # a PVC claim to clone is provided\n        data_source = \"{{workflow.name}}-\" + datasource_res_name\n        vol_res_name = datasource_res_name + \"-clone\"\n        vname += \"-clone\"\n        access_mode = dsl.VOLUME_MODE_ROM\n\n    vop = dsl.VolumeOp(\n        name=vname,\n        resource_name=vol_res_name,\n        storage_class=\"standard-rwo\",\n        data_source=data_source,\n        modes=access_mode,\n        size=size,\n    )\n    return vop.after(last_op) if last_op else vop, vol_res_name\n\n\ndef release_pvc(pvc_res_name: str, last_op):\n    pvc_release_cmd = 'kubectl delete pvc {{workflow.name}}-%s -n kubeflow --wait=false && \\\n                       kubectl patch pvc {{workflow.name}}-%s -n kubeflow -p \\'{\"metadata\":{\"finalizers\": []}}\\' --type=merge' % (pvc_res_name, pvc_res_name)\n    return dsl.ContainerOp(\n        name=\"release-shared-volume\",\n        image=\"google/cloud-sdk:216.0.0\",\n        command=[\"bash\", \"-c\"],\n        arguments=[pvc_release_cmd]).after(last_op)\n\n\ndef clone_repo(vop: dsl.VolumeOp, git_repo: str):\n    repo_name = \"cloned_repo\"\n    repo_path = os.path.join(_PERSISTENT_VOL_CLAIM_PATH, repo_name)\n    cases_outpath = \"/tmp/test.config\"\n    get_cases_args = f\"rm -r {repo_path}; \\\n                       cat /root/.ssh/id_rsa; \\\n                       echo git clone {git_repo} {repo_path}; \\\n                       git clone {git_repo} {repo_path}; \\\n                       ls {repo_path}; \\\n                       python3 -c \\\"import os; import json; json.dump([os.path.join('{repo_path}', f) for f in os.listdir('{repo_path}') if os.path.isfile(os.path.join('{repo_path}', f))], open('{cases_outpath}', 'w'))\\\"; \\\n                       cat {cases_outpath};\"\n    return dsl.ContainerOp(\n        name=\"get-files-from-repo\",\n        image=_IMAGE,\n        command=[\"bash\", \"-c\"],\n        arguments=[f\"{get_cases_args}\"],\n        file_outputs={\"config\": \"/tmp/test.config\"},\n        pvolumes={_PERSISTENT_VOL_CLAIM_PATH: vop.volume},\n    )\n\n\ndef node_op(filepath: str, gcs_results_location: str, vop_clone: dsl.VolumeOp, vol_clone_res_name: str):\n    srun = dsl.ContainerOp(\n        name=\"single-file-upload\",\n        image=_IMAGE,\n        command=[\"bash\", \"-c\"],\n        arguments=[f'ls /mnt/pvolume/cloned_repo && sleep 15 && gsutil cp \"{filepath}\" \"{gcs_results_location}\"'],\n    )\n    # set these resource requests/limits to ensure that each pod is assigned to a single node/machine \n    srun.set_cpu_request(\"500m\")\n    srun.set_cpu_limit(\"800m\")\n    srun.add_volume(\n        V1Volume(\n            name=vop_clone.name,\n            persistent_volume_claim={\"claimName\": \"{{workflow.name}}-\" + vol_clone_res_name, \"readOnly\": True},\n        )\n    )\n    srun.add_volume_mount(V1VolumeMount(mount_path=_PERSISTENT_VOL_CLAIM_PATH, name=vop_clone.name, read_only=True))\n    return srun\n\n\n@dsl.pipeline(name=\"Verification pipeline\", description=\"A simple pipeline that fetches a repo and uploads its files to a GCP bucket in parallel.\")\ndef si_verpipeline(\n    git_repo: str = \"git@github.com:hh10/AIHack2022.git\",\n    gcs_results_location: str = \"gs://si-testbucket-1\",\n    run_label: str = \"default\",\n):\n    vol_size = \"1Gi\"\n    vop, vol_res_name = create_volume(vol_size, vol_res_name=\"pvc\")\n    repo_files = clone_repo(vop, git_repo)\n    \n    gcs_results_location = f\"{gcs_results_location}/{run_label}/\"\n    vop_clone, vol_clone_res_name = create_volume(vol_size, datasource_res_name=vol_res_name, last_op=repo_files)\n    with dsl.ParallelFor(repo_files.output).after(vop_clone) as filepath:\n        single_result = node_op(filepath, gcs_results_location, vop_clone, vol_clone_res_name)\n    # cleanup pv(c)s\n    vop_clone_release = release_pvc(vol_clone_res_name, single_result)\n    release_pvc(vol_res_name, vop_clone_release)\n\n\nif __name__ == \"__main__\":\n    assert len(sys.argv) == 2, print(\"only one input, i.e., output yaml path should be provided.\")\n    output_path = sys.argv[1]\n    kfp.compiler.Compiler().compile(si_verpipeline, output_path)\n    os.system(\n        \"sed -E -i \\\"s/^( *)add: \\\\['- SYS_ADMIN'\\\\]/\\\\1add:\\\\n\\\\1- SYS_ADMIN/g\\\" {pipeline}\".format(\n            pipeline=output_path\n        )\n    )\n"
  },
  {
    "repo": "shahriar0999/ml-pipeline-with-kubeflow",
    "file_path": "kubeflow_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/shahriar0999/ml-pipeline-with-kubeflow/main/kubeflow_pipeline.py",
    "content": "from typing import Dict, List\nfrom kfp import dsl\nfrom kfp import compiler\nimport kfp\nfrom kfp.dsl import Input, Output, Dataset, Model, component\n\n# Step 1: Load Dataset\n@dsl.component(base_image=\"python:3.9\")\ndef load_data(output_csv: Output[Dataset]):\n    import subprocess\n    subprocess.run([\"pip\", \"install\", \"pandas\", \"scikit-learn\"], check=True)\n\n    from sklearn.datasets import load_iris\n    import pandas as pd\n    iris = load_iris()\n    df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n    df['target'] = iris.target\n\n    # Save the dataset to the output artifact path\n    df.to_csv(output_csv.path, index=False)\n\n# Step 2: Preprocess Data\n@dsl.component(base_image=\"python:3.9\")\ndef preprocess_data(input_csv: Input[Dataset], output_train: Output[Dataset], output_test: Output[Dataset], \n                    output_ytrain: Output[Dataset], output_ytest: Output[Dataset]):\n    import subprocess\n    subprocess.run([\"pip\", \"install\", \"pandas\", \"scikit-learn\"], check=True)\n\n    import pandas as pd\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.model_selection import train_test_split\n\n    # Load dataset\n    df = pd.read_csv(input_csv.path)\n\n    # Debug: Check for NaN values\n    print(\"Initial dataset shape:\", df.shape)\n    print(\"Missing values before preprocessing:\\n\", df.isnull().sum())\n\n    # Handle missing values\n    if df.isnull().values.any():\n        print(\"Missing values detected. Handling them...\")\n        df = df.dropna()  # Drop rows with any NaN values\n    \n    # Validate that there are no NaNs in the target column\n    assert not df['target'].isnull().any(), \"Target column contains NaN values after handling missing values.\"\n\n    features = df.drop(columns=['target'])\n    target = df['target']\n\n    # Standardize features\n    scaler = StandardScaler()\n    scaled_features = scaler.fit_transform(features)\n\n    # Train-test split\n    X_train, X_test, y_train, y_test = train_test_split(scaled_features, target, test_size=0.2, random_state=42)\n\n    # Debug: Validate splits\n    print(\"Shapes after train-test split:\")\n    print(\"X_train:\", X_train.shape, \"X_test:\", X_test.shape) \n    print(\"y_train:\", y_train.shape, \"y_test:\", y_test.shape)\n    print(\"Missing values in y_train:\", y_train.isnull().sum())\n\n    # Ensure no NaNs in the split data\n    assert not y_train.isnull().any(), \"y_train contains NaN values.\"\n    assert not y_test.isnull().any(), \"y_test contains NaN values.\"\n\n    # Create DataFrames for train and test sets\n    X_train_df = pd.DataFrame(X_train, columns=features.columns)\n    print(\"X_train_df:\", X_train_df) \n\n    y_train_df = pd.DataFrame(y_train) \n    print(\"y_train_df: \", y_train_df)  \n\n    X_test_df = pd.DataFrame(X_test, columns=features.columns)\n    print(\"X_test_df:\", X_test_df) \n\n    y_test_df = pd.DataFrame(y_test) \n    print(\"y_test_df: \", y_test_df) \n\n    # Save processed train and test data\n    X_train_df.to_csv(output_train.path, index=False)  \n    X_test_df.to_csv(output_test.path, index=False)\n\n    y_train_df.to_csv(output_ytrain.path, index=False)  \n    y_test_df.to_csv(output_ytest.path, index=False) \n\n# Step 3: Train Model\n@dsl.component(base_image=\"python:3.9\")\ndef train_model(train_data: Input[Dataset], ytrain_data: Input[Dataset], model_output: Output[Model]):\n\n    import subprocess\n    subprocess.run([\"pip\", \"install\", \"pandas\", \"scikit-learn\", \"joblib\"], check=True)\n\n    import pandas as pd\n    from sklearn.linear_model import LogisticRegression\n    from joblib import dump\n\n    # Load training data\n    train_df = pd.read_csv(train_data.path)\n    print(\"Shape of train_df:\", train_df.shape)\n    print(\"train_df:\", train_df)\n    X_train = train_df \n\n    y_train = pd.read_csv(ytrain_data.path)\n    print(\"Shape of ytrain_df:\", y_train.shape)\n    print(\"y_train_df:\", y_train)\n\n    # Debug: Validate splits\n    print(\"Shapes of X_train and y_train: \")\n    print(\"X_train:\", X_train.shape)\n    print(\"y_train:\", y_train.shape) \n    print(\"Missing values in X_train:\", X_train.isnull().sum())\n    print(\"Missing values in y_train:\", y_train.isnull().sum()) \n\n    # Ensure no NaN values\n    assert not X_train.isnull().values.any(), \"X_train contains NaN values.\"\n    assert not y_train.isnull().values.any(), \"y_train contains NaN values.\" \n\n    # Train model\n    model = LogisticRegression()\n    model.fit(X_train, y_train)\n\n    # Save model\n    dump(model, model_output.path)\n\n# Step 4: Evaluate Model\n@dsl.component(base_image=\"python:3.9\")\ndef evaluate_model(test_data: Input[Dataset], ytest_data: Input[Dataset], model: Input[Model], metrics_output: Output[Dataset]):\n    import subprocess\n    subprocess.run([\"pip\", \"install\", \"pandas\", \"scikit-learn\", \"matplotlib\", \"joblib\"], check=True)\n\n    import pandas as pd\n    from sklearn.metrics import classification_report, confusion_matrix\n    import matplotlib.pyplot as plt\n    from joblib import load\n\n    # Load test data\n    X_test = pd.read_csv(test_data.path)\n\n    y_test = pd.read_csv(ytest_data.path)  \n\n    # Load model\n    model = load(model.path)\n\n    # Predict\n    y_pred = model.predict(X_test)\n\n    # Generate metrics\n    report = classification_report(y_test, y_pred, output_dict=True)\n    cm = confusion_matrix(y_test, y_pred)\n\n    # Save metrics to a file\n    metrics_path = metrics_output.path\n    with open(metrics_path, 'w') as f:\n        f.write(str(report))\n\n    # Plot confusion matrix\n    plt.figure(figsize=(8, 6))\n    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n    plt.title('Confusion Matrix')\n    plt.colorbar()\n    plt.xlabel('Predicted Label')\n    plt.ylabel('True Label')\n    plt.savefig(metrics_path.replace('.txt', '.png'))\n\n# Define the pipeline\n@dsl.pipeline(name=\"ml-pipeline\")\ndef ml_pipeline():\n    # Step 1: Load Dataset\n    load_op = load_data()\n\n    # Step 2: Preprocess Data\n    preprocess_op = preprocess_data(input_csv=load_op.outputs[\"output_csv\"])\n\n    # Step 3: Train Model\n    train_op = train_model(train_data=preprocess_op.outputs[\"output_train\"], ytrain_data=preprocess_op.outputs[\"output_ytrain\"])\n\n    # Step 4: Evaluate Model\n    evaluate_op = evaluate_model(test_data=preprocess_op.outputs[\"output_test\"], ytest_data=preprocess_op.outputs[\"output_ytest\"], model=train_op.outputs[\"model_output\"]) \n\n# Compile the pipeline\nif __name__ == \"__main__\":\n    compiler.Compiler().compile(pipeline_func=ml_pipeline, package_path=\"kubeflow_pipeline.yaml\")\n"
  },
  {
    "repo": "quynhtl/build-pipeline-on-Kubeflow",
    "file_path": "pipeline01.py",
    "raw_url": "https://raw.githubusercontent.com/quynhtl/build-pipeline-on-Kubeflow/main/pipeline01.py",
    "content": "\nfrom kfp import dsl\nfrom kfp import compiler\nimport os\n\n\n@dsl.pipeline(name='My Pipeline', description='A pipeline with 3 steps')\ndef my_pipeline():\n    data_prep = dsl.ContainerOp(\n        name='Data Preparation',\n        image='quynhtl/today_code1:latest',\n        command=['python', 'download.py'],\n        file_outputs = {\"output\": \"/data\"}\n    )\n\n    model_train = dsl.ContainerOp(\n        name='Model Training',\n        image='quynhtl/today_code1:latest',\n        command=['python', 'model_train.py'],\n        arguments=['--input-folder', data_prep.output]\n    )\n    model_train.after(data_prep)\n\n    # model_version = dsl.ContainerOp(\n    #     name='Model Versioning',\n    #     image='quynhtl/today_code1:latest',\n    #     command=['python', 'version_data.py'],\n    #     arguments=['--model-dir', model_train.outputs['model_path'], '--version-dir', '/version']\n    # )\n    # model_version.after(model_train)\n\n\nif __name__ == '__main__':\n    compiler.Compiler().compile(my_pipeline, 'my_pipeline.zip')"
  },
  {
    "repo": "quynhtl/build-pipeline-on-Kubeflow",
    "file_path": "pipeline_base/test1.py",
    "raw_url": "https://raw.githubusercontent.com/quynhtl/build-pipeline-on-Kubeflow/main/pipeline_base/test1.py",
    "content": "\nimport kfp\nfrom kfp import dsl\nimport kfp.components as comp\n\n\n@comp.create_component_from_func\ndef echo_op():\n    print(\"Hello world\")\n\n@dsl.pipeline(\n    name='my-first-pipeline',\n    description='A hello world pipeline.'\n)\ndef hello_world_pipeline():\n    echo_task = echo_op()\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(hello_world_pipeline, __file__ + '.yaml')"
  },
  {
    "repo": "ruteee/kubeflow-pipeline-gcp-deploy",
    "file_path": "source/main.py",
    "raw_url": "https://raw.githubusercontent.com/ruteee/kubeflow-pipeline-gcp-deploy/main/source/main.py",
    "content": "import logging\nimport sys\nfrom kfp.v2 import compiler, dsl\nfrom kfp.v2.dsl import Input, Output, Dataset, Model\nfrom google.cloud import aiplatform\n\n\nlogging.basicConfig(level=\"INFO\", stream=sys.stdout)\n\ndef compile_pipeline(pipeline_func):   \n    compiler.Compiler().compile(\n        pipeline_func=pipeline_func,\n        package_path=\"./tmp/my_pipeline.json\")\n    \n\n@dsl.component(\n    packages_to_install=[\n        \"ucimlrepo==0.0.7\",\n        \"fastparquet==2023.7.0\"\n    ],\n    base_image=\"python:3.9\")\ndef load_data(dataset: Output[Dataset]):\n    \"\"\"\n    Get iris dataset from UCI reposiory\n    Returns: \n        df_data - Dataframe containing 4 features \n        regarding iris dataset and the target\n    \"\"\"\n    import logging\n    from ucimlrepo import fetch_ucirepo\n\n    logging.info(\"Getting Dataset\")\n    data_iris = fetch_ucirepo(id=53) \n    df_data = data_iris.data.features \n    df_data.rename(columns = {\n        'sepal length' : 'sepal_length',\n        'sepal width' : 'sepal_width',\n        'petal length' : 'petal_length',\n        'petal width': 'petal_width'\n    }, inplace=True)\n    target_array = data_iris.data.targets['class']\n\n    df_data['target'] = target_array\n\n    df_data.to_csv(dataset.path)\n\n\n@dsl.component(packages_to_install=[\"scikit-learn==1.5.2\"], base_image=\"python:3.9\")\ndef set_training_pipeline(pipeline_out: Output[Model]):\n    \"\"\"\n    Defines training pipeline steps and returns the pipeline\n    \"\"\"\n    import joblib\n    from sklearn.impute import SimpleImputer\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.pipeline import Pipeline\n    from sklearn.preprocessing import StandardScaler\n    \n    pipeline = Pipeline(steps = [\n        ('Imputer', SimpleImputer(strategy='mean', keep_empty_features=True)),\n        ('normalization', StandardScaler()),\n        ('estimator', LogisticRegression() )\n        ]\n    )\n    joblib.dump(pipeline, pipeline_out.path)\n\n\n@dsl.component(packages_to_install=[\"scikit-learn==1.5.2\"], base_image=\"python:3.9\")\ndef train_model(\n    dataset: Input[Dataset],\n    pipeline: Input[Model],\n    output_model: Output[Model]\n)-> Model:\n    import logging\n    import joblib\n    import pandas as pd\n    from sklearn.metrics import accuracy_score\n    from sklearn.model_selection import GridSearchCV, train_test_split\n\n    dataset = pd.read_csv(dataset.path)\n    logging.info(f\"Spliting dataset\")\n    X_train, X_test, y_train, y_test = train_test_split(\n        dataset.drop(columns=\"target\"),\n        dataset[\"target\"],\n        test_size=0.2, \n        random_state=14)\n\n    logging.info(f\"Fittig model with train data\")\n\n    parameters = {\n        'estimator__solver': ['newton-cg'],\n        'estimator__tol': [ 0.0001, 0.003, 0.01],\n        'estimator__penalty': [None, 'l2'],\n    }\n\n    model = GridSearchCV(estimator=pipeline,\n                            param_grid=parameters,\n                            scoring= {\"AUC\": \"roc_auc_ovr\"},\n                            refit=\"AUC\",\n                            cv=5,\n                            verbose=1,\n                            error_score='raise')\n    \n    pipeline = joblib.load(pipeline)\n    model = pipeline.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n\n    logging.info(f\"Computing scores\")\n    model_score = model.score(X_test, y_test)\n    logging.info(f\"Model AUC Score: {model_score}\")\n\n    test_acc_score = accuracy_score(y_test, y_pred)\n    logging.info(f\"Accuracy test score: {test_acc_score}\")\n\n    logging.info(\"Saving model\")\n    joblib.dump(model, output_model.path)\n\n\nPIPELINE_ROOT=\"/tmp\"\n@dsl.pipeline(\n    name=\"my-pipeline-\",\n    description=\"A class project\",\n    pipeline_root=PIPELINE_ROOT\n)\ndef my_pipeline_func():\n    load_data_component = load_data()\n    set_training_pipe_component = set_training_pipeline(\n        \n    ).after(load_data_component)\n\n    fit_model_component = train_model(\n        dataset = load_data_component.output,\n        pipeline = set_training_pipe_component.outputs['pipeline_out']\n    ).after(set_training_pipe_component)\n\n\ndef execute_pipeline():\n    compile_pipeline(my_pipeline_func)\n    PIPELINE_ROOT = \"./tmp\"\n    aiplatform.init(project=\"personal-448814\",\n                    location=\"us-central1\",\n                    staging_bucket=(\n                        f\"gs://kfp_bucket_pipeline/{PIPELINE_ROOT}/\"\n                    ))\n    job = aiplatform.PipelineJob(\n        display_name=\"A pipeline for a class project\",\n        template_path=f\"{PIPELINE_ROOT}/my_pipeline.json\",\n        pipeline_root=f\"gs://kfp_bucket_pipeline/{PIPELINE_ROOT}/\",\n        project=\"personal-448814\",\n        location=\"us-central1\",\n        enable_caching=False\n        \n    )\n    job.run(\n        service_account=(\n            \"personal@personal-448814.iam.gserviceaccount.com\"\n        )\n    )\n"
  },
  {
    "repo": "lajd/local_kubeflow",
    "file_path": "examples/pytorch_lightning_cifar10/pl_train_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/lajd/local_kubeflow/main/examples/pytorch_lightning_cifar10/pl_train_pipeline.py",
    "content": "#!/usr/bin/env python3\n# Ref: https://pytorch-lightning.readthedocs.io/en/stable/notebooks/lightning_examples/cifar10-baseline.html\n\nimport kfp.dsl as kfp\n\n\ndef training_op(\n        lr: float=0.05,\n        momentum: float=0.9,\n        wd: float=5e-4,\n        max_lr: float=0.1,\n        batch_size: int = 64,\n        num_workers: int = 8,\n        max_epochs: int = 30,\n        step_name: str = 'training'\n):\n  return kfp.ContainerOp(\n    name=step_name,\n    image='lajd94/kubeflow_examples:pytorch_lightning_example',\n    command=['/bin/bash', '-c'],\n    arguments=[\n      f'python train.py'\n      f' --lr {lr}'\n      f' --momentum {momentum}'\n      f' --wd {wd}'\n      f' --max_lr {max_lr}'\n      f' --batch_size {batch_size}'\n      f' --num_workers {num_workers}'\n      f' --max_epochs {max_epochs}'\n    ],\n    # TODO: Handle directory output\n    # file_outputs={'output': '/opt/model/lightning_logs/'},\n  )\n\n\n@kfp.pipeline(\n  name='Pipeline Example',\n  description='Demonstrate the Kubeflow pipelines SDK'\n)\n\ndef kubeflow_training(\n    lr: kfp.PipelineParam = kfp.PipelineParam(name='lr', value=0.05),\n    momentum: kfp.PipelineParam = kfp.PipelineParam(name='momentum', value=0.9),\n    wd: kfp.PipelineParam = kfp.PipelineParam(name='wd', value=5e-4),\n    max_lr: kfp.PipelineParam = kfp.PipelineParam(name='max_lr', value=0.1),\n    batch_size: kfp.PipelineParam = kfp.PipelineParam(name='batch_size', value=64),\n    num_workers: kfp.PipelineParam = kfp.PipelineParam(name='num_workers', value=8),\n    max_epochs: kfp.PipelineParam = kfp.PipelineParam(name='max_epochs', value=30),\n  ):\n\n  training = training_op(lr, momentum, wd, max_lr, batch_size, num_workers, max_epochs)\n\n\nif __name__ == '__main__':\n  import kfp.compiler as compiler\n  compiler.Compiler().compile(kubeflow_training, __file__ + '.tar.gz')\n"
  },
  {
    "repo": "lajd/local_kubeflow",
    "file_path": "examples/simple_pipeline/simple_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/lajd/local_kubeflow/main/examples/simple_pipeline/simple_pipeline.py",
    "content": "#!/usr/bin/env python3\n# Ref: https://github.com/kubeflow/examples/tree/master/demos/simple_pipeline\n\nimport kfp.dsl as kfp\n\n\ndef training_op(learning_rate: float,\n                num_layers: int,\n                optimizer='ftrl',\n                step_name='training'):\n  return kfp.ContainerOp(\n    name=step_name,\n    image='katib/mxnet-mnist-example',\n    command=['python', '/mxnet/example/image-classification/train_mnist.py'],\n    arguments=[\n      '--batch-size', '64',\n      '--lr', learning_rate,\n      '--num-layers', num_layers,\n      '--optimizer', optimizer\n    ],\n    file_outputs={'output': '/etc/timezone'},\n  )\n\n\ndef postprocessing_op(output,\n                      step_name='postprocessing'):\n  return kfp.ContainerOp(\n    name=step_name,\n    image='library/bash:4.4.23',\n    command=['sh', '-c'],\n    arguments=['echo \"%s\"' % output]\n  )\n\n\n@kfp.pipeline(\n  name='Pipeline Example',\n  description='Demonstrate the Kubeflow pipelines SDK'\n)\n\ndef kubeflow_training(\n  learning_rate: kfp.PipelineParam = kfp.PipelineParam(name='learningrate', value=0.1),\n  num_layers: kfp.PipelineParam = kfp.PipelineParam(name='numlayers', value='2'),\n  optimizer: kfp.PipelineParam = kfp.PipelineParam(name='optimizer', value='ftrl')):\n\n  training = training_op(learning_rate, num_layers, optimizer)\n  postprocessing = postprocessing_op(training.output)  # pylint: disable=unused-variable\n\n\nif __name__ == '__main__':\n  import kfp.compiler as compiler\n  compiler.Compiler().compile(kubeflow_training, __file__ + '.tar.gz')\n"
  },
  {
    "repo": "lajd/local_kubeflow",
    "file_path": "examples/spark/spark_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/lajd/local_kubeflow/main/examples/spark/spark_pipeline.py",
    "content": "# Ref: https://github.com/sbakiu/kubeflow-spark/blob/main/kubeflow_pipeline.py\n\nimport json\nimport time\nimport yaml\n\nimport kfp.components as comp\nimport kfp.dsl as dsl\n\nSPARK_COMPLETED_STATE = \"COMPLETED\"\nSPARK_APPLICATION_KIND = \"sparkapplications\"\nSPARK_JOB_YAML_PATH = \"./spark_job.yaml\"\nK8S_GET_COMPONENT_PATH = \"./k8s_get_component.yaml\"\nK8S_APPLY_COMPONENT_PATH = \"./k8s_apply_component.yaml\"\n\n\ndef get_spark_job_definition():\n    \"\"\"\n    Read Spark Operator job manifest file and return the corresponding dictionary and\n    add some randomness in the job name\n    :return: dictionary defining the spark job\n    \"\"\"\n    # Read manifest file\n    with open(SPARK_JOB_YAML_PATH,  \"r\") as stream:\n        spark_job_manifest = yaml.safe_load(stream)\n\n    # Add epoch time in the job name\n    epoch = int(time.time())\n    spark_job_manifest[\"metadata\"][\"name\"] = spark_job_manifest[\"metadata\"][\"name\"].format(epoch=epoch)\n\n    return spark_job_manifest\n\n\ndef print_op(msg):\n    \"\"\"\n    Op to print a message.\n    \"\"\"\n    return dsl.ContainerOp(\n        name=\"Print message.\",\n        image=\"alpine:3.6\",\n        command=[\"echo\", msg],\n    )\n\n\n@dsl.graph_component  # Graph component decorator is used to annotate recursive functions\ndef graph_component_spark_app_status(input_application_name):\n    k8s_get_op = comp.load_component_from_file(K8S_GET_COMPONENT_PATH)\n    check_spark_application_status_op = k8s_get_op(\n        name=input_application_name,\n        kind=SPARK_APPLICATION_KIND\n    )\n    # Remove cache\n    check_spark_application_status_op.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n\n    time.sleep(5)\n    with dsl.Condition(check_spark_application_status_op.outputs[\"applicationstate\"] != SPARK_COMPLETED_STATE):\n        graph_component_spark_app_status(check_spark_application_status_op.outputs[\"name\"])\n\n\n@dsl.pipeline(\n    name=\"Spark Operator job pipeline\",\n    description=\"Spark Operator job pipeline\"\n)\ndef spark_job_pipeline():\n\n    # Load spark job manifest\n    spark_job_definition = get_spark_job_definition()\n\n    # Load the kubernetes apply component\n    k8s_apply_op = comp.load_component_from_file(K8S_APPLY_COMPONENT_PATH)\n\n    # Execute the apply command\n    spark_job_op = k8s_apply_op(object=json.dumps(spark_job_definition))\n\n    # Fetch spark job name\n    spark_job_name = spark_job_op.outputs[\"name\"]\n\n    # Remove cache for the apply operator\n    spark_job_op.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n\n    spark_application_status_op = graph_component_spark_app_status(spark_job_op.outputs[\"name\"])\n    spark_application_status_op.after(spark_job_op)\n\n    print_message = print_op(f\"Job {spark_job_name} is completed.\")\n    print_message.after(spark_application_status_op)\n    print_message.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n\n\nif __name__ == \"__main__\":\n    # Compile the pipeline\n    import kfp.compiler as compiler\n    import logging\n    logging.basicConfig(level=logging.INFO)\n    pipeline_func = spark_job_pipeline\n    pipeline_filename = pipeline_func.__name__ + \".yaml\"\n    compiler.Compiler().compile(pipeline_func, pipeline_filename)\n    logging.info(f\"Generated pipeline file: {pipeline_filename}.\")\n"
  },
  {
    "repo": "awskosehy/fashion_mnist_kubeflow_pipeline",
    "file_path": "pipeline/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/awskosehy/fashion_mnist_kubeflow_pipeline/main/pipeline/pipeline.py",
    "content": "import kfp\nimport kfp.dsl as dsl\nimport requests\nimport os\nimport json\nimport config\n\nfrom kfp import components\nfrom kfp import onprem\nfrom kubernetes.client.models import V1EnvVar\n\ncreate_pytorchjob_fashion_mnist_katib_experiment_op = kfp.components.load_component_from_file(\n    \"/src/2_katib/component.yaml\"\n)\ncreate_tensorboard_launcher_op = kfp.components.load_component_from_file(\n    \"/src/3_tensorboard/component.yaml\"\n)\ncreate_model_path_launcher_op = kfp.components.load_component_from_file(\n    \"/src/4_model_path/component.yaml\"\n)\n\ndef extract_best_trial_name(katib_results) -> str:\n    \"\"\" Extract the best trial name.\n      \n    Args:\n    katib_results: The JSON object formatted the hyperparameter set of the best experiment trial result\n\n    Returns:\n    best_trial_name: string which contain the best experiment trial name\n    \"\"\"\n    import json\n    import pprint\n    katib_results_json = json.loads(katib_results)\n    best_trial_name = katib_results_json[\"currentOptimalTrial\"][\"bestTrialName\"] + '-master-0'\n\n    return best_trial_name\n\n\n@dsl.pipeline(\n    name=config.PIPELINE_NAME,\n    description=config.DESCRIPTION\n)\n\ndef create_pytorch_fashion_mnist_pipeline():\n    fashion_mnist_katib_experiment = create_pytorchjob_fashion_mnist_katib_experiment_op(\n        experiment_name=config.EXPERIMENT_NAME,\n        experiment_namespace=config.EXPERIMENT_NAMESPACE,\n        dataset_path=config.DATASET_PATH,\n        log_dir=config.MODEL_DIR,\n        max_trial_count=config.MAX_TRIAL_COUNT,\n        max_failed_trial_count=config.MAX_FAILED_TRIAL_COUNT,\n        parallel_trial_count=config.PARALLEL_TRIAL_COUNT,\n        epochs=config.EPOCHS,\n        loss_goal=config.LOSS_GOAL,\n        min_learning_rate=config.MIN_LR,\n        max_learning_rate=config.MAX_LR,\n        min_momentum=config.MIN_MOMENTUM,\n        max_momentum=config.MAX_MOMENTUM,\n        pytorch_fashion_mnist_image=config.PYTORCH_FAHION_MNIST_IMAGE,\n    )\n\n    tensorboard_pipeline = create_tensorboard_launcher_op(\n        s3_path = 's3://tensorboard' + config.LOG_DIR,\n    ).apply(onprem.mount_pvc('minio-pvc-volume','minio-pv-volume','/workspace/minio')) \\\n    .add_env_variable(V1EnvVar(name='S3_ENDPOINT', value=config.S3_ENDPOINT)) \\\n    .add_env_variable(V1EnvVar(name='AWS_ENDPOINT_URL', value=config.AWS_ENDPOINT_URL)) \\\n    .add_env_variable(V1EnvVar(name='AWS_ACCESS_KEY_ID', value=config.AWS_ACCESS_KEY_ID)) \\\n    .add_env_variable(V1EnvVar(name='AWS_SECRET_ACCESS_KEY', value=config.AWS_SECRET_ACCESS_KEY)) \\\n    .add_env_variable(V1EnvVar(name='AWS_REGION', value=config.AWS_REGION)) \\\n    .add_env_variable(V1EnvVar(name='S3_USE_HTTPS', value='0')) \\\n    .add_env_variable(V1EnvVar(name='SE_VERIFY_SSL', value='0'))\n\n    extract_best_trial_name_op =components.func_to_container_op(extract_best_trial_name)\n    best_trial_name = extract_best_trial_name_op(\n        fashion_mnist_katib_experiment.output\n    ).after(fashion_mnist_katib_experiment)\n\n    model_path = create_model_path_launcher_op(\n        model_path = config.MINIO_ADDR + config.MINIO_PATH + str(best_trial_name.output) + '/'\n    ).after(best_trial_name)\n\nif __name__==\"__main__\":\n    # resolve kfp_server_api.exceptions.ApiException: (400) NAMESPACE is empty issue\n    os.system('mkdir -p ~/.config/kfp')\n    context = {\n        \"namespace\": \"admin\"\n    }\n\n    with open(\"context.json\", \"w\") as write_file:\n        json.dump(context, write_file)\n    os.system('mv context.json ~/.config/kfp/context.json')\n    \n    pipeline_func = create_pytorch_fashion_mnist_pipeline\n\n    session = requests.Session()\n    response = session.get(config.HOST)\n\n    headers = {\n        \"Content-Type\": \"application/x-www-form-urlencoded\",\n    }\n\n    data = {\"login\": config.USERNAME, \"password\": config.PASSWORD}\n    session.post(response.url, headers=headers, data=data)\n    print(session.cookies.get_dict())\n    session_cookie = session.cookies.get_dict()[\"authservice_session\"]\n\n    print(f'session_cookie: {session_cookie}')\n\n    from kubernetes import client as k8s_client\n    pipeline_conf = kfp.dsl.PipelineConf()\n    pipeline_conf.set_image_pull_secrets([k8s_client.V1ObjectReference(name=\"registry-credentials\")])\n\n    # Compile pipeline to generate compressed YAML definition of the pipeline.\n    kfp_client=kfp.Client(\n        host=f\"{config.HOST}/pipeline\",\n        cookies=f\"authservice_session={session_cookie}\",\n        namespace=config.NAMESPACE\n    )\n    run_id=kfp_client.create_run_from_pipeline_func(pipeline_func,\n                                                    arguments={},\n                                                    pipeline_conf=pipeline_conf,\n                                                    )\n "
  },
  {
    "repo": "anilkharde/FlexiKubeflowPipelineSolutions",
    "file_path": "pipeline/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/anilkharde/FlexiKubeflowPipelineSolutions/main/pipeline/pipeline.py",
    "content": "import kfp\nfrom kfp import dsl\nfrom kfp.compiler import Compiler\nimport kfp.components as components\nimport requests\nfrom kubernetes import client as k8s_client\n\nclass FlexiPipeline():\n    def __init__(self, config_path):\n        \"\"\"\n        Initialize the FlexiPipeline class with the configuration path.\n        \n        Args:\n            config_path (str): Path to the JSON configuration file.\n        \"\"\"\n        self.config_data = self.read_json(config_path)\n       \n    def read_json(self, path):\n        \"\"\"\n        Read and parse a JSON file from the given path.\n        \n        Args:\n            path (str): Path to the JSON file.\n        \n        Returns:\n            dict: Parsed JSON data.\n        \"\"\"\n        import json\n        with open(path, 'r') as file:\n            cf = json.load(file)\n        import pprint\n        pprint.pprint(cf)\n        return cf\n    \n    @staticmethod\n    def pre_process():\n        \"\"\"\n        Pre-processing function to be executed before the main processing.\n        \"\"\"\n        import time\n        print(\"Pre processing...\")\n        time.sleep(5)\n        print(\"Pre processing done...\")\n\n    @staticmethod\n    def custom_process(config_data: dict, image_id: str):\n        \"\"\"\n        Custom processing function to process each image.\n        \n        Args:\n            config_data (dict): Configuration data.\n            image_id (str): ID of the image to be processed.\n        \"\"\"\n        import os\n        import sys\n        import logging\n            \n        code_path = config_data['pipeline_config']['code_path']\n        logging.info(os.listdir(code_path))\n        sys.path.append(code_path)\n\n        # Import the custom processing module\n        from process import CustomProcess\n\n        try:\n            # Process implementation\n            print(\"code_path: \", code_path)\n            print(f\"Running Custom Process for image id {image_id}...\")\n            obj = CustomProcess(5)\n            param = config_data['process_input']['input_parameter']\n            print(param)\n        except Exception as e:\n            print(\"Exception\", e)\n\n    @staticmethod\n    def post_process():\n        \"\"\"\n        Post-processing function to be executed after the main processing.\n        \"\"\"\n        import time\n        print(\"Post processing...\")\n        time.sleep(5)\n        print(\"Post processing done...\")     \n    \n    def get_pre_process_fn(self):\n        \"\"\"\n        Get the pre-processing function.\n        \n        Returns:\n            function: Pre-processing function.\n        \"\"\"\n        return self.pre_process\n    \n    def get_custom_process_fn(self):\n        \"\"\"\n        Get the custom processing function.\n        \n        Returns:\n            function: Custom processing function.\n        \"\"\"\n        return self.custom_process\n    \n    def get_post_process_fn(self):\n        \"\"\"\n        Get the post-processing function.\n        \n        Returns:\n            function: Post-processing function.\n        \"\"\"\n        return self.post_process\n    \n    def pipeline(self, image_ids):\n        \"\"\"\n        Define the pipeline for processing images.\n        \n        Args:\n            image_ids (list): List of image IDs to be processed.\n        \n        Returns:\n            function: The pipeline function.\n        \"\"\"\n        \n        config_data = self.config_data\n        \n        pre_process_fn = self.get_pre_process_fn()\n        custom_process_fn = self.get_custom_process_fn()\n        post_process_fn = self.get_post_process_fn()\n\n        @dsl.pipeline(\"Process Name\", \"Process Description\")\n        def custom_process_pipeline(self):\n\n            packages_installation = config_data['pipeline_config']['packages_to_install']\n           \n            # Define the code PVC (Persistent Volume Claim) --------------------------------------------------------------------------------\n            code_mount = k8s_client.V1VolumeMount(\n                    name=config_data['pipeline_config']['pvc']['name'], \n                    mount_path= config_data['pipeline_config']['pvc']['mount_path'])\n            code_volume = k8s_client.V1Volume(\n                    name=config_data['pipeline_config']['pvc']['name'], \n                    persistent_volume_claim=k8s_client.V1PersistentVolumeClaimVolumeSource(\n                        claim_name=config_data['pipeline_config']['pvc']['name']))\n            \n            # Pre-process task\n            create_pre_process_op = components.create_component_from_func(\n                                                func=pre_process_fn,\n                                                base_image=\"python:3.9\"\n                                                )\n            create_pre_process_task = create_pre_process_op()\n            create_pre_process_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\" # Disable caching\n            create_pre_process_task.set_retry(1, policy=\"Always\", backoff_duration=\"2m\")\n\n            # Custom process tasks for each image ID\n            create_custom_process_op = components.create_component_from_func(\n                                                func=custom_process_fn,\n                                                base_image=\"python:3.9\",\n                                                packages_to_install=packages_installation\n                                                )\n            tasks = []\n            for image_id in image_ids:\n                task = create_custom_process_op(\n                    config_data=config_data, \n                    image_id=image_id\n                    )  \\\n                    .set_memory_request(config_data['pipeline_config']['custom_process']['memory_request']) \\\n                    .set_cpu_request(config_data['pipeline_config']['custom_process']['cpu_request']) \\\n                    .set_memory_limit(config_data['pipeline_config']['custom_process']['memory_limit']) \\\n                    .set_cpu_limit(config_data['pipeline_config']['custom_process']['cpu_limit'])\n                task.add_volume_mount(code_mount)\n                task.add_volume(code_volume)\n                task.execution_options.caching_strategy.max_cache_staleness = \"P0D\" # Disable caching\n                \n                task.set_display_name(f\"Process for image id {image_id}\")\n                task.set_retry(1, policy=\"Always\", backoff_duration=\"2m\")\n                task.after(create_pre_process_task)\n                tasks.append(task)\n\n            # Post-process task\n            create_post_process_op = components.create_component_from_func(\n                                                func=post_process_fn,\n                                                base_image=\"python:3.9\"\n                                                )\n            create_post_process_task = create_post_process_op()\n            create_post_process_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\" # Disable caching\n            create_post_process_task.set_retry(1, policy=\"Always\", backoff_duration=\"2m\")\n\n            # Ensure the post-process task runs after all custom process tasks\n            for task in tasks:\n                create_post_process_task.after(task)\n\n        return custom_process_pipeline\n    \n    def create_run(self, image_ids):\n        \"\"\"\n        Create and start a new run of the pipeline.\n        \n        Args:\n            image_ids (list): List of image IDs to be processed.\n        \n        Returns:\n            str: The run ID of the created pipeline run.\n        \"\"\"\n        import datetime\n        import requests\n\n        kubeflow_endpoint = self.config_data['pipeline_config']['kubeflow_endpoint']\n        username = self.config_data['pipeline_config']['username']\n        password = self.config_data['pipeline_config']['password']\n\n        # Authenticate with the Kubeflow endpoint\n        session = requests.Session()\n        response = session.get(kubeflow_endpoint)\n        headers = {\n            \"Content-Type\": \"application/x-www-form-urlencoded\",\n        }\n        data = {\"login\": username, \"password\": password}\n        session.post(response.url, headers=headers, data=data)\n        session_cookie = session.cookies.get_dict()[\"authservice_session\"]\n\n        # Create a client for the Kubeflow Pipelines\n        client = kfp.Client(host=f\"{kubeflow_endpoint}/pipeline\", cookies=f\"authservice_session={session_cookie}\")\n        print(\"Image_ids in create run:\", image_ids)\n\n        try: \n            # Generate a timestamp for the run name\n            timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n            # Create and start the pipeline run\n            run_id = client.create_run_from_pipeline_func(\n                                self.pipeline(image_ids), {},\n                                run_name=f\"custom_process_run_{timestamp}\",\n                                experiment_name=\"custom-process-exp\").run_id\n            print(\"Custom process run created successfully:\", run_id)\n            return run_id\n        except Exception as e:\n            print(\"Exception:\", e)"
  },
  {
    "repo": "ImranRiazChohan/ml_pipeline_using_kubeflow",
    "file_path": "pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/ImranRiazChohan/ml_pipeline_using_kubeflow/main/pipeline.py",
    "content": "import kfp\r\nfrom kfp import dsl\r\n\r\n\r\ndef preprocess_op():\r\n\r\n    return dsl.ContainerOp(\r\n        name='Preprocess Data',\r\n        image='usamachohan76/ml_pipeline_usingkubeflow:latest',\r\n        arguments=[],\r\n        file_outputs={\r\n            'x_train': '/app/x_train.npy',\r\n            'x_test': '/app/x_test.npy',\r\n            'y_train': '/app/y_train.npy',\r\n            'y_test': '/app/y_test.npy',\r\n        }\r\n    )\r\ndef train_op(x_train, y_train):\r\n\r\n    return dsl.ContainerOp(\r\n        name='Train Model',\r\n        image='usamachohan76/ml_pipeline_usingkubeflow:latest',\r\n        arguments=[\r\n            '--x_train', x_train,\r\n            '--y_train', y_train\r\n        ],\r\n        file_outputs={\r\n            'model': '/app/model.pkl'\r\n        }\r\n    )\r\n\r\n\r\ndef test_op(x_test, y_test, model):\r\n    return dsl.ContainerOp(\r\n        name='Test Model',\r\n        image='usamachohan76/ml_pipeline_usingkubeflow:latest',\r\n        arguments=[\r\n            '--x_test', x_test,\r\n            '--y_test', y_test,\r\n            '--model', model\r\n        ],\r\n        file_outputs={\r\n            'mean_squared_error': '/app/output.txt'\r\n        }\r\n    )\r\n\r\n\r\ndef deploy_model_op(model):\r\n    return dsl.ContainerOp(\r\n        name='Deploy Model',\r\n        image='usamachohan76/ml_pipeline_usingkubeflow:latest',\r\n        arguments=[\r\n            '--model', model\r\n        ]\r\n    )\r\n\r\n\r\n@dsl.pipeline(\r\n    name='Boston Housing Pipeline',\r\n    description='An example pipeline that trains and logs a regression model.'\r\n)\r\ndef boston_pipeline():\r\n    _preprocess_op = preprocess_op()\r\n\r\n    _train_op = train_op(\r\n        dsl.InputArgumentPath(_preprocess_op.outputs['x_train']),\r\n        dsl.InputArgumentPath(_preprocess_op.outputs['y_train'])\r\n    ).after(_preprocess_op)\r\n\r\n    _test_op = test_op(\r\n        dsl.InputArgumentPath(_preprocess_op.outputs['x_test']),\r\n        dsl.InputArgumentPath(_preprocess_op.outputs['y_test']),\r\n        dsl.InputArgumentPath(_train_op.outputs['model'])\r\n    ).after(_train_op)\r\n\r\n    deploy_model_op(\r\n        dsl.InputArgumentPath(_train_op.outputs['model'])\r\n    ).after(_test_op)\r\n\r\nclient = kfp.Client()\r\nclient.create_run_from_pipeline_func(boston_pipeline, arguments={})\r\n"
  },
  {
    "repo": "vyomagg/poc_kubeflow_regression_pipeline",
    "file_path": "pipeline/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/vyomagg/poc_kubeflow_regression_pipeline/develop/pipeline/pipeline.py",
    "content": "import kfp\nfrom kfp import dsl\nimport yaml\n\n\ndef extract_data_op(train_samples, test_samples):\n    return dsl.ContainerOp(\n        name='Extract Data',\n        image='vyomagg/regression_pipeline_extract_data:latest',\n        arguments=[\n            '--train_samples', train_samples,\n            '--test_samples', test_samples\n        ],\n        file_outputs={\n            'out_train': '/app/data/raw/train.pkl',\n            'out_test': '/app/data/raw/test.pkl'\n        }\n    )\n\n\ndef prepare_op(input_train, input_test, co_relation_threshold):\n    return dsl.ContainerOp(\n        name='Prepare Data',\n        image='vyomagg/regression_pipeline_prepare:latest',\n        arguments=[\n            '--input_train', input_train,\n            '--input_test', input_test,\n            '--co_relation_threshold', co_relation_threshold\n        ],\n        file_outputs={\n            'out_train': '/app/data/prepared/train.pkl',\n            'out_test': '/app/data/prepared/test.pkl'\n        }\n    )\n\n\ndef train_op(input_train, fit_intercept , normalize, n_jobs, copy_X):\n    return dsl.ContainerOp(\n        name='Train Model',\n        image='vyomagg/regression_pipeline_train:latest',\n        arguments=[\n            '--input_train', input_train,\n            '--fit_intercept', fit_intercept,\n            '--normalize',normalize,\n            '--n_jobs',n_jobs,\n            '--copy_X', copy_X\n        ],\n        file_outputs={\n            'train_model': '/app/model/Regression_checkpoints/best.pkl'\n        }\n    )\n\n\ndef evaluate_op(data_path, model_ckpt_dir, metrics):\n    return dsl.ContainerOp(\n        name='Test Model',\n        image='vyomagg/regression_pipeline_evaluate:latest',\n        arguments=[\n            '--data_path', data_path,\n            '--model_ckpt_dir', model_ckpt_dir,\n            '--metrics', metrics\n        ],\n        file_outputs={\n            'mean_squared_error': '/app/results/train_stats_mse.json',\n            'root_mean_squared_error': '/app/results/train_stats_rmse.json',\n            'mean_absolute_error': '/app/results/train_stats_mae.json',\n            'r_square_error': 'app/results/train_stats_rsquare.json'\n        }\n    )\n\n\ndef deploy_model_op(model):\n    return dsl.ContainerOp(\n        name='Deploy Model',\n        image='vyomagg/regression_pipeline_deploy_model:latest',\n        arguments=[\n            '--model', model\n        ]\n    )\n\n\n@dsl.pipeline(\n    name='Regression Kubeflow Pipeline',\n    description='An example pipeline that trains and logs a regression model.'\n)\ndef regression_pipeline(train_samples, test_samples, co_relation_threshold, fit_intercept,\n                        normalize, n_jobs, copy_X, metrics ) :\n\n    _extract_op = extract_data_op(train_samples, test_samples)\n\n    _prepare_op = prepare_op(\n        dsl.InputArgumentPath(_extract_op.outputs['out_train']),\n        dsl.InputArgumentPath(_extract_op.outputs['out_test']),\n        co_relation_threshold\n    ).after(_extract_op)\n\n    _train_op = train_op(\n        dsl.InputArgumentPath(_prepare_op.outputs['out_train']),\n        fit_intercept, normalize, n_jobs, copy_X\n    ).after(_prepare_op)\n\n    _evaluate_op = evaluate_op(\n        dsl.InputArgumentPath(_prepare_op.outputs['out_test']),\n        dsl.InputArgumentPath(_train_op.outputs['train_model']),\n        metrics\n    ).after(_train_op)\n\n    deploy_model_op(\n        dsl.InputArgumentPath(_train_op.outputs['train_model'])\n    ).after(_evaluate_op)\n\n\n#kfp.compiler.Compiler().compile(regression_pipeline, 'regression_pipeline.zip')\n\n## Global Parameters\nparams = yaml.safe_load(open('../params.yaml'))\nextract_params = params['extract']\nprepare_params = params['prepare']\ntrain_params = params['train']\nevaluate_params = params['evaluate']\n\n\narguments = { 'train_samples' : extract_params['train_samples'], 'test_samples' : extract_params['test_samples'] ,\n            'co_relation_threshold' : prepare_params['co_relation_threshold'] , 'fit_intercept' : train_params['fit_intercept'],\n            'normalize' : train_params['normalize'], 'n_jobs' : train_params['n_jobs'] , 'copy_X' : train_params['copy_X'],\n            'metrics' : evaluate_params['metrics']}\n\n\n## Auto Execution of pipeline\nclient = kfp.Client(host='http://127.0.0.1:8081', namespace='kubeflow')\nclient.create_run_from_pipeline_func(regression_pipeline, arguments= arguments)\n"
  },
  {
    "repo": "AlbughdadiM/kubeflow-pipeline-crop-classification",
    "file_path": "pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/AlbughdadiM/kubeflow-pipeline-crop-classification/master/pipeline.py",
    "content": "import kfp\nfrom kfp import dsl\nfrom kfp.components import  InputPath\n\n\n\n\n\n@dsl.pipeline(name='advanced-crop-classification-pipeline', description='Classify crops extracted from RPG.')\ndef crop_classification_pipeline(json_img: str,shp:str,cross_validation: bool = False,iterations: int = 2,cv: int = 2):\n\n    def compare_models(xgboost_csv : InputPath(str), lstm_csv : InputPath(str)) -> str:\n        import pandas as pd\n        xgb_df = pd.read_csv(xgboost_csv)\n        xgb_acc = xgb_df['precision'][2]\n        \n        lstm_df = pd.read_csv(lstm_csv)\n        lstm_acc = lstm_df['precision'][2]\n        \n        if xgb_acc>=lstm_acc:\n            print (\"XGBoost model will be used for serving\")\n            return \"XGB\"\n        else:\n            print (\"LSTM will be used for serving\")\n            return \"LSTM\"\n\n    # create components from yaml manifest \n    download_img = kfp.components.load_component_from_file('process_img/process_img.yaml')\n    temporal_stats = kfp.components.load_component_from_file('temporal_stats/temporal_stats.yaml')\n    preprocess = kfp.components.load_component_from_file('preprocess_data/preprocess_data.yaml')\n    xgboost_classif = kfp.components.load_component_from_file('extreme_gradient_boost/extreme_gradient_boost.yaml')\n    lstm_classif = kfp.components.load_component_from_file('lstm/lstm.yaml')\n    compare = kfp.components.create_component_from_func(\n                        func=compare_models,\n                        base_image='python:3.7', \n                        #output_component_file='compare_models.yaml', \n                        packages_to_install=['pandas==0.24'],\n                    )\n\n    # Run first task\n    download_task = download_img(json_img,shp)\n    #download_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n\n    # create temporal stats from results of the previous task\n    temporal_task = temporal_stats(download_task.output)\n    #temporal_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n\n    # preprocess data \n    preprocess_task = preprocess(temporal_task.output)\n    #preprocess_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n\n    # classification with XGBoost\n    xgboost_task = xgboost_classif(preprocess_task.output,cross_validation,iterations,cv)\n    #xgboost_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n\n    # classification with LSTM\n    lstm_task = lstm_classif(preprocess_task.output,cross_validation,iterations,cv)\n    #lstm_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n\n    # compare models\n    compare_task = compare(xgboost_task.outputs['Report'],lstm_task.outputs['Report'])\n    compare_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n   \n\n\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(crop_classification_pipeline, 'advanced-crop-classification-pipeline.yaml')\n"
  },
  {
    "repo": "biswaroop1547/skit-pipelines",
    "file_path": "skit_pipelines/utils/k8s.py",
    "raw_url": "https://raw.githubusercontent.com/biswaroop1547/skit-pipelines/main/skit_pipelines/utils/k8s.py",
    "content": "from kfp.dsl import PipelineConf\n\nimport skit_pipelines.constants as const\nfrom skit_pipelines.api import models\n\n\ndef get_pipeline_config_kfp(pipeline_name):\n    ## since currently gpu node doesn't support all integrations with db,\n    ## we set default pipeline nodeselector to be CPU node\n    ## while setting gpu node only for the required component in pipeline.\n\n    # node_type = models.PodNodeSelectorMap[pipeline_name]\n    # if node_type == const.CPU_NODE_LABEL:\n    return PipelineConf().set_default_pod_node_selector(\n        label_name=const.POD_NODE_SELECTOR_LABEL,\n        value=const.CPU_NODE_LABEL,  # node_type\n    )\n"
  },
  {
    "repo": "biswaroop1547/skit-pipelines",
    "file_path": "skit_pipelines/pipelines/asr_tune/__init__.py",
    "raw_url": "https://raw.githubusercontent.com/biswaroop1547/skit-pipelines/main/skit_pipelines/pipelines/asr_tune/__init__.py",
    "content": "import os\n\nimport kfp\n\nfrom skit_pipelines import constants as pipeline_constants\nfrom skit_pipelines.components import (\n    asr_tune_op,\n    create_true_transcript_labels_op,\n    download_directory_from_s3_op,\n    download_file_from_s3_op,\n    extract_true_transcript_labels_to_txt_op,\n    fetch_tagged_dataset_op,\n    process_true_transcript_labels_op,\n    slack_notification_op,\n    upload2s3_op,\n)\n\nBUCKET = pipeline_constants.BUCKET\n\n\n@kfp.dsl.pipeline(\n    name=\"ASR Language Model Tune Pipeline\",\n    description=\"Tunes LM on provided corpus using the val_corpus for validation.\",\n)\ndef asr_tune(\n    *,\n    lang: str,\n    base_model_path: str,\n    general_lm_path: str,\n    target_model_path: str,\n    corpus_path: str = \"\",\n    val_corpus_path: str = \"\",\n    corpus_tog_job_ids: str = \"\",\n    val_corpus_tog_job_ids: str = \"\",\n    augment_wordlist_path: str = \"\",\n    remove_wordlist_path: str = \"\",\n    storage_options: str = '{\"type\": \"s3\",\"bucket\": \"vernacular-asr-models\"}',\n    notify: str = \"\",\n    channel: str = \"\",\n    slack_thread: str = \"\",\n):\n    \"\"\"\n    TODO: Docstring.\n    \"\"\"\n    augment_wordlist_op = download_file_from_s3_op(\n        storage_path=augment_wordlist_path, empty_possible=True\n    )\n    remove_wordlist_op = download_file_from_s3_op(\n        storage_path=remove_wordlist_path, empty_possible=True\n    )\n\n    # create a component that can make sure:\n    # 1. target_model_path does not already exist.\n    # 2. target_model_path is a valid s3 path.\n    # TODO: check_s3_path_does_not_exist_op(target_model_path)\n\n    base_model_op = download_directory_from_s3_op(storage_path=base_model_path)\n    general_lm_op = download_file_from_s3_op(storage_path=general_lm_path)\n\n    with kfp.dsl.Condition(corpus_path == \"\", \"corpus_path\"):\n        corpus_op = fetch_tagged_dataset_op(\n            job_id=corpus_tog_job_ids,\n        )\n        val_corpus_op = fetch_tagged_dataset_op(\n            job_id=val_corpus_tog_job_ids,\n        )\n        true_label_column = \"transctipt_y\"\n        corpus_op = create_true_transcript_labels_op(\n            corpus_op.outputs[\"output\"], true_label_column\n        )\n        corpus_op = process_true_transcript_labels_op(\n            corpus_op.outputs[\"output\"],\n            true_label_column,\n        )\n        corpus_op = extract_true_transcript_labels_to_txt_op(\n            corpus_op.outputs[\"output\"], true_label_column\n        )\n        val_corpus_op = create_true_transcript_labels_op(\n            val_corpus_op.outputs[\"output\"], true_label_column\n        )\n        val_corpus_op = process_true_transcript_labels_op(\n            val_corpus_op.outputs[\"output\"],\n            true_label_column,\n        )\n        val_corpus_op = extract_true_transcript_labels_to_txt_op(\n            val_corpus_op.outputs[\"output\"], true_label_column\n        )\n        tune_op = asr_tune_op(\n            corpus_op.outputs[\"output\"],\n            val_corpus_op.outputs[\"output\"],\n            augment_wordlist_op.outputs[\"output\"],\n            remove_wordlist_op.outputs[\"output\"],\n            base_model_op.outputs[\"output\"],\n            general_lm_op.outputs[\"output\"],\n            lang=lang,\n        ).set_ephemeral_storage_limit(\"20G\")\n        tune_op.execution_options.caching_strategy.max_cache_staleness = (\n            \"P0D\"  # disables caching\n        )\n        upload = upload2s3_op(\n            path_on_disk=tune_op.outputs[\"output\"],\n            output_path=target_model_path,\n            storage_options=storage_options,\n            ext=\"\",\n            upload_as_directory=True,\n        ).after(tune_op)\n        upload.execution_options.caching_strategy.max_cache_staleness = (\n            \"P0D\"  # disables caching\n        )\n    with kfp.dsl.Condition(corpus_tog_job_ids == \"\", \"corpus_tog_job_ids\"):\n        corpus_op_2 = download_file_from_s3_op(storage_path=corpus_path)\n        val_corpus_op_2 = download_file_from_s3_op(storage_path=val_corpus_path)\n        tune_op_2 = asr_tune_op(\n            corpus_op_2.outputs[\"output\"],\n            val_corpus_op_2.outputs[\"output\"],\n            augment_wordlist_op.outputs[\"output\"],\n            remove_wordlist_op.outputs[\"output\"],\n            base_model_op.outputs[\"output\"],\n            general_lm_op.outputs[\"output\"],\n            lang=lang,\n        ).set_ephemeral_storage_limit(\"20G\")\n        tune_op_2.execution_options.caching_strategy.max_cache_staleness = (\n            \"P0D\"  # disables caching\n        )\n        upload_2 = upload2s3_op(\n            path_on_disk=tune_op_2.outputs[\"output\"],\n            output_path=target_model_path,\n            storage_options=storage_options,\n            ext=\"\",\n            upload_as_directory=True,\n        ).after(tune_op_2)\n        upload_2.execution_options.caching_strategy.max_cache_staleness = (\n            \"P0D\"  # disables caching\n        )\n\n    with kfp.dsl.Condition(notify != \"\", \"notify\").after(\n        upload, upload_2\n    ) as upload_check:\n        notification_text = f\"The ASR Tuning pipeline is completed.\"\n        tune_notif = slack_notification_op(\n            notification_text, channel=channel, cc=notify, thread_id=slack_thread\n        )\n        tune_notif.execution_options.caching_strategy.max_cache_staleness = (\n            \"P0D\"  # disables caching\n        )\n\n\n__all__ = [\"asr_tune\"]\n"
  },
  {
    "repo": "biswaroop1547/skit-pipelines",
    "file_path": "skit_pipelines/pipelines/eval_asr_pipeline/__init__.py",
    "raw_url": "https://raw.githubusercontent.com/biswaroop1547/skit-pipelines/main/skit_pipelines/pipelines/eval_asr_pipeline/__init__.py",
    "content": "import kfp\n\nfrom skit_pipelines import constants as pipeline_constants\nfrom skit_pipelines.components import (\n    create_true_transcript_labels_op,\n    create_utterances_op,\n    download_csv_from_s3_op,\n    gen_asr_metrics_op,\n    process_true_transcript_labels_op,\n    slack_notification_op,\n    upload2s3_op,\n)\n\nUTTERANCES = pipeline_constants.UTTERANCES\nTRANSCRIPT_Y = pipeline_constants.TRANSCRIPT_Y\nBUCKET = pipeline_constants.BUCKET\nINTENT = pipeline_constants.INTENT\n\n\n@kfp.dsl.pipeline(\n    name=\"ASR Transcription vs Transcription tags Eval Pipeline\",\n    description=\"Produces asr metrics for the transcriptions present against transcription tags.\",\n)\ndef eval_asr_pipeline(\n    *,\n    s3_path_data: str,\n    org_id: str,\n    notify: str = \"\",\n    channel: str = \"\",\n    true_label_column: str = \"transcript_y\",\n    pred_label_column: str = \"utterances\",\n    slack_thread: str = \"\",\n):\n    \"\"\"\n    Evaluates ASR transcriptions using transcription tags.\n\n    .. _p_eval_asr_pipeline:\n\n    Example payload to invoke this pipeline via slack integrations:\n\n        @charon run eval_asr_pipeline\n\n        .. code-block:: python\n\n            {\n                \"s3_path_data\": \"s3://bucket-name/data/\",\n                \"org_id\": \"org\"\n            }\n\n    :param s3_path_data: S3 path to a tagged dataset (.csv).\n    :type s3_path_data: str\n    :param org_id: reference path to save the metrics.\n    :type org_id: str\n    :param channel: The slack channel to send the notification, defaults to \"\"\n    :type channel: str, optional\n    :param slack_thread: The slack thread to send the notification, defaults to \"\"\n    :type slack_thread: str, optional\n    \"\"\"\n    tagged_data_op = download_csv_from_s3_op(storage_path=s3_path_data)\n\n    # Create true label column\n    preprocess_data_op = create_utterances_op(tagged_data_op.outputs[\"output\"]).after(\n        tagged_data_op\n    )\n\n    # Create utterance column\n    preprocess_step_2_data_op = create_true_transcript_labels_op(\n        preprocess_data_op.outputs[\"output\"], true_label_column\n    ).after(preprocess_data_op)\n\n    preprocess_step_3_data_op = process_true_transcript_labels_op(\n        preprocess_step_2_data_op.outputs[\"output\"],\n        true_label_column,\n    ).after(preprocess_step_2_data_op)\n\n    asr_metrics_op = gen_asr_metrics_op(\n        preprocess_step_3_data_op.outputs[\"output\"],\n        true_label_column=true_label_column,\n        pred_label_column=pred_label_column,\n    )\n\n    # produce test set metrics.\n    upload_metrics = upload2s3_op(\n        path_on_disk=asr_metrics_op.outputs[\"output\"],\n        reference=org_id,\n        file_type=\"asr-metrics\",\n        bucket=BUCKET,\n        ext=\"\",\n        upload_as_directory=True,\n    )\n    upload_metrics.execution_options.caching_strategy.max_cache_staleness = (\n        \"P0D\"  # disables caching\n    )\n\n    with kfp.dsl.Condition(notify != \"\", \"notify\").after(upload_metrics) as asr_check:\n        notification_text = f\"Here are the ASR eval results.\"\n        code_block = f\"aws s3 cp {upload_metrics.output} .\"\n        asr_notif = slack_notification_op(\n            notification_text,\n            channel=channel,\n            cc=notify,\n            code_block=code_block,\n            thread_id=slack_thread,\n        )\n        asr_notif.execution_options.caching_strategy.max_cache_staleness = (\n            \"P0D\"  # disables caching\n        )\n\n\n__all__ = [\"eval_asr_pipeline\"]\n"
  },
  {
    "repo": "biswaroop1547/skit-pipelines",
    "file_path": "skit_pipelines/pipelines/evaluate_slu/__init__.py",
    "raw_url": "https://raw.githubusercontent.com/biswaroop1547/skit-pipelines/main/skit_pipelines/pipelines/evaluate_slu/__init__.py",
    "content": "import kfp\n\nfrom skit_pipelines import constants as pipeline_constants\nfrom skit_pipelines.components import (\n    download_csv_from_s3_op,\n    download_yaml_op,\n    fetch_tagged_dataset_op,\n    evalution_slu_from_repo_op,\n    slack_notification_op,\n    upload2s3_op,\n)\n\nUTTERANCES = pipeline_constants.UTTERANCES\nINTENT_Y = pipeline_constants.INTENT_Y\nBUCKET = pipeline_constants.BUCKET\nCSV_FILE = pipeline_constants.CSV_FILE\nCPU_NODE_LABEL = pipeline_constants.CPU_NODE_LABEL\nGPU_NODE_LABEL = pipeline_constants.GPU_NODE_LABEL\nNODESELECTOR_LABEL = pipeline_constants.POD_NODE_SELECTOR_LABEL\n\n\n@kfp.dsl.pipeline(\n    name=\"SLU evaluating Pipeline\",\n    description=\"Evaluate an existing SLU model.\",\n)\n\ndef evaluate_slu(\n    *,\n    repo_name: str,\n    repo_branch: str = \"master\",\n    compare_branch: str = \"master\",\n    job_ids: str = \"\",\n    test_dataset_path: str = \"\",\n    labelstudio_project_ids: str = \"\",\n    job_start_date: str = \"\",\n    job_end_date: str = \"\",\n    remove_intents: str = \"\",\n    alias_yaml_path: str = \"\",\n    notify: str = \"\",\n    channel: str = \"\",\n    slack_thread: str = \"\",\n    core_slu_repo_name: str = \"core-slu-service\",\n    core_slu_repo_branch: str = \"master\",\n    customization_repo_name: str = \"customization\",\n    customization_repo_branch: str = \"master\",\n):\n    \"\"\"\n    A pipeline to evaluate an existing SLU model.\n\n    .. _p_evaluate_slu:\n\n    Example payload to invoke via slack integrations:\n\n    A minimal example:\n\n        @charon run evaluate_slu\n\n        .. code-block:: python\n\n            {\n                \"repo_name\": \"slu_repo_name\",\n                \"labelstudio_project_ids\": \"10,13\",\n                \"test_dataset_path\":\"s3://bucket/data.csv\"\n            }\n\n\n    A full available parameters example:\n\n        @charon run evaluate_slu\n\n        .. code-block:: python\n\n            {\n                \"repo_name\": \"slu_repo_name\",\n                \"repo_branch\": \"master\",\n                \"test_dataset_path\": \"s3://bucket-name/path1/to1/data1.csv,s3://bucket-name/path2/to2/data2.csv\",\n                \"job_ids\": \"4011,4012\",\n                \"labelstudio_project_ids\": \"10,13\",\n                \"job_start_date\": \"2022-08-01\",\n                \"job_end_date\": \"2022-09-19\",\n                \"remove_intents\": \"_confirm_,_oos_,audio_speech_unclear,ood\",\n                \"alias_yaml_path\": \"intents/oppo/alias.yaml\"\n            }\n\n\n    :param repo_name: SLU repository name under /vernacularai/ai/clients org in gitlab.\n    :type repo_name: str\n\n    :param repo_branch: The branch name in the SLU repository one wants to use, defaults to master.\n    :type repo_name: str, optional\n\n    :param test_dataset_path: The S3 URI or the S3 key for the tagged dataset (can be multiple - comma separated).\n    :type dataset_path: str, optional\n\n    :param job_ids: The job ids as per tog. Optional if labestudio_project_ids is provided.\n    :type job_ids: str\n\n    :param labelstudio_project_ids: The labelstudio project id (this is a number) since this is optional, defaults to \"\".\n    :type labelstudio_project_ids: str\n\n    :param job_start_date: The start date range (YYYY-MM-DD) to filter tagged data.\n    :type job_start_date: str, optional\n\n    :param job_end_date: The end date range (YYYY-MM-DD) to filter tagged data\n    :type job_end_date: str, optional\n\n    :param remove_intents: Comma separated list of intents to remove from dataset while training.\n    :type remove_intents: str, optional\n\n    :param alias_yaml_path: eevee's intent_report alias.yaml, refer docs `here <https://skit-ai.github.io/eevee/metrics/intents.html#aliasing>`_ . Upload your yaml to eevee-yamls repository `here <https://github.com/skit-ai/eevee-yamls>`_ & pass the relative path of the yaml from base of the repository.\n    :type alias_yaml_path: str, optional\n\n    :param core_slu_repo_name: Name of repository for core slu service. Defaults to core-slu-service\n    :type core_slu_repo_name: str, optional\n\n    :param core_slu_repo_branch: Branch to check out for core slu repository. Defaults to master\n    :type core_slu_repo_branch: str, optional\n\n    :param customization_repo_name: Name of repository for customization service. Defaults to customization\n    :type customization_repo_name: str, optional\n\n    :param customization_repo_branch: Branch to check out for customization service repository. Defaults to master\n    :type customization_repo_branch: str, optional\n\n    :param notify: Whether to send a slack notification, defaults to \"\"\n    :type notify: str, optional\n\n    :param channel: The slack channel to send the notification, defaults to \"\"\n    :type channel: str, optional\n\n    :param slack_thread: The slack thread to send the notification, defaults to \"\"\n    :type slack_thread: str, optional\n\n    \"\"\"\n\n    tagged_s3_data_op = download_csv_from_s3_op(storage_path=test_dataset_path)\n    tagged_s3_data_op.execution_options.caching_strategy.max_cache_staleness = (\n        \"P0D\"  # disables caching\n    )\n\n    tagged_job_data_op = fetch_tagged_dataset_op(\n        job_id=job_ids,\n        project_id=labelstudio_project_ids,\n        task_type=\"conversation\",\n        timezone=\"Asia/Kolkata\",\n        start_date=job_start_date,\n        end_date=job_end_date,\n        empty_possible=True,\n    )\n    tagged_job_data_op.execution_options.caching_strategy.max_cache_staleness = (\n        \"P0D\"  # disables caching\n    )\n\n    downloaded_alias_yaml_op = download_yaml_op(\n        git_host_name=pipeline_constants.GITLAB,\n        yaml_path=alias_yaml_path,\n    )\n\n    validate_evaluation_setup_op = evalution_slu_from_repo_op(\n        tagged_s3_data_op.outputs[\"output\"],\n        tagged_job_data_op.outputs[\"output\"],\n        downloaded_alias_yaml_op.outputs[\"output\"],\n        bucket=BUCKET,\n        repo_name=repo_name,\n        compare_branch=compare_branch,\n        branch=repo_branch,\n        remove_intents=remove_intents,\n        validate_setup=True,\n        customization_repo_name=customization_repo_name,\n        customization_repo_branch=customization_repo_branch,\n        core_slu_repo_name=core_slu_repo_name,\n        core_slu_repo_branch=core_slu_repo_branch,\n    ).set_ephemeral_storage_limit(\"20G\")\n    validate_evaluation_setup_op.display_name = \"Validate Evaluation Setup\"\n\n    evaluate_op = evalution_slu_from_repo_op(\n        tagged_s3_data_op.outputs[\"output\"],\n        tagged_job_data_op.outputs[\"output\"],\n        downloaded_alias_yaml_op.outputs[\"output\"],\n        bucket=BUCKET,\n        repo_name=repo_name,\n        compare_branch=compare_branch,\n        branch=repo_branch,\n        remove_intents=remove_intents,\n        customization_repo_name=customization_repo_name,\n        customization_repo_branch=customization_repo_branch,\n        core_slu_repo_name=core_slu_repo_name,\n        core_slu_repo_branch=core_slu_repo_branch,\n    ).after(validate_evaluation_setup_op)\n    evaluate_op.set_gpu_limit(1).add_node_selector_constraint(\n        label_name=NODESELECTOR_LABEL, value=GPU_NODE_LABEL\n    )\n\n    comparison_upload_cf = upload2s3_op(\n        path_on_disk=evaluate_op.outputs[\"comparison_classification_report\"],\n        reference=repo_name,\n        file_type=\"comparison_classification_report\",\n        bucket=BUCKET,\n        ext=CSV_FILE,\n    )\n    comparison_upload_cf.execution_options.caching_strategy.max_cache_staleness = (\n        \"P0D\"  # disables caching\n    )\n\n    comparison_upload_cm = upload2s3_op(\n        path_on_disk=evaluate_op.outputs[\"comparison_confusion_matrix\"],\n        reference=repo_name,\n        file_type=\"comparison_confusion_matrix\",\n        bucket=BUCKET,\n        ext=CSV_FILE,\n    )\n\n    with kfp.dsl.Condition(notify != \"\", name=\"slack_notify\").after(\n        comparison_upload_cf, comparison_upload_cm\n    ):\n        notification_text = f\"Here's the IRR report.\"\n        code_block = f\"aws s3 cp {comparison_upload_cf.output} .\"\n        irr_notif = slack_notification_op(\n            notification_text,\n            channel=channel,\n            cc=notify,\n            code_block=code_block,\n            thread_id=slack_thread,\n        )\n        irr_notif.execution_options.caching_strategy.max_cache_staleness = (\n            \"P0D\"  # disables caching\n        )\n\n        notification_text = f\"Here's the confusion matrix.\"\n        code_block = f\"aws s3 cp {comparison_upload_cm.output} .\"\n        cm_notif = slack_notification_op(\n            notification_text,\n            channel=channel,\n            cc=notify,\n            code_block=code_block,\n            thread_id=slack_thread,\n        )\n        cm_notif.execution_options.caching_strategy.max_cache_staleness = (\n            \"P0D\"  # disables caching\n        )\n\n\n__all__ = [\"evaluate_slu\"]\n"
  },
  {
    "repo": "biswaroop1547/skit-pipelines",
    "file_path": "skit_pipelines/pipelines/fetch_calls_pipeline/__init__.py",
    "raw_url": "https://raw.githubusercontent.com/biswaroop1547/skit-pipelines/main/skit_pipelines/pipelines/fetch_calls_pipeline/__init__.py",
    "content": "import kfp\n\nfrom skit_pipelines import constants as pipeline_constants\nfrom skit_pipelines.components import fetch_calls_op, slack_notification_op\n\nUSE_FSM_URL = pipeline_constants.USE_FSM_URL\nREMOVE_EMPTY_AUDIOS = False if USE_FSM_URL else True\n\n\n@kfp.dsl.pipeline(\n    name=\"Fetch Calls Pipeline\",\n    description=\"fetches calls from production db with respective arguments\",\n)\ndef fetch_calls_pipeline(\n    lang: str,\n    start_date: str = \"\",\n    end_date: str = \"\",\n    client_id: str = \"\",\n    ignore_callers: str = \"\",\n    reported: bool = False,\n    template_id: str = \"\",\n    use_case: str = \"\",\n    flow_name: str = \"\",\n    min_duration: str = \"\",\n    asr_provider: str = \"\",\n    states: str = \"\",\n    intents: str = \"\",\n    call_quantity: int = 200,\n    call_type: str = \"\",\n    remove_empty_audios: bool = REMOVE_EMPTY_AUDIOS,\n    notify: str = \"\",\n    channel: str = \"\",\n    slack_thread: str = \"\",\n    use_fsm_url: bool = False,\n    flow_ids: str = \"\",\n):\n    \"\"\"\n    A pipeline to randomly sample calls for a given voice-bot project.\n\n    .. _p_fetch_calls_pipeline:\n\n\n    Example payload to invoke this pipeline via slack integrations:\n\n        @charon run fetch_calls_pipeline\n\n        .. code-block:: python\n\n            {\n                \"client_id\": 1,\n                \"start_date\": \"2020-01-01\",\n                \"lang\": \"en\",\n                \"end_date\": \"2020-01-01\",\n                \"reported\": false,\n                \"call_quantity\": 200\n            }\n\n    :param client_id: The comma separated client ids as per fsm db.\n    :type client_id: str, optional\n    :param start_date: The start date range to filter calls in YYYY-MM-DD format.\n    :type start_date: str\n    :param lang: The language code of the calls to filter. eg: en, hi, ta, te, etc.\n    :type lang: str\n    :param end_date: The end date range to filter calls in YYYY-MM-DD format.\n    :type end_date: str\n    :param ignore_callers: Comma separated list of callers to ignore, defaults to \"\"\n    :type ignore_callers: str, optional\n    :param reported: Pick only reported calls, defaults to False\n    :type reported: bool\n    :param template_id: The flow template id to filter calls, defaults to \"\"\n    :type template_id: str, optional\n    :param use_case: Voice bot project's use-case, defaults to \"\"\n    :type use_case: str, optional\n    :param flow_name: Identifier for a whole/part of a voicebot conversation flow, defaults to \"\"\n    :type flow_name: str, optional\n    :param min_duration: Call duration filter, defaults to \"\"\n    :type min_duration: str, optional\n    :param asr_provider: The ASR vendor (google/VASR), defaults to \"\"\n    :type asr_provider: str, optional\n    :param states: Filter calls in a comma separated list of states, defaults to \"\"\n    :type states: str, optional\n    :param intents: Filter turns in sampled calls from a comma separated list of intents, defaults to \"\"\n    :type intents: str, optional\n    :param call_quantity: Number of calls to sample, defaults to 200\n    :type call_quantity: int, optional\n    :param call_type: inbound, outbound vs subtesting call filters. We can currently choose only one of these, or defaults to \"INBOUND\" and \"OUTBOUND\" both\n    :type call_type: str, optional\n    :param remove_empty_audios: to remove calls with call audios being empty/broken, defaults to True\n    :type remove_empty_audios: bool\n    :param notify: A comma separated list of slack ids: \"@apples, @orange.fruit\" etc, defaults to \"\"\n    :type notify: str, optional\n    :param channel: The slack channel to send the notification, defaults to \"\"\n    :type channel: str, optional\n    :param slack_thread: The slack thread to send the notification, defaults to \"\"\n    :type slack_thread: str, optional\n    :param use_fsm_url: Whether to use turn audio url from fsm or s3 path., defaults to False\n    :type use_fsm_url: bool, optional\n    :param flow_id: Id for a whole/part of a voicebot conversation flow, defaults to \"\"\n    :type flow_id: str, optional\n    \"\"\"\n    \n    calls = fetch_calls_op(\n        client_id=client_id,\n        start_date=start_date,\n        end_date=end_date,\n        lang=lang,\n        call_quantity=call_quantity,\n        call_type=call_type,\n        ignore_callers=ignore_callers,\n        reported=reported,\n        template_id=template_id,\n        use_case=use_case,\n        flow_name=flow_name,\n        min_duration=min_duration,\n        asr_provider=asr_provider,\n        intents=intents,\n        states=states,\n        remove_empty_audios=remove_empty_audios,\n        use_fsm_url=USE_FSM_URL or use_fsm_url,\n        flow_ids=flow_ids\n    )\n    calls.execution_options.caching_strategy.max_cache_staleness = (\n        \"P0D\"  # disables caching\n    )\n\n    with kfp.dsl.Condition(notify != \"\", \"notify\").after(calls) as check1:\n        notification_text = f\"Finished a request for {call_quantity} calls. Fetched from {start_date} to {end_date} for {client_id=}.\"\n        code_block = f\"aws s3 cp {calls.output} .\"\n        task_no_cache = slack_notification_op(\n            notification_text,\n            channel=channel,\n            cc=notify,\n            code_block=code_block,\n            thread_id=slack_thread,\n        )\n        task_no_cache.execution_options.caching_strategy.max_cache_staleness = (\n            \"P0D\"  # disables caching\n        )\n\n\n__all__ = [\"fetch_calls_pipeline\"]\n"
  },
  {
    "repo": "biswaroop1547/skit-pipelines",
    "file_path": "skit_pipelines/pipelines/fetch_n_tag_turns_and_calls/__init__.py",
    "raw_url": "https://raw.githubusercontent.com/biswaroop1547/skit-pipelines/main/skit_pipelines/pipelines/fetch_n_tag_turns_and_calls/__init__.py",
    "content": "import kfp\n\nfrom skit_pipelines import constants as pipeline_constants\nfrom skit_pipelines.components import (\n    fetch_calls_for_slots_op,\n    fetch_calls_op,\n    fetch_gpt_intent_prediction_op,\n    org_auth_token_op,\n    slack_notification_op,\n    tag_calls_op,\n)\n\nUSE_FSM_URL = pipeline_constants.USE_FSM_URL\nREMOVE_EMPTY_AUDIOS = False if USE_FSM_URL else True\n\n\n@kfp.dsl.pipeline(\n    name=\"Fetch and push for tagging turns & calls pipeline\",\n    description=\"fetches calls from production db (or an s3_path) with respective arguments and uploads turns & calls to labelstudio for tagging intent, entities, slots & call metrics.\",\n)\ndef fetch_n_tag_turns_and_calls(\n    org_id: str,\n    lang: str,\n    client_id: str = \"\",\n    data_label: str = \"\",\n    start_date: str = \"\",\n    end_date: str = \"\",\n    labelstudio_project_id: str = \"\",\n    call_project_id: str = \"\",\n    ignore_callers: str = \"\",\n    template_id: str = \"\",\n    use_case: str = \"\",\n    flow_name: str = \"\",\n    min_duration: str = \"\",\n    asr_provider: str = \"\",\n    states: str = \"\",\n    intents: str = \"\",\n    reported: bool = False,\n    call_quantity: int = 200,\n    call_type: str = \"\",\n    start_date_offset: int = 0,\n    end_date_offset: int = 0,\n    start_time_offset: int = 0,\n    end_time_offset: int = 0,\n    calls_file_s3_path: str = \"\",\n    notify: str = \"\",\n    channel: str = \"\",\n    slack_thread: str = \"\",\n    use_fsm_url: bool = False,\n    remove_empty_audios: bool = REMOVE_EMPTY_AUDIOS,\n    use_assisted_annotation: bool = False,\n    flow_ids: str = \"\"\n):\n    \"\"\"\n    A pipeline to randomly sample calls and upload for annotating turns for intents & entities and annotating calls for slots & call level metrics.\n\n    .. _p_fetch_n_tag_turns_and_calls:\n\n    Example payload to invoke via slack integrations:\n\n        @charon run fetch_n_tag_turns_and_calls\n\n        .. code-block:: python\n\n            {\n                \"client_id\": 41,\n                \"org_id\": 34,\n                \"lang\": \"en\",\n                \"start_date\": \"2022-11-10\",\n                \"end_date\": \"2022-11-11\",\n                \"labelstudio_project_id\": 195,\n                \"call_project_id\": 194,\n                \"data_label\": \"Client\"\n            }\n\n    To use labelstudio:\n\n        @charon run fetch_n_tag_turns_and_calls\n\n        .. code-block:: python\n\n            {\n                \"org_id\": 34,\n                \"client_id\": 41,\n                \"start_date\": \"2022-09-16\",\n                \"end_date\": \"2022-09-19\",\n                \"lang\": \"en\",\n                \"reported\": false,\n                \"call_quantity\": 1000,\n                \"flow_name\" : \"indigo_domain_tuning_english\"\n                \"labelstudio_project_id\": \"135\",\n                \"call_project_id\": 194\n            }\n\n    :param client_id: The comma separated client ids as per fsm db.\n    :type client_id: str, optional\n\n    :param org_id: The organization id as per api-gateway.\n    :type org_id: str\n\n    :param labelstudio_project_id: The labelstudio project id for turn level tagging (intent & entities) (this is a number) since this is optional, defaults to \"\".\n    :type labelstudio_project_id: str\n\n    :param call_project_id: The labelstudio project id for call level tagging (slots & call metrics) (this is a number) since this is optional, defaults to \"\".\n    :type call_project_id: str\n\n    :param data_label: A label to identify the source of a datapoint\n    :type data_label: str, optional. Defaults to \"Live\"\n\n    :param start_date: The start date range to filter calls in YYYY-MM-DD format.\n    :type start_date: str\n\n    :param lang: The language code of the calls to filter. eg: en, hi, ta, te, etc.\n    :type lang: str\n\n    :param end_date: The end date range to filter calls in YYYY-MM-DD format.\n    :type end_date: str\n\n    :param ignore_callers: Comma separated list of callers to ignore, defaults to \"\"\n    :type ignore_callers: str, optional\n\n    :param reported: Pick only reported calls, defaults to False\n    :type reported: bool\n\n    :param template_id: The flow template id to filter calls, defaults to \"\"\n    :type template_id: str, optional\n\n    :param use_case: Voice bot project's use-case, defaults to \"\"\n    :type use_case: str, optional\n\n    :param flow_name: Identifier for a whole/part of a voicebot conversation flow, defaults to \"\"\n    :type flow_name: str, optional\n\n    :param min_duration: Call duration filter, defaults to \"\"\n    :type min_duration: str, optional\n\n    :param asr_provider: The ASR vendor (google/VASR), defaults to \"\"\n    :type asr_provider: str, optional\n\n    :param states: Filter calls in a comma separated list of states, defaults to \"\"\n    :type states: str, optional\n\n    :param intents: Filter turns in sampled calls from a comma separated list of intents, defaults to \"\"\n    :type intents: str, optional\n\n    :param start_date_offset: Offset the start date by an integer value, defaults to 0\n    :type start_date_offset: int, optional\n\n    :param end_date_offset: Offset the end date by an integer value, defaults to 0\n    :type end_date_offset: int, optional\n\n    :param start_time_offset: Offset the start time by an integer value, defaults to 0\n    :type start_time_offset: int, optional\n\n    :param end_time_offset: Offset the end time by an integer value, defaults to 0\n    :type end_time_offset: int, optional\n\n    :param calls_file_s3_path: The s3_path to upload the turns from instead of querying from FSM_db, defaults to \"\"\n    :type calls_file_s3_path: str, optional\n\n    :param call_quantity: Number of calls to sample, defaults to 200\n    :type call_quantity: int, optional\n\n    :param call_type: INBOUND, OUTBOUND, or CALL_TEST call filters. We can currently choose only one of these, or defaults to \"INBOUND\" and \"OUTBOUND\" both\n    :type call_type: str, optional\n\n    :param notify: Whether to send a slack notification, defaults to \"\"\n    :type notify: str, optional\n\n    :param channel: The slack channel to send the notification, defaults to \"\"\n    :type channel: str, optional\n\n    :param slack_thread: The slack thread to send the notification, defaults to \"\"\n    :type slack_thread: float, optional\n\n    :param use_fsm_url: Whether to use turn audio url from fsm or s3 path., defaults to False\n    :type use_fsm_url: bool, optional\n\n    :param remove_empty_audios: Whether to turns of empty audio., defaults to False\n    :type remove_empty_audios: bool, optional\n\n    :param use_assisted_annotation: Whether to use GPT for intent prediction, only applicable to US collections, defaults to False\n    :type use_assisted_annotation: bool, optional\n    \n    :param flow_ids: Id for a whole/part of a voicebot conversation flow, defaults to \"\"\n    :type flow_ids: str, optional\n    \"\"\"\n    calls = fetch_calls_op(\n        client_id=client_id,\n        start_date=start_date,\n        end_date=end_date,\n        lang=lang,\n        call_quantity=call_quantity,\n        call_type=call_type,\n        start_date_offset=start_date_offset,\n        end_date_offset=end_date_offset,\n        start_time_offset=start_time_offset,\n        end_time_offset=end_time_offset,\n        ignore_callers=ignore_callers,\n        reported=reported,\n        template_id=template_id,\n        use_case=use_case,\n        flow_name=flow_name,\n        min_duration=min_duration,\n        asr_provider=asr_provider,\n        intents=intents,\n        states=states,\n        calls_file_s3_path=calls_file_s3_path,\n        use_fsm_url=USE_FSM_URL or use_fsm_url,\n        remove_empty_audios=remove_empty_audios,\n        flow_ids=flow_ids\n    )\n\n    calls.execution_options.caching_strategy.max_cache_staleness = (\n        \"P0D\"  # disables caching\n    )\n\n    auth_token = org_auth_token_op(org_id)\n    auth_token.execution_options.caching_strategy.max_cache_staleness = (\n        \"P0D\"  # disables caching\n    )\n\n    with kfp.dsl.Condition(calls.output != \"\", \"calls_found\").after(calls):\n        # Get intent response from GPT for qualifying turns\n        gpt_response_path = fetch_gpt_intent_prediction_op(\n            s3_file_path=calls.output, use_assisted_annotation=use_assisted_annotation\n        )\n\n        # uploads data for turn level intent, entity & transcription tagging\n        tag_turns_output = tag_calls_op(\n            input_file=gpt_response_path.output,\n            project_id=labelstudio_project_id,\n            data_label=data_label,\n        )\n\n        fetch_slot_and_calls_output = fetch_calls_for_slots_op(\n            untagged_records_path=calls.output,\n            org_id=org_id,\n            language_code=lang,\n            start_date=start_date,\n            end_date=end_date,\n        )\n\n        # uploads data for call & slot level tagging to labelstudio\n        tag_calls_output = tag_calls_op(\n            input_file=fetch_slot_and_calls_output.output,\n            call_project_id=call_project_id,\n            data_label=data_label,\n        )\n\n        with kfp.dsl.Condition(notify != \"\", \"notify\").after(tag_turns_output):\n            df_sizes = tag_turns_output.outputs[\"df_sizes\"]\n            errors = tag_turns_output.outputs[\"errors\"]\n\n            notification_text = f\"\"\"Finished a request for {call_quantity} calls. Fetched from {start_date} to {end_date} for {client_id=}.\n            Uploaded {getattr(calls, 'output')} ({df_sizes}, {org_id=}) for tagging to {labelstudio_project_id=}.\"\"\"\n            notification_text += f\"\\nErrors: {errors}\" if errors else \"\"\n\n            task_no_cache = slack_notification_op(\n                notification_text, channel=channel, cc=notify, thread_id=slack_thread\n            )\n            task_no_cache.execution_options.caching_strategy.max_cache_staleness = (\n                \"P0D\"  # disables caching\n            )\n\n            df_sizes2 = tag_calls_output.outputs[\"df_sizes\"]\n            errors2 = tag_calls_output.outputs[\"errors\"]\n\n            notification_text = f\"\"\"Finished a request for {call_quantity} calls. Fetched from {start_date} to {end_date} for {client_id=}.\n            Uploaded {getattr(fetch_slot_and_calls_output, 'output')} ({df_sizes2}, {org_id=}) for call & slot tagging to {call_project_id=}.\"\"\"\n            notification_text += f\"\\nErrors: {errors2}\" if errors else \"\"\n\n            task_no_cache2 = slack_notification_op(\n                notification_text, channel=channel, cc=notify, thread_id=slack_thread\n            )\n            task_no_cache2.execution_options.caching_strategy.max_cache_staleness = (\n                \"P0D\"  # disables caching\n            )\n\n    with kfp.dsl.Condition(calls.output == \"\", \"no_calls\").after(calls):\n        with kfp.dsl.Condition(notify != \"\", \"notify\").after(calls):\n            notification_text = f\"\"\"No calls could be found from {start_date} to {end_date} for {client_id=}.\n                        Please verify the parameters you have used or refer to the debugging guide on Notion.\"\"\"\n\n            task_no_cache2 = slack_notification_op(\n                notification_text, channel=channel, cc=notify, thread_id=slack_thread\n            )\n            task_no_cache2.execution_options.caching_strategy.max_cache_staleness = (\n                \"P0D\"  # disables caching\n            )\n\n\n__all__ = [\"fetch_n_tag_turns_and_calls\"]\n"
  },
  {
    "repo": "biswaroop1547/skit-pipelines",
    "file_path": "skit_pipelines/pipelines/fetch_tagged_calls_dataset/__init__.py",
    "raw_url": "https://raw.githubusercontent.com/biswaroop1547/skit-pipelines/main/skit_pipelines/pipelines/fetch_tagged_calls_dataset/__init__.py",
    "content": "import tempfile\n\nimport kfp\n\nfrom skit_pipelines import constants as pipeline_constants\nfrom skit_pipelines.components import (\n    fetch_tagged_dataset_op,\n    slack_notification_op,\n    upload2s3_op,\n)\n\n\n@kfp.dsl.pipeline(\n    name=\"Fetch Tagged Dataset Pipeline\",\n    description=\"fetches tagged dataset from tog with respective arguments\",\n)\ndef fetch_tagged_calls_dataset(\n    org_id: str,\n    job_id: str = \"\",\n    labelstudio_project_id: str = \"\",\n    start_date: str = \"\",\n    end_date: str = \"\",\n    timezone: str = \"Asia/Kolkata\",\n    task_type: str = \"conversation\",\n    notify: str = \"\",\n    channel: str = \"\",\n    slack_thread: str = \"\",\n):\n    \"\"\"\n    A pipeline to fetch tagged dataset.\n\n    .. _p_fetch_tagged_calls_dataset:\n\n    Example payload to invoke via slack integrations:\n\n        @charon run fetch_tagged_calls_dataset\n\n        .. code-block:: python\n\n            {\n                \"org_id\": 1,\n                \"job_id\": \"4011\",\n                \"start_date\": \"2020-01-01\",\n                \"end_date\": \"2020-01-01\"\n            }\n\n    To use labelstudio:\n\n        @charon run fetch_tagged_calls_dataset\n\n        .. code-block:: python\n\n            {\n                \"org_id\": 1,\n                \"labelstudio_project_id\": \"40\",\n                \"start_date\": \"2020-01-01\",\n                \"end_date\": \"2020-01-01\"\n            }\n\n    :param org_id: reference path to save the metrics.\n    :type org_id: str\n    :param job_ids: The job ids as per tog. Optional if labestudio project id is provided.\n    :type job_id: str\n    :param labelstudio_project_id: The labelstudio project id (this is a number) since this is optional, defaults to \"\".\n    :type labelstudio_project_id: str\n    :param start_date: The start date range (YYYY-MM-DD) to filter tagged data.\n    :type start_date: str\n    :param end_date: The end date range (YYYY-MM-DD) to filter tagged data\n    :type end_date: str\n    :param timezone: The timezone to apply for multi-region datasets, defaults to \"Asia/Kolkata\"\n    :type timezone: str, optional\n    :param task_type: https://github.com/skit-ai/skit-labels#task-types, defaults to \"conversation\"\n    :type task_type: str, optional\n    :param notify: A comma separated list of slack ids: \"@apples, @orange.fruit\" etc, defaults to \"\"\n    :type notify: str, optional\n    :param channel: The slack channel to send the notification, defaults to \"\"\n    :type channel: str, optional\n    :param slack_thread: The slack thread to send the notification, defaults to \"\"\n    :type slack_thread: str, optional\n    \"\"\"\n    tagged_df = fetch_tagged_dataset_op(\n        job_id=job_id,\n        project_id=labelstudio_project_id,\n        task_type=task_type,\n        timezone=timezone,\n        start_date=start_date,\n        end_date=end_date,\n    )\n    tagged_df.execution_options.caching_strategy.max_cache_staleness = (\n        \"P0D\"  # disables caching\n    )\n    s3_upload = upload2s3_op(\n        path_on_disk=tagged_df.outputs[\"output\"],\n        reference=f\"datasets/{org_id}_{job_id}\",\n        file_type=f\"tagged\",\n        bucket=pipeline_constants.KUBEFLOW_SANDBOX_BUCKET,\n        ext=\".csv\",\n    )\n\n    notification_text = f\"Here is your data for {org_id=} and {job_id=}.\"\n    code_block = f\"aws s3 cp {s3_upload.output} .\"\n    with kfp.dsl.Condition(notify != \"\", \"notify\").after(s3_upload) as check1:\n        task_no_cache = slack_notification_op(\n            notification_text,\n            code_block=code_block,\n            cc=notify,\n            channel=channel,\n            thread_id=slack_thread,\n        )\n        task_no_cache.execution_options.caching_strategy.max_cache_staleness = (\n            \"P0D\"  # disables caching\n        )\n\n\n__all__ = [\"fetch_tagged_calls_dataset\"]\n"
  },
  {
    "repo": "biswaroop1547/skit-pipelines",
    "file_path": "skit_pipelines/pipelines/fetch_tagged_data_from_labelstore/__init__.py",
    "raw_url": "https://raw.githubusercontent.com/biswaroop1547/skit-pipelines/main/skit_pipelines/pipelines/fetch_tagged_data_from_labelstore/__init__.py",
    "content": "import kfp\n\nfrom skit_pipelines import constants as pipeline_constants\nfrom skit_pipelines.components import (\n    fetch_tagged_data_label_store_op,\n    slack_notification_op,\n    upload2s3_op,\n)\n\n\n@kfp.dsl.pipeline(\n    name=\"Fetch annotated data from label store\",\n    description=\"A pipeline aimed at querying intent, entity, and transcriptions that happen across Skit\",\n)\ndef fetch_tagged_data_from_labelstore(\n    flow_id: str,\n    start_date: str = \"\",\n    end_date: str = \"\",\n    limit: int = 2000,\n    notify: str = \"\",\n    channel: str = \"\",\n    slack_thread: str = \"\",\n    data_labels: str = \"\",\n):\n    \"\"\"\n    A pipeline aimed at querying intent, entity, and transcriptions that happen across Skit\n\n    .. _p_fetch_tagged_data_from_labelstore:\n\n    Example payload to invoke via slack integrations:\n\n        @charon run fetch_tagged_data_from_labelstore\n\n        .. code-block:: python\n\n            {\n                \"flow_id\": \"294\",\n                \"limit\": 20,\n                \"start_date\": \"2022-11-12\",\n                \"end_date\": \"2022-11-16\",\n                \"data_labels\": \"Client, Live\"\n            }\n\n    :param flow_id: The id of the flow from which annotated data should be queried\n    :type flow_id: str\n\n    :param start_date: The start date range (YYYY-MM-DD) to filter tagged data. defaults to yesterday\n    :type start_date: str, optional\n\n    :param end_date: The end date range (YYYY-MM-DD) to filter tagged data, defaults to today\n    :type end_date: str, optional\n\n    :param limit: Number of annotations to fetch, defaults to 2000\n    :type limit: int, optional\n\n    :param notify: Whether to send a slack notification, defaults to \"\"\n    :type notify: str, optional\n\n    :param channel: The slack channel to send the notification, defaults to \"\"\n    :type channel: str, optional\n\n    :param slack_thread: The slack thread to send the notification, defaults to \"\"\n    :type slack_thread: float, optional\n\n    :param data_labels: Comma seperated data labels to filter, defaults to \"\"\n    :type data_labels: str, optional\n    \"\"\"\n    tagged_df = fetch_tagged_data_label_store_op(\n        flow_id=flow_id,\n        start_date=start_date,\n        end_date=end_date,\n        limit=limit,\n        data_labels=data_labels,\n    )\n\n    tagged_df.execution_options.caching_strategy.max_cache_staleness = (\n        \"P0D\"  # disables caching\n    )\n\n    s3_upload = upload2s3_op(\n        path_on_disk=tagged_df.outputs[\"output\"],\n        reference=f\"{flow_id}-{start_date}-{end_date}\",\n        file_type=f\"annotations-with-call-context\",\n        bucket=pipeline_constants.KUBEFLOW_SANDBOX_BUCKET,\n        ext=\".csv\",\n    )\n\n    s3_upload.execution_options.caching_strategy.max_cache_staleness = (\n        \"P0D\"  # disables caching\n    )\n\n    notification_text = (\n        f\"Here is your data for {flow_id=} and date_range: {start_date=}, {end_date=}.\"\n    )\n    code_block = f\"aws s3 cp {s3_upload.output} .\"\n    with kfp.dsl.Condition(notify != \"\", \"notify\").after(s3_upload) as check1:\n        task_no_cache = slack_notification_op(\n            notification_text,\n            code_block=code_block,\n            cc=notify,\n            channel=channel,\n            thread_id=slack_thread,\n        )\n        task_no_cache.execution_options.caching_strategy.max_cache_staleness = (\n            \"P0D\"  # disables caching\n        )\n\n\n__all__ = [\"fetch_tagged_data_from_labelstore\"]\n"
  },
  {
    "repo": "biswaroop1547/skit-pipelines",
    "file_path": "skit_pipelines/pipelines/fetch_tagged_entity_dataset/__init__.py",
    "raw_url": "https://raw.githubusercontent.com/biswaroop1547/skit-pipelines/main/skit_pipelines/pipelines/fetch_tagged_entity_dataset/__init__.py",
    "content": "import kfp\n\nfrom skit_pipelines import constants as pipeline_constants\nfrom skit_pipelines.components import (\n    fetch_tagged_dataset_op,\n    modify_entity_dataset_op,\n    slack_notification_op,\n    upload2s3_op,\n)\n\n\n@kfp.dsl.pipeline(\n    name=\"Fetch Tagged Entities Dataset Pipeline\",\n    description=\"fetches tagged entity dataset from tog & does few modifications for eval\",\n)\ndef fetch_tagged_entity_dataset(\n    org_id: str,\n    job_id: str = \"\",\n    labelstudio_project_id: str = \"\",\n    start_date: str = \"\",\n    end_date: str = \"\",\n    timezone: str = \"Asia/Kolkata\",\n    task_type: str = \"conversation\",\n    notify: str = \"\",\n    channel: str = \"\",\n    slack_thread: str = \"\",\n):\n    \"\"\"\n    A pipeline to fetch tagged entity dataset wiht modifications ready for eval.\n\n    .. _p_fetch_tagged_entity_dataset:\n\n    Example payload to invoke via slack integrations:\n\n        @charon run fetch_tagged_entity_dataset\n\n        .. code-block:: python\n\n            {\n                \"org_id\": 1,\n                \"job_id\": \"4011\",\n                \"start_date\": \"2020-01-01\",\n                \"end_date\": \"2020-01-01\"\n            }\n\n    To use labelstudio:\n\n        @charon run fetch_tagged_entity_dataset\n\n        .. code-block:: python\n\n            {\n                \"org_id\": 1,\n                \"labelstudio_project_id\": \"40\",\n                \"start_date\": \"2020-01-01\",\n                \"end_date\": \"2020-01-01\"\n            }\n\n    :param org_id: reference path to save the metrics.\n    :type org_id: str\n    :param job_ids: The job ids as per tog. Optional if labestudio project id is provided.\n    :type job_id: str\n    :param labelstudio_project_id: The labelstudio project id (this is a number) since this is optional, defaults to \"\".\n    :type labelstudio_project_id: str\n    :param start_date: The start date range (YYYY-MM-DD) to filter tagged data.\n    :type start_date: str\n    :param end_date: The end date range (YYYY-MM-DD) to filter tagged data\n    :type end_date: str\n    :param timezone: The timezone to apply for multi-region datasets, defaults to \"Asia/Kolkata\"\n    :type timezone: str, optional\n    :param task_type: https://github.com/skit-ai/skit-labels#task-types, defaults to \"conversation\"\n    :type task_type: str, optional\n    :param notify: A comma separated list of slack ids: \"@apples, @orange.fruit\" etc, defaults to \"\"\n    :type notify: str, optional\n    :param channel: The slack channel to send the notification, defaults to \"\"\n    :type channel: str, optional\n    :param slack_thread: The slack thread to send the notification, defaults to \"\"\n    :type slack_thread: str, optional\n    \"\"\"\n\n    tagged_df = fetch_tagged_dataset_op(\n        job_id=job_id,\n        project_id=labelstudio_project_id,\n        task_type=task_type,\n        timezone=timezone,\n        start_date=start_date,\n        end_date=end_date,\n    )\n    tagged_df.execution_options.caching_strategy.max_cache_staleness = (\n        \"P0D\"  # disables caching\n    )\n\n    modified_df = modify_entity_dataset_op(\n        tagged_df.outputs[\"output\"],\n        tog_job_id=job_id,\n        labelstudio_project_id=labelstudio_project_id,\n        timezone=timezone,\n    )\n    modified_df.execution_options.caching_strategy.max_cache_staleness = (\n        \"P0D\"  # disables caching\n    )\n\n    s3_upload = upload2s3_op(\n        path_on_disk=modified_df.outputs[\"output\"],\n        reference=f\"{org_id}_{job_id}\",\n        file_type=f\"tagged_entity\",\n        bucket=pipeline_constants.KUBEFLOW_SANDBOX_BUCKET,\n        ext=\".csv\",\n    )\n\n    notification_text = f\"Here is your tagged entity data for {org_id=} and {job_id=}.\"\n    code_block = f\"aws s3 cp {s3_upload.output} .\"\n    with kfp.dsl.Condition(notify != \"\", \"notify\").after(s3_upload) as check1:\n        task_no_cache = slack_notification_op(\n            notification_text,\n            code_block=code_block,\n            cc=notify,\n            channel=channel,\n            thread_id=slack_thread,\n        )\n        task_no_cache.execution_options.caching_strategy.max_cache_staleness = (\n            \"P0D\"  # disables caching\n        )\n\n\n__all__ = [\"fetch_tagged_entity_dataset\"]\n"
  },
  {
    "repo": "biswaroop1547/skit-pipelines",
    "file_path": "skit_pipelines/pipelines/generate_and_tag_conversations/__init__.py",
    "raw_url": "https://raw.githubusercontent.com/biswaroop1547/skit-pipelines/main/skit_pipelines/pipelines/generate_and_tag_conversations/__init__.py",
    "content": "import kfp\nfrom kfp.components import OutputPath\nfrom typing import Optional\n\nfrom skit_pipelines import constants as pipeline_constants\nfrom skit_pipelines.components import (\n    upload2s3_op,\n    zip_file_and_notify_op,\n    slack_notification_op,\n    validate_and_add_situations_to_db_op,\n    final_conversation_generator_op,\n    upload_conv_to_label_studio_op,\n    upload_conversation_data_to_metrics_db_op,\n)\n\n\n@kfp.dsl.pipeline(\n    name=\"Generate and tag conversations\",\n    description=\"Generate final conversations based on the situation data provided and upload it to labelstudio for tagging\",\n)\ndef generate_and_tag_conversations(\n    *,\n    situations: str = \"\",\n    scenario: str = \"\",\n    scenario_category: str = \"\",\n    s3_links_to_prompts: str = \"\",\n    llm_trainer_repo_name: str = \"LLMtrainer\",\n    llm_trainer_repo_branch: str = \"main\",\n    model: str = 'gpt-4',\n    n_iter: int = 1,\n    n_choice: int = 2,\n    temperature: float = 0.99,\n    client_id: str,\n    template_id: str,\n    labelstudio_project_id: str,\n    data_label: str = \"\",\n    project_name: str = \"\",\n    notify: str = \"\",\n    channel: str = \"\",\n    slack_thread: str = \"\"\n    ):\n    \"\"\"\n    A pipeline to generate and tag conversations given a situation\n    \n    .. _p_generate_and_tag_conversations:\n\n    Example payload to invoke via slack integrations:\n\n    A minimal example:\n\n        @charon run generate_and_tag_conversations\n\n        .. code-block:: python\n\n            {   \"situations\" : \"The user disputes the debt, so the agent transfers the call to the agent :: The user cannot pay any amount as they have a difficult situation, so the agent hangs up the call. \",\n                \"scenario\" : \"Test scenario\",\n                \"scenario_category\" : \"Test scenario category\",\n                \"llm_trainer_repo_branch\" : \"refactor-data-gen-script\",\n                \"client_id\" : \"85\",\n                \"template_id\" : \"0\",\n                \"labelstudio_project_id\" : \"95\",\n                \"s3_links_to_prompts\": \"s3://kubeflow-us-cluster/pipeline_uploads/prompt/test_prompt.txt\",\n                \"data_label\" : \"UAT\",\n                \"project_name\" : \"test project name\"\n            }\n\n\n    A full available parameters example:\n\n        @charon run generate_and_tag_conversations\n\n        .. code-block:: python\n\n            {   \"situations\" : \"The user disputes the debt, so the agent transfers the call to the agent :: The user cannot pay any amount as they have a difficult situation, so the agent hangs up the call. \",\n                \"scenario\" : \"Test scenario\",\n                \"scenario_category\" : \"Test scenario category\",\n                \"llm_trainer_repo_branch\" : \"refactor-data-gen-script\",\n                \"client_id\" : \"85\",\n                \"template_id\" : \"0\",\n                \"labelstudio_project_id\" : \"95\",\n                \"s3_links_to_prompts\": \"s3://kubeflow-us-cluster/pipeline_uploads/prompt/test_prompt.txt\",\n                \"data_label\" : \"UAT\",\n                \"project_name\" : \"test project name\"\n            }\n    \n    :param situations: The situations for generating the conversations, use delimiter :: to pass multiple situations\n    :type situations: optional\n\n    :param scenario: The scenario linked to the situation\n    :type scenario: optional\n    \n    :param scenario_category: The scenarios category\n    :type scenario_category: optional\n    \n    :param prompt: Prompt to the model for data generation\n    type prompt: str\n    \n    :param s3_links_to_prompts: s3 links to the prompt to the model for data generation\n    :type s3_links_to_prompts: str\n    \n    :param output_dir: The output directory where the generated conversations gets stored\n    :type output_dir: str\n\n    :param filename: Acts as a prfix to the default naming used\n    :type filename: str\n\n    :param llm_trainer_repo_name: The conversation generation repo name in Github.\n    :type llm_trainer_repo_name: str\n    \n    :param llm_trainer_repo_branch: The branch name in the conversation generation repo to use , defaults to main.\n    :type llm_trainer_repo_branch: str, optional\n    \n    :param model: Optional model to be used for generating data \n    :type model: str\n    \n    :param n_iter: No of times we make iterate on scenarios list to generate conversations\n    type n_iter: int\n    \n    :param n_choice: No of convs generated in a single time from a scenario.\n    type n_choice: int\n    \n    :param temperature: Temperature\n    type temperature: float\n    \n    :param client_id: id of the client for which data is being generated\n    :type client_id : str\n    \n    :param template_id: template id for which data is being generated\n    :type template_id : str\n    \n    :param project_name: project name to distinguish between various experiments\n    :type project_name : str\n    \n    :param notify: Whether to send a slack notification, defaults to \"\"\n    :type notify: str, optional\n\n    :param channel: The slack channel to send the notification, defaults to \"\"\n    :type channel: str, optional\n\n    :param slack_thread: The slack thread to send the notification, defaults to \"\"\n    :type slack_thread: str, optional\n\n    \"\"\"\n    \n    validate_situations = validate_and_add_situations_to_db_op(situations=situations,\n                                                         scenario=scenario ,  \n                                                         scenario_category=scenario_category)\n    \n    situations_id_info = validate_situations.outputs['situation_mapping_info']\n    conv_s3_dir_name = f'llm_artifacts/generated_conversations/{client_id}_{template_id}'\n    \n    conv_generation_output= final_conversation_generator_op(situation_info_list=situations_id_info,\n                                                        s3_links_to_prompts = s3_links_to_prompts,\n                                                        n_iter=n_iter,\n                                                        n_choice=n_choice,\n                                                        temperature=temperature,\n                                                        model=model,\n                                                        llm_trainer_repo_name=llm_trainer_repo_name,\n                                                        llm_trainer_repo_branch=llm_trainer_repo_branch,\n                                                    )\n    conversations_dir = conv_generation_output.outputs[\"output\"]\n        \n    conversation_s3_upload = upload2s3_op(\n            path_on_disk=conversations_dir,\n            reference=conv_s3_dir_name,\n            bucket=pipeline_constants.KUBEFLOW_SANDBOX_BUCKET,\n            upload_as_directory=True,\n            ext=\"\"\n        )\n\n    tag_calls_output = upload_conv_to_label_studio_op(project_id=labelstudio_project_id, \n                                                      conversations_dir= conversations_dir, \n                                                      data_label=data_label, \n                                                      situations_id_info=situations_id_info)\n    \n    \n    upload_df_sizes = tag_calls_output.outputs[\"df_sizes\"]\n    upload_errors = tag_calls_output.outputs[\"errors\"] \n    \n    \n    with kfp.dsl.Condition(notify != \"\", \"notify\").after(conversation_s3_upload) as check1:\n        notification_text_1 = f\"Generated conversations are successfully uploaded to s3 for client_id  : {client_id}.\"\n        code_block = f\"aws s3 cp {conversation_s3_upload.output} .\"\n        prompt_s3_notif = slack_notification_op(\n            message=notification_text_1,\n            channel=channel,\n            cc=notify,\n            code_block=code_block,\n            thread_id=slack_thread,\n        )\n        \n        prompt_s3_notif.execution_options.caching_strategy.max_cache_staleness = (\n            \"P0D\"  # disables caching\n        )\n        \n        notification_text_2 = \"Here is the ZIP file generated by the Generate Sample conversations Pipeline.\"\n        zip_file_and_notify = zip_file_and_notify_op(\n                    path_on_disk = conversations_dir, \n                    message = notification_text_2,\n                    channel = channel,\n                    thread_id = slack_thread,\n                    file_title = 'generated_conversations',\n                    file_name = 'generated_conversations.zip',\n                    notify = notify,\n                    display_sample = True,\n                    ).after(prompt_s3_notif)\n        \n        zip_file_and_notify.execution_options.caching_strategy.max_cache_staleness = (\n            \"P0D\"  # disables caching\n        )\n    \n    with kfp.dsl.Condition(notify != \"\", \"notify\").after(tag_calls_output) as check2:\n\n        notification_text = f\"\"\"Uploaded the {upload_df_sizes} conversations for tagging to {labelstudio_project_id=}.\"\"\"\n        \n        notification_text += f\"\\nErrors: {upload_errors}\" if upload_errors else \"\"\n\n        task_no_cache = slack_notification_op(\n            notification_text, channel=channel, cc=notify, thread_id=slack_thread\n        )\n        task_no_cache.execution_options.caching_strategy.max_cache_staleness = (\n            \"P0D\"  # disables caching\n        )\n\n    with kfp.dsl.Condition(upload_errors == [], \"upload_to_metrics_db\").after(tag_calls_output) as check3:\n        upload_to_metrics_db_op = upload_conversation_data_to_metrics_db_op(situations_id_info=situations_id_info, client_id=client_id,\n                                                                            template_id=template_id, \n                                                                            generated_conversations_s3_link=conversation_s3_upload.output,\n                                                                            prompt_links_in_s3=s3_links_to_prompts, conv_directory=conversations_dir, \n                                                                            project_name=project_name)\n    \n\n__all__ = [\"generate_and_tag_conversations\"]\n"
  },
  {
    "repo": "biswaroop1547/skit-pipelines",
    "file_path": "skit_pipelines/pipelines/generate_sample_conversations/__init__.py",
    "raw_url": "https://raw.githubusercontent.com/biswaroop1547/skit-pipelines/main/skit_pipelines/pipelines/generate_sample_conversations/__init__.py",
    "content": "import kfp\nfrom kfp.components import OutputPath\nfrom typing import Optional\n\nfrom skit_pipelines import constants as pipeline_constants\nfrom skit_pipelines.components import (\n    sample_conversations_generator_op,\n    upload2s3_op,\n    zip_file_and_notify_op,\n    slack_notification_op\n)\n\n\n@kfp.dsl.pipeline(\n    name=\"Generate sample conversations\",\n    description=\"Generate sample conversations based on the situation data provided\",\n)\ndef generate_sample_conversations(\n    *,\n    situations: Optional[str],\n    s3_links_to_prompts: str = \"\",\n    filename: str = \"\",\n    llm_trainer_repo_name: str = \"LLMtrainer\",\n    llm_trainer_repo_branch: str = \"main\",\n    model: str = 'gpt-4',\n    n_iter: int = 1,\n    n_choice: int = 2,\n    temperature: float = 0.99,\n    notify: str = \"\",\n    channel: str = \"\",\n    slack_thread: str = \"\"\n    ):\n    \"\"\"\n    A pipeline to sample conversations given a situation\n    \n    .. _p_generate_sample_conversations:\n\n    Example payload to invoke via slack integrations:\n\n    A minimal example:\n\n        @charon run generate_sample_conversations\n\n        .. code-block:: python\n\n            {\n                \"situations\": \"The user wants to talk to a human agent, so the agent transfers the call\",\n                \"llm_trainer_repo_name\": \"LLMtrainer\",\n                \"llm_trainer_repo_branch\": \"main\"\n                }\n\n\n    A full available parameters example:\n\n        @charon run generate_sample_conversations\n\n        .. code-block:: python\n\n            {\n                \"situations\": \"The user disputes the debt, so the agent transfers the call to the agent :: The user cannot pay any amount as they have a difficult situation, so the agent hangs up the call. \"\n                \"llm_trainer_repo_name\": \"LLMtrainer\",\n                \"llm_trainer_repo_branch\": \"main\",\n            }\n\n    :param situations: The situations for generating the conversations\n    :type situations: optional\n    \n    :param prompt: Prompt to the model for data generation\n    type prompt: str\n\n    :param filename: Acts as a prfix to the default naming used\n    :type filename: str\n\n    :param llm_trainer_repo_name: The conversation generation repo name in Github.\n    :type llm_trainer_repo_name: str\n    \n    :param llm_trainer_repo_branch: The branch name in the conversation generation repo to use , defaults to main.\n    :type llm_trainer_repo_branch: str, optional\n    \n    :param model: Optional model to be used for generating data \n    :type model: str\n    \n    :param n_iter: No of times we make iterate on sub_scenarios list to generate conversations\n    type n_iter: int\n    \n    :param n_choice: No of convs generated in a single time from a scenario.\n    type n_choice: int\n    \n    :param temperature: Temperature\n    type temperature: float\n    \n    :param notify: Whether to send a slack notification, defaults to \"\"\n    :type notify: str, optional\n\n    :param channel: The slack channel to send the notification, defaults to \"\"\n    :type channel: str, optional\n\n    :param slack_thread: The slack thread to send the notification, defaults to \"\"\n    :type slack_thread: str, optional\n\n    \"\"\"\n    from skit_pipelines import constants as pipeline_constants\n    \n    prompt_generation = sample_conversations_generator_op(\n        situations=situations,\n        llm_trainer_repo_name=llm_trainer_repo_name,\n        llm_trainer_repo_branch=llm_trainer_repo_branch,\n        filename=filename,\n        model=model,\n        prompt_file_path=s3_links_to_prompts,\n        n_iter=n_iter,\n        n_choice=n_choice,\n        temperature=temperature\n    )\n\n    prompt_s3_upload = upload2s3_op(\n            path_on_disk=prompt_generation.outputs[\"output\"],\n            reference='llm_artifacts/generated_conversations/',\n            bucket=pipeline_constants.KUBEFLOW_SANDBOX_BUCKET,\n            upload_as_directory=True,\n            ext=\"\"\n        )\n\n    with kfp.dsl.Condition(notify != \"\", \"notify\").after(prompt_s3_upload):\n        notification_text_1 = f\"Generated conversations are successfully uploaded to s3.\"\n        code_block = f\"aws s3 cp {prompt_s3_upload.output} .\"\n        prompt_s3_notif = slack_notification_op(\n            message=notification_text_1,\n            channel=channel,\n            cc=notify,\n            code_block=code_block,\n            thread_id=slack_thread,\n        )\n        \n        prompt_s3_notif.execution_options.caching_strategy.max_cache_staleness = (\n            \"P0D\"  # disables caching\n        )\n        \n        notification_text_2 = \"Here is the ZIP file generated by the Generate Sample conversations Pipeline.\"\n        zip_file_and_notify = zip_file_and_notify_op(\n                    path_on_disk = prompt_generation.outputs[\"output\"], \n                    message = notification_text_2,\n                    channel = channel,\n                    thread_id = slack_thread,\n                    file_title = 'generated_conversations',\n                    file_name = 'generated_conversations.zip',\n                    notify = notify,\n                    display_sample = True,\n                    ).after(prompt_s3_notif)\n        \n        zip_file_and_notify.execution_options.caching_strategy.max_cache_staleness = (\n            \"P0D\"  # disables caching\n        )\n\n\n__all__ = [\"generate_sample_conversations\"]\n"
  },
  {
    "repo": "biswaroop1547/skit-pipelines",
    "file_path": "skit_pipelines/pipelines/invalidate_llm_situations_in_db/__init__.py",
    "raw_url": "https://raw.githubusercontent.com/biswaroop1547/skit-pipelines/main/skit_pipelines/pipelines/invalidate_llm_situations_in_db/__init__.py",
    "content": "from typing import Optional\n\nimport kfp\n\nfrom skit_pipelines.components import (\n    invalidate_situations_in_db_op,\n    slack_notification_op\n)\n\n\n@kfp.dsl.pipeline(\n    name=\"Invalidate situations\",\n    description=\"\"\"Sets conversations as invalid, thereby preventing it from being used for training\"\"\",\n)\ndef invalidate_llm_situations_in_db(\n    situation_id_list: str = \"\",\n    notify: str = \"\",\n    channel: str = \"\",\n    slack_thread: str = \"\",\n):\n    \"\"\"\n    A pipeline to discard situations by setting flag is_valid to False for situations that are no longer needed\n\n    .. invalidate_llm_situations_in_db:\n\n    Example payload to invoke via slack integrations:\n\n    @charon run invalidate_llm_situations_in_db\n\n    .. code-block:: python\n\n        {\n            \"situation_id_list\": \"1, 3, 5\"\n        }\n    \n    :param situation_id_list: A comma separated list of situation ids from the situation_scenario_mapper table: \"1, 2\" etc, defaults to \"\"\n    :type situation_id_list: str \n    :param notify: A comma separated list of slack ids: \"@apples, @orange.fruit\" etc, defaults to \"\"\n    :type notify: str, optional\n    :param channel: The slack channel to send the notification, defaults to \"\"\n    :type channel: str, optional\n    :param slack_thread: The slack thread to send the notification, defaults to \"\"\n    :type slack_thread: str, optional\n\n    \"\"\"\n\n    update_situation_validity = invalidate_situations_in_db_op(\n        situation_id=situation_id_list\n    )\n\n    with kfp.dsl.Condition(notify != \"\", \"notify\").after(update_situation_validity):\n        notification_text = f\"is_valid has been set to False for the situations : {situation_id_list}\"\n\n        task_no_cache = slack_notification_op(\n            notification_text,\n            cc=notify,\n            channel=channel,\n            thread_id=slack_thread,\n        )\n        task_no_cache.execution_options.caching_strategy.max_cache_staleness = (\n            \"P0D\"  # disables caching\n        )\n\n\n__all__ = [\"invalidate_llm_situations_in_db\"]\n"
  },
  {
    "repo": "biswaroop1547/skit-pipelines",
    "file_path": "skit_pipelines/pipelines/publish_compliance_breaches/__init__.py",
    "raw_url": "https://raw.githubusercontent.com/biswaroop1547/skit-pipelines/main/skit_pipelines/pipelines/publish_compliance_breaches/__init__.py",
    "content": "from typing import Optional\n\nimport kfp\n\nfrom skit_pipelines.components import (\n    fetch_calls_op,\n    identify_compliance_breaches_llm_op,\n    push_compliance_report_to_postgres_op,\n    slack_notification_op,\n)\n\n\n@kfp.dsl.pipeline(\n    name=\"Check calls for compliance breaches\",\n    description=\"\"\"Fetches sampled calls from production db and checks for potential compliance breaches (only \n        for US collections application\"\"\",\n)\ndef publish_compliance_breaches(\n    lang: str,\n    template_id: Optional[str] = None,\n    start_date: str = \"\",\n    end_date: str = \"\",\n    start_date_offset: int = 0,\n    end_date_offset: int = 0,\n    reported: bool = False,\n    call_quantity: int = 1000,\n    notify: str = \"\",\n    channel: str = \"\",\n    slack_thread: str = \"\",\n):\n    \"\"\"\n    A pipeline to sample calls in a given time range and check if there are any compliance breaches. A LLM model is\n    used to identify these breaches by sending entire conversations. The results are persisted in the 'ML_metrics'\n    database from where they can be queried whenever required.\n\n    .. _publish_compliance_breaches:\n\n    Example payload to invoke via slack integrations:\n\n    @charon run publish_compliance_breaches\n\n    .. code-block:: python\n\n        {\n            \"lang\": \"en\",\n            \"template_id\": 100,\n            \"start_date\": \"2022-11-10\",\n            \"end_date\": \"2022-11-11\",\n            \"reported\": false,\n            \"call_quantity\": 500\n        }\n\n    :param lang: The language code of the calls to filter. eg: en, hi, ta, te, etc.\n    :type lang: str\n    :param template_id: The flow template id to filter calls, defaults to \"\"\n    :type template_id: str, optional\n    :param start_date: The start date range to filter calls in YYYY-MM-DD format.\n    :type start_date: str\n    :param end_date: The end date range to filter calls in YYYY-MM-DD format.\n    :type end_date: str\n    :param start_date_offset: Number of days from current date to start querying calls\n    :type start_date_offset: int, optional\n    :param end_date_offset: Number of days from current date to stop querying calls\n    :type end_date_offset: int, optional\n    :param reported: Pick only reported calls, defaults to False\n    :type reported: bool\n    :param call_quantity: Number of calls to sample, defaults to 1000\n    :type call_quantity: int, optional\n    :param notify: A comma separated list of slack ids: \"@apples, @orange.fruit\" etc, defaults to \"\"\n    :type notify: str, optional\n    :param channel: The slack channel to send the notification, defaults to \"\"\n    :type channel: str, optional\n    :param slack_thread: The slack thread to send the notification, defaults to \"\"\n    :type slack_thread: str, optional\n\n    \"\"\"\n\n    calls = fetch_calls_op(\n        lang=lang,\n        start_date=start_date,\n        end_date=end_date,\n        call_quantity=call_quantity,\n        start_date_offset=start_date_offset,\n        end_date_offset=end_date_offset,\n        template_id=template_id,\n        reported=reported,\n        client_id=\"\",\n        remove_empty_audios=False,\n    )\n\n    compliance_breach_report = identify_compliance_breaches_llm_op(\n        s3_file_path=calls.output\n    )\n\n    push_to_postgres = push_compliance_report_to_postgres_op(\n        s3_file_path=compliance_breach_report.output\n    )\n\n    with kfp.dsl.Condition(notify != \"\", \"notify\").after(push_to_postgres):\n        notification_text = f\" Tried generating report for {call_quantity} calls. Among these {push_to_postgres.output} had compliance breaches\"\n        code_block = f\"aws s3 cp {compliance_breach_report.output} .\"\n\n        task_no_cache = slack_notification_op(\n            notification_text,\n            cc=notify,\n            channel=channel,\n            code_block=code_block,\n            thread_id=slack_thread,\n        )\n        task_no_cache.execution_options.caching_strategy.max_cache_staleness = (\n            \"P0D\"  # disables caching\n        )\n\n\n__all__ = [\"publish_compliance_breaches\"]\n"
  },
  {
    "repo": "biswaroop1547/skit-pipelines",
    "file_path": "skit_pipelines/pipelines/retrain_slu/__init__.py",
    "raw_url": "https://raw.githubusercontent.com/biswaroop1547/skit-pipelines/main/skit_pipelines/pipelines/retrain_slu/__init__.py",
    "content": "import kfp\n\nfrom skit_pipelines import constants as pipeline_constants\nfrom skit_pipelines.components import (\n    create_mr_op,\n    download_csv_from_s3_op,\n    download_yaml_op,\n    fetch_tagged_dataset_op,\n    file_contents_to_markdown_s3_op,\n    retrain_slu_from_repo_op,\n    slack_notification_op,\n    upload2s3_op,\n)\n\nUTTERANCES = pipeline_constants.UTTERANCES\nINTENT_Y = pipeline_constants.INTENT_Y\nBUCKET = pipeline_constants.BUCKET\nCSV_FILE = pipeline_constants.CSV_FILE\nCPU_NODE_LABEL = pipeline_constants.CPU_NODE_LABEL\nGPU_NODE_LABEL = pipeline_constants.GPU_NODE_LABEL\nNODESELECTOR_LABEL = pipeline_constants.POD_NODE_SELECTOR_LABEL\n\n\n@kfp.dsl.pipeline(\n    name=\"SLU retraining Pipeline\",\n    description=\"Retrains an existing SLU model.\",\n)\ndef retrain_slu(\n    *,\n    repo_name: str,\n    repo_branch: str = \"master\",\n    job_ids: str = \"\",\n    dataset_path: str = \"\",\n    custom_test_dataset_path: str = \"\",\n    labelstudio_project_ids: str = \"\",\n    job_start_date: str = \"\",\n    job_end_date: str = \"\",\n    remove_intents: str = \"\",\n    alias_yaml_path: str = \"\",\n    initial_training: bool = False,\n    use_previous_dataset: bool = True,\n    epochs: int = 10,\n    train_split_percent: int = 85,\n    stratify: bool = False,\n    target_mr_branch: str = \"sandbox\",\n    notify: str = \"\",\n    channel: str = \"\",\n    slack_thread: str = \"\",\n    core_slu_repo_name: str = \"core-slu-service\",\n    core_slu_repo_branch: str = \"master\",\n    customization_repo_name: str = \"customization\",\n    customization_repo_branch: str = \"master\",\n):\n    \"\"\"\n    A pipeline to retrain an existing SLU model.\n\n    .. _p_retrain_slu:\n\n    Example payload to invoke via slack integrations:\n\n    A minimal example:\n\n        @charon run retrain_slu\n\n        .. code-block:: python\n\n            {\n                \"repo_name\": \"slu_repo_name\",\n                \"labelstudio_project_ids\": \"10,13\"\n            }\n\n\n    A full available parameters example:\n\n        @charon run retrain_slu\n\n        .. code-block:: python\n\n            {\n                \"repo_name\": \"slu_repo_name\",\n                \"repo_branch\": \"master\",\n                \"dataset_path\": \"s3://bucket-name/path1/to1/data1.csv,s3://bucket-name/path2/to2/data2.csv\",\n                \"job_ids\": \"4011,4012\",\n                \"labelstudio_project_ids\": \"10,13\",\n                \"job_start_date\": \"2022-08-01\",\n                \"job_end_date\": \"2022-09-19\",\n                \"remove_intents\": \"_confirm_,_oos_,audio_speech_unclear,ood\",\n                \"alias_yaml_path\": \"intents/oppo/alias.yaml\",\n                \"use_previous_dataset\": True,\n                \"train_split_percent\": 85,\n                \"stratify\": False,\n                \"epochs\": 10,\n            }\n\n\n    Training an SLU for first time example:\n\n        @charon run retrain_slu\n\n        .. code-block:: python\n\n            {\n                \"repo_name\": \"slu_repo_name\",\n                \"repo_branch\": \"master\",\n                \"labelstudio_project_ids\": \"10,13\",\n                \"initial_training\": True\n            }\n\n\n    :param repo_name: SLU repository name under /vernacularai/ai/clients org in gitlab.\n    :type repo_name: str\n\n    :param repo_branch: The branch name in the SLU repository one wants to use, defaults to master.\n    :type repo_name: str, optional\n\n    :param dataset_path: The S3 URI or the S3 key for the tagged dataset (can be multiple - comma separated).\n    :type dataset_path: str, optional\n\n    :param custom_test_dataset_path: The S3 URI or the S3 key for the tagged dataset to be used for model evaluation (can be multiple - comma separated).\n    :type custom_test_dataset_path: str, optional\n\n    :param job_ids: The job ids as per tog. Optional if labestudio_project_ids is provided.\n    :type job_ids: str\n\n    :param labelstudio_project_ids: The labelstudio project id (this is a number) since this is optional, defaults to \"\".\n    :type labelstudio_project_ids: str\n\n    :param job_start_date: The start date range (YYYY-MM-DD) to filter tagged data.\n    :type job_start_date: str, optional\n\n    :param job_end_date: The end date range (YYYY-MM-DD) to filter tagged data\n    :type job_end_date: str, optional\n\n    :param remove_intents: Comma separated list of intents to remove from dataset while training.\n    :type remove_intents: str, optional\n\n    :param alias_yaml_path: eevee's intent_report alias.yaml, refer docs `here <https://skit-ai.github.io/eevee/metrics/intents.html#aliasing>`_ . Upload your yaml to eevee-yamls repository `here <https://github.com/skit-ai/eevee-yamls>`_ & pass the relative path of the yaml from base of the repository.\n    :type alias_yaml_path: str, optional\n\n    :param initial_training: Set to true only if you're training a model for the first time, defaults to False.\n    :type initial_training: bool, optional\n\n    :param use_previous_dataset: Before retraining combines new dataset with last dataset the model was trained on, defaults to True.\n    :type use_previous_dataset: bool, optional\n\n    :param train_split_percent: Percentage of new data one should train the model on, defaults to 85.\n    :type train_split_percent: int, optional\n\n    :param stratify: For stratified splitting of dataset into train and test set, defaults to False.\n    :type stratify: bool, optional\n\n    :param core_slu_repo_name: Name of repository for core slu service. Defaults to core-slu-service\n    :type core_slu_repo_name: str, optional\n\n    :param core_slu_repo_branch: Branch to check out for core slu repository. Defaults to master\n    :type core_slu_repo_branch: str, optional\n\n    :param customization_repo_name: Name of repository for customization service. Defaults to customization\n    :type customization_repo_name: str, optional\n\n    :param customization_repo_branch: Branch to check out for customization service repository. Defaults to master\n    :type customization_repo_branch: str, optional\n\n    :param target_mr_branch: Target branch against which the MR will be created. Defaults to sandbox\n    :type target_mr_branch: str, optional\n\n    :param notify: Whether to send a slack notification, defaults to \"\"\n    :type notify: str, optional\n\n    :param channel: The slack channel to send the notification, defaults to \"\"\n    :type channel: str, optional\n\n    :param slack_thread: The slack thread to send the notification, defaults to \"\"\n    :type slack_thread: str, optional\n\n    \"\"\"\n\n    tagged_s3_data_op = download_csv_from_s3_op(storage_path=dataset_path)\n    tagged_s3_data_op.execution_options.caching_strategy.max_cache_staleness = (\n        \"P0D\"  # disables caching\n    )\n\n    tagged_job_data_op = fetch_tagged_dataset_op(\n        job_id=job_ids,\n        project_id=labelstudio_project_ids,\n        task_type=\"conversation\",\n        timezone=\"Asia/Kolkata\",\n        start_date=job_start_date,\n        end_date=job_end_date,\n        empty_possible=True,\n    )\n    tagged_job_data_op.execution_options.caching_strategy.max_cache_staleness = (\n        \"P0D\"  # disables caching\n    )\n\n    custom_test_tagged_s3_data_op = download_csv_from_s3_op(\n        storage_path=custom_test_dataset_path, empty_possible=True\n    )\n    custom_test_tagged_s3_data_op.execution_options.caching_strategy.max_cache_staleness = (\n        \"P0D\"  # disables caching\n    )\n\n    downloaded_alias_yaml_op = download_yaml_op(\n        git_host_name=pipeline_constants.GITLAB,\n        yaml_path=alias_yaml_path,\n    )\n\n    validate_training_setup_op = retrain_slu_from_repo_op(\n        tagged_s3_data_op.outputs[\"output\"],\n        custom_test_tagged_s3_data_op.outputs[\"output\"],\n        tagged_job_data_op.outputs[\"output\"],\n        downloaded_alias_yaml_op.outputs[\"output\"],\n        bucket=BUCKET,\n        repo_name=repo_name,\n        branch=repo_branch,\n        remove_intents=remove_intents,\n        use_previous_dataset=use_previous_dataset,\n        train_split_percent=train_split_percent,\n        stratify=stratify,\n        epochs=epochs,\n        initial_training=initial_training,\n        labelstudio_project_ids=labelstudio_project_ids,\n        s3_paths=dataset_path,\n        validate_setup=True,\n        customization_repo_name=customization_repo_name,\n        customization_repo_branch=customization_repo_branch,\n        core_slu_repo_name=core_slu_repo_name,\n        core_slu_repo_branch=core_slu_repo_branch,\n    ).set_ephemeral_storage_limit(\"20G\")\n    validate_training_setup_op.display_name = \"Validate Training Setup\"\n\n    retrained_op = retrain_slu_from_repo_op(\n        tagged_s3_data_op.outputs[\"output\"],\n        custom_test_tagged_s3_data_op.outputs[\"output\"],\n        tagged_job_data_op.outputs[\"output\"],\n        downloaded_alias_yaml_op.outputs[\"output\"],\n        bucket=BUCKET,\n        repo_name=repo_name,\n        branch=repo_branch,\n        remove_intents=remove_intents,\n        use_previous_dataset=use_previous_dataset,\n        train_split_percent=train_split_percent,\n        stratify=stratify,\n        epochs=epochs,\n        initial_training=initial_training,\n        labelstudio_project_ids=labelstudio_project_ids,\n        s3_paths=dataset_path,\n        customization_repo_name=customization_repo_name,\n        customization_repo_branch=customization_repo_branch,\n        core_slu_repo_name=core_slu_repo_name,\n        core_slu_repo_branch=core_slu_repo_branch,\n    ).after(validate_training_setup_op)\n    retrained_op.set_gpu_limit(1).add_node_selector_constraint(\n        label_name=NODESELECTOR_LABEL, value=GPU_NODE_LABEL\n    )\n\n    comparison_upload_cf = upload2s3_op(\n        path_on_disk=retrained_op.outputs[\"comparison_classification_report\"],\n        reference=repo_name,\n        file_type=\"comparison_classification_report\",\n        bucket=BUCKET,\n        ext=CSV_FILE,\n    )\n    comparison_upload_cf.execution_options.caching_strategy.max_cache_staleness = (\n        \"P0D\"  # disables caching\n    )\n    comparison_classification_report_markdown_file_op = (\n        file_contents_to_markdown_s3_op(\n            ext=CSV_FILE,\n            path_on_disk=retrained_op.outputs[\"comparison_classification_report\"],\n            file_title=\"## Comparison Classification Report (latest,prod)\",\n        )\n    )\n\n    comparison_upload_cm = upload2s3_op(\n        path_on_disk=retrained_op.outputs[\"comparison_confusion_matrix\"],\n        reference=repo_name,\n        file_type=\"comparison_confusion_matrix\",\n        bucket=BUCKET,\n        ext=CSV_FILE,\n    )\n    comparison_confusion_report_markdown_file_op = (\n        file_contents_to_markdown_s3_op(\n            ext=CSV_FILE,\n            path_on_disk=retrained_op.outputs[\"comparison_confusion_matrix\"],\n            file_title=\"## Comparison Confusion Matrix (latest, prod)\",\n        )\n    )\n\n    mr_response_op = create_mr_op(\n        git_host_name=pipeline_constants.GITLAB,\n        repo_name=repo_name,\n        project_path=pipeline_constants.GITLAB_SLU_PROJECT_CONFIG_PATH,\n        target_branch=target_mr_branch,\n        source_branch=retrained_op.outputs[\"output\"],\n        mr_title=\"Auto retrained changes\",\n        s3_description_paths=f\"{comparison_classification_report_markdown_file_op.output}, {comparison_confusion_report_markdown_file_op.output}\",\n    )\n\n    with kfp.dsl.Condition(notify != \"\", \"notify\").after(retrained_op, mr_response_op):\n        notification_text = f\"Finished training {repo_name} SLU, please review <{mr_response_op.output}|this MR>\"\n        task_no_cache = slack_notification_op(\n            notification_text,\n            channel=channel,\n            cc=notify,\n            thread_id=slack_thread,\n        )\n        task_no_cache.execution_options.caching_strategy.max_cache_staleness = (\n            \"P0D\"  # disables caching\n        )\n\n    with kfp.dsl.Condition(notify != \"\", name=\"slack_notify\").after(\n        comparison_upload_cf, comparison_upload_cm\n    ):\n        notification_text = f\"Here's the IRR report.\"\n        code_block = f\"aws s3 cp {comparison_upload_cf.output} .\"\n        irr_notif = slack_notification_op(\n            notification_text,\n            channel=channel,\n            cc=notify,\n            code_block=code_block,\n            thread_id=slack_thread,\n        )\n        irr_notif.execution_options.caching_strategy.max_cache_staleness = (\n            \"P0D\"  # disables caching\n        )\n\n        notification_text = f\"Here's the confusion matrix.\"\n        code_block = f\"aws s3 cp {comparison_upload_cm.output} .\"\n        cm_notif = slack_notification_op(\n            notification_text,\n            channel=channel,\n            cc=notify,\n            code_block=code_block,\n            thread_id=slack_thread,\n        )\n        cm_notif.execution_options.caching_strategy.max_cache_staleness = (\n            \"P0D\"  # disables caching\n        )\n\n\n__all__ = [\"retrain_slu\"]\n"
  },
  {
    "repo": "biswaroop1547/skit-pipelines",
    "file_path": "skit_pipelines/pipelines/retrain_slu_old/__init__.py",
    "raw_url": "https://raw.githubusercontent.com/biswaroop1547/skit-pipelines/main/skit_pipelines/pipelines/retrain_slu_old/__init__.py",
    "content": "import kfp\n\nfrom skit_pipelines import constants as pipeline_constants\nfrom skit_pipelines.components import (\n    create_mr_op,\n    download_csv_from_s3_op,\n    download_repo_op,\n    download_yaml_op,\n    fetch_tagged_dataset_op,\n    file_contents_to_markdown_s3_op,\n    retrain_slu_from_repo_op_old,\n    slack_notification_op,\n    upload2s3_op,\n)\n\nUTTERANCES = pipeline_constants.UTTERANCES\nINTENT_Y = pipeline_constants.INTENT_Y\nBUCKET = pipeline_constants.BUCKET\nCSV_FILE = pipeline_constants.CSV_FILE\nCPU_NODE_LABEL = pipeline_constants.CPU_NODE_LABEL\nGPU_NODE_LABEL = pipeline_constants.GPU_NODE_LABEL\nNODESELECTOR_LABEL = pipeline_constants.POD_NODE_SELECTOR_LABEL\n\n\n@kfp.dsl.pipeline(\n    name=\"SLU retraining Pipeline\",\n    description=\"Retrains an existing SLU model.\",\n)\ndef retrain_slu_old(\n    *,\n    repo_name: str,\n    repo_branch: str = \"master\",\n    job_ids: str = \"\",\n    dataset_path: str = \"\",\n    labelstudio_project_ids: str = \"\",\n    job_start_date: str = \"\",\n    job_end_date: str = \"\",\n    remove_intents: str = \"\",\n    alias_yaml_path: str = \"\",\n    initial_training: bool = False,\n    use_previous_dataset: bool = True,\n    epochs: int = 10,\n    train_split_percent: int = 85,\n    stratify: bool = False,\n    target_mr_branch: str = \"sandbox\",\n    notify: str = \"\",\n    channel: str = \"\",\n    slack_thread: str = \"\",\n    customization_repo_name: str = \"slu-customization\",\n    customization_repo_branch: str = \"master\",\n):\n    \"\"\"\n    A pipeline to retrain an existing SLU model.\n\n    .. _p_retrain_slu:\n\n    Example payload to invoke via slack integrations:\n\n    A minimal example:\n\n        @charon run retrain_slu\n\n        .. code-block:: python\n\n            {\n                \"repo_name\": \"slu_repo_name\",\n                \"labelstudio_project_ids\": \"10,13\"\n            }\n\n\n    A full available parameters example:\n\n        @charon run retrain_slu\n\n        .. code-block:: python\n\n            {\n                \"repo_name\": \"slu_repo_name\",\n                \"repo_branch\": \"master\",\n                \"dataset_path\": \"s3://bucket-name/path1/to1/data1.csv,s3://bucket-name/path2/to2/data2.csv\",\n                \"job_ids\": \"4011,4012\",\n                \"labelstudio_project_ids\": \"10,13\",\n                \"job_start_date\": \"2022-08-01\",\n                \"job_end_date\": \"2022-09-19\",\n                \"remove_intents\": \"_confirm_,_oos_,audio_speech_unclear,ood\",\n                \"alias_yaml_path\": \"intents/oppo/alias.yaml\",\n                \"use_previous_dataset\": True,\n                \"train_split_percent\": 85,\n                \"stratify\": False,\n                \"epochs\": 10,\n            }\n\n\n    Training an SLU for first time example:\n\n        @charon run retrain_slu\n\n        .. code-block:: python\n\n            {\n                \"repo_name\": \"slu_repo_name\",\n                \"repo_branch\": \"master\",\n                \"labelstudio_project_ids\": \"10,13\",\n                \"initial_training\": True\n            }\n\n\n    :param repo_name: SLU repository name under /vernacularai/ai/clients org in gitlab.\n    :type repo_name: str\n\n    :param repo_branch: The branch name in the SLU repository one wants to use, defaults to master.\n    :type repo_name: str, optional\n\n    :param dataset_path: The S3 URI or the S3 key for the tagged dataset (can be multiple - comma separated).\n    :type dataset_path: str, optional\n\n    :param job_ids: The job ids as per tog. Optional if labestudio_project_ids is provided.\n    :type job_ids: str\n\n    :param labelstudio_project_ids: The labelstudio project id (this is a number) since this is optional, defaults to \"\".\n    :type labelstudio_project_ids: str\n\n    :param epochs: Number of epchs to train the model, defaults to 10\n    :type epochs: int, optional\n\n    :param job_start_date: The start date range (YYYY-MM-DD) to filter tagged data.\n    :type job_start_date: str, optional\n\n    :param job_end_date: The end date range (YYYY-MM-DD) to filter tagged data\n    :type job_end_date: str, optional\n\n    :param remove_intents: Comma separated list of intents to remove from dataset while training.\n    :type remove_intents: str, optional\n\n    :param alias_yaml_path: eevee's intent_report alias.yaml, refer docs `here <https://skit-ai.github.io/eevee/metrics/intents.html#aliasing>`_ . Upload your yaml to eevee-yamls repository `here <https://github.com/skit-ai/eevee-yamls>`_ & pass the relative path of the yaml from base of the repository.\n    :type alias_yaml_path: str, optional\n\n    :param initial_training: Set to true only if you're training a model for the first time, defaults to False.\n    :type initial_training: bool, optional\n\n    :param use_previous_dataset: Before retraining combines new dataset with last dataset the model was trained on, defaults to True.\n    :type use_previous_dataset: bool, optional\n\n    :param train_split_percent: Percentage of new data one should train the model on, defaults to 85.\n    :type train_split_percent: int, optional\n\n    :param stratify: For stratified splitting of dataset into train and test set, defaults to False.\n    :type stratify: bool, optional\n\n    :param notify: Whether to send a slack notification, defaults to \"\"\n    :type notify: str, optional\n\n    :param channel: The slack channel to send the notification, defaults to \"\"\n    :type channel: str, optional\n\n    :param slack_thread: The slack thread to send the notification, defaults to \"\"\n    :type slack_thread: str, optional\n\n    \"\"\"\n\n    tagged_s3_data_op = download_csv_from_s3_op(storage_path=dataset_path)\n    tagged_s3_data_op.execution_options.caching_strategy.max_cache_staleness = (\n        \"P0D\"  # disables caching\n    )\n\n    tagged_job_data_op = fetch_tagged_dataset_op(\n        job_id=job_ids,\n        project_id=labelstudio_project_ids,\n        task_type=\"conversation\",\n        timezone=\"Asia/Kolkata\",\n        start_date=job_start_date,\n        end_date=job_end_date,\n        empty_possible=True,\n    )\n    tagged_job_data_op.execution_options.caching_strategy.max_cache_staleness = (\n        \"P0D\"  # disables caching\n    )\n\n    downloaded_repo_op = download_repo_op(\n        git_host_name=pipeline_constants.GITLAB,\n        repo_name=repo_name,\n        project_path=pipeline_constants.GITLAB_SLU_PROJECT_PATH,\n    )\n\n    downloaded_repo_op.execution_options.caching_strategy.max_cache_staleness = (\n        \"P0D\"  # disables caching\n    )\n\n    # downloaded_customization_repo_op = download_repo_op(\n    #     repo_name=customization_repo_name,\n    # )\n    # downloaded_customization_repo_op.display_name = \"Download SLU customization repo\"\n\n    # downloaded_customization_repo_op.execution_options.caching_strategy.max_cache_staleness = (\n    #     \"P0D\"  # disables caching\n    # )\n\n    downloaded_alias_yaml_op = download_yaml_op(\n        git_host_name=pipeline_constants.GITLAB,\n        yaml_path=alias_yaml_path,\n    )\n\n    validate_training_setup_op = retrain_slu_from_repo_op_old(\n        tagged_s3_data_op.outputs[\"output\"],\n        tagged_job_data_op.outputs[\"output\"],\n        downloaded_repo_op.outputs[\"repo\"],\n        downloaded_alias_yaml_op.outputs[\"output\"],\n        bucket=BUCKET,\n        repo_name=repo_name,\n        branch=repo_branch,\n        remove_intents=remove_intents,\n        use_previous_dataset=use_previous_dataset,\n        train_split_percent=train_split_percent,\n        stratify=stratify,\n        epochs=epochs,\n        initial_training=initial_training,\n        job_ids=job_ids,\n        labelstudio_project_ids=labelstudio_project_ids,\n        s3_paths=dataset_path,\n        validate_setup=True,\n        customization_repo_name=customization_repo_name,\n        customization_repo_branch=customization_repo_branch,\n    )\n    validate_training_setup_op.display_name = \"Validate Training Setup\"\n\n    retrained_op = retrain_slu_from_repo_op_old(\n        tagged_s3_data_op.outputs[\"output\"],\n        tagged_job_data_op.outputs[\"output\"],\n        downloaded_repo_op.outputs[\"repo\"],\n        downloaded_alias_yaml_op.outputs[\"output\"],\n        bucket=BUCKET,\n        repo_name=repo_name,\n        branch=repo_branch,\n        remove_intents=remove_intents,\n        use_previous_dataset=use_previous_dataset,\n        train_split_percent=train_split_percent,\n        stratify=stratify,\n        epochs=epochs,\n        initial_training=initial_training,\n        job_ids=job_ids,\n        labelstudio_project_ids=labelstudio_project_ids,\n        s3_paths=dataset_path,\n        customization_repo_name=customization_repo_name,\n        customization_repo_branch=customization_repo_branch,\n    ).after(validate_training_setup_op)\n    retrained_op.set_gpu_limit(1).add_node_selector_constraint(\n        label_name=NODESELECTOR_LABEL, value=GPU_NODE_LABEL\n    )\n\n    # upload test set metrics.\n    upload_cf = upload2s3_op(\n        path_on_disk=retrained_op.outputs[\"output_classification_report\"],\n        reference=repo_name,\n        file_type=\"test_classification_report\",\n        bucket=BUCKET,\n        ext=CSV_FILE,\n    )\n\n    upload_cm = upload2s3_op(\n        path_on_disk=retrained_op.outputs[\"output_confusion_matrix\"],\n        reference=repo_name,\n        file_type=\"test_confusion_matrix\",\n        bucket=BUCKET,\n        ext=CSV_FILE,\n    )\n    upload_cm.execution_options.caching_strategy.max_cache_staleness = (\n        \"P0D\"  # disables caching\n    )\n\n    classification_report_markdown_file_op = file_contents_to_markdown_s3_op(\n        ext=CSV_FILE,\n        path_on_disk=retrained_op.outputs[\"output_classification_report\"],\n        file_title=\"## Classification Report\",\n    )\n\n    confusion_matrix_markdown_file_op = file_contents_to_markdown_s3_op(\n        ext=CSV_FILE,\n        path_on_disk=retrained_op.outputs[\"output_confusion_matrix\"],\n        file_title=\"## Confusion Matrix\",\n    )\n\n    mr_response_op = create_mr_op(\n        git_host_name=pipeline_constants.GITLAB,\n        repo_name=repo_name,\n        project_path=pipeline_constants.GITLAB_SLU_PROJECT_PATH,\n        target_branch=target_mr_branch,\n        source_branch=retrained_op.outputs[\"output\"],\n        mr_title=\"Auto retrained changes\",\n        s3_description_paths=f\"{classification_report_markdown_file_op.output},{confusion_matrix_markdown_file_op.output}\",\n    )\n\n    with kfp.dsl.Condition(notify != \"\", \"notify\").after(retrained_op, mr_response_op):\n        notification_text = f\"Finished training {repo_name} SLU, please review <{mr_response_op.output}|this MR>\"\n        task_no_cache = slack_notification_op(\n            notification_text,\n            channel=channel,\n            cc=notify,\n            thread_id=slack_thread,\n        )\n        task_no_cache.execution_options.caching_strategy.max_cache_staleness = (\n            \"P0D\"  # disables caching\n        )\n\n    with kfp.dsl.Condition(notify != \"\", name=\"slack_notify\").after(\n        upload_cf, upload_cm\n    ):\n        notification_text = f\"Here's the IRR report.\"\n        code_block = f\"aws s3 cp {upload_cf.output} .\"\n        irr_notif = slack_notification_op(\n            notification_text,\n            channel=channel,\n            cc=notify,\n            code_block=code_block,\n            thread_id=slack_thread,\n        )\n        irr_notif.execution_options.caching_strategy.max_cache_staleness = (\n            \"P0D\"  # disables caching\n        )\n\n        notification_text = f\"Here's the confusion matrix.\"\n        code_block = f\"aws s3 cp {upload_cm.output} .\"\n        cm_notif = slack_notification_op(\n            notification_text,\n            channel=channel,\n            cc=notify,\n            code_block=code_block,\n            thread_id=slack_thread,\n        )\n        cm_notif.execution_options.caching_strategy.max_cache_staleness = (\n            \"P0D\"  # disables caching\n        )\n\n\n__all__ = [\"retrain_slu_old\"]\n"
  },
  {
    "repo": "biswaroop1547/skit-pipelines",
    "file_path": "skit_pipelines/pipelines/transcription_pipeline/__init__.py",
    "raw_url": "https://raw.githubusercontent.com/biswaroop1547/skit-pipelines/main/skit_pipelines/pipelines/transcription_pipeline/__init__.py",
    "content": "import kfp\nimport pandas as pd\n\nfrom skit_pipelines import constants as pipeline_constants\nfrom skit_pipelines.components import (\n    audio_transcription_op,\n    download_audio_wavs_op,\n    download_csv_from_s3_op,\n    download_file_from_s3_op,\n    overlay_transcription_csv_op,\n    re_presign_s3_urls_op,\n    slack_notification_op,\n    upload2s3_op,\n)\n\nBUCKET = pipeline_constants.BUCKET\n\n\n@kfp.dsl.pipeline(\n    name=\"Transcription Pipeline\",\n    description=\"Transcribe the audio data using the mentioned ASR models\",\n)\ndef transcription_pipeline(\n    *,\n    data_s3_path: str,\n    config_s3_path: str,\n    audio_sample_rate: str = \"8k\",\n    audio_download_workers: int = 30,\n    transcription_concurrency: int = 8,\n    notify: str = \"\",\n    channel: str = \"\",\n    slack_thread: str = \"\",\n):\n\n    \"\"\"\n    A pipeline to transcribe the audio files present in a dataset using different ASRs.\n\n    .. _p_transcription_pipeline:\n\n    Example payload to invoke via slack integrations:\n\n        @charon run transcription_pipeline\n\n        .. code-block:: python\n\n            {\n\n            }\n\n    :param data_s3_path: S3 path of the data in CSV\n    :type data_s3_path: str\n    :param config_s3_path: the config yaml to be used by blaze. Refer to (https://github.com/skit-ai/blaze#config) for more info.\n    :type config_s3_path: str\n    :param audio_sample_rate: audio sample rate / frequency of output audios. (default \"8k\").\n    :type audio_sample_rate: str\n    :param audio_download_workers: maximum workers while downloading the audios (default 30).\n    :type audio_download_workers: int\n    :param transcription_concurrency: maximum workers while transcribing the audios (default 8).\n    :type transcription_concurrency: int\n\n    \"\"\"\n    # Download CSV files with audio\n    original_data_op = download_csv_from_s3_op(storage_path=data_s3_path)\n    original_data_op.execution_options.caching_strategy.max_cache_staleness = (\n        \"P0D\"  # disables caching\n    )\n\n    config_data_op = download_file_from_s3_op(storage_path=config_s3_path)\n    config_data_op.execution_options.caching_strategy.max_cache_staleness = (\n        \"P0D\"  # disables caching\n    )\n\n    # re-presign the s3 links present in .csv, so that they are accessible\n    # does presigning again only if the links are expired\n    re_presigned_op = re_presign_s3_urls_op(original_data_op.outputs[\"output\"])\n    re_presigned_op.execution_options.caching_strategy.max_cache_staleness = (\"P0D\")\n\n    # Download audio files from CSV\n    audio_wavs_op = download_audio_wavs_op(\n        re_presigned_op.outputs[\"output\"], audio_sample_rate, audio_download_workers\n    )\n\n    # Transcribing\n    transcribed_sqlite_op = audio_transcription_op(\n        audio_wavs_op.outputs[\"output\"],\n        config_data_op.outputs[\"output\"],\n        concurrency=transcription_concurrency,\n    )\n\n    # overlay the original csv (original_data_op) with the new transcriptions (transcribed_sqlite_op)\n    overlayed_data_op = overlay_transcription_csv_op(\n        transcribed_sqlite_op.outputs[\"output\"], original_data_op.outputs[\"output\"]\n    )\n\n    # Returning S3 path\n    audio_s3_path = upload2s3_op(\n        path_on_disk=overlayed_data_op.outputs[\"output\"], bucket=BUCKET, ext=\".csv\"\n    )\n\n    with kfp.dsl.Condition(notify != \"\", name=\"slack_notify\").after(\n        audio_s3_path\n    ) as audio_check:\n        notification_text = f\"Here's the CSV after transcription.\"\n        code_block = f\"aws s3 cp {audio_s3_path.output} .\"\n        audio_notif = slack_notification_op(\n            notification_text,\n            channel=channel,\n            cc=notify,\n            code_block=code_block,\n            thread_id=slack_thread,\n        )\n        audio_notif.execution_options.caching_strategy.max_cache_staleness = (\n            \"P0D\"  # disables caching\n        )\n\n\n__all__ = [\"transcription_pipeline\"]\n"
  },
  {
    "repo": "Taha-Cakir/Kubeflow-Pipelines-Deployment-GCP",
    "file_path": "model-deployment-kubeflow.py",
    "raw_url": "https://raw.githubusercontent.com/Taha-Cakir/Kubeflow-Pipelines-Deployment-GCP/main/model-deployment-kubeflow.py",
    "content": "from kfp.v2 import dsl\nfrom kfp.v2.dsl import (Input,Output,Metrics,component,Model)\nfrom google.cloud.aiplatform import pipeline_jobs\nfrom typing import NamedTuple\nfrom kfp.v2 import compiler\n\n\n@component(\npackages_to_install=[\"gcsfs\",\"pandas\",\"google-cloud-storage\"]\n)\ndef validate_input_ds(filename:str)-> NamedTuple(\"output\", [(\"input_validation\", str)]):\n\n    import logging\n    from google.cloud import storage\n    import pandas as pd\n\n    logging.basicConfig(level=logging.INFO)\n\n    logging.info(f\"Reading file: {filename}\")\n    df = pd.read_csv(filename)\n    expected_num_cols = 26\n    num_cols = len(df.columns)\n\n    logging.info(f\"Number of columns: {num_cols}\")\n    \n    input_validation=\"true\"\n    \n    if num_cols != expected_num_cols:\n        input_validation=\"false\"\n        \n    expected_col_names = ['destination', 'passanger', 'weather', 'temperature', 'time', 'coupon',\n                               'expiration', 'gender', 'age', 'maritalStatus', 'has_children',\n                               'education', 'occupation', 'income', 'car', 'Bar', 'CoffeeHouse',\n                               'CarryAway', 'RestaurantLessThan20', 'Restaurant20To50',\n                               'toCoupon_GEQ5min', 'toCoupon_GEQ15min', 'toCoupon_GEQ25min',\n                               'direction_same', 'direction_opp', 'Y']\n\n    if set(df.columns) != set(expected_col_names):\n        input_validation=\"false\"\n\n    return (input_validation,)\n\n\n@component(\npackages_to_install=[\"google-cloud-aiplatform\",\"gcsfs\",\"xgboost\",\"category_encoders\",\"imblearn\",\"pandas\",\"google-cloud-storage\"]\n)\ndef custom_training_job_component(\n    max_depth:int,\n    learning_rate:float,\n    n_estimators:int,\n    metrics: Output[Metrics]\n)->NamedTuple(\"output\", [(\"model_validation\", str)]):\n    import pandas as pd\n    from sklearn.metrics import precision_score, recall_score, roc_auc_score, accuracy_score\n    from sklearn.model_selection import train_test_split\n    from category_encoders import HashingEncoder\n    from imblearn.over_sampling import SMOTE\n    from xgboost import XGBClassifier\n    from google.cloud import storage\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(\"mlops-cakir-kubeflow-v1\")\n\n    def load_data(file_path):\n        df = pd.read_csv(file_path)\n        return df\n\n    def preprocess_data(df):\n\n        df = df.drop(columns=['car', 'toCoupon_GEQ5min', 'direction_opp'])\n        df = df.fillna(df.mode().iloc[0])\n        df = df.drop_duplicates()\n\n        df_dummy = df.copy()\n        age_list = []\n        for i in df['age']:\n            if i == 'below21':\n                age = '<21'\n            elif i in ['21', '26']:\n                age = '21-30'\n            elif i in ['31', '36']:\n                age = '31-40'\n            elif i in ['41', '46']:\n                age = '41-50'\n            else:\n                age = '>50'\n            age_list.append(age)\n        df_dummy['age'] = age_list\n\n        df_dummy['passanger_destination'] = df_dummy['passanger'].astype(str) + '-' + df_dummy['destination'].astype(str)\n        df_dummy['marital_hasChildren'] = df_dummy['maritalStatus'].astype(str) + '-' + df_dummy['has_children'].astype(str)\n        df_dummy['temperature_weather'] = df_dummy['temperature'].astype(str) + '-' + df_dummy['weather'].astype(str)\n        df_dummy = df_dummy.drop(columns=['passanger', 'destination', 'maritalStatus', 'has_children', 'temperature','weather', 'Y'])\n\n        df_dummy = pd.concat([df_dummy, df['Y']], axis = 1)\n        df_dummy = df_dummy.drop(columns=['gender', 'RestaurantLessThan20'])\n        df_le = df_dummy.replace({\n            'expiration':{'2h': 0, '1d' : 1},\n            'age':{'<21': 0, '21-30': 1, '31-40': 2, '41-50': 3, '>50': 4},\n            'education':{'Some High School': 0, 'High School Graduate': 1, 'Some college - no degree': 2,\n                         'Associates degree': 3, 'Bachelors degree': 4, 'Graduate degree (Masters or Doctorate)': 5},\n            'Bar':{'never': 0, 'less1': 1, '1~3': 2, '4~8': 3, 'gt8': 4},\n            'CoffeeHouse':{'never': 0, 'less1': 1, '1~3': 2, '4~8': 3, 'gt8': 4}, \n            'CarryAway':{'never': 0, 'less1': 1, '1~3': 2, '4~8': 3, 'gt8': 4}, \n            'Restaurant20To50':{'never': 0, 'less1': 1, '1~3': 2, '4~8': 3, 'gt8': 4},\n            'income':{'Less than $12500':0, '$12500 - $24999':1, '$25000 - $37499':2, '$37500 - $49999':3,\n                      '$50000 - $62499':4, '$62500 - $74999':5, '$75000 - $87499':6, '$87500 - $99999':7,\n                      '$100000 or More':8},\n            'time':{'7AM':0, '10AM':1, '2PM':2, '6PM':3, '10PM':4}\n        })\n\n        x = df_le.drop('Y', axis=1)\n        y = df_le.Y\n\n        return x, y\n\n    def train_model(x_train, y_train,max_depth,learning_rate,n_estimators):\n        \n        model = XGBClassifier(\n            max_depth=max_depth,\n            learning_rate=learning_rate,\n            n_estimators=n_estimators,\n            random_state=42,\n            use_label_encoder=False\n        )\n        model.fit(x_train, y_train)\n        return model\n\n    def evaluate_model(model, x_test, y_test, x_sm_train_hashing, y_sm_train):\n        y_pred = model.predict(x_test)\n        y_pred_proba = model.predict_proba(x_test)\n        y_pred_train = model.predict(x_sm_train_hashing)\n        y_pred_train_proba = model.predict_proba(x_sm_train_hashing)\n\n        accuracy = accuracy_score(y_test, y_pred)\n        precision = precision_score(y_test, y_pred)\n        recall = recall_score(y_test, y_pred)\n\n        # roc_auc_train_proba = roc_auc_score(y_sm_train, y_pred_train_proba[:, 1])\n        # roc_auc_test_proba = roc_auc_score(y_test, y_pred_proba[:, 1])\n\n        return accuracy,precision,recall\n\n    def encode_features(x, n_components=27):\n        hashing_ros_enc = HashingEncoder(cols=['passanger_destination', 'marital_hasChildren', 'occupation', 'coupon',\n                                               'temperature_weather'], n_components=n_components).fit(x)\n        x_test_hashing = hashing_ros_enc.transform(x.reset_index(drop=True))\n        return x_test_hashing\n\n    def oversample_data(x_train_hashing, y_train):\n        sm = SMOTE(random_state=42)\n        x_sm_train_hashing, y_sm_train = sm.fit_resample(x_train_hashing, y_train)\n        return x_sm_train_hashing, y_sm_train\n\n    def save_model_artifact(pipeline):\n        artifact_name = 'model.bst'\n        pipeline.save_model(artifact_name)\n        model_artifact = bucket.blob('mlops-recommendation/artifacts/'+artifact_name)\n        model_artifact.upload_from_filename(artifact_name)\n\n    input_file = \"gs://mlops-cakir-kubeflow-v1-kubeflow-v1/mlops-recommendation/in-vehicle-coupon-recommendation.csv\"\n    df = load_data(input_file)\n    x, y = preprocess_data(df)\n\n    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n\n    x_train.fillna(x_train.mode().iloc[0], inplace=True)\n    x_test.fillna(x_train.mode().iloc[0], inplace=True)\n    \n    model_name = 'xgboost'\n    print(\"Training and evaluating\", model_name, \"model:\")\n    x_train_hashing = encode_features(x_train)\n    x_test_hashing = encode_features(x_test)\n    x_sm_train_hashing, y_sm_train = oversample_data(x_train_hashing,y_train)\n\n    pipeline = train_model(x_sm_train_hashing,y_sm_train,max_depth,learning_rate,n_estimators)\n\n    accuracy,precision,recall = evaluate_model(pipeline,x_test_hashing,y_test,x_sm_train_hashing,y_sm_train)\n    metrics.log_metric(\"accurancy\", accuracy)\n    metrics.log_metric(\"precision\", precision)\n    metrics.log_metric(\"recall\", recall)\n    \n    model_validation = \"true\"\n    if accuracy>0.5 and precision>0.5 :\n        save_model_artifact(pipeline)\n        model_validation=\"true\"\n    else :\n        model_validation=\"false\"\n\n    return (model_validation,)\n\n\n\n@component(\n    packages_to_install=[\"google-cloud-aiplatform\"]\n)\ndef model_deployment()-> NamedTuple(\"endpoint\", [(\"endpoint\", str)]):\n    \n    from google.cloud import aiplatform\n    \n    aiplatform.init(project=\"cakir-kubeflow\", location=\"us-central1\", staging_bucket=\"gs://mlops-cakir-kubeflow-v1\")\n    \n    model = aiplatform.Model.upload(\n        display_name=\"mlops-recommendation-model\",\n        artifact_uri=\"gs://mlops-cakir-kubeflow-v1/mlops-recommendation/artifacts/\",\n        serving_container_image_uri = \"us-docker.pkg.dev/vertex-ai/prediction/xgboost-cpu.1-6:latest\",\n        sync=False\n    )\n    \n    DEPLOYED_NAME = \"coupon-model-endpoint\"\n    TRAFFIC_SPLIT = {\"0\": 100}\n    MIN_NODES = 1\n    MAX_NODES = 1\n\n    endpoint = model.deploy(\n        deployed_model_display_name=DEPLOYED_NAME,\n        traffic_split=TRAFFIC_SPLIT,\n        machine_type=\"n1-standard-4\",\n        min_replica_count=MIN_NODES,\n        max_replica_count=MAX_NODES\n    )\n\n\n@dsl.pipeline(\n    pipeline_root=\"gs://mlops-cakir-kubeflow-v1/coupon-pipeline-v1\",\n    name=\"coupon-model-training-pipeline\",\n)\ndef pipeline(\n    project: str = \"cakir-kubeflow\",\n    region: str = \"us-central1\"\n    ):\n    \n    max_depth=5\n    learning_rate=0.2\n    n_estimators=40\n    \n    file_name = \"gs://mlops-cakir-kubeflow-v1/mlops-recommendation/in-vehicle-coupon-recommendation.csv\"\n    input_validation_task = validate_input_ds(file_name)\n    \n    with dsl.Condition(input_validation_task.outputs[\"input_validation\"] == \"true\"):\n        model_training = custom_training_job_component(\n            max_depth=max_depth,\n            learning_rate=learning_rate,\n            n_estimators=n_estimators,\n        ).after(input_validation_task)\n        \n        with dsl.Condition(model_training.outputs[\"model_validation\"] == \"true\"):\n            task_deploy_model = model_deployment().after(model_training)\n\n\nif __name__ == \"__main__\":\n     # Create an argument parser\n    parser = argparse.ArgumentParser(description='Data Drift Script')\n    parser.add_argument('--display_name', type=str, help='pipeline display name')\n    parser.add_argument('--location', type=str, help='region of pipeline')\n\n    # Parse the command-line arguments\n    args = parser.parse_args()\n    location = args.location\n    display_name = args.display_name\n\n    compiler.Compiler().compile(pipeline_func=pipeline,package_path='coupon-pipeline-deploy-v1.json')\n\n    start_pipeline = pipeline_jobs.PipelineJob(\n        display_name=display_name,\n        template_path=\"mlops-recommendation-pipeline-deploy-v1.json\",\n        enable_caching=False,\n        location=args.test,\n    )\n\n    start_pipeline.run()"
  },
  {
    "repo": "kaiomurz/kubeflow-imdb",
    "file_path": "imdb_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/kaiomurz/kubeflow-imdb/main/imdb_pipeline.py",
    "content": "\nimport imp\nimport kfp\nimport kfp.components as comp\n\n\n######################################\n########## IMPORT FUNCTIONS ##########\n######################################\n\nfrom functions.download_and_save_data import download_and_save_data_from_s3\nfrom functions.shuffle import shuffle\nfrom functions.split_to_x_y import split_to_x_y\nfrom functions.encode_target import encode_target\nfrom functions.clean_text import clean_text\nfrom functions.preprocess_text import preprocess_text\nfrom functions.split_to_train_test import split_to_train_test\nfrom functions.create_and_train_model import create_and_train_model\nfrom functions.create_and_save_performance_artifacts import create_and_save_performance_artifacts\n\n# dsl-compile --py imdb_pipeline.py --output imdb_pipeline.yaml\n# https://github.com/kubeflow/pipelines/tree/sdk/release-1.8/samples/core\n\n\n#####################################################\n########## CONVERT FUNCTIONS TO COMPONENTS ##########\n#####################################################\n\nbase_image = \"kaiomurz/kubeflow-imdb:latest\"\n\ndownload_and_save_data_from_s3_op = kfp.components.create_component_from_func(\n    download_and_save_data_from_s3,\n    base_image=base_image\n)\n\nshuffle_op = kfp.components.create_component_from_func(\n    shuffle,\n    base_image=base_image\n)\n\nsplit_to_x_y_op = kfp.components.create_component_from_func(\n    split_to_x_y,\n    base_image=base_image\n)\n\nencode_target_op = kfp.components.create_component_from_func(\n    encode_target,\n    base_image=base_image\n)\n\nclean_text_op = kfp.components.create_component_from_func(\n    clean_text,\n    base_image=base_image\n)\n\npreprocess_text_op = kfp.components.create_component_from_func(\n    preprocess_text,\n    base_image=base_image\n)\n\nsplit_to_train_test_op = kfp.components.create_component_from_func(\n    split_to_train_test,\n    base_image=base_image    \n)\ncreate_and_train_model_op = kfp.components.create_component_from_func(\n    create_and_train_model,\n    base_image=base_image\n)\ncreate_and_save_performance_artifacts_op = kfp.components.create_component_from_func(\n    create_and_save_performance_artifacts,\n    base_image=base_image\n)\n\n# test_pickling_op = kfp.components.create_component_from_func(\n#     test_pickling,\n#     base_image=base_image\n# )\n# pickle_load_test_op = kfp.components.create_component_from_func(\n#     pickle_load_test,\n#     base_image=base_image\n# )\n# split_to_train_test_op = kfp.components.create_component_from_func(\n#     split_to_train_test,\n#     base_image=base_image\n# )\n\n\n\n###########################################\n########## CREATING THE PIPELINE ##########\n###########################################\n\n\n@kfp.dsl.pipeline(\n    name='imdb_pipeline',\n    description='Pipeline to train and serve sentiment analysis model on IMDB dataset'\n)\ndef imdb_pipeline(\n    aws_access_key_id: str,\n    aws_secret_access_key: str,\n    Bucket: str='imdb-kubeflow',\n    Key: str='data.csv',\n    y_heading: str='sentiment',\n    tokenizer_num_words: int=1000,\n    shuffle: bool=True,\n    split: float = 0.8,\n    fraction: float=0.1,\n    num_epochs: int=10\n    ):\n\n    download_and_save_data_from_s3_task = download_and_save_data_from_s3_op(\n        Bucket, \n        Key,\n        aws_access_key_id,\n        aws_secret_access_key\n    )\n\n    shuffle_task = shuffle_op(\n        download_and_save_data_from_s3_task.outputs['data_csv'],\n        shuffle,\n        fraction\n    )\n\n    split_to_x_y_task = split_to_x_y_op(\n        shuffle_task.outputs['out_data_csv'],\n        y_heading\n    )\n\n    encode_target_task = encode_target_op(\n        split_to_x_y_task.outputs['out_y_csv']\n    )\n\n    clean_text_task = clean_text_op(\n        split_to_x_y_task.outputs['out_X_csv']        \n    )\n\n    preprocess_text_task = preprocess_text_op(\n        clean_text_task.outputs['out_X_csv'],\n        tokenizer_num_words\n    )\n    split_to_train_test_task = split_to_train_test_op(\n        preprocess_text_task.outputs['out_X_pkl'],\n        encode_target_task.outputs['out_y_pkl'],\n        split\n    )\n    create_and_train_model_task = create_and_train_model_op(\n        split_to_train_test_task.outputs['out_X_train_pkl'],\n        split_to_train_test_task.outputs['out_y_train_pkl'],\n        tokenizer_num_words,\n        num_epochs\n    )\n    create_and_save_performance_artifacts_task = create_and_save_performance_artifacts_op(\n        create_and_train_model_task.outputs['model_pkl'],\n        split_to_train_test_task.outputs['out_X_test_pkl'],\n        split_to_train_test_task.outputs['out_y_test_pkl']\n    )\n\n    # test_pickling_task = test_pickling_op(\n    #     split_to_train_test_task.outputs['out_X_train_pkl'],\n    #     split_to_train_test_task.outputs['out_y_train_pkl'],\n    #     split_to_train_test_task.outputs['out_X_test_pkl'],\n    #     split_to_train_test_task.outputs['out_y_test_pkl']\n    # )\n\n    # pickle_load_test_task = pickle_load_test_op(\n    #     preprocess_text_task.outputs['out_X_pkl']\n    # )\n    # split_to_train_test_task = split_to_train_test_op(\n    #     download_and_save_data_from_s3_task.outputs['data_csv'],\n    #     split=0.8,\n    #     shuffle=True  \n    # )\n\n    # def pickle_load_test(\n#     in_X_pkl: comp.InputPath('PKL')\n#     ):\n#     import pickle\n#     X = pickle.load(open(in_X_pkl, 'rb'))    \n    \n\n# def test_pickling(\n#     in_X_train_pkl: comp.InputPath('PKL'),\n#     in_y_train_pkl: comp.InputPath('PKL'),\n#     in_X_test_pkl: comp.InputPath('PKL'),    \n#     in_y_test_pkl: comp.InputPath('PKL')\n#     ):\n#     import pickle\n#     import numpy\n\n#     X_train = pickle.load(open(in_X_train_pkl, 'rb'))\n#     y_train = pickle.load(open(in_y_train_pkl, 'rb'))\n#     X_test = pickle.load(open(in_X_test_pkl, 'rb'))\n#     y_test = pickle.load(open(in_y_test_pkl, 'rb'))\n\n#     objs = {\n#         'X train':X_train,\n#         'y train': y_train,\n#         'X test': X_test,\n#         'y test': y_test\n#         }\n\n#     for obj in objs:\n#         print(obj, type(objs[obj]), objs[obj].shape, \"\\n\")\n        \n#         for i in range(3):\n#             print(objs[obj][i], \"\\n\")\n#         print(\"#####################################################, \\n\\n\\n\")    \n"
  },
  {
    "repo": "aakashbajaj/Retinal-OCT-Kubeflow",
    "file_path": "pipelines/e2e_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/aakashbajaj/Retinal-OCT-Kubeflow/master/pipelines/e2e_pipeline.py",
    "content": "import kfp.dsl as dsl\nimport kfp.gcp as gcp\n\n\n@dsl.pipeline(\n  name='Retinal_OCT',\n  description='Retinal OCT detection'\n)\ndef dp_inf_pipe(\n  # Important Parameters on top\n\n  project_id: dsl.PipelineParam = dsl.PipelineParam(name='project-id', value=\"YOUR_PROJECT_ID\"),\n  inp_dir: dsl.PipelineParam = dsl.PipelineParam(name='input-dir', value='GCS_IMAGE_INPDIR_HERE'),\n  out_dir: dsl.PipelineParam = dsl.PipelineParam(name='data-dir', value='GCS_TFRECORD_OUTDIR_HERE'),\n  model_dir: dsl.PipelineParam = dsl.PipelineParam(name='model-dir', value='MODEL_CHECKPOINT_DIR_HERE'),\n  save_model_dir: dsl.PipelineParam = dsl.PipelineParam(name='save-model-dir', value=\"DIR_TO_EXPORT_SAVED_MODEL\"),\n  model_name: dsl.PipelineParam = dsl.PipelineParam(name='model-name', value='MODEL_NAME_FOR_SERVING (No spaces or underscores)'),\n  epochs: dsl.PipelineParam = dsl.PipelineParam(name='train-num-epochs', value=1),\n  batch_size: dsl.PipelineParam = dsl.PipelineParam(name='batch-size-train', value=32),\n\n  train_flag: dsl.PipelineParam = dsl.PipelineParam(name='train-flag', value=1),\n  dataprep_flag: dsl.PipelineParam = dsl.PipelineParam(name='dataprep-flag', value=0),\n\n  num_shards: dsl.PipelineParam = dsl.PipelineParam(name='num-shards', value=5),\n  split_flag: dsl.PipelineParam = dsl.PipelineParam(name='split-flag', value=2),\n  train_split: dsl.PipelineParam = dsl.PipelineParam(name='train-split', value=0.8),\n  seed: dsl.PipelineParam = dsl.PipelineParam(name='seed', value=123),\n  height: dsl.PipelineParam = dsl.PipelineParam(name='height', value=256),\n  width: dsl.PipelineParam = dsl.PipelineParam(name='width', value=256),\n  channels: dsl.PipelineParam = dsl.PipelineParam(name='channels', value=1),\n  \n  eval_steps: dsl.PipelineParam = dsl.PipelineParam(name='eval-steps', value=10000),\n  max_train_steps: dsl.PipelineParam = dsl.PipelineParam(name='max-train-steps', value=10000),\n  prefetch_buffer_size: dsl.PipelineParam = dsl.PipelineParam(name='prefetch-buffer', value=-1),\n\n  num_gpus_serve: dsl.PipelineParam = dsl.PipelineParam(name='num-gpus-serve', value=0),\n):\n\n  dataprep = dsl.ContainerOp(\n    name='dataprep',\n    image='gcr.io/speedy-aurora-193605/prep_tfr_df:latest',\n    arguments=[\"--input-dir\", inp_dir,\n      \"--output-dir\", out_dir,\n      \"--dataprep-flag\", dataprep_flag,\n      \"--num-shards\", num_shards,\n      \"--split-flag\", split_flag,\n      \"--train-split\", train_split,\n      \"--project-id\", project_id,\n      \"--seed\", seed,\n      \"--height\", height,\n      \"--width\", width,\n      ],\n      \n\n      ).apply(gcp.use_gcp_secret(secret_name='user-gcp-sa', secret_file_path_in_volume='/user-gcp-sa.json', volume_name='gcp-credentials-user-gcp-sa'))\n\n  train = dsl.ContainerOp(\n    name='train',\n    image='gcr.io/speedy-aurora-193605/cnn_train_dis:latest',\n    arguments=[\"--conv-dir\", out_dir,\n        \"--model-dir\", model_dir,\n        \"--save-model-dir\", save_model_dir,\n        \"--train-flag\", train_flag,\n        \"--num-epochs\", epochs,\n        \"--batch-size\", batch_size,\n        \"--max-train-steps\", max_train_steps,\n        \"--eval-steps\", eval_steps,\n        \n        \"--prefetch-buffer\", prefetch_buffer_size,\n        \"--height\", height,\n        \"--width\", width,\n        \"--channels\", channels,\n        ]\n    ).apply(gcp.use_gcp_secret(secret_name='user-gcp-sa', secret_file_path_in_volume='/user-gcp-sa.json', volume_name='gcp-credentials-user-gcp-sa'))\n\n  tensorboard = dsl.ContainerOp(\n    name='tensorboard',\n    image='gcr.io/speedy-aurora-193605/model-tensorboard:latest',\n    arguments=[\"--model-dir\", model_dir,\n      ],\n      ).apply(gcp.use_gcp_secret(secret_name='user-gcp-sa', secret_file_path_in_volume='/user-gcp-sa.json', volume_name='gcp-credentials-user-gcp-sa'))\n\n  tfserve = dsl.ContainerOp(\n    name='tfserve',\n    image='gcr.io/speedy-aurora-193605/retina-tfserve:latest',\n    arguments=[\"--model_name\", model_name,\n      \"--model_path\", save_model_dir,\n      \"--num_gpus\", num_gpus_serve,\n      ],\n      ).apply(gcp.use_gcp_secret(secret_name='admin-gcp-sa', secret_file_path_in_volume='/admin-gcp-sa.json', volume_name='gcp-credentials-admin-gcp-sa'))\n      \n  train.set_gpu_limit('2')\n  train.set_memory_request('8G')\n  train.set_cpu_request('4')\n  train.after(dataprep)\n  tfserve.after(train)\n  tensorboard.after(dataprep)\n\nif __name__ == '__main__':\n  import kfp.compiler as compiler\n  compiler.Compiler().compile(dp_inf_pipe, 'retinal_oct_fin.tar.gz')"
  },
  {
    "repo": "ZeynepRuveyda/MLOPS_Automation_SyntheticDataCreation_and_MatsimSimulation",
    "file_path": "kubernetes_last_codes/code/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/ZeynepRuveyda/MLOPS_Automation_SyntheticDataCreation_and_MatsimSimulation/master/kubernetes_last_codes/code/pipeline.py",
    "content": "from kfp import dsl\n\n@dsl.pipeline(\n    name='Hello World',\n    description='A pipeline to demonstrate use of PersistentVolumeClaim'\n)\ndef pvc_pipeline(log:str =\"/mnt/data/kubeflow\",\n                 data_folder_directory:str = \"\",\n                 output_folder_directory:str=\"\",\n                 tmp_folder_directory:str=\"\",\n                 sampling_rate:float=0.001):\n    #git clone\n    def clone_equasim():\n        return dsl.ContainerOp(\n        name = 'Git Clone Equasim',\n        image = 'zeynep02/pipeline-v0.0.4:latest',\n        command = 'python3',\n        arguments = [\n            \"/mnt/data/kubeflow/code/clone.py\",  \n            \"--log_dir\",\n            log\n           \n\n        ],\n        pvolumes={'/mnt/data/kubeflow': dsl.PipelineVolume(pvc='my-pvc')}\n    )\n    def edit_config():\n        return dsl.ContainerOp(\n        name = 'Edit Config Yml',\n        image = 'zeynep02/pipeline-v0.0.4:latest',\n        command = 'python3',\n        arguments = [\n            \"/mnt/data/kubeflow/code/edit_config.py\",\n            \"--log_dir\",\n            log,\n            \"--data_folder_directory\",\n            data_folder_directory,\n            \"--output_folder_directory\",\n            output_folder_directory,\n            \"--tmp_folder_directory\",\n            tmp_folder_directory,\n            \"--sampling_rate\",\n            sampling_rate\n\n        ],\n        pvolumes={'/mnt/data/kubeflow': dsl.PipelineVolume(pvc='my-pvc')}\n\n        )\n    \n    def synpp():\n        return dsl.ContainerOp(\n        name='Run synp',\n        image='zeynep02/pipeline-v0.0.4:latest',\n        command='python3',\n        arguments=[ \n            \"/mnt/data/kubeflow/code/synpp.py\",\n            \"--log_dir\",\n            log,],\n        pvolumes={'/mnt/data/kubeflow': dsl.PipelineVolume(pvc='my-pvc')}\n        )\n    \n    clone_git = clone_equasim()\n    clone_git.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n\n    update_config = edit_config()\n    update_config.execution_options.caching_strategy.max_cache_staleness = \"P0D\"   \n    update_config.after(clone_git)\n    synpp_run = synpp()\n    synpp_run.execution_options.caching_strategy.max_cache_staleness = \"P0D\" \n    synpp_run.after(update_config)\n\n\n\n    \n\nif __name__ == '__main__':\n    import kfp.compiler as compiler\n    compiler.Compiler().compile(pvc_pipeline, 'pipeline_v_4.yaml')\n"
  },
  {
    "repo": "ZeynepRuveyda/MLOPS_Automation_SyntheticDataCreation_and_MatsimSimulation",
    "file_path": "kubernetes_last_codes/code/pipeline_demo.py",
    "raw_url": "https://raw.githubusercontent.com/ZeynepRuveyda/MLOPS_Automation_SyntheticDataCreation_and_MatsimSimulation/master/kubernetes_last_codes/code/pipeline_demo.py",
    "content": "from kfp import dsl\n\ndef write():\n    return dsl.ContainerOp(\n        name='Write',\n        image='busybox',\n        command=['sh', '-c'],\n        arguments=['echo \"Hello\" > /mnt/data/kubeflow/hello_world.txt'],\n        pvolumes={'/mnt/data/kubeflow': dsl.PipelineVolume(pvc='my-pvc')}\n    )\n\ndef read():\n    return dsl.ContainerOp(\n        name='Read',\n        image='busybox',\n        command=['sh', '-c'],\n        arguments=['cat /mnt/data/kubeflow/hello_world.txt'],\n        pvolumes={'/mnt/data/kubeflow': dsl.PipelineVolume(pvc='my-pvc')}\n    )\n\n@dsl.pipeline(\n    name='Hello World',\n    description='A pipeline to demonstrate use of PersistentVolumeClaim'\n)\ndef pvc_pipeline():\n    producer = write()\n    consumer = read().after(producer)\n\nif __name__ == '__main__':\n    import kfp.compiler as compiler\n    compiler.Compiler().compile(pvc_pipeline, 'hello_world.yaml')\n"
  },
  {
    "repo": "ZeynepRuveyda/MLOPS_Automation_SyntheticDataCreation_and_MatsimSimulation",
    "file_path": "kubernetes_last_codes/code/pipeline_hello_world.py",
    "raw_url": "https://raw.githubusercontent.com/ZeynepRuveyda/MLOPS_Automation_SyntheticDataCreation_and_MatsimSimulation/master/kubernetes_last_codes/code/pipeline_hello_world.py",
    "content": "from kfp import dsl\n\ndef write():\n    return dsl.ContainerOp(\n        name='Write',\n        image='busybox',\n        command=['sh', '-c'],\n        arguments=['echo \"Hello\" > /mnt/data/kubeflow/hello_world.txt'],\n        pvolumes={'/mnt/data/kubeflow': dsl.PipelineVolume(pvc='my-pvc')}\n    )\n\ndef read():\n    return dsl.ContainerOp(\n        name='Read',\n        image='busybox',\n        command=['sh', '-c'],\n        arguments=['cat /mnt/data/kubeflow/hello_world.txt'],\n        pvolumes={'/mnt/data/kubeflow': dsl.PipelineVolume(pvc='my-pvc')}\n    )\n\n@dsl.pipeline(\n    name='Hello World',\n    description='A pipeline to demonstrate use of PersistentVolumeClaim'\n)\ndef pvc_pipeline():\n    producer = write()\n    consumer = read().after(producer)\n\nif __name__ == '__main__':\n    import kfp.compiler as compiler\n    compiler.Compiler().compile(pvc_pipeline, 'hello_world.yaml')\n"
  },
  {
    "repo": "SaschaDittmann/kubeflow-azurepipeline",
    "file_path": "code/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/SaschaDittmann/kubeflow-azurepipeline/master/code/pipeline.py",
    "content": "import kfp.dsl as dsl\nfrom kubernetes import client as k8s_client\n\n@dsl.pipeline(\n    name='Tacos vs. Burritos',\n    description='Simple TF CNN for binary classifier between burritos and tacos'\n)\ndef tacosandburritos_train(\n    tenant_id,\n    service_principal_id,\n    service_principal_password,\n    subscription_id,\n    resource_group,\n    workspace,\n    persistent_volume_name='azure',\n    persistent_volume_path='/mnt/azure',\n    data_download='https://github.com/SaschaDittmann/kubeflow-azurepipeline/raw/main/data/tacodata.zip',\n    epochs=5,\n    batch=32,\n    learning_rate=0.0001,\n    imagetag='latest',\n    model_name='tacosandburritos',\n    profile_name='tacoprofile',\n    service_name='tacosandburritos-service'\n):\n\n    operations = {}\n    image_size = 160\n    training_folder = 'train'\n    training_dataset = 'train.txt'\n    model_folder = 'model'\n\n    # preprocess data\n    operations['preprocess'] = dsl.ContainerOp(\n        name='preprocess',\n        image='bytesmith/kubeflow-azurepipeline:latest-preprocess',\n        command=['python'],\n        arguments=[\n            '/scripts/data.py',\n            '--base_path', persistent_volume_path,\n            '--data', training_folder,\n            '--target', training_dataset,\n            '--img_size', image_size,\n            '--zipfile', data_download\n        ]\n    )\n\n    #train\n    operations['training'] = dsl.ContainerOp(\n        name='training',\n        image='bytesmith/kubeflow-azurepipeline:latest-training',\n        command=['python'],\n        arguments=[\n            '/scripts/train.py',\n            '--base_path', persistent_volume_path,\n            '--data', training_folder, \n            '--epochs', epochs, \n            '--batch', batch, \n            '--image_size', image_size, \n            '--lr', learning_rate, \n            '--outputs', model_folder, \n            '--dataset', training_dataset\n        ]\n    )\n    operations['training'].after(operations['preprocess'])\n\n    # register model\n    operations['register'] = dsl.ContainerOp(\n        name='register',\n        image='bytesmith/kubeflow-azurepipeline:latest-register',\n        command=['python'],\n        arguments=[\n            '/scripts/register.py',\n            '--base_path', persistent_volume_path,\n            '--model', 'latest.h5',\n            '--model_name', model_name,\n            '--tenant_id', tenant_id,\n            '--service_principal_id', service_principal_id,\n            '--service_principal_password', service_principal_password,\n            '--subscription_id', subscription_id,\n            '--resource_group', resource_group,\n            '--workspace', workspace\n        ]\n    )\n    operations['register'].after(operations['training'])\n\n    operations['profile'] = dsl.ContainerOp(\n        name='profile',\n        image='bytesmith/kubeflow-azurepipeline:latest-profile',\n        command=['/bin/bash'],\n        arguments=[\n            '/scripts/profile.sh',\n            '-n', profile_name,\n            '-e', '/scripts/score.py',\n            '-d', '{ \"schemaVersion\": 1, \"datasetType\": \"Tabular\", \"parameters\": { \"path\": [ \"https://github.com/SaschaDittmann/kubeflow-azurepipeline/raw/master/data/profiledata.json\" ], \"sourceType\": \"json_lines_files\" }, \"registration\": { \"createNewVersion\": true, \"name\": \"tacosandburritos-dataset\", \"tags\": { \"mlops-system\": \"kubeflow\" } } }',\n            '-t', tenant_id,\n            '-r', resource_group,\n            '-w', workspace,\n            '-s', service_principal_id,\n            '-p', service_principal_password,\n            '-u', subscription_id,\n            '-b', persistent_volume_path\n        ]\n    )\n    operations['profile'].after(operations['register'])\n\n    operations['deploy'] = dsl.ContainerOp(\n        name='deploy',\n        image='bytesmith/kubeflow-azurepipeline:latest-deploy',\n        command=['/bin/bash'],\n        arguments=[\n            '/scripts/deploy.sh',\n            '-n', service_name,\n            '-e', '/scripts/score.py',\n            '-d', '/scripts/acideploymentconfig.json',\n            '-t', tenant_id,\n            '-r', resource_group,\n            '-w', workspace,\n            '-s', service_principal_id,\n            '-p', service_principal_password,\n            '-u', subscription_id,\n            '-b', persistent_volume_path\n        ]\n    )\n    operations['deploy'].after(operations['profile'])\n\n    for _, op in operations.items():\n        op.container.set_image_pull_policy(\"Always\")\n        op.add_volume(\n            k8s_client.V1Volume(\n                name='azure',\n                persistent_volume_claim=k8s_client.V1PersistentVolumeClaimVolumeSource(\n                    claim_name='azure-managed-disk')\n                )\n            ).add_volume_mount(k8s_client.V1VolumeMount(\n                mount_path='/mnt/azure', \n                name='azure')\n            )\n\n\nif __name__ == '__main__':\n   import kfp.compiler as compiler\n   compiler.Compiler().compile(tacosandburritos_train, __file__ + '.tar.gz')\n"
  },
  {
    "repo": "Epochex/MLOps_Kubeflow_TwinStream_Pipeline",
    "file_path": "kfp_pipeline/build_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/Epochex/MLOps_Kubeflow_TwinStream_Pipeline/main/kfp_pipeline/build_pipeline.py",
    "content": "# kfp_pipeline/build_pipeline.py\nimport pathlib, sys, kfp\nfrom kfp import dsl\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u5e38\u91cf & \u8def\u5f84 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nROOT  = pathlib.Path(__file__).resolve().parents[1]\nCOMP  = ROOT / \"kfp_pipeline\" / \"components\"\nIMAGE = \"hirschazer/flux_demo5:latest\"      # \u4f60\u7684\u4e1a\u52a1\u955c\u50cf\n\n# \u8ba9 Python \u627e\u5f97\u5230 components \u5305\nsys.path.append(str(COMP.parent))\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u5bfc\u5165\u5df2\u88c5\u9970\u597d\u7684\u7ec4\u4ef6\u51fd\u6570 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nfrom components.split_data          import split_data\nfrom components.offline_train       import offline_train\nfrom components.launch_katib        import launch_katib\nfrom components.apply_k8s_resource  import apply_k8s_resource\n\n# \u5982\u9700\u7edf\u4e00\u955c\u50cf\uff0c\u53ef\u5728\u8fd9\u91cc\u52a8\u6001\u8986\u5199\uff08\u4efb\u9009\uff09\nfor c in (split_data, offline_train, launch_katib):\n    c.component_spec.implementation.container.image = IMAGE\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 DAG \u5b9a\u4e49 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\n@dsl.pipeline(\n    name=\"twin-stream-pipeline\",\n    description=\"Katib HPO \u2192 Batch \u8bad\u7ec3 \u2192 Streaming Job\"\n)\ndef pipeline(\n    csv_uri: str = \"s3://katib-flux-demo/datasets/load_stimulus_global.csv\",\n):\n    # \u2460 40/60 \u5207\u5206\uff0c\u81ea\u52a8\u628a train/stream.csv \u4e0a\u4f20\u56de\u540c Bucket\n    split = split_data(csv_uri=csv_uri)\n\n    # \u2461 Katib \u968f\u673a\u641c\u7d22\n    hpo = launch_katib(\n        train_image = IMAGE,\n        train_csv   = split.outputs[\"train_csv\"],\n        out_dir     = \"/mnt/data\",\n    )\n\n    # \u2462 \u79bb\u7ebf Batch \u8bad\u7ec3\n    train = offline_train(\n        csv       = split.outputs[\"train_csv\"],\n        model_key = \"models/ann_batch_model.pth\",\n    ).after(hpo)\n\n    # \u2463 kubectl apply Producer + Consumer Job\n    apply_k8s_resource(\n        yaml_path = str(COMP / \"stream_job.yaml\"),\n    ).after(train)\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u7f16\u8bd1 + \u53ef\u9009\uff1a\u81ea\u52a8\u89e6\u53d1\u4e00\u6b21 Run \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nif __name__ == \"__main__\":\n    pkg = str(ROOT / \"pipeline.json\")\n    kfp.compiler.Compiler().compile(\n        pipeline_func = pipeline,\n        package_path  = pkg,\n    )\n    print(\"\u2705  pipeline.json generated\")\n\n    # ----- \u81ea\u52a8\u89e6\u53d1\uff08\u53ef\u5220\u6389\uff09 -----\n    from kfp import Client\n    client = Client(host=\"http://ml-pipeline.kubeflow:8888\")  # in-cluster \u53ef\u7701 host\n    run = client.create_run_from_pipeline_package(\n        pipeline_file   = pkg,\n        arguments       = {\"csv_uri\": \"s3://katib-flux-demo/datasets/load_stimulus_global.csv\"},\n        experiment_name = \"twin-stream\",\n        run_name        = \"twin-stream-auto\",\n    )\n    print(\"\ud83d\ude80  triggered run:\", run.run_id)\n\n\n"
  },
  {
    "repo": "Epochex/MLOps_Kubeflow_TwinStream_Pipeline",
    "file_path": "kfp_pipeline/components/split_data.py",
    "raw_url": "https://raw.githubusercontent.com/Epochex/MLOps_Kubeflow_TwinStream_Pipeline/main/kfp_pipeline/components/split_data.py",
    "content": "# kfp_pipeline/components/split_data.py\nfrom kfp import dsl\nfrom kfp.dsl import OutputPath\nimport pandas as pd\nimport s3fs\nimport os\n\n@dsl.component(\n    base_image=\"python:3.10\",  # \u63a8\u8350\u81ea\u5b9a\u4e49\u955c\u50cf\u4ee5\u52a0\u901f\n    packages_to_install=[\"pandas\", \"s3fs\"]\n)\ndef split_data(\n    csv_uri: str,\n    train_csv: OutputPath(str),\n    stream_csv: OutputPath(str),\n    ratio: float = 0.4\n):\n    \"\"\"\n    \u7ec4\u4ef6\u529f\u80fd:\n    \u2022 csv_uri: \u5fc5\u987b\u662f s3://bucket/key.csv\n    \u2022 \u6309 ratio \u5207\u5206\u6570\u636e\uff0c\u8f93\u51fa train_csv / stream_csv\n\n    \u73af\u5883\u53d8\u91cf:\n    - MINIO_KEY\n    - MINIO_SECRET\n    - MINIO_ENDPOINT\n    \"\"\"\n    # 1\ufe0f\u20e3 \u8fde\u63a5 S3 (MinIO)\n    fs = s3fs.S3FileSystem(\n        key=os.getenv(\"MINIO_KEY\", \"minio\"),\n        secret=os.getenv(\"MINIO_SECRET\", \"minio123\"),\n        client_kwargs={\n            \"endpoint_url\": os.getenv(\"MINIO_ENDPOINT\", \"http://minio.kubeflow:9000\"),\n            \"region_name\": \"us-east-1\"  # \u901a\u5e38\u4fdd\u6301\u9ed8\u8ba4\uff0c\u5bf9 MinIO \u65e0\u5f3a\u5236\u8981\u6c42\n        }\n    )\n\n    # 2\ufe0f\u20e3 \u8bfb\u53d6\u6570\u636e\n    with fs.open(csv_uri, \"rb\") as f:\n        df = (\n            pd.read_csv(f)\n            .replace(['<not counted>', ' '], pd.NA)\n            .dropna()\n        )\n\n    # 3\ufe0f\u20e3 \u5207\u5206 & \u4fdd\u5b58\n    cut = int(len(df) * ratio)\n    df.iloc[:cut].to_csv(train_csv, index=False)\n    df.iloc[cut:].to_csv(stream_csv, index=False)\n\n    print(f\"[split] rows={len(df)}  ratio={ratio}\")\n    print(\"train_csv \u2192\", train_csv)\n    print(\"stream_csv\u2192\", stream_csv)\n\n\n\n"
  },
  {
    "repo": "Vishwajyoti/Pipelines",
    "file_path": "demo_kf_pipelines.py",
    "raw_url": "https://raw.githubusercontent.com/Vishwajyoti/Pipelines/master/demo_kf_pipelines.py",
    "content": "\"\"\"\nCreated on Tue Sep  3 14:58:04 2019\n\n@author: vispande2\n\"\"\"\n\n\nfrom kfp import compiler\nimport kfp.dsl as dsl\nimport kfp.gcp as gcp\nimport kfp.notebook\nimport sys\nimport json\n\nclass ObjectDict(dict):\n  def __getattr__(self, name):\n    if name in self:\n      return self[name]\n    else:\n      raise AttributeError(\"No such attribute: \" + name)\n\n\n@dsl.pipeline(\n  name='Basic KubeFlow Pipeline',\n  description='Feature Eng,Training,Testing & Deployment'\n)\ndef ftreng_train_test_and_deploy(\n        project='cohesive-gadget-166410',\n        bucket_uri='gs://vishwa/',\n        region='us-central1',\n        test_size=0.3,\n        file_name='Boston.csv',\n        target_var='target',\n        hyper_param=json.dumps({'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000],'max_features': ['auto', 'sqrt'],'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None],'min_samples_split': [2, 5, 10],'min_samples_leaf': [1, 2, 4],'bootstrap': [True, False]}),\n        search_type=1,\n        model_bucket_name='rf-vj-model',\n        model_name='rf-vj',\n        framework='scikit-learn',\n        version_key_word='rf_model_'\n        \n        ):\n# Step 1: create training dataset using Apache Beam on Cloud Dataflow\n    feature_eng = dsl.ContainerOp(\n            name='feature_eng',\n            # image needs to be a compile-time string\n            image='gcr.io/cohesive-gadget-166410/feature-eng-vj:latest',\n            arguments=[\n                    '--path',bucket_uri,\n                    '--filename',file_name,\n                    '--t_size',test_size\n                    ]\n            #,file_outputs={'bucket': '/output.txt'}\n            ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n\n# Step 2: Train the model and find best set of hyperparameter.\n    train = dsl.ContainerOp(\n            name='train',\n            # image needs to be a compile-time string\n            image='gcr.io/cohesive-gadget-166410/train-rf-vj:latest',\n            arguments=[\n                    '--path',bucket_uri,\n                    '--target',target_var,\n                    '--h_param',hyper_param,\n                    '--search_type',search_type\n                    ]\n            #,file_outputs={'jobname': '/output.txt'}\n            ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n    train.after(feature_eng)\n\n# Step 3: Train the model some more, but on the pipelines cluster itself\n    test = dsl.ContainerOp(\n            name='test',\n            # image needs to be a compile-time string\n            image='gcr.io/cohesive-gadget-166410/test-rf-vj:latest',\n            #image='gcr.io/cloud-training-demos/babyweight-pipeline-traintuned-trainer@sha256:3d73c805430a16d0675aeafa9819d6d2cfbad0f0f34cff5fb9ed4e24493bc9a8',\n            arguments=[\n                    '--path',bucket_uri,\n                    '--target',target_var\n                    ]\n            #,file_outputs={'train': '/output.txt'}\n            ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n    test.after(train)\n\n# Step 4: Deploy the trained model to Cloud ML Engine\n    deploy_cmle = dsl.ContainerOp(\n            name='deploycmle',\n            # image needs to be a compile-time string\n            image='gcr.io/cohesive-gadget-166410/deploy-rf-vj:latest',\n            arguments=[\n                    model_bucket_name,\n                    project,\n                    region,\n                    bucket_uri,\n                    framework,\n                    version_key_word,\n                    model_name   \n                    ]\n\n        ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n    deploy_cmle.after(test)\n\nif __name__ == '__main__':\n    filename=sys.argv[1]\n    compiler.Compiler().compile(ftreng_train_test_and_deploy,filename)\n"
  },
  {
    "repo": "II-VSB-II/TaxiClassificationKubeflowPipelinePVC",
    "file_path": "Taxi-Pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/II-VSB-II/TaxiClassificationKubeflowPipelinePVC/main/Taxi-Pipeline.py",
    "content": "import kfp\r\nfrom kfp import components\r\nfrom kfp import dsl\r\nfrom kfp import gcp\r\nfrom kfp import onprem\r\n\r\nplatform = 'local'\r\n\r\n#proxy=\"http://test:8080\"\r\nproxy = \"\"\r\n\r\ndataflow_tf_data_validation_op  = components.load_component_from_url('https://raw.githubusercontent.com/II-VSB-II/TaxiClassificationPipelinePVC/main/yamls/tfdv_component.yaml')\r\ndataflow_tf_transform_op        = components.load_component_from_url('https://raw.githubusercontent.com/II-VSB-II/TaxiClassificationPipelinePVC/main/yamls/tft_component.yaml')\r\ntf_train_op                     = components.load_component_from_url('https://raw.githubusercontent.com/II-VSB-II/TaxiClassificationPipelinePVC/main/yamls/dnntrainer_component.yaml')\r\ndataflow_tf_model_analyze_op    = components.load_component_from_url('https://raw.githubusercontent.com/II-VSB-II/TaxiClassificationPipelinePVC/main/yamls/tfma_component.yaml')\r\ndataflow_tf_predict_op          = components.load_component_from_url('https://raw.githubusercontent.com/II-VSB-II/TaxiClassificationPipelinePVC/main/yamls/predict_component.yaml')\r\n\r\nconfusion_matrix_op             = components.load_component_from_url('https://raw.githubusercontent.com/II-VSB-II/TaxiClassificationPipelinePVC/main/yamls/confusion_matrix_component.yaml')\r\nroc_op                          = components.load_component_from_url('https://raw.githubusercontent.com/II-VSB-II/TaxiClassificationPipelinePVC/main/yamls/roc_component.yaml')\r\n\r\nkubeflow_deploy_op              = components.load_component_from_url('https://raw.githubusercontent.com/II-VSB-II/TaxiClassificationPipelinePVC/main/yamls/deployer_component.yaml')\r\n\r\n@dsl.pipeline(\r\n  name='TFX Taxi Cab Classification Pipeline Example',\r\n  description='Example pipeline that does classification with model analysis based on a public BigQuery dataset.'\r\n)\r\ndef taxi_cab_classification(\r\n    project,\r\n    output=\"/mnt/shared\",\r\n    column_names='/mnt/shared/pipelines/column-names.json',\r\n    key_columns='trip_start_timestamp',\r\n    train='/mnt/shared/pipelines/train.csv',\r\n    evaluation='/mnt/shared/pipelines/eval.csv',\r\n    mode='local',\r\n    preprocess_module='/mnt/shared/pipelines/preprocessing.py',\r\n    learning_rate=0.1,\r\n    hidden_layer_size='1500',\r\n    steps=3000,\r\n    analyze_slice_column='trip_start_hour'\r\n):\r\n    output_template = str(output) + '/{{workflow.uid}}/{{pod.name}}/data'\r\n    target_lambda = \"\"\"lambda x: (x['target'] > x['fare'] * 0.2)\"\"\"\r\n    target_class_lambda = \"\"\"lambda x: 1 if (x['target'] > x['fare'] * 0.2) else 0\"\"\"\r\n\r\n    tf_server_name = 'taxi-cab-classification-model-{{workflow.uid}}'\r\n\r\n    if platform != 'GCP':\r\n        vop = dsl.VolumeOp(\r\n            name=\"create_pvc\",\r\n            resource_name=\"pipeline-pvc\",\r\n            modes=dsl.VOLUME_MODE_RWM,\r\n            size=\"1Gi\"\r\n        )\r\n        if proxy != \"\":\r\n            checkout = dsl.ContainerOp(\r\n            name=\"checkout\",\r\n            image=\"alpine/git:latest\",\r\n            command=[\"git\", \"clone\", \"https://github.com/II-VSB-II/TaxiClassificationKubeflowPipelinePVC.git\", str(output) + \"/pipelines\", \"-c\", \"http.proxy={}\".format(proxy)],\r\n        ).apply(onprem.mount_pvc(vop.outputs[\"name\"], 'local-storage', output))\r\n        else:\r\n            checkout = dsl.ContainerOp(\r\n            name=\"checkout\",\r\n            image=\"alpine/git:latest\",\r\n            command=[\"git\", \"clone\", \"https://github.com/II-VSB-II/TaxiClassificationKubeflowPipelinePVC.git\", str(output) + \"/pipelines\"],\r\n        ).apply(onprem.mount_pvc(vop.outputs[\"name\"], 'local-storage', output))\r\n        \r\n        checkout.after(vop)\r\n\r\n    validation = dataflow_tf_data_validation_op(\r\n        inference_data=train,\r\n        validation_data=evaluation,\r\n        column_names=column_names,\r\n        key_columns=key_columns,\r\n        gcp_project=project,\r\n        run_mode=mode,\r\n        validation_output=output_template,\r\n    )\r\n    if platform != 'GCP':\r\n        validation.after(checkout)\r\n\r\n    preprocess = dataflow_tf_transform_op(\r\n        training_data_file_pattern=train,\r\n        evaluation_data_file_pattern=evaluation,\r\n        schema=validation.outputs['schema'],\r\n        gcp_project=project,\r\n        run_mode=mode,\r\n        preprocessing_module=preprocess_module,\r\n        transformed_data_dir=output_template\r\n    )\r\n\r\n    training = tf_train_op(\r\n        transformed_data_dir=preprocess.output,\r\n        schema=validation.outputs['schema'],\r\n        learning_rate=learning_rate,\r\n        hidden_layer_size=hidden_layer_size,\r\n        steps=steps,\r\n        target='tips',\r\n        preprocessing_module=preprocess_module,\r\n        training_output_dir=output_template\r\n    )\r\n\r\n    analysis = dataflow_tf_model_analyze_op(\r\n        model=training.output,\r\n        evaluation_data=evaluation,\r\n        schema=validation.outputs['schema'],\r\n        gcp_project=project,\r\n        run_mode=mode,\r\n        slice_columns=analyze_slice_column,\r\n        analysis_results_dir=output_template\r\n    )\r\n\r\n    prediction = dataflow_tf_predict_op(\r\n        data_file_pattern=evaluation,\r\n        schema=validation.outputs['schema'],\r\n        target_column='tips',\r\n        model=training.output,\r\n        run_mode=mode,\r\n        gcp_project=project,\r\n        predictions_dir=output_template\r\n    )\r\n\r\n    cm = confusion_matrix_op(\r\n        predictions=prediction.output,\r\n        target_lambda=target_lambda,\r\n        output_dir=output_template\r\n    )\r\n\r\n    roc = roc_op(\r\n        predictions_dir=prediction.output,\r\n        target_lambda=target_class_lambda,\r\n        output_dir=output_template\r\n    )\r\n\r\n    if platform == 'GCP':\r\n        deploy = kubeflow_deploy_op(\r\n            model_dir=str(training.output) + '/export/export',\r\n            server_name=tf_server_name\r\n        )\r\n    else:\r\n        deploy = kubeflow_deploy_op(\r\n            cluster_name=project,\r\n            model_dir=str(training.output) + '/export/export',\r\n            pvc_name=vop.outputs[\"name\"],\r\n            server_name=tf_server_name\r\n        )\r\n\r\n    steps = [validation, preprocess, training, analysis, prediction, cm, roc, deploy]\r\n    for step in steps:\r\n        if platform == 'GCP':\r\n            step.apply(gcp.use_gcp_secret('user-gcp-sa'))\r\n        else:\r\n            step.apply(onprem.mount_pvc(vop.outputs[\"name\"], 'local-storage', output))\r\n\r\n\r\nif __name__ == '__main__':\r\n    kfp.compiler.Compiler().compile(taxi_cab_classification, \"TaxiPipelinePVC\" + '.zip')"
  },
  {
    "repo": "seoeun25/ml-ops",
    "file_path": "distributed_training/pytorch-op/mnist/pytorch_mnist_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/seoeun25/ml-ops/master/distributed_training/pytorch-op/mnist/pytorch_mnist_pipeline.py",
    "content": "import json\nfrom typing import NamedTuple\nfrom collections import namedtuple\nimport kfp\nimport kfp.dsl as dsl\nfrom kfp import components\nfrom kfp.dsl.types import Integer\n\ndef kfp_client():\n    \"\"\"\n    Returns Kubeflow pipelines client inside cluster.\n    \"\"\"\n    end_point=\"http://ml-pipeline.kubeflow.svc.cluster.local:8888\"\n    credentials = kfp.auth.ServiceAccountTokenVolumeCredentials(path=None)\n    client = kfp.Client(host=end_point, credentials=credentials)\n\n    return client\n\ndef base_image() -> str:\n    import os\n    import re\n\n    iam = os.environ.get(\"AWS_ROLE_ARN\")\n    account = re.findall(\"arn:aws:iam::(.*):role*\", iam)[0]\n    region = os.environ.get(\"AWS_REGION\")\n    base_image = \"{}.dkr.ecr.{}.amazonaws.com/aladin-runtime:anaconda-cpu\".format(account, region)\n    print(\"base_image = {}\".format(base_image))\n    return base_image\ndef get_current_namespace():\n    \"\"\"Returns current namespace if available, else kubeflow\"\"\"\n    try:\n        current_namespace = open(\n            \"/var/run/secrets/kubernetes.io/serviceaccount/namespace\"\n        ).read()\n    except:\n        current_namespace = \"kubeflow\"\n    return current_namespace\n\n\ndef create_worker_spec(\n        worker_num: int = 0\n) -> NamedTuple(\n    \"CreatWorkerSpec\", [(\"worker_spec\", dict)]\n):\n    from collections import namedtuple\n    \"\"\"\n    Creates pytorch-job worker spec\n    \"\"\"\n    worker = {}\n    if worker_num > 0:\n        worker = {\n            \"replicas\": worker_num,\n            \"restartPolicy\": \"OnFailure\",\n            \"template\": {\n                \"metadata\": {\n                    \"annotations\": {\n                        \"sidecar.istio.io/inject\": \"false\"\n                    }\n                },\n                \"spec\": {\n                    \"containers\": [\n                        {\n                            \"command\": [\n                                \"python\",\n                                \"/opt/mnist/src/mnist.py\"\n                            ],\n                            \"args\": [\n                                \"--backend\",\n                                \"gloo\",\n                            ],\n                            \"image\": \"public.ecr.aws/pytorch-samples/pytorch_dist_mnist:latest\",\n                            \"name\": \"pytorch\",\n                            # \"resources\": {\n                            #     \"requests\": {\n                            #         \"memory\": \"4Gi\",\n                            #         \"cpu\": \"2000m\",\n                            #         # Uncomment for GPU\n                            #         # \"nvidia.com/gpu\": 1,\n                            #     },\n                            #     \"limits\": {\n                            #         \"memory\": \"4Gi\",\n                            #         \"cpu\": \"2000m\",\n                            #         # Uncomment for GPU\n                            #         # \"nvidia.com/gpu\": 1,\n                            #     },\n                            # },\n                        }\n                    ]\n                },\n            },\n        }\n\n    worker_spec_output = namedtuple(\n        \"MyWorkerOutput\", [\"worker_spec\"]\n    )\n    return worker_spec_output(worker)\n\n\nworker_spec_op = components.func_to_container_op(\n    create_worker_spec,\n    base_image=base_image(),\n)\n\n\n@dsl.pipeline(\n    name=\"launch-kubeflow-pytorchjob\",\n    description=\"An example to launch pytorch.\",\n)\ndef mnist_train(\n        namespace: str = get_current_namespace(),\n        worker_replicas: int = 1,\n        ttl_seconds_after_finished: int = -1,\n        job_timeout_minutes: int = 60,\n        delete_after_done: bool = False,\n):\n    print(\"mnist_train_pipeline: namespace={}, worker_replicas={}, ttl_seconds_after_finished={}, job_timeout_minutes={}, delete_after_done={}\"\n          .format(namespace, worker_replicas, ttl_seconds_after_finished, job_timeout_minutes, delete_after_done))\n    pytorchjob_launcher_op = components.load_component_from_file(\n        \"../launcher/component.yaml\"\n    )\n\n    master = {\n        \"replicas\": 1,\n        \"restartPolicy\": \"OnFailure\",\n        \"template\": {\n            \"metadata\": {\n                \"annotations\": {\n                    # See https://github.com/kubeflow/website/issues/2011\n                    \"sidecar.istio.io/inject\": \"false\"\n                }\n            },\n            \"spec\": {\n                \"containers\": [\n                    {\n                        #To override default command\n                        \"command\": [\n                            \"python\",\n                            \"/opt/mnist/src/mnist.py\"\n                        ],\n                        \"args\": [\n                            \"--backend\",\n                            \"gloo\",\n                        ],\n                        # Or, create your own image from\n                        # https://github.com/kubeflow/pytorch-operator/tree/master/examples/mnist\n                        \"image\": \"public.ecr.aws/pytorch-samples/pytorch_dist_mnist:latest\",\n                        \"name\": \"pytorch\",\n                        # \"resources\": {\n                        #     \"requests\": {\n                        #         \"memory\": \"4Gi\",\n                        #         \"cpu\": \"2000m\",\n                        #         # Uncomment for GPU\n                        #         # \"nvidia.com/gpu\": 1,\n                        #     },\n                        #     \"limits\": {\n                        #         \"memory\": \"4Gi\",\n                        #         \"cpu\": \"2000m\",\n                        #         # Uncomment for GPU\n                        #         # \"nvidia.com/gpu\": 1,\n                        #     },\n                        # },\n                    }\n                ],\n                # If imagePullSecrets required\n                # \"imagePullSecrets\": [\n                #     {\"name\": \"image-pull-secret\"},\n                # ],\n            },\n        },\n    }\n\n    worker_spec_create = worker_spec_op(\n        worker_replicas\n    )\n\n    # Launch and monitor the job with the launcher\n    pytorchjob_launcher_op(\n        # Note: name needs to be a unique pytorchjob name in the namespace.\n        # Using RUN_ID_PLACEHOLDER is one way of getting something unique.\n        name=f\"pytorch-mnist-{kfp.dsl.RUN_ID_PLACEHOLDER}\",\n        namespace=namespace,\n        master_spec=master,\n        # pass worker_spec as a string because the JSON serializer will convert\n        # the placeholder for worker_replicas (which it sees as a string) into\n        # a quoted variable (eg a string) instead of an unquoted variable\n        # (number).  If worker_replicas is quoted in the spec, it will break in\n        # k8s.  See https://github.com/kubeflow/pipelines/issues/4776\n        worker_spec=worker_spec_create.outputs[\n            \"worker_spec\"\n        ],\n        ttl_seconds_after_finished=ttl_seconds_after_finished,\n        job_timeout_minutes=job_timeout_minutes,\n        delete_after_done=delete_after_done,\n    )\n\n\nif __name__ == \"__main__\":\n    import kfp.compiler as compiler\n\n    pipeline_file = \"pytorch_mnist_pipeline.yaml\"\n    print(\n        f\"Compiling pipeline as {pipeline_file}\"\n    )\n    compiler.Compiler(mode=kfp.dsl.PipelineExecutionMode.V1_LEGACY).compile(\n        mnist_train, pipeline_file\n    )\n    client = kfp_client()\n    client.upload_pipeline(pipeline_package_path=pipeline_file, pipeline_name=\"pytorch_mnist_pipeline\", description=\"pytorch_mnist_pipeline\")\n    print(f\"Created pipeline \")\n#     # To run:\n#     client = kfp.Client()\n#     run = client.create_run_from_pipeline_package(\n#         pipeline_file,\n#         arguments={},\n#         run_name=\"test pytorchjob run\"\n#     )\n#     print(f\"Created run {run}\")\n"
  },
  {
    "repo": "seoeun25/ml-ops",
    "file_path": "kubeflow_pipeline/samples/add/add.py",
    "raw_url": "https://raw.githubusercontent.com/seoeun25/ml-ops/master/kubeflow_pipeline/samples/add/add.py",
    "content": "###\n# $ dsl-compile --py add.py --output add_pipeline.yaml\n# Need manually upload add_pipeline.yaml on kubeflow-pipeline-ui\n#\n###\nimport kfp\nfrom kfp.components import create_component_from_func\n\ndef add(a: int, b: int) -> int:\n    ret = a + b\n    return ret\n\ndef substract(a: int, b: int) -> int:\n    ret = a - b\n    return ret\n\ndef multiply(a: int, b: int) -> int:\n    return a * b\n\nadd_op = create_component_from_func(add)\nsubstract_op = create_component_from_func(substract)\nmultiply_op = create_component_from_func(multiply)\n\nfrom kfp.dsl import pipeline\n@pipeline(name=\"add example\", description=\"example of addition calculation\")\ndef my_pipeline(a: int, b: int):\n    task_1 = add_op(a, b)\n    task_2 = substract_op(a, b)\n    task_3 = multiply_op(task_1.output, task_2.output)\n\n## $ dsl-compile --py add.py --output add_pipeline.yaml\n## upload add_pipeline.yaml on kubeflow-pipeline-ui"
  },
  {
    "repo": "seoeun25/ml-ops",
    "file_path": "kubeflow_pipeline/samples/add/add2.py",
    "raw_url": "https://raw.githubusercontent.com/seoeun25/ml-ops/master/kubeflow_pipeline/samples/add/add2.py",
    "content": "###\n# $ python add_2.py\n# Need manually upload add_pipeline_2.yaml on kubeflow-pipeline-ui\n#\n###\nimport kfp.compiler\nfrom kfp.components import create_component_from_func\n\ndef add(a: int, b: int) -> int:\n    ret = a + b\n    return ret\n\ndef substract(a: int, b: int) -> int:\n    ret = a - b\n    return ret\n\ndef multiply(a: int, b: int) -> int:\n    return a * b\n\nadd_op = create_component_from_func(add)\nsubstract_op = create_component_from_func(substract)\nmultiply_op = create_component_from_func(multiply)\n\nimport kfp.dsl as dsl\n@dsl.pipeline(name='Addition pipeline', description='An example pipeline that perform addition calculations')\ndef my_pipeline(a: int, b: int):\n    task_1 = add_op(a, b)\n    task_2 = substract_op(a, b)\n    task_3 = multiply_op(task_1.output, task_2.output)\n\nif __name__ == \"__main__\":\n    kfp.compiler.Compiler(mode=kfp.dsl.PipelineExecutionMode.V1_LEGACY).compile(my_pipeline, \"./add_pipeline_2.yaml\")\n"
  },
  {
    "repo": "seoeun25/ml-ops",
    "file_path": "kubeflow_pipeline/samples/add/add_cpu.py",
    "raw_url": "https://raw.githubusercontent.com/seoeun25/ml-ops/master/kubeflow_pipeline/samples/add/add_cpu.py",
    "content": "\"\"\"\n# $ python add_cpu.py\n# This will upload add_cpu.yaml. Check this on kubeflow-pipeline-ui\n#\n\"\"\"\n\nimport kfp\nfrom kfp.components import create_component_from_func\nfrom kubernetes.client import V1Toleration, V1Affinity, V1NodeSelector, V1NodeSelectorRequirement, V1NodeSelectorTerm\n\ndef kfp_client():\n    \"\"\"\n    Kubeflow pipelines client inside cluster.\n    \"\"\"\n    end_point=\"http://ml-pipeline.kubeflow.svc.cluster.local:8888\"\n    credentials = kfp.auth.ServiceAccountTokenVolumeCredentials(path=None)\n    client = kfp.Client(host=end_point, credentials=credentials)\n\n    return client\n\ndef add(a: float, b: float) -> float:\n    '''Calculates sum of two arguments'''\n    return a + b\n\nadd_op = create_component_from_func(add, output_component_file='add_component.yaml')\n\nimport kfp.dsl as dsl\n@dsl.pipeline(name=\"First Addition pipeline - python function base\", description=\"example of python function based pipeline\")\ndef add_pipeline(a='1', b='7',):\n    toleration = V1Toleration(\n        effect='NoSchedule',\n        key='cpu.aladin.skt/type',\n        value='common'\n    )\n    # toleration1 = V1Toleration(\n    #     effect='NoSchedule',\n    #     key='gpu.aladin.skt/type',\n    #     value='v100'\n    # )\n\n    first_add_task = add_op(a, 4)\n    first_add_task.add_toleration(toleration)\n    #first_add_task.add_toleration(toleration1)\n    first_add_task.set_cpu_request(\"4\").set_cpu_limit(\"4\").set_memory_request(\"16G\").set_memory_limit(\"16G\")\n    second_add_task = add_op(first_add_task.output, b)\n\n# Specify argument values for your pipeline run.\narguments = {'a': '7', 'b': '8'}\n\n# Create a pipeline run, using the client you initialized in a prior step.\nclient=kfp_client()\nkfp.compiler.Compiler(mode=kfp.dsl.PipelineExecutionMode.V1_LEGACY).compile(add_pipeline, \"./add_cpu.yaml\")\nclient.upload_pipeline(pipeline_package_path=\"./add_cpu.yaml\", pipeline_name=\"add_cpu\", description=\"Addition Python function based pipeline\")\n"
  },
  {
    "repo": "seoeun25/ml-ops",
    "file_path": "kubeflow_pipeline/samples/add/addition_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/seoeun25/ml-ops/master/kubeflow_pipeline/samples/add/addition_pipeline.py",
    "content": "\"\"\"\n# $ python addition_pipeline.py\n# This will upload addition_pipeline.yaml. Check this on kubeflow-pipeline-ui\n#\n\"\"\"\n\nimport kfp\nfrom kfp.components import create_component_from_func\n\ndef kfp_client():\n    \"\"\"\n    Kubeflow pipelines client inside cluster.\n    \"\"\"\n    end_point=\"http://ml-pipeline.kubeflow.svc.cluster.local:8888\"\n    credentials = kfp.auth.ServiceAccountTokenVolumeCredentials(path=None)\n    client = kfp.Client(host=end_point, credentials=credentials)\n\n    return client\n\ndef add(a: float, b: float) -> float:\n    '''Calculates sum of two arguments'''\n    return a + b\n\nadd_op = create_component_from_func(add, output_component_file='add_component.yaml')\n\nimport kfp.dsl as dsl\n@dsl.pipeline(name=\"First Addition pipeline - python function base\", description=\"example of python function based pipeline\")\ndef add_pipeline(\n        a='1',\n        b='7',\n):\n    # Passes a pipeline parameter and a constant value to the `add_op` factory\n    # function.\n    first_add_task = add_op(a, 4)\n    # Passes an output reference from `first_add_task` and a pipeline parameter\n    # to the `add_op` factory function. For operations with a single return\n    # value, the output reference can be accessed as `task.output` or\n    # `task.outputs['output_name']`.\n    second_add_task = add_op(first_add_task.output, b)\n\n# Specify argument values for your pipeline run.\narguments = {'a': '7', 'b': '8'}\n\n# Create a pipeline run, using the client you initialized in a prior step.\nclient=kfp_client()\n## experiments\nlist_experiments = client.list_experiments()\nprint(\"experiments\", )\nfor i in range(list_experiments.total_size):\n    print(list_experiments.experiments[i].id)\n# create experiment\n#client.create_experiment(name=\"add_org\", description=\"addition pipeline using python function base\")\n\n# creat run. not pipeline\n#client.create_run_from_pipeline_func(add_pipeline, arguments=arguments)\n# compile: create pipeline.yaml\nkfp.compiler.Compiler(mode=kfp.dsl.PipelineExecutionMode.V1_LEGACY).compile(add_pipeline, \"./addition_pipeline.yaml\")\nclient.upload_pipeline(pipeline_package_path=\"./addition_pipeline.yaml\", pipeline_name=\"addition_pipeline\", description=\"Addition Python function based pipeline\")\n"
  },
  {
    "repo": "seoeun25/ml-ops",
    "file_path": "kubeflow_pipeline/samples/edd_monitor/edd_checker.py",
    "raw_url": "https://raw.githubusercontent.com/seoeun25/ml-ops/master/kubeflow_pipeline/samples/edd_monitor/edd_checker.py",
    "content": "\"\"\"\nThis is pipeline to monitor whether edd data loading is successful.\n    $ python edd_checker.py\n\n    * create and upload edd_checker.yaml(pipeline) to kubeflow.\n    * check ede_monitor at kubeflow.\n\"\"\"\nimport kfp.compiler\nfrom kfp.components import create_component_from_func\nimport kfp\n\ndef kfp_client():\n    \"\"\"\n    Kubeflow pipelines client inside cluster.\n    \"\"\"\n    end_point=\"http://ml-pipeline.kubeflow.svc.cluster.local:8888\"\n    credentials = kfp.auth.ServiceAccountTokenVolumeCredentials(path=None)\n    client = kfp.Client(host=end_point, credentials=credentials)\n\n    return client\n\ndef base_image() -> str:\n    import os\n    import re\n\n    iam = os.environ.get(\"AWS_ROLE_ARN\")\n    account = re.findall(\"arn:aws:iam::(.*):role*\", iam)[0]\n    region = os.environ.get(\"AWS_REGION\")\n    # base_image = \"{}.dkr.ecr.{}.amazonaws.com/aladin-runtime:anaconda-cpu\".format(account, region)\n    base_image = \"{}.dkr.ecr.{}.amazonaws.com/aladin:runtime-common-cpu-202404r1\".format(account, region)\n    print(\"base_image = {}\".format(base_image))\n    return base_image\n\ndef apollo(cur_date: str, debug:bool, args: str) -> str:\n    from aladin import trino\n    from aladin import edd_util\n    from datetime import datetime\n\n    if cur_date is None or cur_date == \"\":\n        cur_date = datetime.today().strftime(\"%Y%m%d\")\n    print(\"cur_date = {}, debug={}, args = {}\".format(cur_date, debug, args))\n\n    def check_last_data(last_date: str, cur_date: str, days: int):\n        if debug:\n            return\n        if last_date < edd_util.date_add(cur_date, days).strftime(\"%Y%m%d\"):\n            raise Exception(\"cur_date = {}, last_date = {}. expected last_date = {} \".format(cur_date, last_date, edd_util.date_add(cur_date, days).strftime(\"%Y%m%d\")))\n\n    catalog_name = \"edd_hive\"\n    schema_name = \"apollo\"\n    def apollo_helper(cur_date: str, args: str):\n        table_name = \"luna_user\"\n        query = f\"SELECT count(*) as cnt FROM {catalog_name}.{schema_name}.{table_name} t \"\n        r1 = edd_util.fetchmany(conn=conn, query=query, size=5, column_name=[\"cnt\"])\n\n        table_name = \"luna_id_apollo_sub\"\n        query = f\"SELECT dt as p_dt FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by dt desc\"\n        r2 = edd_util.fetchmany(conn=conn, query=query, size=5, column_name=[\"p_dt\"])\n        check_last_data(r2, cur_date, -2)\n\n        table_name = \"luna_comm_log\"\n        query = f\"SELECT dt as p_dt FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by dt desc\"\n        r3 = edd_util.fetchmany(conn=conn, query=query, size=5, column_name=[\"p_dt\"])\n        check_last_data(r3, cur_date, -2)\n\n        table_name = \"user_context_log\"\n        query = f\"SELECT dt as p_dt, hh as p_hh FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by dt desc, hh desc\"\n        r4 = edd_util.fetchmany(conn, query=query, size=5, column_name=[\"p_dt\", \"p_hh\"])\n        check_last_data(r4, cur_date, -2)\n\n    conn = trino.connect(catalog=catalog_name, schema=schema_name)\n    try:\n        apollo_helper(cur_date, args)\n    except Exception as error:\n        print(\"-- Exception --\", error)\n        raise\n\n    # Close trino connection\n    conn.close()\n    print(\"Finish {}\".format(schema_name))\n    return \"ok\"\n\ndef cpm(previous:str, cur_date: str, debug:bool, args: str) -> str:\n    from aladin import trino\n    from aladin import edd_util\n    from datetime import datetime\n\n    if cur_date is None or cur_date == \"\":\n        cur_date = datetime.today().strftime(\"%Y%m%d\")\n    print(\"previous={}, cur_date = {}, debug={}, args = {}\".format(previous, cur_date, debug, args))\n\n    def check_last_data(last_date: str, cur_date: str, days: int):\n        if debug:\n            return\n        if last_date < edd_util.date_add(cur_date, days).strftime(\"%Y%m%d\"):\n            raise Exception(\"cur_date = {}, last_date = {}. expected last_date = {} \".format(cur_date, last_date, edd_util.date_add(cur_date, days).strftime(\"%Y%m%d\")))\n\n    catalog_name = \"edd_hive\"\n    schema_name = \"cpm\"\n\n    def cpm_helper(cur_date: str, args: str):\n        table_name = \"life_locationfeature_monthly\"\n        query = f\"SELECT ym as p_ym FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by ym desc\"\n        r1 = edd_util.fetchmany(conn=conn, query=query, size=5, column_name=[\"p_ym\"])\n        check_last_data(r1, cur_date, -60)\n\n        table_name = \"life_visit_poi_monthly\"\n        query = f\"SELECT ym as p_ym FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by ym desc\"\n        r2 = edd_util.fetchmany(conn=conn, query=query, size=5, column_name=[\"p_ym\"])\n        check_last_data(r2, cur_date, -60)\n\n    conn = trino.connect(catalog=catalog_name, schema=schema_name)\n    try:\n        cpm_helper(cur_date, args)\n    except Exception as error:\n        print(\"-- Exception --\", error)\n        raise\n\n    # Close trino connection\n    conn.close()\n    print(\"Finish {}\".format(schema_name))\n    return \"ok\"\n\ndef di_cpm(previous:str, cur_date: str, debug:bool, args: str) -> str:\n    from aladin import trino\n    from aladin import edd_util\n    from datetime import datetime\n\n    if cur_date is None or cur_date == \"\":\n        cur_date = datetime.today().strftime(\"%Y%m%d\")\n    print(\"previous={}, cur_date = {}, debug={}, args = {}\".format(previous, cur_date, debug, args))\n\n    def check_last_data(last_date: str, cur_date: str, days: int):\n        if debug:\n            return\n        if last_date < edd_util.date_add(cur_date, days).strftime(\"%Y%m%d\"):\n            raise Exception(\"cur_date = {}, last_date = {}. expected last_date = {} \".format(cur_date, last_date, edd_util.date_add(cur_date, days).strftime(\"%Y%m%d\")))\n\n    catalog_name = \"edd_hive\"\n    schema_name = \"di_cpm\"\n\n    def di_cpm_helper(cur_date: str, args: str):\n        table_name = \"base_tasa_rel_pred_monthly\"\n        query = f\"SELECT ym as p_ym FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by ym desc\"\n        r1 = edd_util.fetchmany(conn=conn, query=query, size=5, column_name=[\"p_ym\"])\n        check_last_data(r1, cur_date, -60)\n        r2 = edd_util.fetchmany(conn=conn, query=query, size=5, column_name=[\"p_ym\"])\n\n        table_name = \"fmly_hhld_pf_svc_monthly\"\n        query = f\"SELECT ym as p_ym FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by ym desc\"\n        r2 = edd_util.fetchmany(conn=conn, query=query, size=5, column_name=[\"p_ym\"])\n        check_last_data(r2, cur_date, -60)\n\n        table_name = \"fmly_pf_edge_monthly\"\n        query = f\"SELECT ym as p_ym FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by ym desc\"\n        r3 = edd_util.fetchmany(conn=conn, query=query, size=5, column_name=[\"p_ym\"])\n        check_last_data(r3, cur_date, -60)\n\n        table_name = \"general_pf_svc_monthly\"\n        query = f\"SELECT * FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by ym desc\"\n        r4 = edd_util.fetchmany(conn=conn, query=query, size=5, column_name=[\"ym\", \"cat1\", \"cat2\"])\n        check_last_data(r4, cur_date, -60)\n\n        table_name = \"seg_profile_inference_svc_monthly\"\n        query = f\"SELECT * FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by ym desc\"\n        r5 = edd_util.fetchmany(conn=conn, query=query, size=5, column_name=[\"ym\", \"age_group_cd\"])\n        check_last_data(r5, cur_date, -60)\n\n        table_name = \"seg_profile_seg_meta\"\n        query = f\"SELECT count(*) as cnt FROM {catalog_name}.{schema_name}.{table_name}\"\n        r6 = edd_util.fetchmany(conn=conn, query=query, size=5, column_name=[\"cnt\"])\n\n    conn = trino.connect(catalog=catalog_name, schema=schema_name)\n    try:\n        di_cpm_helper(cur_date, args)\n    except Exception as error:\n        print(\"-- Exception --\", error)\n        raise\n\n    # Close trino connection\n    conn.close()\n    print(\"Finish {}\".format(schema_name))\n    return \"ok\"\n\ndef ict_11st_11st(previous:str, cur_date: str, debug:bool, args: str) -> str:\n    from aladin import trino\n    from aladin import edd_util\n    from datetime import datetime\n\n    if cur_date is None or cur_date == \"\":\n        cur_date = datetime.today().strftime(\"%Y%m%d\")\n    print(\"previous={}, cur_date = {}, debug={}, args = {}\".format(previous, cur_date, debug, args))\n\n    def check_last_data(last_date: str, cur_date: str, days: int):\n        if debug:\n            return\n        if last_date < edd_util.date_add(cur_date, days).strftime(\"%Y%m%d\"):\n            raise Exception(\"cur_date = {}, last_date = {}. expected last_date = {} \".format(cur_date, last_date, edd_util.date_add(cur_date, days).strftime(\"%Y%m%d\")))\n\n    catalog_name = \"edd_hive\"\n    schema_name = \"ict_11st_11st\"\n\n    def ict_11st_11st_helper(cur_date: str, args: str):\n        table_name = \"tlounge_itg_agr_st11_dealings\"\n        query = f\"SELECT part_date FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by part_date desc\"\n        r1 = edd_util.fetchmany(conn=conn, query=query, size=5, column_name=[\"part_date\"])\n        check_last_data(r1, cur_date, -3)\n\n        table_name = \"tlounge_itg_agr_st11_member\"\n        query = f\"SELECT part_date FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by part_date desc\"\n        r2 = edd_util.fetchmany(conn=conn, query=query, size=5, column_name=[\"part_date\"])\n        check_last_data(r2, cur_date, -3)\n\n    conn = trino.connect(catalog=catalog_name, schema=schema_name)\n    try:\n        ict_11st_11st_helper(cur_date, args)\n    except Exception as error:\n        print(\"-- Exception --\", error)\n        raise\n    # Close trino connection\n    conn.close()\n    print(\"Finish {}\".format(schema_name))\n    return \"ok\"\n\ndef ict_skb_acc(previous:str, cur_date: str, debug:bool, args: str) -> str:\n    from aladin import trino\n    from aladin import edd_util\n    from datetime import datetime\n\n    if cur_date is None or cur_date == \"\":\n        cur_date = datetime.today().strftime(\"%Y%m%d\")\n    print(\"previous={}, cur_date = {}, debug={}, args = {}\".format(previous, cur_date, debug, args))\n\n    def check_last_data(last_date: str, cur_date: str, days: int):\n        if debug:\n            return\n        if last_date < edd_util.date_add(cur_date, days).strftime(\"%Y%m%d\"):\n            raise Exception(\"cur_date = {}, last_date = {}. expected last_date = {} \".format(cur_date, last_date, edd_util.date_add(cur_date, days).strftime(\"%Y%m%d\")))\n\n    catalog_name = \"edd_hive\"\n    schema_name = \"ict_skb_acc\"\n\n    def ict_skb_acc_helper(cur_date: str, args: str):\n        table_name = \"cc_svc_prst_month\"\n        query = f\"SELECT ym as p_ym FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by ym desc\"\n        r1 = edd_util.fetchmany(conn=conn, query=query, size=5, column_name=[\"p_ym\"])\n        check_last_data(r1, cur_date, -60)\n\n    conn = trino.connect(catalog=catalog_name, schema=schema_name)\n    try:\n        ict_skb_acc_helper(cur_date, args)\n    except Exception as error:\n        print(\"-- Exception --\", error)\n        raise\n    # Close trino connection\n    conn.close()\n    print(\"Finish {}\".format(schema_name))\n    return \"ok\"\n\ndef ict_skt_common(previous:str, cur_date: str, debug:bool, args: str) -> str:\n    from aladin import trino\n    from aladin import edd_util\n    from datetime import datetime\n\n    if cur_date is None or cur_date == \"\":\n        cur_date = datetime.today().strftime(\"%Y%m%d\")\n    print(\"previous={}, cur_date = {}, debug={}, args = {}\".format(previous, cur_date, debug, args))\n\n    def check_last_data(last_date: str, cur_date: str, days: int):\n        if debug:\n            return\n        if last_date < edd_util.date_add(cur_date, days).strftime(\"%Y%m%d\"):\n            raise Exception(\"cur_date = {}, last_date = {}. expected last_date = {} \".format(cur_date, last_date, edd_util.date_add(cur_date, days).strftime(\"%Y%m%d\")))\n\n    catalog_name = \"edd_hive\"\n    schema_name = \"ict_skt_common\"\n\n    def ict_skt_common_helper(cur_date: str, args: str):\n        table_name = \"ci_mst_u14\"\n        query = f\"SELECT svc_name FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" \"\n        edd_util.fetchmany(conn=conn, query=query, size=5, column_name=[\"svc_name\"])\n        query = f\"SELECT count(*) as cnt FROM {catalog_name}.{schema_name}.{table_name} where svc_name='mobile'\"\n        edd_util.fetchmany(conn=conn, query=query, size=5, column_name=[\"cnt\"])\n\n        table_name = \"ict_key_mst\"\n        query = f\"SELECT site FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" \"\n        edd_util.fetchmany(conn=conn, query=query, size=5, column_name=[\"site\"])\n        query = f\"SELECT count(*) as cnt FROM {catalog_name}.{schema_name}.{table_name} where site='11st'\"\n        edd_util.fetchmany(conn=conn, query=query, size=5, column_name=[\"cnt\"])\n\n        table_name = \"ict_key_mst_tmm\"\n        query = f\"SELECT site FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" \"\n        edd_util.fetchmany(conn=conn, query=query, size=5, column_name=[\"site\"])\n        query = f\"SELECT count(*) as cnt FROM {catalog_name}.{schema_name}.{table_name} where site='11st'\"\n        edd_util.fetchmany(conn=conn, query=query, size=5, column_name=[\"cnt\"])\n\n    conn = trino.connect(catalog=catalog_name, schema=schema_name)\n    try:\n        ict_skt_common_helper(cur_date, args)\n    except Exception as error:\n        print(\"-- Exception --\", error)\n        raise\n    # Close trino connection\n    conn.close()\n    print(\"Finish {}\".format(schema_name))\n    return \"ok\"\n\ndef ict_tmm_tmap(previous:str, cur_date: str, debug:bool, args: str) -> str:\n    from aladin import trino\n    from aladin import edd_util\n    from datetime import datetime\n\n    if cur_date is None or cur_date == \"\":\n        cur_date = datetime.today().strftime(\"%Y%m%d\")\n    print(\"previous={}, cur_date = {}, debug={}, args = {}\".format(previous, cur_date, debug, args))\n\n    def check_last_data(last_date: str, cur_date: str, days: int):\n        if debug:\n            return\n        if last_date < edd_util.date_add(cur_date, days).strftime(\"%Y%m%d\"):\n            raise Exception(\"cur_date = {}, last_date = {}. expected last_date = {} \".format(cur_date, last_date, edd_util.date_add(cur_date, days).strftime(\"%Y%m%d\")))\n\n    catalog_name = \"edd_hive\"\n    schema_name = \"ict_tmm_tmap\"\n\n    def ict_tmm_tmap_helper(cur_date: str, args: str):\n        table_name = \"tmap_favorate\"\n        query = f\"SELECT dt as p_dt FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by dt desc\"\n        r1 = edd_util.fetchmany(conn=conn, query=query, size=5, column_name=[\"p_dt\"])\n        check_last_data(r1, cur_date, -2)\n\n        table_name = \"tmap_poimeta\"\n        query = f\"SELECT dt as p_dt FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by dt desc\"\n        r2 = edd_util.fetchmany(conn=conn, query=query, size=5, column_name=[\"p_dt\"])\n        check_last_data(r2, cur_date, -2)\n\n        table_name = \"tmap_routehistory\"\n        query = f\"SELECT dt as p_dt, hh as p_hh FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by dt desc\"\n        r3 = edd_util.fetchmany(conn=conn, query=query, size=5, column_name=[\"p_dt\", \"p_hh\"])\n        check_last_data(r3, cur_date, -2)\n\n        table_name = \"tmap_rprsd\"\n        query = f\"SELECT dt as p_dt, hh as p_hh FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by dt desc\"\n        r4 = edd_util.fetchmany(conn=conn, query=query, size=5, column_name=[\"p_dt\", \"p_hh\"])\n        check_last_data(r4, cur_date, -2)\n\n    conn = trino.connect(catalog=catalog_name, schema=schema_name)\n    try:\n        ict_tmm_tmap_helper(cur_date, args)\n    except Exception as error:\n        print(\"-- Exception --\", error)\n        raise\n    # Close trino connection\n    conn.close()\n    print(\"Finish {}\".format(schema_name))\n    return \"ok\"\n\ndef litmus(previous:str, cur_date: str, debug:bool, args: str) -> str:\n    from aladin import trino\n    from aladin import edd_util\n    from datetime import datetime\n\n    if cur_date is None or cur_date == \"\":\n        cur_date = datetime.today().strftime(\"%Y%m%d\")\n    print(\"previous={}, cur_date = {}, debug={}, args = {}\".format(previous, cur_date, debug, args))\n\n    def check_last_data(last_date: str, cur_date: str, days: int):\n        if debug:\n            return\n        if last_date < edd_util.date_add(cur_date, days).strftime(\"%Y%m%d\"):\n            raise Exception(\"cur_date = {}, last_date = {}. expected last_date = {} \".format(cur_date, last_date, edd_util.date_add(cur_date, days).strftime(\"%Y%m%d\")))\n\n    catalog_name = \"edd_hive\"\n    schema_name = \"litmus\"\n\n    def litmus_helper(cur_date: str, args: str):\n        table_name = \"litmus_trip\"\n        query = f\"SELECT dt as p_dt FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by dt desc\"\n        r1 = edd_util.fetchmany(conn=conn, query=query, size=5, column_name=[\"p_dt\"])\n        check_last_data(r1, cur_date, -5)\n\n    conn = trino.connect(catalog=catalog_name, schema=schema_name)\n    try:\n        litmus_helper(cur_date, args)\n    except Exception as error:\n        print(\"-- Exception --\", error)\n        raise\n    # Close trino connection\n    conn.close()\n    print(\"Finish {}\".format(schema_name))\n    return \"ok\"\n\ndef loc_meta_raw(previous:str, cur_date: str, debug:bool, args: str) -> str:\n    from aladin import trino\n    from aladin import edd_util\n    from datetime import datetime\n\n    if cur_date is None or cur_date == \"\":\n        cur_date = datetime.today().strftime(\"%Y%m%d\")\n    print(\"previous={}, cur_date = {}, debug={}, args = {}\".format(previous, cur_date, debug, args))\n\n    def check_last_data(last_date: str, cur_date: str, days: int):\n        if debug:\n            return\n        if last_date < edd_util.date_add(cur_date, days).strftime(\"%Y%m%d\"):\n            raise Exception(\"cur_date = {}, last_date = {}. expected last_date = {} \".format(cur_date, last_date, edd_util.date_add(cur_date, days).strftime(\"%Y%m%d\")))\n\n    catalog_name = \"edd_hive\"\n    schema_name = \"loc_meta_raw\"\n\n    def loc_meta_raw_helper(cur_date: str, args: str):\n        table_name = \"enb_base\"\n        query = f\"SELECT dt as p_dt FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by dt desc\"\n        r1 = edd_util.fetchmany(conn=conn, query=query, size=5, column_name=[\"p_dt\"])\n        check_last_data(r1, cur_date, -2)\n\n    conn = trino.connect(catalog=catalog_name, schema=schema_name)\n    try:\n        loc_meta_raw_helper(cur_date, args)\n    except Exception as error:\n        print(\"-- Exception --\", error)\n        raise\n    # Close trino connection\n    conn.close()\n    print(\"Finish {}\".format(schema_name))\n    return \"ok\"\n\nbase_image = base_image()\napollo_op = create_component_from_func(apollo, base_image=base_image)\ncpm_op = create_component_from_func(cpm, base_image=base_image)\ndi_cpm_op = create_component_from_func(di_cpm, base_image=base_image)\nict_11st_11st_op = create_component_from_func(ict_11st_11st, base_image=base_image)\nict_skb_acc_op = create_component_from_func(ict_skb_acc, base_image=base_image)\nict_skt_common_op = create_component_from_func(ict_skt_common, base_image=base_image)\nict_tmm_tmap_op = create_component_from_func(ict_tmm_tmap, base_image=base_image)\nlitmus_op = create_component_from_func(litmus, base_image=base_image)\nloc_meta_raw_op = create_component_from_func(loc_meta_raw, base_image=base_image)\n\nimport kfp.dsl as dsl\n@dsl.pipeline(name='edd_checker_pipeline', description='EDD data loading monitor')\ndef my_pipeline(cur_date: str, debug: bool, args: str):\n    print(\"my_pipeline: cur_date={}, debug={}, args={}\".format(cur_date, debug, args))\n    data_op = dsl.VolumeOp(name=\"data-pvc\",\n                           resource_name=\"data-volume\",\n                           generate_unique_name=False,\n                           action='apply',\n                           size=\"10Gi\",\n                           modes=dsl.VOLUME_MODE_RWO)\n    task_1 = apollo_op(cur_date=cur_date, debug=debug, args=args).add_pvolumes({\"/data\": data_op.volume})\n    task_2 = cpm_op(previous=task_1.output, cur_date=cur_date, debug=debug, args=args)\n    task_3 = di_cpm_op(task_2.output, cur_date=cur_date, debug=debug, args=args)\n    task_4 = ict_11st_11st_op(task_3.output, cur_date=cur_date, debug=debug, args=args)\n    task_5 = ict_skb_acc_op(task_4.output, cur_date=cur_date, debug=debug, args=args)\n    task_6 = ict_skt_common_op(task_5.output, cur_date=cur_date, debug=debug, args=args)\n    task_7 = ict_tmm_tmap_op(task_6.output, cur_date=cur_date, debug=debug, args=args)\n    task_8 = litmus_op(task_7.output, cur_date=cur_date, debug=debug, args=args)\n    task_9 = loc_meta_raw_op(task_8.output, cur_date=cur_date, debug=debug, args=args)\n\nif __name__ == \"__main__\":\n    kfp.compiler.Compiler(mode=kfp.dsl.PipelineExecutionMode.V1_LEGACY).compile(my_pipeline, \"./edd_checker.yaml\")\n    client = kfp_client()\n    client.upload_pipeline(pipeline_package_path=\"./edd_checker.yaml\", pipeline_name=\"edd_checker\", description=\"EDD Checker pipeline\")\n\n"
  },
  {
    "repo": "seoeun25/ml-ops",
    "file_path": "kubeflow_pipeline/samples/edd_monitor/edd_monitor.py",
    "raw_url": "https://raw.githubusercontent.com/seoeun25/ml-ops/master/kubeflow_pipeline/samples/edd_monitor/edd_monitor.py",
    "content": "\"\"\"\nThis is pipeline to monitor whether edd data loading is successful.\n    $ python edd_monitor.py\n\n    * create and upload edd_monitor.yaml(pipeline) to kubeflow.\n    * check ede_monitor at kubeflow.\n\"\"\"\nimport kfp.compiler\nfrom kfp.components import create_component_from_func\nimport kfp\nfrom datetime import datetime\n\ndef kfp_client():\n    \"\"\"\n    Kubeflow pipelines client inside cluster.\n    \"\"\"\n    end_point=\"http://ml-pipeline.kubeflow.svc.cluster.local:8888\"\n    credentials = kfp.auth.ServiceAccountTokenVolumeCredentials(path=None)\n    client = kfp.Client(host=end_point, credentials=credentials)\n\n    return client\n\ndef base_image() -> str:\n    import os\n    import re\n\n    iam = os.environ.get(\"AWS_ROLE_ARN\")\n    account = re.findall(\"arn:aws:iam::(.*):role*\", iam)[0]\n    region = os.environ.get(\"AWS_REGION\")\n    base_image = \"{}.dkr.ecr.{}.amazonaws.com/aladin-runtime:anaconda-cpu\".format(account, region)\n    print(\"base_image = {}\".format(base_image))\n    return base_image\n\ndef apollo(cur_date: str, debug:bool, args: str) -> str:\n    import pandas as pd\n    from datetime import datetime, timedelta\n    if cur_date is None or cur_date == \"\":\n        cur_date = datetime.today().strftime(\"%Y%m%d\")\n    print(\"cur_date = {}, debug={}, args = {}\".format(cur_date, debug, args))\n\n    schema_name = \"apollo\"\n\n    def log_error(schema_name: str, query: str, error: str):\n        print(\"[{}], error={}, query={}\".format(schema_name, error, query))\n\n    def check_last_data(last_date: str, cur_date: str, days: int):\n        if debug:\n            return\n        if last_date < date_add(cur_date, days).strftime(\"%Y%m%d\"):\n            raise Exception(\"cur_date = {}, last_date = {}. expected last_date = {} \".format(cur_date, last_date, date_add(cur_date, days).strftime(\"%Y%m%d\")))\n    def date_add(cur_date:str, days: int) -> datetime:\n        \"\"\"\n        Convert date string to datetime.\n        ex> date_deleta(\"20231020\" -2): datetime object (2023-10-18 00:00:00)\n        :param cur_date: cur_date in %Y%m%d format\n        :param days: delta days. plus or minus days\n        :return: datetime\n        \"\"\"\n        return datetime.strptime(cur_date, \"%Y%m%d\") + timedelta(days=days)\n\n    def fetchmany(query: str, size: int, column_name: list) -> object:\n        \"\"\"\n        Fetch data.\n\n        Parameters\n        ----------\n        query: string. SQL.\n        size: int. fetch rows\n        column_name: list. fetch columns\n\n        Returns\n        -------\n        object: first row, first column data\n        \"\"\"\n        print(\"----------------------\")\n        print(query)\n        cursor.execute(query)\n        df = pd.DataFrame(cursor.fetchmany(size=size), columns=column_name)\n        print(\"rows =\", len(df))\n        print(\"columns = \", df.columns)\n        min_row = len(df)\n\n        for i in range(min_row):\n            projection = []\n            for j in range(len(column_name)):\n                projection.append(df.loc[i][column_name[j]])\n                if i == 0 and j == 0:\n                    result = df.loc[i][column_name[j]]\n            print(projection)\n\n        return result\n\n    catalog_name = \"edd_hive\"\n    def apollo_helper(cur_date: str, args: str):\n        table_name = \"luna_user\"\n        query = f\"SELECT count(*) as cnt FROM {catalog_name}.{schema_name}.{table_name} t \"\n        r1 = fetchmany(query, 5, [\"cnt\"])\n\n        table_name = \"luna_id_apollo_sub\"\n        query = f\"SELECT dt as p_dt FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by dt desc\"\n        r2 = fetchmany(query, 5, [\"p_dt\"])\n        check_last_data(r2, cur_date, -2)\n\n        table_name = \"luna_comm_log\"\n        query = f\"SELECT dt as p_dt FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by dt desc\"\n        r3 = fetchmany(query, 5, [\"p_dt\"])\n        check_last_data(r3, cur_date, -2)\n\n        table_name = \"user_context_log\"\n        query = f\"SELECT dt as p_dt, hh as p_hh FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by dt desc, hh desc\"\n        r4 = fetchmany(query, 5, [\"p_dt\", \"p_hh\"])\n        check_last_data(r4, cur_date, -2)\n\n    from aladin import trino\n    conn = trino.connect(catalog=catalog_name, schema=schema_name)\n    cursor = conn.cursor()\n    try:\n        apollo_helper(cur_date, args)\n    except Exception as error:\n        print(\"-- Exception --\", error)\n        raise\n\n    # Close trino connection\n    conn.close()\n    print(\"Finish {}\".format(schema_name))\n    return \"ok\"\n\ndef cpm(previous:str, cur_date: str, debug:bool, args: str) -> str:\n    import pandas as pd\n    from datetime import datetime, timedelta\n    if cur_date is None or cur_date == \"\":\n        cur_date = datetime.today().strftime(\"%Y%m%d\")\n    print(\"previous={}, cur_date = {}, debug={}, args = {}\".format(previous, cur_date, debug, args))\n\n    schema_name = \"cpm\"\n    def log_error(schema_name: str, query: str, error: str):\n        print(\"[{}], error={}, query={}\".format(schema_name, error, query))\n\n    def check_last_data(last_date: str, cur_date: str, days: int):\n        if debug:\n            return\n        if last_date < date_add(cur_date, days).strftime(\"%Y%m%d\"):\n            raise Exception(\"cur_date = {}, last_date = {}. expected last_date = {} \".format(cur_date, last_date, date_add(cur_date, days).strftime(\"%Y%m%d\")))\n    def date_add(cur_date:str, days: int) -> datetime:\n        return datetime.strptime(cur_date, \"%Y%m%d\") + timedelta(days=days)\n\n    def fetchmany(query: str, size: int, column_name: list) -> object:\n        print(\"----------------------\")\n        print(query)\n        cursor.execute(query)\n        df = pd.DataFrame(cursor.fetchmany(size=size), columns=column_name)\n        print(\"rows =\", len(df))\n        print(\"columns = \", df.columns)\n        min_row = len(df)\n\n        for i in range(min_row):\n            projection = []\n            for j in range(len(column_name)):\n                projection.append(df.loc[i][column_name[j]])\n                if i == 0 and j == 0:\n                    result = df.loc[i][column_name[j]]\n            print(projection)\n\n        return result\n\n    catalog_name = \"edd_hive\"\n    def cpm_helper(cur_date: str, args: str):\n        table_name = \"life_locationfeature_monthly\"\n        query = f\"SELECT ym as p_ym FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by ym desc\"\n        r1 = fetchmany(query=query, size=5, column_name=[\"p_ym\"])\n        check_last_data(r1, cur_date, -60)\n\n        table_name = \"life_visit_poi_monthly\"\n        query = f\"SELECT ym as p_ym FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by ym desc\"\n        r2 = fetchmany(query=query, size=5, column_name=[\"p_ym\"])\n        check_last_data(r2, cur_date, -60)\n\n    from aladin import trino\n    conn = trino.connect(catalog=catalog_name, schema=schema_name)\n    cursor = conn.cursor()\n    try:\n        cpm_helper(cur_date, args)\n    except Exception as error:\n        print(\"-- Exception --\", error)\n        raise\n\n    # Close trino connection\n    conn.close()\n    print(\"Finish {}\".format(schema_name))\n    return \"ok\"\n\ndef di_cpm(previous:str, cur_date: str, debug:bool, args: str) -> str:\n    import pandas as pd\n    from datetime import datetime, timedelta\n    if cur_date is None or cur_date == \"\":\n        cur_date = datetime.today().strftime(\"%Y%m%d\")\n    print(\"previous={}, cur_date = {}, debug={}, args = {}\".format(previous, cur_date, debug, args))\n\n    schema_name = \"di_cpm\"\n    def check_last_data(last_date: str, cur_date: str, days: int):\n        if debug:\n            return\n        if last_date < date_add(cur_date, days).strftime(\"%Y%m%d\"):\n            raise Exception(\"cur_date = {}, last_date = {}. expected last_date = {} \".format(cur_date, last_date, date_add(cur_date, days).strftime(\"%Y%m%d\")))\n    def date_add(cur_date:str, days: int) -> datetime:\n        return datetime.strptime(cur_date, \"%Y%m%d\") + timedelta(days=days)\n\n    def fetchmany(query: str, size: int, column_name: list) -> object:\n        print(\"----------------------\")\n        print(query)\n        cursor.execute(query)\n        df = pd.DataFrame(cursor.fetchmany(size=size), columns=column_name)\n        print(\"rows =\", len(df))\n        print(\"columns = \", df.columns)\n        min_row = len(df)\n\n        for i in range(min_row):\n            projection = []\n            for j in range(len(column_name)):\n                projection.append(df.loc[i][column_name[j]])\n                if i == 0 and j == 0:\n                    result = df.loc[i][column_name[j]]\n            print(projection)\n\n        return result\n\n    catalog_name = \"edd_hive\"\n    def di_cpm_helper(cur_date: str, args: str):\n        table_name = \"base_tasa_rel_pred_monthly\"\n        query = f\"SELECT ym as p_ym FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by ym desc\"\n        r1 = fetchmany(query=query, size=5, column_name=[\"p_ym\"])\n        check_last_data(r1, cur_date, -60)\n        r2 = fetchmany(query=query, size=5, column_name=[\"p_ym\"])\n\n        table_name = \"fmly_hhld_pf_svc_monthly\"\n        query = f\"SELECT ym as p_ym FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by ym desc\"\n        r2 = fetchmany(query=query, size=5, column_name=[\"p_ym\"])\n        check_last_data(r2, cur_date, -60)\n\n        table_name = \"fmly_pf_edge_monthly\"\n        query = f\"SELECT ym as p_ym FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by ym desc\"\n        r3 = fetchmany(query=query, size=5, column_name=[\"p_ym\"])\n        check_last_data(r3, cur_date, -60)\n\n        table_name = \"general_pf_svc_monthly\"\n        query = f\"SELECT * FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by ym desc\"\n        r4 = fetchmany(query=query, size=5, column_name=[\"ym\", \"cat1\", \"cat2\"])\n        check_last_data(r4, cur_date, -60)\n\n        table_name = \"seg_profile_inference_svc_monthly\"\n        query = f\"SELECT * FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by ym desc\"\n        r5 = fetchmany(query=query, size=5, column_name=[\"ym\", \"age_group_cd\"])\n        check_last_data(r5, cur_date, -60)\n\n        table_name = \"seg_profile_seg_meta\"\n        query = f\"SELECT count(*) as cnt FROM {catalog_name}.{schema_name}.{table_name}\"\n        r6 = fetchmany(query=query, size=5, column_name=[\"cnt\"])\n\n    from aladin import trino\n    conn = trino.connect(catalog=catalog_name, schema=schema_name)\n    cursor = conn.cursor()\n    try:\n        di_cpm_helper(cur_date, args)\n    except Exception as error:\n        print(\"-- Exception --\", error)\n        raise\n\n    # Close trino connection\n    conn.close()\n    print(\"Finish {}\".format(schema_name))\n    return \"ok\"\n\ndef ict_11st_11st(previous:str, cur_date: str, debug:bool, args: str) -> str:\n    import pandas as pd\n    from datetime import datetime, timedelta\n    if cur_date is None or cur_date == \"\":\n        cur_date = datetime.today().strftime(\"%Y%m%d\")\n    print(\"previous={}, cur_date = {}, debug={}, args = {}\".format(previous, cur_date, debug, args))\n\n    schema_name = \"ict_11st_11st\"\n    def check_last_data(last_date: str, cur_date: str, days: int):\n        if debug:\n            return\n        if last_date < date_add(cur_date, days).strftime(\"%Y%m%d\"):\n            raise Exception(\"cur_date = {}, last_date = {}. expected last_date = {} \".format(cur_date, last_date, date_add(cur_date, days).strftime(\"%Y%m%d\")))\n    def date_add(cur_date:str, days: int) -> datetime:\n        return datetime.strptime(cur_date, \"%Y%m%d\") + timedelta(days=days)\n\n    def fetchmany(query: str, size: int, column_name: list) -> object:\n        print(\"----------------------\")\n        print(query)\n        cursor.execute(query)\n        df = pd.DataFrame(cursor.fetchmany(size=size), columns=column_name)\n        print(\"rows =\", len(df))\n        print(\"columns = \", df.columns)\n        min_row = len(df)\n\n        for i in range(min_row):\n            projection = []\n            for j in range(len(column_name)):\n                projection.append(df.loc[i][column_name[j]])\n                if i == 0 and j == 0:\n                    result = df.loc[i][column_name[j]]\n            print(projection)\n\n        return result\n\n    catalog_name = \"edd_hive\"\n    def ict_11st_11st_helper(cur_date: str, args: str):\n        table_name = \"tlounge_itg_agr_st11_dealings\"\n        query = f\"SELECT part_date FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by part_date desc\"\n        r1 = fetchmany(query=query, size=5, column_name=[\"part_date\"])\n        check_last_data(r1, cur_date, -3)\n\n        table_name = \"tlounge_itg_agr_st11_member\"\n        query = f\"SELECT part_date FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by part_date desc\"\n        r2 = fetchmany(query=query, size=5, column_name=[\"part_date\"])\n        check_last_data(r2, cur_date, -3)\n\n    from aladin import trino\n    conn = trino.connect(catalog=catalog_name, schema=schema_name)\n    cursor = conn.cursor()\n    try:\n        ict_11st_11st_helper(cur_date, args)\n    except Exception as error:\n        print(\"-- Exception --\", error)\n        raise\n    # Close trino connection\n    conn.close()\n    print(\"Finish {}\".format(schema_name))\n    return \"ok\"\n\ndef ict_skb_acc(previous:str, cur_date: str, debug:bool, args: str) -> str:\n    import pandas as pd\n    from datetime import datetime, timedelta\n    if cur_date is None or cur_date == \"\":\n        cur_date = datetime.today().strftime(\"%Y%m%d\")\n    print(\"previous={}, cur_date = {}, debug={}, args = {}\".format(previous, cur_date, debug, args))\n\n    schema_name = \"ict_skb_acc\"\n    def check_last_data(last_date: str, cur_date: str, days: int):\n        if debug:\n            return\n        if last_date < date_add(cur_date, days).strftime(\"%Y%m%d\"):\n            raise Exception(\"cur_date = {}, last_date = {}. expected last_date = {} \".format(cur_date, last_date, date_add(cur_date, days).strftime(\"%Y%m%d\")))\n    def date_add(cur_date:str, days: int) -> datetime:\n        return datetime.strptime(cur_date, \"%Y%m%d\") + timedelta(days=days)\n\n    def fetchmany(query: str, size: int, column_name: list) -> object:\n        print(\"----------------------\")\n        print(query)\n        cursor.execute(query)\n        df = pd.DataFrame(cursor.fetchmany(size=size), columns=column_name)\n        print(\"rows =\", len(df))\n        print(\"columns = \", df.columns)\n        min_row = len(df)\n\n        for i in range(min_row):\n            projection = []\n            for j in range(len(column_name)):\n                projection.append(df.loc[i][column_name[j]])\n                if i == 0 and j == 0:\n                    result = df.loc[i][column_name[j]]\n            print(projection)\n\n        return result\n\n    catalog_name = \"edd_hive\"\n    def ict_skb_acc_helper(cur_date: str, args: str):\n        table_name = \"cc_svc_prst_month\"\n        query = f\"SELECT ym as p_ym FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by ym desc\"\n        r1 = fetchmany(query=query, size=5, column_name=[\"p_ym\"])\n        check_last_data(r1, cur_date, -60)\n\n    from aladin import trino\n    conn = trino.connect(catalog=catalog_name, schema=schema_name)\n    cursor = conn.cursor()\n    try:\n        ict_skb_acc_helper(cur_date, args)\n    except Exception as error:\n        print(\"-- Exception --\", error)\n        raise\n    # Close trino connection\n    conn.close()\n    print(\"Finish {}\".format(schema_name))\n    return \"ok\"\n\ndef ict_skt_common(previous:str, cur_date: str, debug:bool, args: str) -> str:\n    import pandas as pd\n    from datetime import datetime, timedelta\n    if cur_date is None or cur_date == \"\":\n        cur_date = datetime.today().strftime(\"%Y%m%d\")\n    print(\"previous={}, cur_date = {}, debug={}, args = {}\".format(previous, cur_date, debug, args))\n\n    schema_name = \"ict_skt_common\"\n    def check_last_data(last_date: str, cur_date: str, days: int):\n        if debug:\n            return\n        if last_date < date_add(cur_date, days).strftime(\"%Y%m%d\"):\n            raise Exception(\"cur_date = {}, last_date = {}. expected last_date = {} \".format(cur_date, last_date, date_add(cur_date, days).strftime(\"%Y%m%d\")))\n    def date_add(cur_date:str, days: int) -> datetime:\n        return datetime.strptime(cur_date, \"%Y%m%d\") + timedelta(days=days)\n\n    def fetchmany(query: str, size: int, column_name: list) -> object:\n        print(\"----------------------\")\n        print(query)\n        cursor.execute(query)\n        df = pd.DataFrame(cursor.fetchmany(size=size), columns=column_name)\n        print(\"rows =\", len(df))\n        print(\"columns = \", df.columns)\n        min_row = len(df)\n\n        for i in range(min_row):\n            projection = []\n            for j in range(len(column_name)):\n                projection.append(df.loc[i][column_name[j]])\n                if i == 0 and j == 0:\n                    result = df.loc[i][column_name[j]]\n            print(projection)\n\n        return result\n\n    catalog_name = \"edd_hive\"\n    def ict_skt_common_helper(cur_date: str, args: str):\n        table_name = \"ci_mst_u14\"\n        query = f\"SELECT svc_name FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" \"\n        fetchmany(query=query, size=5, column_name=[\"svc_name\"])\n        query = f\"SELECT count(*) as cnt FROM {catalog_name}.{schema_name}.{table_name} where svc_name='mobile'\"\n        fetchmany(query=query, size=5, column_name=[\"cnt\"])\n\n        table_name = \"ict_key_mst\"\n        query = f\"SELECT site FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" \"\n        fetchmany(query=query, size=5, column_name=[\"site\"])\n        query = f\"SELECT count(*) as cnt FROM {catalog_name}.{schema_name}.{table_name} where site='11st'\"\n        fetchmany(query=query, size=5, column_name=[\"cnt\"])\n\n        table_name = \"ict_key_mst_tmm\"\n        query = f\"SELECT site FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" \"\n        fetchmany(query=query, size=5, column_name=[\"site\"])\n        query = f\"SELECT count(*) as cnt FROM {catalog_name}.{schema_name}.{table_name} where site='11st'\"\n        fetchmany(query=query, size=5, column_name=[\"cnt\"])\n\n    from aladin import trino\n    conn = trino.connect(catalog=catalog_name, schema=schema_name)\n    cursor = conn.cursor()\n    try:\n        ict_skt_common_helper(cur_date, args)\n    except Exception as error:\n        print(\"-- Exception --\", error)\n        raise\n    # Close trino connection\n    conn.close()\n    print(\"Finish {}\".format(schema_name))\n    return \"ok\"\n\ndef ict_tmm_tmap(previous:str, cur_date: str, debug:bool, args: str) -> str:\n    import pandas as pd\n    from datetime import datetime, timedelta\n    if cur_date is None or cur_date == \"\":\n        cur_date = datetime.today().strftime(\"%Y%m%d\")\n    print(\"previous={}, cur_date = {}, debug={}, args = {}\".format(previous, cur_date, debug, args))\n\n    schema_name = \"ict_tmm_tmap\"\n    def check_last_data(last_date: str, cur_date: str, days: int):\n        if debug:\n            return\n        if last_date < date_add(cur_date, days).strftime(\"%Y%m%d\"):\n            raise Exception(\"cur_date = {}, last_date = {}. expected last_date = {} \".format(cur_date, last_date, date_add(cur_date, days).strftime(\"%Y%m%d\")))\n    def date_add(cur_date:str, days: int) -> datetime:\n        return datetime.strptime(cur_date, \"%Y%m%d\") + timedelta(days=days)\n\n    def fetchmany(query: str, size: int, column_name: list) -> object:\n        print(\"----------------------\")\n        print(query)\n        cursor.execute(query)\n        df = pd.DataFrame(cursor.fetchmany(size=size), columns=column_name)\n        print(\"rows =\", len(df))\n        print(\"columns = \", df.columns)\n        min_row = len(df)\n\n        for i in range(min_row):\n            projection = []\n            for j in range(len(column_name)):\n                projection.append(df.loc[i][column_name[j]])\n                if i == 0 and j == 0:\n                    result = df.loc[i][column_name[j]]\n            print(projection)\n\n        return result\n\n    catalog_name = \"edd_hive\"\n    def ict_tmm_tmap_helper(cur_date: str, args: str):\n        table_name = \"tmap_favorate\"\n        query = f\"SELECT dt as p_dt FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by dt desc\"\n        r1 = fetchmany(query, 5, [\"p_dt\"])\n        check_last_data(r1, cur_date, -2)\n\n        table_name = \"tmap_poimeta\"\n        query = f\"SELECT dt as p_dt FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by dt desc\"\n        r2 = fetchmany(query, 5, [\"p_dt\"])\n        check_last_data(r2, cur_date, -2)\n\n        table_name = \"tmap_routehistory\"\n        query = f\"SELECT dt as p_dt, hh as p_hh FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by dt desc\"\n        r3 = fetchmany(query, 5, [\"p_dt\", \"p_hh\"])\n        check_last_data(r3, cur_date, -2)\n\n        table_name = \"tmap_rprsd\"\n        query = f\"SELECT dt as p_dt, hh as p_hh FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by dt desc\"\n        r4 = fetchmany(query, 5, [\"p_dt\", \"p_hh\"])\n        check_last_data(r4, cur_date, -2)\n\n    from aladin import trino\n    conn = trino.connect(catalog=catalog_name, schema=schema_name)\n    cursor = conn.cursor()\n    try:\n        ict_tmm_tmap_helper(cur_date, args)\n    except Exception as error:\n        print(\"-- Exception --\", error)\n        raise\n    # Close trino connection\n    conn.close()\n    print(\"Finish {}\".format(schema_name))\n    return \"ok\"\n\ndef litmus(previous:str, cur_date: str, debug:bool, args: str) -> str:\n    import pandas as pd\n    from datetime import datetime, timedelta\n    if cur_date is None or cur_date == \"\":\n        cur_date = datetime.today().strftime(\"%Y%m%d\")\n    print(\"previous={}, cur_date = {}, debug={}, args = {}\".format(previous, cur_date, debug, args))\n\n    schema_name = \"litmus\"\n    def check_last_data(last_date: str, cur_date: str, days: int):\n        if debug:\n            return\n        if last_date < date_add(cur_date, days).strftime(\"%Y%m%d\"):\n            raise Exception(\"cur_date = {}, last_date = {}. expected last_date = {} \".format(cur_date, last_date, date_add(cur_date, days).strftime(\"%Y%m%d\")))\n    def date_add(cur_date:str, days: int) -> datetime:\n        return datetime.strptime(cur_date, \"%Y%m%d\") + timedelta(days=days)\n\n    def fetchmany(query: str, size: int, column_name: list) -> object:\n        print(\"----------------------\")\n        print(query)\n        cursor.execute(query)\n        df = pd.DataFrame(cursor.fetchmany(size=size), columns=column_name)\n        print(\"rows =\", len(df))\n        print(\"columns = \", df.columns)\n        min_row = len(df)\n\n        for i in range(min_row):\n            projection = []\n            for j in range(len(column_name)):\n                projection.append(df.loc[i][column_name[j]])\n                if i == 0 and j == 0:\n                    result = df.loc[i][column_name[j]]\n            print(projection)\n\n        return result\n\n    catalog_name = \"edd_hive\"\n    def litmus_helper(cur_date: str, args: str):\n        table_name = \"litmus_trip\"\n        query = f\"SELECT dt as p_dt FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by dt desc\"\n        r1 = fetchmany(query=query, size=5, column_name=[\"p_dt\"])\n        check_last_data(r1, cur_date, -5)\n\n    from aladin import trino\n    conn = trino.connect(catalog=catalog_name, schema=schema_name)\n    cursor = conn.cursor()\n    try:\n        litmus_helper(cur_date, args)\n    except Exception as error:\n        print(\"-- Exception --\", error)\n        raise\n    # Close trino connection\n    conn.close()\n    print(\"Finish {}\".format(schema_name))\n    return \"ok\"\n\ndef loc_meta_raw(previous:str, cur_date: str, debug:bool, args: str) -> str:\n    import pandas as pd\n    from datetime import datetime, timedelta\n    if cur_date is None or cur_date == \"\":\n        cur_date = datetime.today().strftime(\"%Y%m%d\")\n    print(\"previous={}, cur_date = {}, debug={}, args = {}\".format(previous, cur_date, debug, args))\n\n    schema_name = \"loc_meta_raw\"\n    def check_last_data(last_date: str, cur_date: str, days: int):\n        if debug:\n            return\n        if last_date < date_add(cur_date, days).strftime(\"%Y%m%d\"):\n            raise Exception(\"cur_date = {}, last_date = {}. expected last_date = {} \".format(cur_date, last_date, date_add(cur_date, days).strftime(\"%Y%m%d\")))\n    def date_add(cur_date:str, days: int) -> datetime:\n        return datetime.strptime(cur_date, \"%Y%m%d\") + timedelta(days=days)\n\n    def fetchmany(query: str, size: int, column_name: list) -> object:\n        print(\"----------------------\")\n        print(query)\n        cursor.execute(query)\n        df = pd.DataFrame(cursor.fetchmany(size=size), columns=column_name)\n        print(\"rows =\", len(df))\n        print(\"columns = \", df.columns)\n        min_row = len(df)\n\n        for i in range(min_row):\n            projection = []\n            for j in range(len(column_name)):\n                projection.append(df.loc[i][column_name[j]])\n                if i == 0 and j == 0:\n                    result = df.loc[i][column_name[j]]\n            print(projection)\n\n        return result\n\n    catalog_name = \"edd_hive\"\n    def loc_meta_raw_helper(cur_date: str, args: str):\n        table_name = \"enb_base\"\n        query = f\"SELECT dt as p_dt FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by dt desc\"\n        r1 = fetchmany(query=query, size=5, column_name=[\"p_dt\"])\n        check_last_data(r1, cur_date, -2)\n\n    from aladin import trino\n    conn = trino.connect(catalog=catalog_name, schema=schema_name)\n    cursor = conn.cursor()\n    try:\n        loc_meta_raw_helper(cur_date, args)\n    except Exception as error:\n        print(\"-- Exception --\", error)\n        raise\n    # Close trino connection\n    conn.close()\n    print(\"Finish {}\".format(schema_name))\n    return \"ok\"\n\nbase_image = base_image()\napollo_op = create_component_from_func(apollo, base_image=base_image)\ncpm_op = create_component_from_func(cpm, base_image=base_image)\ndi_cpm_op = create_component_from_func(di_cpm, base_image=base_image)\nict_11st_11st_op = create_component_from_func(ict_11st_11st, base_image=base_image)\nict_skb_acc_op = create_component_from_func(ict_skb_acc, base_image=base_image)\nict_skt_common_op = create_component_from_func(ict_skt_common, base_image=base_image)\nict_tmm_tmap_op = create_component_from_func(ict_tmm_tmap, base_image=base_image)\nlitmus_op = create_component_from_func(litmus, base_image=base_image)\nloc_meta_raw_op = create_component_from_func(loc_meta_raw, base_image=base_image)\n\nimport kfp.dsl as dsl\n@dsl.pipeline(name='edd_monitor_pipeline', description='EDD data loading monitor')\ndef my_pipeline(cur_date: str, debug: bool, args: str):\n    print(\"my_pipeline: cur_date={}, debug={}, args={}\".format(cur_date, debug, args))\n    data_op = dsl.VolumeOp(name=\"data-pvc\",\n                           resource_name=\"data-volume\",\n                           generate_unique_name=False,\n                           action='apply',\n                           size=\"10Gi\",\n                           modes=dsl.VOLUME_MODE_RWO)\n    task_1 = apollo_op(cur_date=cur_date, debug=debug, args=args).add_pvolumes({\"/data\": data_op.volume})\n    task_2 = cpm_op(previous=task_1.output, cur_date=cur_date, debug=debug, args=args)\n    task_3 = di_cpm_op(task_2.output, cur_date=cur_date, debug=debug, args=args)\n    task_4 = ict_11st_11st_op(task_3.output, cur_date=cur_date, debug=debug, args=args)\n    task_5 = ict_skb_acc_op(task_4.output, cur_date=cur_date, debug=debug, args=args)\n    task_6 = ict_skt_common_op(task_5.output, cur_date=cur_date, debug=debug, args=args)\n    task_7 = ict_tmm_tmap_op(task_6.output, cur_date=cur_date, debug=debug, args=args)\n    task_8 = litmus_op(task_7.output, cur_date=cur_date, debug=debug, args=args)\n    task_9 = loc_meta_raw_op(task_8.output, cur_date=cur_date, debug=debug, args=args)\n\nif __name__ == \"__main__\":\n    kfp.compiler.Compiler(mode=kfp.dsl.PipelineExecutionMode.V1_LEGACY).compile(my_pipeline, \"./edd_monitor.yaml\")\n    client = kfp_client()\n    client.upload_pipeline(pipeline_package_path=\"./edd_monitor.yaml\", pipeline_name=\"edd_monitor\", description=\"EDD Monitor pipeline\")\n\n"
  },
  {
    "repo": "seoeun25/ml-ops",
    "file_path": "kubeflow_pipeline/samples/iris_python/iris_python_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/seoeun25/ml-ops/master/kubeflow_pipeline/samples/iris_python/iris_python_pipeline.py",
    "content": "\"\"\"\nThis is kubeflow pipeline built using python sdk.\nWe define a stand-alone python function which is converted to pipeline component.\n\n1. create persistent volume on Kubeflow Volumes UI\n    name: test-data-volume\n2. Execute below command inside kubeflow cluster.\n    $ python iris_python_pipeline.py\nThis will create iris_python pipeline and upload it. Check this on Kubeflow Pipeline UI.\n\"\"\"\n\nimport kfp.compiler\nfrom kfp.components import create_component_from_func\nimport kfp\n\ndef kfp_client():\n    \"\"\"\n    Kubeflow pipelines client inside cluster.\n    \"\"\"\n    ## TODO os.environ\n    end_point=\"http://ml-pipeline.kubeflow.svc.cluster.local:8888\"\n    credentials = kfp.auth.ServiceAccountTokenVolumeCredentials(path=None)\n    client = kfp.Client(host=end_point, credentials=credentials)\n\n    return client\n\ndef base_image() -> str:\n    import os\n    import re\n\n    iam = os.environ.get(\"AWS_ROLE_ARN\")\n    account = re.findall(\"arn:aws:iam::(.*):role*\", iam)[0]\n    region = os.environ.get(\"AWS_REGION\")\n    base_image = \"{}.dkr.ecr.{}.amazonaws.com/aladin-runtime:anaconda-cpu\".format(account, region)\n    print(\"base_image={}\".format(base_image))\n    return base_image\n\ndef load_data(id_from: int, id_to: int) -> str:\n    print(\"data_from={}, data_to={}\".format(id_from, id_to))\n    import os\n    import pandas as pd\n    from pathlib import Path\n\n    catalog_name = \"aidp_bigquery\"\n    schema_name = \"aladin\"\n    label_table_name = 'iris'\n    metadata_table_name = 'iris_metadata'\n    columns = ['Id', 'SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm', 'Species']\n\n    def load_helper(id_from: int, id_to: int):\n        # Fetch iris dataset from trino (federated query)\n        query = f\"SELECT t.id, m.sepallengthcm, m.sepalwidthcm, m.petallengthcm, m.petalwidthcm, t.species \\\n        FROM {catalog_name}.{schema_name}.{label_table_name} t \\\n        JOIN {catalog_name}.{schema_name}.{metadata_table_name} m on m.id = t.id \\\n        WHERE m.id >= {id_from} AND m.id <= {id_to}\"\n\n        cursor = conn.cursor()\n        cursor.execute(query)\n\n        # Convert data to a pandas dataframe\n        df = pd.DataFrame(cursor.fetchall(), columns=columns)\n\n        # Write iris dataset to csv file\n        df.to_csv(iris_csv_file, index=False)\n\n    from aladin import trino\n    conn = trino.connect(catalog=catalog_name, schema=schema_name)\n    id_from = int(os.environ.get(\"data_id_from\", '1'))\n    id_to = int(os.environ.get(\"data_id_to\", '150'))\n    iris_csv_file = os.environ.get(\"iris_csv_file\", '/data/dataset/iris.csv')\n    print(\"iris_csv_file(os,environ)=\", iris_csv_file)\n    print(\"iris_csv_file(default)=\", os.environ.get(\"iris_csv_file\", \"default_csv\"))\n\n    Path(iris_csv_file).parent.mkdir(parents=True, exist_ok=True)\n    load_helper(id_from, id_to)\n    # Close trino connection\n    conn.close()\n    print(\"hello load_data. saved=\", iris_csv_file)\n    return iris_csv_file\n\ndef train_model(iris_csv_file: str) -> str:\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import LabelEncoder\n    from sklearn.metrics import accuracy_score\n    import pickle\n    import pandas as pd\n    from pathlib import Path\n    import os\n    def read_helper(iris_csv_file):\n        iris = pd.read_csv(iris_csv_file, sep=',')\n        print(iris.shape)\n        return iris\n    def get_train_test_data_helper(iris):\n        encode = LabelEncoder()\n        iris.Species = encode.fit_transform(iris.Species)\n\n        train , test = train_test_split(iris, test_size=0.2, random_state=0)\n        print('shape of training data : ', train.shape)\n        print('shape of testing data', test.shape)\n\n        X_train = train.drop(columns=['Species'], axis=1)\n        y_train = train['Species']\n        X_test = test.drop(columns=['Species'], axis=1)\n        y_test = test['Species']\n\n        return X_train, X_test, y_train, y_test\n\n    iris_csv_file = '/data/dataset/iris.csv'\n    model_pickle = '/data/models/model.pkl'\n\n    iris_data = read_helper(iris_csv_file)\n    iris_data.drop(columns='Id', inplace=True)\n\n    X_train, X_test, y_train, y_test = get_train_test_data_helper(iris_data)\n\n    model = LogisticRegression(max_iter=5000)\n    model.fit(X_train, y_train)\n\n    # Evaluate\n    y_pred = model.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n\n    # Create metric file to check in UI\n    from aladin import metric\n\n    metric.update(key='accuracy-score', value=accuracy, percent=True)\n    metric.update(key=\"power\", value=0.5, percent=False)\n    metric.dump()\n    # Save trained model\n    Path(model_pickle).parent.mkdir(parents=True, exist_ok=True)\n    with open(model_pickle, 'wb') as file:\n        pickle.dump(model, file)\n\n    import numpy as np\n\n    with open(\"/data/models/model.pkl\", \"rb\") as file:\n        m = pickle.load(file)\n    req = pd.DataFrame(columns=[\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"], data= [[5.6, 1.0, 3.5, 1.7]])\n    iris_type = {\n        0: 'setosa',\n        1: 'versicolor',\n        2: 'virginica'\n    }\n\n    proba = m.predict_proba(req)\n    print(iris_type[np.argmax(proba)])\n    print(round(max(proba[0]),4))\n\n    return model_pickle\n\ndef upload_to_msp(model_file: str):\n    import subprocess\n    import os\n    import time\n\n    print(\"pwd = \", subprocess.check_output(\"pwd\", shell=True))\n    output_path=\"./aladin_msp\"\n    model_path = \"/data/models\"\n    command = \"aladin msp init -i {}\".format(model_path)\n    result = subprocess.check_output(command, shell=True)\n    print(\"aladin msp init = \", result)\n\n    command = \"aladin msp deploy -d aladin_msp -n 'iris-python'\"\n    result = subprocess.check_output(command, shell=True)\n    print(\"aladin msp deploy = \", result)\n    time.sleep(1)\n    print(output_path, os.listdir(output_path))\n    print(\"{}/code\".format(output_path), os.listdir(\"{}/code\".format(output_path)))\n\nbase_image = base_image()\nload_data_op = create_component_from_func(load_data, base_image=base_image)\ntrain_model_op = create_component_from_func(train_model, base_image=base_image)\nupload_to_msp_op = create_component_from_func(upload_to_msp, base_image=base_image)\n\nimport kfp.dsl as dsl\n@dsl.pipeline(name='iris-python-pipeline', description='An example pipeline using python function.')\ndef my_pipeline(id_from: int, id_to: int):\n    print(\"my_pipeline: id_from={}, id_to={}\".format(id_from, id_to))\n    data_op = dsl.VolumeOp(name=\"data-pvc\",\n                           resource_name=\"test-data-volume\",\n                           generate_unique_name=False,\n                           action='apply',\n                           size=\"2Gi\",\n                           modes=dsl.VOLUME_MODE_RWO)\n    task_1 = load_data_op(id_from, id_to).add_pvolumes({\"/data\": data_op.volume})\n    task_2 = train_model_op(task_1.output).add_pvolumes({\"/data\": data_op.volume})\n    task_3 = upload_to_msp_op(task_2.output).add_pvolumes({\"/data\": data_op.volume})\n\nif __name__ == \"__main__\":\n    kfp.compiler.Compiler(mode=kfp.dsl.PipelineExecutionMode.V1_LEGACY).compile(my_pipeline, \"./iris_python.yaml\")\n    client = kfp_client()\n    client.upload_pipeline(pipeline_package_path=\"./iris_python.yaml\", pipeline_name=\"iris_python\", description=\"Iris python function based pipeline\")\n\n"
  },
  {
    "repo": "seoeun25/ml-ops",
    "file_path": "kubeflow_pipeline/samples/test/kfp_op_test.py",
    "raw_url": "https://raw.githubusercontent.com/seoeun25/ml-ops/master/kubeflow_pipeline/samples/test/kfp_op_test.py",
    "content": "\"\"\"\n$ python iris_python_pipeline.py\n# This will upload iris_python.yaml. Check this on kubeflow-pipeline-ui\n\"\"\"\n\nimport kfp.compiler\nfrom kfp.components import create_component_from_func\nimport kfp\n\ndef kfp_client():\n    \"\"\"\n    Kubeflow pipelines client inside cluster.\n    \"\"\"\n    ## TODO\n    end_point=\"http://ml-pipeline.kubeflow.svc.cluster.local:8888\"\n    credentials = kfp.auth.ServiceAccountTokenVolumeCredentials(path=None)\n    client = kfp.Client(host=end_point, credentials=credentials)\n\n    return client\ndef base_image() -> str:\n    import os\n    import re\n\n    iam = os.environ.get(\"AWS_ROLE_ARN\")\n    account = re.findall(\"arn:aws:iam::(.*):role*\", iam)[0]\n    region = os.environ.get(\"AWS_REGION\")\n    base_image = \"{}.dkr.ecr.{}.amazonaws.com/aladin-runtime:anaconda-cpu\".format(account, region)\n    print(\"base_image={}\".format(base_image))\n    return base_image\ndef volume_test(model_file: str):\n    import subprocess\n    import os\n\n    print(\"pwd = \", subprocess.check_output(\"pwd\", shell=True))\n\n    # list file and directories\n    data_path=\"/data\"\n    print(\"dir={} :: \".format(data_path), os.listdir(data_path))\n\ndef delete_file(file:str):\n    import subprocess\n    import os\n\n    print(\"pwd = \", subprocess.check_output(\"pwd\", shell=True))\n\n    # list file and directories\n    data_path=\"/data\"\n    print(\"dir={} :: \".format(data_path), os.listdir(data_path))\n\nbase_image = base_image()\nvolume_test_op = create_component_from_func(volume_test, base_image=base_image)\n\nimport kfp.dsl as dsl\n@dsl.pipeline(name='my-pipeline', description='An example pipeline')\ndef my_pipeline(id_from: int, id_to: int):\n    print(\"my_pipeline: id_from={}, id_to={}\".format(id_from, id_to))\n    data_op = dsl.VolumeOp(name=\"data-pvc\",\n                           resource_name=\"data-volume\",\n                           generate_unique_name=False,\n                           action='apply',\n                           size=\"2Gi\",\n                           modes=dsl.VOLUME_MODE_RWO)\n    task_1 = volume_test_op(\"models\").add_pvolumes({\"/data\": data_op.volume})\n\narguments = {'id_from': '7', 'id_to': '8'}\nif __name__ == \"__main__\":\n    client = kfp_client()\n    client.create_run_from_pipeline_func(my_pipeline, experiment_name=\"test-seoeun\", arguments=arguments)\n\n\n"
  },
  {
    "repo": "tryster7/FuelKFP",
    "file_path": "pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/tryster7/FuelKFP/master/pipeline.py",
    "content": "# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nKubeflow Pipelines MNIST example\n\nRun this script to compile pipeline\n\"\"\"\n\nimport kfp.dsl as dsl\nimport kfp.gcp as gcp\nimport kfp.onprem as onprem\n\nplatform = 'GCP'\n\n@dsl.pipeline(\n  name='Fuel',\n  description='Fuel Prediction pipeline.'\n)\ndef fuel_pipeline(bucket_name='gs://your-bucket/export',\n                   input_file='folder/file',\n                   output_folder='output folder',\n                   epochs = 10):\n  preprocess= dsl.ContainerOp(\n      name='preprocess',\n      image='gcr.io/kb-poc-262417/fuel:latest',\n      arguments=[\n          '--input_file', input_file,\n          '--output_folder', output_folder,\n          '--bucket_name', bucket_name\n          ]\n  )\n\n  train= dsl.ContainerOp(\n      name='train',\n      image='gcr.io/kb-poc-262417/fuel/train:latest',\n      arguments=[\n          '--bucket_name', bucket_name,\n          '--epochs', epochs\n          ]\n  )\n  train.after(preprocess)\n  \n  serve= dsl.ContainerOp(\n      name='serve',\n      image='gcr.io/kb-poc-262417/fuel/serve:latest',\n      arguments=[\n          '--bucket_name',bucket_name\n          ]\n  )\n\n  serve.after(train)\n\n  steps = [preprocess, train, serve]\n  for step in steps:\n    if platform == 'GCP':\n      step.apply(gcp.use_gcp_secret('user-gcp-sa'))\n    else:\n      step.apply(onprem.mount_pvc(pvc_name, 'local-storage', '/mnt'))\n\nif __name__ == '__main__':\n  import kfp.compiler as compiler\n  compiler.Compiler().compile(fuel_pipeline, __file__ + '.tar.gz')\n"
  },
  {
    "repo": "simanadler/kfp-components",
    "file_path": "samples/house_price_estimates/pipeline-argo.py",
    "raw_url": "https://raw.githubusercontent.com/simanadler/kfp-components/master/samples/house_price_estimates/pipeline-argo.py",
    "content": "import kfp.dsl as dsl\nimport kfp.components as components\n\n@dsl.pipeline(\n    name = \"Fybrik housing price estimate pipeline\",\n    description = \"Pipeline that provides data policy governed access to cataloged data, analyses data, trains model, and writes the results and catalogs them\"\n)\n\ndef get_current_namespace():\n    import kubernetes\n    import os\n\n    ns_path = \"/var/run/secrets/kubernetes.io/serviceaccount/namespace\"\n    if os.path.exists(ns_path):\n        with open(ns_path) as f:\n            return f.read().strip()\n    try:\n        _, active_context = kubernetes.config.list_kube_config_contexts()\n        return active_context[\"context\"][\"namespace\"]\n    except KeyError:\n        return \"kubeflow\"\n\ndef houseprice_pipeline(\n    test_dataset_id: str,\n    train_dataset_id: str\n):\n    # Get the namespace in which kfp is running\n    namespace = get_current_namespace()\n\n    # Get the ID of the run.  Make sure it's lower case and starts with a letter for our use\n    run_name = \"run-\" + dsl.RUN_ID_PLACEHOLDER.lower()\n\n    # Where to store parameters passed between workflow steps\n    result_name = \"submission-\" + str(run_name)\n\n    getDataEndpointsOp = components.load_component_from_file('../../get_data_endpoints/component.yaml')\n    getDataEndpointsStep = getDataEndpointsOp(train_dataset_id=train_dataset_id, test_dataset_id=test_dataset_id, namespace=namespace, run_name=run_name, result_name=result_name)\n    \n    visualizeTableOp = components.load_component_from_file('./visualize_table/component.yaml')\n    visualizeTableStep = visualizeTableOp(train_endpoint='%s'% getDataEndpointsStep.outputs['train_endpoint'], train_dataset_id=train_dataset_id, namespace=namespace)\n    visualizeTableStep.after(getDataEndpointsStep)\n\n    trainModelOp = components.load_component_from_file('./train_model/component.yaml')\n    trainModelStep = trainModelOp(train_endpoint_path='%s' % getDataEndpointsStep.outputs['train_endpoint'],\n                                test_endpoint_path='%s' % getDataEndpointsStep.outputs['test_endpoint'],\n                                result_name=result_name,\n                                result_endpoint_path='%s' % getDataEndpointsStep.outputs['result_endpoint'],\n                                train_dataset_id=train_dataset_id,\n                                test_dataset_id=test_dataset_id,\n                                namespace=namespace)\n    trainModelStep.after(visualizeTableStep)\n\n    sumbitResultOp = components.load_component_from_file('./submit_result/component.yaml')\n    submitResultStep = sumbitResultOp(getDataEndpointsStep.outputs['result_catalogid'])\n    submitResultStep.after(trainModelStep)\n \nif __name__ == '__main__':\n\n    # Set environment values to ensure persistent volumes used to pass parameters are allocated successfully \n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--test_dataset_id', type=str)\n    parser.add_argument('--train_dataset_id', type=str)\n \n    args = parser.parse_args()\n    import kfp.compiler as compiler\n    compiler.Compiler().compile(pipeline_func=houseprice_pipeline, package_path=__file__.replace('.py', '.yaml'))"
  },
  {
    "repo": "simanadler/kfp-components",
    "file_path": "samples/house_price_estimates/pipeline-tekton.py",
    "raw_url": "https://raw.githubusercontent.com/simanadler/kfp-components/master/samples/house_price_estimates/pipeline-tekton.py",
    "content": "import kfp.dsl as dsl\nimport kfp.components as components\n\n@dsl.pipeline(\n    name = \"Fybrik housing price estimate pipeline\",\n    description = \"Pipeline that provides data policy governed access to cataloged data, analyses data, trains model, and writes the results and catalogs them\"\n)\n\ndef get_current_namespace():\n    import kubernetes\n    import os\n\n    ns_path = \"/var/run/secrets/kubernetes.io/serviceaccount/namespace\"\n    if os.path.exists(ns_path):\n        with open(ns_path) as f:\n            return f.read().strip()\n    try:\n        _, active_context = kubernetes.config.list_kube_config_contexts()\n        return active_context[\"context\"][\"namespace\"]\n    except KeyError:\n        return \"kubeflow\"\n\ndef houseprice_pipeline(\n    test_dataset_id: str,\n    train_dataset_id: str\n):\n    # Get the namespace in which kfp is running\n    namespace = get_current_namespace()\n\n    # Get the ID of the run.  Make sure it's lower case and starts with a letter for our use\n    run_name = \"run-\" + dsl.RUN_ID_PLACEHOLDER.lower()\n\n    # Where to store parameters passed between workflow steps\n    result_name = \"submission-\" + str(run_name)\n\n    getDataEndpointsOp = components.load_component_from_file('../../get_data_endpoints/component.yaml')\n    getDataEndpointsStep = getDataEndpointsOp(train_dataset_id=train_dataset_id, test_dataset_id=test_dataset_id, namespace=namespace, run_name=run_name, result_name=result_name)\n    \n    visualizeTableOp = components.load_component_from_file('./visualize_table/component.yaml')\n    visualizeTableStep = visualizeTableOp(train_endpoint='%s'% getDataEndpointsStep.outputs['train_endpoint'], train_dataset_id=train_dataset_id, namespace=namespace)\n    visualizeTableStep.after(getDataEndpointsStep)\n\n    trainModelOp = components.load_component_from_file('./train_model/component.yaml')\n    trainModelStep = trainModelOp(train_endpoint_path='%s' % getDataEndpointsStep.outputs['train_endpoint'],\n                                test_endpoint_path='%s' % getDataEndpointsStep.outputs['test_endpoint'],\n                                result_name=result_name,\n                                result_endpoint_path='%s' % getDataEndpointsStep.outputs['result_endpoint'],\n                                train_dataset_id=train_dataset_id,\n                                test_dataset_id=test_dataset_id,\n                                namespace=namespace)\n    trainModelStep.after(visualizeTableStep)\n\n    sumbitResultOp = components.load_component_from_file('./submit_result/component.yaml')\n    submitResultStep = sumbitResultOp(getDataEndpointsStep.outputs['result_catalogid'])\n    submitResultStep.after(trainModelStep)\n \nif __name__ == '__main__':\n\n    # Set environment values to ensure persistent volumes used to pass parameters are allocated successfully \n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--test_dataset_id', type=str)\n    parser.add_argument('--train_dataset_id', type=str)\n\n    args = parser.parse_args()\n    from kfp_tekton.compiler import TektonCompiler\n \n    TektonCompiler().compile(houseprice_pipeline, __file__.replace('.py', '.yaml'))"
  },
  {
    "repo": "cliffvj/sparta-kubeflow",
    "file_path": "boston_housing/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/cliffvj/sparta-kubeflow/main/boston_housing/pipeline.py",
    "content": "import kfp\r\nfrom kfp import dsl\r\n\r\ndef preprocess_op():\r\n\r\n    return dsl.ContainerOp(\r\n        name='Preprocess Data',\r\n        image='cliffvj/boston_pipeline_preprocessing:latest',\r\n        arguments=[],\r\n        file_outputs={\r\n            'x_train': '/app/x_train.npy',\r\n            'x_test': '/app/x_test.npy',\r\n            'y_train': '/app/y_train.npy',\r\n            'y_test': '/app/y_test.npy',\r\n        }\r\n    )\r\n\r\ndef train_op(x_train, y_train):\r\n\r\n    return dsl.ContainerOp(\r\n        name='Train Model',\r\n        image='cliffvj/boston_pipeline_train:latest',\r\n        arguments=[\r\n            '--x_train', x_train,\r\n            '--y_train', y_train\r\n        ],\r\n        file_outputs={\r\n            'model': '/app/model.pkl'\r\n        }\r\n    )\r\n\r\ndef test_op(x_test, y_test, model):\r\n\r\n    return dsl.ContainerOp(\r\n        name='Test Model',\r\n        image='cliffvj/boston_pipeline_test:latest',\r\n        arguments=[\r\n            '--x_test', x_test,\r\n            '--y_test', y_test,\r\n            '--model', model\r\n        ],\r\n        file_outputs={\r\n            'mean_squared_error': '/app/output.txt'\r\n        }\r\n    )\r\n\r\ndef deploy_model_op(model):\r\n\r\n    return dsl.ContainerOp(\r\n        name='Deploy Model',\r\n        image='cliffvj/boston_pipeline_deploy_model:latest',\r\n        arguments=[\r\n            '--model', model\r\n        ]\r\n    )\r\n\r\n@dsl.pipeline(\r\n   name='Boston Housing Pipeline',\r\n   description='An example pipeline that trains and logs a regression model.'\r\n)\r\ndef boston_pipeline():\r\n    _preprocess_op = preprocess_op()\r\n    \r\n    _train_op = train_op(\r\n        dsl.InputArgumentPath(_preprocess_op.outputs['x_train']),\r\n        dsl.InputArgumentPath(_preprocess_op.outputs['y_train'])\r\n    ).after(_preprocess_op)\r\n\r\n    _test_op = test_op(\r\n        dsl.InputArgumentPath(_preprocess_op.outputs['x_test']),\r\n        dsl.InputArgumentPath(_preprocess_op.outputs['y_test']),\r\n        dsl.InputArgumentPath(_train_op.outputs['model'])\r\n    ).after(_train_op)\r\n\r\n    deploy_model_op(\r\n        dsl.InputArgumentPath(_train_op.outputs['model'])\r\n    ).after(_test_op)\r\n\r\nclient = kfp.Client()\r\nclient.create_run_from_pipeline_func(boston_pipeline, arguments={})"
  },
  {
    "repo": "deinal/jec-pipeline",
    "file_path": "pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/deinal/jec-pipeline/master/pipeline.py",
    "content": "import argparse\nimport uuid\nimport kfp\nfrom kfp import dsl\nimport http\nimport yaml\nimport json\n\n\ndef load_cookies(cookie_file, domain):\n    cookiejar = http.cookiejar.MozillaCookieJar(cookie_file)\n    cookiejar.load()\n    for cookie in cookiejar:\n        if cookie.domain == domain:\n            cookies = f'{cookie.name}={cookie.value}'\n            break\n    return cookies\n\ndef get_pipeline(name, description):\n    @dsl.pipeline(name=name, description=description)\n    def pipeline(\n        run_id: str,\n        s3_bucket: str,\n        data_train: str,\n        data_val: str,\n        data_test: str,\n        data_config: str,\n        network_config: str,\n        num_replicas: int,\n        num_gpus: int,\n        num_cpus: int,\n        memory: str,\n        delete_train_experiment: bool,\n        delete_export_job: bool,\n    ):\n\n        train = train_op(\n            id=run_id,\n            s3_bucket=s3_bucket,\n            num_replicas=num_replicas,\n            num_gpus=num_gpus,\n            num_cpus=num_cpus,\n            memory=memory,\n            data_train=data_train,\n            data_val=data_val,\n            data_test=data_test,\n            data_config=data_config,\n            network_config=network_config,\n            delete_experiment=delete_train_experiment,\n        )\n\n        export = export_op(\n            id=run_id,\n            s3_bucket=s3_bucket,\n            data_config=data_config,\n            network_config=network_config,\n            delete_job=delete_export_job,\n            pt_path=train.outputs['optimal_model_path'],\n            network_option=train.outputs['network_option'],\n        )\n\n        serve = serve_op(\n            model_name=run_id,\n            model_path=export.outputs['model_path'],\n        )\n\n    return pipeline\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='Pipeline Params')\n    parser.add_argument('--namespace', type=str, default='dholmber', \n                        help='Kubeflow namespace to run pipeline in')\n    parser.add_argument('--experiment-name', type=str, default='jec-experiment', \n                        help='name for KFP experiment on Kubeflow')\n    parser.add_argument('--num-replicas', type=int, default=1,\n                        help='number of nodes to train on')\n    parser.add_argument('--num-gpus', type=int, default=1,\n                        help='number of gpus per node, limit is 1')\n    parser.add_argument('--num-cpus', type=int, default=1, \n                        help='number of cpus to use')\n    parser.add_argument('--memory', type=str, default='12Gi', \n                        help='memory in gigabyte')                    \n    parser.add_argument('--data-config', type=str, default='data/jec_pfn_open.yaml', \n                        help='data configuration yaml file')\n    parser.add_argument('--network-config', type=str, default='networks/pfn_regressor_open.py', \n                        help='network architecture configuration file')\n    parser.add_argument('--s3-bucket', type=str, default='s3://jec-data', \n                        help='s3 bucket used by the pipeline for storing models and tensorboard log dirs')\n    parser.add_argument('--data-train', type=str, default='s3://jec-data/open/train/*.root',\n                        help='training data')\n    parser.add_argument('--data-val', type=str, default='s3://jec-data/open/val/*.root',\n                        help='validation data')\n    parser.add_argument('--data-test', type=str, default='s3://jec-data/open/test/*.root',\n                        help='test data')\n    parser.add_argument('--delete-train-experiment', action='store_true', default=False,\n                        help='whether or not to delete the hp tuning experiment once finished')\n    parser.add_argument('--delete-export-job', action='store_true', default=False,\n                        help='whether or not to delete the export job once finished')\n    args = parser.parse_args()\n\n    # Define pipeline variables\n    description = 'Jet Energy Corrections Pipeline'\n    network = args.network_config.split(\"/\")[-1].split(\".py\")[0].replace('_', '-')\n    run_id = f'{network}-{uuid.uuid4().hex[:6]}'\n    pipeline_name = f'jec-pipeline-{run_id}'\n    package_path = f'packages/{pipeline_name}.tar.gz'\n\n    # Import pipeline components\n    train_op = kfp.components.load_component_from_file('training/component.yaml')\n    export_op = kfp.components.load_component_from_file('exporting/component.yaml')\n    serve_op = kfp.components.load_component_from_file('serving/component.yaml')\n\n    # Get pipeline instance\n    pipeline = get_pipeline(pipeline_name, description)\n\n    # Compile pipeline\n    kfp.compiler.Compiler().compile(pipeline_func=pipeline, package_path=package_path)\n\n    # Load cookies to access Kubeflow at CERN\n    cookies = load_cookies(cookie_file='cookies.txt', domain='ml.cern.ch')\n    \n    # Load Kubeflow pipeline client\n    client = kfp.Client(host='https://ml.cern.ch/pipeline', cookies=cookies)\n\n    # Upload pipeline \n    client.upload_pipeline(pipeline_package_path=package_path, pipeline_name=pipeline_name, description=description)\n\n    # Create KFP experiment\n    experiment = client.create_experiment(name=args.experiment_name, namespace=args.namespace)\n\n    # Run pipeline\n    run = client.run_pipeline(\n        pipeline_package_path=package_path,\n        experiment_id=experiment.id,\n        job_name=f'run-{run_id}',\n        params={\n            'run_id': run_id,\n            's3_bucket': args.s3_bucket,\n            'data_train': args.data_train,\n            'data_val': args.data_val,\n            'data_test': args.data_test,\n            'data_config': args.data_config,\n            'network_config': args.network_config,\n            'num_replicas': args.num_replicas,\n            'num_gpus': args.num_gpus,\n            'num_cpus': args.num_cpus,\n            'memory': args.memory,\n            'delete_train_experiment': args.delete_train_experiment,\n            'delete_export_job': args.delete_export_job,\n        }\n    )\n\n    print('Deployed', pipeline_name)\n"
  },
  {
    "repo": "sol-demos-01/kubeflow-ml1-demo-app",
    "file_path": "components/golang/component.py",
    "raw_url": "https://raw.githubusercontent.com/sol-demos-01/kubeflow-ml1-demo-app/master/components/golang/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef processing_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('GOLANG_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=[script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n"
  },
  {
    "repo": "sol-demos-01/kubeflow-ml1-demo-app",
    "file_path": "components/training/component.py",
    "raw_url": "https://raw.githubusercontent.com/sol-demos-01/kubeflow-ml1-demo-app/master/components/training/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef training_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('TRAINING_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=['/usr/local/bin/python', script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n\n"
  },
  {
    "repo": "ajperry2/mapillary_pipeline",
    "file_path": "mapillary_pipeline/download_data_components.py",
    "raw_url": "https://raw.githubusercontent.com/ajperry2/mapillary_pipeline/main/mapillary_pipeline/download_data_components.py",
    "content": "from kfp.dsl import component, Output, Dataset\n\n\n@component()\ndef download_data(\n    url: str,\n    dataset: Output[Dataset]\n):\n    import urllib.request\n    import zipfile\n    import os\n    from pathlib import Path\n    dataset_path = Path(dataset.path)\n    dataset_path.mkdir(parents=True, exist_ok=True)\n    urllib.request.urlretrieve(url, f\"{dataset_path}/data.zip\")\n    with zipfile.ZipFile(dataset_path / \"data.zip\", 'r') as zip_ref:\n        zip_ref.extractall(dataset_path)\n    os.remove(dataset_path / \"data.zip\")\n\n"
  },
  {
    "repo": "ajperry2/mapillary_pipeline",
    "file_path": "mapillary_pipeline/download_data_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/ajperry2/mapillary_pipeline/main/mapillary_pipeline/download_data_pipeline.py",
    "content": "from kfp import dsl, kubernetes\n\nfrom .download_data_components import download_data\n\n\n@dsl.pipeline\ndef pipeline_func(url: str) -> None:\n\n    task = download_data(url=url).set_caching_options(enable_caching=False).ignore_upstream_failure()"
  },
  {
    "repo": "stackdemos/kf-overlay-acm",
    "file_path": "components/golang/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/kf-overlay-acm/master/components/golang/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef processing_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('GOLANG_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=[script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n"
  },
  {
    "repo": "stackdemos/kf-overlay-acm",
    "file_path": "components/training/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/kf-overlay-acm/master/components/training/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef training_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('TRAINING_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=['/usr/local/bin/python', script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n\n"
  },
  {
    "repo": "Shunpoco/kfp-sample",
    "file_path": "pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/Shunpoco/kfp-sample/main/pipeline.py",
    "content": "from types import FunctionType\n\nfrom kfp.compiler import Compiler\nfrom kfp import dsl\nimport kfp.components as comp\n\n\n@dsl.pipeline(\n    name=\"Test\",\n    description=\"test\",\n)\ndef pipeline():\n    c:FunctionType = comp.load_component_from_file(\"./components/task1/component.yaml\")\n\n    task1 = c(\n        input_path=\"hogehoge/fugafuga\",\n    )\n\n    c(\n        input_path=task1.outputs[\"output_path\"],\n    )\n\ndef main():\n    Compiler().compile(\n        pipeline_func=pipeline,\n        package_path=\"./pipeline.yaml\",\n    )\n\nif __name__ == \"__main__\":\n    main()\n"
  },
  {
    "repo": "kshokn2/kfp_VertexAI",
    "file_path": "run_kfp_vertexAI.py",
    "raw_url": "https://raw.githubusercontent.com/kshokn2/kfp_VertexAI/master/run_kfp_vertexAI.py",
    "content": "import os\nimport kfp\nfrom kfp import dsl, compiler\nfrom kfp.dsl import (Artifact, Dataset, Input, Output, Model, Metrics, Markdown, HTML, component, InputPath, OutputPath, PipelineTaskFinalStatus)\n\nfrom google_cloud_pipeline_components.types.artifact_types import VertexEndpoint\nfrom google_cloud_pipeline_components.v1.endpoint import EndpointCreateOp\nfrom google_cloud_pipeline_components.v1.custom_job import utils\nfrom google_cloud_pipeline_components.v1 import hyperparameter_tuning_job\nfrom google_cloud_pipeline_components.v1 hyperparameter_tuning_job import HyperparameterTuningJobRunOp\n\nfrom google.cloud import aiplatform\n\nmyparam1 = os.getenv(\"param1\")\n\n# \ub2e8\uc704 \ud14c\uc2a4\ud2b8\uc6a9\n# myparam1 = \"parameter1\"\n\nPROJECT_ID = \"my-project-id\"\nPROJECT_NUM = \"1234567890\"\nREGION = \"my-region\"\nPIPELINEJOB_SA = \"my-service-account@~~~~~\"\nPRIVATE_EP_VPC = \"my-vpc-name\"\n\nPIPELINE_ROOT = \"gs://my-mlops-bucket\"\nPIPELINE_NAME = \"my-pipeline-name\"\n\nBASE_IMAGE = \"my-docker-kfp_2_7_0-image\" # kfp 2.7.0 \uc124\uce58\ub41c \uae30\ubcf8 \ub3c4\ucee4 \uc774\ubbf8\uc9c0\n\ndef main():\n\n    @component(\n        base_image=BASE_IMAGE,\n        # packages_to_install=[\"install_library1==0.0.1\", \"install_library2==1.0.0\"]\n    )\n    def download_data(\n        # project: str,\n        # location: str,\n        bukcet_name: str,\n        download_path: OutputPath(\"Any\"),\n    ):\n        import os\n        import sys\n        import logging\n\n        # from google.cloud import storage\n        from google.cloud import logging as gc_logging\n\n        def download_bucket_with_transfer_manager(\n            bucket_name, blob_name_prefix, target_list=None, destination_directory=\"\", workers=8\n        ):\n            # https://cloud.google.com/storage/docs/samples/storage-transfer-manager-download-bucket?hl=ko\n\n            import os\n            import time\n            from google.cloud.storage import Client, transfer_manager\n\n            storage_client = Client()\n            bucket = storage_client.bucket(bucket_name)\n\n            if target_list is None:\n                blob_names = [blob.name.replace(blob_name_prefix, '') for blob in bucket.list_blobs(prefix=blob_name_prefix) if blob.name != blob_name_prefix]\n            else:\n                blob_names = target_list\n\n            results = transfer_manager.download_many_to_path(\n                bucket, blob_names, destination_directory=destination_directory, blob_name_prefix=blob_name_prefix, max_workers=workers)\n            \n            retry_blobs = []\n\n            for name, result in zip(blob_names, results):\n                if isinstance(result, Exception):\n                    # print(\"Failed to download {} due to exception: {}\".format(name, result))\n                    retry_blobs.append(name)\n                # else:\n                #     print(\"Downloaded {} to {}.\".format(name, destination_directory + name))\n\n            if len(retry_blobs) != 0:\n                retry_results = transfer_manager.download_many_to_path(\n                    bucket, retry_blobs, destination_directory=destination_directory, blob_name_prefix=blob_name_prefix, max_workers=workers)\n                \n                for name, result in zip(retry_blobs, retry_results):\n                    if isinstance(result, Exception):\n                        logging.error(\"Failed to download {} due to exception: {}\".format(name, result))\n                        os.remove(f'{destination_directory}/{name}')\n                        time.sleep(0.05)\n\n        workers = int(os.cpu_count()/2) if os.cpu_count() > 61 else os.cpu_count()\n        os.makedirs(download_path, exist_ok=True)\n\n        download_bucket_with_transfer_manager(\n            bucket_name=bukcet_name,\n            blob_name_prefix=f'my_blob/',\n            destination_directory=download_path,\n            workers=workers\n        )\n\n        if len(os.listdir(download_path)) == 0:\n            logging.error(\"download_data component failed..\")\n            sys.exit(1)\n\n        logging.info(\"download_data component completed..\")\n\n\n    @component(\n        base_image=BASE_IMAGE,\n        # packages_to_install=[\"install_library1==0.0.1\", \"install_library2==1.0.0\"]\n    )\n    def data_preparation(\n        # project: str,\n        # location: str,\n        rand_seed: int,\n        ratio: float,\n        batch_size: int,\n        label_files: str,\n        download_path: InputPath(\"Any\"),\n        trainset_path: OutputPath(\"Any\"),\n        validset_path: OutputPath(\"Any\"),\n        testset_path: OutputPath(\"Any\"),\n    ):\n        import os\n        import sys\n        import random\n        import numpy as np\n        from sklearn.model_selection import train_test_split\n        import tensorflow as tf\n        import logging\n\n        from google.cloud import storage\n        from google.cloud import logging as gc_logging\n\n        random.seed(rand_seed)\n\n        file_list = os.listdir(download_path)\n        \"\"\"\n        label \uc815\ubcf4 \uac00\uc838\uc624\uae30\n        \"\"\"\n\n        array_data = np.array([np.load(os.path.join(download_path, filenm+\".npy\")) for filenm in file_list])\n        array_class = label_files#\ub85c \ub9cc\ub4e4\uae30\n\n        # split dataset\n        train_data, temp_data, train_labels, temp_labels = train_test_split(\n            array_data, array_class, test_size=1-ratio, random_state=rand_seed, stratify=array_class)\n        \n        rest_ratio = 0.5\n        valid_data, test_data, valid_labels, test_labels = train_test_split(\n            temp_data, temp_labels, test_size=rest_ratio, random_state=rand_seed, stratify=temp_labels)\n        \n        # tf.data.Dataset \uc0dd\uc131\n        train_dataset = tf.data.Dataset.from_tensor_slices((train_data, train_labels))\n        train_dataset = train_dataset.shuffle(00).batch(batch_size)\n        \n        valid_dataset = tf.data.Dataset.from_tensor_slices((valid_data, valid_labels))\n        valid_dataset = valid_dataset.shuffle(00).batch(batch_size)\n        \n        test_dataset = tf.data.Dataset.from_tensor_slices((test_data, test_labels))\n        test_dataset = test_dataset.shuffle(00).batch(batch_size)\n\n        # artifacts\uc758 metadata\uc5d0 \uc800\uc7a5\n        train_dataset.save(trainset_path)\n        valid_dataset.save(validset_path)\n        test_dataset.save(testset_path)\n\n        logging.info(\"data_preparation component completed..\")\n\n\n    @component(\n        base_image=BASE_IMAGE,\n    )\n    def worker_pool_spec(\n        hpt_epochs: int,\n        hpt_image: str,\n        myparam1: str,\n    ) -> list:\n        CMDARGS = [\n            \"--epochs\", str(hpt_epochs),\n            \"--param1\", myparam1,\n        ]\n\n        worker_pool_spec = [\n            {\n                \"machine_spec\": {\n                    # gpu\n                    \"machine_type\": \"my-gcp-machine\",\n                    \"accelerator_type\": \"my-gcp-accelerator\",\n                    \"accelerator_count\": 1,\n                },\n                \"replica_count\": 1,\n                \"container_spec\": {\"image_uri\": hpt_image, \"args\": CMDARGS},\n            }\n        ]\n\n        logging.info(\"worker_pool_spec component completed..\")\n\n        return worker_pool_spec\n\n\n    @component(\n        base_image=BASE_IMAGE,\n        # packages_to_install=[\"google-cloud-aiplatform\", \"google-cloud-pipeline-components\", \"protobuf\"],\n    )\n    def GetBestTrialOp(\n        gcp_resources: str,\n        study_spec_metrics: list,\n    ) -> str:\n        import sys\n        import logging\n\n        from google.cloud import aiplatform\n        from google_cloud_pipeline_components.proto.gcp_resources_pb2 import GcpResources\n        from google.protobuf.json_format import Parse\n        from google.cloud.aiplatform_v1.types import study\n        from google.cloud import logging as gc_logging\n\n        api_endpoint_suffix = '-aiplatform.googleapis.com'\n        gcp_resources_proto = Parse(gcp_resources, GcpResources())\n        gcp_resources_split = gcp_resources_proto.resources[0].resource_uri.partition('projects')\n        resource_name = gcp_resources_split[1] + gcp_resources_split[2]\n        prefix_str = gcp_resources_split[0]\n        prefix_str = prefix_str[:prefix_str.find(api_endpoint_suffix)]\n        api_endpoint = prefix_str[(prefix_str.rfind('//') + 2):] + api_endpoint_suffix\n\n        client_options = {'api_endpoint': api_endpoint}\n        job_client = aiplatform.gapic.JobServiceClient(client_options=client_options)\n        response = job_client.get_hyperparameter_tuning_job(name=resource_name)\n        \n        trials = [study.Trial.to_json(trial) for trial in response.trials]\n\n        if len(study_spec_metrics) > 1:\n            raise RuntimeError('Unable to determine best parameters for multi-objective hyperparameter tuning.')\n            logging.error(\"Unable to determine best parameters for multi-objective hyperparameter tuning.\")\n            sys.exit(1)\n        trials_list = [study.Trial.from_json(trial) for trial in trials]\n        best_trial = None\n        goal = study_spec_metrics[0]['goal']\n        best_fn = None\n        if goal == study.StudySpec.MetricSpec.GoalType.MAXIMIZE:\n            best_fn = max\n        elif goal == study.StudySpec.MetricSpec.GoalType.MINIMIZE:\n            best_fn = min\n        best_trial = best_fn(\n            trials_list, key=lambda trial: trial.final_measurement.metrics[0].value)\n        \n        logging.info(\"GetBestTrialOp component completed..\")\n\n        return study.Trial.to_json(best_trial)\n\n\n    @component(\n        base_image=BASE_IMAGE,\n    )\n    def GetHyperparametersOp(\n        trial: str,\n    ) -> list:\n        from google.cloud.aiplatform_v1.types import study\n\n        trial_proto = study.Trial.from_json(trial)\n\n        return [ study.Trial.Parameter.to_json(param) for param in trial_proto.parameters ]\n    \n\n    @component(\n        base_image=BASE_IMAGE,\n    )\n    def training_model(\n        rand_seed: int,\n        training_params: dict,\n        best_params: list,\n        tensorboard_root: str,\n        dataset_train: InputPath(\"Any\"),\n        dataset_valid: InputPath(\"Any\"),\n        dataset_test: InputPath(\"Any\"),\n        model: Output[Model],\n    ):\n        import os\n        import sys\n        import json\n        import random\n        import tensorflow as tf\n        from tensorflow import keras\n        from tensorflow.keras import layers\n        from sklearn.utils import class_weight\n\n        from google.cloud import logging as gc_logging\n\n        import logging\n\n        random.seed(rand_seed)\n        learning_rate = json.loads(best_params[0])[\"value\"] # format(parma 1\uac1c\uc77c \ub54c): [{\"parameterId\": \"learning_rate\", \"value\": 0.00123}]\n\n        train_dataset = tf.data.Dataset.load(dataset_train)\n        valid_dataset = tf.data.Dataset.load(dataset_valid)\n        test_dataset = tf.data.Dataset.load(dataset_test)\n\n        os.environ['AIP_TENSORBOARD_LOG_DIR'] = tensorboard_root\n        gpus = tf.config.experimental.list_physical_devices('GPU')\n\n        # \ubaa8\ub378 \uc815\uc758\n        if len(gpus) > 1:\n            strategy = tf.distribute.MirroredStrategy()\n            with strategy.scope():\n                new_model = create_model()\n        else:\n            new_model = create_model()\n\n        # \ubaa8\ub378 \ucef4\ud30c\uc77c\n        new_model.complie(\n            loss = loss\uc124\uc815,\n            optimizer = opt\uc124\uc815(learning_rate=learning_rate),\n            metrics = [\uba54\ud2b8\ub9ad1, \uba54\ud2b8\ub9ad2]\n        )\n\n        # \ubaa8\ub378 \ud559\uc2b5\n        new_model.fit(\n            train_dataset,\n            validation_data=valid_dataset,\n            epochs = training_params[\"epochs\"],\n            verbose = 1,\n            # class_weight=class_weight,\n            callbacks = [\n                tf.keras.callbacks.Tensorboard(log_dir=os.environ['AIP_TENSORBOARD_LOG_DIR'], histogram_freq=1),\n                keras.callbacks.EarlyStopping(\uc124\uc815),\n                keras.callbacks.ModelCheckpoint(filepath=model.path, monitor=\uba54\ud2b8\ub9ad1, mode='max', save_best_only=True, save_wweights_only=True)\n            ]\n        )\n\n        logging.info(\"training_model component completed..\")\n\n\n    @component(\n        base_image=BASE_IMAGE,\n    )\n    def evaluate_model(\n        # project: str,\n        # location: str,\n        dataset_test: InputPath(\"Any\"),\n        model: Input[Model],\n    ) -> str:\n        from google.cloud import logging as gc_logging\n\n        import tensorflow as tf\n        import logging\n\n        \"\"\"\n        \ubaa8\ub378 \uc815\uc758 \ucf54\ub4dc\n        \"\"\"\n\n        load_model = create_model()\n        load_model.load_weights(model.path)\n\n        \"\"\"\n        \ubaa8\ub378 \ud3c9\uac00 \ucf54\ub4dc\n        \"\"\"\n\n        logging.info(\"evaluate_model component completed..\")\n\n        #\ud3c9\uac00 \ud1b5\uacfc\n        if contidion:\n            logging.info(\"Model evaluation criteria satisfied.\")\n            return \"true\" # \ubc30\ud3ec\n        \n        else:\n            logging.info(\"Model evaluation criteria not satisfied.\")\n            return \"false\" # \uc7ac\ud559\uc2b5\n\n\n    @component(\n        base_image=BASE_IMAGE,\n    )\n    def deploy_model(\n        project: str,\n        location: str,\n        model_name: str,\n        model: Input[Model],\n        endpoint: Input[VertexEndpoint],\n        upload_model: Output[Model],\n        deploy_model: Output[Model],\n    ):\n        from google.cloud import aiplatform\n        from google.cloud import logging as gc_logging\n\n        import logging\n\n        model_list = aiplatform.Model.list(\n            filter=f'display_name=\"{model_name}\"')\n        \n        if len(model_list) > 0:\n            # \ub4f1\ub85d\ub41c \ubaa8\ub378\uc5d0 \uc0c8 \ubc84\uc804 \uc0dd\uc131\n            my_model = model_list[0]\n            logging.info(f'Name of uploaded model: {my_model.resource_name}')\n\n            uploaded_model = aiplatform.Model.upload_tensorflow_saved_model(\n                saved_model_dir = model.path,\n                tensorflow_version = \"tf \ubc84\uc804\",\n                use_gpu = False,\n                parent_model = my_model.resource_name,\n                is_default_version = True,\n                version_description = f\"\ub4f1\ub85d\ud560 \ubaa8\ub378 \uc124\uba85\",\n            )\n        else:\n            # \uc0c8\ub85c\uc6b4 \ubaa8\ub378 \uc774\ub984\uc73c\ub85c \ub4f1\ub85d\n            logging.info(f'Name of New uploaded model: {my_model.resource_name}')\n\n            uploaded_model = aiplatform.Model.upload_tensorflow_saved_model(\n                saved_model_dir = model.path,\n                tensorflow_version = \"tf \ubc84\uc804\",\n                use_gpu = False,\n                parent_model = model_name,\n                is_default_version = True,\n                version_description = f\"\ub4f1\ub85d\ud560 \ubaa8\ub378 \uc124\uba85\",\n            )\n\n        created_private_ep = None\n        for i in aiplatform.PrivateEndpoint.list():\n            if i.resource_name == endpoint.metadata['resourceName']:\n                created_private_ep = i\n                break\n\n        if created_private_ep:\n            # \uc5d4\ub4dc\ud3ec\uc778\ud2b8 \uc7ac\uc0dd\uc131 \ud544\uc694\n            pass\n        else:\n            # \uc55e\uc5d0\uc11c \uc0dd\uc131\ud55c ep\uc5d0 \ubaa8\ub378 \ubc30\ud3ec\n            deployed_model = uploaded_model.deploy(\n                endpoint = created_private_ep,\n                machine_type = \"my-machine\",\n                # deployed_model_display_name = \"my-display-name\" # \ud544\uc694\uc2dc\n            )\n\n        upload_model.uri = uploaded_model.resource_name\n        deploy_model.uri = deployed_model.resource_name\n\n\n        logging.info(\"deploy_model component completed..\")\n\n\n    @component(\n        base_image=BASE_IMAGE,\n    )\n    def get_run_state(status: dict) -> str:\n        return status['state']\n        \n\n    @component(\n        base_image=BASE_IMAGE,\n    )\n    def logging_op(msg: str, severity: str):\n        from google.cloud import logging as gc_logging\n        import logging\n\n        if severity == \"WARNING\":\n            logging.warn(msg)\n        elif severity == \"ERROR\":\n            logging.error(msg)\n        else:\n            logging.info(msg)\n\n\n    @dsl.pipeline(name='conditional-notification')\n    def exit_op(status: PipelineTaskFinalStatus):\n        with dsl.If(get_run_state(status=status).output == \"FAILED\"):\n            # clearnup_op() # \ud30c\uc774\ud504\ub77c\uc778 \uc2e4\ud328\ud558\uba74 clearnup\n            logging_op(msg=\"\uc2e4\ud328 \uba54\uc2dc\uc9c0\", severity=\"ERROR\").set_display_name(\"fail-alarm\")\n        \n        with dsl.Else():\n            logging_op(msg=\"\uc131\uacf5 \uba54\uc2dc\uc9c0\", severity=\"INFO\").set_display_name(\"success-alarm\")\n\n    \n    @dsl.pipeline(\n        name=PIPELINE_NAME,\n        pipeline_root=PIPELINE_ROOT\n    )\n    def pipeline(\n        project_id: str,\n        project_num: str,\n        region: str,\n        vpc_network: str,\n        param1: str, # hpt\uc5d0 \uc0ac\uc6a9\ud558\ub294 \ud30c\ub77c\ubbf8\ud130 \uc608\uc2dc\n        tensorboard_root_try1: str,\n        create_private_ep_name: str,\n        upload_model_name: str,\n    ):\n        \"\"\"\n        \uac01\uc885 \ud544\uc694\ud55c \ud30c\ub77c\ubbf8\ud130\ub4e4 \uc124\uc815..\n        \"\"\"\n        rand_seed = 42\n        download_bukcet_name = \"my-bucket\"\n        hpt_container_image = \"hpyerparamter tuning image\"\n        hpt_epochs = 10\n        hpt_max_trial_count = 8\n        hpt_parallel_trial_count = 2\n\n        tr_label_files = \"my-training-label-file\"\n        tensorboard_id = \"1234567890\" # tensorboard id in gcp\n        sa_custom_training = \"my-service-account\"\n\n        training_params = {\n            \"batch_size\": 64,\n            \"ratio\": 0.8,\n            \"epochs\": 1000\n        }\n\n        # Data download\n        data_download_op = download_data(\n            # project = project_id,\n            # location = region,\n            bukcet_name = download_bukcet_name,\n        ).set_display_name(\"data-download\").set_caching_options(False)\n\n        # Data preparation\n        preparation_op = data_preparation(\n            # project = project_id,\n            # location = region,\n            rand_seed = rand_seed,\n            ratio = training_params[\"ratio\"],\n            batch_size = training_params[\"batch_size\"],\n            label_files = tr_label_files,\n            download_path = data_download_op.outputs[\"download_path\"],\n        ).set_display_name(\"data-preparation\").set_cpu_limit('8').set_memory_limit('16G').set_caching_options(False)\n\n        # worker_pool_specs, study_spec_metrics, study_spec_parameters\n        hpt_worker_pool_spec = worker_pool_spec(\n            hpt_epochs = hpt_epochs,\n            hpt_image = hpt_container_image,\n            myparam1 = param1,\n        ).set_caching_options(False)\n\n        hpt_study_spec_metrics = hyperparameter_tuning_job.serialize_metrics({\"\uba54\ud2b8\ub9ad1\": \"maximize\"})\n        \n        hpt_study_spec_parameters = hyperparameter_tuning_job.serialize_metrics(\n            {\n                \"learning_rate\": aiplatform.hyperparameter_tuning.DoubleParameterSpec(\n                    min=0.0001, max=0.01, scale=\"log\",\n                ),\n                # aiplatform.hyperparameter_tuning.DiscreteParameterSpec\n            }\n        )\n\n        # hyperparameter tuning\n        tuning_op = HyperparameterTuningJobRunOp(\n            display_name=\"hyperparameter-tuning\",\n            project = project_id,\n            location = region,\n            worker_pool_specs = hpt_worker_pool_spec.output,\n            study_spec_metrics = hpt_study_spec_metrics,\n            study_spec_parameters = hpt_study_spec_parameters,\n            max_trial_count = hpt_max_trial_count,\n            parallel_trial_count = hpt_parallel_trial_count,\n            base_output_directory = PIPELINE_ROOT,\n        ).after(preparation_op).set_caching_options(False)\n\n        best_trial_op = GetBestTrialOp(\n            gcp_resources = tuning_op.outputs[\"gcp_resources\"],\n            study_spec_metrics = hpt_study_spec_metrics,\n        ).set_display_name(\"get-best-trial\").set_caching_options(False)\n\n        best_param_op = GetHyperparametersOp(\n            trial = best_trial_op.output,\n        ).set_display_name(\"get-best-parameters\").set_caching_options(False)\n\n        tensorboard = aiplatform.Tensorboard(\n            tensorboard_name = tensorboard_id,\n            project = \"my-prj-id\", # \ubcc0\uc218\ub85c \ubc1b\uc73c\uba74 \uc548\ub40c\n            location = \"my-region\", # \ubcc0\uc218\ub85c \ubc1b\uc73c\uba74 \uc548\ub40c\n        )\n\n        # training\n        custom_job_training_op = utils.create_custom_training_job_op_from_component(\n            training_model,\n            tensorboard = tensorboard.resource_name,\n            base_output_directory = PIPELINE_ROOT,\n            service_account = sa_custom_training,\n            # gpu\n            machine_type = \"my-gcp-machine\",\n            accelerator_type = \"my-gcp-accelerator\",\n            accelerator_count = \"1\",\n            replica_count = 1\n        )\n        training_model_op = custom_job_training_op(\n            project = project_id, # \ud544\uc218\n            location = region, # \ud544\uc218\n            rand_seed = rand_seed,\n            training_params = training_params,\n            best_params = best_param_op.output, # ['{\"parameterId\":\"learning_rate\",\"value\":0.0001}']\n            tensorboard_root = tensorboard_root_try1, # f-string \ub4f1\uc73c\ub85c \uc870\ud569\ud558\uba74 \uc548\ub40c\n            dataset_train = preparation_op.outputs[\"trainset_path\"],\n            dataset_valid = preparation_op.outputs[\"validset_path\"],\n            dataset_test = preparation_op.outputs[\"testset_path\"],\n        ).set_display_name(\"train-model\").set_retry(num_retries=20, backoff_duration=\"60s\", backoff_factor=2, backoff_max_duration=\"3600s\").set_caching_options(False)\n\n        evaluate_op = evaluate_model(\n            # project = project_id,\n            # location = region,\n            dataset_test = preparation_op.outputs[\"testset_path\"],\n            model = training_model_op.outputs[\"model\"],\n        ).set_display_name(\"model-evaluation\").set_cpu_limit('8').set_memory_limit('16G').set_caching_options(False)\n\n        # \uc7ac\ud559\uc2b5\n        with kfp.dsl.If(evaluate_op.output == \"false\"):\n            new_seed = int(time.time())\n            print(f'Generate new seed. (value:{new_seed})')\n\n            \"\"\"\n            \uc7ac\ud559\uc2b5\ud558\ub294 \ucef4\ud3ec\ub10c\ud2b8(\ub610\ub294 \uc704\uc758 \ucef4\ud3ec\ub10c\ud2b8\ub4e4 \uc7ac\uc815\uc758)\n            \"\"\"\n\n            retry_evaluate_op = evaluate_model(\n                # project = project_id,\n                # location = region,\n                dataset_test = retry_preparation_op.outputs[\"testset_path\"],\n                model = retry_training_model_op.outputs[\"model\"],\n            ).set_display_name(\"model-evaluation\").set_cpu_limit('8').set_memory_limit('16G').set_caching_options(False)\n\n            # \uc7ac\ud559\uc2b5 \uc131\uacf5\uc73c\ub85c \ubc30\ud3ec\n            with kfp.dsl.If(retry_evaluate_op.output == \"true\"):\n                \"\"\"\n                \uc5d4\ub4dc\ud3ec\uc778\ud2b8 \uc0dd\uc131 \ubc0f \ubc30\ud3ec\ud558\ub294 \ucef4\ud3ec\ub10c\ud2b8\n                \"\"\"\n\n            # \uc7ac\ud559\uc2b5 \uc2e4\ud328\ub85c \ud30c\uc774\ud504\ub77c\uc778 \uc885\ub8cc\n            with kfp.dsl.Else():\n                logging_op(msg=\"\uc7ac\ud559\uc2b5 \uc2e4\ud328 \uba54\uc2dc\uc9c0\", severity=\"ERROR\").set_display_name(\"retraining-fail-alert\").set_caching_options(False)\n\n\n        with kfp.dsl.Else():\n            # \uc5d4\ub4dc\ud3ec\uc778\ud2b8 \uc0dd\uc131\n            create_endpoint_op = EndpointCreateOp(\n                display_name = create_private_ep_name,\n                project = project_id,\n                location = region,\n                network = vpc_network,\n            ).after(evaluate_op).set_display_name(\"create-private-endpoint\").set_caching_options(False)\n\n            model_deploy_op = deploy_model(\n                project = project_id,\n                location = region,\n                model_name = upload_model_name,\n                model = training_model_op.outputs[\"model\"],\n                endpoint = create_endpoint_op.outputs[\"endpoint\"],\n            ).set_display_name(\"deploy-model\").set_caching_options(False)\n\n    @dsl.pipeline(name='kfp-pipeline-exit-handler')\n    def pipeline_exit_handler(\n        project_id: str,\n        project_num: str,\n        region: str,\n        vpc_network: str,\n        param1: str, # hpt\uc5d0 \uc0ac\uc6a9\ud558\ub294 \ud30c\ub77c\ubbf8\ud130 \uc608\uc2dc\n        tensorboard_root_try1: str,\n        create_private_ep_name: str,\n        upload_model_name: str,\n    ):\n        exit_task = exit_op()\n\n        with dsl.ExitHandler(exit_task):\n            pipeline(\n                project_id = PROJECT_ID,\n                project_num = PROJECT_NUM,\n                region = REGION,\n                vpc_network = PRIVATE_EP_VPC,\n                param1 = myparam1,\n                tensorboard_root_try1 = tensorboard_root_try1,\n                create_private_ep_name = create_private_ep_name,\n                upload_model_name = upload_model_name,\n            )\n\n    yaml_file = \"./kfp_pipeline.yaml\"\n\n    compiler.Compiler().compile(\n        pipeline_func = pipeline_exit_handler,\n        package_path = yaml_file\n    )\n\n    aiplatform.init(project=PROJECT_ID, location=REGION)\n\n    job = aiplatform.PipelineJob(\n        display_name = PIPELINE_NAME,\n        template_path = yaml_file,\n        pipeline_root = PIPELINE_ROOT,\n        parameter_values = {\n            \"project_id\": PROJECT_ID,\n            \"project_num\": PROJECT_NUM,\n            \"region\": REGION,\n            \"vpc_network\": PRIVATE_EP_VPC,\n            \"param1\": myparam1, # cloud run\uc73c\ub85c \ub118\uaca8\ubc1b\ub294 \ud30c\ub77c\ubbf8\ud1301\n            \"tensorboard_root_try1\": f\"{PIPELINE_ROOT}/experiment/tensorboard_log/try1/\",\n            \"create_private_ep_name\": f\"my-private-endpoint-{241231}\",\n            \"upload_model_name\": f\"my-upload-model-{1}\",\n        }\n    )\n\n    job.submit(\n        service_account = PIPELINEJOB_SA,\n        network = PRIVATE_EP_VPC,\n    )\n\nif __name__ == \"__main__\":\n    main()"
  },
  {
    "repo": "niyushabaghayi/telecom_churn_kfp",
    "file_path": "telecom_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/niyushabaghayi/telecom_churn_kfp/dev/telecom_pipeline.py",
    "content": "import kfp\nfrom kfp import dsl\nfrom kfp.components import func_to_container_op\n\n@func_to_container_op\ndef show_results(ada_boost : float, logistic_regression : float, random_forest : float, svm : float, xg_boost : float) -> None:\n    # Given the outputs from decision_tree and logistic regression components\n    # the results are shown.\n\n    print(f\"Ada Boost (accuracy): {ada_boost}\")\n    print(f\"Logistic regression (accuracy): {logistic_regression}\")\n    print(f\"Random Forest (accuracy): {random_forest}\")\n    print(f\"SVM (accuracy): {svm}\")\n    print(f\"XG Boost (accuracy): {xg_boost}\")\n\n\n@dsl.pipeline(name=\"Telecom Churn Prediction\", description=\"Applies Several Models for classification problem.\")\ndef telecom_pipeline():\n\n    # Loads the yaml manifest for each component\n    ingestion = kfp.components.load_component_from_file(\"ingestion/ingestion.yaml\")\n    preprocess = kfp.components.load_component_from_file(\"preprocess/preprocess.yaml\")\n    ada_boost = kfp.components.load_component_from_file(\"ada_boost/ada_boost.yaml\")\n    logistic_regression = kfp.components.load_component_from_file(\"logistic_regression/logistic_regression.yaml\")\n    random_forest = kfp.components.load_component_from_file(\"random_forest/random_forest.yaml\")\n    svm = kfp.components.load_component_from_file(\"svm/svm.yaml\")\n    xg_boost = kfp.components.load_component_from_file(\"xg_boost/xg_boost.yaml\")\n\n    # Run ingestion task\n    ingestion_task = ingestion()\n\n    # Run preprocess task\n    preprocess_task = preprocess(ingestion_task.output)\n\n    # Run tasks Models given\n    # the output generated by \"ingestion_task\".\n    ada_boost_task = ada_boost(preprocess_task.output)\n    logistic_regression_task = logistic_regression(preprocess_task.output)\n    random_forest_task = random_forest(preprocess_task.output)\n    svm_task = svm(preprocess_task.output)\n    xg_boost_task = xg_boost(preprocess_task.output)\n\n    # Given the outputs from \"decision_tree\" and \"logistic_regression\"\n    # the component \"show_results\" is called to print the results.\n    show_results(ada_boost_task.output, logistic_regression_task.output, random_forest_task.output, svm_task.output, xg_boost_task.output)\n\n\n\nif __name__ == \"__main__\":\n    kfp.compiler.Compiler().compile(telecom_pipeline, \"TelecomChurn.yaml\")\n    # kfp.Client().create_run_from_pipeline_func(basic_pipeline, arguments={})"
  },
  {
    "repo": "kangkannnng/Final-Project",
    "file_path": "pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/kangkannnng/Final-Project/main/pipeline.py",
    "content": "import kfp\nfrom kfp import dsl\n\ndef mount_emptydir(name=None, mount_path='/tmp'):\n  from kubernetes import client as k8sc\n  if not name:\n    import uuid\n    name=str(uuid.uuid4())[:8]\n  \n  def _mount_emptydir(task):\n    task.add_volume(\n      k8sc.V1Volume(\n        name=name,\n        empty_dir=k8sc.V1EmptyDirVolumeSource()\n      )\n    )\n    task.add_volume_mount(\n      k8sc.V1VolumeMount(\n        name=name,\n        mount_path=mount_path\n      )\n    )\n    return task\n  \n  return _mount_emptydir\n\ndef preprocess_op():\n    return dsl.ContainerOp(\n        name='Preprocess Data',\n        image='kang/preprocess:v30'\n    )\n\n\ndef train_op():\n    return dsl.ContainerOp(\n        name='Train Model',\n        image='kang/train:v30'\n    )\n\n\ndef test_op():\n    return dsl.ContainerOp(\n        name='Test Model',\n        image='kang/test:v30'\n    )\n\ndef serve_op():\n    return dsl.ContainerOp(\n        name='Serve Model',\n        image='kang/serve:v10'\n    )\n\n\n@dsl.pipeline(\n    name='Translation Pipeline',\n    description='An example pipeline that translate from Chinese to English.'\n)\n\ndef translation_pipeline():\n    _preprocess_op = preprocess_op()\n    _preprocess_op.apply(mount_emptydir(mount_path='/tmp'))\n\n    _train_op = train_op().after(_preprocess_op)\n    _train_op.apply(mount_emptydir(mount_path='/tmp'))\n\n    _test_op = test_op().after(_train_op)\n    _test_op.apply(mount_emptydir(mount_path='/tmp'))\n\n    _serve_op = serve_op().after(_test_op)\n    _serve_op.apply(mount_emptydir(mount_path='/tmp')) \n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(translation_pipeline, __file__ + '.yaml')\n"
  },
  {
    "repo": "MuhamedAyoub/kubeflow-mlops",
    "file_path": "src/main.py",
    "raw_url": "https://raw.githubusercontent.com/MuhamedAyoub/kubeflow-mlops/master/src/main.py",
    "content": "from kfp.compiler import Compiler\n\nfrom kfp import dsl\n\nfrom src.pipelines.training_model.training_pipeline import model_training_component\nfrom src.pipelines.validation_model.validation_pipeline import model_validation_with_serializable_metrics\nfrom src.pipelines.data_ingestion.ingestion_pipeline import data_ingestion_component\nfrom src.pipelines.serving_model.serving_model import deploy_model_with_kserve_sdk\n\nenv = {\n    \"minio_bucket\": \"titanic-model\",\n    \"minio_endpoint\": \"minio.kubeflow:9000\",\n    \"minio_access_key\": \"minio\",\n    \"minio_secret_key\": \"minio123\"\n    \n}\n\n@dsl.pipeline(\n    name='Titanic Survival Prediction Pipeline',\n    description='Pipeline for training and validating a Titanic survival prediction model'\n)\n\ndef titanic_pipeline(\n        dataset_url: str,\n        output_dir: str,\n        test_size: float = 0.2,\n        random_state: int = 42\n):\n    \n    data_task = data_ingestion_component(\n        dataset_url=dataset_url,\n        output_dir=output_dir,\n        minio_access_key=env[\"minio_access_key\"],\n        minio_secret_key=env[\"minio_secret_key\"],\n        minio_endpoint=env[\"minio_endpoint\"],\n    )\n\n    model_task = model_training_component(\n        train_data_dir=data_task.output,\n        random_state=random_state,\n        output_dir=output_dir,\n        minio_access_key=env[\"minio_access_key\"],\n        minio_secret_key=env[\"minio_secret_key\"],\n        minio_endpoint=env[\"minio_endpoint\"],\n        test_size=test_size,\n        \n    )\n\n    validation_task = model_validation_with_serializable_metrics(\n        model_path=model_task.output,\n        test_data_dir=data_task.output,\n        minio_access_key=env[\"minio_access_key\"],\n        minio_secret_key=env[\"minio_secret_key\"],\n        minio_endpoint=env[\"minio_endpoint\"],\n        test_size=test_size,\n    )\n    \n    deploy_task = deploy_model_with_kserve_sdk(\n        minio_access_key=env[\"minio_access_key\"],\n        minio_endpoint=env[\"minio_endpoint\"],\n        minio_secret_key=env[\"minio_secret_key\"],\n        model_bucket=env[\"minio_bucket\"],\n        model_key=model_task.output,\n        model_name=\"logistic_model.pkl\",\n        namespace=\"kubeflow\",\n        \n    )\n    deploy_task.after(validation_task)\n# Create a pipeline run\n\ndef main():\n    \"\"\"\n    Compile the pipeline to YAML and optionally run it.\n    \"\"\"\n\n    # Compile the pipeline\n    pipeline_filename = 'titanic_pipeline.yaml'\n    Compiler().compile(\n        pipeline_func=titanic_pipeline,\n        package_path=pipeline_filename\n    )\n\n    print(f\"Pipeline compiled successfully to {pipeline_filename}\")\n\n\nif __name__ == '__main__':\n    main()\n"
  },
  {
    "repo": "MuhamedAyoub/kubeflow-mlops",
    "file_path": "src/test.py",
    "raw_url": "https://raw.githubusercontent.com/MuhamedAyoub/kubeflow-mlops/master/src/test.py",
    "content": "import kfp\nfrom kfp import dsl\n\n@dsl.pipeline(\n    name='Simple Pipeline',\n    description='A simple pipeline for testing kfp'\n)\ndef simple_pipeline():\n    op = dsl.ContainerSpec(\n        image='busybox',\n        command=['echo', 'Hello from Kubeflow Pipelines!']\n    )\n\n# Compile the pipeline to a YAML file\nkfp.compiler.Compiler().compile(simple_pipeline, 'simple_pipeline.yaml')\n\nprint(\"Pipeline compiled successfully!\")\n"
  },
  {
    "repo": "MuhamedAyoub/kubeflow-mlops",
    "file_path": "src/pipelines/serving_model/serving_model.py",
    "raw_url": "https://raw.githubusercontent.com/MuhamedAyoub/kubeflow-mlops/master/src/pipelines/serving_model/serving_model.py",
    "content": "from kfp.dsl import component\nfrom typing import NamedTuple\n\n@component(\n    packages_to_install=['kserve', 'boto3'],\n)\ndef deploy_model_with_kserve_sdk(\n    model_name: str,\n    model_bucket: str,\n    model_key: str,\n    minio_endpoint: str = 'minio:9000',\n    minio_access_key: str = 'minio',\n    minio_secret_key: str = 'minio123',\n    namespace: str = 'kubeflow'\n) -> NamedTuple('Outputs', [\n    ('inference_service_name', str),\n    ('status', str),\n]):\n    from kserve import KServeClient, constants\n    from kserve.models import (\n        V1beta1InferenceService,\n        V1beta1InferenceServiceSpec,\n        V1beta1PredictorSpec,\n        V1beta1SKLearnSpec,\n        V1beta1ModelSpec,\n    )\n    import boto3\n    from collections import namedtuple\n\n    # Validate the model exists in MinIO\n    s3 = boto3.client(\n        's3',\n        endpoint_url=f'http://{minio_endpoint}',\n        aws_access_key_id=minio_access_key,\n        aws_secret_access_key=minio_secret_key,\n        config=boto3.session.Config(signature_version='s3v4'),\n        verify=False\n    )\n\n    try:\n        s3.head_object(Bucket=model_bucket, Key=model_key)\n        print(f\"Model '{model_key}' found in bucket '{model_bucket}'.\")\n    except Exception as e:\n        raise RuntimeError(f\"Model not found: {e}\")\n\n    # Initialize the KServe client\n    kserve_client = KServeClient()\n\n    # Create the InferenceService spec using the KServe SDK\n    inference_service = V1beta1InferenceService(\n        api_version=constants.KSERVE_V1BETA1,\n        kind=constants.KSERVE_KIND,\n        metadata={\"name\": model_name, \"namespace\": namespace},\n        spec=V1beta1InferenceServiceSpec(\n            predictor=V1beta1PredictorSpec(\n                sklearn=V1beta1SKLearnSpec(\n                    storage_uri=f\"s3://{model_bucket}/{model_key}\",\n                    env=[\n                        {\"name\": \"AWS_ACCESS_KEY_ID\", \"value\": minio_access_key},\n                        {\"name\": \"AWS_SECRET_ACCESS_KEY\", \"value\": minio_secret_key},\n                        {\"name\": \"AWS_ENDPOINT_URL\", \"value\": f\"http://{minio_endpoint}\"},\n                        {\"name\": \"AWS_S3_FORCE_PATH_STYLE\", \"value\": \"true\"},\n                    ],\n                )\n            )\n        )\n    )\n\n    # Deploy the InferenceService\n    try:\n        kserve_client.create(inference_service)\n        print(f\"InferenceService '{model_name}' deployed successfully.\")\n        status = \"Success\"\n    except Exception as e:\n        print(f\"Failed to deploy InferenceService: {e}\")\n        status = \"Failed\"\n\n    # Return outputs\n    output = namedtuple('Outputs', ['inference_service_name', 'status'])\n    return output(model_name, status)\n \n \n \n \n \n\n\n\n"
  },
  {
    "repo": "MuhamedAyoub/kubeflow-mlops",
    "file_path": "src/pipelines/validation_model/validation_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/MuhamedAyoub/kubeflow-mlops/master/src/pipelines/validation_model/validation_pipeline.py",
    "content": "from kfp.dsl import component, Output, ClassificationMetrics,Markdown\nfrom typing import NamedTuple, Dict, Any\nimport io\n\n@component(\n    packages_to_install=['pandas', 'scikit-learn', 'minio', 'boto3'],\n)\ndef model_validation_with_serializable_metrics(\n    metrics: Output[ClassificationMetrics],\n    markdown: Output[Markdown],\n    model_path: str,\n    test_data_dir: str,\n    minio_endpoint: str = 'minio:9000',\n    minio_access_key: str = 'minio',\n    minio_secret_key: str = 'minio123',\n    test_size: float = 0.2,\n    random_state: int = 42\n) -> NamedTuple('Outputs', [\n    ('accuracy', float), \n    ('precision', float), \n    ('recall', float), \n]):\n    import pandas as pd\n    import joblib\n    import boto3\n    from io import BytesIO\n    from sklearn.metrics import (\n        accuracy_score, \n        precision_score, \n        recall_score, \n        classification_report, \n        confusion_matrix\n    )\n    from sklearn.model_selection import train_test_split\n    from collections import namedtuple\n    try: \n        # Create S3 client\n        s3 = boto3.client(\n            's3',\n            endpoint_url=f'http://{minio_endpoint}',\n            aws_access_key_id=minio_access_key,\n            aws_secret_access_key=minio_secret_key,\n            config=boto3.session.Config(signature_version='s3v4'),\n            verify=False\n        )\n\n        # Load data\n        data_obj = s3.get_object(Bucket='titanic-data', Key='titanic_data.csv')\n        data = pd.read_csv(data_obj['Body'])\n\n        # Load model directly from S3\n        model_obj = s3.get_object(Bucket='titanic-model', Key='logistic_model.pkl')\n        model_bytes = model_obj['Body'].read()\n        model = joblib.load(BytesIO(model_bytes))\n\n        # Prepare features\n        feature_columns = [\n            'Pclass', 'Age', 'SibSp', 'Parch', 'Fare', \n            'Sex_male', 'Embarked_Q', 'Embarked_S'\n        ]\n        X = data[feature_columns]\n        y = data['Survived']\n\n        # Split data\n        X_train, X_test, y_train, y_test = train_test_split(\n            X, y,\n            test_size=test_size,\n            random_state=random_state\n        )\n\n        # Predictions and metrics\n        y_pred = model.predict(X_test)\n        \n        # Compute metrics\n        accuracy = float(accuracy_score(y_test, y_pred))\n        precision = float(precision_score(y_test, y_pred, average='binary'))\n        recall = float(recall_score(y_test, y_pred, average='binary'))\n        \n        # Confusion Matrix\n        cm = confusion_matrix(y_test, y_pred)\n        metrics.log_confusion_matrix(['Not Survived', 'Survived'], cm.tolist())\n        \n        # classification report\n        report = classification_report(y_test, y_pred, output_dict=True)\n        \n        # use markdown to display classification report\n        markdown_content = f\"\"\"\n        # Classification Report\n        | Class | Precision | Recall | F1-Score | Support | \n        | --- | --- | --- | --- | --- |\n        | Not Survived | {report['0']['precision']} | {report['0']['recall']} | {report['0']['f1-score']} | {report['0']['support']} |\n        | Survived | {report['1']['precision']} | {report['1']['recall']} | {report['1']['f1-score']} | {report['1']['support']} |  \n        \"\"\"\n        with open(markdown.path, 'w') as f:\n            f.write(markdown_content)\n        \n        # Prepare output\n        output = namedtuple('Outputs', [\n            'accuracy', 'precision', 'recall'\n        ])\n        return output(\n            accuracy, \n            precision, \n            recall, \n        )\n    except Exception as e:\n        print(f\"Validation failed: {e}\")\n        raise RuntimeError(f\"Validation failed: {e}\")\n    "
  },
  {
    "repo": "bmorphism/kfsummit19",
    "file_path": "resnet_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/bmorphism/kfsummit19/master/resnet_pipeline.py",
    "content": "import kfp\nimport kfp.dsl as dsl\nfrom kfp.components import func_to_container_op\nfrom kfp.onprem import mount_pvc\n\nc= kfp.Client(\"127.0.0.1:8082/pipeline\", namespace=\"kfsummit\")\n\ndef PreprocessOp(name, input_dir, output_dir):\n    return dsl.ContainerOp(\n        name=name,\n        image='eu.gcr.io/kfsummit/preprocess_resnet',\n        arguments=[\n            '--input_dir', input_dir,\n            '--output_dir', output_dir,\n        ],\n        file_outputs={'output': '/output.txt'}\n    )\n\ndef TrainOp(name, input_dir, output_dir, model_name, model_version, epochs):\n    return dsl.ContainerOp(\n        name=name,\n        image='eu.gcr.io/kfsummit/train_resnet',\n        arguments=[\n            '--input_dir', input_dir,\n            '--output_dir', output_dir,\n            '--model_name', model_name,\n            '--model_version', model_version,\n            '--epochs', epochs\n        ],\n        file_outputs={'output': '/output.txt'}\n    )\n\n@dsl.pipeline(\n    name='resnet_cifar10_pipeline',\n    description='Demonstrate an end-to-end training & serving pipeline using ResNet and CIFAR-10'\n)\ndef resnet_pipeline(\n    raw_data_dir='/mnt/workspace/raw_data',\n    processed_data_dir='/mnt/workspace/processed_data',\n    model_dir='/mnt/workspace/saved_model',\n    epochs=50,\n    trtserver_name='trtis',\n    model_name='resnet_graphdef',\n    model_version=1,\n    webapp_prefix='webapp',\n    webapp_port=80\n):\n    pvc_name = 'kfsummit-workspace-read-claim'\n    volume_name = 'kfsummit-workspace'\n    volume_mount_path = '/mnt/workspace'\n\n    op_dict = {}\n\n    op_dict['preprocess'] = PreprocessOp(\n        'preprocess', raw_data_dir, processed_data_dir)\n\n    op_dict['train'] = TrainOp(\n\t'train', op_dict['preprocess'].output, model_dir, model_name, model_version, epochs)\n\n    for _, container_op in op_dict.items():\n        container_op.apply(mount_pvc(pvc_name, volume_name, volume_mount_path))"
  },
  {
    "repo": "anhdan/kfp-first-pipelines",
    "file_path": "l1-addition-pipeline/kfp_client.py",
    "raw_url": "https://raw.githubusercontent.com/anhdan/kfp-first-pipelines/main/l1-addition-pipeline/kfp_client.py",
    "content": "import kfp\nimport my_pipeline as mykfp\n\n# Connect to Kubeflow Pipelines cluster\nclient = kfp.Client( host='http://3.34.205.245:80' )\nprint( client.list_pipelines() )\n\n# Specify pipeline argument values\narguments = {'a': '7', 'b': '8'}\n\n# Create run for the addition pipeline created in my_pipeline.py\nrun = client.create_run_from_pipeline_func(\n    mykfp.add_pipeline,\n    arguments=arguments,\n    mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE,\n    experiment_name='my-addition-experiment'\n)\n"
  },
  {
    "repo": "anhdan/kfp-first-pipelines",
    "file_path": "l1-addition-pipeline/my_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/anhdan/kfp-first-pipelines/main/l1-addition-pipeline/my_pipeline.py",
    "content": "import kfp.dsl as dsl\n\nimport my_components as comps\n\n# Define a pipeline\n@dsl.pipeline(\n    name='Addition pipeline',\n    description='A toy pipeline that performs addition calculations.'\n)\ndef add_pipeline(\n    a: float = 1,\n    b: float = 7,\n):\n    first_add_task = comps.add(a, 4)\n    second_add_task = comps.add(first_add_task.output, b)"
  },
  {
    "repo": "anhdan/kfp-first-pipelines",
    "file_path": "l2-tf-pipeline/kfp_client.py",
    "raw_url": "https://raw.githubusercontent.com/anhdan/kfp-first-pipelines/main/l2-tf-pipeline/kfp_client.py",
    "content": "import kfp\nimport my_pipeline as mykfp\n\n# Connect to Kubeflow Pipelines cluster\nclient = kfp.Client( host='http://3.34.205.245:80' )\nprint( client.list_pipelines() )\n\n# Specify pipeline argument values\narguments = {'a': 7, 'b': 8}\n\n# Submit a pipeline run\nclient.create_run_from_pipeline_func(\n    mykfp.calc_pipeline,\n    arguments=arguments,\n    mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE)"
  },
  {
    "repo": "anhdan/kfp-first-pipelines",
    "file_path": "l2-tf-pipeline/my_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/anhdan/kfp-first-pipelines/main/l2-tf-pipeline/my_pipeline.py",
    "content": "import kfp.dsl as dsl\nimport my_components as comps\n\n@dsl.pipeline(\n   name='calculation-pipeline',\n   description='An example pipeline that performs arithmetic calculations.',\n)\ndef calc_pipeline(\n   a: float=1,\n   b: float=7,\n   c: float=17,\n):\n    # Passes a pipeline parameter and a constant value as operation arguments.\n    add_task = comps.add(a, 4) # The add_op factory function returns\n                            # a dsl.ContainerOp class instance. \n\n    # Passes the output of the add_task and a pipeline parameter as operation\n    # arguments. For an operation with a single return value, the output\n    # reference is accessed using `task.output` or\n    # `task.outputs['output_name']`.\n    divmod_task = comps.my_divmod(add_task.output, b)\n\n    # For an operation with multiple return values, output references are\n    # accessed as `task.outputs['output_name']`.\n    result_task = comps.add(divmod_task.outputs['quotient'], c)\n"
  },
  {
    "repo": "mpaul7/end-to-end-ai-pipelines-using-kubeflow",
    "file_path": "src/maikube/pipeline/feature_extraction.py",
    "raw_url": "https://raw.githubusercontent.com/mpaul7/end-to-end-ai-pipelines-using-kubeflow/main/src/maikube/pipeline/feature_extraction.py",
    "content": "from kfp.dsl import pipeline, get_pipeline_conf\n\nfrom src.maikube.components import load_component\n\n@pipeline(name=\"process NFStream pipeline\")\ndef process_nfstream_pipeline(input_bucket:str, input_pcap:str, output_bucket:str, output_file:str):\n    minio_download = load_component('minio_download')\n    minio_upload = load_component('minio_upload')\n    nfstream = load_component('nfs_feature_extractoion')\n\n    download_op = minio_download(f'kubeflow/{input_pcap_bucket}/{input_pcap_file}')\n    process_op = nfstream(download_op.output)\n    minio_upload(process_op.output, f'kubeflow/{output_bucket}/{output_file}')\n    get_pipeline_conf().set_image_pull_policy(policy=\"Never\")"
  },
  {
    "repo": "DharmitD/mlops-cicd-pipeline",
    "file_path": "dsl-convert.py",
    "raw_url": "https://raw.githubusercontent.com/DharmitD/mlops-cicd-pipeline/main/dsl-convert.py",
    "content": "import sys\nimport os\nimport ast\nimport re\nfrom kfp import dsl\nfrom kfp.dsl import Input, Output, Dataset, Model\n\ndef read_python_file(file_path):\n    \"\"\" Reads the content of the given Python file. \"\"\"\n    with open(file_path, \"r\") as f:\n        return f.read()\n\ndef extract_functions(code):\n    \"\"\" Extracts function definitions from Python code. \"\"\"\n    tree = ast.parse(code)\n    functions = {}\n\n    for node in tree.body:\n        if isinstance(node, ast.FunctionDef):\n            function_body = \"\\n\".join([ast.unparse(stmt) for stmt in node.body])\n            if function_body.strip():\n                args = [arg.arg for arg in node.args.args]\n                num_outputs = function_body.count(\"return \")\n\n                functions[node.name] = {\n                    \"body\": function_body,\n                    \"args\": args,\n                    \"num_outputs\": num_outputs\n                }\n\n    return functions\n\ndef extract_imports(code):\n    \"\"\" Extracts all import statements from Python code. \"\"\"\n    tree = ast.parse(code)\n    imports = []\n\n    for node in tree.body:\n        if isinstance(node, (ast.Import, ast.ImportFrom)):\n            imports.append(ast.unparse(node))\n\n    return \"\\n\".join(imports)\n\ndef convert_to_kfp_dsl(file_path):\n    \"\"\" Converts a given Python ML script into a Kubeflow Pipelines DSL format. \"\"\"\n    code = read_python_file(file_path)\n    functions = extract_functions(code)\n    imports = extract_imports(code)\n\n    if not functions:\n        print(\"No functions found in the provided script.\")\n        sys.exit(1)\n\n    dsl_code = \"\"\"\nfrom kfp import dsl\nfrom kfp.dsl import Input, Output, Dataset, Model\n\"\"\" + imports + \"\\n\"\n\n    component_templates = []\n    function_calls = []\n    previous_outputs = {}\n\n    for function_name, details in functions.items():\n        function_body = details[\"body\"]\n        function_args = details[\"args\"]\n        num_outputs = details[\"num_outputs\"]\n\n        kfp_args = \", \".join([f\"{arg}: Dataset\" for arg in function_args])\n\n        # Fix: Handling multiple outputs correctly\n        if function_name == \"preprocess_data\":\n            return_type = \"Tuple[Dataset, Dataset]\"\n            output_vars = [\"X_train\", \"y_train\"]\n        elif function_name == \"train_model\":\n            return_type = \"Model\"\n            output_vars = [\"model\"]\n        elif function_name == \"load_data\":\n            return_type = \"Dataset\"\n            output_vars = [\"df\"]\n        else:\n            return_type = \"Dataset\"\n            output_vars = [\"output\"]\n\n        outputs = \", \".join(output_vars)\n\n        component_templates.append(f\"\"\"\n@dsl.component(\n    base_image=\"python:3.9\",\n    packages_to_install=[\"pandas\", \"scikit-learn\", \"tensorflow\"]\n)\ndef {function_name}({kfp_args}) -> {return_type}:\n{re.sub(\"^\", \"    \", function_body, flags=re.MULTILINE)}\n\"\"\")\n\n        # Store outputs with correct variable names\n        if function_args:\n            input_args = []\n            for arg in function_args:\n                if arg in previous_outputs:\n                    input_args.append(f\"{previous_outputs[arg]}.output\")\n                else:\n                    print(f\"\u26a0\ufe0f Warning: Argument '{arg}' not found in previous outputs!\")\n                    input_args.append(\"MISSING_ARG\")\n\n            input_args_str = \", \".join(input_args)\n            function_calls.append(f\"    {outputs} = {function_name}({input_args_str})\")\n        else:\n            function_calls.append(f\"    {outputs} = {function_name}()\")\n\n        print(f\"\ud83d\udd39 Storing function output: {function_name} -> {outputs}\")\n\n        # Fix: Store multiple outputs correctly\n        for i, arg in enumerate(output_vars):\n            previous_outputs[arg] = output_vars[i]\n\n    dsl_code += \"\\n\".join(component_templates) + \"\\n\"\n\n    dsl_code += \"\"\"\n@dsl.pipeline(name=\"ml-pipeline\")\ndef ml_pipeline():\n\"\"\" + \"\\n\".join(function_calls) + \"\\n\"\n\n    dsl_code += \"\"\"\nif __name__ == \"__main__\":\n    from kfp import compiler\n    compiler.Compiler().compile(ml_pipeline, \"ml_pipeline.yaml\")\n\"\"\"\n\n    return dsl_code\n\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python dsl-convert.py <input_python_script>\")\n        sys.exit(1)\n\n    input_file = sys.argv[1]\n\n    if not os.path.exists(input_file):\n        print(f\"Error: The file '{input_file}' does not exist.\")\n        sys.exit(1)\n\n    dsl_output = convert_to_kfp_dsl(input_file)\n\n    output_file = \"generated_new_pipeline.py\"\n    with open(output_file, \"w\") as f:\n        f.write(dsl_output)\n\n    print(f\"\u2705 Successfully generated Kubeflow DSL: {output_file}\")\n"
  },
  {
    "repo": "DharmitD/mlops-cicd-pipeline",
    "file_path": "generated_new_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/DharmitD/mlops-cicd-pipeline/main/generated_new_pipeline.py",
    "content": "from click import Tuple\nfrom kfp import dsl\nfrom kfp.dsl import Input, Output, Dataset, Model\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nimport pickle\n\n@dsl.component(\n    base_image=\"python:3.9\",\n    packages_to_install=[\"pandas\", \"scikit-learn\", \"tensorflow\"]\n)\ndef load_data() -> Dataset:\n    df = pd.read_csv('data.csv')\n    return df\n\n\n@dsl.component(\n    base_image=\"python:3.9\",\n    packages_to_install=[\"pandas\", \"scikit-learn\", \"tensorflow\"]\n)\ndef preprocess_data(df: Dataset) -> Tuple[Dataset, Dataset]:\n    X = df.drop(columns=['target'])\n    y = df['target']\n    return train_test_split(X, y, test_size=0.2)\n\n\n@dsl.component(\n    base_image=\"python:3.9\",\n    packages_to_install=[\"pandas\", \"scikit-learn\", \"tensorflow\"]\n)\ndef train_model(X_train: Dataset, y_train: Dataset) -> Model:\n    model = LogisticRegression()\n    model.fit(X_train, y_train)\n    return model\n\n\n@dsl.component(\n    base_image=\"python:3.9\",\n    packages_to_install=[\"pandas\", \"scikit-learn\", \"tensorflow\"]\n)\ndef save_model(model: Dataset) -> Dataset:\n    with open('model.pkl', 'wb') as f:\n        pickle.dump(model, f)\n\n\n@dsl.pipeline(name=\"ml-pipeline\")\ndef ml_pipeline():\n    df = load_data()\n    X_train, y_train = preprocess_data(df.output)\n    model = train_model(X_train.output, y_train.output)\n    output = save_model(model.output)\n\nif __name__ == \"__main__\":\n    from kfp import compiler\n    compiler.Compiler().compile(ml_pipeline, \"ml_pipeline.yaml\")\n"
  },
  {
    "repo": "abzeefly/kubeflow-local-k8s",
    "file_path": "kfp/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/abzeefly/kubeflow-local-k8s/main/kfp/pipeline.py",
    "content": "import argparse\nimport kfp\n\nfrom kfp import components as comp\nfrom kfp.v2 import dsl\nfrom kfp.v2.compiler import Compiler\n\n\ndef parse_args():\n  \"\"\"Parse arguments.\"\"\"\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n      \"--gcp-project-id\",\n      type=str,\n      help=\"ID for the google cloud project to deploy the pipeline to.\",\n      required=True)\n  parser.add_argument(\n      \"--region\",\n      type=str,\n      help=\"Region in which to deploy the pipeline.\",\n      required=True)\n  parser.add_argument(\n      \"--pipeline-root\",\n      type=str,\n      help=\n      \"Path to artifact repository where Kubeflow Pipelines stores a pipeline\u2019s artifacts.\",\n      required=True)\n  parser.add_argument(\n      \"--component-artifact-root\",\n      type=str,\n      help=\n      \"Path to artifact repository where Kubeflow Pipelines components can store artifacts.\",\n      required=True)\n  parser.add_argument(\n      \"--dataflow-staging-root\",\n      type=str,\n      help=\"Path to staging directory for dataflow.\",\n      required=True)\n  parser.add_argument(\n      \"--beam-runner\",\n      type=str,\n      help=\"Beam runner: DataflowRunner or DirectRunner.\",\n      default=\"DirectRunner\")\n  return parser.parse_args()\n\n\n# arguments are parsed as a global variable so\n# they can be used in the pipeline decorator below\nARGS = parse_args()\nPIPELINE_ROOT = vars(ARGS)['pipeline_root']\n\n# [START load_kfp_components]\n# load the kfp components from their yaml files\nDataIngestOp = comp.load_component('components/ingestion/component.yaml')\nDataPreprocessingOp = comp.load_component(\n    'components/preprocessing/component.yaml')\nTrainModelOp = comp.load_component('components/train/component.yaml')\n# [END load_kfp_components]\n\n\n# [START define_kfp_pipeline]\n@dsl.pipeline(\n    pipeline_root=PIPELINE_ROOT,\n    name=\"beam-preprocessing-kfp-example\",\n    description=\"Pipeline to show an apache beam preprocessing example in KFP\")\ndef pipeline(\n    gcp_project_id: str,\n    region: str,\n    component_artifact_root: str,\n    dataflow_staging_root: str,\n    beam_runner: str):\n  \"\"\"KFP pipeline definition.\n\n  Args:\n      gcp_project_id (str): ID for the google cloud project to deploy the pipeline to.\n      region (str): Region in which to deploy the pipeline.\n      component_artifact_root (str): Path to artifact repository where Kubeflow Pipelines\n        components can store artifacts.\n      dataflow_staging_root (str): Path to staging directory for the dataflow runner.\n      beam_runner (str): Beam runner: DataflowRunner or DirectRunner.\n  \"\"\"\n\n  ingest_data_task = DataIngestOp(base_artifact_path=component_artifact_root)\n\n  data_preprocessing_task = DataPreprocessingOp(\n      ingested_dataset_path=ingest_data_task.outputs[\"ingested_dataset_path\"],\n      base_artifact_path=component_artifact_root,\n      gcp_project_id=gcp_project_id,\n      region=region,\n      dataflow_staging_root=dataflow_staging_root,\n      beam_runner=beam_runner)\n\n  train_model_task = TrainModelOp(\n      preprocessed_dataset_path=data_preprocessing_task.\n      outputs[\"preprocessed_dataset_path\"],\n      base_artifact_path=component_artifact_root)\n\n\n# [END define_kfp_pipeline]\n\nif __name__ == \"__main__\":\n  # [START compile_kfp_pipeline]\n  Compiler().compile(pipeline_func=pipeline, package_path=\"pipeline.json\")\n  # [END compile_kfp_pipeline]\n\n  run_arguments = vars(ARGS)\n  del run_arguments['pipeline_root']\n\n  # [START execute_kfp_pipeline]\n  client = kfp.Client(host=\"http://127.0.0.1/pipline\")\n  experiment = client.create_experiment(\"KFP orchestration example\")\n  run_result = client.run_pipeline(\n      experiment_id=experiment.id,\n      job_name=\"KFP orchestration job\",\n      pipeline_package_path=\"pipeline.json\",\n      params=run_arguments)\n  # [END execute_kfp_pipeline]"
  },
  {
    "repo": "stackdemos/clorox-kf",
    "file_path": "components/golang/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/clorox-kf/master/components/golang/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef processing_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('GOLANG_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=[script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n"
  },
  {
    "repo": "stackdemos/clorox-kf",
    "file_path": "components/training/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/clorox-kf/master/components/training/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef training_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('TRAINING_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=['/usr/local/bin/python', script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n\n"
  },
  {
    "repo": "mhash1m/kubeflow_kfp_workflow",
    "file_path": "xgboost_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/mhash1m/kubeflow_kfp_workflow/main/xgboost_pipeline.py",
    "content": "import os\nfrom typing import NamedTuple\n\nimport numpy as np\n\nimport kfp.components as comp\nfrom kfp import dsl, compiler,  Client\n\nfrom client import create_client\n\n# Define the data loading function\ndef load_data_op(random_state:int, test_size:float, \n    train_path: comp.OutputPath('CSV'),\n    test_path: comp.OutputPath('CSV')):\n\n    from sklearn.datasets import make_regression\n    from sklearn.model_selection import train_test_split\n    import pandas as pd\n    \n    # Load the Boston housing dataset\n    X, Y = make_regression()\n    data = pd.DataFrame(X)\n    data['Y'] = Y\n    # Split the data into training and test sets\n    train, test = train_test_split(data, test_size=test_size, random_state=random_state)\n    # Save to output paths\n    train.to_csv(train_path, index=False)\n    test.to_csv(test_path, index=False)\n\n# Define the model training function\ndef train_model_op(train_path: comp.InputPath('CSV'), model_path: comp.OutputPath('PKL')):# -> NamedTuple('Outputs', [('model', object), ('model_path', str)]):#, learning_rate: float, max_depth: int, subsample: float, n_estimators: int):\n    import pickle\n    import os\n    import xgboost as xgb\n    import pandas as pd\n\n    train_data = pd.read_csv(train_path)\n    \n    # Create the XGBoost model\n    xgb_model = xgb.XGBRegressor()\n\n    # Fit the model to the training data\n    xgb_model.fit(train_data.drop('Y', axis = 1), train_data['Y'])\n    \n    # Save model\n    pickle.dump(xgb_model, open(model_path, \"wb\"))\n\n# Define the model evaluation function\ndef evaluate_model_op(test_path: comp.InputPath('CSV'), model_path: comp.InputPath('PKL')) -> NamedTuple('Outputs', [\n  ('mlpipeline_metrics', 'Metrics'),\n]):\n    from sklearn.metrics import mean_squared_error\n    import pandas as pd\n    import pickle\n    import json\n\n    test_data = pd.read_csv(test_path)\n    xgb_model = pickle.load(open(model_path, \"rb\"))\n    # Make predictions on the test set\n    y_pred = xgb_model.predict(test_data.drop('Y', axis = 1))\n\n    # Calculate the mean squared error of the predictions\n    mse = mean_squared_error(test_data['Y'], y_pred)\n    metrics = {\n        'metrics': [{\n        'name': 'mse-score', # The name of the metric. Visualized as the column name in the runs table.\n        'numberValue':  mse, # The value of the metric. Must be a numeric value.\n        'format': \"RAW\",   # The optional format of the metric. Supported values are \"RAW\" (displayed in raw format) and \"PERCENTAGE\" (displayed in percentage format).\n        }]\n    }\n    return [json.dumps(metrics)]\n\ndef create_components():\n    data_op = comp.create_component_from_func(func=load_data_op, base_image='huanjason/scikit-learn', output_component_file='load_data_component.yaml')\n    train_op = comp.create_component_from_func(func=train_model_op, base_image='huanjason/scikit-learn', output_component_file='train_model_component.yaml')\n    eval_op = comp.create_component_from_func(func=evaluate_model_op, base_image='huanjason/scikit-learn', output_component_file='evaluate_model_component.yaml')\n    return data_op, train_op, eval_op\n\n# Define the pipeline\n@dsl.pipeline(\n    name=\"XGBoost Pipeline\",\n    description=\"A pipeline that trains an XGBoost model on the Boston housing dataset.\"\n)\ndef xgboost_pipeline(random_state: int =20, test_size: float =0.2):\n    data_op, train_op, eval_op=create_components()\n    data_prep = data_op(random_state, test_size)\n    trainer = train_op(data_prep.outputs['train'])\n    mse_score = eval_op(data_prep.outputs['test'], trainer.outputs['model']).output\n\nif __name__ == '__main__':\n    client = create_client()\n    client.create_run_from_pipeline_func(xgboost_pipeline, {}, experiment_name=\"Test XG_Boost\")\n"
  },
  {
    "repo": "rujual/telco_churn_pipeline",
    "file_path": "xgb sample pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/rujual/telco_churn_pipeline/master/xgb%20sample%20pipeline.py",
    "content": "# !/usr/bin/env python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport json\nimport kfp\nfrom kfp import components\nfrom kfp import dsl\nimport os\nimport subprocess\n\ndiagnose_me_op = components.load_component_from_url(\n'https://raw.githubusercontent.com/kubeflow/pipelines/566dddfdfc0a6a725b6e50ea85e73d8d5578bbb9/components/diagnostics/diagnose_me/component.yaml')\n\nconfusion_matrix_op = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/2df775a28045bda15372d6dd4644f71dcfe41bfe/components/local/confusion_matrix/component.yaml')\n\nroc_op = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/2df775a28045bda15372d6dd4644f71dcfe41bfe/components/local/roc/component.yaml')\n\ndataproc_create_cluster_op = components.load_component_from_url(\n'https://raw.githubusercontent.com/kubeflow/pipelines/2df775a28045bda15372d6dd4644f71dcfe41bfe/components/gcp/dataproc/create_cluster/component.yaml')\n\ndataproc_delete_cluster_op = components.load_component_from_url(\n'https://raw.githubusercontent.com/kubeflow/pipelines/2df775a28045bda15372d6dd4644f71dcfe41bfe/components/gcp/dataproc/delete_cluster/component.yaml')\n\ndataproc_submit_pyspark_op = components.load_component_from_url(\n'https://raw.githubusercontent.com/kubeflow/pipelines/2df775a28045bda15372d6dd4644f71dcfe41bfe/components/gcp/dataproc/submit_pyspark_job/component.yaml'\n)\n\ndataproc_submit_spark_op = components.load_component_from_url(\n'https://raw.githubusercontent.com/kubeflow/pipelines/2df775a28045bda15372d6dd4644f71dcfe41bfe/components/gcp/dataproc/submit_spark_job/component.yaml'\n)\n\n_PYSRC_PREFIX = 'gs://ml-pipeline-playground/dataproc-example'  # Common path to python src.\n\n_XGBOOST_PKG = 'gs://ml-pipeline-playground/xgboost4j-example-0.8-SNAPSHOT-jar-with-dependencies.jar'\n\n_TRAINER_MAIN_CLS = 'ml.dmlc.xgboost4j.scala.example.spark.XGBoostTrainer'\n\n_PREDICTOR_MAIN_CLS = 'ml.dmlc.xgboost4j.scala.example.spark.XGBoostPredictor'\n\n\ndef delete_directory_from_gcs(dir_path):\n    \"\"\"Delete a GCS dir recursively. Ignore errors.\"\"\"\n    try:\n        subprocess.call(['gsutil', '-m', 'rm', '-r', dir_path])\n    except:\n        pass\n\n\n# ! Please do not forget to enable the Dataproc API in your cluster https://console.developers.google.com/apis/api/dataproc.googleapis.com/overview\n\n# ================================================================\n# The following classes should be provided by components provider.\n\n\ndef dataproc_analyze_op(\n        project,\n        region,\n        cluster_name,\n        schema,\n        train_data,\n        output):\n    \"\"\"Submit dataproc analyze as a pyspark job.\n    :param project: GCP project ID.\n    :param region: Which zone to run this analyze.\n    :param cluster_name: Name of the cluster.\n    :param schema: GCS path to the schema.\n    :param train_data: GCS path to the training data.\n    :param output: GCS path to store the output.\n    \"\"\"\n    return dataproc_submit_pyspark_op(\n        project_id=project,\n        region=region,\n        cluster_name=cluster_name,\n        main_python_file_uri=os.path.join(_PYSRC_PREFIX, 'analyze_run.py'),\n        args=['--output', str(output), '--train', str(train_data), '--schema', str(schema)]\n    )\n\n\ndef dataproc_transform_op(\n        project,\n        region,\n        cluster_name,\n        train_data,\n        eval_data,\n        target,\n        analysis,\n        output\n):\n    \"\"\"Submit dataproc transform as a pyspark job.\n    :param project: GCP project ID.\n    :param region: Which zone to run this analyze.\n    :param cluster_name: Name of the cluster.\n    :param train_data: GCS path to the training data.\n    :param eval_data: GCS path of the eval csv file.\n    :param target: Target column name.\n    :param analysis: GCS path of the analysis results\n    :param output: GCS path to use for output.\n    \"\"\"\n\n    # Remove existing [output]/train and [output]/eval if they exist.\n    delete_directory_from_gcs(os.path.join(output, 'train'))\n    delete_directory_from_gcs(os.path.join(output, 'eval'))\n\n    return dataproc_submit_pyspark_op(\n        project_id=project,\n        region=region,\n        cluster_name=cluster_name,\n        main_python_file_uri=os.path.join(_PYSRC_PREFIX,\n                                          'transform_run.py'),\n        args=[\n            '--output',\n            str(output),\n            '--analysis',\n            str(analysis),\n            '--target',\n            str(target),\n            '--train',\n            str(train_data),\n            '--eval',\n            str(eval_data)\n        ])\n\n\ndef dataproc_train_op(\n        project,\n        region,\n        cluster_name,\n        train_data,\n        eval_data,\n        target,\n        analysis,\n        workers,\n        rounds,\n        output,\n        is_classification=True\n):\n    if is_classification:\n        config = 'gs://ml-pipeline-playground/trainconfcla.json'\n    else:\n        config = 'gs://ml-pipeline-playground/trainconfreg.json'\n\n    return dataproc_submit_spark_op(\n        project_id=project,\n        region=region,\n        cluster_name=cluster_name,\n        main_class=_TRAINER_MAIN_CLS,\n        spark_job=json.dumps({'jarFileUris': [_XGBOOST_PKG]}),\n        args=json.dumps([\n            str(config),\n            str(rounds),\n            str(workers),\n            str(analysis),\n            str(target),\n            str(train_data),\n            str(eval_data),\n            str(output)\n        ]))\n\n\ndef dataproc_predict_op(\n        project,\n        region,\n        cluster_name,\n        data,\n        model,\n        target,\n        analysis,\n        output\n):\n    return dataproc_submit_spark_op(\n        project_id=project,\n        region=region,\n        cluster_name=cluster_name,\n        main_class=_PREDICTOR_MAIN_CLS,\n        spark_job=json.dumps({'jarFileUris': [_XGBOOST_PKG]}),\n        args=json.dumps([\n            str(model),\n            str(data),\n            str(analysis),\n            str(target),\n            str(output)\n        ]))\n\n\n# =======================================================================\n\n@dsl.pipeline(\n    name='XGBoost Trainer',\n    description='A trainer that does end-to-end distributed training for XGBoost models.'\n)\ndef xgb_train_pipeline(\n        output='gs://{{kfp-default-bucket}}',\n        project='{{kfp-project-id}}',\n        diagnostic_mode='HALT_ON_ERROR',\n        rounds=5,\n):\n    output_template = str(output) + '/' + dsl.RUN_ID_PLACEHOLDER + '/data'\n    region = 'us-central1'\n    workers = 2\n    quota_check = [{'region': region, 'metric': 'CPUS', 'quota_needed': 12.0}]\n    train_data = 'gs://ml-pipeline-playground/sfpd/train.csv'\n    eval_data = 'gs://ml-pipeline-playground/sfpd/eval.csv'\n    schema = 'gs://ml-pipeline-playground/sfpd/schema.json'\n    true_label = 'ACTION'\n    target = 'resolution'\n    required_apis = 'dataproc.googleapis.com'\n    cluster_name = 'xgb-%s' % dsl.RUN_ID_PLACEHOLDER\n\n    # Current GCP pyspark/spark op do not provide outputs as return values, instead,\n    # we need to use strings to pass the uri around.\n    analyze_output = output_template\n    transform_output_train = os.path.join(output_template, 'train', 'part-*')\n    transform_output_eval = os.path.join(output_template, 'eval', 'part-*')\n    train_output = os.path.join(output_template, 'train_output')\n    predict_output = os.path.join(output_template, 'predict_output')\n\n    _diagnose_me_op = diagnose_me_op(\n        bucket=output,\n        execution_mode=diagnostic_mode,\n        project_id=project,\n        target_apis=required_apis,\n        quota_check=quota_check)\n\n    with dsl.ExitHandler(exit_op=dataproc_delete_cluster_op(\n            project_id=project,\n            region=region,\n            name=cluster_name\n    )):\n        _create_cluster_op = dataproc_create_cluster_op(\n            project_id=project,\n            region=region,\n            name=cluster_name,\n            initialization_actions=[\n                os.path.join(_PYSRC_PREFIX,\n                             'initialization_actions.sh'),\n            ],\n            image_version='1.2'\n        ).after(_diagnose_me_op)\n\n        _analyze_op = dataproc_analyze_op(\n            project=project,\n            region=region,\n            cluster_name=cluster_name,\n            schema=schema,\n            train_data=train_data,\n            output=output_template\n        ).after(_create_cluster_op).set_display_name('Analyzer')\n\n        _transform_op = dataproc_transform_op(\n            project=project,\n            region=region,\n            cluster_name=cluster_name,\n            train_data=train_data,\n            eval_data=eval_data,\n            target=target,\n            analysis=analyze_output,\n            output=output_template\n        ).after(_analyze_op).set_display_name('Transformer')\n\n        _train_op = dataproc_train_op(\n            project=project,\n            region=region,\n            cluster_name=cluster_name,\n            train_data=transform_output_train,\n            eval_data=transform_output_eval,\n            target=target,\n            analysis=analyze_output,\n            workers=workers,\n            rounds=rounds,\n            output=train_output\n        ).after(_transform_op).set_display_name('Trainer')\n\n        _predict_op = dataproc_predict_op(\n            project=project,\n            region=region,\n            cluster_name=cluster_name,\n            data=transform_output_eval,\n            model=train_output,\n            target=target,\n            analysis=analyze_output,\n            output=predict_output\n        ).after(_train_op).set_display_name('Predictor')\n\n        _cm_op = confusion_matrix_op(\n            predictions=os.path.join(predict_output, 'part-*.csv'),\n            output_dir=output_template\n        ).after(_predict_op)\n\n        _roc_op = roc_op(\n            predictions_dir=os.path.join(predict_output, 'part-*.csv'),\n            true_class=true_label,\n            true_score_column=true_label,\n            output_dir=output_template\n        ).after(_predict_op)\n\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(xgb_train_pipeline, __file__ + '.yaml')\n"
  },
  {
    "repo": "actions-marketplace-validations/f6wbl6_kubeflow-pipelines-deploy-action",
    "file_path": "example/example_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/actions-marketplace-validations/f6wbl6_kubeflow-pipelines-deploy-action/master/example/example_pipeline.py",
    "content": "from kfp import dsl\nfrom kfp.components import InputPath, OutputPath, func_to_container_op\n\n\ndef path_csv_pipeline(github_sha: str):\n    \"\"\"Making arbitrary Dataframe with specified columns and rows\"\"\"\n\n    @func_to_container_op\n    def make_csv(n_cols: int, n_rows: int, output_csv_path: OutputPath(\"CSV\")):\n        import subprocess\n        import random\n\n        subprocess.run([\"pip\", \"install\", \"pandas\"])\n        import pandas as pd\n\n        # make data\n        data = [\n            [random.random() for _ in range(n_cols)] for __ in range(n_rows)\n        ]\n        columns = [f\"col_{i}\" for i in range(n_cols)]\n        index = [f\"idx_{i}\" for i in range(n_rows)]\n        df = pd.DataFrame(\n            data=data,\n            columns=columns,\n            index=index,\n        )\n        df.to_csv(output_csv_path, index=True)\n        print(f\"File path: {output_csv_path}\")\n\n    @func_to_container_op\n    def read_csv(input_csv_path: InputPath(\"CSV\")):\n        import subprocess\n\n        subprocess.run([\"pip\", \"install\", \"pandas\"])\n        import pandas as pd\n\n        df = pd.read_csv(input_csv_path, index_col=0)\n        print(f\"input_csv_path: {input_csv_path}\")\n        print(f\"type: {type(input_csv_path)}\")\n        print(df.head())\n\n    # pipeline\n    @dsl.pipeline(\n        name=\"Sample pipeline\", description=\"Make a csv file and read it.\"\n    )\n    def pipeline(n_cols: int = 5, n_rows: int = 3):\n        make_csv_task = make_csv(n_cols, n_rows)\n        read_csv(input_csv=make_csv_task.outputs[\"output_csv\"])\n\n    return pipeline\n"
  },
  {
    "repo": "Abeshith/KubeFlow-HandsOn",
    "file_path": "pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/Abeshith/KubeFlow-HandsOn/main/pipeline.py",
    "content": "from kfp import dsl\r\nfrom kfp import compiler\r\nfrom kfp.dsl import Input, Output, Dataset, Model, component\r\nfrom typing import Dict, List\r\n\r\n\r\n### Step 1: Load DataSet \r\n@dsl.component(base_image=\"python:3.9\")\r\ndef load_data(output_csv : Output[Dataset]):\r\n    import subprocess\r\n    subprocess.run([\"pip\", \"install\", \"pandas\",\"scikit-learn\"], check=True)\r\n\r\n    import pandas as pd\r\n    from sklearn.datasets import load_iris\r\n\r\n    ## Load the iris dataset\r\n    data = load_iris()\r\n    df = pd.DataFrame(data.data, columns=data.feature_names)\r\n    df['target'] = data.target\r\n\r\n    ## save the dataset to a CSV file\r\n    df.to_csv(output_csv.path, index=False)\r\n\r\n### Step 2: Data Preprocessing\r\n@dsl.component(base_image=\"python:3.9\")\r\ndef preprocess_data(input_csv: Input[Dataset], output_train: Output[Dataset],output_test: Output[Dataset],\r\n                    output_ytrain: Output[Dataset], output_ytest: Output[Dataset]):\r\n    import subprocess\r\n    subprocess.run([\"pip\", \"install\", \"pandas\",\"scikit-learn\"], check=True)\r\n\r\n    import pandas as pd\r\n    from sklearn.preprocessing import StandardScaler\r\n    from sklearn.model_selection import train_test_split\r\n\r\n    df = pd.read_csv(input_csv.path)\r\n\r\n    # Debug: Check for NaN values\r\n    print(\"Initial dataset shape:\", df.shape)\r\n    print(\"Missing values before preprocessing:\\n\", df.isnull().sum())\r\n\r\n    # Handle missing values\r\n    if df.isnull().values.any():\r\n        print(\"Missing values detected. Handling them...\")\r\n        df = df.dropna()  # Drop rows with any NaN values\r\n    \r\n    # Validate that there are no NaNs in the target column\r\n    assert not df['target'].isnull().any(), \"Target column contains NaN values after handling missing values.\"\r\n\r\n    features = df.drop(columns=['target'])\r\n    target = df['target']\r\n\r\n    # Standardize features\r\n    scaler = StandardScaler()\r\n    scaled_features = scaler.fit_transform(features)\r\n\r\n    # Train-test split\r\n    X_train, X_test, y_train, y_test = train_test_split(scaled_features, target, test_size=0.2, random_state=42)\r\n\r\n    # Debug: Validate splits\r\n    print(\"Shapes after train-test split:\")\r\n    print(\"X_train:\", X_train.shape, \"X_test:\", X_test.shape) \r\n    print(\"y_train:\", y_train.shape, \"y_test:\", y_test.shape)\r\n    print(\"Missing values in y_train:\", y_train.isnull().sum())\r\n\r\n    # Ensure no NaNs in the split data\r\n    assert not y_train.isnull().any(), \"y_train contains NaN values.\"\r\n    assert not y_test.isnull().any(), \"y_test contains NaN values.\"\r\n\r\n    # Create DataFrames for train and test sets\r\n    X_train_df = pd.DataFrame(X_train, columns=features.columns)\r\n    print(\"X_train_df:\", X_train_df) \r\n\r\n    y_train_df = pd.DataFrame(y_train) \r\n    print(\"y_train_df: \", y_train_df)  \r\n\r\n    X_test_df = pd.DataFrame(X_test, columns=features.columns)\r\n    print(\"X_test_df:\", X_test_df) \r\n\r\n    y_test_df = pd.DataFrame(y_test) \r\n    print(\"y_test_df: \", y_test_df) \r\n\r\n    # Save processed train and test data\r\n    X_train_df.to_csv(output_train.path, index=False)  \r\n    X_test_df.to_csv(output_test.path, index=False)\r\n\r\n    y_train_df.to_csv(output_ytrain.path, index=False)  \r\n    y_test_df.to_csv(output_ytest.path, index=False) \r\n\r\n### Step 3: Train Model \r\n@dsl.component(base_image=\"python:3.9\")\r\ndef train_model(train_data: Input[Dataset], ytrain_data: Input[Dataset], model_output: Output[Model]):\r\n\r\n    import subprocess\r\n    subprocess.run([\"pip\", \"install\", \"pandas\", \"scikit-learn\", \"joblib\"], check=True)\r\n\r\n    import pandas as pd\r\n    from sklearn.linear_model import LogisticRegression\r\n    from joblib import dump\r\n\r\n    # Load training data\r\n    train_df = pd.read_csv(train_data.path)\r\n    print(\"Shape of train_df:\", train_df.shape)\r\n    print(\"train_df:\", train_df)\r\n    X_train = train_df \r\n\r\n    y_train = pd.read_csv(ytrain_data.path)\r\n    print(\"Shape of ytrain_df:\", y_train.shape)\r\n    print(\"y_train_df:\", y_train)\r\n\r\n    # Debug: Validate splits\r\n    print(\"Shapes of X_train and y_train: \")\r\n    print(\"X_train:\", X_train.shape)\r\n    print(\"y_train:\", y_train.shape) \r\n    print(\"Missing values in X_train:\", X_train.isnull().sum())\r\n    print(\"Missing values in y_train:\", y_train.isnull().sum()) \r\n\r\n    # Ensure no NaN values\r\n    assert not X_train.isnull().values.any(), \"X_train contains NaN values.\"\r\n    assert not y_train.isnull().values.any(), \"y_train contains NaN values.\" \r\n\r\n    # Train model\r\n    model = LogisticRegression()\r\n    model.fit(X_train, y_train)\r\n\r\n    # Save model\r\n    dump(model, model_output.path)\r\n\r\n### Step 4: Evaluate Model\r\n@dsl.component(base_image=\"python:3.9\")\r\ndef evaluate_model(test_data: Input[Dataset], ytest_data: Input[Dataset], model: Input[Model], metrics_output: Output[Dataset]):\r\n    import subprocess\r\n    subprocess.run([\"pip\", \"install\", \"pandas\", \"scikit-learn\", \"matplotlib\", \"joblib\"], check=True)\r\n\r\n    import pandas as pd\r\n    from sklearn.metrics import classification_report, confusion_matrix\r\n    import matplotlib.pyplot as plt\r\n    from joblib import load\r\n\r\n    # Load test data\r\n    X_test = pd.read_csv(test_data.path)\r\n\r\n    y_test = pd.read_csv(ytest_data.path)  \r\n\r\n    # Load model\r\n    model = load(model.path)\r\n\r\n    # Predict\r\n    y_pred = model.predict(X_test)\r\n\r\n    # Generate metrics\r\n    report = classification_report(y_test, y_pred, output_dict=True)\r\n    cm = confusion_matrix(y_test, y_pred)\r\n\r\n    # Save metrics to a file\r\n    metrics_path = metrics_output.path\r\n    with open(metrics_path, 'w') as f:\r\n        f.write(str(report))\r\n\r\n    # Plot confusion matrix\r\n    plt.figure(figsize=(8, 6))\r\n    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\r\n    plt.title('Confusion Matrix')\r\n    plt.colorbar()\r\n    plt.xlabel('Predicted Label')\r\n    plt.ylabel('True Label')\r\n    plt.savefig(metrics_path.replace('.txt', '.png'))\r\n\r\n### Define the pipeline\r\n@dsl.pipeline(name=\"ml-pipeline\")\r\ndef ml_pipeline():\r\n    # Step 1: Load Dataset\r\n    load_op = load_data()\r\n\r\n    # Step 2: Preprocess Data\r\n    preprocess_op = preprocess_data(input_csv=load_op.outputs[\"output_csv\"])\r\n\r\n    # Step 3: Train Model\r\n    train_op = train_model(train_data=preprocess_op.outputs[\"output_train\"], ytrain_data=preprocess_op.outputs[\"output_ytrain\"])\r\n\r\n    # Step 4: Evaluate Model\r\n    evaluate_op = evaluate_model(test_data=preprocess_op.outputs[\"output_test\"], ytest_data=preprocess_op.outputs[\"output_ytest\"], model=train_op.outputs[\"model_output\"]) \r\n\r\n### Compile the pipeline\r\nif __name__ == \"__main__\":\r\n    compiler.Compiler().compile(pipeline_func=ml_pipeline, package_path=\"kubeflow_pipeline.yaml\")\r\n"
  },
  {
    "repo": "bbrowning/docling-kfp-demo",
    "file_path": "docling_convert_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/bbrowning/docling-kfp-demo/main/docling_convert_pipeline.py",
    "content": "# SPDX-License-Identifier: Apache-2.0\n\nfrom typing import List\n\nfrom kfp import dsl\n\nPYTHON_BASE_IMAGE = \"python:3.10\"\n\n@dsl.component(\n    base_image=PYTHON_BASE_IMAGE,\n    packages_to_install=[\"gitpython\"],\n)\ndef import_test_pdfs(\n    output_path: dsl.OutputPath(\"Directory\"),\n):\n    import os\n    import shutil\n    from git import Repo\n\n    docling_github_repo = \"https://github.com/DS4SD/docling/\"\n    full_repo_path = os.path.join(output_path, \"docling\")\n    Repo.clone_from(docling_github_repo, full_repo_path, branch=\"v2.25.0\")\n\n    # Copy some tests pdf up to the root of our output folder\n    pdfs_path = os.path.join(full_repo_path, \"tests\", \"data\", \"pdf\")\n    shutil.copytree(pdfs_path, output_path, dirs_exist_ok=True)\n\n    # Delete the rest of the docling repo, leaving only the PDFs\n    shutil.rmtree(full_repo_path)\n\n@dsl.component(\n    base_image=PYTHON_BASE_IMAGE,\n)\ndef create_pdf_splits(\n    input_path: dsl.InputPath(\"Directory\"),\n    num_splits: int,\n) -> List[List[str]]:\n    import pathlib\n\n    # Split our entire directory of pdfs into n batches, where n == num_splits\n    all_pdfs = [path.name for path in pathlib.Path(input_path).glob(\"*.pdf\")]\n    splits = [all_pdfs[i::num_splits] for i in range(num_splits)]\n    return splits\n\n# A Docling container built from\n# https://github.com/DS4SD/docling/blob/v2.25.0/Dockerfile\n@dsl.component(\n    base_image=\"quay.io/bbrowning/docling-kfp:v2.25.0\",\n)\ndef docling_convert(\n    input_path: dsl.InputPath(\"Directory\"),\n    pdf_split: List[str],\n    output_path: dsl.OutputPath(\"Directory\"),\n):\n    import pathlib\n    import os\n\n    from docling_core.types.doc import ImageRefMode\n    from docling.datamodel.base_models import ConversionStatus, InputFormat\n    from docling.datamodel.pipeline_options import PdfPipelineOptions\n    from docling.document_converter import DocumentConverter, PdfFormatOption\n\n    input_path = pathlib.Path(input_path)\n    output_path = pathlib.Path(output_path)\n    output_path.mkdir(parents=True, exist_ok=True)\n\n    input_pdfs = [input_path / name for name in pdf_split]\n\n    pipeline_options = PdfPipelineOptions()\n    pipeline_options.generate_page_images = True\n\n    doc_converter = DocumentConverter(\n        format_options={\n            InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n        }\n    )\n\n    conv_results = doc_converter.convert_all(\n        input_pdfs,\n        raises_on_error=True,\n    )\n\n    for conv_res in conv_results:\n        # TODO: handle errors, record success/failure somewhere - via\n        # calling some API, writing to some shared storage, or\n        # something else that each parallel task can do independently\n        doc_filename = conv_res.input.file.stem\n        output_json_path = pathlib.Path(output_path) / f\"{doc_filename}.json\"\n        conv_res.document.save_as_json(\n            output_json_path,\n            image_mode=ImageRefMode.PLACEHOLDER,\n        )\n\n@dsl.pipeline()\ndef convert_pipeline():\n    importer = import_test_pdfs()\n\n    pdf_splits = create_pdf_splits(\n        input_path=importer.output,\n        num_splits=3,\n    )\n\n    with dsl.ParallelFor(pdf_splits.output) as pdf_split:\n        docling_convert(\n            input_path=importer.output,\n            pdf_split=pdf_split,\n        )\n\nif __name__ == '__main__':\n    import kfp\n    output_yaml = \"docling_pipeline.yaml\"\n    kfp.compiler.Compiler().compile(convert_pipeline, output_yaml)\n    print(f\"\\nDocling pipeline compiled to {output_yaml}\")\n"
  },
  {
    "repo": "PranavAI2050/Human-Activity-Classification",
    "file_path": "kubeflow_human_activity_recognition.py",
    "raw_url": "https://raw.githubusercontent.com/PranavAI2050/Human-Activity-Classification/main/kubeflow_human_activity_recognition.py",
    "content": "# Import the modules you will use\nimport kfp\n\n# For creating the pipeline\nfrom kfp.v2 import dsl\n\n# For building components\nfrom kfp.v2.dsl import component\n\n# Type annotations for the component artifacts\nfrom kfp.v2.dsl import (\n    Input,\n    Output,\n    Artifact,\n    Dataset,\n    Model,\n    Metrics\n)\n\nfrom kfp import compiler\n\n#data ingestion and formatting\n\n@component(\n    packages_to_install=[\"pandas\", \"openpyxl\", \"scikit-learn\", \"numpy\"],\n    base_image=\"python:3.8\",\n    output_component_file=\"clean_data_component.yaml\"\n)\ndef clean_data(path: str, output_csv: Output[Dataset]):\n    \n    def convert_to_float(x):\n        try:\n            return np.float(x)\n        except:\n            return 0.0\n    \n    column_names = ['user-id', 'activity', 'timestamp', 'x-acc', 'y-acc', 'z-acc']\n    df = pd.read_csv(path, header=None, names=column_names)\n    df['z-acc'].replace(regex=True, inplace=True, to_replace=r';', value=r'')\n    df['z-acc'] = df['z-acc'].apply(convert_to_float)\n    \n    df.dropna(axis=0, how='any', inplace=True)\n    df.to_csv(output_csv.path, index=False)\n\n    \n#spliting the data into test and train\n\n@component(\n    packages_to_install=[\"pandas\", \"scikit-learn\"],\n    output_component_file=\"split_data_component.yaml\"\n)\ndef split_data(input_csv: Input[Dataset], train_csv: Output[Dataset], test_csv: Output[Dataset]):\n    import pandas as pd\n    from sklearn.model_selection import train_test_split\n    df = pd.read_csv(input_csv.path)\n    \n    total_rows = len(df)\n    split_index = int(0.7 * total_rows)\n    \n    train_df = df.iloc[:split_index]   \n    test_df = df.iloc[split_index:] \n    \n    train_df.to_csv(train_csv.path, index=False)\n    test_df.to_csv(test_csv.path, index=False)\n    \n#converting the format of data to time series for prediction of activity \n#based on the mode of activity in respective window\n\n@component(\n    packages_to_install=[\"pandas\", \"numpy\", \"scipy\", \"scikit-learn\"],\n    output_component_file=\"transform_data_component.yaml\"\n)\ndef preprocess_data(\n    input_train_csv: Input[Dataset], \n    input_test_csv: Input[Dataset], \n    output_train_x: Output[Artifact], \n    output_test_x: Output[Artifact],\n    output_train_y: Output[Artifact], \n    output_test_y: Output[Artifact]\n):\n    from scipy import stats\n    from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n    import numpy as np\n    import pandas as pd\n    import pickle\n\n    LABELS = ['Downstairs', 'Jogging', 'Sitting', 'Standing', 'Upstairs', 'Walking']\n    TIME_PERIODS = 80\n    STEP_DISTANCE = 40\n    LABEL = 'activity'\n    N_FEATURES = 3\n    acc_cols = ['x-acc', 'y-acc', 'z-acc']\n\n    train = pd.read_csv(input_train_csv.path)\n    test = pd.read_csv(input_test_csv.path)\n\n    for col in acc_cols:\n        scaler = MinMaxScaler()\n        train[col] = scaler.fit_transform(train[[col]])\n        test[col] = scaler.transform(test[[col]])\n\n    label_encoder = LabelEncoder()\n    train.loc[:, LABEL] = label_encoder.fit_transform(train[LABEL].values.ravel())\n    test.loc[:, LABEL] = label_encoder.transform(test[LABEL].values.ravel())\n\n    def create_segments_and_labels(df, time_period, step_distance, label_name):\n        segments = []\n        labels = []\n        for i in range(0, len(df) - time_period, step_distance):\n            xs = df['x-acc'].values[i: i + time_period]\n            ys = df['y-acc'].values[i: i + time_period]\n            zs = df['z-acc'].values[i: i + time_period]\n\n            label = stats.mode(df[label_name][i: i + time_period])[0][0]\n            segments.append([xs, ys, zs])\n            labels.append(label)\n        reshaped_segments = np.asarray(segments, dtype=np.float32).reshape(-1, time_period, N_FEATURES)\n        labels = np.asarray(labels)\n        return reshaped_segments, labels\n\n    X_train, Y_train = create_segments_and_labels(train, TIME_PERIODS, STEP_DISTANCE, LABEL)\n    X_test, Y_test = create_segments_and_labels(test, TIME_PERIODS, STEP_DISTANCE, LABEL)\n\n    with open(output_train_x.path, 'wb') as f:\n        pickle.dump(X_train, f)\n    with open(output_test_x.path, 'wb') as f:\n        pickle.dump(X_test, f)\n\n    with open(output_train_y.path, 'wb') as f:\n        pickle.dump(Y_train, f)\n    with open(output_test_y.path, 'wb') as f:\n        pickle.dump(Y_test, f)\n\n#tranning a conv-1d model on the train_data \n\n@component(\n    packages_to_install=[\"tensorflow\", \"pandas\"],\n    output_component_file=\"train_model_component.yaml\"\n)\ndef train_model(\n    input_train_x: Input[Artifact], \n    input_train_y: Input[Artifact], \n    output_model: Output[Model], \n    output_history: Output[Artifact]\n):\n    import tensorflow as tf\n    from tensorflow.keras import models, layers\n    from tensorflow.keras.utils import to_categorical\n    import pickle\n\n    with open(input_train_x.path, \"rb\") as file:\n        train_X = pickle.load(file)\n\n    with open(input_train_y.path, \"rb\") as file:\n        train_Y = pickle.load(file)\n\n    Y_one_hot = to_categorical(train_Y)\n\n    def model_builder(train_X):\n        model = models.Sequential()\n        model.add(layers.Conv1D(160, 12, input_shape=(train_X.shape[1], train_X.shape[2]), activation='relu'))\n        model.add(layers.Conv1D(128, 10, activation='relu'))\n        model.add(layers.Conv1D(96, 8, activation='relu'))\n        model.add(layers.Conv1D(64, 6, activation='relu'))\n        model.add(layers.GlobalMaxPooling1D())\n        model.add(layers.Dropout(0.5))\n        model.add(layers.Dense(6, activation='softmax'))\n        print(model.summary())\n        return model\n\n    model = model_builder(train_X)\n    model.compile(optimizer='rmsprop',\n                  loss='categorical_crossentropy',\n                  metrics=['accuracy'])\n\n    history = model.fit(train_X, Y_one_hot, epochs=25, batch_size=1024)\n    model.save(output_model.path)\n\n    with open(output_history.path, \"wb\") as file:\n        pickle.dump(history.history, file)\n\n#evaluation the model on test data and return metrics as logs of pipeline output\n\n@component(\n    packages_to_install=[\"tensorflow\", \"pandas\"],\n    output_component_file=\"eval_model_component.yaml\"\n)\ndef eval_model(\n    input_model: Input[Model], \n    input_history: Input[Artifact], \n    input_test_x: Input[Artifact], \n    input_test_y: Input[Artifact], \n    MLPipeline_Metrics: Output[Metrics]\n):\n    import tensorflow as tf\n    from tensorflow.keras.utils import to_categorical\n    import pickle\n\n    model = tf.keras.models.load_model(input_model.path)\n\n    with open(input_test_x.path, \"rb\") as file:\n        test_X = pickle.load(file)\n\n    with open(input_test_y.path, \"rb\") as file:\n        test_Y = pickle.load(file)\n\n    Y_one_hot = to_categorical(test_Y)\n\n    loss_value, accuracy = model.evaluate(test_X, Y_one_hot)\n    output_string = f\"Loss: {loss_value:.4f}, Accuracy: {accuracy:.4f}\"\n\n    MLPipeline_Metrics.log_metric(\"categorical_crossentropy_loss\", loss_value)\n    MLPipeline_Metrics.log_metric(\"accuracy\", accuracy)\n\n#completely defining the pipeline dag /structure\n\n@dsl.pipeline(\n    name=\"Human Activity Recognition Pipeline\",\n)\ndef kfp_pipeline(data_path: str):\n    \n    clean_data_task = clean_data(path=data_path)\n    \n    split_data_task = split_data(input_csv=clean_data_task.outputs['output_csv'])\n    \n    preprocess_data_task = preprocess_data(\n        input_train_csv=split_data_task.outputs['train_csv'],\n        input_test_csv=split_data_task.outputs['test_csv']\n    )\n    \n    train_model_task = train_model(\n        input_train_x=preprocess_data_task.outputs[\"output_train_x\"],\n        input_train_y=preprocess_data_task.outputs[\"output_train_y\"]\n    )\n    \n    eval_model_task = eval_model(\n        input_model=train_model_task.outputs[\"output_model\"],\n        input_history=train_model_task.outputs[\"output_history\"],\n        input_test_x=preprocess_data_task.outputs[\"output_test_x\"],\n        input_test_y=preprocess_data_task.outputs[\"output_test_y\"]\n    )\n\n\n#compiling the pipeline into yaml to run on kubeflow\nif __name__ == \"__main__\":\n    compiler.Compiler().compile(kfp_pipeline, 'pipeline.yaml')\n"
  },
  {
    "repo": "mozilla-ai/kfp-discovery",
    "file_path": "doc-to-podcast/generate_pipeline_manual.py",
    "raw_url": "https://raw.githubusercontent.com/mozilla-ai/kfp-discovery/main/doc-to-podcast/generate_pipeline_manual.py",
    "content": "from kfp import dsl, compiler, components\n\n\ndownload_document = components.load_component_from_file(\"./_components/downloader.yaml\")\ntransform_document = components.load_component_from_file(\"./_components/transformer.yaml\")\nscriptwriter = components.load_component_from_file(\"./_components/scriptwriter.yaml\")\nperformer = components.load_component_from_file(\"./_components/performer.yaml\")\n\n\n@dsl.pipeline\ndef document_to_podcast(\n    document_url: str,\n    file_type: str = None,\n    audio_format: str = None,\n    host_name: str = None,\n    cohost_name: str = None,\n    host_voice_profile: str = None,\n    cohost_voice_profile: str = None,\n    text_to_text_model: str = None,\n    text_to_speech_model: str = None,\n):\n    \"\"\"Convert a document to a podcast.\n\n    This pipeline downloads a document, processes it, converts it to a script,\n    and finally converts the script to speech (podcast).\n\n    Args:\n        :param document_url: Path to the input document.\n        :param file_type: The file type of the input document. e.g. .html, .txt, .pdf.\n        :param audio_format: Output podcast file type .e.g. WAV, MP3.\n        :param host_name: Name of the host.\n        :param cohost_name: Name of the co-host.\n        :param host_voice_profile: Voice profile for the host.\n        :param cohost_voice_profile: Voice profile for the co-host.\n        :param text_to_text_model: The text-to-speech model to use for script writing.\n        :param text_to_speech_model: The text-to-speech model to use for performing the podcast.\n    \"\"\"\n    download_document_step = download_document(document_url=document_url)\n    download_document_step.set_caching_options(False)\n\n    process_data_step = transform_document(\n        file_path=download_document_step.outputs['downloaded_file_path'],\n        file_type=file_type,\n    ).after(download_document_step)\n    process_data_step.set_caching_options(False)\n\n    scriptwriter_step = scriptwriter(\n        processed_document=process_data_step.outputs['processed_document'],\n        host_name=host_name,\n        cohost_name=cohost_name,\n        model=text_to_text_model,\n    ).after(process_data_step)\n    scriptwriter_step.set_accelerator_type(\"nvidia.com/gpu\")\n    scriptwriter_step.set_accelerator_limit(1)\n    scriptwriter_step.set_caching_options(False)\n\n    performer_step = performer(\n        podcast_script=scriptwriter_step.outputs['podcast_script'],\n        host_voice_profile=host_voice_profile,\n        cohost_voice_profile=cohost_voice_profile,\n        model=text_to_speech_model,\n        audio_format=audio_format,\n    ).after(scriptwriter_step)\n    performer_step.set_accelerator_type(\"nvidia.com/gpu\")\n    performer_step.set_accelerator_limit(1)\n    performer_step.set_caching_options(False)\n\n\nif __name__ == \"__main__\":\n    compiler.Compiler().compile(document_to_podcast, package_path='document_to_podcast.yaml')"
  },
  {
    "repo": "iampatgrady/terraform-vertexai-helloworld",
    "file_path": "pipeline/components.py",
    "raw_url": "https://raw.githubusercontent.com/iampatgrady/terraform-vertexai-helloworld/main/pipeline/components.py",
    "content": "# ./pipeline/components.py\nfrom kfp.dsl import component\n\n@component(\n    base_image=\"python:3.12\", # Specify a base image for reproducibility\n)\ndef produce_message_component(\n    input_text: str,\n) -> str:\n    \"\"\"\n    A simple KFP component that takes an input string, appends a message,\n    logs it, and returns the new string.\n    \"\"\"\n    # For real MLOps, you might load data, preprocess, train, or predict here.\n    # For this \"Hello World\", we just manipulate a string.\n    processed_message = f\"{input_text} - from KFP component\"\n    \n    return processed_message"
  },
  {
    "repo": "iampatgrady/terraform-vertexai-helloworld",
    "file_path": "pipeline/hello_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/iampatgrady/terraform-vertexai-helloworld/main/pipeline/hello_pipeline.py",
    "content": "# ./pipeline/hello_pipeline.py\nfrom kfp import dsl\nfrom .components import produce_message_component\n\n@dsl.pipeline(\n    name=\"minimal-hello-world-pipeline\",\n    description=\"A minimal Vertex AI pipeline that produces a Hello World message, orchestrated by Terraform.\"\n)\ndef minimal_hello_pipeline(\n    # Terraform will pass a value for this.\n    message_to_produce: str \n):\n    \"\"\"\n    Defines the Hello World KFP pipeline structure.\n    It consists of a single component that processes an input message.\n    \"\"\"\n    # Call the component, passing the pipeline parameter to its input.\n    producer_task = produce_message_component(\n        input_text=message_to_produce\n    )\n    # In more complex pipelines, you would chain multiple components here."
  },
  {
    "repo": "Yeshwanththota/MLOps_project5_mlflow_Dagshub_minikube_kubectl_Kubeflow_pipelines_Dockerhub",
    "file_path": "kubeflow_pipeline/mlops_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/Yeshwanththota/MLOps_project5_mlflow_Dagshub_minikube_kubectl_Kubeflow_pipelines_Dockerhub/main/kubeflow_pipeline/mlops_pipeline.py",
    "content": "import kfp\nfrom kfp import dsl\n\ndef data_processing_op():\n    return dsl.ContainerOp(\n        name='Data Processing',\n        image='yeshwanththota33/mlopsproject:latest',\n        command=['python', 'src/data_processing.py'],)\ndef model_training_op():\n    return dsl.ContainerOp(\n        name='Model Training',\n        image='yeshwanththota33/mlopsproject:latest',\n        command=['python', 'src/model_training.py'],)\n\n@dsl.pipeline(\n    name='MLOps Pipeline',\n    description='A MLOps pipeline for data processing and model training.'\n)\n\ndef mlops_pipeline():\n    data_processing = data_processing_op()\n    model_training = model_training_op().after(data_processing)\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(mlops_pipeline, 'mlops_pipeline.yaml')"
  },
  {
    "repo": "yuanchi2807/kfp-codeflare",
    "file_path": "kubeflowpipeline/kfp-ray.py",
    "raw_url": "https://raw.githubusercontent.com/yuanchi2807/kfp-codeflare/main/kubeflowpipeline/kfp-ray.py",
    "content": "# Copyright 2020 kubeflow.org\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom kfp import dsl\nfrom kfp import components\n\ndef deploy_ray_cluster():\n    print(\"lauching a Ray cluster when this KFP operator is deployed\")\n    from test import launch_ray\n    launch_ray()\n\n\ndeploy_ray_cluster_op = components.create_component_from_func(\n    deploy_ray_cluster,\n    base_image='us.icr.io/cil15-shared-registry/preprocessing-pipelines/kfp/kfp-codeflare:0')\n\n@dsl.pipeline(\n    name='test_KFP_deploy_ray_cluster',\n    description='Testing how to use KFP pipeline to deploy a Ray cluster.'\n)\ndef kfp_ray_deployment_pipeline():\n    deploy_ray_cluster_op()\n\n\nif __name__ == '__main__':\n    from kfp_tekton.compiler import TektonCompiler\n    TektonCompiler().compile(kfp_ray_deployment_pipeline, __file__.replace('.py', '.yaml'))\n"
  },
  {
    "repo": "AlexIoannides/kfp-component-lib",
    "file_path": "tests/test_pipelines.py",
    "raw_url": "https://raw.githubusercontent.com/AlexIoannides/kfp-component-lib/main/tests/test_pipelines.py",
    "content": "\"\"\"Tests that components can be assembled into pipeline DAGs that compile.\"\"\"\nfrom pathlib import Path\n\nfrom kfp import compiler, dsl\n\nfrom kfp_component_lib.components import make_numeric_dataset\n\n\n@dsl.pipeline\ndef synthetic_data_pipeline(n_rows: int = 1000) -> None:\n    \"\"\"Create synthetic datasets.\"\"\"\n    task_1 = make_numeric_dataset(n_rows=n_rows)\n    task_2 = make_numeric_dataset(n_rows=n_rows)\n    task_2.after(task_1)\n\n\ndef test_synthetic_data_pipeline_compiles():\n    compiled_pipeline_file = \"pipeline.json\"\n    try:\n        compiler.Compiler().compile(\n            pipeline_func=synthetic_data_pipeline, package_path=compiled_pipeline_file\n        )\n        assert True\n    except Exception:\n        assert False\n    finally:\n        compiled_pipeline_path = Path(compiled_pipeline_file)\n        if compiled_pipeline_path.exists():\n            compiled_pipeline_path.unlink()\n"
  },
  {
    "repo": "MOLOZISE/MLOps_Formation",
    "file_path": "kubeflow_flowers_train_to_mlflow_pipeline_yaml.py",
    "raw_url": "https://raw.githubusercontent.com/MOLOZISE/MLOps_Formation/main/kubeflow_flowers_train_to_mlflow_pipeline_yaml.py",
    "content": "\n# 1. kubeflow pipeline \ud615\uc2dd\uc744 \ub530\ub984\n# 2. mlflow\uc5d0\uc11c \ubaa8\ub378\uc774 \uc800\uc7a5\ub418\ub3c4\ub85d \ud568\n# 3. how : inference\uc5d0\uc11c \ubaa8\ub378 \ub85c\ub529 -> \ub85c\ub529 \ubd80\ubd84\ub9cc \ucd94\ucd9c\ud558\uc5ec \ub85c\ub529 \ud6c4 \ubc14\ub85c \uc800\uc7a5\ud558\ub294 \ubc29\uc2dd\n# 4. \uc0ac\uc804\uc791\uc5c5 : minio mlflow bucket \ub0b4 \ubaa8\ub378 \uc5c5\ub85c\ub529 -> \ubaa8\ub378 \ub85c\ub529 ->\n# mlflow \ud615\uc2dd\uc73c\ub85c \ubaa8\ub378 \uc800\uc7a5(\ud150\uc11c\ud50c\ub85c \ucf00\ub77c\uc2a4, \ud30c\uc774\ud1a0\uce58 \ud615\uc2dd\uc744 \ud655\uc778) -> Seldon Core\ub85c \ubc30\ud3ec\n\n\n# SVC \ubaa8\ub378 Train\n# import pandas as pd\n# from sklearn.datasets import load_iris\n# from sklearn.svm import SVC\n#\n# iris = load_iris()\n#\n# data = pd.DataFrame(iris[\"data\"], columns=iris[\"feature_names\"])\n# target = pd.DataFrame(iris[\"target\"], columns=[\"target\"])\n#\n# clf = SVC(kernel=\"rbf\")\n# clf.fit(data, target)\n\n\n# MLFlow Infos\n\n# from mlflow.models.signature import infer_signature\n# from mlflow.utils.environment import _mlflow_conda_env\n#\n# input_example = data.sample(1)\n# signature = infer_signature(data, clf.predict(data))\n# conda_env = _mlflow_conda_env(additional_pip_deps=[\"dill\", \"pandas\", \"scikit-learn\"])\n\n# Save MLFlow Infos\n\n# from mlflow.sklearn import save_model\n#\n# save_model(\n#     sk_model=clf,\n#     path=\"svc\",\n#     serialization_format=\"cloudpickle\",\n#     conda_env=conda_env,\n#     signature=signature,\n#     input_example=input_example,\n# )\n\n# MLFlow on Server\n\n# import mlflow\n#\n# with mlflow.start_run():\n#     mlflow.log_artifact(\"svc/\")\nfrom functools import partial\n\nimport kfp\nfrom kfp.components import InputPath, OutputPath, create_component_from_func\nfrom kfp.dsl import pipeline\n\n\n@partial(\n    create_component_from_func,\n    packages_to_install=[\"tflite-model-maker\", \"numpy\"],\n)\ndef load_flower_data( # gan data path\n        data_path: OutputPath(\"csv\"),\n        target_path: OutputPath(\"csv\"),\n):\n\n    import pandas as pd\n    from keras.datasets.fashion_mnist import load_data\n\n    # minio \uc811\uadfc?\n    import os\n    os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = \"http://minio-service.kubeflow.svc:9000\"\n    os.environ[\"AWS_ACCESS_KEY_ID\"] = \"minio\"\n    os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"minio123\"\n\n    # minio\ub0b4 mlflow \ud3f4\ub354\n    # flowers = pd.read_csv(\"flowers.csv\")\n    # iris = load_iris()\n    #\n    # data = pd.DataFrame(flowers[\"data\"], columns=iris[\"feature_names\"])\n    # target = pd.DataFrame(flowers[\"target\"], columns=[\"target\"])\n    #\n    # data.to_csv(data_path, index=False)\n    # target.to_csv(target_path, index=False)\n\n    data_path = tf.keras.utils.get_file(\n        'flower_photos',\n        'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n        untar=True)\n\n    data = DataLoader.from_folder(data_path)\n    train_data, test_data = data.split(0.9)\n\n\n@partial(\n    create_component_from_func,\n    packages_to_install=[\"dill\", \"pandas\", \"scikit-learn\", \"mlflow\"],\n)\ndef train_from_csv(\n        train_data_path: InputPath(\"csv\"),\n        train_target_path: InputPath(\"csv\"),\n        model_path: OutputPath(\"dill\"),\n        input_example_path: OutputPath(\"dill\"),\n        signature_path: OutputPath(\"dill\"),\n        conda_env_path: OutputPath(\"dill\"),\n        kernel: str,\n):\n    import dill\n    import pandas as pd\n    from sklearn.svm import SVC\n\n    from mlflow.models.signature import infer_signature\n    from mlflow.utils.environment import _mlflow_conda_env\n\n    train_data = pd.read_csv(train_data_path)\n    train_target = pd.read_csv(train_target_path)\n\n    clf = SVC(kernel=kernel)\n    clf.fit(train_data, train_target)\n\n    with open(model_path, mode=\"wb\") as file_writer:\n        dill.dump(clf, file_writer)\n\n    input_example = train_data.sample(1)\n    with open(input_example_path, \"wb\") as file_writer:\n        dill.dump(input_example, file_writer)\n\n    signature = infer_signature(train_data, clf.predict(train_data))\n    with open(signature_path, \"wb\") as file_writer:\n        dill.dump(signature, file_writer)\n\n    conda_env = _mlflow_conda_env(\n        additional_pip_deps=[\"dill\", \"pandas\", \"scikit-learn\"]\n    )\n    with open(conda_env_path, \"wb\") as file_writer:\n        dill.dump(conda_env, file_writer)\n\n\n@partial(\n    create_component_from_func,\n    packages_to_install=[\"dill\", \"pandas\", \"scikit-learn\", \"mlflow\", \"boto3\", \"tqdm\", ],\n)\ndef upload_sklearn_model_to_mlflow(\n        model_name: str,\n        model_path: InputPath(\"dill\"),\n        input_example_path: InputPath(\"dill\"),\n        signature_path: InputPath(\"dill\"),\n        conda_env_path: InputPath(\"dill\"),\n):\n    import os\n    import dill\n    from mlflow.sklearn import save_model\n    from mlflow.tensorflow import load_model\n\n    from mlflow.tracking.client import MlflowClient\n\n    os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = \"http://minio-service.kubeflow.svc:9000\"\n    os.environ[\"AWS_ACCESS_KEY_ID\"] = \"minio\"\n    os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"minio123\"\n\n    client = MlflowClient(\"http://mlflow-server-service.mlflow-system.svc:5000\")\n\n    with open(model_path, mode=\"rb\") as file_reader:\n        clf = dill.load(file_reader)\n\n    with open(input_example_path, \"rb\") as file_reader:\n        input_example = dill.load(file_reader)\n\n    with open(signature_path, \"rb\") as file_reader:\n        signature = dill.load(file_reader)\n\n    with open(conda_env_path, \"rb\") as file_reader:\n        conda_env = dill.load(file_reader)\n\n    # \uc5f0\uacb0\ub41c \uc0c1\ud0dc\n\n\n    save_model(\n        sk_model=clf,\n        path=model_name,\n        serialization_format=\"cloudpickle\",\n        conda_env=conda_env,\n        signature=signature,\n        input_example=input_example,\n    )\n    run = client.create_run(experiment_id=\"0\")\n    client.log_artifact(run.info.run_id, model_name)\n\n\n@pipeline(name=\"mlflow_pipeline\")\ndef mlflow_pipeline(kernel: str, model_name: str):\n    iris_data = load_iris_data()\n    model = train_from_csv(\n        train_data=iris_data.outputs[\"data\"],\n        train_target=iris_data.outputs[\"target\"],\n        kernel=kernel,\n    )\n    _ = upload_sklearn_model_to_mlflow(\n        model_name=model_name,\n        model=model.outputs[\"model\"],\n        input_example=model.outputs[\"input_example\"],\n        signature=model.outputs[\"signature\"],\n        conda_env=model.outputs[\"conda_env\"],\n    )\n\n\nif __name__ == \"__main__\":\n    kfp.compiler.Compiler().compile(mlflow_pipeline, \"mlflow_pipeline.yaml\")"
  },
  {
    "repo": "MOLOZISE/MLOps_Formation",
    "file_path": "kubeflow_mlflow_pre_trained_model_pipeline_yaml.py",
    "raw_url": "https://raw.githubusercontent.com/MOLOZISE/MLOps_Formation/main/kubeflow_mlflow_pre_trained_model_pipeline_yaml.py",
    "content": "\n# 1. kubeflow pipeline \ud615\uc2dd\uc744 \ub530\ub984\n# 2. mlflow\uc5d0\uc11c \ubaa8\ub378\uc774 \uc800\uc7a5\ub418\ub3c4\ub85d \ud568\n# 3. how : inference\uc5d0\uc11c \ubaa8\ub378 \ub85c\ub529 -> \ub85c\ub529 \ubd80\ubd84\ub9cc \ucd94\ucd9c\ud558\uc5ec \ub85c\ub529 \ud6c4 \ubc14\ub85c \uc800\uc7a5\ud558\ub294 \ubc29\uc2dd\n# 4. \uc0ac\uc804\uc791\uc5c5 : minio mlflow bucket \ub0b4 \ubaa8\ub378 \uc5c5\ub85c\ub529 -> \ubaa8\ub378 \ub85c\ub529 ->\n# mlflow \ud615\uc2dd\uc73c\ub85c \ubaa8\ub378 \uc800\uc7a5(\ud150\uc11c\ud50c\ub85c \ucf00\ub77c\uc2a4, \ud30c\uc774\ud1a0\uce58 \ud615\uc2dd\uc744 \ud655\uc778) -> Seldon Core\ub85c \ubc30\ud3ec\n\n\n# SVC \ubaa8\ub378 Train\n# import pandas as pd\n# from sklearn.datasets import load_iris\n# from sklearn.svm import SVC\n#\n# iris = load_iris()\n#\n# data = pd.DataFrame(iris[\"data\"], columns=iris[\"feature_names\"])\n# target = pd.DataFrame(iris[\"target\"], columns=[\"target\"])\n#\n# clf = SVC(kernel=\"rbf\")\n# clf.fit(data, target)\n\n\n# MLFlow Infos\n\n# from mlflow.models.signature import infer_signature\n# from mlflow.utils.environment import _mlflow_conda_env\n#\n# input_example = data.sample(1)\n# signature = infer_signature(data, clf.predict(data))\n# conda_env = _mlflow_conda_env(additional_pip_deps=[\"dill\", \"pandas\", \"scikit-learn\"])\n\n# Save MLFlow Infos\n\n# from mlflow.sklearn import save_model\n#\n# save_model(\n#     sk_model=clf,\n#     path=\"svc\",\n#     serialization_format=\"cloudpickle\",\n#     conda_env=conda_env,\n#     signature=signature,\n#     input_example=input_example,\n# )\n\n# MLFlow on Server\n\n# import mlflow\n#\n# with mlflow.start_run():\n#     mlflow.log_artifact(\"svc/\")\nfrom functools import partial\n\nimport kfp\nfrom kfp.components import InputPath, OutputPath, create_component_from_func\nfrom kfp.dsl import pipeline\n\n\n@partial(\n    create_component_from_func,\n    packages_to_install=[\"pandas\", \"scikit-learn\"],\n)\ndef load_gan_data( # gan data path\n        data_path: OutputPath(\"csv\"),\n        target_path: OutputPath(\"csv\"),\n):\n\n    import pandas as pd\n    from sklearn.datasets import load_iris\n\n    # minio \uc811\uadfc?\n    import os\n    os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = \"http://minio-service.kubeflow.svc:9000\"\n    os.environ[\"AWS_ACCESS_KEY_ID\"] = \"minio\"\n    os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"minio123\"\n\n    # minio\ub0b4 mlflow \ud3f4\ub354\n\n    iris = load_iris()\n\n    data = pd.DataFrame(iris[\"data\"], columns=iris[\"feature_names\"])\n    target = pd.DataFrame(iris[\"target\"], columns=[\"target\"])\n\n    data.to_csv(data_path, index=False)\n    target.to_csv(target_path, index=False)\n\n\n@partial(\n    create_component_from_func,\n    packages_to_install=[\"dill\", \"pandas\", \"scikit-learn\", \"mlflow\"],\n)\ndef train_from_csv(\n        train_data_path: InputPath(\"csv\"),\n        train_target_path: InputPath(\"csv\"),\n        model_path: OutputPath(\"dill\"),\n        input_example_path: OutputPath(\"dill\"),\n        signature_path: OutputPath(\"dill\"),\n        conda_env_path: OutputPath(\"dill\"),\n        kernel: str,\n):\n    import dill\n    import pandas as pd\n    from sklearn.svm import SVC\n\n    from mlflow.models.signature import infer_signature\n    from mlflow.utils.environment import _mlflow_conda_env\n\n    train_data = pd.read_csv(train_data_path)\n    train_target = pd.read_csv(train_target_path)\n\n    clf = SVC(kernel=kernel)\n    clf.fit(train_data, train_target)\n\n    with open(model_path, mode=\"wb\") as file_writer:\n        dill.dump(clf, file_writer)\n\n    input_example = train_data.sample(1)\n    with open(input_example_path, \"wb\") as file_writer:\n        dill.dump(input_example, file_writer)\n\n    signature = infer_signature(train_data, clf.predict(train_data))\n    with open(signature_path, \"wb\") as file_writer:\n        dill.dump(signature, file_writer)\n\n    conda_env = _mlflow_conda_env(\n        additional_pip_deps=[\"dill\", \"pandas\", \"scikit-learn\"]\n    )\n    with open(conda_env_path, \"wb\") as file_writer:\n        dill.dump(conda_env, file_writer)\n\n\n@partial(\n    create_component_from_func,\n    packages_to_install=[\"dill\", \"pandas\", \"scikit-learn\", \"mlflow\", \"boto3\", \"tqdm\", ],\n)\ndef upload_sklearn_model_to_mlflow(\n        model_name: str,\n        model_path: InputPath(\"dill\"),\n        input_example_path: InputPath(\"dill\"),\n        signature_path: InputPath(\"dill\"),\n        conda_env_path: InputPath(\"dill\"),\n):\n    import os\n    import dill\n    from mlflow.sklearn import save_model\n    from mlflow.tensorflow import load_model\n\n    from mlflow.tracking.client import MlflowClient\n\n    os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = \"http://minio-service.kubeflow.svc:9000\"\n    os.environ[\"AWS_ACCESS_KEY_ID\"] = \"minio\"\n    os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"minio123\"\n\n    client = MlflowClient(\"http://mlflow-server-service.mlflow-system.svc:5000\")\n\n    with open(model_path, mode=\"rb\") as file_reader:\n        clf = dill.load(file_reader)\n\n    with open(input_example_path, \"rb\") as file_reader:\n        input_example = dill.load(file_reader)\n\n    with open(signature_path, \"rb\") as file_reader:\n        signature = dill.load(file_reader)\n\n    with open(conda_env_path, \"rb\") as file_reader:\n        conda_env = dill.load(file_reader)\n\n    # \uc5f0\uacb0\ub41c \uc0c1\ud0dc\n\n\n    save_model(\n        sk_model=clf,\n        path=model_name,\n        serialization_format=\"cloudpickle\",\n        conda_env=conda_env,\n        signature=signature,\n        input_example=input_example,\n    )\n    run = client.create_run(experiment_id=\"0\")\n    client.log_artifact(run.info.run_id, model_name)\n\n\n@pipeline(name=\"mlflow_pipeline\")\ndef mlflow_pipeline(kernel: str, model_name: str):\n    iris_data = load_iris_data()\n    model = train_from_csv(\n        train_data=iris_data.outputs[\"data\"],\n        train_target=iris_data.outputs[\"target\"],\n        kernel=kernel,\n    )\n    _ = upload_sklearn_model_to_mlflow(\n        model_name=model_name,\n        model=model.outputs[\"model\"],\n        input_example=model.outputs[\"input_example\"],\n        signature=model.outputs[\"signature\"],\n        conda_env=model.outputs[\"conda_env\"],\n    )\n\n\nif __name__ == \"__main__\":\n    kfp.compiler.Compiler().compile(mlflow_pipeline, \"mlflow_pipeline.yaml\")"
  },
  {
    "repo": "MOLOZISE/MLOps_Formation",
    "file_path": "01_3_5_8_example/example_kubeflow_pipeline_yaml.py",
    "raw_url": "https://raw.githubusercontent.com/MOLOZISE/MLOps_Formation/main/01_3_5_8_example/example_kubeflow_pipeline_yaml.py",
    "content": "import kfp\nfrom kfp.components import create_component_from_func\nfrom kfp.dsl import pipeline\n\n@create_component_from_func\ndef print_and_return_number(number: int) -> int:\n    print(number)\n    return number\n\n@create_component_from_func\ndef sum_and_print_numbers(number_1: int, number_2: int):\n    print(number_1 + number_2)\n\n@pipeline(name=\"example_pipeline\")\ndef example_pipeline(number_1: int, number_2: int):\n    number_1_result = print_and_return_number(number_1)\n    number_2_result = print_and_return_number(number_2)\n    sum_result = sum_and_print_numbers(\n        number_1=number_1_result.output, number_2=number_2_result.output\n    )\n\nif __name__ == \"__main__\":\n    kfp.compiler.Compiler().compile(example_pipeline, \"example_pipeline.yaml\")"
  },
  {
    "repo": "MOLOZISE/MLOps_Formation",
    "file_path": "02_Iris_example/complex_kubeflow_pipeline_yaml.py",
    "raw_url": "https://raw.githubusercontent.com/MOLOZISE/MLOps_Formation/main/02_Iris_example/complex_kubeflow_pipeline_yaml.py",
    "content": "from kfp.components import InputPath, OutputPath, create_component_from_func\nfrom functools import partial\n\n@partial(\n    create_component_from_func,\n    packages_to_install=[\"dill==0.3.4\", \"pandas==1.3.4\", \"scikit-learn==1.0.1\"],\n)\ndef train_from_csv(\n    train_data_path: InputPath(\"csv\"),\n    train_target_path: InputPath(\"csv\"),\n    model_path: OutputPath(\"dill\"),\n    kernel: str,\n):\n    import dill\n    import pandas as pd\n\n    from sklearn.svm import SVC\n\n    train_data = pd.read_csv(train_data_path)\n    train_target = pd.read_csv(train_target_path)\n\n    clf = SVC(kernel=kernel)\n    clf.fit(train_data, train_target)\n\n    with open(model_path, mode=\"wb\") as file_writer:\n        dill.dump(clf, file_writer)\n\nfrom functools import partial\nfrom kfp.components import InputPath, OutputPath, create_component_from_func\n\n\n@partial(\n    create_component_from_func,\n    packages_to_install=[\"dill==0.3.4\", \"pandas==1.3.4\", \"scikit-learn==1.0.1\"],\n)\ndef load_iris_data(\n    data_path: OutputPath(\"csv\"),\n    target_path: OutputPath(\"csv\"),\n):\n    import pandas as pd\n    from sklearn.datasets import load_iris\n\n    iris = load_iris()\n\n    data = pd.DataFrame(iris[\"data\"], columns=iris[\"feature_names\"])\n    target = pd.DataFrame(iris[\"target\"], columns=[\"target\"])\n\n    data.to_csv(data_path, index=False)\n    target.to_csv(target_path, index=False)\n\n\n### \ud30c\uc774\ud504\ub77c\uc778\n\nfrom kfp.dsl import pipeline\n\n@pipeline(name=\"complex_pipeline\")\ndef complex_pipeline(kernel: str):\n    iris_data = load_iris_data()\n    model = train_from_csv(\n        train_data=iris_data.outputs[\"data\"],\n        train_target=iris_data.outputs[\"target\"],\n        kernel=kernel,\n    )\nimport kfp\n\nif __name__ == \"__main__\":\n    kfp.compiler.Compiler().compile(complex_pipeline, \"complex_pipeline.yaml\")\n\n# from functools import partial\n# from kfp.components import InputPath, OutputPath, create_component_from_func\n#\n# @partial(\n#     create_component_from_func,\n#     packages_to_install=[\"dill==0.3.4\", \"pandas==1.3.4\", \"scikit-learn==1.0.1\"],\n# )\n# def train_from_csv(\n#     train_data_path: InputPath(\"csv\"),\n#     train_target_path: InputPath(\"csv\"),\n#     model_path: OutputPath(\"dill\"),\n#     kernel: str,\n# ):\n#     import dill\n#     import pandas as pd\n#\n#     from sklearn.svm import SVC\n#\n#     train_data = pd.read_csv(train_data_path)\n#     train_target = pd.read_csv(train_target_path)\n#\n#     clf = SVC(kernel=kernel)\n#     clf.fit(train_data, train_target)\n#\n#     with open(model_path, mode=\"wb\") as file_writer:\n#         dill.dump(clf, file_writer)\n#\n# if __name__ == \"__main__\":\n#     train_from_csv.component_spec.save(\"train_from_csv.yaml\")"
  },
  {
    "repo": "MOLOZISE/MLOps_Formation",
    "file_path": "03_Iris_mlflow_example/mlfow_kubeflow_pipeline_yaml.py",
    "raw_url": "https://raw.githubusercontent.com/MOLOZISE/MLOps_Formation/main/03_Iris_mlflow_example/mlfow_kubeflow_pipeline_yaml.py",
    "content": "from functools import partial\n\nimport kfp\nfrom kfp.components import InputPath, OutputPath, create_component_from_func\nfrom kfp.dsl import pipeline\n\n\n@partial(\n    create_component_from_func,\n    packages_to_install=[\"pandas\", \"scikit-learn\"],\n)\ndef load_iris_data(\n        data_path: OutputPath(\"csv\"),\n        target_path: OutputPath(\"csv\"),\n):\n    import pandas as pd\n    from sklearn.datasets import load_iris\n\n    iris = load_iris()\n\n    data = pd.DataFrame(iris[\"data\"], columns=iris[\"feature_names\"])\n    target = pd.DataFrame(iris[\"target\"], columns=[\"target\"])\n\n    data.to_csv(data_path, index=False)\n    target.to_csv(target_path, index=False)\n\n\n@partial(\n    create_component_from_func,\n    packages_to_install=[\"dill\", \"pandas\", \"scikit-learn\", \"mlflow\"],\n)\ndef train_from_csv(\n        train_data_path: InputPath(\"csv\"),\n        train_target_path: InputPath(\"csv\"),\n        model_path: OutputPath(\"dill\"),\n        input_example_path: OutputPath(\"dill\"),\n        signature_path: OutputPath(\"dill\"),\n        conda_env_path: OutputPath(\"dill\"),\n        kernel: str,\n):\n    import dill\n    import pandas as pd\n    from sklearn.svm import SVC\n\n    from mlflow.models.signature import infer_signature\n    from mlflow.utils.environment import _mlflow_conda_env\n\n    train_data = pd.read_csv(train_data_path)\n    train_target = pd.read_csv(train_target_path)\n\n    clf = SVC(kernel=kernel)\n    clf.fit(train_data, train_target)\n\n    with open(model_path, mode=\"wb\") as file_writer:\n        dill.dump(clf, file_writer)\n\n    input_example = train_data.sample(1)\n    with open(input_example_path, \"wb\") as file_writer:\n        dill.dump(input_example, file_writer)\n\n    signature = infer_signature(train_data, clf.predict(train_data))\n    with open(signature_path, \"wb\") as file_writer:\n        dill.dump(signature, file_writer)\n\n    conda_env = _mlflow_conda_env(\n        additional_pip_deps=[\"dill\", \"pandas\", \"scikit-learn\"]\n    )\n    with open(conda_env_path, \"wb\") as file_writer:\n        dill.dump(conda_env, file_writer)\n\n\n@partial(\n    create_component_from_func,\n    packages_to_install=[\"dill\", \"pandas\", \"scikit-learn\", \"mlflow\", \"boto3\"],\n)\ndef upload_sklearn_model_to_mlflow(\n        model_name: str,\n        model_path: InputPath(\"dill\"),\n        input_example_path: InputPath(\"dill\"),\n        signature_path: InputPath(\"dill\"),\n        conda_env_path: InputPath(\"dill\"),\n):\n    import os\n    import dill\n    from mlflow.sklearn import save_model\n\n    from mlflow.tracking.client import MlflowClient\n\n    os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = \"http://minio-service.kubeflow.svc:9000\"\n    os.environ[\"AWS_ACCESS_KEY_ID\"] = \"minio\"\n    os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"minio123\"\n\n    client = MlflowClient(\"http://mlflow-server-service.mlflow-system.svc:5000\")\n\n    with open(model_path, mode=\"rb\") as file_reader:\n        clf = dill.load(file_reader)\n\n    with open(input_example_path, \"rb\") as file_reader:\n        input_example = dill.load(file_reader)\n\n    with open(signature_path, \"rb\") as file_reader:\n        signature = dill.load(file_reader)\n\n    with open(conda_env_path, \"rb\") as file_reader:\n        conda_env = dill.load(file_reader)\n\n    save_model(\n        sk_model=clf,\n        path=model_name,\n        serialization_format=\"cloudpickle\",\n        conda_env=conda_env,\n        signature=signature,\n        input_example=input_example,\n    )\n    run = client.create_run(experiment_id=\"0\")\n    client.log_artifact(run.info.run_id, model_name)\n\n\n@pipeline(name=\"mlflow_pipeline\")\ndef mlflow_pipeline(kernel: str, model_name: str):\n    iris_data = load_iris_data()\n    model = train_from_csv(\n        train_data=iris_data.outputs[\"data\"],\n        train_target=iris_data.outputs[\"target\"],\n        kernel=kernel,\n    )\n    _ = upload_sklearn_model_to_mlflow(\n        model_name=model_name,\n        model=model.outputs[\"model\"],\n        input_example=model.outputs[\"input_example\"],\n        signature=model.outputs[\"signature\"],\n        conda_env=model.outputs[\"conda_env\"],\n    )\n\n\nif __name__ == \"__main__\":\n    kfp.compiler.Compiler().compile(mlflow_pipeline, \"mlflow_pipeline.yaml\")"
  },
  {
    "repo": "MOLOZISE/MLOps_Formation",
    "file_path": "04_kogpt2_mlflow_example[failed]/kogpt2_mlflow_pipeline_yaml.py",
    "raw_url": "https://raw.githubusercontent.com/MOLOZISE/MLOps_Formation/main/04_kogpt2_mlflow_example%5Bfailed%5D/kogpt2_mlflow_pipeline_yaml.py",
    "content": "from functools import partial\n\nimport kfp\nfrom kfp.components import InputPath, OutputPath, create_component_from_func\nfrom kfp.dsl import pipeline\n\n\n@partial(\n    create_component_from_func,\n    base_image=\"ghcr.io/molozise/kogpt2:latest\",\n)\ndef kogpt2_app(\n):\n    import torch\n    import string\n    import streamlit as st\n    from transformers import GPT2LMHeadModel, PreTrainedTokenizerFast\n\n    @st.cache(allow_output_mutation=True)\n    def get_model():\n        model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n        model.eval()\n        return model\n\n    tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n                                                        bos_token='</s>',\n                                                        eos_token='</s>',\n                                                        unk_token='<unk>',\n                                                        pad_token='<pad>',\n                                                        mask_token='<mask>')\n\n    default_text = \"korean is always busy\"\n\n    N_SENT = 3\n\n    model = get_model()\n    st.title(\"KoGPT2 Demo Page(ver 2.0)\")\n\n    st.markdown(\"\"\"\n    ### model\n    | Model       |  # of params |   Type   | # of layers  | # of heads | ffn_dim | hidden_dims | \n    |--------------|:----:|:-------:|--------:|--------:|--------:|--------------:|\n    | `KoGPT2` |  125M  |  Decoder |   12     | 12      | 3072    | 768 | \n    ### sampling method\n    - greedy sampling\n    - max out length : 128/1,024\n    ## Conditional Generation\n    \"\"\")\n\n    text = st.text_area(\"Input Text:\", value=default_text)\n    st.write(text)\n    punct = ('!', '?', '.')\n\n    if text:\n        st.markdown(\"## Predict\")\n        with st.spinner('processing..'):\n            print(f'input > {text}')\n            input_ids = tokenizer(text)['input_ids']\n            gen_ids = model.generate(torch.tensor([input_ids]),\n                                     max_length=128,\n                                     repetition_penalty=2.0)\n            generated = tokenizer.decode(gen_ids[0, :].tolist()).strip()\n            if generated != '' and generated[-1] not in punct:\n                for i in reversed(range(len(generated))):\n                    if generated[i] in punct:\n                        break\n                generated = generated[:(i + 1)]\n            print(f'KoGPT > {generated}')\n        st.write(generated)\n\n@partial(\n    create_component_from_func,\n    base_image=\"ghcr.io/molozise/kogpt2:latest\",\n)\ndef kogpt2_upload_config(\n        model_path: OutputPath(\"dill\"),\n        input_example_path: OutputPath(\"dill\"),\n        signature_path: OutputPath(\"dill\"),\n        conda_env_path: OutputPath(\"dill\"),\n):\n    import torch\n    import string\n    from transformers import GPT2LMHeadModel, PreTrainedTokenizerFast\n\n    import dill\n    import pandas as pd\n\n    from mlflow.models.signature import infer_signature\n    from mlflow.utils.environment import _mlflow_conda_env\n\n    tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n                                                        bos_token='</s>',\n                                                        eos_token='</s>',\n                                                        unk_token='<unk>',\n                                                        pad_token='<pad>',\n                                                        mask_token='<mask>')\n\n    default_text = \"korean is always busy\"\n\n    N_SENT = 3\n    punct = ('!', '?', '.')\n\n    model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n    model.eval()\n\n    def generating(model, text):\n        input_ids = tokenizer(text)['input_ids']\n        gen_ids = model.generate(torch.tensor([input_ids]),\n                                 max_length=128,\n                                 repetition_penalty=2.0)\n        generated = tokenizer.decode(gen_ids[0, :].tolist()).strip()\n        if generated != '' and generated[-1] not in punct:\n            for i in reversed(range(len(generated))):\n                if generated[i] in punct:\n                    break\n            generated = generated[:(i + 1)]\n        return generated\n\n    with open(model_path, mode=\"wb\") as file_writer:\n        dill.dump(model, file_writer)\n\n    input_example = default_text\n    with open(input_example_path, \"wb\") as file_writer:\n        dill.dump(input_example, file_writer)\n\n    signature = infer_signature(default_text, generating(model, default_text))\n    with open(signature_path, \"wb\") as file_writer:\n        dill.dump(signature, file_writer)\n\n    conda_env = _mlflow_conda_env(\n        additional_pip_deps=[\"dill\", \"torch\", \"transformers\"]\n    )\n    with open(conda_env_path, \"wb\") as file_writer:\n        dill.dump(conda_env, file_writer)\n\n@partial(\n    create_component_from_func,\n    base_image=\"ghcr.io/molozise/kogpt2:latest\",\n)\ndef kogpt2_upload_to_mlflow(\n        model_name: str,\n        model_path: InputPath(\"dill\"),\n        input_example_path: InputPath(\"dill\"),\n        signature_path: InputPath(\"dill\"),\n        conda_env_path: InputPath(\"dill\"),\n):\n\n    import os\n    import dill\n    from mlflow.pytorch import save_model\n\n    from mlflow.tracking.client import MlflowClient\n\n    os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = \"http://minio-service.kubeflow.svc:9000\"\n    os.environ[\"AWS_ACCESS_KEY_ID\"] = \"minio\"\n    os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"minio123\"\n\n    client = MlflowClient(\"http://mlflow-server-service.mlflow-system.svc:5000\")\n\n    with open(model_path, mode=\"rb\") as file_reader:\n        kogpt2 = dill.load(file_reader)\n\n    with open(input_example_path, \"rb\") as file_reader:\n        input_example = dill.load(file_reader)\n\n    with open(signature_path, \"rb\") as file_reader:\n        signature = dill.load(file_reader)\n\n    with open(conda_env_path, \"rb\") as file_reader:\n        conda_env = dill.load(file_reader)\n\n    save_model(pytorch_model=kogpt2, path=model_name, conda_env=conda_env, signature=signature, pip_requirements=[\"torch\", \"transformers\"])\n\n    run = client.create_run(experiment_id=\"1\")\n    client.log_artifact(run.info.run_id, model_name)\n\n@pipeline(name=\"mlflow_pipeline kogpt2\")\ndef mlflow_pipeline(model_name: str):\n    _ = kogpt2_app()\n    model = kogpt2_upload_config()\n    _ = kogpt2_upload_to_mlflow(\n        model_name=model_name,\n        model=model.outputs[\"model\"],\n        input_example=model.outputs[\"input_example\"],\n        signature=model.outputs[\"signature\"],\n        conda_env=model.outputs[\"conda_env\"],\n    )\n\n\nif __name__ == \"__main__\":\n    kfp.compiler.Compiler().compile(mlflow_pipeline, \"mlflow_pipeline_kogpt2.yaml\")"
  },
  {
    "repo": "MOLOZISE/MLOps_Formation",
    "file_path": "flowers/flowers_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/MOLOZISE/MLOps_Formation/main/flowers/flowers_pipeline.py",
    "content": "import kfp\nfrom kfp import dsl\nfrom kfp import onprem\ndef preprocess_op(pvc_name, volume_name, volume_mount_path):\n    return dsl.ContainerOp(\n        name='Preprocess Data',\n        image='molozise/kfp-flowers-preprocess:latest',\n        arguments=['--data-path', volume_mount_path,\n                   '--img-size', 224],\n    ).apply(onprem.mount_pvc(pvc_name, volume_name=volume_name, volume_mount_path=volume_mount_path))\ndef hyp_op(pvc_name, volume_name, volume_mount_path, device):\n    return dsl.ContainerOp(\n        name='Hyperparameter Tuning',\n        image='molozise/kfp-flowers-hyperparameter:latest',\n        arguments=['--data-path', volume_mount_path,\n                   '--device', device],\n    ).apply(onprem.mount_pvc(pvc_name, volume_name=volume_name, volume_mount_path=volume_mount_path))\ndef train_op(pvc_name, volume_name, volume_mount_path, device):\n    return dsl.ContainerOp(\n        name='Train Model',\n        image='molozise/kfp-flowers-train:latest',\n        arguments=['--data-path', volume_mount_path,\n                   '--img-size', 224,\n                   '--model-name', 'surface-ConvNeXt-T',\n                   '--device', device]\n    ).apply(onprem.mount_pvc(pvc_name, volume_name=volume_name, volume_mount_path=volume_mount_path)).set_gpu_limit(4)\ndef test_op(pvc_name, volume_name, volume_mount_path, model_path, device):\n    return dsl.ContainerOp(\n        name='Test Model',\n        image='molozise/kfp-flowers-test:latest',\n        arguments=['--data-path', volume_mount_path,\n                   '--img-size', 224,\n                   '--model-path', model_path,\n                   '--device', device]\n    ).apply(onprem.mount_pvc(pvc_name, volume_name=volume_name, volume_mount_path=volume_mount_path)).set_gpu_limit(4)\n@dsl.pipeline(\n    name='Flowers Pipeline',\n    description=''\n)\ndef surface_pipeline(mode_hyp_train_test: str,\n                     preprocess_yes_no: str,\n                     model_path: str,\n                     device: str):\n    pvc_name = \"workspace-flowers\"\n    volume_name = 'pipeline'\n    volume_mount_path = '/home/jovyan'\n    with dsl.Condition(preprocess_yes_no == 'yes'):\n        _preprocess_op = preprocess_op(pvc_name, volume_name, volume_mount_path)\n    with dsl.Condition(mode_hyp_train_test == 'hyp'):\n        _hyp_op = hyp_op(pvc_name, volume_name, volume_mount_path, device).after(_preprocess_op)\n    with dsl.Condition(mode_hyp_train_test == 'train'):\n        _train_op = train_op(pvc_name, volume_name, volume_mount_path, device).after(_preprocess_op)\n    with dsl.Condition(mode_hyp_train_test == 'test'):\n        _train_op = test_op(pvc_name, volume_name, volume_mount_path, model_path, device).after(_preprocess_op)\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(surface_pipeline, './flowers_pipeline.yaml')"
  },
  {
    "repo": "MOLOZISE/MLOps_Formation",
    "file_path": "flowers_gcp/flowers_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/MOLOZISE/MLOps_Formation/main/flowers_gcp/flowers_pipeline.py",
    "content": "#!pip3 install -U kfp\nimport kfp\nimport kfp.components as comp\nfrom kfp import dsl\nfrom kfp import compiler\nfrom kfp.components import func_to_container_op\nimport time\nimport datetime\n# Function for determine deployment\nimport kfp\nfrom kfp import dsl\n\ndef preprocess_op(volume_mount_path):\n    return dsl.ContainerOp(\n        name='Preprocess Data',\n        image='molozise/kfp-flowers-gcp-preprocess:latest',\n        arguments=['--data-path', volume_mount_path,\n                   '--img-size', 224],\n    )\ndef hyp_op(volume_mount_path, device):\n    return dsl.ContainerOp(\n        name='Hyperparameter Tuning',\n        image='molozise/kfp-flowers-gcp-hyperparameter:latest',\n        arguments=['--data-path', volume_mount_path,\n                   '--device', device],\n    )\ndef train_op(volume_mount_path, device):\n    return dsl.ContainerOp(\n        name='Train Model',\n        image='molozise/kfp-flowers-gcp-train:latest',\n        arguments=['--data-path', volume_mount_path,\n                   '--img-size', 224,\n                   '--model-name', 'flowers-ConvNeXt-T',\n                   '--device', device]\n    )\ndef test_op(volume_mount_path, model_path, device):\n    return dsl.ContainerOp(\n        name='Test Model',\n        image='molozise/kfp-flowers-gcp-test:latest',\n        arguments=['--data-path', volume_mount_path,\n                   '--img-size', 224,\n                   '--model-path', model_path,\n                   '--device', device]\n    )\n@dsl.pipeline(\n    name='Flowers Pipeline',\n    description=''\n)\ndef flowers_pipeline(mode_hyp_train_test: str,\n                     preprocess_yes_no: str,\n                     model_path: str,\n                     device: str):\n    PIPELINE_HOST = \"15bf934d55d3d679-dot-us-central1.pipelines.googleusercontent.com\"  # Kubeflow Pipeline URL\n    WORK_BUCKET = \"gs://vertex-ai-example-368505-kubeflowpipelines-default\"  # Cloud Storage Bucket\n    EXPERIMENT_NAME = \"Flowers Classification Experiment\"  # Experiment Name\n    volume_mount_path = WORK_BUCKET\n    with dsl.Condition(preprocess_yes_no == 'yes'):\n        _preprocess_op = preprocess_op(volume_mount_path)\n    with dsl.Condition(mode_hyp_train_test == 'hyp'):\n        _hyp_op = hyp_op(volume_mount_path, device).after(_preprocess_op)\n    with dsl.Condition(mode_hyp_train_test == 'train'):\n        _train_op = train_op(volume_mount_path, device).after(_preprocess_op)\n    with dsl.Condition(mode_hyp_train_test == 'test'):\n        _train_op = test_op(volume_mount_path, model_path, device).after(_preprocess_op)\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(surface_pipeline, './flowers_pipeline.yaml')"
  },
  {
    "repo": "vfcarida/CI-CD-Pipeline-for-machine-learning-with-online-training-in-Kubeflow",
    "file_path": "models/sklearn_spacy_text/pipeline/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/vfcarida/CI-CD-Pipeline-for-machine-learning-with-online-training-in-Kubeflow/master/models/sklearn_spacy_text/pipeline/pipeline.py",
    "content": "#!/usr/bin/env python3\n\n# Copyright 2019 Google LLC. This software is provided as-is, without warranty\n# or representation for any use or purpose. Your use of it is subject to your\n# agreement with Google.\n\nimport kfp\nfrom kfp import components\nfrom kfp import dsl\nfrom kfp import gcp\n\n# confusion_matrix_op = components.load_component_from_url(\n#     'https://raw.githubusercontent.com/kubeflow/pipelines/eb830cd73ca148e5a1a6485a9374c2dc068314bc/components/local/confusion_matrix/component.yaml'\n# )\n# roc_op = components.load_component_from_url(\n#     'https://raw.githubusercontent.com/kubeflow/pipelines/eb830cd73ca148e5a1a6485a9374c2dc068314bc/components/local/roc/component.yaml'\n# )\n\n\ndef train_op(train_data, eval_data, output):\n  return dsl.ContainerOp(\n      name='train_sklearn_spacy_text_model',\n      image='gcr.io/ml-cicd/sklearn_spacy_text_trainer:latest',\n      arguments=[\n          train_data,\n          eval_data,\n          output,\n      ],\n      file_outputs={\n          'model_path': '/output.txt',\n      })\n\n\ndef deploy_op(model_name, model_path, model_version):\n  return dsl.ContainerOp(\n      name='deploy_sklearn_spacy_text',\n      image='gcr.io/ml-cicd/serving_deployer:latest',\n      arguments=[\n          model_name,\n          model_path,\n          model_version,\n      ],\n      file_outputs={\n          'output': '/output.txt',\n      })\n\n\n# =======================================================================\n\n\n@dsl.pipeline(\n    name='SKLearn Model Pipeline',\n    description='A pipeline that does end-to-end training and deployment for SKLearn models.')\ndef train_deploy_pipeline(\n    model_path='gs://ml-cicd/models/sklearn_spacy_text',\n    train_data='gs://ml-cicd/data/sklearn_spacy_text/train/train.csv',\n    eval_data='gs://ml-cicd/data/sklearn_spacy_text/train/train.csv',\n    model_version='000'\n):\n  model_name = 'sklearn-spacy-text'\n\n  output_path = \"{}/{}/{}\".format(model_path, model_name, model_version)\n\n  train_operation = train_op(train_data, eval_data, output_path).apply(\n                          gcp.use_gcp_secret('user-gcp-sa'))\n\n  deploy_operation = deploy_op(model_name,\n                               train_operation.outputs[\"model_path\"],\n                               model_version)\n\n  # predict_op = predict_op(\n  #     project,\n  #     region,\n  #     create_cluster_op.output,\n  #     transform_op.outputs['eval'],\n  #     train_op.output,\n  #     target,\n  #     analyze_op.output,\n  #     output_template\n  # ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n\n  # confusion_matrix_task = confusion_matrix_op(\n  #     predict_op.output,\n  #     output_template\n  # ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n\n  # roc_task = roc_op(\n  #     predictions_dir=predict_op.output,\n  #     true_class=true_label,\n  #     true_score_column=true_label,\n  #     output_dir=output_template\n  # ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n\n\nif __name__ == '__main__':\n  kfp.compiler.Compiler().compile(train_deploy_pipeline, __file__ + '.zip')\n"
  },
  {
    "repo": "ateevwasalreadytaken/kfp_jupyter_notebook",
    "file_path": "kfp_nb_submit.py",
    "raw_url": "https://raw.githubusercontent.com/ateevwasalreadytaken/kfp_jupyter_notebook/master/kfp_nb_submit.py",
    "content": "import datetime\r\nimport kfp.compiler as compiler\r\nimport kfp.dsl as dsl\r\nimport kfp\r\nimport os\r\n\r\ntmp_dir = 'temp' #Temporary directory here\r\nimage = 'image' #Docker Image for pipeline here\r\n\r\nif not os.path.exists(tmp_dir):\r\n    os.makedirs(tmp_dir)\r\n    \r\ndef run_notebook(input_notebook: str, output_notebook: str,\r\n                 cpu: str, memory: str, gpu = None, vendor = None):\r\n    def demo_op(input_notebook: str, output_notebook: str):\r\n        return dsl.ContainerOp(\r\n            name='papermill',\r\n            image=image,\r\n            command=['sh', '-c'],\r\n            pvolumes={\"Mount\": dsl.PipelineVolume(pvc=\"xyz\",name='xyz')},  #Mount here and replace xyz\r\n            arguments=['papermill $0 $1', input_notebook, output_notebook]\r\n        )\r\n    @dsl.pipeline(\r\n        name='papermill demo',\r\n        description='executing notebooks demo'\r\n    )\r\n    def pipeline_func(input_notebook: str, output_notebook: str):\r\n    \r\n        demo_task = demo_op(input_notebook, output_notebook)\r\n        if gpu != None:\r\n            if vendor != None:\r\n                demo_task.set_gpu_limit(gpu, vendor) #default vendor is NVIDIA\r\n            else:\r\n                demo_task.set_gpu_limit(gpu) #number\r\n        demo_task.set_memory_limit(memory) #number followed by 'G' or 'M' etc.\r\n        demo_task.set_cpu_limit(cpu) #number, optionally followed by m indicateing 1/1000\r\n        \r\n        \r\n    filename = tmp_dir + '/demo{dt:%Y%m%d_%H%M%S}.pipeline.tar.gz'.format(dt=datetime.datetime.now())\r\n    print('filename: {}'.format(filename))\r\n    compiler.Compiler().compile(pipeline_func, filename)\r\n    client = kfp.Client()\r\n    experiment = client.create_experiment('name') #name of experiment\r\n    arguments = {'input_notebook': input_notebook, 'output_notebook': output_notebook}\r\n    run_name = 'name' #name of demo run\r\n    run_result = client.run_pipeline(experiment.id, run_name, filename, arguments)\r\n"
  },
  {
    "repo": "hbelmiro/kfp-test-cache-pipeline",
    "file_path": "pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/hbelmiro/kfp-test-cache-pipeline/main/pipeline.py",
    "content": "from kfp import dsl\n\n\n@dsl.component\ndef comp() -> str:\n    from datetime import datetime\n    now = datetime.now()\n    print(now)\n    return now.isoformat()\n\n\n@dsl.pipeline\ndef my_pipeline() -> str:\n    return comp().output\n"
  },
  {
    "repo": "amanknoldus/llm-dolly-v2-3b-fine-tuning-kubeflow-template",
    "file_path": "pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/amanknoldus/llm-dolly-v2-3b-fine-tuning-kubeflow-template/master/pipeline.py",
    "content": "import logging\nfrom kfp.v2 import compiler\nimport kfp\n\nfrom components.process_data import process_data\nfrom components.serve_model import serve_model_component\nfrom components.train_model import fine_tune_model\nfrom components.upload_model import upload_container\nfrom constants import (PIPELINE_DESCRIPTION, PIPELINE_NAME, PIPELINE_ROOT_GCS, ORIGINAL_MODEL_NAME, \\\n                       SAVE_MODEL_BUCKET_NAME, REGION, DATASET_BUCKET, MODEL_DISPLAY_NAME, SERVING_IMAGE, \\\n                       STAGING_BUCKET, COMPONENT_EXECUTION, DATASET_NAME, SERVING_IMAGE_TRIGGER, SERVICE_ACCOUNT_ML,\n                       DEPLOYED_MODEL_DETAILS_FILE,\n                       PIPELINE_JSON_FILE)\n\nlogging.basicConfig(level=logging.DEBUG)\nlogger = logging.getLogger(__name__)\n\n\n@kfp.dsl.pipeline(name=PIPELINE_NAME,\n                  description=PIPELINE_DESCRIPTION,\n                  pipeline_root=PIPELINE_ROOT_GCS)\ndef pipeline(\n        project_id: str,\n        job_id: str\n):\n    \"\"\"Dataset Processing\"\"\"\n    process_data_task = process_data(DATASET_BUCKET, DATASET_NAME).set_display_name(\"Data_Processing\")\n\n    \"\"\"Fine Tune Model Pipeline\"\"\"\n    train_model_task = fine_tune_model(process_data_task.outputs[\"dataset\"],\n                                       ORIGINAL_MODEL_NAME,\n                                       SAVE_MODEL_BUCKET_NAME,\n                                       COMPONENT_EXECUTION) \\\n        .after(process_data_task) \\\n        .set_display_name(\"Dolly Fine Tuning\") \\\n        .set_cpu_request(\"8\") \\\n        .set_memory_limit(\"32G\")\n\n    \"\"\"Upload model package\"\"\"\n    upload_model_task = upload_container(project_id=project_id,\n                                         trigger_id=SERVING_IMAGE_TRIGGER,\n                                         component_execution=COMPONENT_EXECUTION) \\\n        .after(train_model_task) \\\n        .set_display_name(\"Model_Upload\")\n\n    \"\"\"Serve Model To Endpoint\"\"\"\n    serve_model_component(project_id,\n                          REGION,\n                          STAGING_BUCKET,\n                          SERVING_IMAGE,\n                          MODEL_DISPLAY_NAME,\n                          COMPONENT_EXECUTION,\n                          SERVICE_ACCOUNT_ML,\n                          save_model_details_bucket=DATASET_BUCKET,\n                          model_details_file_name=DEPLOYED_MODEL_DETAILS_FILE) \\\n        .after(upload_model_task) \\\n        .set_display_name(\"Serve_Model\") \\\n        .set_cpu_request(\"8\") \\\n        .set_memory_limit(\"32G\")\n\n\ndef compile_pipeline(pipeline_template_name=f'{PIPELINE_JSON_FILE}'):\n    compiler.Compiler().compile(\n        pipeline_func=pipeline,\n        package_path=pipeline_template_name\n    )\n    return None\n\n\nif __name__ == \"__main__\":\n    compile_pipeline()\n"
  },
  {
    "repo": "nomnomnonono/ML-Pipeline-of-Paper-Category-Classification",
    "file_path": "pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/nomnomnonono/ML-Pipeline-of-Paper-Category-Classification/main/pipeline.py",
    "content": "import datetime\nimport os\n\nfrom dotenv import load_dotenv\nfrom google.cloud import aiplatform\nfrom kfp import compiler, components, dsl\n\nload_dotenv(\".env\")\nPROJECT_ID = os.environ.get(\"GCP_PROJECT_ID\")\nAR_REPOSITORY_NAME = os.environ.get(\"AR_REPOSITORY_NAME\")\nLOCATION = os.environ.get(\"LOCATION\")\nSOURCE_CSV_URI = os.environ.get(\"SOURCE_CSV_URI\")\nROOT_BUCKET = os.environ.get(\"ROOT_BUCKET\")\nPIPELINE_NAME = os.environ.get(\"PIPELINE_NAME\")\n\n\n@dsl.pipeline(\n    name=PIPELINE_NAME,\n    description=\"Vertex Piplines sample\",\n    pipeline_root=ROOT_BUCKET,\n)\ndef pipeline() -> None:\n    preprocess_op = components.load_component_from_file(\n        \"components/preprocess/component.yaml\"\n    )\n    preprocess_task = preprocess_op(src_csv=SOURCE_CSV_URI)\n\n    train_op = components.load_component_from_file(\"components/train/component.yaml\")\n    train_task = train_op(dataset=preprocess_task.outputs[\"dataset\"])\n    train_task.custom_job_spec = {\n        \"displayName\": train_task.name,\n        \"jobSpec\": {\n            \"workerPoolSpecs\": [\n                {\n                    \"machineSpec\": {\"machineType\": \"n1-standard-2\"},\n                    \"replicaCount\": 1,\n                }\n            ],\n        },\n    }\n\n    evaluate_op = components.load_component_from_file(\n        \"components/evaluate/component.yaml\"\n    )\n    _ = evaluate_op(\n        dataset=preprocess_task.outputs[\"dataset\"],\n        artifact=train_task.outputs[\"artifact\"],\n    )\n\n    deploy_op = components.load_component_from_file(\"components/deploy/component.yaml\")\n    _ = deploy_op(\n        artifact=train_task.outputs[\"artifact\"],\n        model_name=\"ml-pipeline-arxiv-paper-model\",\n        serving_container_image_uri=f\"asia-northeast1-docker.pkg.dev/{PROJECT_ID}/{AR_REPOSITORY_NAME}/serving:latest\",\n        serving_container_environment_variables='{\"APP_MODULE\": \"server:app\"}',\n        serving_container_ports=80,\n        endpoint_name=\"ml-pipeline-arxiv-paper-endpoint\",\n        deploy_name=\"ml-pipeline-arxiv-paper-deploy\",\n        machine_type=\"n1-standard-2\",\n        min_replicas=1,\n        project=PROJECT_ID,\n        location=LOCATION,\n    )\n\n\ncompiler.Compiler().compile(\n    pipeline_func=pipeline, package_path=\"ml-pipeline-arxiv-paper.json\"\n)\n\njob = aiplatform.PipelineJob(\n    display_name=\"ml-pipeline-arxiv-paper\",\n    template_path=\"ml-pipeline-arxiv-paper.json\",\n    job_id=PIPELINE_NAME + f\"-{datetime.datetime.now().strftime('%Y%m%d%H%M%S%f')[:-4]}\",\n    pipeline_root=ROOT_BUCKET,\n    enable_caching=False,\n    project=PROJECT_ID,\n    location=LOCATION,\n)\n\njob.submit()\n"
  },
  {
    "repo": "hbelmiro/kfp-importer-minio-demo",
    "file_path": "pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/hbelmiro/kfp-importer-minio-demo/main/pipeline.py",
    "content": "from kfp import dsl\nfrom kfp.dsl import Input, Dataset\n\n\n@dsl.component(base_image=\"python:3.10\")\ndef read_dataset(dataset: Input[Dataset]):\n    with open(dataset.path, \"r\") as f:\n        data = f.read()\n    print(\"Dataset content:\", data)\n\n\n@dsl.pipeline\ndef pipeline():\n    importer_task = dsl.importer(\n        artifact_uri=\"minio://mlpipeline/artifacts/input/raw_transaction_datasource.csv\",\n        artifact_class=dsl.Dataset,\n        reimport=True)\n\n    read_dataset(dataset=importer_task.output)\n"
  },
  {
    "repo": "hbelmiro/kfp-print-env-var-demo",
    "file_path": "pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/hbelmiro/kfp-print-env-var-demo/main/pipeline.py",
    "content": "from kfp import dsl\n\n\n@dsl.component(base_image=\"quay.io/hbelmiro/kfp-print-env-var-demo:latest\")\ndef comp(env_var: str) -> str:\n    import os\n\n    value = os.getenv(env_var, \"\")\n\n    if value == \"\":\n        raise Exception(\"Env var is not set\")\n\n    return value\n\n\n@dsl.pipeline\ndef my_pipeline(env_var: str) -> str:\n    comp_task = comp(env_var=env_var)\n    comp_task.set_caching_options(False)\n    return comp_task.output\n"
  },
  {
    "repo": "Taring-Community/kfp-k8s-getstarted",
    "file_path": "first-pipeline/addition_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/Taring-Community/kfp-k8s-getstarted/main/first-pipeline/addition_pipeline.py",
    "content": "import kfp\nfrom kfp import dsl\nfrom kfp.v2.dsl import (\n    component,\n    Input,\n    Output,\n    Dataset,\n    Metrics,\n)\n\n# Connect to Kubeflow Pipelines\nclient = kfp.Client( host='http://3.34.205.245:80' )\nprint( client.list_pipelines() )\n\n#===================== COMPONENT =====================\n# First component\n@component\ndef add( a: float, b: float ) -> float:\n    print( 'Adding two numbers' )\n    return a + b\n\n#===================== PIPELINE =====================\n# Pipeline definition\n@dsl.pipeline(\n    name='addition-pipeline',\n    description='A toy pipeline that performs addition calculations.'\n    # pipeline_root='gs://kubeflow-pipeline-data/addition-pipeline',\n)\ndef add_pipeline(\n    a: float = 1,\n    b: float = 7,\n):\n    first_add_task = add( a, 4 )\n\n    second_add_task = add( first_add_task.output, b )\n\n# Specify pipeline argument values\narguments = { 'a': '7', 'b': 8 }\n\n# Submit a pipeline run using the v2 compatible mode\nclient.create_run_from_pipeline_func(\n    add_pipeline,\n    arguments=arguments\n)"
  },
  {
    "repo": "Saoussen-CH/production-ready-MLOps-on-GCP",
    "file_path": "components/tests/conftest.py",
    "raw_url": "https://raw.githubusercontent.com/Saoussen-CH/production-ready-MLOps-on-GCP/main/components/tests/conftest.py",
    "content": "import pytest\nimport kfp.dsl\nimport json\n\n\n@pytest.fixture(autouse=True)\ndef mock_kfp_artifact(mocker):\n    \"\"\"\n    This fixture mocks the Artifact object (and thus any derived\n    classes i.e Dataset, Model, etc.)\n    to return the URI as the path.\n\n    Unit tests set the URI of artifacts, however, KFP components use Artifact.path to\n    retrieve paths to files. If a URI doesn't start with gs:// or minio:// or s3://,\n    the path with be None. This behaviour is avoided by mocking the Artifact._get_path\n    method.\n\n    Args:\n        mocker: Used to patch the _get_path method in `kfp.dsl.Artifact`.\n\n    Returns:\n        None\n    \"\"\"\n\n    def _get_path(self):\n        return self.uri\n\n    # mock the _get_path method of Artifact which is used by the property path\n    mocker.patch.object(kfp.dsl.Artifact, \"_get_path\", _get_path)\n\n\n@pytest.fixture\ndef mock_output_model(mocker):\n    return mocker.MagicMock()\n\n\n@pytest.fixture\ndef mock_model_list(mocker):\n    return mocker.patch(\"google.cloud.aiplatform.Model.list\")\n\n\n@pytest.fixture\ndef mock_job_service_client(mocker):\n    return mocker.patch(\n        \"google.cloud.aiplatform_v1beta1.services.job_service.JobServiceClient\"\n    )\n\n\n@pytest.fixture\ndef mock_create_batch_prediction_job(mock_job_service_client):\n    return mock_job_service_client.return_value.create_batch_prediction_job\n\n\n@pytest.fixture\ndef mock_get_batch_prediction_job(mock_job_service_client):\n    return mock_job_service_client.return_value.get_batch_prediction_job\n\n\n@pytest.fixture\ndef mock_model_class(mocker):\n    return mocker.patch(\"google.cloud.aiplatform.Model\")\n\n\n@pytest.fixture\ndef mock_model(tmp_path):\n    return type(\n        \"MockModel\",\n        (object,),\n        {\n            \"uri\": str(tmp_path / \"model-uri\"),\n            \"metadata\": {\"resourceName\": \"model-resource-name\"},\n        },\n    )()\n\n\n@pytest.fixture\ndef mock_dataset(tmp_path):\n    return type(\"MockDataset\", (object,), {\"uri\": str(tmp_path / \"test-data-uri\")})()\n\n\n@pytest.fixture\ndef mock_metrics(tmp_path):\n    metrics_path = tmp_path / \"metrics.json\"\n    metrics = {\"problemType\": \"classification\", \"accuracy\": 0.9}\n    with open(metrics_path, \"w\") as f:\n        json.dump(metrics, f)\n    return type(\"MockMetrics\", (object,), {\"path\": str(metrics_path)})()\n\n\n@pytest.fixture\ndef mock_model_service_client(mocker):\n    return mocker.patch(\"google.cloud.aiplatform_v1.ModelServiceClient\")\n"
  },
  {
    "repo": "Saoussen-CH/production-ready-MLOps-on-GCP",
    "file_path": "components/tests/test_lookup_model_op.py",
    "raw_url": "https://raw.githubusercontent.com/Saoussen-CH/production-ready-MLOps-on-GCP/main/components/tests/test_lookup_model_op.py",
    "content": "# test_lookup_model.py\nfrom kfp.dsl import Model\nimport pytest\nimport logging\nimport components\n\nlookup_model_op = components.lookup_model_op.python_func\n\n\ndef test_lookup_single_model_found(mock_model_list, mock_output_model, tmp_path):\n    \"\"\"\n    Assert lookup_model produces expected resource name, and that list method is\n    called with the correct arguments.\n    \"\"\"\n    mock_path = str(tmp_path / \"model\")\n    mock_model_instance = mock_output_model\n    mock_model_instance.resource_name = \"my-model-resource-name\"\n    mock_model_instance.uri = mock_path\n    mock_model_list.return_value = [mock_model_instance]\n\n    found_model_resource_name, _ = lookup_model_op(\n        model_name=\"my-model\",\n        location=\"us-central1\",\n        project=\"my-project-id\",\n        fail_on_model_not_found=False,\n        model=Model(uri=mock_path),\n    )\n\n    assert found_model_resource_name == \"my-model-resource-name\"\n\n    mock_model_list.assert_called_once_with(\n        filter='display_name=\"my-model\"',\n        location=\"us-central1\",\n        project=\"my-project-id\",\n    )\n\n\ndef test_lookup_model_no_model_found(mock_model_list, tmp_path, caplog):\n    \"\"\"\n    Checks that when there are no models and fail_on_model_found = False,\n    lookup_model returns an empty string.\n    \"\"\"\n    mock_model_list.return_value = []\n\n    with caplog.at_level(logging.ERROR):\n        model_resource_name, training_dataset = lookup_model_op(\n            model_name=\"my-model\",\n            location=\"us-central1\",\n            project=\"my-project-id\",\n            fail_on_model_not_found=False,\n            model=Model(uri=str(tmp_path / \"model\")),\n        )\n\n    assert model_resource_name == \"\"\n    assert training_dataset == {}\n    assert \"No model found with name\" in caplog.text\n\n\ndef test_lookup_model_fail_on_model_not_found(mock_model_list, tmp_path):\n    \"\"\"\n    Checks that when there are no models and fail_on_model_found = True,\n    lookup_model raises a RuntimeError.\n    \"\"\"\n    mock_model_list.return_value = []\n\n    with pytest.raises(RuntimeError, match=\"Failed as model was not found\"):\n        lookup_model_op(\n            model_name=\"my-model\",\n            location=\"europe-west4\",\n            project=\"my-project-id\",\n            fail_on_model_not_found=True,\n            model=Model(uri=str(tmp_path / \"model\")),\n        )\n\n\ndef test_multiple_models_found(mock_model_list, mock_output_model, tmp_path):\n    \"\"\"\n    Checks that when multiple models are found, lookup_model raises a RuntimeError.\n    \"\"\"\n    mock_path = str(tmp_path / \"model\")\n    mock_model_instance1 = mock_output_model\n    mock_model_instance1.resource_name = \"my-model-resource-name-1\"\n    mock_model_instance1.uri = mock_path\n\n    mock_model_instance2 = mock_output_model\n    mock_model_instance2.resource_name = \"my-model-resource-name-2\"\n    mock_model_instance2.uri = mock_path\n\n    mock_model_list.return_value = [mock_model_instance1, mock_model_instance2]\n\n    with pytest.raises(\n        RuntimeError, match=\"Multiple models with name my-model were found.\"\n    ):\n        lookup_model_op(\n            model_name=\"my-model\",\n            location=\"europe-west4\",\n            project=\"my-project-id\",\n            fail_on_model_not_found=False,\n            model=Model(uri=mock_path),\n        )\n"
  },
  {
    "repo": "Saoussen-CH/production-ready-MLOps-on-GCP",
    "file_path": "components/tests/test_model_batch_predict_op.py",
    "raw_url": "https://raw.githubusercontent.com/Saoussen-CH/production-ready-MLOps-on-GCP/main/components/tests/test_model_batch_predict_op.py",
    "content": "import pytest\nfrom kfp.dsl import Model\nfrom google.cloud.aiplatform_v1beta1.types import JobState\n\nimport components\n\nmodel_batch_predict_op = components.model_batch_predict_op.python_func\n\nSKEW_THRESHOLD = {\"defaultSkewThreshold\": {\"value\": 0.001}}\nTRAIN_DATASET = {\n    \"gcsSource\": {\"uris\": [\"gs://file.csv\"]},\n    \"dataFormat\": \"csv\",\n    \"targetField\": \"col\",\n}\n\n\n@pytest.mark.parametrize(\n    (\n        \"source_format, destination_format, source_uri, monitoring_training_dataset, \"\n        \"monitoring_alert_email_addresses, monitoring_skew_config\"\n    ),\n    [\n        (\"bigquery\", \"bigquery\", \"bq://a.b.c\", None, None, None),\n        (\"csv\", \"csv\", '[\"gs://file.csv\"]', None, None, None),\n        (\"csv\", \"csv\", '[\"gs://file.csv\"]', TRAIN_DATASET, [], SKEW_THRESHOLD),\n        (\"csv\", \"csv\", '[\"gs://file.csv\"]', TRAIN_DATASET, [\"a@b.com\"], SKEW_THRESHOLD),\n    ],\n)\ndef test_model_batch_predict_successful(\n    mock_create_batch_prediction_job,\n    mock_get_batch_prediction_job,\n    tmp_path,\n    source_format,\n    destination_format,\n    source_uri,\n    monitoring_training_dataset,\n    monitoring_alert_email_addresses,\n    monitoring_skew_config,\n):\n    \"\"\"\n    Test model_batch_predict_op function for a successful batch\n    prediction job creation with different parameter configurations.\n    \"\"\"\n    mock_create_batch_prediction_job.return_value.name = (\n        \"projects/my-project/locations/us-central1/batchPredictionJobs/123456789\"\n    )\n    mock_get_batch_prediction_job.return_value.state = JobState.JOB_STATE_SUCCEEDED\n\n    job_display_name = \"test-batch-prediction-job\"\n    location = \"us-central1\"\n    project = \"my-project\"\n    destination_uri = \"gs://destination-uri\"\n    gcp_resources_path = str(tmp_path / \"gcp_resources.json\")\n\n    (gcp_resources,) = model_batch_predict_op(\n        model=Model(\n            uri=\"gs://model-uri\", metadata={\"resourceName\": \"model-resource-name\"}\n        ),\n        gcp_resources=gcp_resources_path,\n        job_display_name=job_display_name,\n        location=location,\n        project=project,\n        source_uri=source_uri,\n        destination_uri=destination_uri,\n        source_format=source_format,\n        destination_format=destination_format,\n        monitoring_training_dataset=monitoring_training_dataset,\n        monitoring_alert_email_addresses=monitoring_alert_email_addresses,\n        monitoring_skew_config=monitoring_skew_config,\n    )\n\n    assert gcp_resources is not None\n    with open(gcp_resources_path, \"r\") as f:\n        content = f.read()\n        assert \"BatchPredictionJob\" in content\n        assert (\n            \"projects/my-project/locations/us-central1/batchPredictionJobs/123456789\"\n            in content\n        )\n    mock_create_batch_prediction_job.assert_called_once()\n    mock_get_batch_prediction_job.assert_called_once()\n\n\ndef test_model_batch_predict_failed(\n    mock_create_batch_prediction_job, mock_get_batch_prediction_job, tmp_path\n):\n    \"\"\"\n    Test the model_batch_predict_op function for a failed batch prediction job creation.\n    \"\"\"\n    mock_create_batch_prediction_job.return_value.name = (\n        \"projects/my-project/locations/us-central1/batchPredictionJobs/123456789\"\n    )\n    mock_get_batch_prediction_job.return_value.state = JobState.JOB_STATE_FAILED\n\n    job_display_name = \"test-batch-prediction-job\"\n    location = \"us-central1\"\n    project = \"my-project\"\n    source_uri = \"gs://source-uri\"\n    destination_uri = \"gs://destination-uri\"\n    source_format = \"jsonl\"\n    destination_format = \"jsonl\"\n    gcp_resources_path = str(tmp_path / \"gcp_resources.json\")\n\n    with pytest.raises(RuntimeError):\n        model_batch_predict_op(\n            model=Model(\n                uri=\"gs://model-uri\", metadata={\"resourceName\": \"model-resource-name\"}\n            ),\n            gcp_resources=gcp_resources_path,\n            job_display_name=job_display_name,\n            location=location,\n            project=project,\n            source_uri=source_uri,\n            destination_uri=destination_uri,\n            source_format=source_format,\n            destination_format=destination_format,\n            machine_type=\"n1-standard-2\",\n            starting_replica_count=1,\n            max_replica_count=1,\n        )\n"
  },
  {
    "repo": "Saoussen-CH/production-ready-MLOps-on-GCP",
    "file_path": "components/tests/test_upload_best_model_op.py",
    "raw_url": "https://raw.githubusercontent.com/Saoussen-CH/production-ready-MLOps-on-GCP/main/components/tests/test_upload_best_model_op.py",
    "content": "import json\nimport logging\nfrom kfp.dsl import Dataset, Metrics, Model\nfrom unittest import mock\nfrom google.protobuf.json_format import ParseDict\nfrom google.cloud.aiplatform_v1 import ModelEvaluation\nfrom google_cloud_pipeline_components.types.artifact_types import VertexModel\n\nimport components\n\nupload_model = components.upload_best_model_op.python_func\n\n\ndef test_model_upload_no_champion(\n    mock_model_class, mock_model_service_client, caplog, tmp_path\n):\n    caplog.set_level(logging.INFO)\n\n    metrics_json = json.dumps(\n        {\n            \"problemType\": \"regression\",\n            \"auc\": 0.4,\n            \"accuracy\": 0.80,\n        }\n    )\n    metrics_file_path = tmp_path / \"metrics.json\"\n    metrics_file_path.write_text(metrics_json)\n\n    mock_model_class.list.return_value = []\n\n    model = Model(uri=\"dummy-model-uri\")\n    serving_container_image = \"dummy_image:latest\"\n    model_name = \"dummy-model-name\"\n    model_description = \"dummy model_description\"\n    vertex_model = VertexModel.create(\n        name=model_name, uri=\"chall_uri\", model_resource_name=\"chall_resource_name\"\n    )\n    project = \"dummy-project\"\n    location = \"dummy-location\"\n    model_evaluation = Metrics(uri=\"\")\n    model_evaluation.path = str(metrics_file_path)\n    eval_metric = \"auc\"\n    eval_lower_is_better = False\n\n    pipeline_job_id = \"dummy-pipeline-job-id\"\n    test_dataset = Dataset(uri=\"test-dataset-uri\")\n    evaluation_name = \"dummy evaluation name\"\n\n    upload_model(\n        model=model,\n        model_description=model_description,\n        serving_container_image=serving_container_image,\n        vertex_model=vertex_model,\n        project=project,\n        location=location,\n        model_eval_metrics=model_evaluation,\n        eval_metric=eval_metric,\n        eval_lower_is_better=eval_lower_is_better,\n        model_name=model_name,\n        pipeline_job_id=pipeline_job_id,\n        test_data=test_dataset,\n        evaluation_name=evaluation_name,\n    )\n\n    # Check that model lookup is performed, and no existing model is found\n    mock_model_class.list.assert_called_once_with(\n        filter=f'display_name=\"{model_name}\"',\n        location=location,\n        project=project,\n    )\n    assert \"found 0 models\" in caplog.text\n\n    # Check no model comparison occurs\n    assert \"wins\" not in caplog.text\n\n    # Check model upload call\n    mock_model_class.upload.assert_called_once_with(\n        display_name=model_name,\n        description=model_description,\n        artifact_uri=\"dummy-model-uri\",\n        serving_container_image_uri=serving_container_image,\n        parent_model=None,\n        is_default_version=True,\n    )\n\n    # check model output URI\n    assert vertex_model.uri == f\"https://{location}-aiplatform.googleapis.com/v1/\" + (\n        str(mock_model_class.upload.return_value.versioned_resource_name)\n    )\n\n    # check evaluation import\n    mock_model_service_client.return_value.import_model_evaluation.assert_called_once_with(  # noqa\n        parent=mock_model_class.upload.return_value.versioned_resource_name,\n        model_evaluation=mock.ANY,\n    )\n\n\ndef test_model_upload_challenger_wins(\n    mock_model_class, mock_model_service_client, caplog, tmp_path, mocker\n):\n    caplog.set_level(logging.INFO)\n\n    metrics_json = json.dumps(\n        {\n            \"problemType\": \"regression\",\n            \"auc\": 0.4,\n            \"accuracy\": 0.80,\n        }\n    )\n    metrics_file_path = tmp_path / \"metrics.json\"\n    metrics_file_path.write_text(metrics_json)\n\n    # create mock champion model\n    mock_champion_model = mocker.Mock()\n    mock_champion_model.version_id = \"123\"\n    dummy_champion_eval = ModelEvaluation()\n    dummy_champion_metrics = {\n        \"auc\": 0.2,\n        \"f1\": 0.7,\n    }\n    message_dict = {\n        \"displayName\": \"Previously imported evaluation\",\n        \"metricsSchemaUri\": \"gs://google-cloud-aiplatform/schema/modelevaluation/regression_metrics_1.0.0.yaml\",  # noqa\n        \"metrics\": dummy_champion_metrics,\n        \"metadata\": {\n            \"pipeline_job_id\": \"dummy-pipeline-id\",\n            \"evaluation_dataset_type\": \"gcs\",\n            \"evaluation_dataset_path\": [\"dummy-gcs-uri\"],\n        },\n    }\n    ParseDict(message_dict, dummy_champion_eval._pb)\n    mock_champion_model.get_model_evaluation.return_value._gca_resource = (\n        dummy_champion_eval\n    )\n    mock_champion_model.resource_name = \"dummy-champion-resource-name\"\n    mock_model_class.list.return_value = [mock_champion_model]\n\n    # create mock challenger model\n    model = Model(uri=\"dummy-model-uri\")\n    serving_container_image = \"dummy_image:latest\"\n    model_name = \"dummy-model-name\"\n    model_description = \"dummy model_description\"\n    vertex_model = VertexModel.create(\n        name=model_name, uri=\"chall_uri\", model_resource_name=\"chall_resource_name\"\n    )\n    project = \"dummy-project\"\n    location = \"dummy-location\"\n    model_evaluation = Metrics(uri=\"\")\n    model_evaluation.path = str(metrics_file_path)\n    eval_metric = \"auc\"\n    eval_lower_is_better = False\n\n    pipeline_job_id = \"dummy-pipeline-job-id\"\n    test_dataset = Dataset(uri=\"test-dataset-uri\")\n    evaluation_name = \"dummy evaluation name\"\n\n    upload_model(\n        model=model,\n        model_description=model_description,\n        serving_container_image=serving_container_image,\n        vertex_model=vertex_model,\n        project=project,\n        location=location,\n        model_eval_metrics=model_evaluation,\n        eval_metric=eval_metric,\n        eval_lower_is_better=eval_lower_is_better,\n        model_name=model_name,\n        pipeline_job_id=pipeline_job_id,\n        test_data=test_dataset,\n        evaluation_name=evaluation_name,\n    )\n\n    # Check that model lookup is performed, and no existing model is found\n    mock_model_class.list.assert_called_once_with(\n        filter=f'display_name=\"{model_name}\"',\n        location=location,\n        project=project,\n    )\n    assert \"is being challenged by new model\" in caplog.text\n\n    # Check challenger wins in model comparison\n    assert \"Challenger wins!\" in caplog.text\n\n    # Check model upload call\n    mock_model_class.upload.assert_called_once_with(\n        display_name=model_name,\n        description=model_description,\n        artifact_uri=\"dummy-model-uri\",\n        serving_container_image_uri=serving_container_image,\n        parent_model=mock_champion_model.resource_name,\n        is_default_version=True,\n    )\n\n    # check model output URI\n    assert vertex_model.uri == f\"https://{location}-aiplatform.googleapis.com/v1/\" + (\n        str(mock_model_class.upload.return_value.versioned_resource_name)\n    )\n\n    # check evaluation import\n    mock_model_service_client.return_value.import_model_evaluation.assert_called_once_with(  # noqa\n        parent=mock_model_class.upload.return_value.versioned_resource_name,\n        model_evaluation=mock.ANY,\n    )\n\n\ndef test_model_upload_champion_wins(\n    mock_model_class, mock_model_service_client, caplog, tmp_path, mocker\n):\n    caplog.set_level(logging.INFO)\n\n    metrics_json = json.dumps(\n        {\n            \"problemType\": \"regression\",\n            \"auc\": 0.4,\n            \"accuracy\": 0.80,\n        }\n    )\n    metrics_file_path = tmp_path / \"metrics.json\"\n    metrics_file_path.write_text(metrics_json)\n\n    # Create mock champion model\n    mock_champion_model = mocker.Mock()\n    mock_champion_model.version_id = \"123\"\n    dummy_champion_eval = ModelEvaluation()\n    dummy_champion_metrics = {\n        \"auc\": 0.8,\n        \"f1\": 0.7,\n    }\n    message_dict = {\n        \"displayName\": \"Previously imported evaluation\",\n        \"metricsSchemaUri\": \"gs://google-cloud-aiplatform/schema/modelevaluation/regression_metrics_1.0.0.yaml\",  # noqa\n        \"metrics\": dummy_champion_metrics,\n        \"metadata\": {\n            \"pipeline_job_id\": \"dummy-pipeline-id\",\n            \"evaluation_dataset_type\": \"gcs\",\n            \"evaluation_dataset_path\": [\"dummy-gcs-uri\"],\n        },\n    }\n    ParseDict(message_dict, dummy_champion_eval._pb)\n    mock_champion_model.get_model_evaluation.return_value._gca_resource = (\n        dummy_champion_eval\n    )\n    mock_champion_model.resource_name = \"dummy-champion-resource-name\"\n    mock_model_class.list.return_value = [mock_champion_model]\n\n    # Create mock challenger model\n    model = Model(uri=\"dummy-model-uri\")\n    serving_container_image = \"dummy_image:latest\"\n    model_name = \"dummy-model-name\"\n    model_description = \"dummy model_description\"\n    vertex_model = VertexModel.create(\n        name=model_name, uri=\"chall_uri\", model_resource_name=\"chall_resource_name\"\n    )\n    project = \"dummy-project\"\n    location = \"dummy-location\"\n    model_evaluation = Metrics(uri=\"\")\n    model_evaluation.path = str(metrics_file_path)\n    eval_metric = \"auc\"\n    eval_lower_is_better = False\n\n    pipeline_job_id = \"dummy-pipeline-job-id\"\n    test_dataset = Dataset(uri=\"test-dataset-uri\")\n    evaluation_name = \"dummy evaluation name\"\n\n    upload_model(\n        model=model,\n        model_description=model_description,\n        serving_container_image=serving_container_image,\n        vertex_model=vertex_model,\n        project=project,\n        location=location,\n        model_eval_metrics=model_evaluation,\n        eval_metric=eval_metric,\n        eval_lower_is_better=eval_lower_is_better,\n        model_name=model_name,\n        pipeline_job_id=pipeline_job_id,\n        test_data=test_dataset,\n        evaluation_name=evaluation_name,\n    )\n\n    # Check that model lookup is performed, and the existing champion model is found\n    mock_model_class.list.assert_called_once_with(\n        filter=f'display_name=\"{model_name}\"',\n        location=location,\n        project=project,\n    )\n    assert \"is being challenged by new model\" in caplog.text\n\n    # Check champion wins in model comparison\n    assert \"Champion wins!\" in caplog.text\n\n    # Check model upload call\n    mock_model_class.upload.assert_called_once_with(\n        display_name=model_name,\n        description=model_description,\n        artifact_uri=\"dummy-model-uri\",\n        serving_container_image_uri=serving_container_image,\n        parent_model=mock_champion_model.resource_name,\n        is_default_version=False,\n    )\n\n    # Check model output URI\n    assert vertex_model.uri == f\"https://{location}-aiplatform.googleapis.com/v1/\" + (\n        str(mock_model_class.upload.return_value.versioned_resource_name)\n    )\n\n    # check evaluation import\n    mock_model_service_client.return_value.import_model_evaluation.assert_called_once_with(  # noqa\n        parent=mock_model_class.upload.return_value.versioned_resource_name,\n        model_evaluation=mock.ANY,\n    )\n"
  },
  {
    "repo": "Saoussen-CH/production-ready-MLOps-on-GCP",
    "file_path": "components/src/components/extract_table_to_gcs_op.py",
    "raw_url": "https://raw.githubusercontent.com/Saoussen-CH/production-ready-MLOps-on-GCP/main/components/src/components/extract_table_to_gcs_op.py",
    "content": "from kfp.dsl import Dataset, Artifact, component, Input, Output\n\n\n@component(\n    base_image=\"python:3.10.14\", packages_to_install=[\"google-cloud-bigquery==3.24.0\"]\n)\ndef extract_table_to_gcs_op(\n    bq_table: Input[Artifact],\n    dataset: Output[Dataset],\n    location: str = \"US\",\n) -> None:\n    \"\"\"\n    Extract a Big Query table into Google Cloud Storage.\n    \"\"\"\n\n    import google.cloud.bigquery as bq\n\n    project_id = bq_table.metadata[\"projectId\"]\n    dataset_id = bq_table.metadata[\"datasetId\"]\n    table_id = bq_table.metadata[\"tableId\"]\n\n    # Get the table generated on the previous component\n    full_table_id = f\"{project_id}.{dataset_id}.{table_id}\"\n    table = bq.table.Table(table_ref=full_table_id)\n\n    # Initiate the Big Query client to connect with the project\n    # job_config = bq.job.ExtractJobConfig(**{})\n    client = bq.client.Client(project=project_id, location=location)\n\n    # Submit the extract table job to store on GCS\n    extract_job = client.extract_table(table, dataset.uri)\n\n    # Wait for the extract job to complete\n    extract_job.result()\n"
  },
  {
    "repo": "Saoussen-CH/production-ready-MLOps-on-GCP",
    "file_path": "components/src/components/get_custom_job_results_op.py",
    "raw_url": "https://raw.githubusercontent.com/Saoussen-CH/production-ready-MLOps-on-GCP/main/components/src/components/get_custom_job_results_op.py",
    "content": "from kfp.dsl import component, Metrics, Output, Model\n\n\n@component(\n    base_image=\"python:3.10.14\",\n    packages_to_install=[\n        \"google-cloud-pipeline-components==2.14.1\",\n        \"google-cloud-aiplatform==1.55.0\",\n    ],\n)\ndef get_custom_job_results_op(\n    project: str,\n    location: str,\n    job_resource: str,\n    model: Output[Model],\n    metrics: Output[Metrics],\n):\n    import json\n    import shutil\n    from pathlib import Path\n    import google.cloud.aiplatform as aip\n    from google.protobuf.json_format import Parse\n    from google_cloud_pipeline_components.proto.gcp_resources_pb2 import GcpResources\n\n    aip.init(project=project, location=location)\n\n    training_gcp_resources = Parse(job_resource, GcpResources())\n    custom_job_id = training_gcp_resources.resources[0].resource_uri\n    custom_job_name = custom_job_id[custom_job_id.find(\"project\") :]\n\n    job_resource = aip.CustomJob.get(custom_job_name).gca_resource\n\n    job_base_dir = job_resource.job_spec.base_output_directory.output_uri_prefix\n\n    job_base_dir_fuse = job_base_dir.replace(\"gs://\", \"/gcs/\")\n    model_uri_fuse = model.uri.replace(\"gs://\", \"/gcs/\")\n\n    shutil.copytree(\n        f\"{job_base_dir_fuse}/model\", Path(model_uri_fuse), dirs_exist_ok=True\n    )\n\n    with open(f\"{job_base_dir_fuse}/metrics/metrics.json\") as fh:\n        metrics_dict = json.load(fh)\n\n    for k, v in metrics_dict.items():\n        metrics.log_metric(k, v)\n\n    with open(metrics.path, \"w\") as fh:\n        json.dump(metrics_dict, fh)\n\n    shutil.rmtree(f\"{job_base_dir_fuse}/model\")\n    shutil.rmtree(f\"{job_base_dir_fuse}/metrics\")\n"
  },
  {
    "repo": "Saoussen-CH/production-ready-MLOps-on-GCP",
    "file_path": "components/src/components/get_hyperparameter_tuning_results_op.py",
    "raw_url": "https://raw.githubusercontent.com/Saoussen-CH/production-ready-MLOps-on-GCP/main/components/src/components/get_hyperparameter_tuning_results_op.py",
    "content": "from kfp.dsl import component\n\n\n@component(\n    base_image=\"python:3.10.14\",\n    packages_to_install=[\n        \"google-cloud-pipeline-components==2.14.1\",\n        \"google-cloud-aiplatform==1.55.0\",\n    ],\n)\ndef get_hyperparameter_tuning_results_op(\n    project: str, location: str, job_resource: str, study_spec_metrics: list\n) -> dict:\n    import google.cloud.aiplatform as aip\n    from google_cloud_pipeline_components.proto.gcp_resources_pb2 import GcpResources\n    from google.protobuf.json_format import Parse\n    from google.cloud.aiplatform_v1.types import study\n\n    aip.init(project=project, location=location)\n\n    gcp_resources_proto = Parse(job_resource, GcpResources())\n    tuning_job_id = gcp_resources_proto.resources[0].resource_uri\n    tuning_job_name = tuning_job_id[tuning_job_id.find(\"project\") :]\n\n    job_resource = aip.HyperparameterTuningJob.get(tuning_job_name).gca_resource\n\n    trials = job_resource.trials\n\n    if len(study_spec_metrics) > 1:\n        raise RuntimeError(\n            \"Unable to determine best parameters for multi-objective hyperparameter tuning.\"  # noqa: E501\n        )\n\n    goal = study_spec_metrics[0][\"goal\"]\n    if goal == study.StudySpec.MetricSpec.GoalType.MAXIMIZE:\n        best_fn = max\n    elif goal == study.StudySpec.MetricSpec.GoalType.MINIMIZE:\n        best_fn = min\n    best_trial = best_fn(\n        trials, key=lambda trial: trial.final_measurement.metrics[0].value\n    )\n\n    return {p.parameter_id: p.value for p in best_trial.parameters}\n"
  },
  {
    "repo": "Saoussen-CH/production-ready-MLOps-on-GCP",
    "file_path": "components/src/components/get_training_args_dict_op.py",
    "raw_url": "https://raw.githubusercontent.com/Saoussen-CH/production-ready-MLOps-on-GCP/main/components/src/components/get_training_args_dict_op.py",
    "content": "from kfp.dsl import Input, component, Dataset\n\n\n@component(base_image=\"python:3.10.14\")\ndef get_training_args_dict_op(\n    train_data: Input[Dataset],\n    valid_data: Input[Dataset],\n    test_data: Input[Dataset],\n    hypertune: bool,\n) -> dict:\n    return dict(\n        train_data=train_data.path,\n        valid_data=valid_data.path,\n        test_data=test_data.path,\n        hypertune=hypertune,\n    )\n"
  },
  {
    "repo": "Saoussen-CH/production-ready-MLOps-on-GCP",
    "file_path": "components/src/components/get_workerpool_spec_op.py",
    "raw_url": "https://raw.githubusercontent.com/Saoussen-CH/production-ready-MLOps-on-GCP/main/components/src/components/get_workerpool_spec_op.py",
    "content": "from kfp.dsl import component\n\n\n@component(base_image=\"python:3.10.14\")\ndef get_workerpool_spec_op(\n    worker_pool_specs: list,\n    args: dict = {},\n    hyperparams: dict = {},\n    env: dict = {},\n) -> list:\n    for spec in worker_pool_specs:\n        if \"args\" not in spec[\"container_spec\"]:\n            spec[\"container_spec\"][\"args\"] = []\n        for k, v in args.items():\n            spec[\"container_spec\"][\"args\"].append(f\"--{k.replace('_', '-')}={v}\")\n        for k, v in hyperparams.items():\n            spec[\"container_spec\"][\"args\"].append(f\"--{k.replace('_', '-')}={v}\")\n\n        if env:\n            if \"env\" not in spec[\"container_spec\"]:\n                spec[\"container_spec\"][\"env\"] = []\n            for k, v in env.items():\n                spec[\"container_spec\"][\"env\"].append(dict(name=k, value=v))\n\n    return worker_pool_specs\n"
  },
  {
    "repo": "Saoussen-CH/production-ready-MLOps-on-GCP",
    "file_path": "components/src/components/lookup_model_op.py",
    "raw_url": "https://raw.githubusercontent.com/Saoussen-CH/production-ready-MLOps-on-GCP/main/components/src/components/lookup_model_op.py",
    "content": "from kfp.dsl import component, Output, Model\nfrom typing import NamedTuple\n\n\n@component(\n    base_image=\"python:3.10.14\",\n    packages_to_install=[\"google-cloud-aiplatform==1.55.0\"],\n)\ndef lookup_model_op(\n    model_name: str,\n    location: str,\n    project: str,\n    model: Output[Model],\n    fail_on_model_not_found: bool = False,\n) -> NamedTuple(\"Outputs\", [(\"model_resource_name\", str), (\"training_dataset\", dict)]):  # noqa: E501 # type: ignore\n    \"\"\"\n    Fetch a model given a model name (display name) and export to GCS.\n\n    Args:\n        model_name (str): display name of the model\n        location (str): location of the Google Cloud project\n        project (str): project id of the Google Cloud project\n        model (Output[Model]): a Vertex AI model\n        fail_on_model_not_found (bool): if set to True, raise runtime error if\n            model is not found\n\n    Returns:\n        str: Resource name of the found model. Empty string if model not found.\n    \"\"\"\n\n    import json\n    import logging\n    import os\n    from pathlib import Path\n    from google.cloud.aiplatform import Model\n\n    TRAINING_DATASET_INFO = \"training_dataset.json\"\n\n    logging.info(f\"listing models with display name {model_name}\")\n    models = Model.list(\n        filter=f'display_name=\"{model_name}\"',\n        location=location,\n        project=project,\n    )\n    logging.info(f\"found {len(models)} model(s)\")\n\n    training_dataset = {}\n    model_resource_name = \"\"\n    if len(models) == 0:\n        logging.error(\n            f\"No model found with name {model_name} \"\n            + f\"(project: {project} location: {location})\"\n        )\n        if fail_on_model_not_found:\n            raise RuntimeError(\"Failed as model was not found\")\n    elif len(models) == 1:\n        target_model = models[0]\n        model_resource_name = target_model.resource_name\n        logging.info(f\"model display name: {target_model.display_name}\")\n        logging.info(f\"model resource name: {target_model.resource_name}\")\n        logging.info(f\"model uri: {target_model.uri}\")\n        model.uri = target_model.uri\n        model.metadata[\"resourceName\"] = target_model.resource_name\n        logging.info(f\"target_model.resource_name : {target_model.resource_name} \")\n\n        path = Path(model.path) / TRAINING_DATASET_INFO\n        logging.info(f\"Reading training dataset metadata: {path}\")\n\n        if os.path.exists(path):\n            with open(path, \"r\") as fp:\n                training_dataset = json.load(fp)\n            logging.info(f\"Training dataset info for model monitoring path: {path}\")\n            logging.info(f\"Training dataset: {training_dataset}\")\n        else:\n            logging.warning(\"Training dataset metadata doesn't exist!\")\n    else:\n        raise RuntimeError(f\"Multiple models with name {model_name} were found.\")\n\n    return model_resource_name, training_dataset\n"
  },
  {
    "repo": "Saoussen-CH/production-ready-MLOps-on-GCP",
    "file_path": "components/src/components/model_batch_predict_op.py",
    "raw_url": "https://raw.githubusercontent.com/Saoussen-CH/production-ready-MLOps-on-GCP/main/components/src/components/model_batch_predict_op.py",
    "content": "from kfp.dsl import Input, Model, component, OutputPath\nfrom typing import List, NamedTuple\n\n\n@component(\n    base_image=\"python:3.10\",\n    packages_to_install=[\n        \"google-cloud-pipeline-components==2.14.1\",\n        \"google-cloud-aiplatform==1.55.0\",\n    ],\n)\ndef model_batch_predict_op(\n    model: Input[Model],\n    gcp_resources: OutputPath(str),  # type: ignore\n    job_display_name: str,\n    location: str,\n    project: str,\n    source_uri: str,\n    destination_uri: str,\n    source_format: str,\n    destination_format: str,\n    machine_type: str = \"n1-standard-2\",\n    starting_replica_count: int = 1,\n    max_replica_count: int = 1,\n    monitoring_training_dataset: dict = None,\n    monitoring_alert_email_addresses: List[str] = None,\n    notification_channels: List[str] = [],\n    monitoring_skew_config: dict = None,\n    instance_config: dict = None,\n) -> NamedTuple(\"Outputs\", [(\"gcp_resources\", str)]):  # type: ignore\n    \"\"\"\n    Trigger a batch prediction job and enable monitoring.\n\n    Args:\n        model (Input[Model]): Input model to use for calculating predictions.\n        job_display_name: Name of the batch prediction job.\n        location (str): location of the Google Cloud project. Defaults to None.\n        project (str): project id of the Google Cloud project. Defaults to None.\n        source_uri (str): bq:// URI or a list of gcs:// URIs to read input instances.\n        destination_uri (str): bq:// or gs:// URI to store output predictions.\n        source_format (str): E.g. \"bigquery\", \"jsonl\", \"csv\". See:\n            https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform_v1beta1.types.BatchPredictionJob.InputConfig\n        destination_format (str): E.g. \"bigquery\", \"jsonl\", \"csv\". See:\n            https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform_v1beta1.types.BatchPredictionJob.OutputConfig\n        machine_type (str): Machine type.\n        starting_replica_count (int): Starting replica count.\n        max_replica_count (int): Max replicat count.\n        monitoring_skew_config (dict): Configuration of training-serving skew. See:\n            https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform_v1beta1.types.ModelMonitoringObjectiveConfig.TrainingPredictionSkewDetectionConfig\n        monitoring_alert_email_addresses (List[str]):\n            Email addresses to send alerts to (optional).\n        notification_channels (List[str]):\n            Notification channels to send alerts to (optional).\n            Format: projects/<project>/notificationChannels/<notification_channel>\n        monitoring_training_dataset (dict): Metadata of training dataset. See:\n            https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform_v1beta1.types.ModelMonitoringObjectiveConfig.TrainingDataset\n        instance_config (dict): Configuration defining how to transform batch prediction\n            input instances to the instances that the Model accepts. See:\n            https://cloud.google.com/vertex-ai/docs/reference/rest/v1beta1/projects.locations.batchPredictionJobs#instanceconfig\n    Returns:\n        OutputPath: gcp_resources for Vertex AI UI integration.\n    \"\"\"\n\n    import logging\n    import time\n\n    from functools import partial\n    from google.protobuf.json_format import ParseDict, MessageToJson\n    from google.cloud.aiplatform_v1beta1.services.job_service import JobServiceClient\n    from google.cloud.aiplatform_v1beta1.types import (\n        BatchPredictionJob,\n        GetBatchPredictionJobRequest,\n    )\n    from google.cloud.aiplatform_v1beta1.types.job_state import JobState\n    from google_cloud_pipeline_components.container.v1.gcp_launcher.utils import (\n        error_util,\n    )\n    from google_cloud_pipeline_components.container.utils import execution_context\n    from google_cloud_pipeline_components.proto.gcp_resources_pb2 import GcpResources\n\n    def send_cancel_request(client: JobServiceClient, batch_job_uri: str):\n        logging.info(\"Sending BatchPredictionJob cancel request\")\n        client.cancel_batch_prediction_job(name=batch_job_uri)\n\n    def is_job_successful(job_state: JobState) -> bool:\n        _JOB_SUCCESSFUL_STATES = [\n            JobState.JOB_STATE_SUCCEEDED,\n        ]\n        _JOB_FAILED_STATES = [\n            JobState.JOB_STATE_FAILED,\n            JobState.JOB_STATE_CANCELLED,\n            JobState.JOB_STATE_EXPIRED,\n        ]\n\n        if job_state in _JOB_SUCCESSFUL_STATES:\n            logging.info(\n                f\"GetBatchPredictionJobRequest response state={job_state}. \"\n                \"Job completed\"\n            )\n            return True\n        elif job_state in _JOB_FAILED_STATES:\n            raise RuntimeError(\n                \"Job {} failed with error state: {}.\".format(response.name, job_state)\n            )\n        else:\n            logging.info(f\"Job {response.name} is in a non-final state {job_state}.\")\n        return False\n\n    _POLLING_INTERVAL_IN_SECONDS = 20\n    _CONNECTION_ERROR_RETRY_LIMIT = 5\n\n    api_endpoint = f\"{location}-aiplatform.googleapis.com\"\n\n    input_config = {\"instancesFormat\": source_format}\n    output_config = {\"predictionsFormat\": destination_format}\n    if source_format == \"bigquery\" and destination_format == \"bigquery\":\n        input_config[\"bigquerySource\"] = {\"inputUri\": source_uri}\n        output_config[\"bigqueryDestination\"] = {\"outputUri\": destination_uri}\n    else:\n        input_config[\"gcsSource\"] = {\"uris\": [source_uri]}\n        output_config[\"gcsDestination\"] = {\"outputUriPrefix\": destination_uri}\n\n    message = {\n        \"displayName\": job_display_name,\n        \"model\": model.metadata[\"resourceName\"],\n        \"inputConfig\": input_config,\n        \"outputConfig\": output_config,\n        \"dedicatedResources\": {\n            \"machineSpec\": {\"machineType\": machine_type},\n            \"startingReplicaCount\": starting_replica_count,\n            \"maxReplicaCount\": max_replica_count,\n        },\n    }\n\n    if instance_config:\n        message[\"instanceConfig\"] = instance_config\n\n    if monitoring_training_dataset and monitoring_skew_config:\n        logging.info(\"Adding monitoring config to request\")\n        if not monitoring_alert_email_addresses:\n            monitoring_alert_email_addresses = []\n\n        message[\"modelMonitoringConfig\"] = {\n            \"alertConfig\": {\n                \"emailAlertConfig\": {\"userEmails\": monitoring_alert_email_addresses},\n                \"notificationChannels\": notification_channels,\n                \"enableLogging\": True,\n            },\n            \"objectiveConfigs\": [\n                {\n                    \"trainingDataset\": monitoring_training_dataset,\n                    \"trainingPredictionSkewDetectionConfig\": monitoring_skew_config,\n                }\n            ],\n        }\n\n    request = ParseDict(message, BatchPredictionJob()._pb)\n\n    logging.info(f\"Submitting batch prediction job: {job_display_name}\")\n    logging.info(request)\n    client = JobServiceClient(client_options={\"api_endpoint\": api_endpoint})\n    response = client.create_batch_prediction_job(\n        parent=f\"projects/{project}/locations/{location}\",\n        batch_prediction_job=request,\n    )\n    logging.info(f\"Submitted batch prediction job: {response.name}\")\n\n    # output GCP resource for Vertex AI UI integration\n    batch_job_resources = GcpResources()\n    dr = batch_job_resources.resources.add()\n    dr.resource_type = \"BatchPredictionJob\"\n    dr.resource_uri = response.name\n    with open(gcp_resources, \"w\") as f:\n        f.write(MessageToJson(batch_job_resources))\n\n    with execution_context.ExecutionContext(\n        on_cancel=partial(\n            send_cancel_request,\n            api_endpoint,\n            response.name,\n        )\n    ):\n        retry_count = 0\n        while True:\n            try:\n                job_status_request = GetBatchPredictionJobRequest(\n                    {\"name\": response.name}\n                )\n                job_state = client.get_batch_prediction_job(\n                    request=job_status_request\n                ).state\n                retry_count = 0\n            except ConnectionError as err:\n                retry_count += 1\n                if retry_count <= _CONNECTION_ERROR_RETRY_LIMIT:\n                    logging.warning(\n                        f\"ConnectionError ({err}) encountered when polling job: \"\n                        f\"{response.name}. Retrying.\"\n                    )\n                else:\n                    error_util.exit_with_internal_error(\n                        f\"Request failed after {_CONNECTION_ERROR_RETRY_LIMIT} retries.\"\n                    )\n            if is_job_successful(job_state):\n                break\n            logging.info(\n                f\"Waiting for {_POLLING_INTERVAL_IN_SECONDS} seconds for next poll.\"\n            )\n            time.sleep(_POLLING_INTERVAL_IN_SECONDS)\n\n    # return GCP resource for Vertex AI UI integration\n    batch_job_resources = GcpResources()\n    dr = batch_job_resources.resources.add()\n    dr.resource_type = \"BatchPredictionJob\"\n    dr.resource_uri = response.name\n    gcp_resources = MessageToJson(batch_job_resources)\n\n    return (gcp_resources,)\n"
  },
  {
    "repo": "Saoussen-CH/production-ready-MLOps-on-GCP",
    "file_path": "components/src/components/upload_best_model_op.py",
    "raw_url": "https://raw.githubusercontent.com/Saoussen-CH/production-ready-MLOps-on-GCP/main/components/src/components/upload_best_model_op.py",
    "content": "from kfp.dsl import Dataset, Input, Metrics, Model, Output, component\nfrom google_cloud_pipeline_components.types.artifact_types import VertexModel\n\n\n@component(\n    base_image=\"python:3.10\",\n    packages_to_install=[\n        \"google-cloud-aiplatform==1.55.0\",\n        \"google-cloud-pipeline-components==2.14.1\",\n    ],\n)\ndef upload_best_model_op(\n    model: Input[Model],\n    test_data: Input[Dataset],\n    model_eval_metrics: Input[Metrics],\n    vertex_model: Output[VertexModel],\n    project: str,\n    location: str,\n    model_name: str,\n    eval_metric: str,\n    eval_lower_is_better: bool,\n    pipeline_job_id: str,\n    serving_container_image: str,\n    model_description: str = None,\n    evaluation_name: str = \"Imported evaluation\",\n) -> None:\n    \"\"\"\n    Args:\n        model (Model): Input challenger model.\n        test_data (Dataset): Test dataset used for evaluating challenger model.\n        vertex_model (VertexModel): Output model uploaded to Vertex AI Model Registry.\n        model_eval_metricsn (Metrics): Evaluation metrics of challenger model.\n        project (str): project id of the Google Cloud project.\n        location (str): location of the Google Cloud project.\n        pipeline_job_id (str):\n        model_name (str): Name of champion and challenger model in\n            Vertex AI Model Registry.\n        eval_metric (str): Metric name to compare champion and challenger on.\n        eval_lower_is_better (bool): Usually True for losses and\n            False for classification metrics.\n        serving_container_image (str): Container URI for serving the model.\n        model_description (str): Optional. Description of model.\n        evaluation_name (str): Optional. Name of evaluation results which are\n            displayed in the Vertex AI UI of the challenger model.\n    \"\"\"\n\n    import json\n    import logging\n    import google.cloud.aiplatform as aip\n    from google.protobuf.json_format import MessageToDict\n    from google.cloud.aiplatform_v1 import ModelEvaluation, ModelServiceClient\n    from google.protobuf.json_format import ParseDict\n\n    def lookup_model(model_name: str) -> aip.Model:\n        \"\"\"Look up model in model registry.\"\"\"\n        logging.info(f\"listing models with display name {model_name}\")\n        models = aip.Model.list(\n            filter=f'display_name=\"{model_name}\"',\n            location=location,\n            project=project,\n        )\n        logging.info(f\"found {len(models)} models\")\n\n        if len(models) == 0:\n            logging.info(\n                f\"No model found with name {model_name}\"\n                + f\"(project: {project} location: {location})\"\n            )\n            return None\n        elif len(models) == 1:\n            return models[0]\n        else:\n            raise RuntimeError(f\"Multiple models with name {model_name} were found.\")\n\n    def compare_models(\n        champion_metrics: dict,\n        challenger_metrics: dict,\n        eval_lower_is_better: bool,\n    ) -> bool:\n        \"\"\"Compare models by evaluating a primary metric.\"\"\"\n        logging.info(f\"Comparing {eval_metric} of models\")\n        logging.debug(f\"Champion metrics: {champion_metrics}\")\n        logging.debug(f\"Challenger metrics: {challenger_metrics}\")\n\n        m_champ = champion_metrics[eval_metric]\n        m_chall = challenger_metrics[eval_metric]\n        logging.info(f\"Champion={m_champ} Challenger={m_chall}\")\n\n        challenger_wins = (\n            (m_chall < m_champ) if eval_lower_is_better else (m_chall > m_champ)\n        )\n        logging.info(f\"{'Challenger' if challenger_wins else 'Champion'} wins!\")\n\n        return challenger_wins\n\n    def upload_model_to_registry(\n        is_default_version: bool, parent_model_uri: str = None\n    ) -> Model:\n        \"\"\"Upload model to registry.\"\"\"\n        logging.info(f\"Uploading model {model_name} (default: {is_default_version}\")\n        uploaded_model = aip.Model.upload(\n            display_name=model_name,\n            description=model_description,\n            artifact_uri=model.uri,\n            serving_container_image_uri=serving_container_image,\n            parent_model=parent_model_uri,\n            is_default_version=is_default_version,\n        )\n        logging.info(f\"Uploaded model {uploaded_model}\")\n\n        # Output google.VertexModel artifact\n        vertex_model.uri = (\n            f\"https://{location}-aiplatform.googleapis.com/v1/\"\n            f\"{uploaded_model.versioned_resource_name}\"\n        )\n        vertex_model.metadata[\"resourceName\"] = uploaded_model.versioned_resource_name\n\n        return uploaded_model\n\n    def import_evaluation(\n        parsed_metrics: dict,\n        challenger_model: aip.Model,\n        evaluation_name: str,\n    ) -> str:\n        \"\"\"Import model evaluation.\"\"\"\n        logging.info(f\"Evaluation metrics: {parsed_metrics}\")\n        problem_type = parsed_metrics.pop(\"problemType\")\n        schema = (\n            f\"gs://google-cloud-aiplatform/schema/modelevaluation/\"\n            f\"{problem_type}_metrics_1.0.0.yaml\"\n        )\n        evaluation = {\n            \"displayName\": evaluation_name,\n            \"metricsSchemaUri\": schema,\n            \"metrics\": parsed_metrics,\n            \"metadata\": {\n                \"pipeline_job_id\": pipeline_job_id,\n                \"evaluation_dataset_type\": \"gcs\",\n                \"evaluation_dataset_path\": [test_data.uri],\n            },\n        }\n\n        request = ParseDict(evaluation, ModelEvaluation()._pb)\n        logging.debug(f\"Request: {request}\")\n        challenger_name = challenger_model.versioned_resource_name\n        client = ModelServiceClient(\n            client_options={\"api_endpoint\": location + \"-aiplatform.googleapis.com\"}\n        )\n        logging.info(f\"Uploading model evaluation for {challenger_name}\")\n        response = client.import_model_evaluation(\n            parent=challenger_name,\n            model_evaluation=request,\n        )\n        logging.debug(f\"Response: {response}\")\n        return response.name\n\n    # Parse metrics to dict\n    with open(model_eval_metrics.path, \"r\") as f:\n        challenger_metrics = json.load(f)\n\n    champion_model = lookup_model(model_name=model_name)\n\n    challenger_wins = True\n    parent_model_uri = None\n    if champion_model is None:\n        logging.info(\"No champion model found, uploading new model.\")\n    else:\n        # Compare models\n        logging.info(\n            f\"Model default version {champion_model.version_id} \"\n            \"is being challenged by new model.\"\n        )\n        # Look up Vertex model evaluation for champion model\n        champion_eval = champion_model.get_model_evaluation()\n        champion_metrics = MessageToDict(champion_eval._gca_resource._pb)[\"metrics\"]\n\n        challenger_wins = compare_models(\n            champion_metrics=champion_metrics,\n            challenger_metrics=challenger_metrics,\n            eval_lower_is_better=eval_lower_is_better,\n        )\n        parent_model_uri = champion_model.resource_name\n\n    model = upload_model_to_registry(challenger_wins, parent_model_uri)\n\n    import_evaluation(\n        parsed_metrics=challenger_metrics,\n        challenger_model=model,\n        evaluation_name=evaluation_name,\n    )\n"
  },
  {
    "repo": "Saoussen-CH/production-ready-MLOps-on-GCP",
    "file_path": "pipelines/src/pipelines/prediction.py",
    "raw_url": "https://raw.githubusercontent.com/Saoussen-CH/production-ready-MLOps-on-GCP/main/pipelines/src/pipelines/prediction.py",
    "content": "from kfp import compiler, dsl\nfrom os import environ as env\nimport pathlib\n\nfrom components import (\n    lookup_model_op,\n    model_batch_predict_op,\n)\n\nfrom google_cloud_pipeline_components.v1.bigquery import BigqueryQueryJobOp\nfrom pipelines.utils.query import generate_query\n\n# set training-serving skew thresholds and emails to receive alerts:\nALERT_EMAILS = []\nNOTIFICATION_CHANNELS = []\nSKEW_THRESHOLDS = {\"defaultSkewThreshold\": {\"value\": 0.001}}\n# or set different thresholds per feature:\n# SKEW_THRESHOLDS = {\"skewThresholds\": {\"payment_type\": {\"value\": 0.001}}, ... }\n\n\n@dsl.pipeline(name=\"taxifare-batch-prediction-pipeline\")\ndef pipeline(\n    project: str = env.get(\"VERTEX_PROJECT_ID\"),\n    location: str = env.get(\"VERTEX_LOCATION\"),\n    bq_location: str = env.get(\"BQ_LOCATION\"),\n    bq_source_uri: str = \"bigquery-public-data.chicago_taxi_trips.taxi_trips\",\n    dataset: str = \"taxi_trips_dataset\",\n    timestamp: str = \"2022-12-01 00:00:00\",\n    use_latest_data: bool = True,  # Parameter to use the latest data or fixed timestamp\n    model_name: str = \"taxi-traffic-model\",\n    machine_type: str = \"n2-standard-4\",\n    min_replicas: int = 3,\n    max_replicas: int = 10,\n):\n    \"\"\"\n    Prediction pipeline which:\n     1. Looks up the default model version (champion).\n     2. Runs a batch prediction job with BigQuery as input and output\n     3. Optionally monitors training-serving skew\n\n    Args:\n        project (str): project id of the Google Cloud project\n        location (str): location of the Google Cloud project\n        bq_location (str): location of dataset in BigQuery\n        bq_source_uri (str): `<project>.<dataset>.<table>` of ingestion data in BigQuery\n        model_name (str): name of model\n        dataset (str): dataset id to store staging data & predictions in BigQuery\n        timestamp (str): Optional. Empty or a specific timestamp in ISO 8601 format\n            (YYYY-MM-DDThh:mm:ss.sss\u00b1hh:mm or YYYY-MM-DDThh:mm:ss).\n            If any time part is missing, it will be regarded as zero\n        use_latest_data (bool): Whether to use the latest available data\n        machine_type (str): Machine type to be used for Vertex Batch\n            Prediction. Example machine_types - n1-standard-4, n1-standard-16 etc.\n        min_replicas (int): Minimum no of machines to distribute the\n            Vertex Batch Prediction job for horizontal scalability\n        max_replicas (int): Maximum no of machines to distribute the\n            Vertex Batch Prediction job for horizontal scalability\n    \"\"\"\n\n    table = \"prep_prediction_table\"\n\n    queries_folder = pathlib.Path(__file__).parent / \"queries\"\n\n    prep_query = generate_query(\n        input_file=queries_folder / \"ingest_pred.sql\",\n        source=bq_source_uri,\n        location=bq_location,\n        dataset=f\"{project}.{dataset}\",\n        table_=table,\n        label=\"total_fare\",  # Assuming the label is 'total_fare'\n        start_timestamp=timestamp,\n        use_latest_data=use_latest_data,\n    )\n\n    prep_op = BigqueryQueryJobOp(\n        project=project,\n        location=\"US\",\n        query=prep_query,\n    ).set_display_name(\"Ingest & preprocess data\")\n\n    # lookup champion model\n    champion_model = lookup_model_op(\n        model_name=model_name,\n        location=location,\n        project=project,\n        fail_on_model_not_found=True,\n    ).set_display_name(\"Look up champion model\")\n\n    # batch predict from BigQuery to BigQuery\n    (\n        model_batch_predict_op(\n            model=champion_model.outputs[\"model\"],\n            job_display_name=\"taxi-fare-predict-job\",\n            location=location,\n            project=project,\n            source_uri=f\"bq://{project}.{dataset}.{table}\",\n            destination_uri=f\"bq://{project}.{dataset}\",\n            source_format=\"bigquery\",\n            destination_format=\"bigquery\",\n            instance_config={\n                \"instanceType\": \"object\",\n            },\n            machine_type=machine_type,\n            starting_replica_count=min_replicas,\n            max_replica_count=max_replicas,\n            monitoring_training_dataset=champion_model.outputs[\"training_dataset\"],\n            monitoring_alert_email_addresses=ALERT_EMAILS,\n            notification_channels=NOTIFICATION_CHANNELS,\n            monitoring_skew_config=SKEW_THRESHOLDS,\n        )\n        .after(prep_op)\n        .set_display_name(\"Run prediction job\")\n    )\n\n\nif __name__ == \"__main__\":\n    compiler.Compiler().compile(\n        pipeline_func=pipeline, package_path=\"taxifare-prediction-pipeline.yaml\"\n    )\n"
  },
  {
    "repo": "Saoussen-CH/production-ready-MLOps-on-GCP",
    "file_path": "pipelines/src/pipelines/training.py",
    "raw_url": "https://raw.githubusercontent.com/Saoussen-CH/production-ready-MLOps-on-GCP/main/pipelines/src/pipelines/training.py",
    "content": "import pathlib\nimport logging\n\nfrom components import (\n    extract_table_to_gcs_op,\n    get_custom_job_results_op,\n    get_training_args_dict_op,\n    get_workerpool_spec_op,\n    upload_best_model_op,\n    get_hyperparameter_tuning_results_op,\n)\n\nfrom os import environ as env\n\nfrom google_cloud_pipeline_components.v1.bigquery import BigqueryQueryJobOp\nfrom google_cloud_pipeline_components.v1.custom_job import CustomTrainingJobOp\nfrom google_cloud_pipeline_components.v1.hyperparameter_tuning_job import (\n    HyperparameterTuningJobRunOp,\n)\nfrom google_cloud_pipeline_components.v1 import hyperparameter_tuning_job\nfrom google.cloud.aiplatform import hyperparameter_tuning as hpt\n\nfrom kfp import compiler, dsl\n\nfrom pipelines.utils.query import generate_query\n\nbq_source_uri = \"bigquery-public-data.chicago_taxi_trips.taxi_trips\"\ndataset = \"prerocessing\"\ntable = \"taxi_fare\"\nlabel = \"total_fare\"\ntimestamp = \"2022-12-01 00:00:00\"\n\n\n# define the metric spec for hyperparameter tuning\n# for details:\n# https://cloud.google.com/vertex-ai/docs/reference/rest/v1/StudySpec#MetricSpec\nMETRIC_SPEC = dict(val_root_mean_squared_error=\"minimize\")\n\n# define the parameter specs for tuning\n# for details:\n# https://cloud.google.com/vertex-ai/docs/reference/rest/v1/StudySpec#ParameterSpec\nPARAMETER_SPEC = {\n    \"learning-rate\": hpt.DoubleParameterSpec(min=0.0001, max=1, scale=\"log\"),\n    \"batch-size\": hpt.DiscreteParameterSpec(values=[128, 256, 512], scale=\"linear\"),\n}\n\n\nPREDICTION_IMAGE = \"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-11:latest\"\n\n\n@dsl.pipeline(name=\"taxifare-training-pipeline\")\ndef pipeline(\n    project: str = env.get(\"VERTEX_PROJECT_ID\"),\n    location: str = env.get(\"VERTEX_LOCATION\"),\n    bq_location: str = env.get(\"BQ_LOCATION\"),\n    bq_source_uri: str = \"bigquery-public-data.chicago_taxi_trips.taxi_trips\",\n    dataset: str = \"taxi_trips_dataset\",\n    timestamp: str = \"2022-12-01 00:00:00\",  # Optional timestamp parameter\n    use_latest_data: bool = True,  # Parameter to use the latest data or fixed timestamp\n    base_output_dir: str = \"\",\n    training_job_display_name: str = \"\",\n    model_name: str = \"taxi-traffic-model\",\n):\n    \"\"\"\n    Training pipeline which:\n     1. Preprocesses data in BigQuery\n     2. Extracts data to Cloud Storage\n     3. Trains a model using a custom prebuilt container\n     4. Uploads the model to Model Registry\n     5. Evaluates the model against a champion model\n     6. Selects a new champion based on the primary metrics\n\n    Args:\n        project (str): project id of the Google Cloud project\n        location (str): location of the Google Cloud project\n        bq_location (str): location of dataset in BigQuery\n        bq_source_uri (str): `<project>.<dataset>.<table>` of ingestion data in BigQuery\n        model_name (str): name of model\n        dataset (str): dataset id to store staging data & predictions in BigQuery\n        timestamp (str): Optional. Empty or a specific timestamp in ISO 8601 format\n            (YYYY-MM-DDThh:mm:ss.sss\u00b1hh:mm or YYYY-MM-DDThh:mm:ss).\n            If any time part is missing, it will be regarded as zero.\n        use_latest_data (bool): Whether to use the latest available data\n        base_output_dir (str): base output directory for the training job\n        training_job_display_name (str): display name for the training job\n        model_name (str): name of the model\n    \"\"\"\n    PRIMARY_METRIC = \"rootMeanSquaredError\"\n    queries_folder = pathlib.Path(__file__).parent / \"queries\"\n\n    preprocessed_table = \"preprocessed_data\"\n\n    training_image = env.get(\"TRAINING_IMAGE\")\n\n    logging.info(f\"Training image URI: {training_image}\")\n    # define the workerpool spec for the custom jobs\n    # (https://cloud.google.com/vertex-ai/docs/reference/rest/v1/CustomJobSpec)\n    WORKER_POOL_SPECS = [\n        dict(\n            machine_spec=dict(\n                machine_type=\"n1-standard-4\",\n            ),\n            replica_count=1,\n            container_spec=dict(\n                image_uri=training_image,\n            ),\n        )\n    ]\n\n    # Generate the preprocessing query\n    prep_query = generate_query(\n        input_file=queries_folder / \"ingest.sql\",\n        source=bq_source_uri,\n        location=bq_location,\n        dataset=f\"{project}.{dataset}\",\n        table_=preprocessed_table,\n        label=label,\n        start_timestamp=timestamp,\n        use_latest_data=use_latest_data,\n    )\n\n    prep_op = BigqueryQueryJobOp(\n        project=project,\n        location=\"US\",\n        query=prep_query,\n    ).set_display_name(\"Ingest & preprocess data\")\n\n    split_train_query = generate_query(\n        input_file=queries_folder / \"repeatable_splitting.sql\",\n        source_dataset=f\"{project}.{dataset}\",\n        source_table=preprocessed_table,\n        num_lots=10,\n        lots=tuple(range(8)),\n    )\n\n    split_valid_query = generate_query(\n        input_file=queries_folder / \"repeatable_splitting.sql\",\n        source_dataset=f\"{project}.{dataset}\",\n        source_table=preprocessed_table,\n        num_lots=10,\n        lots=\"(8)\",\n    )\n\n    split_test_query = generate_query(\n        input_file=queries_folder / \"repeatable_splitting.sql\",\n        source_dataset=f\"{project}.{dataset}\",\n        source_table=preprocessed_table,\n        num_lots=10,\n        lots=\"(9)\",\n    )\n\n    split_train_data = (\n        BigqueryQueryJobOp(\n            project=project,\n            location=bq_location,\n            query=split_train_query,\n        )\n        .after(prep_op)\n        .set_display_name(\"Split train data\")\n    )\n\n    train_dataset = (\n        extract_table_to_gcs_op(bq_table=split_train_data.outputs[\"destination_table\"])\n        .after(split_train_data)\n        .set_display_name(\"Extract training data from BigQuery to GCS\")\n    )\n\n    split_valid_data = (\n        BigqueryQueryJobOp(\n            project=project,\n            location=bq_location,\n            query=split_valid_query,\n        )\n        .after(prep_op)\n        .set_display_name(\"Split valid data\")\n    )\n\n    valid_dataset = (\n        extract_table_to_gcs_op(bq_table=split_valid_data.outputs[\"destination_table\"])\n        .after(split_valid_data)\n        .set_display_name(\"Extract validation data from BigQuery to GCS\")\n    )\n\n    split_test_data = (\n        BigqueryQueryJobOp(\n            project=project,\n            location=bq_location,\n            query=split_test_query,\n        )\n        .after(prep_op)\n        .set_display_name(\"Split test data\")\n    )\n\n    test_dataset = (\n        extract_table_to_gcs_op(bq_table=split_test_data.outputs[\"destination_table\"])\n        .after(split_test_data)\n        .set_display_name(\"Extract test data from BigQuery to GCS\")\n    )\n\n    # define training args\n    args = dict(\n        train_data=train_dataset.outputs[\"dataset\"],\n        valid_data=valid_dataset.outputs[\"dataset\"],\n        test_data=test_dataset.outputs[\"dataset\"],\n        hypertune=True,\n    )\n\n    hypertune_args_step = get_training_args_dict_op(**args).set_display_name(\n        \"Get-Hypertune-Args\"\n    )\n\n    # create the workerpool spec for hyperparameter tuning\n    # dont provide hyperparams, because they are defined in the PARAMETER_SPEC\n    # and directly passed to the hyperparameter tuning job\n    hypertune_worker_pool_specs_step = get_workerpool_spec_op(\n        worker_pool_specs=WORKER_POOL_SPECS,\n        args=hypertune_args_step.output,\n    ).set_display_name(\"Get-Hypertune-Worker-Pool-Spec\")\n\n    # create the actual hyperparameter tuning job\n    # here you can choose how many trials to do and how many to run in parallel\n    hypertune_step = HyperparameterTuningJobRunOp(\n        display_name=\"hypertune-job\",\n        project=project,\n        location=location,\n        worker_pool_specs=hypertune_worker_pool_specs_step.output,\n        study_spec_metrics=hyperparameter_tuning_job.utils.serialize_metrics(\n            METRIC_SPEC\n        ),\n        study_spec_parameters=hyperparameter_tuning_job.utils.serialize_parameters(\n            PARAMETER_SPEC\n        ),\n        max_trial_count=6,\n        parallel_trial_count=2,\n        base_output_directory=f\"{base_output_dir}/hypertune-job\",\n    ).set_display_name(\"Hypertune-Job\")\n\n    # now we can extract the results of the hyperparameter tuning job\n    hypertune_results_step = get_hyperparameter_tuning_results_op(\n        project=project,\n        location=location,\n        job_resource=hypertune_step.output,\n        study_spec_metrics=hyperparameter_tuning_job.utils.serialize_metrics(\n            METRIC_SPEC\n        ),\n    ).set_display_name(\"Get-Hypertune-Results\")\n\n    # update our args dict for training\n    args.update(dict(hypertune=False))\n\n    # create the args dict\n    training_args_step = get_training_args_dict_op(**args).set_display_name(\n        \"Get-Training-Args\"\n    )\n\n    # create the workerpool spec for training\n    training_worker_pool_specs_step = get_workerpool_spec_op(\n        worker_pool_specs=WORKER_POOL_SPECS,\n        hyperparams=hypertune_results_step.output,\n        args=training_args_step.output,\n    ).set_display_name(\"Get-Training-Worker-Pool-Spec\")\n\n    # Train the model\n    custom_job_task = CustomTrainingJobOp(\n        project=project,\n        display_name=training_job_display_name,\n        worker_pool_specs=training_worker_pool_specs_step.output,\n        base_output_directory=f\"{base_output_dir}/training-job\",\n        location=location,\n    )\n\n    # now we can extract the training results\n    training_results_step = get_custom_job_results_op(\n        project=project, location=location, job_resource=custom_job_task.output\n    ).set_display_name(\"Get-Training-Results\")\n\n    upload_best_model_op(\n        project=project,\n        location=location,\n        model=training_results_step.outputs[\"model\"],\n        model_eval_metrics=training_results_step.outputs[\"metrics\"],\n        test_data=test_dataset.outputs[\"dataset\"],\n        eval_metric=PRIMARY_METRIC,\n        eval_lower_is_better=True,\n        serving_container_image=PREDICTION_IMAGE,\n        model_name=model_name,\n        model_description=\"Predict price of a taxi trip.\",\n        pipeline_job_id=\"{{$.pipeline_job_name}}\",\n    ).set_display_name(\"Upload model\")\n\n\nif __name__ == \"__main__\":\n    compiler.Compiler().compile(\n        pipeline_func=pipeline, package_path=\"taxifare-training-pipeline.yaml\"\n    )\n"
  },
  {
    "repo": "NashTech-Labs/Databricks-Dolly-LLM-Fine-Tuning",
    "file_path": "pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/NashTech-Labs/Databricks-Dolly-LLM-Fine-Tuning/dolly-fine-tuning-and-serving/pipeline.py",
    "content": "import logging\nfrom kfp.v2 import compiler\nimport kfp\n\nfrom components.process_data import process_data\nfrom components.serve_model import serve_model_component\nfrom components.train_model import fine_tune_model\nfrom components.upload_model import upload_container\nfrom constants import (PIPELINE_DESCRIPTION, PIPELINE_NAME, PIPELINE_ROOT_GCS, ORIGINAL_MODEL_NAME, \\\n                       SAVE_MODEL_BUCKET_NAME, REGION, DATASET_BUCKET, MODEL_DISPLAY_NAME, SERVING_IMAGE, \\\n                       STAGING_BUCKET, COMPONENT_EXECUTION, DATASET_NAME, SERVING_IMAGE_TRIGGER, SERVICE_ACCOUNT_ML,\n                       DEPLOYED_MODEL_DETAILS_FILE,\n                       PIPELINE_JSON_FILE)\n\nlogging.basicConfig(level=logging.DEBUG)\nlogger = logging.getLogger(__name__)\n\n\n@kfp.dsl.pipeline(name=PIPELINE_NAME,\n                  description=PIPELINE_DESCRIPTION,\n                  pipeline_root=PIPELINE_ROOT_GCS)\ndef pipeline(\n        project_id: str,\n        job_id: str\n):\n    \"\"\"Dataset Processing\"\"\"\n    process_data_task = process_data(DATASET_BUCKET, DATASET_NAME).set_display_name(\"Data_Processing\")\n\n    \"\"\"Fine Tune Model Pipeline\"\"\"\n    train_model_task = fine_tune_model(process_data_task.outputs[\"dataset\"],\n                                       ORIGINAL_MODEL_NAME,\n                                       SAVE_MODEL_BUCKET_NAME,\n                                       COMPONENT_EXECUTION) \\\n        .after(process_data_task) \\\n        .set_display_name(\"Dolly Fine Tuning\") \\\n        .set_cpu_request(\"8\") \\\n        .set_memory_limit(\"32G\")\n\n    \"\"\"Upload model package\"\"\"\n    upload_model_task = upload_container(project_id=project_id,\n                                         trigger_id=SERVING_IMAGE_TRIGGER,\n                                         component_execution=COMPONENT_EXECUTION) \\\n        .after(train_model_task) \\\n        .set_display_name(\"Model_Upload\")\n\n    \"\"\"Serve Model To Endpoint\"\"\"\n    serve_model_component(project_id,\n                          REGION,\n                          STAGING_BUCKET,\n                          SERVING_IMAGE,\n                          MODEL_DISPLAY_NAME,\n                          COMPONENT_EXECUTION,\n                          SERVICE_ACCOUNT_ML,\n                          save_model_details_bucket=DATASET_BUCKET,\n                          model_details_file_name=DEPLOYED_MODEL_DETAILS_FILE) \\\n        .after(upload_model_task) \\\n        .set_display_name(\"Serve_Model\") \\\n        .set_cpu_request(\"8\") \\\n        .set_memory_limit(\"32G\")\n\n\ndef compile_pipeline(pipeline_template_name=f'{PIPELINE_JSON_FILE}'):\n    compiler.Compiler().compile(\n        pipeline_func=pipeline,\n        package_path=pipeline_template_name\n    )\n    return None\n\n\nif __name__ == \"__main__\":\n    compile_pipeline()\n"
  }
]