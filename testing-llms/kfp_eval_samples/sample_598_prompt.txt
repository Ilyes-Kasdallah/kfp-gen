You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:

Training pipeline which:      1. Preprocesses data in BigQuery      2. Extracts data to Cloud Storage      3. Trains a model using a custom prebuilt container      4. Uploads the model to Model Registry      5. Evaluates the model against a champion model      6. Selects a new champion based on the primary metrics      Args:         project (str): project id of the Google Cloud project         location (str): location of the Google Cloud project         bq_location (str): location of dataset in BigQuery         bq_source_uri (str): `<project>.<dataset>.<table>` of ingestion data in BigQuery         model_name (str): name of model         dataset (str): dataset id to store staging data & predictions in BigQuery         timestamp (str): Optional. Empty or a specific timestamp in ISO 8601 format             (YYYY-MM-DDThh:mm:ss.sssÂ±hh:mm or YYYY-MM-DDThh:mm:ss).             If any time part is missing, it will be regarded as zero.         use_latest_data (bool): Whether to use the latest available data         base_output_dir (str): base output directory for the training job         training_job_display_name (str): display name for the training job         model_name (str): name of the model define the metric spec for hyperparameter tuning for details: https://cloud.google.com/vertex-ai/docs/reference/rest/v1/StudySpec#MetricSpec define the parameter specs for tuning for details: