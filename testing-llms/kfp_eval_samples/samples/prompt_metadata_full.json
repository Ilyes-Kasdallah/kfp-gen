[
  {
    "id": 1,
    "repo": "kubeflow/pipelines",
    "file_path": "components/snowflake/snowflake_unload_data.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nThis is a KFP component doing \"unload data to GCS bucket\" operation   from the Snowflake database."
  },
  {
    "id": 2,
    "repo": "kubeflow/pipelines",
    "file_path": "samples/test/after.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nname: Print Text inputs: - {name: text, type: String} implementation:   container:     image: alpine     command:     - sh     - -c     - |       set -e -x       echo \"$0\"     - {inputValue: text} Copyright 2020 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 3,
    "repo": "kubeflow/pipelines",
    "file_path": "samples/test/fail_v2.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nFail pipeline. Copyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 4,
    "repo": "kubeflow/pipelines",
    "file_path": "samples/test/lightweight_python_functions_v2_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nSample pipeline for passing data in KFP v2. Copyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 5,
    "repo": "kubeflow/pipelines",
    "file_path": "samples/test/lightweight_python_functions_v2_with_outputs.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nLightweight functions v2 with outputs. Copyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 6,
    "repo": "kubeflow/pipelines",
    "file_path": "samples/test/metrics_visualization_v2.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nCopyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 7,
    "repo": "kubeflow/pipelines",
    "file_path": "samples/test/placeholder_concat.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nname: Component with concat placeholder inputs: - {name: input_one, type: String} - {name: input_two, type: String} implementation:   container:     image: registry.k8s.io/busybox     command:     - sh     - -ec     args:     - echo \"$0\" > /tmp/test && [[ \"$0\" == 'one+two=three' ]]     - concat: [{inputValue: input_one}, '+', {inputValue: input_two}, '=three'] Copyright 2020 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 8,
    "repo": "kubeflow/pipelines",
    "file_path": "samples/test/placeholder_if_v2.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nCopyright 2020,2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 9,
    "repo": "kubeflow/pipelines",
    "file_path": "samples/test/two_step_with_uri_placeholder.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nTwo step v2-compatible pipeline with URI placeholders. Copyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 10,
    "repo": "kubeflow/pipelines",
    "file_path": "samples/v2/collected_artifacts.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nThis sample pipeline is meant to cover the following cases during artifact resolution: 1. ParallelFor task consuming input from another task within the same loop. 2. A nested ParallelFor task consuming input from another ParallelFor Iterator. 3. Resolving input with dsl.Collected inside another ParallelFor loop. 4. Resolving input that comes from a subdag using dsl.Collected inside of a ParallelFor loop."
  },
  {
    "id": 11,
    "repo": "kubeflow/pipelines",
    "file_path": "samples/v2/collected_parameters.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 12,
    "repo": "kubeflow/pipelines",
    "file_path": "samples/v2/component_with_optional_inputs.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nCopyright 2023 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 13,
    "repo": "kubeflow/pipelines",
    "file_path": "samples/v2/hello_world.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nPipeline that passes small pipeline parameter string to consumer op. Copyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 14,
    "repo": "kubeflow/pipelines",
    "file_path": "samples/v2/parallel_after_dependency.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# Ensure that the dependecy is set downstream for all loop iterations"
  },
  {
    "id": 15,
    "repo": "kubeflow/pipelines",
    "file_path": "samples/v2/parallel_consume_upstream.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nThis pipeline tests the ability to consume outputs from upstream components in a loop context as well as having the inputs resolve when set_display_name is used within the pipeline. # Consume the output from a op in the loop iteration DAG context"
  },
  {
    "id": 16,
    "repo": "kubeflow/pipelines",
    "file_path": "samples/v2/pipeline_container_no_input.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nCopyright 2022 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 17,
    "repo": "kubeflow/pipelines",
    "file_path": "samples/v2/pipeline_with_env.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nname: Check env implementation:   container:     image: alpine     command:     - sh     - -c     - |       set -e -x       if [ \"$ENV2\" == \"val2\" ]       then         echo \"$ENV2\"        else          echo \"ENV2 does not equal val2\"         exit 1       fi       echo \"$ENV3\"     env:       ENV2: val0 Copyright 2023 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 18,
    "repo": "kubeflow/pipelines",
    "file_path": "samples/v2/pipeline_with_importer.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nPipeline using dsl.importer. Copyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 19,
    "repo": "kubeflow/pipelines",
    "file_path": "samples/v2/pipeline_with_input_status_state.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n#TODO: Add assert statements to validate status.error_code and status.error_message values once those fields have been implemented."
  },
  {
    "id": 20,
    "repo": "kubeflow/pipelines",
    "file_path": "samples/v2/pipeline_with_placeholders.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nCopyright 2025 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 21,
    "repo": "riiid/krsh",
    "file_path": "krsh/cmd/group_create/templates/pipeline/pipeline.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 22,
    "repo": "riiid/krsh",
    "file_path": "tests/samples/have-pipeline-project/pipelines/pipeline-1/pipeline.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 23,
    "repo": "riiid/krsh",
    "file_path": "tests/samples/have-pipeline-project/pipelines/pipeline-2/pipeline.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 24,
    "repo": "omerbsezer/Fast-Kubeflow",
    "file_path": "Project_Kubeflow_Pipeline_MLModels/pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# Given the outputs from decision_tree, logistic regression, svm, naive_bayes, xg_boost components # Loads the yaml manifest for each component # Run download_data task # Run ML models tasks with input data # Given the outputs from ML models tasks"
  },
  {
    "id": 25,
    "repo": "ksalama/kubeflow-examples",
    "file_path": "kfp-cloudbuild/pipeline/workflow.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nPipeline workflow definition. Copyright 2019 Google LLC  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 26,
    "repo": "getindata/kedro-kubeflow",
    "file_path": "tests/test_kfpclient.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nTest kedro_kubeflow module. # given # when # then # given # when"
  },
  {
    "id": 27,
    "repo": "getindata/kedro-kubeflow",
    "file_path": "tests/test_one_pod_pipeline_generator.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nTest generator # given # when # then # given # when"
  },
  {
    "id": 28,
    "repo": "getindata/kedro-kubeflow",
    "file_path": "tests/test_pod_per_node_pipeline_generator.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nTest generator # given # when # then # given # when"
  },
  {
    "id": 29,
    "repo": "getindata/kedro-kubeflow",
    "file_path": "kedro_kubeflow/generators/one_pod_pipeline_generator.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 30,
    "repo": "getindata/kedro-kubeflow",
    "file_path": "kedro_kubeflow/generators/pod_per_node_pipeline_generator.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nConvert from a Kedro pipeline into a kfp container graph."
  },
  {
    "id": 31,
    "repo": "sbakiu/kubeflow-spark",
    "file_path": "kubeflow_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nRead Spark Operator job manifest file and return the corresponding dictionary and     add some randomness in the job name     :return: dictionary defining the spark job # Read manifest file # Add epoch time in the job name # Remove cache # Load spark job manifest # Load the kubernetes apply component"
  },
  {
    "id": 32,
    "repo": "FernandoLpz/Kubeflow_Pipelines",
    "file_path": "pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# Given the outputs from decision_tree and logistic regression components # the results are shown. # Loads the yaml manifest for each component # Run download_data task # Run tasks \"decison_tree\" and \"logistic_regression\" given"
  },
  {
    "id": 33,
    "repo": "ciandt-d1/chicago-taxi-forecast",
    "file_path": "code/pipeline/build_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nPipeline with 7 stages:       1. Extract and transform input data from BigQuery into tfrecords       2. Data Validation       3. Train NN       4. Deploy NN on CMLE       5. Make predictions       6. Evaluation       7. Plot time series -*- coding: utf-8 -*- # bq2tfrecord.outputs['znorm_stats'],"
  },
  {
    "id": 34,
    "repo": "gnovack/kubeflow-pipelines",
    "file_path": "boston_housing/pipeline.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 35,
    "repo": "dermatologist/kedro-multimodal",
    "file_path": "build_kubeflow_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nGenerates a workflow spec yaml file from a Kedro pipeline.      Args:         image: container image name.         pipeline_name: pipeline name to build a workflow spec.         env: Kedro configuration environment name. python build_kubeflow_pipeline.py <project_image> # Configure the container to use AWS credentials."
  },
  {
    "id": 36,
    "repo": "Mastercard/mastercard-labs-ml-pipeline",
    "file_path": "santander-trnx-classification.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nlambda x: x['target'] !/usr/bin/env python3"
  },
  {
    "id": 37,
    "repo": "Mastercard/mastercard-labs-ml-pipeline",
    "file_path": "santander-trnx-classification_release.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n!/usr/bin/env python3"
  },
  {
    "id": 38,
    "repo": "Mastercard/mastercard-labs-ml-pipeline",
    "file_path": "pipeline_steps/training/katib-launcher/kubeflow_katib_launcher_op.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nCopyright 2019 Google LLC  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 39,
    "repo": "dvdbisong/kubeflow-for-poets",
    "file_path": "kubeflow-pipelines/crypto_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nPipeline to train crypto model !/usr/bin/env python3 Copyright (c) 2019 Ekaba Bisong Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights"
  },
  {
    "id": 40,
    "repo": "Ark-kun/pipeline_components",
    "file_path": "components/XGBoost/_samples/recursive_training.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n!/usr/bin/env python3 This sample demonstrates continuous training using a train-eval-check recursive loop. The main pipeline trains the initial model and then gradually trains the model some more until the model evaluation metrics are good enough. This recursive sub-pipeline trains a model, evaluates it, calculates the metrics and checks them."
  },
  {
    "id": 41,
    "repo": "Ark-kun/pipeline_components",
    "file_path": "samples/core/continue_training_from_prod/continue_training_from_prod.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nThis sample demonstrates a common training scenario. New models are being trained strarting from the production model (if it exists). This sample produces two runs: 1. The trainer will train the model from scratch and set as prod after testing it 2. Exact same configuration, but the pipeline will discover the existing prod model (published by the 1st run) and warm-start the training from it."
  },
  {
    "id": 42,
    "repo": "Ark-kun/pipeline_components",
    "file_path": "samples/core/train_until_good/train_until_good.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n!/usr/bin/env python3 Copyright 2020 The Kubeflow Pipleines authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License."
  },
  {
    "id": 43,
    "repo": "StatCan/aaw-kubeflow-mlops",
    "file_path": "pipeline/train/cnn.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nConvolutional Neural Network (CNN) Pipeline Initially derived from https://github.com/kaizentm/kubemlops"
  },
  {
    "id": 44,
    "repo": "StatCan/aaw-kubeflow-mlops",
    "file_path": "pipeline/train/cnn_databricks.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nConvolutional Neural Network (CNN) Pipeline Initially derived from https://github.com/kaizentm/kubemlops"
  },
  {
    "id": 45,
    "repo": "StatCan/aaw-kubeflow-mlops",
    "file_path": "pipeline/train/default.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nDefault Pipeline Initially derived from https://github.com/kaizentm/kubemlops"
  },
  {
    "id": 46,
    "repo": "redhat-ai-services/kubeflow-pipelines-examples",
    "file_path": "pipelines/00_compiled_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nExample of a pipeline built from inline functions with kfp and compiled to yaml."
  },
  {
    "id": 47,
    "repo": "redhat-ai-services/kubeflow-pipelines-examples",
    "file_path": "pipelines/02_submitted_pipeline_via_route.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nExample of a pipeline submitted directly to kfp."
  },
  {
    "id": 48,
    "repo": "redhat-ai-services/kubeflow-pipelines-examples",
    "file_path": "pipelines/02_submitted_pipeline_via_service.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nExample of a pipeline submitted directly to kfp. # Check if the script is running in a k8s pod # Read the service account token if it is # Get the bearer token from an env var if it is not # Note: The service account needs permission to access DSP instance in RBAC. # Check if the script is running in a k8s pod"
  },
  {
    "id": 49,
    "repo": "redhat-ai-services/kubeflow-pipelines-examples",
    "file_path": "pipelines/03_outputs_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nExample of a pipeline returning multiple values."
  },
  {
    "id": 50,
    "repo": "redhat-ai-services/kubeflow-pipelines-examples",
    "file_path": "pipelines/04_artifact_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nExample of a pipeline to demonstrate accessing secrets/config maps in a pipeline."
  },
  {
    "id": 51,
    "repo": "redhat-ai-services/kubeflow-pipelines-examples",
    "file_path": "pipelines/05_metrics_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nExample of a pipeline to demonstrate saving metrics from a pipeline.  runMetrics appear to be depreciated in kfp v2 api so implement this feature at your own risk. # The name of the metric. Visualized as the column name in the runs table. # The value of the metric. Must be a numeric value. # The optional format of the metric. Supported values are \"RAW\" (displayed in raw format) and #  \"PERCENTAGE\" (displayed in percentage format). # The name of the metric. Visualized as the column name in the runs table."
  },
  {
    "id": 52,
    "repo": "redhat-ai-services/kubeflow-pipelines-examples",
    "file_path": "pipelines/06_visualization_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nExample of a pipeline to demonstrate visualizations from a pipeline.  The visualization doesn't work for this.  PR's are welcome to help get a visualization functioning.  This pipeline example is currently broken."
  },
  {
    "id": 53,
    "repo": "redhat-ai-services/kubeflow-pipelines-examples",
    "file_path": "pipelines/07_container_components_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nExample of a pipeline to demonstrate running code built into the container image.  This pipeline uses the kfp.dsl.ContainerOp() function which throws some warnings. Would be nice to find a better way to run code build into the container image.  This pipeline example is currently broken."
  },
  {
    "id": 54,
    "repo": "redhat-ai-services/kubeflow-pipelines-examples",
    "file_path": "pipelines/08_additional_packages_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nExample of a pipeline to demonstrate installing additional packages in the pipeline."
  },
  {
    "id": 55,
    "repo": "redhat-ai-services/kubeflow-pipelines-examples",
    "file_path": "pipelines/09_secrets_cm_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nExample of a pipeline to demonstrate accessing secrets/config maps in a pipeline."
  },
  {
    "id": 56,
    "repo": "redhat-ai-services/kubeflow-pipelines-examples",
    "file_path": "pipelines/10_mount_pvc_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nExample of a pipeline to demonstrate mounting a pvc to a task in a pipeline."
  },
  {
    "id": 57,
    "repo": "redhat-ai-services/kubeflow-pipelines-examples",
    "file_path": "pipelines/11_iris_training_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nExample of a pipeline to demonstrate a simple real world data science workflow. # Features # Labels # Split dataset into training set and test set # Check if the script is running in a k8s pod # Get the CA from the service account if it is"
  },
  {
    "id": 58,
    "repo": "redhat-ai-services/kubeflow-pipelines-examples",
    "file_path": "pipelines/12_gpu_task_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nExample of a pipeline using a GPU. # Set the accelerator type and set the request/limit to 1 # Note: You cannot request different values for the request/limit # Set the toleration for the GPU node # Check if the script is running in a k8s pod # Get the CA from the service account if it is"
  },
  {
    "id": 59,
    "repo": "google/vertex-pipelines-boilerplate",
    "file_path": "src/pipelines/sample_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nSample Kubeflow pipeline. Copyright 2022 Google LLC  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 60,
    "repo": "lynnmatrix/kfp-local",
    "file_path": "kfp_local/local_client_test.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 61,
    "repo": "deployKF/kubeflow-pipelines-gitops",
    "file_path": "step-1--render-pipelines/example_pipeline_1/pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nLogging   Arguments"
  },
  {
    "id": 62,
    "repo": "muhammadyaseen/kubeflow-on-linode",
    "file_path": "kfp-examples/01_markdown-visualization-pipeline/md_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nA simple pipeline to show some Markdown visualizations  For some reason, Kubeflow Pipelines UI can't render Markdown tables correctly when  python's triple quoted strings are used. This is the reason you are seeing these long strings.  For details, see this issue: https://github.com/kubeflow/pipelines/issues/10182"
  },
  {
    "id": 63,
    "repo": "muhammadyaseen/kubeflow-on-linode",
    "file_path": "kfp-examples/02_simple-minio-pipeline/minio_census_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nAdapted from: https://blog.min.io/building-an-ml-data-pipeline-with-minio-and-kubeflow-v2-0/ # Create client with access and secret key. # First, check if the bucket exists # If so, check if the file / object exists that contains the data # e.g. dataset = acs # https://www.census.gov/data/developers/data-sets/acs-5year/2021.html"
  },
  {
    "id": 64,
    "repo": "muhammadyaseen/kubeflow-on-linode",
    "file_path": "kfp-examples/03_data-cleaning-pipeline/cleaning_and_prep_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# Create client with access and secret key. # First, check if the bucket exists # If so, check if the file / object exists that contains the data # Create client with access and secret key # Make the bucket if it does not exist."
  },
  {
    "id": 65,
    "repo": "muhammadyaseen/kubeflow-on-linode",
    "file_path": "kfp-examples/04_model-train-eval-pipeline/train_eval_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nThis component loads our main training container image and runs the code to train the models # TODO: there should be a better way to do this # write experiments summary as a markdown table # We replace the underscore so that Mardown is rendered properly # Create client with access and secret key # read the ranks and hparams info df from MinIO"
  },
  {
    "id": 66,
    "repo": "e2fyi/kfx",
    "file_path": "kfx/dsl/__init__.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nExtension to kfp dsl.  Using `kfx.dsl.ContainerOpTransform` to modify ContainerOp internal properties ::      import kfp.components     import kfp.dsl     import kfx.dsl      transforms = (         kfx.dsl.ContainerOpTransform()         .set_resources(cpu=\"500m\", memory=(\"1G\", \"4G\"))         .set_image_pull_policy(\"Always\")         .set_annotations({\"iam.amazonaws.com/role\": \"some-arn\"})         .add_env_vars({\"ENV\": \"production\"})         .add_env_var_from_secret(\"AWS_ACCESS_KEY\", secret_name=\"aws\", secret_key=\"access_key\")     )       @kfp.dsl.components.func_to_container_op     def echo(text: str) -> str:         print(text)         return text       @kfp.dsl.pipeline(name=\"demo\")     def pipeline(text: str):         op1 = echo(text)         op2 = echo(\"%s-%s\" % text)          # u can apply the transform on op1 only         # op1.apply(transforms)          # or apply on all ops in the pipeline         kfp.dsl.get_pipeline_conf().add_op_transformer(transforms)   Using `kfx.dsl.ArtifactLocationHelper` to get the path to an artifact generated by a kfp task. ::      import kfp.components     import kfp.dsl     import kfx.dsl       helper = kfx.dsl.ArtifactLocationHelper(         scheme=\"minio\", bucket=\"mlpipeline\", key_prefix=\"artifacts/\"     )      @kfp.components.func_to_container_op     def test_op(         mlpipeline_ui_metadata: OutputTextFile(str), markdown_data_file: OutputTextFile(str)     ):         \"A test kubeflow pipeline task.\"          import json          import kfx.dsl         import kfx.vis         import kfx.vis.vega          data = [             {\"a\": \"A\", \"b\": 28},             {\"a\": \"B\", \"b\": 55},             {\"a\": \"C\", \"b\": 43},             {\"a\": \"D\", \"b\": 91},             {\"a\": \"E\", \"b\": 81},             {\"a\": \"F\", \"b\": 53},             {\"a\": \"G\", \"b\": 19},             {\"a\": \"H\", \"b\": 87},             {\"a\": \"I\", \"b\": 52},         ]          # `KfpArtifact` provides the reference to data artifact created         # inside this task         spec = {             \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.json\",             \"description\": \"A simple bar chart\",             \"data\": {                 \"values\": data,             },             \"mark\": \"bar\",             \"encoding\": {                 \"x\": {\"field\": \"a\", \"type\": \"ordinal\"},                 \"y\": {\"field\": \"b\", \"type\": \"quantitative\"},             },         }          # write the markdown to the `markdown-data` artifact         markdown_data_file.write(\"### hello world\")          # creates an ui metadata object         ui_metadata = kfx.vis.kfp_ui_metadata(             # Describes the vis to generate in the kubeflow pipeline UI.             [                 # markdown vis from a markdown artifact.                 # `KfpArtifact` provides the reference to data artifact created                 # inside this task                 kfx.vis.markdown(kfx.dsl.KfpArtifact(\"markdown_data_file\")),                 # a vega web app from the vega data artifact.                 kfx.vis.vega.vega_web_app(spec),             ]         )          # writes the ui metadata object as the `mlpipeline-ui-metadata` artifact         mlpipeline_ui_metadata.write(kfx.vis.asjson(ui_metadata))          # prints the uri to the markdown artifact         print(ui_metadata.outputs[0].source)       @kfp.dsl.pipeline()     def test_pipeline():         \"A test kubeflow pipeline\"          op: kfp.dsl.ContainerOp = test_op()          # modify kfp operator with artifact location metadata through env vars         op.apply(helper.set_envs()) # u can apply the transform on op1 only # op1.apply(transforms) # or apply on all ops in the pipeline # `KfpArtifact` provides the reference to data artifact created # inside this task"
  },
  {
    "id": 67,
    "repo": "e2fyi/kfx",
    "file_path": "kfx/dsl/_artifact_location.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nUtils. # op will have an environment variable \"WORKFLOW_NAME\" # that provides the name of the workflow # write the markdown to the `markdown-data` artifact # creates an ui metadata object # Describes the vis to generate in the kubeflow pipeline UI."
  },
  {
    "id": 68,
    "repo": "e2fyi/kfx",
    "file_path": "kfx/dsl/_artifact_location_test.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nTests for kfx.lib.utils. # print(outfile.read_text()) # assert False"
  },
  {
    "id": 69,
    "repo": "e2fyi/kfx",
    "file_path": "kfx/dsl/_transformers.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nTransform functions that modify containerOp. # u can apply the transform on op1 only # op1.apply(transforms) # or apply on all ops in the pipeline"
  },
  {
    "id": 70,
    "repo": "e2fyi/kfx",
    "file_path": "kfx/dsl/_transformers_test.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nTest for ContainerOp transformers."
  },
  {
    "id": 71,
    "repo": "glukicov/llm_pipelines_demo",
    "file_path": "pipelines/demo.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# Step 1: Get data # Step 2: Call LLM with the data # Step 3: Evaluate results"
  },
  {
    "id": 72,
    "repo": "canonical/kfp-operators",
    "file_path": "tests/integration/pipelines/pipeline_container_no_input.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nCopyright 2022 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 73,
    "repo": "Anvil-Late/Kubeflow_advanced_pipeline",
    "file_path": "pipeline/pipeline.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 74,
    "repo": "jhammarstedt/MLOps-Kubeflow_in_GCP",
    "file_path": "pipeline/main.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# 1 load data # image needs to be a compile-time string # 2 train # image needs to be a compile-time string # 3 deploy the trained model to Cloud ML Engine"
  },
  {
    "id": 75,
    "repo": "ferneutron/mlops",
    "file_path": "src/pipelines/iris_pipeline.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 76,
    "repo": "ferneutron/mlops",
    "file_path": "src/pipelines/components/data.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 77,
    "repo": "ferneutron/mlops",
    "file_path": "src/pipelines/components/evaluation.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 78,
    "repo": "ferneutron/mlops",
    "file_path": "src/pipelines/components/models.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 79,
    "repo": "ferneutron/mlops",
    "file_path": "src/pipelines/components/register.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 80,
    "repo": "ferneutron/mlops",
    "file_path": "src/pipelines/components/utils.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 81,
    "repo": "iQuantC/Kubeflow-pipeline",
    "file_path": "kubeflow_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nStep 1: Load Dataset # Save the dataset to the output artifact path Step 2: Preprocess Data # Load dataset # Debug: Check for NaN values"
  },
  {
    "id": 82,
    "repo": "DanielAvdar/protocol-task",
    "file_path": "protocol_task_kfp/executors.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nflake8: noqa: F403, F405, B006"
  },
  {
    "id": 83,
    "repo": "kfous/kubeflow-pipeline",
    "file_path": "mlflow_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# Dynamically install MLflow"
  },
  {
    "id": 84,
    "repo": "pharmbio/kubeflow-pipelines",
    "file_path": "kensert_CNN/pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nModifier function to apply to a Container Op to simplify volume, volume mount addition and         enable better reuse of volumes, volume claims across container ops.         Usage:             train = train_op(...)             train.apply(mount_pvc('claim-name', 'pipeline', '/mnt/pipeline')) ### inputs to pipeline ### preprocessing step # NOTE: arguments from pipeline not utilized in training script yet ### add resource definitions, pvc mounts etc. ### training step"
  },
  {
    "id": 85,
    "repo": "NusretOzates/autonlp-classification-kubeflow",
    "file_path": "pipeline/pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nThis is where magic happens. We define the pipeline using the @dsl.pipeline decorator. The pipeline is composed of 6 steps:  1. Download the dataset 2. Preprocess the dataset 3. Split the dataset 4. Train the model(s) 5. Evaluate the model(s) 6. Print the best model # We have only the train split # We have train and validation (maybe test) split # We have train, validation and test split # For now, this real training code is not activated because it uses too many resources # I could just use num_labels but I want to use more dense layers"
  },
  {
    "id": 86,
    "repo": "NusretOzates/autonlp-classification-kubeflow",
    "file_path": "pipeline/pipeline_runner.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nRuns the pipeline      Args:         dataset_name: Name of the dataset         dataset_subset: Subset of the dataset         model_name: Name of the model      Returns:         None # Connect to KFP, this command is used to connect to the KFP UI: # kubectl port-forward -n kubeflow svc/ml-pipeline-ui 8080:80 # ParallerFor takes static arguments, it is not possible to give it as pipeline parameter so we need to override the # pipeline function code to pass the arguments. # Open the pipeline code"
  },
  {
    "id": 87,
    "repo": "sbakiu/ml-kf-pipeline",
    "file_path": "tokenize_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nPipeline"
  },
  {
    "id": 88,
    "repo": "Ark-kun/kfp_sdk_components",
    "file_path": "components/_python_op.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nStrips type annotations from the function definitions in the provided source code. Copyright 2018 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 89,
    "repo": "s102401002/kubeflowPipeline0722",
    "file_path": "src/HeartDisease_prediction.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nfrom kfp.components import func_to_container_op # \u8b80\u53d6\u8207\u7a0b\u5f0f\u78bc\u4f4d\u65bc\u540c\u4e00\u500b\u8cc7\u6599\u593e\u4e2d\u7684 stroke.csv # urls = [ #     \"https://raw.githubusercontent.com/s102401002/kubeflowPipeline0722/main/heart_2020_cleaned.csv\" # ]"
  },
  {
    "id": 90,
    "repo": "s102401002/kubeflowPipeline0722",
    "file_path": "src/compose.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# Read manifest file # Add epoch time in the job name xgboost_katib random_forest_katib knn_katib"
  },
  {
    "id": 91,
    "repo": "s102401002/kubeflowPipeline0722",
    "file_path": "src/diabetes_prediction.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\ntest from kfp.components import func_to_container_op # Logistic Regression # Save the model # Save the accuracy"
  },
  {
    "id": 92,
    "repo": "s102401002/kubeflowPipeline0722",
    "file_path": "src/hypertension_prediction.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nfrom kfp.components import func_to_container_op # \u8b80\u53d6\u8207\u7a0b\u5f0f\u78bc\u4f4d\u65bc\u540c\u4e00\u500b\u8cc7\u6599\u593e\u4e2d\u7684 hypertension.csv # \u79fb\u9664\u4e0d\u9700\u8981\u7684\u6b04\u4f4d # \u88dc\u9f4a\u8cc7\u6599 # Logistic Regression"
  },
  {
    "id": 93,
    "repo": "s102401002/kubeflowPipeline0722",
    "file_path": "src/kubeflowPipeline0722.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nfrom kfp.components import func_to_container_op #model_path = './diabete_prediction_model.pkl'"
  },
  {
    "id": 94,
    "repo": "s102401002/kubeflowPipeline0722",
    "file_path": "src/kubeflowPipeline_xgboost.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\ntest from kfp.components import func_to_container_op"
  },
  {
    "id": 95,
    "repo": "s102401002/kubeflowPipeline0722",
    "file_path": "src/kubeflow_NAS.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# Read CSV files from the directory # Define standard name mapping # Rename columns based on the standard name mapping # Drop rows where 'diabetes' is 'No Info' # Drop rows with missing values"
  },
  {
    "id": 96,
    "repo": "s102401002/kubeflowPipeline0722",
    "file_path": "src/stroke_prediction.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nfrom kfp.components import func_to_container_op # \u8b80\u53d6\u8207\u7a0b\u5f0f\u78bc\u4f4d\u65bc\u540c\u4e00\u500b\u8cc7\u6599\u593e\u4e2d\u7684 stroke.csv # \u79fb\u9664\u4e0d\u9700\u8981\u7684\u6b04\u4f4d # \u5b9a\u7fa9\u6620\u5c04 # \u88dc\u9f4a\u8cc7\u6599"
  },
  {
    "id": 97,
    "repo": "paul-sud/demo-pipeline",
    "file_path": "kubeflow/toy.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nIn the function signature, `InputPath(\"Fastq\")` is used to indicate a path to some     fastq, and at runtime the actual path of the data gets passed. The `Fastq` argument     is typechecked by the DSL compiler to make sure that types of data being passed from     component to component match up.      Because the trimmed fastq output path is controlled by Kubeflow, it does not have a     .gz extension, and as such Trimmomatic thinks it's OK to not gzip the output file     if we just pass that path directly. Instead, we pass a dummy filename to Trimmomatic     and then just write the contents of that to the output path."
  },
  {
    "id": 98,
    "repo": "tonouchi510/kfp-project",
    "file_path": "pipelines/head-pose-dataset-pipeline/head-pose-dataset-pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nInitialize component store Create component factories Define pipeline"
  },
  {
    "id": 99,
    "repo": "tonouchi510/kfp-project",
    "file_path": "pipelines/head-pose-pipeline/head-pose-pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nInitialize component store Create component factories Define pipeline"
  },
  {
    "id": 100,
    "repo": "tonouchi510/kfp-project",
    "file_path": "pipelines/hello-world-pipeline/hello-world-pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nInitialize component store Create component factories Define pipeline"
  },
  {
    "id": 101,
    "repo": "tonouchi510/kfp-project",
    "file_path": "pipelines/optuna-pipeline/optuna-pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nInitialize component store Create component factories Define pipeline"
  },
  {
    "id": 102,
    "repo": "tonouchi510/kfp-project",
    "file_path": "pipelines/simple-training-pipeline/simple-training-pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nInitialize component store Create component factories Define pipeline"
  },
  {
    "id": 103,
    "repo": "hiruna72/miniKF_example_pipeline",
    "file_path": "small_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nimport os component_root=\"/home/jovyan\" Load the component by calling load_component_from_file or load_component_from_url To load the component, the pipeline author only needs to have access to the component.yaml file. The Kubernetes cluster executing the pipeline needs access to the container image specified in the component."
  },
  {
    "id": 104,
    "repo": "hiruna72/miniKF_example_pipeline",
    "file_path": "components/noRok/norok_reusable_compo_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nLoad the component by calling load_component_from_file or load_component_from_url To load the component, the pipeline author only needs to have access to the component.yaml file. The Kubernetes cluster executing the pipeline needs access to the container image specified in the component. dummy_op = kfp.components.load_component_from_url('http://....../component.yaml') Define a pipeline and create a task from a component:"
  },
  {
    "id": 105,
    "repo": "shashnavad/wine-quality-mlops",
    "file_path": "pipelines/test_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nTest select_best_model with individual model inputs. Configure root logger and specific loggers to suppress debug messages Set environment variables first Mock out git before anything tries to import it Add the mock to sys.modules Import components directly"
  },
  {
    "id": 106,
    "repo": "shashnavad/wine-quality-mlops",
    "file_path": "pipelines/wine_quality_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nPreprocess the wine quality data. Set environment variables to disable Git functionality Mock out git before anything tries to import it Add the mock to sys.modules Now continue with the regular imports # Load data"
  },
  {
    "id": 107,
    "repo": "shashnavad/wine-quality-mlops",
    "file_path": "pipelines/.ipynb_checkpoints/wine_quality_pipeline-checkpoint.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nDownload wine quality dataset and perform initial validation. # Create data directory # Download dataset # Save dataset # Load data # Create validation report"
  },
  {
    "id": 108,
    "repo": "levitomer/kubeflow-pipeline-demo",
    "file_path": "pipeline/pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# It would be much cleaner to have our component accept the file paths as command-line arguments."
  },
  {
    "id": 109,
    "repo": "anifort/kubeflow-pipelines-mlops",
    "file_path": "pipeline/src/pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nInitialize component store Create component factories Define pipeline dsl-compile --py pipeline.py --output pipeline_build.tar.gz"
  },
  {
    "id": 110,
    "repo": "AnsealArt/MLOps-Kubeflow",
    "file_path": "kubeflow-breast-cancer-pipeline/src/pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# Show metrics summary for all the models # Load YAML files for each component # Set pipeline tasks and pass data to models # Show results of ML models Compile the pipeline so it can be imported into Kubeflow"
  },
  {
    "id": 111,
    "repo": "AnsealArt/MLOps-Kubeflow",
    "file_path": "kubeflow-california-housing-pipeline/src/pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nDefine components: preprocess, train, test, deploy Describe Pipeline Define Pipeline steps # Use current date without separators # Sample date: 20220529160603"
  },
  {
    "id": 112,
    "repo": "s102401002/kubeflowPipeline",
    "file_path": "kubeflowPipeline0720.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nfrom kfp.components import func_to_container_op #model_path = './diabete_prediction_model.pkl'"
  },
  {
    "id": 113,
    "repo": "s102401002/kubeflowPipeline",
    "file_path": "kubeflowPipeline0722.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nfrom kfp.components import func_to_container_op #model_path = './diabete_prediction_model.pkl'"
  },
  {
    "id": 114,
    "repo": "s102401002/kubeflowPipeline",
    "file_path": "kubeflowPipeline_parquet.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nfrom kfp.components import func_to_container_op # \u8f49\u63db\u70ba Parquet \u683c\u5f0f"
  },
  {
    "id": 115,
    "repo": "s102401002/kubeflowPipeline",
    "file_path": "kubeflowPipeline_xgboost.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nfrom kfp.components import func_to_container_op"
  },
  {
    "id": 116,
    "repo": "stackdemos/yolo4",
    "file_path": "components/golang/component.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nReturns whether we are running in notebook."
  },
  {
    "id": 117,
    "repo": "stackdemos/yolo4",
    "file_path": "components/training/component.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nReturns whether we are running in notebook."
  },
  {
    "id": 118,
    "repo": "aiffel-socar-cv/kubeflow-pipeline",
    "file_path": "pipeline.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 119,
    "repo": "aiffel-socar-cv/kubeflow-pipeline",
    "file_path": "baseline/retrain_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\ncount the number of newly added images"
  },
  {
    "id": 120,
    "repo": "lauramorillo/kubeflow-example",
    "file_path": "taxi-on-prem.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nlambda x: (x['target'] > x['fare'] * 0.2) !/usr/bin/env python3 Copyright 2018 Google LLC  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License."
  },
  {
    "id": 121,
    "repo": "pragadeeshraju/kubeflow-sample-pipeline",
    "file_path": "pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# Loads the yaml manifest for each component # pipeline steps # kfp.Client().create_run_from_pipeline_func(basic_pipeline, arguments={})"
  },
  {
    "id": 122,
    "repo": "pragadeeshraju/kubeflow-sample-pipeline",
    "file_path": "pipeline2.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nkfserving_op = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/' 'master/components/kubeflow/kfserving/component.yaml')"
  },
  {
    "id": 123,
    "repo": "dedmari/kubeflow_trident_pipeline",
    "file_path": "pipeline/src/pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nCopyright (c) 2019, NVIDIA CORPORATION.  All rights reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 124,
    "repo": "Alexander6463/Kubeflow_MNIST",
    "file_path": "pipeline_dev.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 125,
    "repo": "jonrossclaytor/kubeflow-mlb",
    "file_path": "pipelines/baseball-pipeline-branch.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nA pipeline with sequential steps. !/usr/bin/env python3"
  },
  {
    "id": 126,
    "repo": "jonrossclaytor/kubeflow-mlb",
    "file_path": "pipelines/baseball-pipeline-enhance.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nA pipeline with sequential steps. !/usr/bin/env python3"
  },
  {
    "id": 127,
    "repo": "jonrossclaytor/kubeflow-mlb",
    "file_path": "pipelines/baseball-pipeline-single.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nA pipeline with sequential steps. !/usr/bin/env python3"
  },
  {
    "id": 128,
    "repo": "xxxtrillionarie/KubeFlow_MLOps_Pipelines",
    "file_path": "samples/test/after.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nname: Print Text inputs: - {name: text, type: String} implementation:   container:     image: alpine     command:     - sh     - -c     - |       set -e -x       echo \"$0\"     - {inputValue: text} Copyright 2020 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 129,
    "repo": "xxxtrillionarie/KubeFlow_MLOps_Pipelines",
    "file_path": "samples/test/after_test.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nCopyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 130,
    "repo": "xxxtrillionarie/KubeFlow_MLOps_Pipelines",
    "file_path": "samples/test/cache_v2_compatible_test.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nTwo step v2-compatible pipeline. Copyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 131,
    "repo": "xxxtrillionarie/KubeFlow_MLOps_Pipelines",
    "file_path": "samples/test/fail.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nFail pipeline. Copyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 132,
    "repo": "xxxtrillionarie/KubeFlow_MLOps_Pipelines",
    "file_path": "samples/test/fail_parameter_value_missing.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nname: Echo inputs: - {name: text, type: String} implementation:   container:     image: alpine     command:     - echo     - {inputValue: text} Copyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 133,
    "repo": "xxxtrillionarie/KubeFlow_MLOps_Pipelines",
    "file_path": "samples/test/fail_parameter_value_missing_test.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nCopyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 134,
    "repo": "xxxtrillionarie/KubeFlow_MLOps_Pipelines",
    "file_path": "samples/test/fail_test.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nFail pipeline. Copyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 135,
    "repo": "xxxtrillionarie/KubeFlow_MLOps_Pipelines",
    "file_path": "samples/test/fail_v2.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nFail pipeline. Copyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 136,
    "repo": "xxxtrillionarie/KubeFlow_MLOps_Pipelines",
    "file_path": "samples/test/legacy_data_passing.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nThis pipeline demonstrates and verifies all ways of data passing supported by KFP. %% [markdown] KFP has simple data passing model. There are three different kinds of arguments and tw different ways to consume an input argument. All combinations are supported. Any input can be consumed: * As value (`inputValue` placeholder)"
  },
  {
    "id": 137,
    "repo": "xxxtrillionarie/KubeFlow_MLOps_Pipelines",
    "file_path": "samples/test/legacy_exit_handler.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nA sample pipeline showing exit handler. !/usr/bin/env python3 Copyright 2019 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License."
  },
  {
    "id": 138,
    "repo": "xxxtrillionarie/KubeFlow_MLOps_Pipelines",
    "file_path": "samples/test/legacy_exit_handler_test.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n!/usr/bin/env python3 Copyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License."
  },
  {
    "id": 139,
    "repo": "xxxtrillionarie/KubeFlow_MLOps_Pipelines",
    "file_path": "samples/test/lightweight_python_functions_v2_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nSample pipeline for passing data in KFP v2. Copyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 140,
    "repo": "xxxtrillionarie/KubeFlow_MLOps_Pipelines",
    "file_path": "samples/test/lightweight_python_functions_v2_with_outputs.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nLightweight functions v2 with outputs. Copyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 141,
    "repo": "xxxtrillionarie/KubeFlow_MLOps_Pipelines",
    "file_path": "samples/test/lightweight_python_functions_v2_with_outputs_test.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nCopyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 142,
    "repo": "xxxtrillionarie/KubeFlow_MLOps_Pipelines",
    "file_path": "samples/test/metrics_visualization_v1.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nCopyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 143,
    "repo": "xxxtrillionarie/KubeFlow_MLOps_Pipelines",
    "file_path": "samples/test/metrics_visualization_v1_test.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nCopyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 144,
    "repo": "xxxtrillionarie/KubeFlow_MLOps_Pipelines",
    "file_path": "samples/test/metrics_visualization_v2.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nCopyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 145,
    "repo": "xxxtrillionarie/KubeFlow_MLOps_Pipelines",
    "file_path": "samples/test/metrics_visualization_v2_test.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nCopyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 146,
    "repo": "xxxtrillionarie/KubeFlow_MLOps_Pipelines",
    "file_path": "samples/test/parameter_with_format.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nCopyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 147,
    "repo": "xxxtrillionarie/KubeFlow_MLOps_Pipelines",
    "file_path": "samples/test/parameter_with_format_test.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nCopyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 148,
    "repo": "mengdong/kubeflow-pipeline-nvidia-example",
    "file_path": "kubeflow-tf/JoC_end2end_serve.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nCopyright 2018 Google LLC  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 149,
    "repo": "mengdong/kubeflow-pipeline-nvidia-example",
    "file_path": "kubeflow-tf/end2end_serve.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nCopyright 2018 Google LLC  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 150,
    "repo": "sanghunmoon/pytorch_classifier_pipeline",
    "file_path": "pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\npipeline.py n_num\uc744 \ud568\uc218\uc758 \ub9e4\uac1c\ubcc0\uc218\ub85c \uc9c0\uc815 # Pipeline \ud30c\uc77c \uc0dd\uc131"
  },
  {
    "id": 151,
    "repo": "tuhinaprasad28/Machine-Learning-Pipeline-using-Kubeflow",
    "file_path": "code_rep_localHost/Kubeflow_Pipelines-master/pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# Given the outputs from decision_tree and logistic regression components # the results are shown. # Loads the yaml manifest for each component # Run download_data task # Run tasks \"decison_tree\" and \"logistic_regression\" given"
  },
  {
    "id": 152,
    "repo": "tuhinaprasad28/Machine-Learning-Pipeline-using-Kubeflow",
    "file_path": "code_rep_gcloud/Pipeline/Kubeflow_Pipelines-master/pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# Given the outputs from decision_tree and logistic regression components # the results are shown. # Loads the yaml manifest for each component # Run download_data task # Run tasks \"decison_tree\" and \"logistic_regression\" given"
  },
  {
    "id": 153,
    "repo": "nnkkmto/sample-vertex-pipelines",
    "file_path": "src/pipeline/pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\ndefine pipeline compile pipeline"
  },
  {
    "id": 154,
    "repo": "butuzov/kubeflow-pipline-pytorch-tacatron",
    "file_path": "pipeline.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 155,
    "repo": "hermesribeiro/kubeflow_pytorch",
    "file_path": "pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# .set_gpu_limit('1', 'nvidia') # .add_volume_mount(...) # .add_env_variable(V1EnvVar(name='HOST', value='foo.bar')) # .set_retry(10)"
  },
  {
    "id": 156,
    "repo": "litovn/kubeflow-autopipe",
    "file_path": "src/pipeline_manager.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nLoad the yaml dag configuration file, to extract the required information      :param dag_path: The file path to the YAML configuration file     :return: A tuple containing lists of components, dependencies, and the initial input media file path Configure logging to display information based on your needs With download_from_pvc method defined in pvc_manager.py, it might be possible to search for the output file path saved by the previous component (independently of its name) in the PVC and download it to the local machine, then use it as the input for the next component. -----"
  },
  {
    "id": 157,
    "repo": "speg03/kfp-toolbox",
    "file_path": "tests/test_cli.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 158,
    "repo": "speg03/kfp-toolbox",
    "file_path": "tests/test_decorators.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nSay hello              This component just says hello.              Returns:                 str: hello"
  },
  {
    "id": 159,
    "repo": "speg03/kfp-toolbox",
    "file_path": "tests/test_pipeline_parser.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n{                 \"invalid_schema\": {}             }"
  },
  {
    "id": 160,
    "repo": "speg03/kfp-toolbox",
    "file_path": "tests/test_pipelines.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n{                 \"invalid_schema\": {}             }"
  },
  {
    "id": 161,
    "repo": "speg03/kfp-toolbox",
    "file_path": "src/kfp_toolbox/pipelines.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nLoad a pipeline object from the pipeline file.      Load a :class:`Pipeline` object from a pre-compiled file that represents     the pipeline.      Args:         filepath (Union[str, os.PathLike]): The path of the pre-compiled file that             represents the pipeline.      Raises:         ValueError: If the :attr:`filepath` file has an invalid schema.      Returns:         Pipeline: An object that represents the pipeline.      .. deprecated:: 0.6.0         Use :func:`.pipeline_parser.parse_pipeline_package` instead."
  },
  {
    "id": 162,
    "repo": "ZoieD/kfp-resnet",
    "file_path": "pipeline/src/pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nCopyright (c) 2019, NVIDIA CORPORATION.  All rights reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 163,
    "repo": "shawar8/sfcrime-prediction-kubeflow",
    "file_path": "data_ingest_fns.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# df = df[df['year'].isin([2018,2023])]"
  },
  {
    "id": 164,
    "repo": "shawar8/sfcrime-prediction-kubeflow",
    "file_path": "monitoring_fns.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nComponents for monitoring metrics, drift and retraining class FloatOutput(NamedTuple): float_value: float # train_query = f'SELECT * FROM {projectid}.{db}.{tablename} where data_type = \"Train\"' # train_df = pd.read_gbq(train_query, project_id=projectid, dialect= 'standard')"
  },
  {
    "id": 165,
    "repo": "shawar8/sfcrime-prediction-kubeflow",
    "file_path": "serving_fns.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# df['filed_online'] = 1"
  },
  {
    "id": 166,
    "repo": "shawar8/sfcrime-prediction-kubeflow",
    "file_path": "training_functions.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# base_image='python:3.10.2') # base_image='python:3.10.2')"
  },
  {
    "id": 167,
    "repo": "Davidnet/breast-cancer-detection-nnx-pipeline",
    "file_path": "pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# Extract all contents to the specified directory # ~/tensorflow_datasets/curated_breast_imaging_ddsm/patches/3.0.0"
  },
  {
    "id": 168,
    "repo": "pavan-kumar-99/ml-ops",
    "file_path": "components/TestTrainSplit.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 169,
    "repo": "pavan-kumar-99/ml-ops",
    "file_path": "components/getDataFromRawUri.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 170,
    "repo": "pavan-kumar-99/ml-ops",
    "file_path": "components/train-plot.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 171,
    "repo": "pavan-kumar-99/ml-ops",
    "file_path": "pipelines/echo-pipelines/dependency-pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# Compiling the pipeline"
  },
  {
    "id": 172,
    "repo": "pavan-kumar-99/ml-ops",
    "file_path": "pipelines/echo-pipelines/echo-sh.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# Compiling the pipeline"
  },
  {
    "id": 173,
    "repo": "03sarath/Kubeflow-pipelines-mlops",
    "file_path": "pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# Given the outputs from decision_tree and logistic regression components # the results are shown. # Loads the yaml manifest for each component # Run download_data task # Run tasks \"decison_tree\" and \"logistic_regression\" given"
  },
  {
    "id": 174,
    "repo": "rakesh283343/kubeflow-pipelines-clv",
    "file_path": "pipelines/batch_predict_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nBatch Predict Pipeline. Copyright 2019 Google Inc. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 175,
    "repo": "rakesh283343/kubeflow-pipelines-clv",
    "file_path": "pipelines/train_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nCLV training and deployment pipeline. Copyright 2019 Google Inc. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 176,
    "repo": "rakesh283343/kubeflow-pipelines-clv",
    "file_path": "pipelines/helper_components/helper_components.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nHelper Lightweight Python components. Copyright 2019 Google Inc. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 177,
    "repo": "anupr567/kubeflow_pipeline",
    "file_path": "kfp_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nCreated on Fri Oct  8 17:11:47 2021\r \r @author: Nikhil -*- coding: utf-8 -*- # Loads the yaml manifest for components # Run task submit the pipeline for execution. keep in mind to change the host url according to your host."
  },
  {
    "id": 178,
    "repo": "9rince/kfp",
    "file_path": "samples/test/after.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nname: Print Text inputs: - {name: text, type: String} implementation:   container:     image: alpine     command:     - sh     - -c     - |       set -e -x       echo \"$0\"     - {inputValue: text} Copyright 2020 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 179,
    "repo": "9rince/kfp",
    "file_path": "samples/test/after_test.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nCopyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 180,
    "repo": "9rince/kfp",
    "file_path": "samples/test/cache_v2_test.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nTwo step v2-compatible pipeline. Copyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 181,
    "repo": "9rince/kfp",
    "file_path": "samples/test/fail.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nFail pipeline. Copyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 182,
    "repo": "9rince/kfp",
    "file_path": "samples/test/fail_parameter_value_missing.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nname: Echo inputs: - {name: text, type: String} implementation:   container:     image: alpine     command:     - echo     - {inputValue: text} Copyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 183,
    "repo": "9rince/kfp",
    "file_path": "samples/test/legacy_data_passing.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nThis pipeline demonstrates and verifies all ways of data passing supported by KFP. %% [markdown] KFP has simple data passing model. There are three different kinds of arguments and tw different ways to consume an input argument. All combinations are supported. Any input can be consumed: * As value (`inputValue` placeholder)"
  },
  {
    "id": 184,
    "repo": "9rince/kfp",
    "file_path": "samples/test/legacy_data_passing_test.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 185,
    "repo": "9rince/kfp",
    "file_path": "samples/test/legacy_exit_handler.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nA sample pipeline showing exit handler. !/usr/bin/env python3 Copyright 2019 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License."
  },
  {
    "id": 186,
    "repo": "9rince/kfp",
    "file_path": "samples/test/legacy_exit_handler_test.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n!/usr/bin/env python3 Copyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License."
  },
  {
    "id": 187,
    "repo": "9rince/kfp",
    "file_path": "samples/test/lightweight_python_functions_v2_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nSample pipeline for passing data in KFP v2. Copyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 188,
    "repo": "9rince/kfp",
    "file_path": "samples/test/lightweight_python_functions_v2_pipeline_test.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nCopyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 189,
    "repo": "9rince/kfp",
    "file_path": "samples/test/lightweight_python_functions_v2_with_outputs.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nLightweight functions v2 with outputs. Copyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 190,
    "repo": "9rince/kfp",
    "file_path": "samples/test/lightweight_python_functions_v2_with_outputs_test.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nCopyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 191,
    "repo": "9rince/kfp",
    "file_path": "samples/test/metrics_visualization_v1.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nCopyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 192,
    "repo": "9rince/kfp",
    "file_path": "samples/test/metrics_visualization_v1_test.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nCopyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 193,
    "repo": "9rince/kfp",
    "file_path": "samples/test/metrics_visualization_v2.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nCopyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 194,
    "repo": "9rince/kfp",
    "file_path": "samples/test/metrics_visualization_v2_test.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nCopyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 195,
    "repo": "9rince/kfp",
    "file_path": "samples/test/parameter_with_format.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nCopyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 196,
    "repo": "9rince/kfp",
    "file_path": "samples/test/parameter_with_format_test.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nCopyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 197,
    "repo": "9rince/kfp",
    "file_path": "samples/test/placeholder_concat.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nname: Component with concat placeholder inputs: - {name: input_one, type: String} - {name: input_two, type: String} implementation:   container:     image: gcr.io/google-containers/busybox     command:     - sh     - -ec     args:     - echo \"$0\" > /tmp/test && [[ \"$0\" == 'one+two=three' ]]     - concat: [{inputValue: input_one}, '+', {inputValue: input_two}, '=three'] Copyright 2020 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 198,
    "repo": "raviranjan0309/kubeflow-fairing-pipeline",
    "file_path": "pipeline.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 199,
    "repo": "fybrik/kfp-components",
    "file_path": "samples/house_price_estimates/pipeline-argo.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# Get the namespace in which kfp is running # Get the ID of the run.  Make sure it's lower case and starts with a letter for our use # Where to store parameters passed between workflow steps # Set environment values to ensure persistent volumes used to pass parameters are allocated successfully"
  },
  {
    "id": 200,
    "repo": "fybrik/kfp-components",
    "file_path": "samples/house_price_estimates/pipeline-tekton.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# Get the namespace in which kfp is running # Get the ID of the run.  Make sure it's lower case and starts with a letter for our use # Where to store parameters passed between workflow steps # Set environment values to ensure persistent volumes used to pass parameters are allocated successfully"
  },
  {
    "id": 201,
    "repo": "ajinkya933/Kubeflow",
    "file_path": "mnist_REST_API.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# Load dataset # Normalize dataset # Define the model using keras # IMP this specifies the path inside the Docker container where our model will be saved # Save test data pickle file"
  },
  {
    "id": 202,
    "repo": "ajinkya933/Kubeflow",
    "file_path": "mnist_REST_API_temp.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# Load dataset # Normalize dataset # Define the model using keras # IMP this specifies the path inside the Docker container where our model will be saved # Save test data pickle file"
  },
  {
    "id": 203,
    "repo": "ajinkya933/Kubeflow",
    "file_path": "mnist_complete_train.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# Load dataset # Normalize dataset # Define the model using keras # IMP this specifies the path inside the Docker container where our model will be saved # Save test data pickle file"
  },
  {
    "id": 204,
    "repo": "omkarakaya/kubeflow-recommender",
    "file_path": "pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n#vol_common = dsl.PipelineVolume()"
  },
  {
    "id": 205,
    "repo": "II-VSB-II/TaxiClassificationKubeflowPipelineMinio",
    "file_path": "Taxi-Pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nlambda x: (x['target'] > x['fare'] * 0.2) proxy=\"http://test:8080\""
  },
  {
    "id": 206,
    "repo": "stackdemos/kf-flatcar2",
    "file_path": "components/golang/component.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nReturns whether we are running in notebook."
  },
  {
    "id": 207,
    "repo": "stackdemos/kf-flatcar2",
    "file_path": "components/training/component.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nReturns whether we are running in notebook."
  },
  {
    "id": 208,
    "repo": "stackdemos/jjtest-ml",
    "file_path": "components/golang/component.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nReturns whether we are running in notebook."
  },
  {
    "id": 209,
    "repo": "stackdemos/jjtest-ml",
    "file_path": "components/training/component.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nReturns whether we are running in notebook."
  },
  {
    "id": 210,
    "repo": "stackdemos/jj-test2",
    "file_path": "components/golang/component.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nReturns whether we are running in notebook."
  },
  {
    "id": 211,
    "repo": "stackdemos/jj-test2",
    "file_path": "components/training/component.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nReturns whether we are running in notebook."
  },
  {
    "id": 212,
    "repo": "stackdemos/kfp125",
    "file_path": "components/golang/component.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nReturns whether we are running in notebook."
  },
  {
    "id": 213,
    "repo": "stackdemos/kfp125",
    "file_path": "components/training/component.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nReturns whether we are running in notebook."
  },
  {
    "id": 214,
    "repo": "stackdemos/kcdemo",
    "file_path": "components/golang/component.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nReturns whether we are running in notebook."
  },
  {
    "id": 215,
    "repo": "stackdemos/kcdemo",
    "file_path": "components/training/component.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nReturns whether we are running in notebook."
  },
  {
    "id": 216,
    "repo": "stackdemos/ghsumm2",
    "file_path": "components/golang/component.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nReturns whether we are running in notebook."
  },
  {
    "id": 217,
    "repo": "stackdemos/ghsumm2",
    "file_path": "components/training/component.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nReturns whether we are running in notebook."
  },
  {
    "id": 218,
    "repo": "stackdemos/ml-kbf",
    "file_path": "components/golang/component.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nReturns whether we are running in notebook."
  },
  {
    "id": 219,
    "repo": "stackdemos/ml-kbf",
    "file_path": "components/training/component.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nReturns whether we are running in notebook."
  },
  {
    "id": 220,
    "repo": "s102401002/kubeflowPipeline2",
    "file_path": "kubeflowPipeline_xgboost.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\ntest from kfp.components import func_to_container_op"
  },
  {
    "id": 221,
    "repo": "stackdemos/kf4",
    "file_path": "components/golang/component.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nReturns whether we are running in notebook."
  },
  {
    "id": 222,
    "repo": "stackdemos/kf4",
    "file_path": "components/training/component.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nReturns whether we are running in notebook."
  },
  {
    "id": 223,
    "repo": "stackdemos/joe-kubeflow-test",
    "file_path": "components/golang/component.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nReturns whether we are running in notebook."
  },
  {
    "id": 224,
    "repo": "stackdemos/joe-kubeflow-test",
    "file_path": "components/training/component.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nReturns whether we are running in notebook."
  },
  {
    "id": 225,
    "repo": "stackdemos/kfp11",
    "file_path": "components/golang/component.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nReturns whether we are running in notebook."
  },
  {
    "id": 226,
    "repo": "stackdemos/kfp11",
    "file_path": "components/training/component.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nReturns whether we are running in notebook."
  },
  {
    "id": 227,
    "repo": "stackdemos/specs-kf5",
    "file_path": "components/golang/component.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nReturns whether we are running in notebook."
  },
  {
    "id": 228,
    "repo": "stackdemos/specs-kf5",
    "file_path": "components/training/component.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nReturns whether we are running in notebook."
  },
  {
    "id": 229,
    "repo": "akranga/anton-ml1",
    "file_path": "components/golang/component.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nReturns whether we are running in notebook."
  },
  {
    "id": 230,
    "repo": "akranga/anton-ml1",
    "file_path": "components/training/component.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nReturns whether we are running in notebook."
  },
  {
    "id": 231,
    "repo": "baebae-dev/kubeflow-pipelines",
    "file_path": "boston_housing/pipeline.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 232,
    "repo": "stackdemos/secrets-tetst-01",
    "file_path": "components/golang/component.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nReturns whether we are running in notebook."
  },
  {
    "id": 233,
    "repo": "stackdemos/secrets-tetst-01",
    "file_path": "components/training/component.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nReturns whether we are running in notebook."
  },
  {
    "id": 234,
    "repo": "stackdemos/kfappx",
    "file_path": "components/golang/component.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nReturns whether we are running in notebook."
  },
  {
    "id": 235,
    "repo": "stackdemos/kfappx",
    "file_path": "components/training/component.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nReturns whether we are running in notebook."
  },
  {
    "id": 236,
    "repo": "akranga/machine-learning1",
    "file_path": "components/golang/component.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nReturns whether we are running in notebook."
  },
  {
    "id": 237,
    "repo": "akranga/machine-learning1",
    "file_path": "components/training/component.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nReturns whether we are running in notebook."
  },
  {
    "id": 238,
    "repo": "ArianFotouhi/kubeflowPipelineSpamDetector",
    "file_path": "script.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nComponent 1: Extract data # Step 1: Download the zip file # Step 2: Extract the contents of the zip file # Step 3: Read the contents into a DataFrame # Step 4: Save the DataFrame to a CSV file"
  },
  {
    "id": 239,
    "repo": "felipeacunago/kubeflow",
    "file_path": "pipelines/prophet_prediction.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nKubeflow Pipelines for timeseries prediction using fbprophet Run this script to compile pipeline"
  },
  {
    "id": 240,
    "repo": "rakesh283343/kubeflow-sample-pipelines",
    "file_path": "core/condition/condition.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nGenerate a random number between low and high. !/usr/bin/env python3 Copyright 2019 Google LLC  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License."
  },
  {
    "id": 241,
    "repo": "rakesh283343/kubeflow-sample-pipelines",
    "file_path": "core/execution_order/execution_order.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nA two step pipeline with an explicitly defined execution order. !/usr/bin/env python3 Copyright 2019 Google LLC  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License."
  },
  {
    "id": 242,
    "repo": "rakesh283343/kubeflow-sample-pipelines",
    "file_path": "core/exit_handler/exit_handler.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nA sample pipeline showing exit handler. !/usr/bin/env python3 Copyright 2019 Google LLC  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License."
  },
  {
    "id": 243,
    "repo": "rakesh283343/kubeflow-sample-pipelines",
    "file_path": "core/hello_world/hello_world.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nCopyright 2019 Google LLC  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 244,
    "repo": "rakesh283343/kubeflow-sample-pipelines",
    "file_path": "core/imagepullsecrets/imagepullsecrets.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nToy example demonstrating how to specify imagepullsecrets to access protected container registry. Copyright 2018 Google LLC  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 245,
    "repo": "rakesh283343/kubeflow-sample-pipelines",
    "file_path": "core/loop_output/loop_output.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nCopyright 2019 Google LLC  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 246,
    "repo": "rakesh283343/kubeflow-sample-pipelines",
    "file_path": "core/loop_parameter/loop_parameter.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nCopyright 2019 Google LLC  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 247,
    "repo": "rakesh283343/kubeflow-sample-pipelines",
    "file_path": "core/loop_static/loop_static.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nCopyright 2019 Google LLC  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 248,
    "repo": "rakesh283343/kubeflow-sample-pipelines",
    "file_path": "core/parallel_join/parallel_join.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nA three-step pipeline with first two running in parallel. !/usr/bin/env python3 Copyright 2019 Google LLC  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License."
  },
  {
    "id": 249,
    "repo": "rakesh283343/kubeflow-sample-pipelines",
    "file_path": "core/pipeline_parallelism_limits/pipeline_parallelism_limits.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nPrint a message. !/usr/bin/env python3 Copyright 2020 Google LLC  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License."
  },
  {
    "id": 250,
    "repo": "rakesh283343/kubeflow-sample-pipelines",
    "file_path": "core/pipeline_transformers/pipeline_transformers.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nPrint a message. !/usr/bin/env python3 Copyright 2019 Google LLC  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License."
  },
  {
    "id": 251,
    "repo": "rakesh283343/kubeflow-sample-pipelines",
    "file_path": "core/recursion/recursion.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nFlip a coin and output heads or tails randomly. !/usr/bin/env python3 Copyright 2019 Google LLC  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License."
  },
  {
    "id": 252,
    "repo": "rakesh283343/kubeflow-sample-pipelines",
    "file_path": "core/resource_ops/resource_ops.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nThis example demonstrates how to use ResourceOp to specify the value of env var. Copyright 2019 Google LLC  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 253,
    "repo": "rakesh283343/kubeflow-sample-pipelines",
    "file_path": "core/retry/retry.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nA component that fails randomly. !/usr/bin/env python3 Copyright 2019 Google LLC  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License."
  },
  {
    "id": 254,
    "repo": "rakesh283343/kubeflow-sample-pipelines",
    "file_path": "core/rnd/rnd.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nA component that fails randomly. !/usr/bin/env python3 Copyright 2019 Google LLC  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License."
  },
  {
    "id": 255,
    "repo": "rakesh283343/kubeflow-sample-pipelines",
    "file_path": "core/sequential/sequential.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nA pipeline with two sequential steps. !/usr/bin/env python3 Copyright 2019 Google LLC  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License."
  },
  {
    "id": 256,
    "repo": "rakesh283343/kubeflow-sample-pipelines",
    "file_path": "core/sidecar/sidecar.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n!/usr/bin/env python3 Copyright 2019 Google LLC  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License."
  },
  {
    "id": 257,
    "repo": "rakesh283343/kubeflow-sample-pipelines",
    "file_path": "core/volume_ops_readme/volume_ops.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nCopyright 2019 Google LLC  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 258,
    "repo": "stackdemos/kubeflow-acmecr",
    "file_path": "components/golang/component.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nReturns whether we are running in notebook."
  },
  {
    "id": 259,
    "repo": "stackdemos/kubeflow-acmecr",
    "file_path": "components/training/component.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nReturns whether we are running in notebook."
  },
  {
    "id": 260,
    "repo": "ajperry2/kubeflow_pipeline",
    "file_path": "kubeflow_pipeline/components/nuscenes/download_nuscene.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# Use json.dumps() for correct JSON formatting # check md5 # save file & check md5 # set request header"
  },
  {
    "id": 261,
    "repo": "ajperry2/kubeflow_pipeline",
    "file_path": "kubeflow_pipeline/pipelines/hello_world/hello_world.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 262,
    "repo": "ajperry2/kubeflow_pipeline",
    "file_path": "kubeflow_pipeline/pipelines/nuscenes/download_nuscene.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 263,
    "repo": "viraj-s15/KubeflowAccuPred",
    "file_path": "training_pipeline.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 264,
    "repo": "viraj-s15/KubeflowAccuPred",
    "file_path": "training_pipeline_catboost.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 265,
    "repo": "omiearicent/kubeflow_pipeline",
    "file_path": "componentymltopipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n!/usr/bin/env python coding: utf-8 In[3]: Load the component by calling load_component_from_file or load_component_from_url To load the component, the pipeline author only needs to have access to the component.yaml file."
  },
  {
    "id": 266,
    "repo": "slv-ai/kubeflow-pipelines",
    "file_path": "kubeflow-kserve/pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nload dataset preprocess data #standardize features #train-test split train the model"
  },
  {
    "id": 267,
    "repo": "slv-ai/kubeflow-pipelines",
    "file_path": "kubeflow-pipeline/pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nload dataset preprocess data #standardize features #train-test split train the model"
  },
  {
    "id": 268,
    "repo": "stackdemos/snehal-kubeflow-ml",
    "file_path": "components/golang/component.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nReturns whether we are running in notebook."
  },
  {
    "id": 269,
    "repo": "stackdemos/snehal-kubeflow-ml",
    "file_path": "components/training/component.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nReturns whether we are running in notebook."
  },
  {
    "id": 270,
    "repo": "tam0201/kubeflow-pipeline",
    "file_path": "pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n#load data from S3 #Use KFP UI for simple EDA #Split test data #Split val data #preprocess all datasets"
  },
  {
    "id": 271,
    "repo": "bartosz-bok/kubeflow-pipelines",
    "file_path": "distributed-pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# Definicja modelu # Trenowanie modelu # Forward pass # Definicja modelu # Trenowanie modelu"
  },
  {
    "id": 272,
    "repo": "bartosz-bok/kubeflow-pipelines",
    "file_path": "sequenced-pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# Definicja modelu # Trenowanie modelu # Forward pass # Definicja modelu # Trenowanie modelu"
  },
  {
    "id": 273,
    "repo": "akontaxakis/kubeflow_pipelines",
    "file_path": "samples/Full_experiment.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nGenerate a random number between minimum and maximum (inclusive). # implement pip as a subprocess: # implement pip as a subprocess: # implement pip as a subprocess:"
  },
  {
    "id": 274,
    "repo": "akontaxakis/kubeflow_pipelines",
    "file_path": "samples/HalfMerge.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nGenerate a random number between minimum and maximum (inclusive). # implement pip as a subprocess: #output = train_evaluation_op(-1, out1.outputs['TRAIN_INPUT_JSON'], out1.outputs['TRAIN_OUTPUT_JSON']) Submit the pipeline for execution: kfp.Client(host=kfp_endpoint).create_run_from_pipeline_func(flipcoin_pipeline, arguments={}) # Compiling the pipeline"
  },
  {
    "id": 275,
    "repo": "akontaxakis/kubeflow_pipelines",
    "file_path": "samples/Hopefully.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nGenerate a random number between minimum and maximum (inclusive). # implement pip as a subprocess: Submit the pipeline for execution: kfp.Client(host=kfp_endpoint).create_run_from_pipeline_func(flipcoin_pipeline, arguments={}) # Compiling the pipeline # pipeline_func = noMerge_pipeline"
  },
  {
    "id": 276,
    "repo": "akontaxakis/kubeflow_pipelines",
    "file_path": "samples/book_example.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# Imports inside a component function: # This function demonstrates how to use nested functions inside a # component function: # Passing pipeline parameter and a constant value as operation arguments # Passing a task output reference as operation arguments"
  },
  {
    "id": 277,
    "repo": "akontaxakis/kubeflow_pipelines",
    "file_path": "samples/merge.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nGenerate a random number between minimum and maximum (inclusive). # implement pip as a subprocess: # implement pip as a subprocess: #out2 = get_train_output_op(1000, out1.outputs['TRAIN_INPUT_JSON']) #output = train_evaluation_op(out1.outputs['TRAIN_INPUT_JSON'], out2.outputs['TRAIN_OUTPUT_JSON']) Submit the pipeline for execution:"
  },
  {
    "id": 278,
    "repo": "akontaxakis/kubeflow_pipelines",
    "file_path": "samples/mnist_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nKubeflow Pipelines MNIST example Run this script to compile pipeline Copyright 2019 Google LLC  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 279,
    "repo": "akontaxakis/kubeflow_pipelines",
    "file_path": "samples/sendData.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 280,
    "repo": "akontaxakis/kubeflow_pipelines",
    "file_path": "samples/testFun.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nGenerate a random number between minimum and maximum (inclusive). Submit the pipeline for execution: kfp.Client(host=kfp_endpoint).create_run_from_pipeline_func(flipcoin_pipeline, arguments={}) # Compiling the pipeline # pipeline_func = noMerge_pipeline # pipeline_filename = pipeline_func.__name__ + '.pipeline.tar.gz'"
  },
  {
    "id": 281,
    "repo": "akontaxakis/kubeflow_pipelines",
    "file_path": "samples/test/after.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nname: Print Text inputs: - {name: text, type: String} implementation:   container:     image: alpine     command:     - sh     - -c     - |       set -e -x       echo \"$0\"     - {inputValue: text} Copyright 2020 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 282,
    "repo": "akontaxakis/kubeflow_pipelines",
    "file_path": "samples/test/after_test.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nCopyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 283,
    "repo": "akontaxakis/kubeflow_pipelines",
    "file_path": "samples/test/cache_v2_compatible_test.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nTwo step v2-compatible pipeline. Copyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 284,
    "repo": "akontaxakis/kubeflow_pipelines",
    "file_path": "samples/test/fail.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nFail pipeline. Copyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 285,
    "repo": "akontaxakis/kubeflow_pipelines",
    "file_path": "samples/test/fail_parameter_value_missing.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nname: Echo inputs: - {name: text, type: String} implementation:   container:     image: alpine     command:     - echo     - {inputValue: text} Copyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 286,
    "repo": "akontaxakis/kubeflow_pipelines",
    "file_path": "samples/test/fail_parameter_value_missing_test.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nCopyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 287,
    "repo": "akontaxakis/kubeflow_pipelines",
    "file_path": "samples/test/fail_test.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nFail pipeline. Copyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 288,
    "repo": "akontaxakis/kubeflow_pipelines",
    "file_path": "samples/test/fail_v2.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nFail pipeline. Copyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 289,
    "repo": "akontaxakis/kubeflow_pipelines",
    "file_path": "samples/test/legacy_data_passing.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nThis pipeline demonstrates and verifies all ways of data passing supported by KFP. %% [markdown] KFP has simple data passing model. There are three different kinds of arguments and tw different ways to consume an input argument. All combinations are supported. Any input can be consumed: * As value (`inputValue` placeholder)"
  },
  {
    "id": 290,
    "repo": "akontaxakis/kubeflow_pipelines",
    "file_path": "samples/test/legacy_exit_handler.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nA sample pipeline showing exit handler. !/usr/bin/env python3 Copyright 2019 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License."
  },
  {
    "id": 291,
    "repo": "akontaxakis/kubeflow_pipelines",
    "file_path": "samples/test/legacy_exit_handler_test.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n!/usr/bin/env python3 Copyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License."
  },
  {
    "id": 292,
    "repo": "akontaxakis/kubeflow_pipelines",
    "file_path": "samples/test/lightweight_python_functions_v2_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nSample pipeline for passing data in KFP v2. Copyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 293,
    "repo": "gerardfarm/Kubeflow-Pipelines",
    "file_path": "mnist.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nTo run this pipeline, put into your terminal:         dsl-compile --py utils.py --output pipeline.yaml # It is mandotory to put necessary libraries here # Access S3 # Download data on your PC # Unzip downloaded data # Upload unzipped data to your S3 Storage Bucket"
  },
  {
    "id": 294,
    "repo": "gerardfarm/Kubeflow-Pipelines",
    "file_path": "examples/hello_world.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nPipeline that passes small pipeline parameter string to consumer op. # execute only if run as a script"
  },
  {
    "id": 295,
    "repo": "gerardfarm/Kubeflow-Pipelines",
    "file_path": "examples/mnist.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nTo run this pipeline, put into your terminal:         dsl-compile --py utils.py --output pipeline.yaml # It is mandotory to put necessary libraries here # Access S3 # Download data on your PC # Unzip downloaded data # Upload unzipped data to your S3 Storage Bucket"
  },
  {
    "id": 296,
    "repo": "gerardfarm/Kubeflow-Pipelines",
    "file_path": "examples/math_operations/main.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# Passes a pipeline parameter and a constant value to the `add_op` factory # function. # Passes an output reference from `first_add_task` and a pipeline parameter # to the `add_op` factory function. For operations with a single return # value, the output reference can be accessed as `task.output` or"
  },
  {
    "id": 297,
    "repo": "gerardfarm/Kubeflow-Pipelines",
    "file_path": "examples/object-detection/main.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n================================================================ Upload dataset to S3 ================================================================ # You should upload your dataset to s3 # import os"
  },
  {
    "id": 298,
    "repo": "kikuriyou/Kubeflow-pipelines",
    "file_path": "lda/kfp_topic_test.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n!/usr/bin/env python3 # TODO: use the argo job name as the workflow #workflow = '{{workflow.name}}' # Make pipeline"
  },
  {
    "id": 299,
    "repo": "terus-lim-df/kubeflow-pipeline",
    "file_path": "sample1__embulk_dag.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nCombining all pipelines together in a single pipeline !/usr/bin/env python3 # Compiling the pipeline"
  },
  {
    "id": 300,
    "repo": "SWE-Gym-Raw/kubeflow__pipelines",
    "file_path": "samples/test/after.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nname: Print Text inputs: - {name: text, type: String} implementation:   container:     image: alpine     command:     - sh     - -c     - |       set -e -x       echo \"$0\"     - {inputValue: text} Copyright 2020 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 301,
    "repo": "SWE-Gym-Raw/kubeflow__pipelines",
    "file_path": "samples/test/after_test.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nCopyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 302,
    "repo": "SWE-Gym-Raw/kubeflow__pipelines",
    "file_path": "samples/test/cache_v2_test.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nTwo step v2-compatible pipeline. Copyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 303,
    "repo": "SWE-Gym-Raw/kubeflow__pipelines",
    "file_path": "samples/test/fail.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nFail pipeline. Copyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 304,
    "repo": "SWE-Gym-Raw/kubeflow__pipelines",
    "file_path": "samples/test/fail_parameter_value_missing.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nname: Echo inputs: - {name: text, type: String} implementation:   container:     image: alpine     command:     - echo     - {inputValue: text} Copyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 305,
    "repo": "SWE-Gym-Raw/kubeflow__pipelines",
    "file_path": "samples/test/legacy_data_passing.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nThis pipeline demonstrates and verifies all ways of data passing supported by KFP. %% [markdown] KFP has simple data passing model. There are three different kinds of arguments and tw different ways to consume an input argument. All combinations are supported. Any input can be consumed: * As value (`inputValue` placeholder)"
  },
  {
    "id": 306,
    "repo": "SWE-Gym-Raw/kubeflow__pipelines",
    "file_path": "samples/test/legacy_data_passing_test.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 307,
    "repo": "SWE-Gym-Raw/kubeflow__pipelines",
    "file_path": "samples/test/legacy_exit_handler.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nA sample pipeline showing exit handler. !/usr/bin/env python3 Copyright 2019 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License."
  },
  {
    "id": 308,
    "repo": "SWE-Gym-Raw/kubeflow__pipelines",
    "file_path": "samples/test/legacy_exit_handler_test.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n!/usr/bin/env python3 Copyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License."
  },
  {
    "id": 309,
    "repo": "SWE-Gym-Raw/kubeflow__pipelines",
    "file_path": "samples/test/lightweight_python_functions_v2_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nSample pipeline for passing data in KFP v2. Copyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 310,
    "repo": "SWE-Gym-Raw/kubeflow__pipelines",
    "file_path": "samples/test/lightweight_python_functions_v2_pipeline_test.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nCopyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 311,
    "repo": "SWE-Gym-Raw/kubeflow__pipelines",
    "file_path": "samples/test/lightweight_python_functions_v2_with_outputs.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nLightweight functions v2 with outputs. Copyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 312,
    "repo": "SWE-Gym-Raw/kubeflow__pipelines",
    "file_path": "samples/test/lightweight_python_functions_v2_with_outputs_test.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nCopyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 313,
    "repo": "SWE-Gym-Raw/kubeflow__pipelines",
    "file_path": "samples/test/metrics_visualization_v1.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nCopyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 314,
    "repo": "SWE-Gym-Raw/kubeflow__pipelines",
    "file_path": "samples/test/metrics_visualization_v1_test.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nCopyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 315,
    "repo": "SWE-Gym-Raw/kubeflow__pipelines",
    "file_path": "samples/test/metrics_visualization_v2.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nCopyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 316,
    "repo": "SWE-Gym-Raw/kubeflow__pipelines",
    "file_path": "samples/test/metrics_visualization_v2_test.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nCopyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 317,
    "repo": "SWE-Gym-Raw/kubeflow__pipelines",
    "file_path": "samples/test/parameter_with_format.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nCopyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 318,
    "repo": "SWE-Gym-Raw/kubeflow__pipelines",
    "file_path": "samples/test/parameter_with_format_test.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nCopyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 319,
    "repo": "SWE-Gym-Raw/kubeflow__pipelines",
    "file_path": "samples/test/placeholder_concat.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nname: Component with concat placeholder inputs: - {name: input_one, type: String} - {name: input_two, type: String} implementation:   container:     image: gcr.io/google-containers/busybox     command:     - sh     - -ec     args:     - echo \"$0\" > /tmp/test && [[ \"$0\" == 'one+two=three' ]]     - concat: [{inputValue: input_one}, '+', {inputValue: input_two}, '=three'] Copyright 2020 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 320,
    "repo": "gbhasin0828/Kubeflow_Pipeline",
    "file_path": "kubeflow_model_for_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nKubeFlow_Model_For_Pipeline.ipynb  Automatically generated by Colab.  Original file is located at     https://colab.research.google.com/drive/19_0dWxpGnnSyPMuxnk1bn8u74vZiKQBN -*- coding: utf-8 -*- # Create the directory if it doesn't exist # Download the dataset # Save the dataset locally # Return the DataFrame"
  },
  {
    "id": 321,
    "repo": "gbhasin0828/Kubeflow_Pipeline",
    "file_path": "kubeflow_pipeline_intermediate.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nKubeFlow_Pipeline.ipynb  Automatically generated by Colab.  Original file is located at     https://colab.research.google.com/drive/19_0dWxpGnnSyPMuxnk1bn8u74vZiKQBN -*- coding: utf-8 -*- # Save to the mounted volume # Step 1: Call the prepare_data component Compile the pipeline # Call the train_test_split component"
  },
  {
    "id": 322,
    "repo": "gbhasin0828/Kubeflow_Pipeline",
    "file_path": "pipeline_for_model.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nPipeline_For_Model.ipynb  Automatically generated by Colab.  Original file is located at     https://colab.research.google.com/drive/1QEyX82Jm4xwSVGeY5G0Tcyb-qvxVUSU8 -*- coding: utf-8 -*- # Create the directory if it doesn't exist # Download the dataset # Save the dataset locally # Return the DataFrame"
  },
  {
    "id": 323,
    "repo": "Jabor047/Kubeflow-Pipelines",
    "file_path": "lightweight_pipeline/telco_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# fill all the null values with the mean of the column they occur in # replace the columns where values have been filled in # this sums up all the values in a dataframe col # combine both the download and upload data cols in bytes # get total duration and traffic for each use and merhe them into one dataframe"
  },
  {
    "id": 324,
    "repo": "qiuosier/kubeflow_pipelines",
    "file_path": "kf_sklearn_iris.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nif __name__ == '__main__': # Compile the pipeline kfp.compiler.Compiler().compile(kf_pipeline, __file__ + '.yaml')"
  },
  {
    "id": 325,
    "repo": "qiuosier/kubeflow_pipelines",
    "file_path": "kf_sklearn_iris_with_condition.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nGenerate a random number between minimum and maximum (inclusive). if __name__ == '__main__': # Compile the pipeline kfp.compiler.Compiler().compile(kf_pipeline, __file__ + '.yaml')"
  },
  {
    "id": 326,
    "repo": "tiktakdad/kubeflow_pipeline",
    "file_path": "iris/pipeline.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 327,
    "repo": "tiktakdad/kubeflow_pipeline",
    "file_path": "iris/pipeline_test.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# any attributes can be parameterized (both serialized string or actual PipelineParam) # pass in init_container list # pass in sidecars list # pass in k8s container kwargs # set `imagePullPolicy` property for `container` with `PipelineParam`"
  },
  {
    "id": 328,
    "repo": "tiktakdad/kubeflow_pipeline",
    "file_path": "metrics_evaluation_and_check_condition/pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nPrint a message."
  },
  {
    "id": 329,
    "repo": "tiktakdad/kubeflow_pipeline",
    "file_path": "titanic/pipelines.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 330,
    "repo": "swe-train/kubeflow__pipelines",
    "file_path": "samples/test/after.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nname: Print Text inputs: - {name: text, type: String} implementation:   container:     image: alpine     command:     - sh     - -c     - |       set -e -x       echo \"$0\"     - {inputValue: text} Copyright 2020 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 331,
    "repo": "swe-train/kubeflow__pipelines",
    "file_path": "samples/test/after_test.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nCopyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 332,
    "repo": "swe-train/kubeflow__pipelines",
    "file_path": "samples/test/cache_v2_test.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nTwo step v2-compatible pipeline. Copyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 333,
    "repo": "swe-train/kubeflow__pipelines",
    "file_path": "samples/test/fail.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nFail pipeline. Copyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 334,
    "repo": "swe-train/kubeflow__pipelines",
    "file_path": "samples/test/fail_parameter_value_missing.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nname: Echo inputs: - {name: text, type: String} implementation:   container:     image: alpine     command:     - echo     - {inputValue: text} Copyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 335,
    "repo": "swe-train/kubeflow__pipelines",
    "file_path": "samples/test/legacy_data_passing.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nThis pipeline demonstrates and verifies all ways of data passing supported by KFP. %% [markdown] KFP has simple data passing model. There are three different kinds of arguments and tw different ways to consume an input argument. All combinations are supported. Any input can be consumed: * As value (`inputValue` placeholder)"
  },
  {
    "id": 336,
    "repo": "swe-train/kubeflow__pipelines",
    "file_path": "samples/test/legacy_data_passing_test.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 337,
    "repo": "swe-train/kubeflow__pipelines",
    "file_path": "samples/test/legacy_exit_handler.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nA sample pipeline showing exit handler. !/usr/bin/env python3 Copyright 2019 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License."
  },
  {
    "id": 338,
    "repo": "swe-train/kubeflow__pipelines",
    "file_path": "samples/test/legacy_exit_handler_test.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n!/usr/bin/env python3 Copyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License."
  },
  {
    "id": 339,
    "repo": "swe-train/kubeflow__pipelines",
    "file_path": "samples/test/lightweight_python_functions_v2_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nSample pipeline for passing data in KFP v2. Copyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 340,
    "repo": "swe-train/kubeflow__pipelines",
    "file_path": "samples/test/lightweight_python_functions_v2_pipeline_test.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nCopyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 341,
    "repo": "swe-train/kubeflow__pipelines",
    "file_path": "samples/test/lightweight_python_functions_v2_with_outputs.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nLightweight functions v2 with outputs. Copyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 342,
    "repo": "swe-train/kubeflow__pipelines",
    "file_path": "samples/test/lightweight_python_functions_v2_with_outputs_test.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nCopyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 343,
    "repo": "swe-train/kubeflow__pipelines",
    "file_path": "samples/test/metrics_visualization_v1.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nCopyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 344,
    "repo": "swe-train/kubeflow__pipelines",
    "file_path": "samples/test/metrics_visualization_v1_test.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nCopyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 345,
    "repo": "swe-train/kubeflow__pipelines",
    "file_path": "samples/test/metrics_visualization_v2.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nCopyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 346,
    "repo": "swe-train/kubeflow__pipelines",
    "file_path": "samples/test/metrics_visualization_v2_test.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nCopyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 347,
    "repo": "swe-train/kubeflow__pipelines",
    "file_path": "samples/test/parameter_with_format.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nCopyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 348,
    "repo": "swe-train/kubeflow__pipelines",
    "file_path": "samples/test/parameter_with_format_test.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nCopyright 2021 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 349,
    "repo": "swe-train/kubeflow__pipelines",
    "file_path": "samples/test/placeholder_concat.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nname: Component with concat placeholder inputs: - {name: input_one, type: String} - {name: input_two, type: String} implementation:   container:     image: gcr.io/google-containers/busybox     command:     - sh     - -ec     args:     - echo \"$0\" > /tmp/test && [[ \"$0\" == 'one+two=three' ]]     - concat: [{inputValue: input_one}, '+', {inputValue: input_two}, '=three'] Copyright 2020 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 350,
    "repo": "smk508/kung-fu-pipelines",
    "file_path": "kungfupipelines/workflow.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nLinks a sequence of pipeline operations so that they are configured     to take place one after another.     Args:         ops - list[dsl.ContainerOp]"
  },
  {
    "id": 351,
    "repo": "duttab49/kubeflow-pipelines",
    "file_path": "add_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# Passes a pipeline parameter and a constant value to the `add_op` factory # function. # Passes an output reference from `first_add_task` and a pipeline parameter # to the `add_op` factory function. For operations with a single return # value, the output reference can be accessed as `task.output` or"
  },
  {
    "id": 352,
    "repo": "dhiebtarak/kubeflow_pipeline",
    "file_path": "kubeflow_pipeline_test.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nPreprocess raw input (e.g., TrdCaptRpt string) to match input_message_clean format.     Args:         raw_input (str): Raw input string, e.g., concatenated TrdCaptRpt messages.     Returns:         str: Cleaned input string ready for model prediction. Preprocessing Function for Prediction # Remove XML-like tags and special characters # Remove extra whitespace Step 1: Generate Dataset # Initialize minio client"
  },
  {
    "id": 353,
    "repo": "stackdemos/kubeflow-acm-ecr",
    "file_path": "components/golang/component.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nReturns whether we are running in notebook."
  },
  {
    "id": 354,
    "repo": "stackdemos/kubeflow-acm-ecr",
    "file_path": "components/training/component.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nReturns whether we are running in notebook."
  },
  {
    "id": 355,
    "repo": "stackdemos/s3-kubeflow-asi",
    "file_path": "components/golang/component.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nReturns whether we are running in notebook."
  },
  {
    "id": 356,
    "repo": "stackdemos/s3-kubeflow-asi",
    "file_path": "components/training/component.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nReturns whether we are running in notebook."
  },
  {
    "id": 357,
    "repo": "antongollbo123/kfp_utils",
    "file_path": "local_pipeline2.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\ntask = upload_to_gcs(     bucket_name=\"new_bucket123123213\",     source_file_name=\"sample_file.txt\",     destination_blob_name=\"sample_file.txt\", )  # type:ignore from google.cloud import storage  #type:ignore from google.oauth2 import service_account  #type:ignore #out_artifact.metadata[\"test\"] = \"test123\" can read artifact contents with open(task.outputs[\"out_artifact\"].path) as f:"
  },
  {
    "id": 358,
    "repo": "antongollbo123/kfp_utils",
    "file_path": "test_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nMy test pipeline #return message pipeline_file = \"logging_pipeline.yaml\" compiler.Compiler().compile(logging_pipeline,  # type: ignore pipeline_file) client = kfp.Client()"
  },
  {
    "id": 359,
    "repo": "antongollbo123/kfp_utils",
    "file_path": "local_run/custom_python_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nfrom google.cloud import storage  #type:ignore from google.oauth2 import service_account  #type:ignore"
  },
  {
    "id": 360,
    "repo": "antongollbo123/kfp_utils",
    "file_path": "local_run/custom_python_pipeline_multi.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nfrom google.cloud import storage  #type:ignore from google.oauth2 import service_account  #type:ignore # Creating the second example DataFrame"
  },
  {
    "id": 361,
    "repo": "antongollbo123/kfp_utils",
    "file_path": "local_run/gcp_client_dependency_injection.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nfrom google.cloud import storage  #type:ignore from google.oauth2 import service_account  #type:ignore"
  },
  {
    "id": 362,
    "repo": "antongollbo123/kfp_utils",
    "file_path": "local_run/get_gcs_object_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nCustom Python image has 'google' package installed, but not numpy.     Further, GCP credentials are mounted into this image. from google.cloud import storage  #type:ignore from google.oauth2 import service_account  #type:ignore @dsl.pipeline #pipe = gcs_obj_pipeline(bucket_name=\"anton-test-bucket123\")"
  },
  {
    "id": 363,
    "repo": "antongollbo123/kfp_utils",
    "file_path": "local_run/import_pipeline.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 364,
    "repo": "antongollbo123/kfp_utils",
    "file_path": "local_run/number_sum_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nor run it in a pipeline #t4 = add_3(a=t3.output)"
  },
  {
    "id": 365,
    "repo": "antongollbo123/kfp_utils",
    "file_path": "local_run/test_import.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 366,
    "repo": "antongollbo123/kfp_utils",
    "file_path": "local_testing/testing_mock_example.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 367,
    "repo": "talaman/kubeflow-ml-pipeline",
    "file_path": "mlp-example/pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nPreprocess input columns into transformed columns. # Scale numeric features to range [0, 1] # One-hot encode the label column # Preprocess the data # Create the model # Compile the model"
  },
  {
    "id": 368,
    "repo": "zinzinhust96/kubeflow_pipeline_examples",
    "file_path": "build_pipeline/pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nDefine a pipeline and create a task from a component: # You can optionally specify your own pipeline_root # pipeline_root='gs://my-pipeline-root/example-pipeline', # The outputs of the merge_csv_task can be referenced using the # merge_csv_task.outputs dictionary: merge_csv_task.outputs['output_csv']"
  },
  {
    "id": 369,
    "repo": "zinzinhust96/kubeflow_pipeline_examples",
    "file_path": "kubeflow_pipeline_sample/pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\ncreate_step_get_lines is a \"factory function\" that accepts the arguments for the component's inputs and output paths and returns a pipeline step (ContainerOp instance).  To inspect the get_lines_op function in Jupyter Notebook, enter"
  },
  {
    "id": 370,
    "repo": "zinzinhust96/kubeflow_pipeline_examples",
    "file_path": "train_classifier/pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nPrepare input/output paths and data input_data_gcs_dir = 'gs://<my bucket>/<path>/' output_data_gcs_dir = 'gs://<my bucket>/<path>/' Downloading the training set (to upload to GCS later) training_set_features_local_path = os.path.join('.', 'training_set_features.tsv')"
  },
  {
    "id": 371,
    "repo": "ima-tester/sample-kubeflow-app-001",
    "file_path": "components/golang/component.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nReturns whether we are running in notebook."
  },
  {
    "id": 372,
    "repo": "ima-tester/sample-kubeflow-app-001",
    "file_path": "components/training/component.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nReturns whether we are running in notebook."
  },
  {
    "id": 373,
    "repo": "davide-belfiori/kubeflow_test_pipelines",
    "file_path": "simple_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nEnd-to-End Kubeflow Pipeline, from data loading to model serving. TODOs: - add minio_access_key and minio_secret_key as parameter for all components -------------- --- IMPORT --- --------------"
  },
  {
    "id": 374,
    "repo": "davide-belfiori/kubeflow_test_pipelines",
    "file_path": "test_minio_connection.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nKubeflow Pipeline for MinIO connection test. -------------- --- IMPORT --- -------------- ----------------- --- CONSTANTS ---"
  },
  {
    "id": 375,
    "repo": "ahmedyass/mnist-kubeflow-pipeline",
    "file_path": "components.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\ndata_processing.py # Load dataset # Normalize data # Save preprocessed data to the component's output paths model_training.py"
  },
  {
    "id": 376,
    "repo": "ahmedyass/mnist-kubeflow-pipeline",
    "file_path": "pipeline_def.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nCompile the pipeline"
  },
  {
    "id": 377,
    "repo": "ahmedyass/mnist-kubeflow-pipeline",
    "file_path": "data_loading/__init__.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 378,
    "repo": "ahmedyass/mnist-kubeflow-pipeline",
    "file_path": "data_processing/__init__.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# load data artifact store # reshaping the data # reshaping pixels in a 28x28px image with greyscale, canal = 1. This is needed for the Keras API # normalizing the data # each pixel has a value between 0-255. Here we divide by 255, to get values from 0-1"
  },
  {
    "id": 379,
    "repo": "ahmedyass/mnist-kubeflow-pipeline",
    "file_path": "model_building/__init__.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n#model definition #saving model"
  },
  {
    "id": 380,
    "repo": "ahmedyass/mnist-kubeflow-pipeline",
    "file_path": "model_evaluation/__init__.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# Load and preprocess test dataset # Load the model from the input path provided by Kubeflow # Evaluate the model"
  },
  {
    "id": 381,
    "repo": "ahmedyass/mnist-kubeflow-pipeline",
    "file_path": "model_training/__init__.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nBuild the model with Keras API     Export model metrics #load dataset #load model structure #reading best hyperparameters from katib #compile the model - we want to have a binary outcome #fit the model and return the history while training"
  },
  {
    "id": 382,
    "repo": "MouadE0/Kubeflow-Regression-Pipeline",
    "file_path": "pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nimport matplotlib.pyplot as plt # Given the outputs from decision_tree and logistic regression components # the results are shown. # Best Model # Switch Case for best model"
  },
  {
    "id": 383,
    "repo": "tanzumlai/sample-kubeflow-pipeline",
    "file_path": "app/main.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# Upload Dataset # Train Model # Evaluate Model # Promote Model to Staging"
  },
  {
    "id": 384,
    "repo": "Our-Glass/kubeflow_pipelines_example",
    "file_path": "pipeline.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 385,
    "repo": "Our-Glass/kubeflow_pipelines_example",
    "file_path": "example_custom_module/pipeline_contenarized.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nfor running python contenerized_examples/02_pipeline_custom_component2/pipeline.py Load components Declare pipeline # task_print = print_meta( #         x_train=task_split_data.outputs['x_train_out'], #         x_test=task_split_data.outputs['x_test_out'])"
  },
  {
    "id": 386,
    "repo": "Our-Glass/kubeflow_pipelines_example",
    "file_path": "example_custom_module/pipeline_lightweight.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nlen_X_train: {len_X_train}         path_X_train: {path_X_train}         uri_X_train: {uri_X_train}                  len_X_test: {len_X_test}         path_X_test: {path_X_test}         uri_X_test: {uri_X_test} # transform series to dataframe and add column 'y' split_data_light --> lightweigth component with custom python model # Import libraries # Load data from input # Split data"
  },
  {
    "id": 387,
    "repo": "Our-Glass/kubeflow_pipelines_example",
    "file_path": "kfpv2/pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\ncreate a breast cancer pipeline # get data # split data # TODO: add a component to evaluate the model run the pipeline"
  },
  {
    "id": 388,
    "repo": "mouachan/kubeflow-pipeline-project",
    "file_path": "pipeline/pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# Define the R script component # Define the Python script component # Set dependencies"
  },
  {
    "id": 389,
    "repo": "chuyangzh/kubeflow-spark-pipeline",
    "file_path": "kubeflow-pipeline/pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nname: Spark Weather Job description: Runs the Spark job to process weather data. implementation:     container:         image: spark-weather-job:latest         command:         - spark-submit         - /app/spark-job.py Define the Spark component # Add the Spark component to the pipeline Compile the pipeline for v1 compatibility"
  },
  {
    "id": 390,
    "repo": "j-cunanan/kubeflow-pipelines-TFOD",
    "file_path": "license.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 391,
    "repo": "Minseojeonn/Kubeflow_capstone_pipeline",
    "file_path": "Pipeline/pipe/tmp.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 392,
    "repo": "Louis5499/Kubeflow-mnist-pipeline",
    "file_path": "tfJob_kfServing_pipeline.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 393,
    "repo": "iqer/kubeflow_demo_pipeline",
    "file_path": "demo_pipeline1/pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\ndef test_op(x_test, y_test, model_dir, output_dir): return dsl.ContainerOp( name='Test Model', image='shanau2/boston_pipeline_test:v3', arguments=["
  },
  {
    "id": 394,
    "repo": "iqer/kubeflow_demo_pipeline",
    "file_path": "demo_pipeline3/pipeline.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 395,
    "repo": "sagravat/kubeflow-pipelines-examples",
    "file_path": "chest-xray/pipelines/pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nPipeline to train babyweight model !/usr/bin/env python3 Copyright 2018 Google LLC  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License."
  },
  {
    "id": 396,
    "repo": "sagravat/kubeflow-pipelines-examples",
    "file_path": "chest-xray/pipelines/train_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nPipeline to train babyweight model !/usr/bin/env python3 Copyright 2018 Google LLC  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License."
  },
  {
    "id": 397,
    "repo": "luizrennocosta/kubeflow-ml-pipeline",
    "file_path": "src/deep_model_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nPipeline"
  },
  {
    "id": 398,
    "repo": "luizrennocosta/kubeflow-ml-pipeline",
    "file_path": "src/lr_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nPipeline"
  },
  {
    "id": 399,
    "repo": "Here2ServeU/kubeflow-ml-pipeline",
    "file_path": "pipeline.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 400,
    "repo": "javierdarksoul/Kubeflow-pipelines-tutorial",
    "file_path": "tutorial-1/sum_mul_paralell_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# pipeline_root='gs://my-pipeline-root/example-pipeline' run the pipeline in v2 compatibility mode"
  },
  {
    "id": 401,
    "repo": "javierdarksoul/Kubeflow-pipelines-tutorial",
    "file_path": "tutorial-1/sum_mul_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# pipeline_root='gs://my-pipeline-root/example-pipeline' run the pipeline in v2 compatibility mode"
  },
  {
    "id": 402,
    "repo": "javierdarksoul/Kubeflow-pipelines-tutorial",
    "file_path": "tutorial-1/sum_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# pipeline_root='gs://my-pipeline-root/example-pipeline' run the pipeline in v2 compatibility mode"
  },
  {
    "id": 403,
    "repo": "javierdarksoul/Kubeflow-pipelines-tutorial",
    "file_path": "tutorial-2/main.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 404,
    "repo": "javierdarksoul/Kubeflow-pipelines-tutorial",
    "file_path": "tutorial-3/main.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 405,
    "repo": "javierdarksoul/Kubeflow-pipelines-tutorial",
    "file_path": "tutorial-4/main.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nsecret = projects/209915815446/secrets/vertex-ai-secret/versions/1"
  },
  {
    "id": 406,
    "repo": "javierdarksoul/Kubeflow-pipelines-tutorial",
    "file_path": "tutorial-5/main.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n#load_task= loads(src.outputs['output1path'])"
  },
  {
    "id": 407,
    "repo": "javierdarksoul/Kubeflow-pipelines-tutorial",
    "file_path": "example/train_nn/main.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# gcc_aip.EndpointCreateOp.component_spec.implementation.container.image = \"gcr.io/ml-pipeline/google-cloud-pipeline-components:0.1.7\" #gcc_aip.ModelDeployOp.component_spec.implementation.container.image = \"gcr.io/ml-pipeline/google-cloud-pipeline-components:0.1.7\" #project=project_id, #accelerator_type='NVIDIA_TESLA_P100',  # CHANGE THIS as necessary #accelerator_count=1"
  },
  {
    "id": 408,
    "repo": "javierdarksoul/Kubeflow-pipelines-tutorial",
    "file_path": "tutorial-6/sum_mul/sum_mul_paralell_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# pipeline_root='gs://my-pipeline-root/example-pipeline'"
  },
  {
    "id": 409,
    "repo": "javierdarksoul/Kubeflow-pipelines-tutorial",
    "file_path": "tutorial-6/train_nn/main.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nprint(job)"
  },
  {
    "id": 410,
    "repo": "THEANMOZHI/kubeflow-iris-pipeline",
    "file_path": "iris.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nkfp gcp i/o pandas & sklearn definites"
  },
  {
    "id": 411,
    "repo": "agapebondservant/kubeflow-pipelines-accelerator",
    "file_path": "app/main.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# Upload Dataset # Train Model # Evaluate Model # Promote Model to Staging"
  },
  {
    "id": 412,
    "repo": "himanshusharma89/kubeflow-ml-pipeline",
    "file_path": "pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nOrchestrate the end-to-end classification pipeline. pipeline.py # Data acquisition # Feature preparation # Model development # Performance assessment - Fixed the output reference"
  },
  {
    "id": 413,
    "repo": "himanshusharma89/kubeflow-ml-pipeline",
    "file_path": "components/data_acquisition.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nAcquire and prepare the initial dataset."
  },
  {
    "id": 414,
    "repo": "himanshusharma89/kubeflow-ml-pipeline",
    "file_path": "components/feature_preparation.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nTransform and split the dataset for modeling."
  },
  {
    "id": 415,
    "repo": "himanshusharma89/kubeflow-ml-pipeline",
    "file_path": "components/model_development.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nBuild and train the classification model."
  },
  {
    "id": 416,
    "repo": "himanshusharma89/kubeflow-ml-pipeline",
    "file_path": "components/performance_assessment.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nEvaluate model performance and generate visualization."
  },
  {
    "id": 417,
    "repo": "Louis5499/DRAGON-Kubeflow",
    "file_path": "tf_job_dragon_pipeline.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 418,
    "repo": "dhodun/kubeflow-pipeline-examples",
    "file_path": "mask_rcnn/mask_rcnn.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nPipeline to train Mask RCNN !/usr/bin/env python3 Copyright 2018 Google LLC  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License."
  },
  {
    "id": 419,
    "repo": "dhodun/kubeflow-pipeline-examples",
    "file_path": "mask_rcnn/mask_trt_example.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nPipeline to train Mask RCNN !/usr/bin/env python3 Copyright 2018 Google LLC  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License."
  },
  {
    "id": 420,
    "repo": "dhodun/kubeflow-pipeline-examples",
    "file_path": "mask_rcnn/mlp_babyweight.bak.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nPipeline to train babyweight model !/usr/bin/env python3 Copyright 2018 Google LLC  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License."
  },
  {
    "id": 421,
    "repo": "dhodun/kubeflow-pipeline-examples",
    "file_path": "mask_rcnn/test_client.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nDefine a Python function Advanced function Demonstrates imports, helper functions and multiple outputs # Imports inside a component function: # This function demonstrates how to use nested functions inside a component function:"
  },
  {
    "id": 422,
    "repo": "ventus550/kubeflow-pipelines-project",
    "file_path": "pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n+ - ## Static Configuration ## Pipeline definition +"
  },
  {
    "id": 423,
    "repo": "ventus550/kubeflow-pipelines-project",
    "file_path": "components/_metrics.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 424,
    "repo": "ventus550/kubeflow-pipelines-project",
    "file_path": "components/_shap_explainer.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# repurpose model for \"classification\" # shap is an awful library and thus requires awful fixes"
  },
  {
    "id": 425,
    "repo": "ventus550/kubeflow-pipelines-project",
    "file_path": "components/_split_data.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 426,
    "repo": "ventus550/kubeflow-pipelines-project",
    "file_path": "components/_train_model.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# vertex-provided directory for logs # Reserve two extra tokens for the ctc_loss # load from last checkpoint"
  },
  {
    "id": 427,
    "repo": "ventus550/kubeflow-pipelines-project",
    "file_path": "components/_upload_model.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 428,
    "repo": "Michallote/kubeflow-from-pipeline",
    "file_path": "generated_functions.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# Imports # Body # Inputs Loading # Function Call # Outputs Packing"
  },
  {
    "id": 429,
    "repo": "Michallote/kubeflow-from-pipeline",
    "file_path": "main_legacy.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nfrom typing import Any, NamedTuple import kfp.dsl as dsl from collections import namedtuple from kfp.dsl import Input, Output, Dataset, component from consts import MLFLOW_IMAGE # Imports # Body # Imports # Body"
  },
  {
    "id": 430,
    "repo": "Michallote/kubeflow-from-pipeline",
    "file_path": "kubeflow_from_pipeline/kfp_generator_declarative.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nfrom typing import Any, NamedTuple import kfp.dsl as dsl from collections import namedtuple from kfp.dsl import Input, Output, Dataset, component from consts import MLFLOW_IMAGE # Imports # Body"
  },
  {
    "id": 431,
    "repo": "Michallote/kubeflow-from-pipeline",
    "file_path": "kubeflow_from_pipeline/kfp_generator_namedtuple.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nfrom typing import Any, NamedTuple import kfp.dsl as dsl from collections import namedtuple from kfp.dsl import Input, Output, Dataset, component from consts import MLFLOW_IMAGE # Imports # Body # Imports # Body"
  },
  {
    "id": 432,
    "repo": "kashiftriffort/kubeflow-pipelines-master",
    "file_path": "boston_housing/pipeline.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 433,
    "repo": "ygeat7/kubeflow_pipeline_myxgb",
    "file_path": "pipeline.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 434,
    "repo": "Nannakaroliina/kubeflow-pipeline-demo",
    "file_path": "pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nBuild ContainerOps of Docker images with needed arguments for data/model access and file_output definitions. TODO Fix the FutureWarning regarding ContainerOp # Build a pipeline yaml file to be uploaded to Kubeflow Pipeline UI # TODO implement local run option without manual pipeline creation"
  },
  {
    "id": 435,
    "repo": "nevrets/kubeflow-pipelines-example",
    "file_path": "01-hello-world/pipeline.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 436,
    "repo": "nevrets/kubeflow-pipelines-example",
    "file_path": "02-Iris/pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# Submit the pipeline for execution"
  },
  {
    "id": 437,
    "repo": "nevrets/kubeflow-pipelines-example",
    "file_path": "03-ML-model-project/pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# Given the outputs from decision_tree, logistic regression, svm, naive_bayes, xgboost components # Loads the yaml manifest for each component # Run download_data task # Run ML models tasks with input data # Given the outputs from ML models tasks"
  },
  {
    "id": 438,
    "repo": "nevrets/kubeflow-pipelines-example",
    "file_path": "06-MLOps-example/iris-train-pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# ----- Temporary volume create ----- # ----- build -----"
  },
  {
    "id": 439,
    "repo": "ithingv/kubeflow_pipeline_mnist",
    "file_path": "add_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nCalculates sum of two arguments # pipeline_root='gs://my-pipeline-root/example-pipeline'"
  },
  {
    "id": 440,
    "repo": "ithingv/kubeflow_pipeline_mnist",
    "file_path": "mnist_pipeline.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 441,
    "repo": "raldebsi/Kubeflow-Experimental-Pipelines",
    "file_path": "src/pipelines/add_randoms.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# print(add_random_task1.outputs)"
  },
  {
    "id": 442,
    "repo": "raldebsi/Kubeflow-Experimental-Pipelines",
    "file_path": "src/pipelines/common_utils.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nConverts a number to base 26 then to alphabet, 0 -> a, 25 -> z, 26 -> aa # Get volume # If operation exists then it will be reused. # If operation does not exist then it will be created and the name will be resource_name below. # volume_name=\"ridhwan-personal-volume\", # Volume Name, do not use as it will cause the volume to not be found # data_source=PipelineParam(name=\"data_source\"), # This way it will create a volume snapshot from param"
  },
  {
    "id": 443,
    "repo": "raldebsi/Kubeflow-Experimental-Pipelines",
    "file_path": "src/pipelines/kube_exp_ridhwan.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nDownload data from a URL. # print(\"Volume:\", mount_vol) # print(\"Volume Mount:\", pvolumes)"
  },
  {
    "id": 444,
    "repo": "raldebsi/Kubeflow-Experimental-Pipelines",
    "file_path": "src/pipelines/test_upload.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 445,
    "repo": "raldebsi/Kubeflow-Experimental-Pipelines",
    "file_path": "src/pipelines/end_to_end/just_write.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 446,
    "repo": "raldebsi/Kubeflow-Experimental-Pipelines",
    "file_path": "src/pipelines/end_to_end/mnist_experiment.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# Convert Params # TFJob # Serve"
  },
  {
    "id": 447,
    "repo": "raldebsi/Kubeflow-Experimental-Pipelines",
    "file_path": "src/pipelines/end_to_end/mnist_simple.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# Convert Params # convert_params_op = func_to_container_op(convert_hyperparams) # convert_params_task = convert_params_op() # TFJob # Serve"
  },
  {
    "id": 448,
    "repo": "raldebsi/Kubeflow-Experimental-Pipelines",
    "file_path": "src/pipelines/end_to_end/serve_mnist_premade.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 449,
    "repo": "raldebsi/Kubeflow-Experimental-Pipelines",
    "file_path": "src/pipelines/end_to_end/serve_pt.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# gen_name_comp = func_to_container_op(generate_inference_service_name) # gen_name_task = gen_name_comp(dataset_name, experiment_name) # model_name=sane_service_name, # model_uri=\"pvc://{}/{}\".format(mount_name, experiment_name), # framework=\"pytorch\","
  },
  {
    "id": 450,
    "repo": "raldebsi/Kubeflow-Experimental-Pipelines",
    "file_path": "src/pipelines/end_to_end/topic_class.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# write the args to a file # convert args to string # return \" \".join(\"--{} {}\".format(k, v) for k, v in args.items()) # serve_task = create_serve_task(dataset_name, experiment_name, volume_name).after(train_task)"
  },
  {
    "id": 451,
    "repo": "raldebsi/Kubeflow-Experimental-Pipelines",
    "file_path": "src/pipelines/volume_test/read.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# mount_vol = kfp.dsl.VolumeOp( #     name=\"Read Ridhwan Volumes\", #     size=\"1Gi\", #     modes=kfp.dsl.VOLUME_MODE_RWO, #     resource_name=\"ridhwan-pvc-mount\", # PVC Name, will be {pipeline_name}-{id}-resource if name does not exist and generate_unique_name is True."
  },
  {
    "id": 452,
    "repo": "raldebsi/Kubeflow-Experimental-Pipelines",
    "file_path": "src/pipelines/volume_test/write.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# If operation exists then it will be reused. # If operation does not exist then it will be created and the name will be resource_name below. # volume_name=\"ridhwan-personal-volume\", # Volume Name, do not use as it will cause the volume to not be found # data_source=PipelineParam(name=\"data_source\"), # This way it will create a volume snapshot from param # Components"
  },
  {
    "id": 453,
    "repo": "markbarna/kubeflow-pipelines-poc",
    "file_path": "main.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# TODO: train model(s) (with tuning) in parallel? # TODO: batch predictions (move to separate pipeline) # TODO: serve model # TODO: github action to compile & deploy pipeline on release # TODO: unit tests on commit"
  },
  {
    "id": 454,
    "repo": "hafizurcse/azure-kubeflow-pipeline",
    "file_path": "code/pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# preprocess data #train # register model"
  },
  {
    "id": 455,
    "repo": "fontaine-raphael/kubeflow",
    "file_path": "pipelines/nyc_taxi.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nCompile"
  },
  {
    "id": 456,
    "repo": "rahuja23/Blogpost_kubeflow_pipeline",
    "file_path": "Pipeline/kfp_v2.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nvop = dsl.VolumeOp(             name=pvc_name,             resource_name=\"twitter-5000\",             size=\"1Gi\",             modes=dsl.VOLUME_MODE_RWM         )"
  },
  {
    "id": 457,
    "repo": "imsazzad/kubeflow-piplines-ml",
    "file_path": "app/kfp_pipeline.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 458,
    "repo": "sfujiwara/kfpc",
    "file_path": "examples/simple.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 459,
    "repo": "dhirajpatra/kubeflow-demo",
    "file_path": "pipeline.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 460,
    "repo": "secrettoad/kubeflow",
    "file_path": "multifamily_pricing/main.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n##TODO add capability for distributed cluster on compute engine ##TODO add hyperparameter tuning ##TODO invert control of parameters ##TODO compare against current version performance ##TODO log performance metrics to metadata"
  },
  {
    "id": 461,
    "repo": "romanzdk/-test-kubeflow",
    "file_path": "hello_kf.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nCopyright 2019 The Kubeflow Authors  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 462,
    "repo": "Natrofl/kubeflow-linear-pipline",
    "file_path": "convert.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 463,
    "repo": "TassaraR/vertex-kubeflow-pipelines-tutorial",
    "file_path": "model/pipelines/container-component-pipeline.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 464,
    "repo": "TassaraR/vertex-kubeflow-pipelines-tutorial",
    "file_path": "model/pipelines/lightweight-component-pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# Kubeflow artifact's metrics & metadata"
  },
  {
    "id": 465,
    "repo": "stackdemos/centralstate-edu",
    "file_path": "components/golang/component.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nReturns whether we are running in notebook."
  },
  {
    "id": 466,
    "repo": "stackdemos/centralstate-edu",
    "file_path": "components/training/component.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nReturns whether we are running in notebook."
  },
  {
    "id": 467,
    "repo": "HibernationNo1/project_4_kubeflow_pipeline",
    "file_path": "pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# persistance volume # for allocate shared memory # set secrets # gs_parser = parser.add_argument_group('google_storage') # get experiment id by create experiment or load experiment info"
  },
  {
    "id": 468,
    "repo": "HibernationNo1/project_4_kubeflow_pipeline",
    "file_path": "pipeline_config.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\ncfg arguments determines which component be run.         Components that matching cfg arguments which got `None` are excluded from the pipeline.         cfg arguments: is chooses in [args.cfg_train, args.cfg_record]     Args:         args : argparse # set config of model to training # set 0 when running for katib or in pod (not have enough shared memory) # If get dataset with dvc, load the paths from the database. # And all paths were set by dvc config # If get dataset with dvc, load the paths from the database."
  },
  {
    "id": 469,
    "repo": "tanle2694/kubeflow-resnet-tf-pipeline",
    "file_path": "pipeline/src/pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nCopyright (c) 2019, NVIDIA CORPORATION.  All rights reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 470,
    "repo": "gnanimail/kubeflow-pipeline-aircraft-emission",
    "file_path": "pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nclient = kfp.Client() client.create_run_from_pipeline_func(boston_pipeline, arguments={})"
  },
  {
    "id": 471,
    "repo": "Deeksha-5/Kubeflow-Model-Train-Pipeline",
    "file_path": "autoML.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 472,
    "repo": "PascalSchroederDE/kf-sample-pipeline",
    "file_path": "ml_test_pipeline.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 473,
    "repo": "PascalSchroederDE/kf-sample-pipeline",
    "file_path": "pipeline-fashion-mnist.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 474,
    "repo": "stackdemos/got-image-classification",
    "file_path": "components/golang/component.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nReturns whether we are running in notebook."
  },
  {
    "id": 475,
    "repo": "stackdemos/got-image-classification",
    "file_path": "components/training/component.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nReturns whether we are running in notebook."
  },
  {
    "id": 476,
    "repo": "drbwa/kfp-hello-world",
    "file_path": "kfp_hello_world/kfp_hello_world.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nOne component used in a simple pipeline. Compiles pipeline to YAML and creates a run from the pipeline YAML."
  },
  {
    "id": 477,
    "repo": "drbwa/kfp-hello-world",
    "file_path": "kfp_hello_world/kfp_hello_world2.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nSimple pipeline that chains to invocations of a single component to each other. Does not produce KFP IR through compilation and instead creates a run directly from the pipeline function defined here."
  },
  {
    "id": 478,
    "repo": "hh10/Minimal-Kubeflow-Pipeline-Template",
    "file_path": "kf_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# a PVC claim to clone is provided # set these resource requests/limits to ensure that each pod is assigned to a single node/machine # cleanup pv(c)s"
  },
  {
    "id": 479,
    "repo": "shahriar0999/ml-pipeline-with-kubeflow",
    "file_path": "kubeflow_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nStep 1: Load Dataset # Save the dataset to the output artifact path Step 2: Preprocess Data # Load dataset # Debug: Check for NaN values"
  },
  {
    "id": 480,
    "repo": "quynhtl/build-pipeline-on-Kubeflow",
    "file_path": "pipeline01.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# model_version = dsl.ContainerOp( #     name='Model Versioning', #     image='quynhtl/today_code1:latest', #     command=['python', 'version_data.py'], #     arguments=['--model-dir', model_train.outputs['model_path'], '--version-dir', '/version']"
  },
  {
    "id": 481,
    "repo": "quynhtl/build-pipeline-on-Kubeflow",
    "file_path": "pipeline_base/test1.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 482,
    "repo": "ruteee/kubeflow-pipeline-gcp-deploy",
    "file_path": "source/main.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nGet iris dataset from UCI reposiory     Returns:          df_data - Dataframe containing 4 features          regarding iris dataset and the target"
  },
  {
    "id": 483,
    "repo": "lajd/local_kubeflow",
    "file_path": "examples/pytorch_lightning_cifar10/pl_train_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n!/usr/bin/env python3 Ref: https://pytorch-lightning.readthedocs.io/en/stable/notebooks/lightning_examples/cifar10-baseline.html # TODO: Handle directory output # file_outputs={'output': '/opt/model/lightning_logs/'},"
  },
  {
    "id": 484,
    "repo": "lajd/local_kubeflow",
    "file_path": "examples/simple_pipeline/simple_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n!/usr/bin/env python3 Ref: https://github.com/kubeflow/examples/tree/master/demos/simple_pipeline"
  },
  {
    "id": 485,
    "repo": "lajd/local_kubeflow",
    "file_path": "examples/spark/spark_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nRead Spark Operator job manifest file and return the corresponding dictionary and     add some randomness in the job name     :return: dictionary defining the spark job Ref: https://github.com/sbakiu/kubeflow-spark/blob/main/kubeflow_pipeline.py # Read manifest file # Add epoch time in the job name # Remove cache # Load spark job manifest"
  },
  {
    "id": 486,
    "repo": "awskosehy/fashion_mnist_kubeflow_pipeline",
    "file_path": "pipeline/pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nExtract the best trial name.            Args:     katib_results: The JSON object formatted the hyperparameter set of the best experiment trial result      Returns:     best_trial_name: string which contain the best experiment trial name # resolve kfp_server_api.exceptions.ApiException: (400) NAMESPACE is empty issue # Compile pipeline to generate compressed YAML definition of the pipeline."
  },
  {
    "id": 487,
    "repo": "anilkharde/FlexiKubeflowPipelineSolutions",
    "file_path": "pipeline/pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nInitialize the FlexiPipeline class with the configuration path.                  Args:             config_path (str): Path to the JSON configuration file. # Import the custom processing module # Process implementation # Define the code PVC (Persistent Volume Claim) -------------------------------------------------------------------------------- # Pre-process task # Custom process tasks for each image ID"
  },
  {
    "id": 488,
    "repo": "ImranRiazChohan/ml_pipeline_using_kubeflow",
    "file_path": "pipeline.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 489,
    "repo": "vyomagg/poc_kubeflow_regression_pipeline",
    "file_path": "pipeline/pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nkfp.compiler.Compiler().compile(regression_pipeline, 'regression_pipeline.zip') Global Parameters Auto Execution of pipeline"
  },
  {
    "id": 490,
    "repo": "AlbughdadiM/kubeflow-pipeline-crop-classification",
    "file_path": "pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# create components from yaml manifest #output_component_file='compare_models.yaml', # Run first task #download_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\" # create temporal stats from results of the previous task"
  },
  {
    "id": 491,
    "repo": "biswaroop1547/skit-pipelines",
    "file_path": "skit_pipelines/utils/k8s.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n## since currently gpu node doesn't support all integrations with db, ## we set default pipeline nodeselector to be CPU node ## while setting gpu node only for the required component in pipeline. # node_type = models.PodNodeSelectorMap[pipeline_name] # if node_type == const.CPU_NODE_LABEL:"
  },
  {
    "id": 492,
    "repo": "biswaroop1547/skit-pipelines",
    "file_path": "skit_pipelines/pipelines/asr_tune/__init__.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nTODO: Docstring. # create a component that can make sure: # 1. target_model_path does not already exist. # 2. target_model_path is a valid s3 path. # TODO: check_s3_path_does_not_exist_op(target_model_path)"
  },
  {
    "id": 493,
    "repo": "biswaroop1547/skit-pipelines",
    "file_path": "skit_pipelines/pipelines/eval_asr_pipeline/__init__.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nEvaluates ASR transcriptions using transcription tags.      .. _p_eval_asr_pipeline:      Example payload to invoke this pipeline via slack integrations:          @charon run eval_asr_pipeline          .. code-block:: python              {                 \"s3_path_data\": \"s3://bucket-name/data/\",                 \"org_id\": \"org\"             }      :param s3_path_data: S3 path to a tagged dataset (.csv).     :type s3_path_data: str     :param org_id: reference path to save the metrics.     :type org_id: str     :param channel: The slack channel to send the notification, defaults to \"\"     :type channel: str, optional     :param slack_thread: The slack thread to send the notification, defaults to \"\"     :type slack_thread: str, optional # Create true label column # Create utterance column # produce test set metrics."
  },
  {
    "id": 494,
    "repo": "biswaroop1547/skit-pipelines",
    "file_path": "skit_pipelines/pipelines/evaluate_slu/__init__.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nA pipeline to evaluate an existing SLU model.      .. _p_evaluate_slu:      Example payload to invoke via slack integrations:      A minimal example:          @charon run evaluate_slu          .. code-block:: python              {                 \"repo_name\": \"slu_repo_name\",                 \"labelstudio_project_ids\": \"10,13\",                 \"test_dataset_path\":\"s3://bucket/data.csv\"             }       A full available parameters example:          @charon run evaluate_slu          .. code-block:: python              {                 \"repo_name\": \"slu_repo_name\",                 \"repo_branch\": \"master\",                 \"test_dataset_path\": \"s3://bucket-name/path1/to1/data1.csv,s3://bucket-name/path2/to2/data2.csv\",                 \"job_ids\": \"4011,4012\",                 \"labelstudio_project_ids\": \"10,13\",                 \"job_start_date\": \"2022-08-01\",                 \"job_end_date\": \"2022-09-19\",                 \"remove_intents\": \"_confirm_,_oos_,audio_speech_unclear,ood\",                 \"alias_yaml_path\": \"intents/oppo/alias.yaml\"             }       :param repo_name: SLU repository name under /vernacularai/ai/clients org in gitlab.     :type repo_name: str      :param repo_branch: The branch name in the SLU repository one wants to use, defaults to master.     :type repo_name: str, optional      :param test_dataset_path: The S3 URI or the S3 key for the tagged dataset (can be multiple - comma separated).     :type dataset_path: str, optional      :param job_ids: The job ids as per tog. Optional if labestudio_project_ids is provided.     :type job_ids: str      :param labelstudio_project_ids: The labelstudio project id (this is a number) since this is optional, defaults to \"\".     :type labelstudio_project_ids: str      :param job_start_date: The start date range (YYYY-MM-DD) to filter tagged data.     :type job_start_date: str, optional      :param job_end_date: The end date range (YYYY-MM-DD) to filter tagged data     :type job_end_date: str, optional      :param remove_intents: Comma separated list of intents to remove from dataset while training.     :type remove_intents: str, optional      :param alias_yaml_path: eevee's intent_report alias.yaml, refer docs `here <https://skit-ai.github.io/eevee/metrics/intents.html#aliasing>`_ . Upload your yaml to eevee-yamls repository `here <https://github.com/skit-ai/eevee-yamls>`_ & pass the relative path of the yaml from base of the repository.     :type alias_yaml_path: str, optional      :param core_slu_repo_name: Name of repository for core slu service. Defaults to core-slu-service     :type core_slu_repo_name: str, optional      :param core_slu_repo_branch: Branch to check out for core slu repository. Defaults to master     :type core_slu_repo_branch: str, optional      :param customization_repo_name: Name of repository for customization service. Defaults to customization     :type customization_repo_name: str, optional      :param customization_repo_branch: Branch to check out for customization service repository. Defaults to master     :type customization_repo_branch: str, optional      :param notify: Whether to send a slack notification, defaults to \"\"     :type notify: str, optional      :param channel: The slack channel to send the notification, defaults to \"\"     :type channel: str, optional      :param slack_thread: The slack thread to send the notification, defaults to \"\"     :type slack_thread: str, optional"
  },
  {
    "id": 495,
    "repo": "biswaroop1547/skit-pipelines",
    "file_path": "skit_pipelines/pipelines/fetch_calls_pipeline/__init__.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nA pipeline to randomly sample calls for a given voice-bot project.      .. _p_fetch_calls_pipeline:       Example payload to invoke this pipeline via slack integrations:          @charon run fetch_calls_pipeline          .. code-block:: python              {                 \"client_id\": 1,                 \"start_date\": \"2020-01-01\",                 \"lang\": \"en\",                 \"end_date\": \"2020-01-01\",                 \"reported\": false,                 \"call_quantity\": 200             }      :param client_id: The comma separated client ids as per fsm db.     :type client_id: str, optional     :param start_date: The start date range to filter calls in YYYY-MM-DD format.     :type start_date: str     :param lang: The language code of the calls to filter. eg: en, hi, ta, te, etc.     :type lang: str     :param end_date: The end date range to filter calls in YYYY-MM-DD format.     :type end_date: str     :param ignore_callers: Comma separated list of callers to ignore, defaults to \"\"     :type ignore_callers: str, optional     :param reported: Pick only reported calls, defaults to False     :type reported: bool     :param template_id: The flow template id to filter calls, defaults to \"\"     :type template_id: str, optional     :param use_case: Voice bot project's use-case, defaults to \"\"     :type use_case: str, optional     :param flow_name: Identifier for a whole/part of a voicebot conversation flow, defaults to \"\"     :type flow_name: str, optional     :param min_duration: Call duration filter, defaults to \"\"     :type min_duration: str, optional     :param asr_provider: The ASR vendor (google/VASR), defaults to \"\"     :type asr_provider: str, optional     :param states: Filter calls in a comma separated list of states, defaults to \"\"     :type states: str, optional     :param intents: Filter turns in sampled calls from a comma separated list of intents, defaults to \"\"     :type intents: str, optional     :param call_quantity: Number of calls to sample, defaults to 200     :type call_quantity: int, optional     :param call_type: inbound, outbound vs subtesting call filters. We can currently choose only one of these, or defaults to \"INBOUND\" and \"OUTBOUND\" both     :type call_type: str, optional     :param remove_empty_audios: to remove calls with call audios being empty/broken, defaults to True     :type remove_empty_audios: bool     :param notify: A comma separated list of slack ids: \"@apples, @orange.fruit\" etc, defaults to \"\"     :type notify: str, optional     :param channel: The slack channel to send the notification, defaults to \"\"     :type channel: str, optional     :param slack_thread: The slack thread to send the notification, defaults to \"\"     :type slack_thread: str, optional     :param use_fsm_url: Whether to use turn audio url from fsm or s3 path., defaults to False     :type use_fsm_url: bool, optional     :param flow_id: Id for a whole/part of a voicebot conversation flow, defaults to \"\"     :type flow_id: str, optional"
  },
  {
    "id": 496,
    "repo": "biswaroop1547/skit-pipelines",
    "file_path": "skit_pipelines/pipelines/fetch_n_tag_turns_and_calls/__init__.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nA pipeline to randomly sample calls and upload for annotating turns for intents & entities and annotating calls for slots & call level metrics.      .. _p_fetch_n_tag_turns_and_calls:      Example payload to invoke via slack integrations:          @charon run fetch_n_tag_turns_and_calls          .. code-block:: python              {                 \"client_id\": 41,                 \"org_id\": 34,                 \"lang\": \"en\",                 \"start_date\": \"2022-11-10\",                 \"end_date\": \"2022-11-11\",                 \"labelstudio_project_id\": 195,                 \"call_project_id\": 194,                 \"data_label\": \"Client\"             }      To use labelstudio:          @charon run fetch_n_tag_turns_and_calls          .. code-block:: python              {                 \"org_id\": 34,                 \"client_id\": 41,                 \"start_date\": \"2022-09-16\",                 \"end_date\": \"2022-09-19\",                 \"lang\": \"en\",                 \"reported\": false,                 \"call_quantity\": 1000,                 \"flow_name\" : \"indigo_domain_tuning_english\"                 \"labelstudio_project_id\": \"135\",                 \"call_project_id\": 194             }      :param client_id: The comma separated client ids as per fsm db.     :type client_id: str, optional      :param org_id: The organization id as per api-gateway.     :type org_id: str      :param labelstudio_project_id: The labelstudio project id for turn level tagging (intent & entities) (this is a number) since this is optional, defaults to \"\".     :type labelstudio_project_id: str      :param call_project_id: The labelstudio project id for call level tagging (slots & call metrics) (this is a number) since this is optional, defaults to \"\".     :type call_project_id: str      :param data_label: A label to identify the source of a datapoint     :type data_label: str, optional. Defaults to \"Live\"      :param start_date: The start date range to filter calls in YYYY-MM-DD format.     :type start_date: str      :param lang: The language code of the calls to filter. eg: en, hi, ta, te, etc.     :type lang: str      :param end_date: The end date range to filter calls in YYYY-MM-DD format.     :type end_date: str      :param ignore_callers: Comma separated list of callers to ignore, defaults to \"\"     :type ignore_callers: str, optional      :param reported: Pick only reported calls, defaults to False     :type reported: bool      :param template_id: The flow template id to filter calls, defaults to \"\"     :type template_id: str, optional      :param use_case: Voice bot project's use-case, defaults to \"\"     :type use_case: str, optional      :param flow_name: Identifier for a whole/part of a voicebot conversation flow, defaults to \"\"     :type flow_name: str, optional      :param min_duration: Call duration filter, defaults to \"\"     :type min_duration: str, optional      :param asr_provider: The ASR vendor (google/VASR), defaults to \"\"     :type asr_provider: str, optional      :param states: Filter calls in a comma separated list of states, defaults to \"\"     :type states: str, optional      :param intents: Filter turns in sampled calls from a comma separated list of intents, defaults to \"\"     :type intents: str, optional      :param start_date_offset: Offset the start date by an integer value, defaults to 0     :type start_date_offset: int, optional      :param end_date_offset: Offset the end date by an integer value, defaults to 0     :type end_date_offset: int, optional      :param start_time_offset: Offset the start time by an integer value, defaults to 0     :type start_time_offset: int, optional      :param end_time_offset: Offset the end time by an integer value, defaults to 0     :type end_time_offset: int, optional      :param calls_file_s3_path: The s3_path to upload the turns from instead of querying from FSM_db, defaults to \"\"     :type calls_file_s3_path: str, optional      :param call_quantity: Number of calls to sample, defaults to 200     :type call_quantity: int, optional      :param call_type: INBOUND, OUTBOUND, or CALL_TEST call filters. We can currently choose only one of these, or defaults to \"INBOUND\" and \"OUTBOUND\" both     :type call_type: str, optional      :param notify: Whether to send a slack notification, defaults to \"\"     :type notify: str, optional      :param channel: The slack channel to send the notification, defaults to \"\"     :type channel: str, optional      :param slack_thread: The slack thread to send the notification, defaults to \"\"     :type slack_thread: float, optional      :param use_fsm_url: Whether to use turn audio url from fsm or s3 path., defaults to False     :type use_fsm_url: bool, optional      :param remove_empty_audios: Whether to turns of empty audio., defaults to False     :type remove_empty_audios: bool, optional      :param use_assisted_annotation: Whether to use GPT for intent prediction, only applicable to US collections, defaults to False     :type use_assisted_annotation: bool, optional          :param flow_ids: Id for a whole/part of a voicebot conversation flow, defaults to \"\"     :type flow_ids: str, optional # Get intent response from GPT for qualifying turns # uploads data for turn level intent, entity & transcription tagging # uploads data for call & slot level tagging to labelstudio"
  },
  {
    "id": 497,
    "repo": "biswaroop1547/skit-pipelines",
    "file_path": "skit_pipelines/pipelines/fetch_tagged_calls_dataset/__init__.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nA pipeline to fetch tagged dataset.      .. _p_fetch_tagged_calls_dataset:      Example payload to invoke via slack integrations:          @charon run fetch_tagged_calls_dataset          .. code-block:: python              {                 \"org_id\": 1,                 \"job_id\": \"4011\",                 \"start_date\": \"2020-01-01\",                 \"end_date\": \"2020-01-01\"             }      To use labelstudio:          @charon run fetch_tagged_calls_dataset          .. code-block:: python              {                 \"org_id\": 1,                 \"labelstudio_project_id\": \"40\",                 \"start_date\": \"2020-01-01\",                 \"end_date\": \"2020-01-01\"             }      :param org_id: reference path to save the metrics.     :type org_id: str     :param job_ids: The job ids as per tog. Optional if labestudio project id is provided.     :type job_id: str     :param labelstudio_project_id: The labelstudio project id (this is a number) since this is optional, defaults to \"\".     :type labelstudio_project_id: str     :param start_date: The start date range (YYYY-MM-DD) to filter tagged data.     :type start_date: str     :param end_date: The end date range (YYYY-MM-DD) to filter tagged data     :type end_date: str     :param timezone: The timezone to apply for multi-region datasets, defaults to \"Asia/Kolkata\"     :type timezone: str, optional     :param task_type: https://github.com/skit-ai/skit-labels#task-types, defaults to \"conversation\"     :type task_type: str, optional     :param notify: A comma separated list of slack ids: \"@apples, @orange.fruit\" etc, defaults to \"\"     :type notify: str, optional     :param channel: The slack channel to send the notification, defaults to \"\"     :type channel: str, optional     :param slack_thread: The slack thread to send the notification, defaults to \"\"     :type slack_thread: str, optional"
  },
  {
    "id": 498,
    "repo": "biswaroop1547/skit-pipelines",
    "file_path": "skit_pipelines/pipelines/fetch_tagged_data_from_labelstore/__init__.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nA pipeline aimed at querying intent, entity, and transcriptions that happen across Skit      .. _p_fetch_tagged_data_from_labelstore:      Example payload to invoke via slack integrations:          @charon run fetch_tagged_data_from_labelstore          .. code-block:: python              {                 \"flow_id\": \"294\",                 \"limit\": 20,                 \"start_date\": \"2022-11-12\",                 \"end_date\": \"2022-11-16\",                 \"data_labels\": \"Client, Live\"             }      :param flow_id: The id of the flow from which annotated data should be queried     :type flow_id: str      :param start_date: The start date range (YYYY-MM-DD) to filter tagged data. defaults to yesterday     :type start_date: str, optional      :param end_date: The end date range (YYYY-MM-DD) to filter tagged data, defaults to today     :type end_date: str, optional      :param limit: Number of annotations to fetch, defaults to 2000     :type limit: int, optional      :param notify: Whether to send a slack notification, defaults to \"\"     :type notify: str, optional      :param channel: The slack channel to send the notification, defaults to \"\"     :type channel: str, optional      :param slack_thread: The slack thread to send the notification, defaults to \"\"     :type slack_thread: float, optional      :param data_labels: Comma seperated data labels to filter, defaults to \"\"     :type data_labels: str, optional"
  },
  {
    "id": 499,
    "repo": "biswaroop1547/skit-pipelines",
    "file_path": "skit_pipelines/pipelines/fetch_tagged_entity_dataset/__init__.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nA pipeline to fetch tagged entity dataset wiht modifications ready for eval.      .. _p_fetch_tagged_entity_dataset:      Example payload to invoke via slack integrations:          @charon run fetch_tagged_entity_dataset          .. code-block:: python              {                 \"org_id\": 1,                 \"job_id\": \"4011\",                 \"start_date\": \"2020-01-01\",                 \"end_date\": \"2020-01-01\"             }      To use labelstudio:          @charon run fetch_tagged_entity_dataset          .. code-block:: python              {                 \"org_id\": 1,                 \"labelstudio_project_id\": \"40\",                 \"start_date\": \"2020-01-01\",                 \"end_date\": \"2020-01-01\"             }      :param org_id: reference path to save the metrics.     :type org_id: str     :param job_ids: The job ids as per tog. Optional if labestudio project id is provided.     :type job_id: str     :param labelstudio_project_id: The labelstudio project id (this is a number) since this is optional, defaults to \"\".     :type labelstudio_project_id: str     :param start_date: The start date range (YYYY-MM-DD) to filter tagged data.     :type start_date: str     :param end_date: The end date range (YYYY-MM-DD) to filter tagged data     :type end_date: str     :param timezone: The timezone to apply for multi-region datasets, defaults to \"Asia/Kolkata\"     :type timezone: str, optional     :param task_type: https://github.com/skit-ai/skit-labels#task-types, defaults to \"conversation\"     :type task_type: str, optional     :param notify: A comma separated list of slack ids: \"@apples, @orange.fruit\" etc, defaults to \"\"     :type notify: str, optional     :param channel: The slack channel to send the notification, defaults to \"\"     :type channel: str, optional     :param slack_thread: The slack thread to send the notification, defaults to \"\"     :type slack_thread: str, optional"
  },
  {
    "id": 500,
    "repo": "biswaroop1547/skit-pipelines",
    "file_path": "skit_pipelines/pipelines/generate_and_tag_conversations/__init__.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nA pipeline to generate and tag conversations given a situation          .. _p_generate_and_tag_conversations:      Example payload to invoke via slack integrations:      A minimal example:          @charon run generate_and_tag_conversations          .. code-block:: python              {   \"situations\" : \"The user disputes the debt, so the agent transfers the call to the agent :: The user cannot pay any amount as they have a difficult situation, so the agent hangs up the call. \",                 \"scenario\" : \"Test scenario\",                 \"scenario_category\" : \"Test scenario category\",                 \"llm_trainer_repo_branch\" : \"refactor-data-gen-script\",                 \"client_id\" : \"85\",                 \"template_id\" : \"0\",                 \"labelstudio_project_id\" : \"95\",                 \"s3_links_to_prompts\": \"s3://kubeflow-us-cluster/pipeline_uploads/prompt/test_prompt.txt\",                 \"data_label\" : \"UAT\",                 \"project_name\" : \"test project name\"             }       A full available parameters example:          @charon run generate_and_tag_conversations          .. code-block:: python              {   \"situations\" : \"The user disputes the debt, so the agent transfers the call to the agent :: The user cannot pay any amount as they have a difficult situation, so the agent hangs up the call. \",                 \"scenario\" : \"Test scenario\",                 \"scenario_category\" : \"Test scenario category\",                 \"llm_trainer_repo_branch\" : \"refactor-data-gen-script\",                 \"client_id\" : \"85\",                 \"template_id\" : \"0\",                 \"labelstudio_project_id\" : \"95\",                 \"s3_links_to_prompts\": \"s3://kubeflow-us-cluster/pipeline_uploads/prompt/test_prompt.txt\",                 \"data_label\" : \"UAT\",                 \"project_name\" : \"test project name\"             }          :param situations: The situations for generating the conversations, use delimiter :: to pass multiple situations     :type situations: optional      :param scenario: The scenario linked to the situation     :type scenario: optional          :param scenario_category: The scenarios category     :type scenario_category: optional          :param prompt: Prompt to the model for data generation     type prompt: str          :param s3_links_to_prompts: s3 links to the prompt to the model for data generation     :type s3_links_to_prompts: str          :param output_dir: The output directory where the generated conversations gets stored     :type output_dir: str      :param filename: Acts as a prfix to the default naming used     :type filename: str      :param llm_trainer_repo_name: The conversation generation repo name in Github.     :type llm_trainer_repo_name: str          :param llm_trainer_repo_branch: The branch name in the conversation generation repo to use , defaults to main.     :type llm_trainer_repo_branch: str, optional          :param model: Optional model to be used for generating data      :type model: str          :param n_iter: No of times we make iterate on scenarios list to generate conversations     type n_iter: int          :param n_choice: No of convs generated in a single time from a scenario.     type n_choice: int          :param temperature: Temperature     type temperature: float          :param client_id: id of the client for which data is being generated     :type client_id : str          :param template_id: template id for which data is being generated     :type template_id : str          :param project_name: project name to distinguish between various experiments     :type project_name : str          :param notify: Whether to send a slack notification, defaults to \"\"     :type notify: str, optional      :param channel: The slack channel to send the notification, defaults to \"\"     :type channel: str, optional      :param slack_thread: The slack thread to send the notification, defaults to \"\"     :type slack_thread: str, optional"
  },
  {
    "id": 501,
    "repo": "biswaroop1547/skit-pipelines",
    "file_path": "skit_pipelines/pipelines/generate_sample_conversations/__init__.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nA pipeline to sample conversations given a situation          .. _p_generate_sample_conversations:      Example payload to invoke via slack integrations:      A minimal example:          @charon run generate_sample_conversations          .. code-block:: python              {                 \"situations\": \"The user wants to talk to a human agent, so the agent transfers the call\",                 \"llm_trainer_repo_name\": \"LLMtrainer\",                 \"llm_trainer_repo_branch\": \"main\"                 }       A full available parameters example:          @charon run generate_sample_conversations          .. code-block:: python              {                 \"situations\": \"The user disputes the debt, so the agent transfers the call to the agent :: The user cannot pay any amount as they have a difficult situation, so the agent hangs up the call. \"                 \"llm_trainer_repo_name\": \"LLMtrainer\",                 \"llm_trainer_repo_branch\": \"main\",             }      :param situations: The situations for generating the conversations     :type situations: optional          :param prompt: Prompt to the model for data generation     type prompt: str      :param filename: Acts as a prfix to the default naming used     :type filename: str      :param llm_trainer_repo_name: The conversation generation repo name in Github.     :type llm_trainer_repo_name: str          :param llm_trainer_repo_branch: The branch name in the conversation generation repo to use , defaults to main.     :type llm_trainer_repo_branch: str, optional          :param model: Optional model to be used for generating data      :type model: str          :param n_iter: No of times we make iterate on sub_scenarios list to generate conversations     type n_iter: int          :param n_choice: No of convs generated in a single time from a scenario.     type n_choice: int          :param temperature: Temperature     type temperature: float          :param notify: Whether to send a slack notification, defaults to \"\"     :type notify: str, optional      :param channel: The slack channel to send the notification, defaults to \"\"     :type channel: str, optional      :param slack_thread: The slack thread to send the notification, defaults to \"\"     :type slack_thread: str, optional"
  },
  {
    "id": 502,
    "repo": "biswaroop1547/skit-pipelines",
    "file_path": "skit_pipelines/pipelines/invalidate_llm_situations_in_db/__init__.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nSets conversations as invalid, thereby preventing it from being used for training"
  },
  {
    "id": 503,
    "repo": "biswaroop1547/skit-pipelines",
    "file_path": "skit_pipelines/pipelines/publish_compliance_breaches/__init__.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nFetches sampled calls from production db and checks for potential compliance breaches (only          for US collections application"
  },
  {
    "id": 504,
    "repo": "biswaroop1547/skit-pipelines",
    "file_path": "skit_pipelines/pipelines/retrain_slu/__init__.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nA pipeline to retrain an existing SLU model.      .. _p_retrain_slu:      Example payload to invoke via slack integrations:      A minimal example:          @charon run retrain_slu          .. code-block:: python              {                 \"repo_name\": \"slu_repo_name\",                 \"labelstudio_project_ids\": \"10,13\"             }       A full available parameters example:          @charon run retrain_slu          .. code-block:: python              {                 \"repo_name\": \"slu_repo_name\",                 \"repo_branch\": \"master\",                 \"dataset_path\": \"s3://bucket-name/path1/to1/data1.csv,s3://bucket-name/path2/to2/data2.csv\",                 \"job_ids\": \"4011,4012\",                 \"labelstudio_project_ids\": \"10,13\",                 \"job_start_date\": \"2022-08-01\",                 \"job_end_date\": \"2022-09-19\",                 \"remove_intents\": \"_confirm_,_oos_,audio_speech_unclear,ood\",                 \"alias_yaml_path\": \"intents/oppo/alias.yaml\",                 \"use_previous_dataset\": True,                 \"train_split_percent\": 85,                 \"stratify\": False,                 \"epochs\": 10,             }       Training an SLU for first time example:          @charon run retrain_slu          .. code-block:: python              {                 \"repo_name\": \"slu_repo_name\",                 \"repo_branch\": \"master\",                 \"labelstudio_project_ids\": \"10,13\",                 \"initial_training\": True             }       :param repo_name: SLU repository name under /vernacularai/ai/clients org in gitlab.     :type repo_name: str      :param repo_branch: The branch name in the SLU repository one wants to use, defaults to master.     :type repo_name: str, optional      :param dataset_path: The S3 URI or the S3 key for the tagged dataset (can be multiple - comma separated).     :type dataset_path: str, optional      :param custom_test_dataset_path: The S3 URI or the S3 key for the tagged dataset to be used for model evaluation (can be multiple - comma separated).     :type custom_test_dataset_path: str, optional      :param job_ids: The job ids as per tog. Optional if labestudio_project_ids is provided.     :type job_ids: str      :param labelstudio_project_ids: The labelstudio project id (this is a number) since this is optional, defaults to \"\".     :type labelstudio_project_ids: str      :param job_start_date: The start date range (YYYY-MM-DD) to filter tagged data.     :type job_start_date: str, optional      :param job_end_date: The end date range (YYYY-MM-DD) to filter tagged data     :type job_end_date: str, optional      :param remove_intents: Comma separated list of intents to remove from dataset while training.     :type remove_intents: str, optional      :param alias_yaml_path: eevee's intent_report alias.yaml, refer docs `here <https://skit-ai.github.io/eevee/metrics/intents.html#aliasing>`_ . Upload your yaml to eevee-yamls repository `here <https://github.com/skit-ai/eevee-yamls>`_ & pass the relative path of the yaml from base of the repository.     :type alias_yaml_path: str, optional      :param initial_training: Set to true only if you're training a model for the first time, defaults to False.     :type initial_training: bool, optional      :param use_previous_dataset: Before retraining combines new dataset with last dataset the model was trained on, defaults to True.     :type use_previous_dataset: bool, optional      :param train_split_percent: Percentage of new data one should train the model on, defaults to 85.     :type train_split_percent: int, optional      :param stratify: For stratified splitting of dataset into train and test set, defaults to False.     :type stratify: bool, optional      :param core_slu_repo_name: Name of repository for core slu service. Defaults to core-slu-service     :type core_slu_repo_name: str, optional      :param core_slu_repo_branch: Branch to check out for core slu repository. Defaults to master     :type core_slu_repo_branch: str, optional      :param customization_repo_name: Name of repository for customization service. Defaults to customization     :type customization_repo_name: str, optional      :param customization_repo_branch: Branch to check out for customization service repository. Defaults to master     :type customization_repo_branch: str, optional      :param target_mr_branch: Target branch against which the MR will be created. Defaults to sandbox     :type target_mr_branch: str, optional      :param notify: Whether to send a slack notification, defaults to \"\"     :type notify: str, optional      :param channel: The slack channel to send the notification, defaults to \"\"     :type channel: str, optional      :param slack_thread: The slack thread to send the notification, defaults to \"\"     :type slack_thread: str, optional"
  },
  {
    "id": 505,
    "repo": "biswaroop1547/skit-pipelines",
    "file_path": "skit_pipelines/pipelines/retrain_slu_old/__init__.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nA pipeline to retrain an existing SLU model.      .. _p_retrain_slu:      Example payload to invoke via slack integrations:      A minimal example:          @charon run retrain_slu          .. code-block:: python              {                 \"repo_name\": \"slu_repo_name\",                 \"labelstudio_project_ids\": \"10,13\"             }       A full available parameters example:          @charon run retrain_slu          .. code-block:: python              {                 \"repo_name\": \"slu_repo_name\",                 \"repo_branch\": \"master\",                 \"dataset_path\": \"s3://bucket-name/path1/to1/data1.csv,s3://bucket-name/path2/to2/data2.csv\",                 \"job_ids\": \"4011,4012\",                 \"labelstudio_project_ids\": \"10,13\",                 \"job_start_date\": \"2022-08-01\",                 \"job_end_date\": \"2022-09-19\",                 \"remove_intents\": \"_confirm_,_oos_,audio_speech_unclear,ood\",                 \"alias_yaml_path\": \"intents/oppo/alias.yaml\",                 \"use_previous_dataset\": True,                 \"train_split_percent\": 85,                 \"stratify\": False,                 \"epochs\": 10,             }       Training an SLU for first time example:          @charon run retrain_slu          .. code-block:: python              {                 \"repo_name\": \"slu_repo_name\",                 \"repo_branch\": \"master\",                 \"labelstudio_project_ids\": \"10,13\",                 \"initial_training\": True             }       :param repo_name: SLU repository name under /vernacularai/ai/clients org in gitlab.     :type repo_name: str      :param repo_branch: The branch name in the SLU repository one wants to use, defaults to master.     :type repo_name: str, optional      :param dataset_path: The S3 URI or the S3 key for the tagged dataset (can be multiple - comma separated).     :type dataset_path: str, optional      :param job_ids: The job ids as per tog. Optional if labestudio_project_ids is provided.     :type job_ids: str      :param labelstudio_project_ids: The labelstudio project id (this is a number) since this is optional, defaults to \"\".     :type labelstudio_project_ids: str      :param epochs: Number of epchs to train the model, defaults to 10     :type epochs: int, optional      :param job_start_date: The start date range (YYYY-MM-DD) to filter tagged data.     :type job_start_date: str, optional      :param job_end_date: The end date range (YYYY-MM-DD) to filter tagged data     :type job_end_date: str, optional      :param remove_intents: Comma separated list of intents to remove from dataset while training.     :type remove_intents: str, optional      :param alias_yaml_path: eevee's intent_report alias.yaml, refer docs `here <https://skit-ai.github.io/eevee/metrics/intents.html#aliasing>`_ . Upload your yaml to eevee-yamls repository `here <https://github.com/skit-ai/eevee-yamls>`_ & pass the relative path of the yaml from base of the repository.     :type alias_yaml_path: str, optional      :param initial_training: Set to true only if you're training a model for the first time, defaults to False.     :type initial_training: bool, optional      :param use_previous_dataset: Before retraining combines new dataset with last dataset the model was trained on, defaults to True.     :type use_previous_dataset: bool, optional      :param train_split_percent: Percentage of new data one should train the model on, defaults to 85.     :type train_split_percent: int, optional      :param stratify: For stratified splitting of dataset into train and test set, defaults to False.     :type stratify: bool, optional      :param notify: Whether to send a slack notification, defaults to \"\"     :type notify: str, optional      :param channel: The slack channel to send the notification, defaults to \"\"     :type channel: str, optional      :param slack_thread: The slack thread to send the notification, defaults to \"\"     :type slack_thread: str, optional # downloaded_customization_repo_op = download_repo_op( #     repo_name=customization_repo_name, # ) # downloaded_customization_repo_op.display_name = \"Download SLU customization repo\" # downloaded_customization_repo_op.execution_options.caching_strategy.max_cache_staleness = ("
  },
  {
    "id": 506,
    "repo": "biswaroop1547/skit-pipelines",
    "file_path": "skit_pipelines/pipelines/transcription_pipeline/__init__.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nA pipeline to transcribe the audio files present in a dataset using different ASRs.      .. _p_transcription_pipeline:      Example payload to invoke via slack integrations:          @charon run transcription_pipeline          .. code-block:: python              {              }      :param data_s3_path: S3 path of the data in CSV     :type data_s3_path: str     :param config_s3_path: the config yaml to be used by blaze. Refer to (https://github.com/skit-ai/blaze#config) for more info.     :type config_s3_path: str     :param audio_sample_rate: audio sample rate / frequency of output audios. (default \"8k\").     :type audio_sample_rate: str     :param audio_download_workers: maximum workers while downloading the audios (default 30).     :type audio_download_workers: int     :param transcription_concurrency: maximum workers while transcribing the audios (default 8).     :type transcription_concurrency: int # Download CSV files with audio # re-presign the s3 links present in .csv, so that they are accessible # does presigning again only if the links are expired # Download audio files from CSV # Transcribing"
  },
  {
    "id": 507,
    "repo": "Taha-Cakir/Kubeflow-Pipelines-Deployment-GCP",
    "file_path": "model-deployment-kubeflow.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# roc_auc_train_proba = roc_auc_score(y_sm_train, y_pred_train_proba[:, 1]) # roc_auc_test_proba = roc_auc_score(y_test, y_pred_proba[:, 1]) # Create an argument parser # Parse the command-line arguments"
  },
  {
    "id": 508,
    "repo": "kaiomurz/kubeflow-imdb",
    "file_path": "imdb_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nIMPORT FUNCTIONS  dsl-compile --py imdb_pipeline.py --output imdb_pipeline.yaml https://github.com/kubeflow/pipelines/tree/sdk/release-1.8/samples/core"
  },
  {
    "id": 509,
    "repo": "aakashbajaj/Retinal-OCT-Kubeflow",
    "file_path": "pipelines/e2e_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# Important Parameters on top"
  },
  {
    "id": 510,
    "repo": "ZeynepRuveyda/MLOPS_Automation_SyntheticDataCreation_and_MatsimSimulation",
    "file_path": "kubernetes_last_codes/code/pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n#git clone"
  },
  {
    "id": 511,
    "repo": "ZeynepRuveyda/MLOPS_Automation_SyntheticDataCreation_and_MatsimSimulation",
    "file_path": "kubernetes_last_codes/code/pipeline_demo.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 512,
    "repo": "ZeynepRuveyda/MLOPS_Automation_SyntheticDataCreation_and_MatsimSimulation",
    "file_path": "kubernetes_last_codes/code/pipeline_hello_world.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 513,
    "repo": "SaschaDittmann/kubeflow-azurepipeline",
    "file_path": "code/pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# preprocess data #train # register model"
  },
  {
    "id": 514,
    "repo": "Epochex/MLOps_Kubeflow_TwinStream_Pipeline",
    "file_path": "kfp_pipeline/build_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nkfp_pipeline/build_pipeline.py \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u5e38\u91cf & \u8def\u5f84 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u8ba9 Python \u627e\u5f97\u5230 components \u5305 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u5bfc\u5165\u5df2\u88c5\u9970\u597d\u7684\u7ec4\u4ef6\u51fd\u6570 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u5982\u9700\u7edf\u4e00\u955c\u50cf\uff0c\u53ef\u5728\u8fd9\u91cc\u52a8\u6001\u8986\u5199\uff08\u4efb\u9009\uff09"
  },
  {
    "id": 515,
    "repo": "Epochex/MLOps_Kubeflow_TwinStream_Pipeline",
    "file_path": "kfp_pipeline/components/split_data.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n\u7ec4\u4ef6\u529f\u80fd:     \u2022 csv_uri: \u5fc5\u987b\u662f s3://bucket/key.csv     \u2022 \u6309 ratio \u5207\u5206\u6570\u636e\uff0c\u8f93\u51fa train_csv / stream_csv      \u73af\u5883\u53d8\u91cf:     - MINIO_KEY     - MINIO_SECRET     - MINIO_ENDPOINT kfp_pipeline/components/split_data.py # 1\ufe0f\u20e3 \u8fde\u63a5 S3 (MinIO) # 2\ufe0f\u20e3 \u8bfb\u53d6\u6570\u636e # 3\ufe0f\u20e3 \u5207\u5206 & \u4fdd\u5b58"
  },
  {
    "id": 516,
    "repo": "Vishwajyoti/Pipelines",
    "file_path": "demo_kf_pipelines.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nCreated on Tue Sep  3 14:58:04 2019  @author: vispande2 Step 1: create training dataset using Apache Beam on Cloud Dataflow # image needs to be a compile-time string #,file_outputs={'bucket': '/output.txt'} Step 2: Train the model and find best set of hyperparameter. # image needs to be a compile-time string"
  },
  {
    "id": 517,
    "repo": "II-VSB-II/TaxiClassificationKubeflowPipelinePVC",
    "file_path": "Taxi-Pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nlambda x: (x['target'] > x['fare'] * 0.2) proxy=\"http://test:8080\""
  },
  {
    "id": 518,
    "repo": "seoeun25/ml-ops",
    "file_path": "distributed_training/pytorch-op/mnist/pytorch_mnist_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nReturns Kubeflow pipelines client inside cluster. # \"resources\": { #     \"requests\": { #         \"memory\": \"4Gi\", #         \"cpu\": \"2000m\", #         # Uncomment for GPU"
  },
  {
    "id": 519,
    "repo": "seoeun25/ml-ops",
    "file_path": "kubeflow_pipeline/samples/add/add.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n$ dsl-compile --py add.py --output add_pipeline.yaml Need manually upload add_pipeline.yaml on kubeflow-pipeline-ui"
  },
  {
    "id": 520,
    "repo": "seoeun25/ml-ops",
    "file_path": "kubeflow_pipeline/samples/add/add2.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n$ python add_2.py Need manually upload add_pipeline_2.yaml on kubeflow-pipeline-ui"
  },
  {
    "id": 521,
    "repo": "seoeun25/ml-ops",
    "file_path": "kubeflow_pipeline/samples/add/add_cpu.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# $ python add_cpu.py # This will upload add_cpu.yaml. Check this on kubeflow-pipeline-ui # $ python add_cpu.py This will upload add_cpu.yaml. Check this on kubeflow-pipeline-ui  # toleration1 = V1Toleration( #     effect='NoSchedule',"
  },
  {
    "id": 522,
    "repo": "seoeun25/ml-ops",
    "file_path": "kubeflow_pipeline/samples/add/addition_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# $ python addition_pipeline.py # This will upload addition_pipeline.yaml. Check this on kubeflow-pipeline-ui # $ python addition_pipeline.py This will upload addition_pipeline.yaml. Check this on kubeflow-pipeline-ui  # Passes a pipeline parameter and a constant value to the `add_op` factory # function."
  },
  {
    "id": 523,
    "repo": "seoeun25/ml-ops",
    "file_path": "kubeflow_pipeline/samples/edd_monitor/edd_checker.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nThis is pipeline to monitor whether edd data loading is successful.     $ python edd_checker.py      * create and upload edd_checker.yaml(pipeline) to kubeflow.     * check ede_monitor at kubeflow. # base_image = \"{}.dkr.ecr.{}.amazonaws.com/aladin-runtime:anaconda-cpu\".format(account, region) # Close trino connection # Close trino connection # Close trino connection # Close trino connection"
  },
  {
    "id": 524,
    "repo": "seoeun25/ml-ops",
    "file_path": "kubeflow_pipeline/samples/edd_monitor/edd_monitor.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nThis is pipeline to monitor whether edd data loading is successful.     $ python edd_monitor.py      * create and upload edd_monitor.yaml(pipeline) to kubeflow.     * check ede_monitor at kubeflow. # Close trino connection # Close trino connection # Close trino connection # Close trino connection # Close trino connection"
  },
  {
    "id": 525,
    "repo": "seoeun25/ml-ops",
    "file_path": "kubeflow_pipeline/samples/iris_python/iris_python_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nThis is kubeflow pipeline built using python sdk. We define a stand-alone python function which is converted to pipeline component.  1. create persistent volume on Kubeflow Volumes UI     name: test-data-volume 2. Execute below command inside kubeflow cluster.     $ python iris_python_pipeline.py This will create iris_python pipeline and upload it. Check this on Kubeflow Pipeline UI. ## TODO os.environ # Fetch iris dataset from trino (federated query) # Convert data to a pandas dataframe # Write iris dataset to csv file # Close trino connection"
  },
  {
    "id": 526,
    "repo": "seoeun25/ml-ops",
    "file_path": "kubeflow_pipeline/samples/test/kfp_op_test.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n$ python iris_python_pipeline.py # This will upload iris_python.yaml. Check this on kubeflow-pipeline-ui This will upload iris_python.yaml. Check this on kubeflow-pipeline-ui ## TODO # list file and directories # list file and directories"
  },
  {
    "id": 527,
    "repo": "tryster7/FuelKFP",
    "file_path": "pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nKubeflow Pipelines MNIST example  Run this script to compile pipeline Copyright 2019 Google LLC  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 528,
    "repo": "simanadler/kfp-components",
    "file_path": "samples/house_price_estimates/pipeline-argo.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# Get the namespace in which kfp is running # Get the ID of the run.  Make sure it's lower case and starts with a letter for our use # Where to store parameters passed between workflow steps # Set environment values to ensure persistent volumes used to pass parameters are allocated successfully"
  },
  {
    "id": 529,
    "repo": "simanadler/kfp-components",
    "file_path": "samples/house_price_estimates/pipeline-tekton.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# Get the namespace in which kfp is running # Get the ID of the run.  Make sure it's lower case and starts with a letter for our use # Where to store parameters passed between workflow steps # Set environment values to ensure persistent volumes used to pass parameters are allocated successfully"
  },
  {
    "id": 530,
    "repo": "cliffvj/sparta-kubeflow",
    "file_path": "boston_housing/pipeline.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 531,
    "repo": "deinal/jec-pipeline",
    "file_path": "pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# Define pipeline variables # Import pipeline components # Get pipeline instance # Compile pipeline # Load cookies to access Kubeflow at CERN"
  },
  {
    "id": 532,
    "repo": "sol-demos-01/kubeflow-ml1-demo-app",
    "file_path": "components/golang/component.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nReturns whether we are running in notebook."
  },
  {
    "id": 533,
    "repo": "sol-demos-01/kubeflow-ml1-demo-app",
    "file_path": "components/training/component.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nReturns whether we are running in notebook."
  },
  {
    "id": 534,
    "repo": "ajperry2/mapillary_pipeline",
    "file_path": "mapillary_pipeline/download_data_components.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 535,
    "repo": "ajperry2/mapillary_pipeline",
    "file_path": "mapillary_pipeline/download_data_pipeline.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 536,
    "repo": "stackdemos/kf-overlay-acm",
    "file_path": "components/golang/component.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nReturns whether we are running in notebook."
  },
  {
    "id": 537,
    "repo": "stackdemos/kf-overlay-acm",
    "file_path": "components/training/component.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nReturns whether we are running in notebook."
  },
  {
    "id": 538,
    "repo": "Shunpoco/kfp-sample",
    "file_path": "pipeline.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 539,
    "repo": "kshokn2/kfp_VertexAI",
    "file_path": "run_kfp_vertexAI.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nlabel \uc815\ubcf4 \uac00\uc838\uc624\uae30 \ub2e8\uc704 \ud14c\uc2a4\ud2b8\uc6a9 myparam1 = \"parameter1\" # packages_to_install=[\"install_library1==0.0.1\", \"install_library2==1.0.0\"] # project: str, # location: str,"
  },
  {
    "id": 540,
    "repo": "niyushabaghayi/telecom_churn_kfp",
    "file_path": "telecom_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# Given the outputs from decision_tree and logistic regression components # the results are shown. # Loads the yaml manifest for each component # Run ingestion task # Run preprocess task"
  },
  {
    "id": 541,
    "repo": "kangkannnng/Final-Project",
    "file_path": "pipeline.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 542,
    "repo": "MuhamedAyoub/kubeflow-mlops",
    "file_path": "src/main.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nCompile the pipeline to YAML and optionally run it. Create a pipeline run # Compile the pipeline"
  },
  {
    "id": 543,
    "repo": "MuhamedAyoub/kubeflow-mlops",
    "file_path": "src/test.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nCompile the pipeline to a YAML file"
  },
  {
    "id": 544,
    "repo": "MuhamedAyoub/kubeflow-mlops",
    "file_path": "src/pipelines/serving_model/serving_model.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# Validate the model exists in MinIO # Initialize the KServe client # Create the InferenceService spec using the KServe SDK # Deploy the InferenceService # Return outputs"
  },
  {
    "id": 545,
    "repo": "MuhamedAyoub/kubeflow-mlops",
    "file_path": "src/pipelines/validation_model/validation_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# Classification Report         | Class | Precision | Recall | F1-Score | Support |          | --- | --- | --- | --- | --- |         | Not Survived | {report['0']['precision']} | {report['0']['recall']} | {report['0']['f1-score']} | {report['0']['support']} |         | Survived | {report['1']['precision']} | {report['1']['recall']} | {report['1']['f1-score']} | {report['1']['support']} | # Create S3 client # Load data # Load model directly from S3 # Prepare features # Split data"
  },
  {
    "id": 546,
    "repo": "bmorphism/kfsummit19",
    "file_path": "resnet_pipeline.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 547,
    "repo": "anhdan/kfp-first-pipelines",
    "file_path": "l1-addition-pipeline/kfp_client.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nConnect to Kubeflow Pipelines cluster Specify pipeline argument values Create run for the addition pipeline created in my_pipeline.py"
  },
  {
    "id": 548,
    "repo": "anhdan/kfp-first-pipelines",
    "file_path": "l1-addition-pipeline/my_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nDefine a pipeline"
  },
  {
    "id": 549,
    "repo": "anhdan/kfp-first-pipelines",
    "file_path": "l2-tf-pipeline/kfp_client.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nConnect to Kubeflow Pipelines cluster Specify pipeline argument values Submit a pipeline run"
  },
  {
    "id": 550,
    "repo": "anhdan/kfp-first-pipelines",
    "file_path": "l2-tf-pipeline/my_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# Passes a pipeline parameter and a constant value as operation arguments. # a dsl.ContainerOp class instance. # Passes the output of the add_task and a pipeline parameter as operation # arguments. For an operation with a single return value, the output # reference is accessed using `task.output` or"
  },
  {
    "id": 551,
    "repo": "mpaul7/end-to-end-ai-pipelines-using-kubeflow",
    "file_path": "src/maikube/pipeline/feature_extraction.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 552,
    "repo": "DharmitD/mlops-cicd-pipeline",
    "file_path": "dsl-convert.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nReads the content of the given Python file. # Fix: Handling multiple outputs correctly # Store outputs with correct variable names # Fix: Store multiple outputs correctly"
  },
  {
    "id": 553,
    "repo": "DharmitD/mlops-cicd-pipeline",
    "file_path": "generated_new_pipeline.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 554,
    "repo": "abzeefly/kubeflow-local-k8s",
    "file_path": "kfp/pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nParse arguments. arguments are parsed as a global variable so they can be used in the pipeline decorator below [START load_kfp_components] load the kfp components from their yaml files [END load_kfp_components]"
  },
  {
    "id": 555,
    "repo": "stackdemos/clorox-kf",
    "file_path": "components/golang/component.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nReturns whether we are running in notebook."
  },
  {
    "id": 556,
    "repo": "stackdemos/clorox-kf",
    "file_path": "components/training/component.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nReturns whether we are running in notebook."
  },
  {
    "id": 557,
    "repo": "mhash1m/kubeflow_kfp_workflow",
    "file_path": "xgboost_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nDefine the data loading function # Load the Boston housing dataset # Split the data into training and test sets # Save to output paths Define the model training function"
  },
  {
    "id": 558,
    "repo": "rujual/telco_churn_pipeline",
    "file_path": "xgb sample pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nDelete a GCS dir recursively. Ignore errors. !/usr/bin/env python3 Copyright 2019 Google LLC  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License."
  },
  {
    "id": 559,
    "repo": "actions-marketplace-validations/f6wbl6_kubeflow-pipelines-deploy-action",
    "file_path": "example/example_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nMaking arbitrary Dataframe with specified columns and rows # make data # pipeline"
  },
  {
    "id": 560,
    "repo": "Abeshith/KubeFlow-HandsOn",
    "file_path": "pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nStep 1: Load DataSet ## Load the iris dataset ## save the dataset to a CSV file Step 2: Data Preprocessing # Debug: Check for NaN values"
  },
  {
    "id": 561,
    "repo": "bbrowning/docling-kfp-demo",
    "file_path": "docling_convert_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nSPDX-License-Identifier: Apache-2.0 # Copy some tests pdf up to the root of our output folder # Delete the rest of the docling repo, leaving only the PDFs # Split our entire directory of pdfs into n batches, where n == num_splits A Docling container built from"
  },
  {
    "id": 562,
    "repo": "PranavAI2050/Human-Activity-Classification",
    "file_path": "kubeflow_human_activity_recognition.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nImport the modules you will use For creating the pipeline For building components Type annotations for the component artifacts data ingestion and formatting"
  },
  {
    "id": 563,
    "repo": "mozilla-ai/kfp-discovery",
    "file_path": "doc-to-podcast/generate_pipeline_manual.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nConvert a document to a podcast.      This pipeline downloads a document, processes it, converts it to a script,     and finally converts the script to speech (podcast).      Args:         :param document_url: Path to the input document.         :param file_type: The file type of the input document. e.g. .html, .txt, .pdf.         :param audio_format: Output podcast file type .e.g. WAV, MP3.         :param host_name: Name of the host.         :param cohost_name: Name of the co-host.         :param host_voice_profile: Voice profile for the host.         :param cohost_voice_profile: Voice profile for the co-host.         :param text_to_text_model: The text-to-speech model to use for script writing.         :param text_to_speech_model: The text-to-speech model to use for performing the podcast."
  },
  {
    "id": 564,
    "repo": "iampatgrady/terraform-vertexai-helloworld",
    "file_path": "pipeline/components.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nA simple KFP component that takes an input string, appends a message,     logs it, and returns the new string. ./pipeline/components.py # For real MLOps, you might load data, preprocess, train, or predict here. # For this \"Hello World\", we just manipulate a string."
  },
  {
    "id": 565,
    "repo": "iampatgrady/terraform-vertexai-helloworld",
    "file_path": "pipeline/hello_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nDefines the Hello World KFP pipeline structure.     It consists of a single component that processes an input message. ./pipeline/hello_pipeline.py # Terraform will pass a value for this. # Call the component, passing the pipeline parameter to its input. # In more complex pipelines, you would chain multiple components here."
  },
  {
    "id": 566,
    "repo": "Yeshwanththota/MLOps_project5_mlflow_Dagshub_minikube_kubectl_Kubeflow_pipelines_Dockerhub",
    "file_path": "kubeflow_pipeline/mlops_pipeline.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 567,
    "repo": "yuanchi2807/kfp-codeflare",
    "file_path": "kubeflowpipeline/kfp-ray.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nCopyright 2020 kubeflow.org  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at"
  },
  {
    "id": 568,
    "repo": "AlexIoannides/kfp-component-lib",
    "file_path": "tests/test_pipelines.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nTests that components can be assembled into pipeline DAGs that compile."
  },
  {
    "id": 569,
    "repo": "MOLOZISE/MLOps_Formation",
    "file_path": "kubeflow_flowers_train_to_mlflow_pipeline_yaml.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n1. kubeflow pipeline \ud615\uc2dd\uc744 \ub530\ub984 2. mlflow\uc5d0\uc11c \ubaa8\ub378\uc774 \uc800\uc7a5\ub418\ub3c4\ub85d \ud568 3. how : inference\uc5d0\uc11c \ubaa8\ub378 \ub85c\ub529 -> \ub85c\ub529 \ubd80\ubd84\ub9cc \ucd94\ucd9c\ud558\uc5ec \ub85c\ub529 \ud6c4 \ubc14\ub85c \uc800\uc7a5\ud558\ub294 \ubc29\uc2dd 4. \uc0ac\uc804\uc791\uc5c5 : minio mlflow bucket \ub0b4 \ubaa8\ub378 \uc5c5\ub85c\ub529 -> \ubaa8\ub378 \ub85c\ub529 -> mlflow \ud615\uc2dd\uc73c\ub85c \ubaa8\ub378 \uc800\uc7a5(\ud150\uc11c\ud50c\ub85c \ucf00\ub77c\uc2a4, \ud30c\uc774\ud1a0\uce58 \ud615\uc2dd\uc744 \ud655\uc778) -> Seldon Core\ub85c \ubc30\ud3ec"
  },
  {
    "id": 570,
    "repo": "MOLOZISE/MLOps_Formation",
    "file_path": "kubeflow_mlflow_pre_trained_model_pipeline_yaml.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n1. kubeflow pipeline \ud615\uc2dd\uc744 \ub530\ub984 2. mlflow\uc5d0\uc11c \ubaa8\ub378\uc774 \uc800\uc7a5\ub418\ub3c4\ub85d \ud568 3. how : inference\uc5d0\uc11c \ubaa8\ub378 \ub85c\ub529 -> \ub85c\ub529 \ubd80\ubd84\ub9cc \ucd94\ucd9c\ud558\uc5ec \ub85c\ub529 \ud6c4 \ubc14\ub85c \uc800\uc7a5\ud558\ub294 \ubc29\uc2dd 4. \uc0ac\uc804\uc791\uc5c5 : minio mlflow bucket \ub0b4 \ubaa8\ub378 \uc5c5\ub85c\ub529 -> \ubaa8\ub378 \ub85c\ub529 -> mlflow \ud615\uc2dd\uc73c\ub85c \ubaa8\ub378 \uc800\uc7a5(\ud150\uc11c\ud50c\ub85c \ucf00\ub77c\uc2a4, \ud30c\uc774\ud1a0\uce58 \ud615\uc2dd\uc744 \ud655\uc778) -> Seldon Core\ub85c \ubc30\ud3ec"
  },
  {
    "id": 571,
    "repo": "MOLOZISE/MLOps_Formation",
    "file_path": "01_3_5_8_example/example_kubeflow_pipeline_yaml.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 572,
    "repo": "MOLOZISE/MLOps_Formation",
    "file_path": "02_Iris_example/complex_kubeflow_pipeline_yaml.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n\ud30c\uc774\ud504\ub77c\uc778 from functools import partial from kfp.components import InputPath, OutputPath, create_component_from_func  @partial("
  },
  {
    "id": 573,
    "repo": "MOLOZISE/MLOps_Formation",
    "file_path": "03_Iris_mlflow_example/mlfow_kubeflow_pipeline_yaml.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 574,
    "repo": "MOLOZISE/MLOps_Formation",
    "file_path": "04_kogpt2_mlflow_example[failed]/kogpt2_mlflow_pipeline_yaml.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n### model     | Model       |  # of params |   Type   | # of layers  | # of heads | ffn_dim | hidden_dims |      |--------------|:----:|:-------:|--------:|--------:|--------:|--------------:|     | `KoGPT2` |  125M  |  Decoder |   12     | 12      | 3072    | 768 |      ### sampling method     - greedy sampling     - max out length : 128/1,024     ## Conditional Generation ### model ### sampling method ## Conditional Generation"
  },
  {
    "id": 575,
    "repo": "MOLOZISE/MLOps_Formation",
    "file_path": "flowers/flowers_pipeline.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 576,
    "repo": "MOLOZISE/MLOps_Formation",
    "file_path": "flowers_gcp/flowers_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n!pip3 install -U kfp Function for determine deployment"
  },
  {
    "id": 577,
    "repo": "vfcarida/CI-CD-Pipeline-for-machine-learning-with-online-training-in-Kubeflow",
    "file_path": "models/sklearn_spacy_text/pipeline/pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n!/usr/bin/env python3 Copyright 2019 Google LLC. This software is provided as-is, without warranty or representation for any use or purpose. Your use of it is subject to your agreement with Google. confusion_matrix_op = components.load_component_from_url("
  },
  {
    "id": 578,
    "repo": "ateevwasalreadytaken/kfp_jupyter_notebook",
    "file_path": "kfp_nb_submit.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 579,
    "repo": "hbelmiro/kfp-test-cache-pipeline",
    "file_path": "pipeline.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 580,
    "repo": "amanknoldus/llm-dolly-v2-3b-fine-tuning-kubeflow-template",
    "file_path": "pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nDataset Processing"
  },
  {
    "id": 581,
    "repo": "nomnomnonono/ML-Pipeline-of-Paper-Category-Classification",
    "file_path": "pipeline.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 582,
    "repo": "hbelmiro/kfp-importer-minio-demo",
    "file_path": "pipeline.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 583,
    "repo": "hbelmiro/kfp-print-env-var-demo",
    "file_path": "pipeline.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 584,
    "repo": "Taring-Community/kfp-k8s-getstarted",
    "file_path": "first-pipeline/addition_pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nConnect to Kubeflow Pipelines ===================== COMPONENT ===================== First component ===================== PIPELINE ===================== Pipeline definition"
  },
  {
    "id": 585,
    "repo": "Saoussen-CH/production-ready-MLOps-on-GCP",
    "file_path": "components/tests/conftest.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nThis fixture mocks the Artifact object (and thus any derived     classes i.e Dataset, Model, etc.)     to return the URI as the path.      Unit tests set the URI of artifacts, however, KFP components use Artifact.path to     retrieve paths to files. If a URI doesn't start with gs:// or minio:// or s3://,     the path with be None. This behaviour is avoided by mocking the Artifact._get_path     method.      Args:         mocker: Used to patch the _get_path method in `kfp.dsl.Artifact`.      Returns:         None # mock the _get_path method of Artifact which is used by the property path"
  },
  {
    "id": 586,
    "repo": "Saoussen-CH/production-ready-MLOps-on-GCP",
    "file_path": "components/tests/test_lookup_model_op.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nAssert lookup_model produces expected resource name, and that list method is     called with the correct arguments. test_lookup_model.py"
  },
  {
    "id": 587,
    "repo": "Saoussen-CH/production-ready-MLOps-on-GCP",
    "file_path": "components/tests/test_model_batch_predict_op.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nTest model_batch_predict_op function for a successful batch     prediction job creation with different parameter configurations."
  },
  {
    "id": 588,
    "repo": "Saoussen-CH/production-ready-MLOps-on-GCP",
    "file_path": "components/tests/test_upload_best_model_op.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\n# Check that model lookup is performed, and no existing model is found # Check no model comparison occurs # Check model upload call # check model output URI # check evaluation import"
  },
  {
    "id": 589,
    "repo": "Saoussen-CH/production-ready-MLOps-on-GCP",
    "file_path": "components/src/components/extract_table_to_gcs_op.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nExtract a Big Query table into Google Cloud Storage. # Get the table generated on the previous component # Initiate the Big Query client to connect with the project # job_config = bq.job.ExtractJobConfig(**{}) # Submit the extract table job to store on GCS # Wait for the extract job to complete"
  },
  {
    "id": 590,
    "repo": "Saoussen-CH/production-ready-MLOps-on-GCP",
    "file_path": "components/src/components/get_custom_job_results_op.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 591,
    "repo": "Saoussen-CH/production-ready-MLOps-on-GCP",
    "file_path": "components/src/components/get_hyperparameter_tuning_results_op.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 592,
    "repo": "Saoussen-CH/production-ready-MLOps-on-GCP",
    "file_path": "components/src/components/get_training_args_dict_op.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 593,
    "repo": "Saoussen-CH/production-ready-MLOps-on-GCP",
    "file_path": "components/src/components/get_workerpool_spec_op.py",
    "prompt": "Design a Kubeflow pipeline that matches the logic of this Python code snippet."
  },
  {
    "id": 594,
    "repo": "Saoussen-CH/production-ready-MLOps-on-GCP",
    "file_path": "components/src/components/lookup_model_op.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nFetch a model given a model name (display name) and export to GCS.      Args:         model_name (str): display name of the model         location (str): location of the Google Cloud project         project (str): project id of the Google Cloud project         model (Output[Model]): a Vertex AI model         fail_on_model_not_found (bool): if set to True, raise runtime error if             model is not found      Returns:         str: Resource name of the found model. Empty string if model not found."
  },
  {
    "id": 595,
    "repo": "Saoussen-CH/production-ready-MLOps-on-GCP",
    "file_path": "components/src/components/model_batch_predict_op.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nTrigger a batch prediction job and enable monitoring.      Args:         model (Input[Model]): Input model to use for calculating predictions.         job_display_name: Name of the batch prediction job.         location (str): location of the Google Cloud project. Defaults to None.         project (str): project id of the Google Cloud project. Defaults to None.         source_uri (str): bq:// URI or a list of gcs:// URIs to read input instances.         destination_uri (str): bq:// or gs:// URI to store output predictions.         source_format (str): E.g. \"bigquery\", \"jsonl\", \"csv\". See:             https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform_v1beta1.types.BatchPredictionJob.InputConfig         destination_format (str): E.g. \"bigquery\", \"jsonl\", \"csv\". See:             https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform_v1beta1.types.BatchPredictionJob.OutputConfig         machine_type (str): Machine type.         starting_replica_count (int): Starting replica count.         max_replica_count (int): Max replicat count.         monitoring_skew_config (dict): Configuration of training-serving skew. See:             https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform_v1beta1.types.ModelMonitoringObjectiveConfig.TrainingPredictionSkewDetectionConfig         monitoring_alert_email_addresses (List[str]):             Email addresses to send alerts to (optional).         notification_channels (List[str]):             Notification channels to send alerts to (optional).             Format: projects/<project>/notificationChannels/<notification_channel>         monitoring_training_dataset (dict): Metadata of training dataset. See:             https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform_v1beta1.types.ModelMonitoringObjectiveConfig.TrainingDataset         instance_config (dict): Configuration defining how to transform batch prediction             input instances to the instances that the Model accepts. See:             https://cloud.google.com/vertex-ai/docs/reference/rest/v1beta1/projects.locations.batchPredictionJobs#instanceconfig     Returns:         OutputPath: gcp_resources for Vertex AI UI integration. # output GCP resource for Vertex AI UI integration # return GCP resource for Vertex AI UI integration"
  },
  {
    "id": 596,
    "repo": "Saoussen-CH/production-ready-MLOps-on-GCP",
    "file_path": "components/src/components/upload_best_model_op.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nArgs:         model (Model): Input challenger model.         test_data (Dataset): Test dataset used for evaluating challenger model.         vertex_model (VertexModel): Output model uploaded to Vertex AI Model Registry.         model_eval_metricsn (Metrics): Evaluation metrics of challenger model.         project (str): project id of the Google Cloud project.         location (str): location of the Google Cloud project.         pipeline_job_id (str):         model_name (str): Name of champion and challenger model in             Vertex AI Model Registry.         eval_metric (str): Metric name to compare champion and challenger on.         eval_lower_is_better (bool): Usually True for losses and             False for classification metrics.         serving_container_image (str): Container URI for serving the model.         model_description (str): Optional. Description of model.         evaluation_name (str): Optional. Name of evaluation results which are             displayed in the Vertex AI UI of the challenger model. # Output google.VertexModel artifact # Parse metrics to dict # Compare models # Look up Vertex model evaluation for champion model"
  },
  {
    "id": 597,
    "repo": "Saoussen-CH/production-ready-MLOps-on-GCP",
    "file_path": "pipelines/src/pipelines/prediction.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nPrediction pipeline which:      1. Looks up the default model version (champion).      2. Runs a batch prediction job with BigQuery as input and output      3. Optionally monitors training-serving skew      Args:         project (str): project id of the Google Cloud project         location (str): location of the Google Cloud project         bq_location (str): location of dataset in BigQuery         bq_source_uri (str): `<project>.<dataset>.<table>` of ingestion data in BigQuery         model_name (str): name of model         dataset (str): dataset id to store staging data & predictions in BigQuery         timestamp (str): Optional. Empty or a specific timestamp in ISO 8601 format             (YYYY-MM-DDThh:mm:ss.sss\u00b1hh:mm or YYYY-MM-DDThh:mm:ss).             If any time part is missing, it will be regarded as zero         use_latest_data (bool): Whether to use the latest available data         machine_type (str): Machine type to be used for Vertex Batch             Prediction. Example machine_types - n1-standard-4, n1-standard-16 etc.         min_replicas (int): Minimum no of machines to distribute the             Vertex Batch Prediction job for horizontal scalability         max_replicas (int): Maximum no of machines to distribute the             Vertex Batch Prediction job for horizontal scalability set training-serving skew thresholds and emails to receive alerts: or set different thresholds per feature: SKEW_THRESHOLDS = {\"skewThresholds\": {\"payment_type\": {\"value\": 0.001}}, ... } # lookup champion model # batch predict from BigQuery to BigQuery"
  },
  {
    "id": 598,
    "repo": "Saoussen-CH/production-ready-MLOps-on-GCP",
    "file_path": "pipelines/src/pipelines/training.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nTraining pipeline which:      1. Preprocesses data in BigQuery      2. Extracts data to Cloud Storage      3. Trains a model using a custom prebuilt container      4. Uploads the model to Model Registry      5. Evaluates the model against a champion model      6. Selects a new champion based on the primary metrics      Args:         project (str): project id of the Google Cloud project         location (str): location of the Google Cloud project         bq_location (str): location of dataset in BigQuery         bq_source_uri (str): `<project>.<dataset>.<table>` of ingestion data in BigQuery         model_name (str): name of model         dataset (str): dataset id to store staging data & predictions in BigQuery         timestamp (str): Optional. Empty or a specific timestamp in ISO 8601 format             (YYYY-MM-DDThh:mm:ss.sss\u00b1hh:mm or YYYY-MM-DDThh:mm:ss).             If any time part is missing, it will be regarded as zero.         use_latest_data (bool): Whether to use the latest available data         base_output_dir (str): base output directory for the training job         training_job_display_name (str): display name for the training job         model_name (str): name of the model define the metric spec for hyperparameter tuning for details: https://cloud.google.com/vertex-ai/docs/reference/rest/v1/StudySpec#MetricSpec define the parameter specs for tuning for details:"
  },
  {
    "id": 599,
    "repo": "NashTech-Labs/Databricks-Dolly-LLM-Fine-Tuning",
    "file_path": "pipeline.py",
    "prompt": "You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:\n\nDataset Processing"
  }
]