# kfp_pipeline/build_pipeline.py
import pathlib, sys, kfp
from kfp import dsl

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ å¸¸é‡ & è·¯å¾„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
ROOT  = pathlib.Path(__file__).resolve().parents[1]
COMP  = ROOT / "kfp_pipeline" / "components"
IMAGE = "hirschazer/flux_demo5:latest"      # ä½ çš„ä¸šåŠ¡é•œåƒ

# è®© Python æ‰¾å¾—åˆ° components åŒ…
sys.path.append(str(COMP.parent))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ å¯¼å…¥å·²è£…é¥°å¥½çš„ç»„ä»¶å‡½æ•° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
from components.split_data          import split_data
from components.offline_train       import offline_train
from components.launch_katib        import launch_katib
from components.apply_k8s_resource  import apply_k8s_resource

# å¦‚éœ€ç»Ÿä¸€é•œåƒï¼Œå¯åœ¨è¿™é‡ŒåŠ¨æ€è¦†å†™ï¼ˆä»»é€‰ï¼‰
for c in (split_data, offline_train, launch_katib):
    c.component_spec.implementation.container.image = IMAGE

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ DAG å®šä¹‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
@dsl.pipeline(
    name="twin-stream-pipeline",
    description="Katib HPO â†’ Batch è®­ç»ƒ â†’ Streaming Job"
)
def pipeline(
    csv_uri: str = "s3://katib-flux-demo/datasets/load_stimulus_global.csv",
):
    # â‘  40/60 åˆ‡åˆ†ï¼Œè‡ªåŠ¨æŠŠ train/stream.csv ä¸Šä¼ å›åŒ Bucket
    split = split_data(csv_uri=csv_uri)

    # â‘¡ Katib éšæœºæœç´¢
    hpo = launch_katib(
        train_image = IMAGE,
        train_csv   = split.outputs["train_csv"],
        out_dir     = "/mnt/data",
    )

    # â‘¢ ç¦»çº¿ Batch è®­ç»ƒ
    train = offline_train(
        csv       = split.outputs["train_csv"],
        model_key = "models/ann_batch_model.pth",
    ).after(hpo)

    # â‘£ kubectl apply Producer + Consumer Job
    apply_k8s_resource(
        yaml_path = str(COMP / "stream_job.yaml"),
    ).after(train)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ç¼–è¯‘ + å¯é€‰ï¼šè‡ªåŠ¨è§¦å‘ä¸€æ¬¡ Run â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
if __name__ == "__main__":
    pkg = str(ROOT / "pipeline.json")
    kfp.compiler.Compiler().compile(
        pipeline_func = pipeline,
        package_path  = pkg,
    )
    print("âœ…  pipeline.json generated")

    # ----- è‡ªåŠ¨è§¦å‘ï¼ˆå¯åˆ æ‰ï¼‰ -----
    from kfp import Client
    client = Client(host="http://ml-pipeline.kubeflow:8888")  # in-cluster å¯çœ host
    run = client.create_run_from_pipeline_package(
        pipeline_file   = pkg,
        arguments       = {"csv_uri": "s3://katib-flux-demo/datasets/load_stimulus_global.csv"},
        experiment_name = "twin-stream",
        run_name        = "twin-stream-auto",
    )
    print("ğŸš€  triggered run:", run.run_id)


