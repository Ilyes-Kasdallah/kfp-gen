You are an MLOps engineer. Write a Kubeflow pipeline based on the following description and code logic:

Prediction pipeline which:      1. Looks up the default model version (champion).      2. Runs a batch prediction job with BigQuery as input and output      3. Optionally monitors training-serving skew      Args:         project (str): project id of the Google Cloud project         location (str): location of the Google Cloud project         bq_location (str): location of dataset in BigQuery         bq_source_uri (str): `<project>.<dataset>.<table>` of ingestion data in BigQuery         model_name (str): name of model         dataset (str): dataset id to store staging data & predictions in BigQuery         timestamp (str): Optional. Empty or a specific timestamp in ISO 8601 format             (YYYY-MM-DDThh:mm:ss.sssÂ±hh:mm or YYYY-MM-DDThh:mm:ss).             If any time part is missing, it will be regarded as zero         use_latest_data (bool): Whether to use the latest available data         machine_type (str): Machine type to be used for Vertex Batch             Prediction. Example machine_types - n1-standard-4, n1-standard-16 etc.         min_replicas (int): Minimum no of machines to distribute the             Vertex Batch Prediction job for horizontal scalability         max_replicas (int): Maximum no of machines to distribute the             Vertex Batch Prediction job for horizontal scalability set training-serving skew thresholds and emails to receive alerts: or set different thresholds per feature: SKEW_THRESHOLDS = {"skewThresholds": {"payment_type": {"value": 0.001}}, ... } # lookup champion model # batch predict from BigQuery to BigQuery