[
  {
    "repo": "kubeflow/pipelines",
    "file_path": "components/snowflake/snowflake_unload_data.py",
    "raw_url": "https://raw.githubusercontent.com/kubeflow/pipelines/master/components/snowflake/snowflake_unload_data.py",
    "content": "\"\"\"\nThis is a KFP component doing \"unload data to GCS bucket\" operation\n  from the Snowflake database.\n\"\"\"\nfrom kfp import compiler\nfrom kfp.dsl import component\n\n@component(\n    base_image=\"python:3.11\",\n    packages_to_install=[\"snowflake-connector-python:3.12.3\"]\n)\ndef snowflake_unload_op(\n    output_gcs_path: str,\n    sf_storage_integration: str,\n    query: str,\n    sf_user: str,\n    sf_password: str,\n    sf_account: str,\n    sf_warehouse: str,\n    sf_database: str\n    ) -> str:\n    \"\"\"\n    Run COPY in snowflake to unload data to GCS bucket.\n\n    output_gcs_path: the location to land the data\n    sf_storage_integration: the Snowflake Storage Integration name\n    query: the query to execute in the COPY command\n    sf_user: the snowflake username\n    sf_password: the snowflake password\n    sf_warehouse: the snowflake warehouse name\n    sf_database: the database to use\n    \"\"\"\n    import snowflake.connector\n    conn = snowflake.connector.connect(user=sf_user,\n                                       password=sf_password,\n                                       account=sf_account,\n                                       role=\"ACCOUNTADMIN\")\n\n    conn.cursor().execute(f\"USE WAREHOUSE {sf_warehouse};\")\n    conn.cursor().execute(f\"USE DATABASE {sf_database};\")\n    result = conn.cursor().execute(f\"\"\"\n    COPY INTO 'gcs://{output_gcs_path}'\n    FROM ({query})\n    FILE_FORMAT = (TYPE = CSV COMPRESSION=NONE)\n    STORAGE_INTEGRATION = {sf_storage_integration}\n    HEADER = TRUE\n    \"\"\")\n    _ = result.fetchall()\n    if output_gcs_path.endswith(\"/\"):\n        return output_gcs_path + \"data_0_0_0.csv\"\n    else:\n        return output_gcs_path\n\nif __name__ == \"__main__\":\n    compiler.Compiler().compile(pipeline_func=snowflake_unload_op,\n                                package_path=\"component.yaml\")\n"
  },
  {
    "repo": "kubeflow/pipelines",
    "file_path": "samples/test/after.py",
    "raw_url": "https://raw.githubusercontent.com/kubeflow/pipelines/master/samples/test/after.py",
    "content": "# Copyright 2020 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom kfp import components\nfrom kfp import dsl\nfrom kfp import compiler\n\ncomponent_op = components.load_component_from_text(\"\"\"\nname: Print Text\ninputs:\n- {name: text, type: String}\nimplementation:\n  container:\n    image: alpine\n    command:\n    - sh\n    - -c\n    - |\n      set -e -x\n      echo \"$0\"\n    - {inputValue: text}\n\"\"\")\n\n\n@dsl.pipeline(name='pipeline-with-after')\ndef my_pipeline():\n    task1 = component_op(text='1st task')\n    task2 = component_op(text='2nd task').after(task1)\n    task3 = component_op(text='3rd task').after(task1, task2)\n\n\nif __name__ == '__main__':\n    compiler.Compiler().compile(\n        pipeline_func=my_pipeline, output_path=__file__ + '.json')\n"
  },
  {
    "repo": "kubeflow/pipelines",
    "file_path": "samples/test/fail_v2.py",
    "raw_url": "https://raw.githubusercontent.com/kubeflow/pipelines/master/samples/test/fail_v2.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Fail pipeline.\"\"\"\n\nfrom kfp import dsl\n\n\n@dsl.component\ndef fail():\n    '''Fails'''\n    import sys\n    sys.exit(1)\n\n\n@dsl.pipeline(name='fail-pipeline')\ndef fail_pipeline():\n    fail_task = fail()\n"
  },
  {
    "repo": "kubeflow/pipelines",
    "file_path": "samples/test/lightweight_python_functions_v2_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/kubeflow/pipelines/master/samples/test/lightweight_python_functions_v2_pipeline.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Sample pipeline for passing data in KFP v2.\"\"\"\nfrom typing import Dict, List\nimport os\n\nfrom kfp import dsl\nfrom kfp.dsl import Input, InputPath, Output, OutputPath, Dataset, Model, component\nimport kfp.compiler as compiler\n\n# In tests, we install a KFP package from the PR under test. Users should not\n# normally need to specify `kfp_package_path` in their component definitions.\n_KFP_PACKAGE_PATH = os.getenv('KFP_PACKAGE_PATH')\n\n\n@component(kfp_package_path=_KFP_PACKAGE_PATH)\ndef preprocess(\n    # An input parameter of type string.\n    message: str,\n    # Use Output[T] to get a metadata-rich handle to the output artifact\n    # of type `Dataset`.\n    output_dataset_one: Output[Dataset],\n    # A locally accessible filepath for another output artifact of type\n    # `Dataset`.\n    output_dataset_two_path: OutputPath('Dataset'),\n    # A locally accessible filepath for an output parameter of type string.\n    output_parameter_path: OutputPath(str),\n    # A locally accessible filepath for an output parameter of type bool.\n    output_bool_parameter_path: OutputPath(bool),\n    # A locally accessible filepath for an output parameter of type dict.\n    output_dict_parameter_path: OutputPath(Dict[str, int]),\n    # A locally accessible filepath for an output parameter of type list.\n    output_list_parameter_path: OutputPath(List[str]),\n    # An input message that defaults to empty.\n    empty_message: str = \"\",\n):\n    \"\"\"Dummy preprocessing step\"\"\"\n\n    # Use Dataset.path to access a local file path for writing.\n    # One can also use Dataset.uri to access the actual URI file path.\n    with open(output_dataset_one.path, 'w') as f:\n        f.write(message)\n\n    # OutputPath is used to just pass the local file path of the output artifact\n    # to the function.\n    with open(output_dataset_two_path, 'w') as f:\n        f.write(message)\n\n    with open(output_parameter_path, 'w') as f:\n        f.write(message)\n\n    with open(output_bool_parameter_path, 'w') as f:\n        f.write(\n            str(True)\n        )  # use either `str()` or `json.dumps()` for bool values.\n\n    import json\n    with open(output_dict_parameter_path, 'w') as f:\n        f.write(json.dumps({'A': 1, 'B': 2}))\n\n    with open(output_list_parameter_path, 'w') as f:\n        f.write(json.dumps(['a', 'b', 'c']))\n\n\n@component(kfp_package_path=_KFP_PACKAGE_PATH)\ndef train(\n    # Use InputPath to get a locally accessible path for the input artifact\n    # of type `Dataset`.\n    dataset_one_path: InputPath('Dataset'),\n    # Use Input[T] to get a metadata-rich handle to the input artifact\n    # of type `Dataset`.\n    dataset_two: Input[Dataset],\n    # An input parameter of type string.\n    message: str,\n    # Use Output[T] to get a metadata-rich handle to the output artifact\n    # of type `Dataset`.\n    model: Output[Model],\n    # An input parameter of type bool.\n    input_bool: bool,\n    # An input parameter of type dict.\n    input_dict: Dict[str, int],\n    # An input parameter of type List[str].\n    input_list: List[str],\n    # An input parameter of type int with a default value.\n    num_steps: int = 100,\n):\n    \"\"\"Dummy Training step\"\"\"\n    with open(dataset_one_path, 'r') as input_file:\n        dataset_one_contents = input_file.read()\n\n    with open(dataset_two.path, 'r') as input_file:\n        dataset_two_contents = input_file.read()\n\n    line = (\n        f'dataset_one_contents: {dataset_one_contents} || '\n        f'dataset_two_contents: {dataset_two_contents} || '\n        f'message: {message} || '\n        f'input_bool: {input_bool}, type {type(input_bool)} || '\n        f'input_dict: {input_dict}, type {type(input_dict)} || '\n        f'input_list: {input_list}, type {type(input_list)} \\n'\n    )\n\n    with open(model.path, 'w') as output_file:\n        for i in range(num_steps):\n            output_file.write('Step {}\\n{}\\n=====\\n'.format(i, line))\n\n    # model is an instance of Model artifact, which has a .metadata dictionary\n    # to store arbitrary metadata for the output artifact.\n    model.metadata['accuracy'] = 0.9\n\n\n@dsl.pipeline(name='my-test-pipeline-beta')\ndef pipeline(message: str = 'message'):\n    preprocess_task = preprocess(message=message)\n    train_task = train(\n        dataset_one_path=preprocess_task.outputs['output_dataset_one'],\n        dataset_two=preprocess_task.outputs['output_dataset_two_path'],\n        message=preprocess_task.outputs['output_parameter_path'],\n        input_bool=preprocess_task.outputs['output_bool_parameter_path'],\n        input_dict=preprocess_task.outputs['output_dict_parameter_path'],\n        input_list=preprocess_task.outputs['output_list_parameter_path'],\n    )\n\n\nif __name__ == '__main__':\n    compiler.Compiler().compile(\n        pipeline_func=pipeline, package_path=__file__.replace('.py', '.yaml')\n    )\n"
  },
  {
    "repo": "kubeflow/pipelines",
    "file_path": "samples/test/lightweight_python_functions_v2_with_outputs.py",
    "raw_url": "https://raw.githubusercontent.com/kubeflow/pipelines/master/samples/test/lightweight_python_functions_v2_with_outputs.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Lightweight functions v2 with outputs.\"\"\"\nfrom typing import NamedTuple\nimport os\n\nfrom kfp import compiler, dsl\nfrom kfp.dsl import Input, Dataset, Model, Metrics, component\n\n# In tests, we install a KFP package from the PR under test. Users should not\n# normally need to specify `kfp_package_path` in their component definitions.\n_KFP_PACKAGE_PATH = os.getenv('KFP_PACKAGE_PATH')\n\n\n@component(kfp_package_path=_KFP_PACKAGE_PATH)\ndef concat_message(first: str, second: str) -> str:\n    return first + second\n\n\n@component(kfp_package_path=_KFP_PACKAGE_PATH)\ndef add_numbers(first: int, second: int) -> int:\n    return first + second\n\n\n@component(kfp_package_path=_KFP_PACKAGE_PATH)\ndef output_artifact(number: int, message: str) -> Dataset:\n    result = [message for _ in range(number)]\n    return '\\n'.join(result)\n\n\n@component(kfp_package_path=_KFP_PACKAGE_PATH)\ndef output_named_tuple(\n    artifact: Input[Dataset]\n) -> NamedTuple('Outputs', [\n    ('scalar', str),\n    ('metrics', Metrics),\n    ('model', Model),\n]):\n    scalar = \"123\"\n\n    import json\n    metrics = json.dumps({\n        'metrics': [{\n            'name': 'accuracy',\n            'numberValue': 0.9,\n            'format': \"PERCENTAGE\",\n        }]\n    })\n\n    with open(artifact.path, 'r') as f:\n        artifact_contents = f.read()\n    model = \"Model contents: \" + artifact_contents\n\n    from collections import namedtuple\n    output = namedtuple('Outputs', ['scalar', 'metrics', 'model'])\n    return output(scalar, metrics, model)\n\n\n@dsl.pipeline(name='functions-with-outputs')\ndef pipeline(\n    first_message: str = 'first',\n    second_message: str = 'second',\n    first_number: int = 1,\n    second_number: int = 2,\n):\n    concat_task = concat_message(first=first_message, second=second_message)\n    add_numbers_task = add_numbers(first=first_number, second=second_number)\n    output_artifact_task = output_artifact(\n        number=add_numbers_task.output, message=concat_task.output)\n    output_name_tuple = output_named_tuple(artifact=output_artifact_task.output)\n\n\nif __name__ == '__main__':\n    compiler.Compiler().compile(\n        pipeline_func=pipeline, package_path=__file__ + '.yaml')\n"
  },
  {
    "repo": "kubeflow/pipelines",
    "file_path": "samples/test/metrics_visualization_v2.py",
    "raw_url": "https://raw.githubusercontent.com/kubeflow/pipelines/master/samples/test/metrics_visualization_v2.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport os\n\nfrom kfp import dsl\nfrom kfp.dsl import (component, Output, ClassificationMetrics, Metrics, HTML,\n                     Markdown)\n\n# In tests, we install a KFP package from the PR under test. Users should not\n# normally need to specify `kfp_package_path` in their component definitions.\n_KFP_PACKAGE_PATH = os.getenv('KFP_PACKAGE_PATH')\n\n\n@component(\n    packages_to_install=['scikit-learn'],\n    base_image='python:3.9',\n    kfp_package_path=_KFP_PACKAGE_PATH,\n)\ndef digit_classification(metrics: Output[Metrics]):\n    from sklearn import model_selection\n    from sklearn.linear_model import LogisticRegression\n    from sklearn import datasets\n    from sklearn.metrics import accuracy_score\n\n    # Load digits dataset\n    iris = datasets.load_iris()\n\n    # # Create feature matrix\n    X = iris.data\n\n    # Create target vector\n    y = iris.target\n\n    #test size\n    test_size = 0.33\n\n    seed = 7\n    #cross-validation settings\n    kfold = model_selection.KFold(n_splits=10, random_state=seed, shuffle=True)\n\n    #Model instance\n    model = LogisticRegression()\n    scoring = 'accuracy'\n    results = model_selection.cross_val_score(\n        model, X, y, cv=kfold, scoring=scoring)\n\n    #split data\n    X_train, X_test, y_train, y_test = model_selection.train_test_split(\n        X, y, test_size=test_size, random_state=seed)\n    #fit model\n    model.fit(X_train, y_train)\n\n    #accuracy on test set\n    result = model.score(X_test, y_test)\n    metrics.log_metric('accuracy', (result * 100.0))\n\n\n@component(\n    packages_to_install=['scikit-learn'],\n    base_image='python:3.9',\n    kfp_package_path=_KFP_PACKAGE_PATH,\n)\ndef wine_classification(metrics: Output[ClassificationMetrics]):\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.metrics import roc_curve\n    from sklearn.datasets import load_wine\n    from sklearn.model_selection import train_test_split, cross_val_predict\n\n    X, y = load_wine(return_X_y=True)\n    # Binary classification problem for label 1.\n    y = y == 1\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n    rfc = RandomForestClassifier(n_estimators=10, random_state=42)\n    rfc.fit(X_train, y_train)\n    y_scores = cross_val_predict(\n        rfc, X_train, y_train, cv=3, method='predict_proba')\n    y_predict = cross_val_predict(rfc, X_train, y_train, cv=3, method='predict')\n    fpr, tpr, thresholds = roc_curve(\n        y_true=y_train, y_score=y_scores[:, 1], pos_label=True)\n    \n    # avoid inf thresholds\n    epsilon = 1e-6\n    thresholds = [1 - epsilon if t == float('inf') else t for t in thresholds]\n    \n    metrics.log_roc_curve(fpr, tpr, thresholds)\n\n\n@component(\n    packages_to_install=['scikit-learn'],\n    base_image='python:3.9',\n    kfp_package_path=_KFP_PACKAGE_PATH,\n)\ndef iris_sgdclassifier(test_samples_fraction: float,\n                       metrics: Output[ClassificationMetrics]):\n    from sklearn import datasets, model_selection\n    from sklearn.linear_model import SGDClassifier\n    from sklearn.metrics import confusion_matrix\n\n    iris_dataset = datasets.load_iris()\n    train_x, test_x, train_y, test_y = model_selection.train_test_split(\n        iris_dataset['data'],\n        iris_dataset['target'],\n        test_size=test_samples_fraction)\n\n    classifier = SGDClassifier()\n    classifier.fit(train_x, train_y)\n    predictions = model_selection.cross_val_predict(\n        classifier, train_x, train_y, cv=3)\n    metrics.log_confusion_matrix(\n        ['Setosa', 'Versicolour', 'Virginica'],\n        confusion_matrix(\n            train_y,\n            predictions).tolist()  # .tolist() to convert np array to list.\n    )\n\n\n@component(\n    kfp_package_path=_KFP_PACKAGE_PATH,)\ndef html_visualization(html_artifact: Output[HTML]):\n    html_content = '<!DOCTYPE html><html><body><h1>Hello world</h1></body></html>'\n    with open(html_artifact.path, 'w') as f:\n        f.write(html_content)\n\n\n@component(\n    kfp_package_path=_KFP_PACKAGE_PATH,)\ndef markdown_visualization(markdown_artifact: Output[Markdown]):\n    markdown_content = '## Hello world \\n\\n Markdown content'\n    with open(markdown_artifact.path, 'w') as f:\n        f.write(markdown_content)\n\n\n@dsl.pipeline(name='metrics-visualization-pipeline')\ndef metrics_visualization_pipeline():\n    wine_classification_op = wine_classification()\n    iris_sgdclassifier_op = iris_sgdclassifier(test_samples_fraction=0.3)\n    digit_classification_op = digit_classification()\n    html_visualization_op = html_visualization()\n    markdown_visualization_op = markdown_visualization()\n"
  },
  {
    "repo": "kubeflow/pipelines",
    "file_path": "samples/test/placeholder_concat.py",
    "raw_url": "https://raw.githubusercontent.com/kubeflow/pipelines/master/samples/test/placeholder_concat.py",
    "content": "# Copyright 2020 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom kfp import components\nfrom kfp import dsl\nimport kfp.compiler as compiler\n\ncomponent_op = components.load_component_from_text(\"\"\"\nname: Component with concat placeholder\ninputs:\n- {name: input_one, type: String}\n- {name: input_two, type: String}\nimplementation:\n  container:\n    image: registry.k8s.io/busybox\n    command:\n    - sh\n    - -ec\n    args:\n    - echo \"$0\" > /tmp/test && [[ \"$0\" == 'one+two=three' ]]\n    - concat: [{inputValue: input_one}, '+', {inputValue: input_two}, '=three']\n\"\"\")\n\n\n@dsl.pipeline(name='one-step-pipeline-with-concat-placeholder')\ndef pipeline_with_concat_placeholder():\n    component = component_op(input_one='one', input_two='two')\n"
  },
  {
    "repo": "kubeflow/pipelines",
    "file_path": "samples/test/placeholder_if_v2.py",
    "raw_url": "https://raw.githubusercontent.com/kubeflow/pipelines/master/samples/test/placeholder_if_v2.py",
    "content": "# Copyright 2020,2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom kfp import dsl, components\n\ncomponent_op = components.load_component_from_text('''\nname: Component with optional inputs\ninputs:\n- {name: required_input, type: String, optional: false}\n- {name: optional_input_1, type: String, optional: true}\n- {name: optional_input_2, type: String, optional: true}\nimplementation:\n  container:\n    image: registry.k8s.io/busybox\n    command:\n    - echo\n    args:\n    - --arg0\n    - {inputValue: required_input}\n    - if:\n        cond:\n          isPresent: optional_input_1\n        then:\n          - --arg1\n          - {inputValue: optional_input_1}\n    - if:\n        cond:\n          isPresent: optional_input_2\n        then:\n          - --arg2\n          - {inputValue: optional_input_2}\n        else:\n          - --arg2\n          - 'default value'\n''')\n\n\n@dsl.pipeline(name='one-step-pipeline-with-if-placeholder-supply-both')\ndef pipeline_both(input0: str = 'input0',\n                  input1: str = 'input1',\n                  input2: str = 'input2'):\n    # supply both optional_input_1 and optional_input_2\n    component = component_op(\n        required_input=input0, optional_input_1=input1, optional_input_2=input2)\n\n\n@dsl.pipeline(name='one-step-pipeline-with-if-placeholder-supply-none')\ndef pipeline_none(input0: str = 'input0'):\n    # supply neither optional_input_1 nor optional_input_2\n    # Note, KFP only supports compile-time optional arguments, e.g. it's not\n    # supported to write a pipeline that supplies both inputs and pass None\n    # at runtime -- in that case, the input arguments will be interpreted as\n    # the raw text \"None\".\n    component = component_op(required_input=input0)\n"
  },
  {
    "repo": "kubeflow/pipelines",
    "file_path": "samples/test/two_step_with_uri_placeholder.py",
    "raw_url": "https://raw.githubusercontent.com/kubeflow/pipelines/master/samples/test/two_step_with_uri_placeholder.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Two step v2-compatible pipeline with URI placeholders.\"\"\"\nfrom kfp import components, dsl\n\nwrite_to_gcs_op = components.load_component_from_text(\"\"\"\nname: write-to-gcs\ninputs:\n- {name: msg, type: String, description: 'Content to be written to GCS'}\noutputs:\n- {name: artifact, type: Artifact, description: 'GCS file path'}\nimplementation:\n  container:\n    image: google/cloud-sdk:slim\n    command:\n    - sh\n    - -c\n    - |\n      set -e -x\n      echo \"$0\" | gsutil cp - \"$1\"\n    - {inputValue: msg}\n    - {outputUri: artifact}\n\"\"\")\n\nread_from_gcs_op = components.load_component_from_text(\"\"\"\nname: read-from-gcs\ninputs:\n- {name: artifact, type: Artifact, description: 'GCS file path'}\nimplementation:\n  container:\n    image: google/cloud-sdk:slim\n    command:\n    - sh\n    - -c\n    - |\n      set -e -x\n      gsutil cat \"$0\"\n    - {inputUri: artifact}\n\"\"\")\n\n\n@dsl.pipeline(name='two-step-with-uri-placeholders')\ndef two_step_with_uri_placeholder(msg: str = 'Hello world!'):\n    write_to_gcs = write_to_gcs_op(msg=msg)\n    read_from_gcs = read_from_gcs_op(artifact=write_to_gcs.outputs['artifact'])\n"
  },
  {
    "repo": "kubeflow/pipelines",
    "file_path": "samples/v2/collected_artifacts.py",
    "raw_url": "https://raw.githubusercontent.com/kubeflow/pipelines/master/samples/v2/collected_artifacts.py",
    "content": "from typing import List\n\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import Artifact, Dataset, Model\nfrom kfp.dsl import Output\n\n# This sample pipeline is meant to cover the following cases during artifact resolution:\n# 1. ParallelFor task consuming input from another task within the same loop.\n# 2. A nested ParallelFor task consuming input from another ParallelFor Iterator.\n# 3. Resolving input with dsl.Collected inside another ParallelFor loop.\n# 4. Resolving input that comes from a subdag using dsl.Collected inside of a ParallelFor loop.\n# 5. Returning a dsl.Collected, loop-nested, task as subdag's output.\n# 6. Feeding in the subdag's collected output as input for a downstream task.\n\n@dsl.component()\ndef split_ids(model_ids: str) -> list:\n    return model_ids.split(',')\n\n\n@dsl.component()\ndef create_file(file: Output[Artifact], content: str):\n    print(f'Creating file with content: {content}')\n    with open(file.path, 'w') as f:\n        f.write(content)\n\n\n@dsl.component()\ndef read_files(files: List[Artifact]) -> str:\n    for f in files:\n        print(f'Reading artifact {f.name} file: {f.path}')\n        with open(f.path, 'r') as f:\n            print(f.read())\n\n    return 'files read'\n\n\n@dsl.component()\ndef read_single_file(file: Artifact) -> str:\n    print(f'Reading file: {file.path}')\n    with open(file.path, 'r') as f:\n        print(f.read())\n\n    return file.uri\n\n@dsl.component()\ndef split_chars(model_ids: str) -> list:\n    return model_ids.split(',')\n\n\n@dsl.component()\ndef create_dataset(data: Output[Dataset], content: str):\n    print(f'Creating file with content: {content}')\n    with open(data.path, 'w') as f:\n        f.write(content)\n\n\n@dsl.component()\ndef read_datasets(data: List[Dataset]) -> str:\n    for d in data:\n        print(f'Reading dataset {d.name} file: {d.path}')\n        with open(d.path, 'r') as f:\n            print(f.read())\n\n    return 'files read'\n\n\n@dsl.component()\ndef read_single_dataset_generate_model(data: Dataset, id: str, results:Output[Model]):\n    print(f'Reading file: {data.path}')\n    with open(data.path, 'r') as f:\n        info = f.read()\n        with open(results.path, 'w') as f2:\n            f2.write(f\"{info}-{id}\")\n            results.metadata['model'] = info\n            results.metadata['model_name'] = f\"model-artifact-inner-iteration-{info}-{id}\"\n\n\n@dsl.component()\ndef read_models(models: List[Model],) -> str:\n    for m in models:\n        print(f'Reading model {m.name} file: {m.path}')\n        with open(m.path, 'r') as f:\n            info = f.read()\n            print(f\"Model raw data: {info}\")\n            print(f\"Model metadata: {m.metadata}\")\n    return 'models read'\n    \n@dsl.pipeline()\ndef single_node_dag(char:str)-> Dataset:\n    create_dataset_op = create_dataset(content=char)\n    create_dataset_op.set_caching_options(False)\n    return create_dataset_op.outputs[\"data\"]\n\n@dsl.pipeline()\ndef collecting_artifacts(model_ids: str = '', model_chars: str = '') -> List[Model]:\n    ids_split_op = split_ids(model_ids=model_ids)\n    ids_split_op.set_caching_options(False)\n\n    char_split_op = split_chars(model_ids=model_chars)\n    char_split_op.set_caching_options(False)\n    \n    with dsl.ParallelFor(ids_split_op.output) as model_id:\n        create_file_op = create_file(content=model_id)\n        create_file_op.set_caching_options(False)\n\n        read_single_file_op = read_single_file(\n            file=create_file_op.outputs['file'])\n        read_single_file_op.set_caching_options(False)\n\n        with dsl.ParallelFor(char_split_op.output) as model_char:\n            single_dag_op = single_node_dag(char=model_char)\n            single_dag_op.set_caching_options(False)\n\n            random_subdag_op = read_single_dataset_generate_model_op = read_single_dataset_generate_model(data=single_dag_op.output, id=model_id)\n            read_single_dataset_generate_model_op.set_caching_options(False)\n        \n        read_models_op = read_models(\n            models=dsl.Collected(read_single_dataset_generate_model_op.outputs['results']))\n        read_models_op.set_caching_options(False)\n\n        read_datasets_op = read_datasets(\n            data=dsl.Collected(single_dag_op.output))\n        read_datasets_op.set_caching_options(False)\n    \n    return dsl.Collected(read_single_dataset_generate_model_op.outputs[\"results\"])\n\n\n@dsl.pipeline()\ndef collected_artifact_pipeline():\n    model_ids = 's1,s2,s3'\n    model_chars = 'x,y,z'\n    dag = collecting_artifacts(model_ids=model_ids, model_chars=model_chars)\n    dag.set_caching_options(False)\n    read_files_op = read_models(models=dag.output)\n    read_files_op.set_caching_options(False)\n\n\nif __name__ == '__main__':\n    client = kfp.Client()\n    run = client.create_run_from_pipeline_func(\n        collected_artifact_pipeline,\n        arguments={},\n        enable_caching=False,\n    )\n"
  },
  {
    "repo": "kubeflow/pipelines",
    "file_path": "samples/v2/collected_parameters.py",
    "raw_url": "https://raw.githubusercontent.com/kubeflow/pipelines/master/samples/v2/collected_parameters.py",
    "content": "from typing import List\n\nimport kfp\nfrom kfp import dsl\n\n\n@dsl.component()\ndef split_ids(ids: str) -> list:\n    return ids.split(',')\n\n\n@dsl.component()\ndef prepend_id(content: str) -> str:\n    print(f\"prepending: {content} with 'model_id'\")\n    return f'model_id_{content}'\n\n\n@dsl.component()\ndef consume_ids(ids: List[str]) -> str:\n    for id in ids:\n        print(f'Consuming: {id}')\n    return 'completed'\n\n\n@dsl.component()\ndef consume_single_id(id: str) -> str:\n    print(f'Consuming single: {id}')\n    return 'completed'\n\n\n@dsl.pipeline()\ndef collecting_parameters(model_ids: str = '',) -> List[str]:\n    ids_split_op = split_ids(ids=model_ids)\n    with dsl.ParallelFor(ids_split_op.output) as model_id:\n        prepend_id_op = prepend_id(content=model_id)\n        prepend_id_op.set_caching_options(False)\n        \n        consume_single_id_op = consume_single_id(id=prepend_id_op.output)\n        consume_single_id_op.set_caching_options(False)\n    \n    consume_ids_op = consume_ids(ids=dsl.Collected(prepend_id_op.output))\n    consume_ids_op.set_caching_options(False)\n\n    return dsl.Collected(prepend_id_op.output)\n\n\n@dsl.pipeline()\ndef collected_param_pipeline():\n    model_ids = 's1,s2,s3'\n    dag = collecting_parameters(model_ids=model_ids)\n    dag.set_caching_options(False)\n    \n    consume_ids_op = consume_ids(ids=dag.output)\n    consume_ids_op.set_caching_options(False)\n\nif __name__ == '__main__':\n    client = kfp.Client()\n    run = client.create_run_from_pipeline_func(\n        collected_param_pipeline,\n        arguments={},\n        enable_caching=False,\n    )\n"
  },
  {
    "repo": "kubeflow/pipelines",
    "file_path": "samples/v2/component_with_optional_inputs.py",
    "raw_url": "https://raw.githubusercontent.com/kubeflow/pipelines/master/samples/v2/component_with_optional_inputs.py",
    "content": "# Copyright 2023 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom typing import Optional, Dict, List\n\nfrom kfp import compiler\nfrom kfp import dsl\nfrom kfp.dsl import component\n\n\n@component\ndef component_op(\n    input_str1: Optional[str] = 'string default value',\n    input_str2: Optional[str] = None,\n    input_str3: Optional[str] = None,\n    input_str4_from_pipeline: Optional[str] = \"Some component default\",\n    input_str5_from_pipeline: Optional[str] = \"Some component default\",\n    input_str6_from_pipeline: Optional[str] = None,\n    input_bool1: Optional[bool] = True,\n    input_bool2: Optional[bool] = None,\n    input_dict: Optional[Dict[str, int]] = {\"a\": 1},\n    input_list: Optional[List[str]] = [\"123\"],\n    input_int: Optional[int] = 100,\n):\n    print(f'input_str1: {input_str1}, type: {type(input_str1)}')\n    print(f'input_str2: {input_str2}, type: {type(input_str2)}')\n    print(f'input_str3: {input_str3}, type: {type(input_str3)}')\n    print(f'input_str4_from_pipeline: {input_str4_from_pipeline}, type: {type(input_str4_from_pipeline)}')\n    print(f'input_str5_from_pipeline: {input_str5_from_pipeline}, type: {type(input_str5_from_pipeline)}')\n    print(f'input_str6_from_pipeline: {input_str6_from_pipeline}, type: {type(input_str6_from_pipeline)}')\n    print(f'input_bool1: {input_bool1}, type: {type(input_bool1)}')\n    print(f'input_bool2: {input_bool2}, type: {type(input_bool2)}')\n    print(f'input_bool: {input_dict}, type: {type(input_dict)}')\n    print(f'input_bool: {input_list}, type: {type(input_list)}')\n    print(f'input_bool: {input_int}, type: {type(input_int)}')\n\n\n@dsl.pipeline(name='v2-component-optional-input')\ndef pipeline(input_str4: Optional[str] = None, input_str5: Optional[str] = \"Some pipeline default\", input_str6: Optional[str] = None):\n    component_op(\n        input_str1='Hello',\n        input_str2='World',\n        input_str4_from_pipeline=input_str4,\n        input_str5_from_pipeline=input_str5,\n        input_str6_from_pipeline=input_str6,\n    )\n\n\nif __name__ == '__main__':\n    compiler.Compiler().compile(\n        pipeline_func=pipeline, package_path=__file__.replace('.py', '.yaml'))\n"
  },
  {
    "repo": "kubeflow/pipelines",
    "file_path": "samples/v2/hello_world.py",
    "raw_url": "https://raw.githubusercontent.com/kubeflow/pipelines/master/samples/v2/hello_world.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport os\n\nfrom kfp import dsl\nfrom kfp import compiler\n\n# In tests, we install a KFP package from the PR under test. Users should not\n# normally need to specify `kfp_package_path` in their component definitions.\n_KFP_PACKAGE_PATH = os.getenv('KFP_PACKAGE_PATH')\n\n\n@dsl.component(kfp_package_path=_KFP_PACKAGE_PATH)\ndef hello_world(text: str) -> str:\n    print(text)\n    return text\n\n\n@dsl.pipeline(name='hello-world', description='A simple intro pipeline')\ndef pipeline_hello_world(text: str = 'hi there'):\n    \"\"\"Pipeline that passes small pipeline parameter string to consumer op.\"\"\"\n\n    consume_task = hello_world(\n        text=text)  # Passing pipeline parameter as argument to consumer op\n\n\nif __name__ == \"__main__\":\n    # execute only if run as a script\n    compiler.Compiler().compile(\n        pipeline_func=pipeline_hello_world,\n        package_path='hello_world_pipeline.json')\n"
  },
  {
    "repo": "kubeflow/pipelines",
    "file_path": "samples/v2/parallel_after_dependency.py",
    "raw_url": "https://raw.githubusercontent.com/kubeflow/pipelines/master/samples/v2/parallel_after_dependency.py",
    "content": "from kfp import Client, dsl\n\n\n@dsl.component\ndef print_op(message: str) -> str:\n    print(message)\n    return message\n\n\n@dsl.pipeline()\ndef loop_with_after_dependency_set():\n    with dsl.ParallelFor([1, 2, 3]):\n        one = print_op(message='foo')\n    # Ensure that the dependecy is set downstream for all loop iterations\n    two = print_op(message='bar').after(one)\n    three = print_op(message='baz').after(one)\n\n\nif __name__ == '__main__':\n    client = Client()\n    run = client.create_run_from_pipeline_func(loop_with_after_dependency_set)\n"
  },
  {
    "repo": "kubeflow/pipelines",
    "file_path": "samples/v2/parallel_consume_upstream.py",
    "raw_url": "https://raw.githubusercontent.com/kubeflow/pipelines/master/samples/v2/parallel_consume_upstream.py",
    "content": "# This pipeline tests the ability to consume outputs from upstream components in\n# a loop context as well as having the inputs resolve when set_display_name is\n# used within the pipeline.\nfrom kfp import Client\nfrom kfp import dsl\nfrom kfp.dsl import Artifact\nfrom kfp.dsl import Input\nfrom kfp.dsl import Output\n\n\n@dsl.component\ndef split_input(input: str) -> list:\n    return input.split(',')\n\n\n@dsl.component\ndef create_file(file: Output[Artifact], content: str):\n    with open(file.path, 'w') as f:\n        f.write(content)\n\n\n@dsl.component\ndef read_file(file: Input[Artifact]) -> str:\n    with open(file.path, 'r') as f:\n        print(f.read())\n    return file.path\n\n\n@dsl.component\ndef print_input(input: list):\n    for item in input:\n        print(f'Input item: {item}')\n\n\n@dsl.pipeline()\ndef loop_consume_upstream():\n    model_ids_split_op = split_input(input='component1,component2,component3')\n    model_ids_split_op.set_caching_options(False)\n    model_ids_split_op.set_display_name('same display mame')\n    with dsl.ParallelFor(model_ids_split_op.output) as model_id:\n        create_file_op = create_file(content=model_id)\n        create_file_op.set_caching_options(False)\n        create_file_op.set_display_name('same display name')\n        # Consume the output from a op in the loop iteration DAG context\n        read_file_op = read_file(file=create_file_op.outputs['file'])\n        read_file_op.set_caching_options(False)\n        read_file_op.set_display_name('same display name')\n\n    print_input_op = print_input(input=model_ids_split_op.output)\n    print_input_op.set_caching_options(False)\n    print_input_op.set_display_name('same display name')\n\n\nif __name__ == '__main__':\n    client = Client()\n    run = client.create_run_from_pipeline_func(loop_consume_upstream)\n"
  },
  {
    "repo": "kubeflow/pipelines",
    "file_path": "samples/v2/pipeline_container_no_input.py",
    "raw_url": "https://raw.githubusercontent.com/kubeflow/pipelines/master/samples/v2/pipeline_container_no_input.py",
    "content": "# Copyright 2022 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport os\n\nfrom kfp import compiler\nfrom kfp import dsl\n\n\n@dsl.container_component\ndef container_no_input():\n    return dsl.ContainerSpec(\n        image='python:3.7',\n        command=['echo', 'hello world'],\n        args=[],\n    )\n\n\n@dsl.pipeline(name='v2-container-component-no-input')\ndef pipeline_container_no_input():\n    container_no_input()\n\n\nif __name__ == '__main__':\n    # execute only if run as a script\n    compiler.Compiler().compile(\n        pipeline_func=pipeline_container_no_input,\n        package_path='pipeline_container_no_input.yaml')\n"
  },
  {
    "repo": "kubeflow/pipelines",
    "file_path": "samples/v2/pipeline_with_env.py",
    "raw_url": "https://raw.githubusercontent.com/kubeflow/pipelines/master/samples/v2/pipeline_with_env.py",
    "content": "# Copyright 2023 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom kfp import compiler\nfrom kfp import components\nfrom kfp import dsl\nfrom kfp.dsl import component\n\n\n@component\ndef print_env_op():\n    import os\n    print('ENV1', os.environ.get('ENV1'))\n    print('ENV2', os.environ.get('ENV2'))\n\n\nprint_env_2_op = components.load_component_from_text(\"\"\"\nname: Check env\nimplementation:\n  container:\n    image: alpine\n    command:\n    - sh\n    - -c\n    - |\n      set -e -x\n      if [ \"$ENV2\" == \"val2\" ]\n      then\n        echo \"$ENV2\" \n      else \n        echo \"ENV2 does not equal val2\"\n        exit 1\n      fi\n      echo \"$ENV3\"\n    env:\n      ENV2: val0\n\"\"\")\n\n\n@dsl.pipeline(name='pipeline-with-env')\ndef pipeline_with_env():\n    print_env_op().set_env_variable(name='ENV1', value='val1')\n    print_env_2_op().set_env_variable(\n        name='ENV2', value='val2').set_env_variable(\n            name='ENV3', value='val3')\n\n\nif __name__ == '__main__':\n    compiler.Compiler().compile(\n        pipeline_func=pipeline_with_env, package_path='pipeline_with_env.yaml')\n"
  },
  {
    "repo": "kubeflow/pipelines",
    "file_path": "samples/v2/pipeline_with_importer.py",
    "raw_url": "https://raw.githubusercontent.com/kubeflow/pipelines/master/samples/v2/pipeline_with_importer.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Pipeline using dsl.importer.\"\"\"\nimport os\nfrom typing import NamedTuple\n\nfrom kfp import compiler, dsl\nfrom kfp.dsl import Dataset, Input, Model, component, importer\n\n# In tests, we install a KFP package from the PR under test. Users should not\n# normally need to specify `kfp_package_path` in their component definitions.\n_KFP_PACKAGE_PATH = os.getenv('KFP_PACKAGE_PATH')\n\n\n@component(kfp_package_path=_KFP_PACKAGE_PATH)\ndef train(\n    dataset: Input[Dataset]\n) -> NamedTuple('Outputs', [\n    ('scalar', str),\n    ('model', Model),\n]):\n    \"\"\"Dummy Training step.\"\"\"\n    with open(dataset.path, 'r') as f:\n        data = f.read()\n    print('Dataset:', data)\n\n    scalar = '123'\n    model = 'My model trained using data: {}'.format(data)\n\n    from collections import namedtuple\n    output = namedtuple('Outputs', ['scalar', 'model'])\n    return output(scalar, model)\n\n\n@dsl.pipeline(name='pipeline-with-importer')\ndef pipeline_with_importer():\n\n    importer1 = importer(\n        artifact_uri='gs://ml-pipeline-playground/shakespeare1.txt',\n        artifact_class=Dataset,\n        reimport=False)\n    train(dataset=importer1.output)\n\n\nif __name__ == \"__main__\":\n    # execute only if run as a script\n    compiler.Compiler().compile(\n        pipeline_func=pipeline_with_importer,\n        package_path='pipeline_with_importer.json')\n"
  },
  {
    "repo": "kubeflow/pipelines",
    "file_path": "samples/v2/pipeline_with_input_status_state.py",
    "raw_url": "https://raw.githubusercontent.com/kubeflow/pipelines/master/samples/v2/pipeline_with_input_status_state.py",
    "content": "\nfrom kfp import dsl\n\n\n@dsl.component\ndef echo_state(status: dsl.PipelineTaskFinalStatus):\n    assert(status.state == 'COMPLETE')\n    assert('status-state-pipeline' in status.pipeline_job_resource_name)\n    assert(status.pipeline_task_name == 'exit-handler-1')\n    #TODO: Add assert statements to validate status.error_code and status.error_message values once those fields have been implemented.\n\n@dsl.component\ndef some_task():\n    print('Executing some_task()...')\n\n@dsl.pipeline\ndef status_state_pipeline():\n    echo_state_task = echo_state()\n    with dsl.ExitHandler(exit_task=echo_state_task):\n        some_task()\n"
  },
  {
    "repo": "kubeflow/pipelines",
    "file_path": "samples/v2/pipeline_with_placeholders.py",
    "raw_url": "https://raw.githubusercontent.com/kubeflow/pipelines/master/samples/v2/pipeline_with_placeholders.py",
    "content": "# Copyright 2025 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom kfp import compiler\nfrom kfp import dsl\nfrom kfp.dsl import component\n\n\n@component\ndef print_all_placeholders(\n    job_name: str,\n    job_resource_name: str,\n    job_id: str,\n    task_name: str,\n    task_id: str,\n):\n    allPlaceholders = [job_name, job_resource_name, job_id, task_name, task_id]\n\n    for placeholder in allPlaceholders:\n        if \"\\{\\{\" in placeholder or placeholder == \"\":\n            raise RuntimeError(\n                \"Expected the placeholder to be replaced with a value: \" + placeholder\n            )\n\n    assert task_name == \"print-all-placeholders\"\n    assert job_name.startswith(\"pipeline-with-placeholders \")\n    assert job_resource_name.startswith(\"pipeline-with-placeholders-\")\n\n    output = \", \".join(allPlaceholders)\n    print(output)\n\n\n@dsl.pipeline(name=\"pipeline-with-placeholders\")\ndef pipeline_with_placeholders():\n    print_all_placeholders(\n        job_name=dsl.PIPELINE_JOB_NAME_PLACEHOLDER,\n        job_resource_name=dsl.PIPELINE_JOB_RESOURCE_NAME_PLACEHOLDER,\n        job_id=dsl.PIPELINE_JOB_ID_PLACEHOLDER,\n        task_name=dsl.PIPELINE_TASK_NAME_PLACEHOLDER,\n        task_id=dsl.PIPELINE_TASK_ID_PLACEHOLDER,\n    ).set_caching_options(False)\n\n\nif __name__ == \"__main__\":\n    compiler.Compiler().compile(\n        pipeline_func=pipeline_with_placeholders,\n        package_path=__file__.replace(\".py\", \".yaml\"),\n    )\n"
  },
  {
    "repo": "riiid/krsh",
    "file_path": "krsh/cmd/group_create/templates/pipeline/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/riiid/krsh/master/krsh/cmd/group_create/templates/pipeline/pipeline.py",
    "content": "import kfp\n\n\n@kfp.dsl.pipeline(name=\"{name}\")\ndef pipeline():\n    pass\n"
  },
  {
    "repo": "riiid/krsh",
    "file_path": "tests/samples/have-pipeline-project/pipelines/pipeline-1/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/riiid/krsh/master/tests/samples/have-pipeline-project/pipelines/pipeline-1/pipeline.py",
    "content": "import sys\nsys.path.append(\"../..\")\n\nimport kfp\n\n\n@kfp.dsl.pipeline(name=\"pipeline-1\")\ndef pipeline():\n    pass\n"
  },
  {
    "repo": "riiid/krsh",
    "file_path": "tests/samples/have-pipeline-project/pipelines/pipeline-2/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/riiid/krsh/master/tests/samples/have-pipeline-project/pipelines/pipeline-2/pipeline.py",
    "content": "import sys\nsys.path.append(\"../..\")\n\nimport kfp\n\n@kfp.dsl.pipeline(name=\"pipeline-2\")\ndef pipeline():\n    pass\n"
  },
  {
    "repo": "omerbsezer/Fast-Kubeflow",
    "file_path": "Project_Kubeflow_Pipeline_MLModels/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/omerbsezer/Fast-Kubeflow/main/Project_Kubeflow_Pipeline_MLModels/pipeline.py",
    "content": "import kfp\nfrom kfp import dsl\nfrom kfp.components import func_to_container_op\n\n@func_to_container_op\ndef show_results(decision_tree : float, logistic_regression : float, svm : float, naive_bayes : float, xg_boost : float) -> None:\n    # Given the outputs from decision_tree, logistic regression, svm, naive_bayes, xg_boost components\n\n    print(f\"Decision tree (accuracy): {decision_tree}\")\n    print(f\"Logistic regression (accuracy): {logistic_regression}\")\n    print(f\"SVM (SVC) (accuracy): {svm}\")\n    print(f\"Naive Bayes (Gaussian) (accuracy): {naive_bayes}\")\n    print(f\"XG Boost (accuracy): {xg_boost}\")\n\n\n@dsl.pipeline(name='ML Models Pipeline', description='Applies Decision Tree, Logistic Regression, SVM, Naive Bayes, XG Boost for classification problem.')\ndef ml_models_pipeline():\n\n    # Loads the yaml manifest for each component\n    download = kfp.components.load_component_from_file('download_data/download_data.yaml')\n    decision_tree = kfp.components.load_component_from_file('decision_tree/decision_tree.yaml')\n    logistic_regression = kfp.components.load_component_from_file('logistic_regression/logistic_regression.yaml')\n    svm = kfp.components.load_component_from_file('svm/svm.yaml')\n    naive_bayes = kfp.components.load_component_from_file('naive_bayes/naive_bayes.yaml')\n    xg_boost = kfp.components.load_component_from_file('xg_boost/xg_boost.yaml')\n\n    # Run download_data task\n    download_task = download()\n\n    # Run ML models tasks with input data\n    decision_tree_task = decision_tree(download_task.output)\n    logistic_regression_task = logistic_regression(download_task.output)\n    svm_task = svm(download_task.output)\n    naive_bayes_task = naive_bayes(download_task.output)\n    xg_boost_task = xg_boost(download_task.output)\n\n    # Given the outputs from ML models tasks\n    # the component \"show_results\" is called to print the results.\n    show_results(decision_tree_task.output, logistic_regression_task.output, svm_task.output, naive_bayes_task.output, xg_boost_task.output)\n\n\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(ml_models_pipeline, 'MLModelsPipeline.yaml')"
  },
  {
    "repo": "ksalama/kubeflow-examples",
    "file_path": "kfp-cloudbuild/pipeline/workflow.py",
    "raw_url": "https://raw.githubusercontent.com/ksalama/kubeflow-examples/master/kfp-cloudbuild/pipeline/workflow.py",
    "content": "# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Pipeline workflow definition.\"\"\"\n\nimport kfp\nimport kfp.dsl as dsl\n\n# Initialize component store\ncomponent_store = kfp.components.ComponentStore(\n  local_search_paths=['components'])\n\n# Create component factories\nadd_op = component_store.load_component('my_add')\ndivide_op = component_store.load_component('my_divide')\n\n# Define pipeline\n@dsl.pipeline(\n  name='A Simple CI pipeline',\n  description='Basic sample to show how to do CI with KFP using CloudBuild'\n)\ndef pipeline(\n  x_value: int=1,\n  y_value: int=1,\n  z_value: int=1,\n):\n\n  add_step = add_op(x_value=x_value, y_value=y_value)\n  add_step.set_display_name('Add x and y')\n  add_result = add_step.outputs\n  sum_value = add_result['sum']\n  with kfp.dsl.Condition(sum_value != 0):\n    divide_step = divide_op(x_value=sum_value, y_value=z_value)\n    divide_step.set_display_name('Divide sum by z')\n    add_step2 = add_op(\n      x_value=divide_step.outputs['quotient'],\n      y_value=divide_step.outputs['remainder'])\n    add_step2.set_display_name('Add quotient and remainder')\n\n\n\n"
  },
  {
    "repo": "getindata/kedro-kubeflow",
    "file_path": "tests/test_kfpclient.py",
    "raw_url": "https://raw.githubusercontent.com/getindata/kedro-kubeflow/develop/tests/test_kfpclient.py",
    "content": "\"\"\"Test kedro_kubeflow module.\"\"\"\n\nimport os\nimport unittest\nfrom tempfile import NamedTemporaryFile\nfrom unittest.mock import patch\n\nfrom kfp import dsl\n\nfrom kedro_kubeflow.config import PluginConfig\nfrom kedro_kubeflow.generators.one_pod_pipeline_generator import (\n    OnePodPipelineGenerator,\n)\nfrom kedro_kubeflow.kfpclient import KubeflowClient\nfrom kedro_kubeflow.utils import strip_margin\nfrom tests.common import MinimalConfigMixin\n\nTEST_TIMEOUT_DUMMY = 60 * 60\n\n\nclass TestKubeflowClient(unittest.TestCase, MinimalConfigMixin):\n    def create_experiment(self, id=\"123\"):\n        return type(\"obj\", (object,), {\"id\": id})\n\n    def create_empty_pipelines_list(self):\n        return type(\n            \"obj\",\n            (object,),\n            {\"pipelines\": None},\n        )\n\n    def create_pipelines_list(self):\n        return type(\n            \"obj\",\n            (object,),\n            {\"pipelines\": [type(\"obj\", (object,), {\"name\": \"somename\", \"id\": \"someid\"})]},\n        )\n\n    def create_recurring_jobs_list(self, job_name=\"job_name\"):\n        return type(\n            \"obj\",\n            (object,),\n            {\n                \"jobs\": [\n                    type(\n                        \"obj\",\n                        (object,),\n                        {\n                            \"name\": job_name,\n                            \"id\": job_name + \"ID\",\n                        },\n                    )\n                ]\n            },\n        )\n\n    def test_should_list_pipelines_tabularized(self):\n        # given\n        self.kfp_client_mock.list_pipelines.return_value = self.create_pipelines_list()\n\n        # when\n        output = self.client_under_test.list_pipelines()\n\n        # then\n        expected_output = \"\"\"\n        |Name      ID\n        |--------  ------\n        |somename  someid\"\"\"\n        self.assertEqual(output, strip_margin(expected_output))\n\n    def test_should_run_pipeline_without_waiting(self):\n        # given\n        run_mock = unittest.mock.MagicMock()\n        self.kfp_client_mock.create_run_from_pipeline_func.return_value = run_mock\n\n        # when\n        self.client_under_test.run_once(\n            run_name=\"unittest\",\n            pipeline=\"pipeline\",\n            image=\"unittest-image\",\n            experiment_name=\"experiment\",\n            wait=False,\n            timeout=TEST_TIMEOUT_DUMMY,\n            experiment_namespace=\"exp_namespace\",\n        )\n\n        # then\n        self.kfp_client_mock.create_run_from_pipeline_func.assert_called()\n        run_mock.wait_for_run_completion.assert_not_called()\n        (\n            args,\n            kwargs,\n        ) = self.kfp_client_mock.create_run_from_pipeline_func.call_args\n        assert kwargs == {\n            \"arguments\": {},\n            \"experiment_name\": \"experiment\",\n            \"run_name\": \"unittest\",\n            \"namespace\": \"exp_namespace\",\n        }\n\n    def test_should_run_pipeline_and_wait(self):\n        # given\n        run_mock = unittest.mock.MagicMock()\n        self.kfp_client_mock.create_run_from_pipeline_func.return_value = run_mock\n\n        # when\n        self.client_under_test.run_once(\n            run_name=\"unittest\",\n            pipeline=\"pipeline\",\n            image=\"unittest-image\",\n            experiment_name=\"experiment\",\n            wait=True,\n            timeout=TEST_TIMEOUT_DUMMY,\n            experiment_namespace=None,\n        )\n\n        # then\n        self.kfp_client_mock.create_run_from_pipeline_func.assert_called()\n        run_mock.wait_for_run_completion.assert_called()\n\n    def test_should_run_pipeline_adjusting_the_name(self):\n        # given\n        run_mock = unittest.mock.MagicMock()\n        self.kfp_client_mock.create_run_from_pipeline_func.return_value = run_mock\n\n        # when\n        self.client_under_test.run_once(\n            run_name=\"unittest for region {region}\",\n            pipeline=\"pipeline\",\n            image=\"unittest-image\",\n            experiment_name=\"experiment\",\n            wait=False,\n            timeout=TEST_TIMEOUT_DUMMY,\n            experiment_namespace=\"exp_namespace\",\n            parameters={\"region\": \"ABC\"},\n        )\n\n        # then\n        self.kfp_client_mock.create_run_from_pipeline_func.assert_called()\n        run_mock.wait_for_run_completion.assert_not_called()\n        (\n            args,\n            kwargs,\n        ) = self.kfp_client_mock.create_run_from_pipeline_func.call_args\n        assert kwargs == {\n            \"arguments\": {\"region\": \"ABC\"},\n            \"experiment_name\": \"experiment\",\n            \"run_name\": \"unittest for region ABC\",\n            \"namespace\": \"exp_namespace\",\n        }\n\n    def test_should_compile_pipeline(self):\n        with NamedTemporaryFile(suffix=\".yaml\") as f:\n            # when\n            self.client_under_test.compile(pipeline=\"pipeline\", image=\"unittest-image\", output=f.name)\n\n            # then\n            with open(f.name) as yamlfile:\n                assert \"generateName: my-awesome-project-\" in yamlfile.read()\n\n    @patch(\"kedro_kubeflow.kfpclient.AuthHandler\")\n    @patch(\"kedro_kubeflow.kfpclient.PodPerNodePipelineGenerator\")\n    @patch(\"kedro_kubeflow.kfpclient.Client\")\n    def test_should_use_jwt_token_in_kfp_client(self, kfp_client_mock, pipeline_generator_mock, auth_handler_mock):\n        # given\n        os.environ[\"IAP_CLIENT_ID\"] = \"unittest-client-id\"\n        auth_handler_mock.return_value.obtain_id_token.return_value = \"unittest-token\"\n        auth_handler_mock.return_value.obtain_dex_authservice_session.return_value = None\n\n        # when\n        self.client_under_test = KubeflowClient(\n            PluginConfig(**self.minimal_config({\"host\": \"http://unittest\", \"run_config\": {}})),\n            None,\n            None,\n        )\n\n        # then\n        kfp_client_mock.assert_called_with(host=\"http://unittest\", existing_token=\"unittest-token\")\n\n    @patch(\"kedro_kubeflow.kfpclient.AuthHandler\")\n    @patch(\"kedro_kubeflow.kfpclient.PodPerNodePipelineGenerator\")\n    @patch(\"kedro_kubeflow.kfpclient.Client\")\n    def test_should_use_dex_session_in_kfp_client(self, kfp_client_mock, pipeline_generator_mock, auth_handler_mock):\n        # given\n        auth_handler_mock.return_value.obtain_id_token.return_value = None\n        auth_handler_mock.return_value.obtain_dex_authservice_session.return_value = \"session_id\"\n\n        # when\n        self.client_under_test = KubeflowClient(\n            PluginConfig(**self.minimal_config({\"host\": \"http://unittest\", \"run_config\": {}})),\n            None,\n            None,\n        )\n\n        # then\n        kfp_client_mock.assert_called_with(host=\"http://unittest\", cookies=\"authservice_session=session_id\")\n\n    def test_should_schedule_pipeline(self):\n        # given\n        self.kfp_client_mock.get_experiment.return_value = self.create_experiment()\n        self.kfp_client_mock.get_pipeline_id.return_value = \"someid\"\n\n        # when\n        self.client_under_test.schedule(\n            pipeline=None,\n            experiment_name=\"EXPERIMENT\",\n            cron_expression=\"0 * * * * *\",\n            experiment_namespace=None,\n            run_name=\"scheduled run of pipeline X\",\n            parameters={},\n            env=\"kubeflow-env\",\n        )\n\n        # then\n        self.kfp_client_mock.get_experiment.assert_called()\n        self.kfp_client_mock.create_experiment.assert_not_called()\n        self.kfp_client_mock.create_recurring_run.assert_called_with(\n            \"123\",\n            \"scheduled run of pipeline X\",\n            cron_expression=\"0 * * * * *\",\n            pipeline_id=\"someid\",\n            params={},\n        )\n\n    def test_should_schedule_pipeline_and_create_experiment_if_needed(self):\n        # given\n        self.kfp_client_mock.get_experiment.side_effect = ValueError(\"No experiment is found with name ....\")\n        self.kfp_client_mock.create_experiment.return_value = self.create_experiment()\n        self.kfp_client_mock.get_pipeline_id.return_value = \"someid\"\n\n        # when\n        self.client_under_test.schedule(\n            pipeline=None,\n            experiment_name=\"EXPERIMENT\",\n            cron_expression=\"0 * * * * *\",\n            experiment_namespace=None,\n            run_name=\"pipeline X\",\n            parameters={},\n            env=\"kubeflow-env\",\n        )\n\n        # then\n        self.kfp_client_mock.get_experiment.assert_called()\n        self.kfp_client_mock.create_experiment.assert_called()\n        self.kfp_client_mock.create_recurring_run.assert_called_with(\n            \"123\",\n            \"pipeline X\",\n            cron_expression=\"0 * * * * *\",\n            pipeline_id=\"someid\",\n            params={},\n        )\n\n    def test_should_disable_old_runs_before_schedule(self):\n        # given\n        self.kfp_client_mock.get_experiment.return_value = self.create_experiment()\n        self.kfp_client_mock.get_pipeline_id.return_value = \"someid\"\n        self.kfp_client_mock.list_recurring_runs.return_value = self.create_recurring_jobs_list(\n            \"scheduled run for region ABC\"\n        )\n\n        # when\n        self.client_under_test.schedule(\n            pipeline=None,\n            experiment_name=\"EXPERIMENT\",\n            cron_expression=\"0 * * * * *\",\n            experiment_namespace=None,\n            run_name=\"scheduled run for region {region}\",\n            parameters={\"region\": \"ABC\"},\n            env=\"kubeflow-env\",\n        )\n\n        # then\n        self.kfp_client_mock.get_experiment.assert_called()\n        self.kfp_client_mock.create_experiment.assert_not_called()\n        self.kfp_client_mock.jobs.delete_job.assert_called()\n        self.kfp_client_mock.create_recurring_run.assert_called_with(\n            \"123\",\n            \"scheduled run for region ABC\",\n            cron_expression=\"0 * * * * *\",\n            pipeline_id=\"someid\",\n            params={\"region\": \"ABC\"},\n        )\n\n    def test_should_upload_new_pipeline(self):\n        # given\n        self.create_client({\"description\": \"Very Important Pipeline\"})\n        self.kfp_client_mock.get_pipeline_id.return_value = None\n\n        # when\n        self.client_under_test.upload(\n            pipeline_name=\"pipeline_name\",\n            image=\"unittest-image\",\n            image_pull_policy=\"Always\",\n            env=\"kubeflow-env\",\n        )\n\n        # then\n        self.kfp_client_mock.pipeline_uploads.upload_pipeline.assert_called()\n        self.kfp_client_mock.pipeline_uploads.upload_pipeline_version.assert_not_called()\n        (\n            args,\n            kwargs,\n        ) = self.kfp_client_mock.pipeline_uploads.upload_pipeline.call_args\n        assert kwargs[\"name\"] == \"[my-awesome-project] pipeline_name (env: kubeflow-env)\"\n        assert kwargs[\"description\"] == \"Very Important Pipeline\"\n\n    @patch(\"kedro_kubeflow.kfpclient.Client\")\n    @patch(\"kedro.framework.context.context.KedroContext\")\n    def test_can_create_client_with_node_strategy_full(self, context, _):\n        client = KubeflowClient(\n            PluginConfig(\n                **self.minimal_config(\n                    {\n                        \"host\": \"http://unittest\",\n                        \"run_config\": {\"node_merge_strategy\": \"full\"},\n                    }\n                )\n            ),\n            \"unit-test-project\",\n            context,\n        )\n\n        assert isinstance(client.generator, OnePodPipelineGenerator)\n\n    def test_should_truncated_the_pipeline_name_to_100_characters_on_upload(\n        self,\n    ):\n        # given\n        self.create_client({\"description\": \"Very Important Pipeline\"})\n        self.kfp_client_mock.get_pipeline_id.return_value = None\n\n        # when\n        self.client_under_test.upload(\n            pipeline_name=\"pipeline_name\",\n            image=\"unittest-image\",\n            image_pull_policy=\"Always\",\n            env=\"kubeflow-env\" + \"1\" * 100,\n        )\n\n        # then\n        self.kfp_client_mock.pipeline_uploads.upload_pipeline.assert_called()\n        self.kfp_client_mock.pipeline_uploads.upload_pipeline_version.assert_not_called()\n        (\n            args,\n            kwargs,\n        ) = self.kfp_client_mock.pipeline_uploads.upload_pipeline.call_args\n        assert len(kwargs[\"name\"]) == 100\n\n    def test_should_upload_new_version_of_existing_pipeline(self):\n        # given\n        self.kfp_client_mock.get_pipeline_id.return_value = \"123\"\n\n        # when\n        self.client_under_test.upload(\n            pipeline_name=\"pipeline\",\n            image=\"unittest-image\",\n            image_pull_policy=\"Always\",\n            env=\"kubeflow-env\",\n        )\n\n        # then\n        self.kfp_client_mock.pipeline_uploads.upload_pipeline.assert_not_called()\n        self.kfp_client_mock.pipeline_uploads.upload_pipeline_version.assert_called()\n\n    @patch(\"kedro_kubeflow.kfpclient.Client\")\n    def test_should_raise_error_if_invalid_node_merge_strategy(self, kfp_client_mock):\n        with self.assertRaises(ValueError) as raises:\n            KubeflowClient(\n                PluginConfig(\n                    **self.minimal_config(\n                        {\n                            \"host\": \"http://unittest\",\n                            \"run_config\": {\"node_merge_strategy\": \"other\"},\n                        }\n                    )\n                ),\n                None,\n                None,\n            )\n        assert \"validation error\" in str(raises.exception)\n\n    @patch(\"kedro_kubeflow.kfpclient.PodPerNodePipelineGenerator\")\n    @patch(\"kedro_kubeflow.kfpclient.Client\")\n    def create_client(self, config, kfp_client_mock, pipeline_generator_mock):\n        project_name = \"my-awesome-project\"\n        self.client_under_test = KubeflowClient(\n            PluginConfig(**self.minimal_config({\"host\": \"http://unittest\", \"run_config\": config})),\n            project_name,\n            None,  # context,\n        )\n        self.client_under_test.client = kfp_client_mock\n        self.kfp_client_mock = self.client_under_test.client\n\n        @dsl.pipeline(name=project_name)\n        def empty_pipeline():\n            pass\n\n        self.client_under_test.generator.generate_pipeline.return_value = empty_pipeline\n\n    def mock_mlflow(self, enabled=False):\n        def fakeimport(name, *args, **kw):\n            if not enabled and name == \"mlflow\":\n                raise ImportError\n            return self.realimport(name, *args, **kw)\n\n        __builtins__[\"__import__\"] = fakeimport\n\n    def setUp(self):\n        self.realimport = __builtins__[\"__import__\"]\n        self.mock_mlflow(False)\n        self.create_client({})\n\n    def tearDown(self):\n        __builtins__[\"__import__\"] = self.realimport\n        os.environ[\"IAP_CLIENT_ID\"] = \"\"\n"
  },
  {
    "repo": "getindata/kedro-kubeflow",
    "file_path": "tests/test_one_pod_pipeline_generator.py",
    "raw_url": "https://raw.githubusercontent.com/getindata/kedro-kubeflow/develop/tests/test_one_pod_pipeline_generator.py",
    "content": "\"\"\"Test generator\"\"\"\n\nimport datetime\nimport os\nimport unittest\nfrom inspect import signature\nfrom unittest.mock import MagicMock, patch\n\nimport kfp\nfrom kedro.pipeline import Pipeline, node\n\nfrom kedro_kubeflow.config import PluginConfig\nfrom kedro_kubeflow.generators.one_pod_pipeline_generator import (\n    OnePodPipelineGenerator,\n)\nfrom tests.common import MinimalConfigMixin\n\n\ndef identity(input1: str):\n    return input1  # pragma: no cover\n\n\nclass TestGenerator(unittest.TestCase, MinimalConfigMixin):\n    def test_support_modification_of_pull_policy(self):\n        # given\n        self.create_generator()\n\n        # when\n        with patch(\n            \"kedro.framework.project.pipelines\",\n            new=self.pipelines_under_test,\n        ):\n            pipeline = self.generator_under_test.generate_pipeline(\"pipeline\", \"unittest-image\", \"Never\")\n            with kfp.dsl.Pipeline(None) as dsl_pipeline:\n                pipeline()\n\n            # then\n            assert len(dsl_pipeline.ops) == 1\n            assert dsl_pipeline.ops[\"pipeline\"].container.image == \"unittest-image\"\n            assert dsl_pipeline.ops[\"pipeline\"].container.image_pull_policy == \"Never\"\n\n    def test_should_support_params_and_inject_them_to_the_node(self):\n        # given\n        self.create_generator(\n            params={\n                \"param1\": 0.3,\n                \"param2\": 42,\n                \"param3\": datetime.date(2022, 2, 24),\n            }\n        )\n\n        # when\n        with patch(\n            \"kedro.framework.project.pipelines\",\n            new=self.pipelines_under_test,\n        ):\n            with kfp.dsl.Pipeline(None) as dsl_pipeline:\n                pipeline = self.generator_under_test.generate_pipeline(\"pipeline\", \"unittest-image\", \"Always\")\n                default_params = signature(pipeline).parameters\n                pipeline()\n\n            # then\n            assert len(default_params) == 3\n            assert default_params[\"param1\"].default == 0.3\n            assert default_params[\"param2\"].default == 42\n            assert default_params[\"param3\"].default == \"2022-02-24\"\n            assert dsl_pipeline.ops[\"pipeline\"].container.args[1:] == [\n                \"param1\",\n                \"{{pipelineparam:op=;name=param1}}\",\n                \"param2\",\n                \"{{pipelineparam:op=;name=param2}}\",\n                \"param3\",\n                \"{{pipelineparam:op=;name=param3}}\",\n            ]\n\n    def test_should_support_nested_params_and_inject_them_to_the_node(self):\n        # given\n        self.create_generator(\n            params={\n                \"param1\": {\"nested1\": {\"nested2\": 1, \"nested3\": 2}},\n                \"param2\": 42,\n                \"param3\": datetime.date(2022, 2, 24),\n            }\n        )\n\n        # when\n        with patch(\n            \"kedro.framework.project.pipelines\",\n            new=self.pipelines_under_test,\n        ):\n            with kfp.dsl.Pipeline(None) as dsl_pipeline:\n                pipeline = self.generator_under_test.generate_pipeline(\"pipeline\", \"unittest-image\", \"Always\")\n                default_params = signature(pipeline).parameters\n                pipeline()\n\n            # then\n            assert len(default_params) == 3\n            assert default_params[\"param1\"].default == {\"nested1\": {\"nested2\": 1, \"nested3\": 2}}\n            assert default_params[\"param2\"].default == 42\n            assert default_params[\"param3\"].default == \"2022-02-24\"\n            assert dsl_pipeline.ops[\"pipeline\"].container.args[1:] == [\n                \"param1\",\n                \"{{pipelineparam:op=;name=param1}}\",\n                \"param2\",\n                \"{{pipelineparam:op=;name=param2}}\",\n                \"param3\",\n                \"{{pipelineparam:op=;name=param3}}\",\n            ]\n\n    def test_should_support_namespaced_params_and_inject_them_to_the_node(\n        self,\n    ):\n        # given\n        self.create_generator(\n            params={\n                \"outer_namespace.inner_namespace1.param1\": \"outer_namespace.inner_namespace1.param1_v\",\n                \"outer_namespace.inner_namespace1.param2\": \"outer_namespace.inner_namespace1.param2_v\",\n                \"outer_namespace.inner_namespace2.param1\": \"outer_namespace.inner_namespace2.param1_v\",\n                \"outer_namespace.inner_namespace2.param2\": \"outer_namespace.inner_namespace2.param2_v\",\n                \"outer_namespace.param\": \"outer_namespace.param\",\n                \"param1\": 42,\n            }\n        )\n\n        # when\n        with patch(\n            \"kedro.framework.project.pipelines\",\n            new=self.pipelines_under_test,\n        ):\n            with kfp.dsl.Pipeline(None) as dsl_pipeline:\n                pipeline = self.generator_under_test.generate_pipeline(\"pipeline\", \"unittest-image\", \"Always\")\n                default_params = signature(pipeline).parameters\n                pipeline()\n\n            # then\n            assert len(default_params) == 2\n            assert default_params[\"outer_namespace\"].default == {\n                \"inner_namespace1\": {\n                    \"param1\": \"outer_namespace.inner_namespace1.param1_v\",\n                    \"param2\": \"outer_namespace.inner_namespace1.param2_v\",\n                },\n                \"inner_namespace2\": {\n                    \"param1\": \"outer_namespace.inner_namespace2.param1_v\",\n                    \"param2\": \"outer_namespace.inner_namespace2.param2_v\",\n                },\n                \"param\": \"outer_namespace.param\",\n            }\n            assert default_params[\"param1\"].default == 42\n            assert dsl_pipeline.ops[\"pipeline\"].container.args[1:] == [\n                \"outer_namespace\",\n                \"{{pipelineparam:op=;name=outer_namespace}}\",\n                \"param1\",\n                \"{{pipelineparam:op=;name=param1}}\",\n            ]\n\n    def test_should_use_default_resources_spec_if_not_requested(self):\n        # given\n        self.create_generator(config={})\n\n        # when\n        with patch(\n            \"kedro.framework.project.pipelines\",\n            new=self.pipelines_under_test,\n        ):\n            pipeline = self.generator_under_test.generate_pipeline(\"pipeline\", \"unittest-image\", \"Always\")\n            with kfp.dsl.Pipeline(None) as dsl_pipeline:\n                pipeline()\n\n            # then\n            assert dsl_pipeline.ops[\"pipeline\"].container.resources is not None\n            assert dsl_pipeline.ops[\"pipeline\"].container.resources.limits[\"cpu\"]\n            assert dsl_pipeline.ops[\"pipeline\"].container.resources.limits[\"memory\"]\n\n    def test_should_add_resources_spec(self):\n        # given\n        self.create_generator(\n            config={\n                \"resources\": {\n                    \"__default__\": {\"cpu\": \"100m\", \"memory\": \"8Gi\"},\n                    \"node1\": {\"cpu\": \"400m\", \"memory\": \"64Gi\"},\n                }\n            }\n        )\n\n        # when\n        with patch(\n            \"kedro.framework.project.pipelines\",\n            new=self.pipelines_under_test,\n        ):\n            pipeline = self.generator_under_test.generate_pipeline(\"pipeline\", \"unittest-image\", \"Always\")\n            with kfp.dsl.Pipeline(None) as dsl_pipeline:\n                pipeline()\n\n            # then\n            resources = dsl_pipeline.ops[\"pipeline\"].container.resources\n            assert resources.limits == {\"cpu\": \"100m\", \"memory\": \"8Gi\"}\n            assert resources.requests == {\"cpu\": \"100m\", \"memory\": \"8Gi\"}\n\n    def test_should_not_add_retry_policy_if_not_requested(self):\n        # given\n        self.create_generator(config={})\n\n        # when\n        with patch(\n            \"kedro.framework.project.pipelines\",\n            new=self.pipelines_under_test,\n        ):\n            pipeline = self.generator_under_test.generate_pipeline(\"pipeline\", \"unittest-image\", \"Always\")\n            with kfp.dsl.Pipeline(None) as dsl_pipeline:\n                pipeline()\n\n            # then\n            op = dsl_pipeline.ops[\"pipeline\"]\n            assert op.num_retries == 0\n            assert op.retry_policy is None\n            assert op.backoff_factor is None\n            assert op.backoff_duration is None\n            assert op.backoff_max_duration is None\n\n    def test_should_add_retry_policy(self):\n        # given\n        self.create_generator(\n            config={\n                \"retry_policy\": {\n                    \"__default__\": {\n                        \"num_retries\": 4,\n                        \"backoff_duration\": \"60s\",\n                        \"backoff_factor\": 2,\n                    },\n                    \"node1\": {\n                        \"num_retries\": 100,\n                        \"backoff_duration\": \"5m\",\n                        \"backoff_factor\": 1,\n                    },\n                }\n            }\n        )\n\n        # when\n        with patch(\n            \"kedro.framework.project.pipelines\",\n            new=self.pipelines_under_test,\n        ):\n            pipeline = self.generator_under_test.generate_pipeline(\"pipeline\", \"unittest-image\", \"Always\")\n            with kfp.dsl.Pipeline(None) as dsl_pipeline:\n                pipeline()\n\n            # then\n            op = dsl_pipeline.ops[\"pipeline\"]\n            assert op.num_retries == 4\n            assert op.retry_policy == \"Always\"\n            assert op.backoff_factor == 2\n            assert op.backoff_duration == \"60s\"\n            assert op.backoff_max_duration is None\n\n    def test_should_set_description(self):\n        # given\n        self.create_generator(config={\"description\": \"DESC\"})\n\n        # when\n        pipeline = self.generator_under_test.generate_pipeline(\"pipeline\", \"unittest-image\", \"Never\")\n\n        # then\n        assert pipeline._component_description == \"DESC\"\n\n    def test_artifact_registration(self):\n        # given\n        self.create_generator(\n            catalog={\n                \"B\": {\n                    \"type\": \"pandas.CSVDataSet\",\n                    \"filepath\": \"data/02_intermediate/b.csv\",\n                }\n            }\n        )\n\n        # when\n        with patch(\n            \"kedro.framework.project.pipelines\",\n            new=self.pipelines_under_test,\n        ):\n            pipeline = self.generator_under_test.generate_pipeline(\"pipeline\", \"unittest-image\", \"Always\")\n            with kfp.dsl.Pipeline(None) as dsl_pipeline:\n                pipeline()\n\n            # then\n            assert dsl_pipeline.ops[\"pipeline\"].file_outputs == {\"B\": \"/home/kedro/data/02_intermediate/b.csv\"}\n\n    def test_should_skip_artifact_registration_if_requested(self):\n        # given\n        self.create_generator(\n            catalog={\n                \"B\": {\n                    \"type\": \"pandas.CSVDataSet\",\n                    \"filepath\": \"data/02_intermediate/b.csv\",\n                }\n            },\n            config={\"store_kedro_outputs_as_kfp_artifacts\": False},\n        )\n\n        # when\n        with patch(\n            \"kedro.framework.project.pipelines\",\n            new=self.pipelines_under_test,\n        ):\n            pipeline = self.generator_under_test.generate_pipeline(\"pipeline\", \"unittest-image\", \"Always\")\n            with kfp.dsl.Pipeline(None) as dsl_pipeline:\n                pipeline()\n\n            # then\n            assert dsl_pipeline.ops[\"pipeline\"].file_outputs == {}\n\n    def test_should_pass_kedro_config_env_to_nodes(self):\n        # given\n        self.create_generator(params={\"param1\": 0.3, \"param2\": 42})\n        os.environ[\"KEDRO_CONFIG_MY_KEY\"] = \"42\"\n        os.environ[\"SOME_VALUE\"] = \"100\"\n\n        try:\n            # when\n            with patch(\n                \"kedro.framework.project.pipelines\",\n                new=self.pipelines_under_test,\n            ):\n                pipeline = self.generator_under_test.generate_pipeline(\"pipeline\", \"unittest-image\", \"Always\")\n                with kfp.dsl.Pipeline(None) as dsl_pipeline:\n                    pipeline()\n\n                # then\n                env_values = {e.name: e.value for e in dsl_pipeline.ops[\"pipeline\"].container.env}\n                assert \"KEDRO_CONFIG_MY_KEY\" in env_values\n                assert env_values[\"KEDRO_CONFIG_MY_KEY\"] == \"42\"\n                assert \"SOME_VALUE\" not in env_values\n        finally:\n            del os.environ[\"KEDRO_CONFIG_MY_KEY\"]\n            del os.environ[\"SOME_VALUE\"]\n\n    def test_should_pass_kubeflow_run_id_to_nodes(self):\n        # given\n        self.create_generator(params={\"param1\": 0.3, \"param2\": 42})\n\n        # when\n        with patch(\n            \"kedro.framework.project.pipelines\",\n            new=self.pipelines_under_test,\n        ):\n            pipeline = self.generator_under_test.generate_pipeline(\"pipeline\", \"unittest-image\", \"Always\")\n            with kfp.dsl.Pipeline(None) as dsl_pipeline:\n                pipeline()\n\n            # then\n            env_values = {e.name: e.value for e in dsl_pipeline.ops[\"pipeline\"].container.env}\n            assert \"KUBEFLOW_RUN_ID\" in env_values\n            assert env_values[\"KUBEFLOW_RUN_ID\"] == \"{{workflow.uid}}\"\n\n    def test_should_generate_exit_handler_if_requested(self):\n        # given\n        self.create_generator(config={\"on_exit_pipeline\": \"notify_via_slack\"})\n\n        # when\n        with patch(\n            \"kedro.framework.project.pipelines\",\n            new=self.pipelines_under_test,\n        ):\n            pipeline = self.generator_under_test.generate_pipeline(\"pipeline\", \"unittest-image\", \"Always\")\n            with kfp.dsl.Pipeline(None) as dsl_pipeline:\n                pipeline()\n\n            # then\n            assert len(dsl_pipeline.ops) == 2\n            assert \"on-exit\" in dsl_pipeline.ops\n            assert (\n                dsl_pipeline.ops[\"on-exit\"]\n                .container.command[-1]\n                .endswith(\"kedro run --config config.yaml \" \"--env unittests --pipeline notify_via_slack\")\n            )\n\n    def test_should_generate_exit_handler_with_max_staleness(self):\n        # given\n        self.create_generator(\n            config={\n                \"on_exit_pipeline\": \"notify_via_slack\",\n                \"max_cache_staleness\": \"P0D\",\n            }\n        )\n\n        # when\n        with kfp.dsl.Pipeline(None) as dsl_pipeline:\n            pipeline = self.generator_under_test.generate_pipeline(\"pipeline\", \"unittest-image\", \"Always\")\n            pipeline()\n\n            assert dsl_pipeline.ops[\"on-exit\"].execution_options.caching_strategy.max_cache_staleness == \"P0D\"\n\n    def create_generator(self, config=None, params=None, catalog=None):\n        if config is None:\n            config = {}\n        if params is None:\n            params = {}\n        if catalog is None:\n            catalog = {}\n        config_loader = MagicMock()\n        config_loader.get.return_value = catalog\n        context = type(\n            \"obj\",\n            (object,),\n            {\n                \"env\": \"unittests\",\n                \"params\": params,\n                \"config_loader\": config_loader,\n            },\n        )\n        self.pipelines_under_test = {\n            \"pipeline\": Pipeline(\n                [\n                    node(identity, \"A\", \"B\", name=\"node1\"),\n                    node(identity, \"B\", \"C\", name=\"node2\"),\n                ]\n            )\n        }\n        self.generator_under_test = OnePodPipelineGenerator(\n            config=PluginConfig(**self.minimal_config({\"host\": \"http://unittest\", \"run_config\": config})),\n            project_name=\"my-awesome-project\",\n            context=context,\n        )\n"
  },
  {
    "repo": "getindata/kedro-kubeflow",
    "file_path": "tests/test_pod_per_node_pipeline_generator.py",
    "raw_url": "https://raw.githubusercontent.com/getindata/kedro-kubeflow/develop/tests/test_pod_per_node_pipeline_generator.py",
    "content": "\"\"\"Test generator\"\"\"\n\nimport os\nimport unittest\nfrom inspect import signature\nfrom unittest.mock import MagicMock, patch\n\nimport kfp\nfrom kedro.pipeline import Pipeline, node\n\nfrom kedro_kubeflow.config import PluginConfig\nfrom kedro_kubeflow.generators.pod_per_node_pipeline_generator import (\n    PodPerNodePipelineGenerator,\n)\nfrom tests.common import MinimalConfigMixin\n\n\ndef identity(input1: str):\n    return input1  # pragma: no cover\n\n\nclass TestGenerator(unittest.TestCase, MinimalConfigMixin):\n    def test_support_modification_of_pull_policy(self):\n        # given\n        self.create_generator()\n\n        # when\n        with patch(\n            \"kedro.framework.project.pipelines\",\n            new=self.pipelines_under_test,\n        ):\n            pipeline = self.generator_under_test.generate_pipeline(\"pipeline\", \"unittest-image\", \"Never\")\n            with kfp.dsl.Pipeline(None) as dsl_pipeline:\n                pipeline()\n\n            # then\n            self.assertEqual(dsl_pipeline.ops[\"node1\"].container.image, \"unittest-image\")\n            assert dsl_pipeline.ops[\"node1\"].container.image_pull_policy == \"Never\"\n\n    def test_should_support_inter_steps_volume_with_defaults(self):\n        # given\n        self.create_generator(config={\"volume\": {}})\n\n        # when\n        with patch(\n            \"kedro.framework.project.pipelines\",\n            new=self.pipelines_under_test,\n        ):\n            pipeline = self.generator_under_test.generate_pipeline(\"pipeline\", \"unittest-image\", \"IfNotPresent\")\n            with kfp.dsl.Pipeline(None) as dsl_pipeline:\n                pipeline()\n\n            # then\n            self.assertEqual(len(dsl_pipeline.ops), 6)\n            assert \"on-exit\" in dsl_pipeline.ops\n            assert (\n                dsl_pipeline.ops[\"on-exit\"]\n                .container.command[-1]\n                .endswith(\"kedro kubeflow delete-pipeline-volume \" \"{{workflow.name}}-pipeline-data-volume\")\n            )\n            volume_spec = dsl_pipeline.ops[\"data-volume-create\"].k8s_resource.spec\n            self.assertEqual(volume_spec.resources.requests[\"storage\"], \"1Gi\")\n            self.assertEqual(volume_spec.access_modes, [\"ReadWriteOnce\"])\n            assert volume_spec.storage_class_name is None\n            volume_init_spec = dsl_pipeline.ops[\"data-volume-init\"].container\n            self.assertEqual(volume_init_spec.image, \"unittest-image\")\n            self.assertEqual(volume_init_spec.image_pull_policy, \"IfNotPresent\")\n            self.assertEqual(volume_init_spec.security_context.run_as_user, 0)\n            assert volume_init_spec.args[0].startswith(\"cp --verbose -r\")\n            for node_name in [\"data-volume-init\", \"node1\", \"node2\"]:\n                volumes = dsl_pipeline.ops[node_name].container.volume_mounts\n                self.assertEqual(len(volumes), 1)\n                self.assertEqual(volumes[0].name, \"data-volume-create\")\n                assert dsl_pipeline.ops[node_name].container.security_context.run_as_user == 0\n\n    def test_should_generate_on_exit_pipeline_run(self):\n        # given\n        self.create_generator(config={\"on_exit_pipeline\": \"notify_via_slack\"})\n\n        # when\n        with patch(\n            \"kedro.framework.project.pipelines\",\n            new=self.pipelines_under_test,\n        ):\n            pipeline = self.generator_under_test.generate_pipeline(\"pipeline\", \"unittest-image\", \"IfNotPresent\")\n            with kfp.dsl.Pipeline(None) as dsl_pipeline:\n                pipeline()\n\n            # then\n            assert \"on-exit\" in dsl_pipeline.ops\n            assert (\n                dsl_pipeline.ops[\"on-exit\"]\n                .container.command[-1]\n                .endswith(\"kedro run --config config.yaml \" \"--env unittests --pipeline notify_via_slack\")\n            )\n\n    def test_should_generate_volume_removal_and_on_exit_pipeline_run(self):\n        # given\n        self.create_generator(config={\"volume\": {}, \"on_exit_pipeline\": \"notify_via_slack\"})\n\n        # when\n        with patch(\n            \"kedro.framework.project.pipelines\",\n            new=self.pipelines_under_test,\n        ):\n            pipeline = self.generator_under_test.generate_pipeline(\"pipeline\", \"unittest-image\", \"IfNotPresent\")\n            with kfp.dsl.Pipeline(None) as dsl_pipeline:\n                pipeline()\n\n            # then\n            assert \"on-exit\" in dsl_pipeline.ops\n            assert (\n                dsl_pipeline.ops[\"on-exit\"]\n                .container.command[-1]\n                .endswith(\n                    \"kedro kubeflow delete-pipeline-volume \"\n                    \"{{workflow.name}}-pipeline-data-volume;\"\n                    \"kedro run --config config.yaml \"\n                    \"--env unittests --pipeline notify_via_slack\"\n                )\n            )\n\n    def test_should_support_inter_steps_volume_with_given_spec(self):\n        # given\n        self.create_generator(\n            config={\n                \"volume\": {\n                    \"storageclass\": \"nfs\",\n                    \"size\": \"1Mi\",\n                    \"access_modes\": [\"ReadWriteOnce\"],\n                }\n            }\n        )\n\n        # when\n        with patch(\n            \"kedro.framework.project.pipelines\",\n            new=self.pipelines_under_test,\n        ):\n            pipeline = self.generator_under_test.generate_pipeline(\"pipeline\", \"unittest-image\", \"Always\")\n            with kfp.dsl.Pipeline(None) as dsl_pipeline:\n                pipeline()\n\n            # then\n            self.assertEqual(len(dsl_pipeline.ops), 6)\n            assert \"on-exit\" in dsl_pipeline.ops\n            volume_spec = dsl_pipeline.ops[\"data-volume-create\"].k8s_resource.spec\n            self.assertEqual(volume_spec.resources.requests[\"storage\"], \"1Mi\")\n            self.assertEqual(volume_spec.access_modes, [\"ReadWriteOnce\"])\n            self.assertEqual(volume_spec.storage_class_name, \"nfs\")\n\n    def test_should_change_effective_user_if_to_volume_owner(self):\n        # given\n        self.create_generator(\n            config={\n                \"volume\": {\n                    \"storageclass\": \"nfs\",\n                    \"size\": \"1Mi\",\n                    \"access_modes\": [\"ReadWriteOnce\"],\n                    \"owner\": 47,\n                }\n            }\n        )\n\n        # when\n        with patch(\n            \"kedro.framework.project.pipelines\",\n            new=self.pipelines_under_test,\n        ):\n            pipeline = self.generator_under_test.generate_pipeline(\"pipeline\", \"unittest-image\", \"Always\")\n            with kfp.dsl.Pipeline(None) as dsl_pipeline:\n                pipeline()\n\n            # then\n            volume_init_spec = dsl_pipeline.ops[\"data-volume-init\"].container\n            self.assertEqual(volume_init_spec.security_context.run_as_user, 47)\n            for node_name in [\"data-volume-init\", \"node1\", \"node2\"]:\n                assert dsl_pipeline.ops[node_name].container.security_context.run_as_user == 47\n\n    def test_should_add_mlflow_init_step_if_enabled(self):\n        # given\n        self.create_generator()\n        self.mock_mlflow(True)\n\n        # when\n        with patch(\n            \"kedro.framework.project.pipelines\",\n            new=self.pipelines_under_test,\n        ):\n            pipeline = self.generator_under_test.generate_pipeline(\"pipeline\", \"unittest-image\", \"Always\")\n            with kfp.dsl.Pipeline(None) as dsl_pipeline:\n                pipeline()\n\n            # then\n            self.assertEqual(len(dsl_pipeline.ops), 4)\n            init_step = dsl_pipeline.ops[\"mlflow-start-run\"].container\n            self.assertEqual(init_step.image, \"unittest-image\")\n            self.assertEqual(\n                init_step.args,\n                [\n                    \"kubeflow\",\n                    \"--env\",\n                    \"unittests\",\n                    \"mlflow-start\",\n                    \"{{workflow.uid}}\",\n                ],\n            )\n            assert \"MLFLOW_RUN_ID\" not in {e.name for e in init_step.env}\n            for node_name in [\"node1\", \"node2\"]:\n                env = {e.name: e.value for e in dsl_pipeline.ops[node_name].container.env}\n                assert \"MLFLOW_RUN_ID\" in env\n                assert env[\"MLFLOW_RUN_ID\"] == \"{{pipelineparam:op=mlflow-start-run;name=mlflow_run_id}}\"\n\n    def test_should_skip_volume_init_if_requested(self):\n        # given\n        self.create_generator(config={\"volume\": {\"skip_init\": True}})\n\n        # when\n        with patch(\n            \"kedro.framework.project.pipelines\",\n            new=self.pipelines_under_test,\n        ):\n            pipeline = self.generator_under_test.generate_pipeline(\"pipeline\", \"unittest-image\", \"Always\")\n            with kfp.dsl.Pipeline(None) as dsl_pipeline:\n                pipeline()\n\n            # then\n            self.assertEqual(len(dsl_pipeline.ops), 5)\n            assert \"data-volume-create\" in dsl_pipeline.ops\n            assert \"on-exit\" in dsl_pipeline.ops\n            assert \"data-volume-init\" not in dsl_pipeline.ops\n            for node_name in [\"node1\", \"node2\"]:\n                volumes = dsl_pipeline.ops[node_name].container.volume_mounts\n                self.assertEqual(len(volumes), 1)\n                self.assertEqual(volumes[0].name, \"data-volume-create\")\n\n    def test_should_support_params_and_inject_them_to_the_nodes(self):\n        # given\n        self.create_generator(params={\"param1\": 0.3, \"param2\": 42})\n\n        # when\n        with patch(\n            \"kedro.framework.project.pipelines\",\n            new=self.pipelines_under_test,\n        ):\n            pipeline = self.generator_under_test.generate_pipeline(\"pipeline\", \"unittest-image\", \"Always\")\n            with kfp.dsl.Pipeline(None) as dsl_pipeline:\n                default_params = signature(pipeline).parameters\n                pipeline()\n\n            # then\n            self.assertEqual(len(default_params), 2)\n            self.assertEqual(default_params[\"param1\"].default, 0.3)\n            self.assertEqual(default_params[\"param2\"].default, 42)\n            for node_name in [\"node1\", \"node2\"]:\n                args = dsl_pipeline.ops[node_name].container.args\n                self.assertEqual(\n                    args,\n                    [\n                        \"_\",\n                        \"param1\",\n                        \"{{pipelineparam:op=;name=param1}}\",\n                        \"param2\",\n                        \"{{pipelineparam:op=;name=param2}}\",\n                    ],\n                )\n\n    def test_should_support_nested_params_and_inject_them_to_the_node(self):\n        # given\n        self.create_generator(\n            params={\n                \"param1\": {\"nested1\": {\"nested2\": 1, \"nested3\": 2}},\n                \"param2\": 42,\n            }\n        )\n\n        # when\n        with patch(\n            \"kedro.framework.project.pipelines\",\n            new=self.pipelines_under_test,\n        ):\n            with kfp.dsl.Pipeline(None) as dsl_pipeline:\n                pipeline = self.generator_under_test.generate_pipeline(\"pipeline\", \"unittest-image\", \"Always\")\n                default_params = signature(pipeline).parameters\n                pipeline()\n\n            # then\n            assert len(default_params) == 2\n            assert default_params[\"param1\"].default == {\"nested1\": {\"nested2\": 1, \"nested3\": 2}}\n            assert default_params[\"param2\"].default == 42\n            for node_name in [\"node1\", \"node2\", \"node3\"]:\n                assert dsl_pipeline.ops[node_name].container.args[1:] == [\n                    \"param1\",\n                    \"{{pipelineparam:op=;name=param1}}\",\n                    \"param2\",\n                    \"{{pipelineparam:op=;name=param2}}\",\n                ]\n\n    def test_should_support_namespaced_params_and_inject_them_to_the_node(\n        self,\n    ):\n        # given\n        self.create_generator(\n            params={\n                \"outer_namespace.inner_namespace1.param1\": \"outer_namespace.inner_namespace1.param1_v\",\n                \"outer_namespace.inner_namespace1.param2\": \"outer_namespace.inner_namespace1.param2_v\",\n                \"outer_namespace.inner_namespace2.param1\": \"outer_namespace.inner_namespace2.param1_v\",\n                \"outer_namespace.inner_namespace2.param2\": \"outer_namespace.inner_namespace2.param2_v\",\n                \"outer_namespace.param\": \"outer_namespace.param\",\n                \"param1\": 42,\n            }\n        )\n\n        # when\n        with patch(\n            \"kedro.framework.project.pipelines\",\n            new=self.pipelines_under_test,\n        ):\n            with kfp.dsl.Pipeline(None) as dsl_pipeline:\n                pipeline = self.generator_under_test.generate_pipeline(\"pipeline\", \"unittest-image\", \"Always\")\n                default_params = signature(pipeline).parameters\n                pipeline()\n\n            # then\n            assert len(default_params) == 2\n            assert default_params[\"outer_namespace\"].default == {\n                \"inner_namespace1\": {\n                    \"param1\": \"outer_namespace.inner_namespace1.param1_v\",\n                    \"param2\": \"outer_namespace.inner_namespace1.param2_v\",\n                },\n                \"inner_namespace2\": {\n                    \"param1\": \"outer_namespace.inner_namespace2.param1_v\",\n                    \"param2\": \"outer_namespace.inner_namespace2.param2_v\",\n                },\n                \"param\": \"outer_namespace.param\",\n            }\n            assert default_params[\"param1\"].default == 42\n            for node_name in [\"node1\", \"node2\", \"node3\"]:\n                assert dsl_pipeline.ops[node_name].container.args[1:] == [\n                    \"outer_namespace\",\n                    \"{{pipelineparam:op=;name=outer_namespace}}\",\n                    \"param1\",\n                    \"{{pipelineparam:op=;name=param1}}\",\n                ]\n\n    def test_should_fallbackto_default_resources_spec_if_not_requested(self):\n        # given\n        self.create_generator(config={})\n\n        # when\n        with patch(\n            \"kedro.framework.project.pipelines\",\n            new=self.pipelines_under_test,\n        ):\n            pipeline = self.generator_under_test.generate_pipeline(\"pipeline\", \"unittest-image\", \"Always\")\n            with kfp.dsl.Pipeline(None) as dsl_pipeline:\n                pipeline()\n\n            # then\n            for node_name in [\"node1\", \"node2\"]:\n                spec = dsl_pipeline.ops[node_name].container\n                assert spec.resources is not None\n\n    def test_should_add_resources_spec(self):\n        # given\n        self.create_generator(\n            config={\n                \"resources\": {\n                    \"__default__\": {\"cpu\": \"100m\"},\n                    \"node1\": {\"cpu\": \"400m\", \"memory\": \"64Gi\"},\n                    \"node3\": {\"memory\": \"32Gi\", \"nvidia.com/gpu\": \"1\"},\n                }\n            }\n        )\n\n        # when\n        with patch(\n            \"kedro.framework.project.pipelines\",\n            new=self.pipelines_under_test,\n        ):\n            pipeline = self.generator_under_test.generate_pipeline(\"pipeline\", \"unittest-image\", \"Always\")\n            with kfp.dsl.Pipeline(None) as dsl_pipeline:\n                pipeline()\n\n            # then\n            node1_spec = dsl_pipeline.ops[\"node1\"].container.resources\n            node2_spec = dsl_pipeline.ops[\"node2\"].container.resources\n            node3_spec = dsl_pipeline.ops[\"node3\"].container.resources\n            for spec, result in zip(\n                (node1_spec, node2_spec, node3_spec),\n                (\n                    {\"cpu\": \"400m\", \"memory\": \"64Gi\"},\n                    {\"cpu\": \"100m\"},\n                    {\"cpu\": \"100m\", \"memory\": \"32Gi\", \"nvidia.com/gpu\": \"1\"},\n                ),\n            ):\n                for precise_spec in (spec.limits, spec.requests):\n                    self.assertDictEqual(precise_spec, result)\n            # self.assertEqual(node1_spec.limits , {\"cpu\": \"400m\", \"memory\": \"64Gi\"})\n            # self.assertEqual(node1_spec.requests , {\"cpu\": \"400m\", \"memory\": \"64Gi\"})\n            # self.assertEqual(node2_spec.limits , {\"cpu\": \"100m\"})\n            # self.assertEqual(node2_spec.requests , {\"cpu\": \"100m\"})\n\n    def test_can_add_extra_volumes(self):\n        self.create_generator(\n            config={\n                \"extra_volumes\": {\n                    \"node1\": [\n                        {\n                            \"mount_path\": \"/my/volume\",\n                            \"volume\": {\n                                \"name\": \"my_volume\",\n                                \"empty_dir\": {\n                                    \"cls\": \"V1EmptyDirVolumeSource\",\n                                    \"params\": {\"medium\": \"Memory\"},\n                                },\n                            },\n                        }\n                    ]\n                }\n            }\n        )\n\n        with patch(\n            \"kedro.framework.project.pipelines\",\n            new=self.pipelines_under_test,\n        ):\n            pipeline = self.generator_under_test.generate_pipeline(\"pipeline\", \"unittest-image\", \"Always\")\n            with kfp.dsl.Pipeline(None) as dsl_pipeline:\n                pipeline()\n\n            volume_mounts = dsl_pipeline.ops[\"node1\"].container.volume_mounts\n            self.assertEqual(len(volume_mounts), 1)\n\n    def test_should_not_add_retry_policy_if_not_requested(self):\n        # given\n        self.create_generator(config={})\n\n        # when\n        with patch(\n            \"kedro.framework.project.pipelines\",\n            new=self.pipelines_under_test,\n        ):\n            pipeline = self.generator_under_test.generate_pipeline(\"pipeline\", \"unittest-image\", \"Always\")\n            with kfp.dsl.Pipeline(None) as dsl_pipeline:\n                pipeline()\n\n            # then\n            for node_name in [\"node1\", \"node2\"]:\n                op = dsl_pipeline.ops[node_name]\n                self.assertEqual(op.num_retries, 0)\n                assert op.retry_policy is None\n                assert op.backoff_factor is None\n                assert op.backoff_duration is None\n                assert op.backoff_max_duration is None\n\n    def test_should_add_retry_policy(self):\n        # given\n        self.create_generator(\n            config={\n                \"retry_policy\": {\n                    \"__default__\": {\n                        \"num_retries\": 4,\n                        \"backoff_duration\": \"60s\",\n                        \"backoff_factor\": 2,\n                    },\n                    \"node1\": {\n                        \"num_retries\": 100,\n                        \"backoff_duration\": \"5m\",\n                        \"backoff_factor\": 1,\n                    },\n                }\n            }\n        )\n\n        # when\n        with patch(\n            \"kedro.framework.project.pipelines\",\n            new=self.pipelines_under_test,\n        ):\n            pipeline = self.generator_under_test.generate_pipeline(\"pipeline\", \"unittest-image\", \"Always\")\n            with kfp.dsl.Pipeline(None) as dsl_pipeline:\n                pipeline()\n\n            # then\n            op1 = dsl_pipeline.ops[\"node1\"]\n            self.assertEqual(op1.num_retries, 100)\n            self.assertEqual(op1.retry_policy, \"Always\")\n            self.assertEqual(op1.backoff_factor, 1)\n            self.assertEqual(op1.backoff_duration, \"5m\")\n            assert op1.backoff_max_duration is None\n            op2 = dsl_pipeline.ops[\"node2\"]\n            self.assertEqual(op2.num_retries, 4)\n            self.assertEqual(op2.retry_policy, \"Always\")\n            self.assertEqual(op2.backoff_factor, 2)\n            self.assertEqual(op2.backoff_duration, \"60s\")\n            assert op2.backoff_max_duration is None\n\n    def test_should_add_max_cache_staleness(self):\n        self.create_generator(config={\"max_cache_staleness\": \"P0D\"})\n\n        with patch(\n            \"kedro.framework.project.pipelines\",\n            new=self.pipelines_under_test,\n        ):\n            pipeline = self.generator_under_test.generate_pipeline(\"pipeline\", \"unittest-image\", \"Always\")\n            with kfp.dsl.Pipeline(None) as dsl_pipeline:\n                pipeline()\n\n            op1 = dsl_pipeline.ops[\"node1\"]\n            assert op1.execution_options.caching_strategy.max_cache_staleness == \"P0D\"\n\n    def test_should_set_description(self):\n        # given\n        self.create_generator(config={\"description\": \"DESC\"})\n\n        # when\n        with patch(\n            \"kedro.framework.project.pipelines\",\n            new=self.pipelines_under_test,\n        ):\n            pipeline = self.generator_under_test.generate_pipeline(\"pipeline\", \"unittest-image\", \"Never\")\n\n            # then\n            self.assertEqual(pipeline._component_description, \"DESC\")\n\n    def test_artifact_registration(self):\n        # given\n        self.create_generator(\n            catalog={\n                \"B\": {\n                    \"type\": \"pandas.CSVDataSet\",\n                    \"filepath\": \"data/02_intermediate/b.csv\",\n                }\n            }\n        )\n\n        # when\n        with patch(\n            \"kedro.framework.project.pipelines\",\n            new=self.pipelines_under_test,\n        ):\n            pipeline = self.generator_under_test.generate_pipeline(\"pipeline\", \"unittest-image\", \"Always\")\n            with kfp.dsl.Pipeline(None) as dsl_pipeline:\n                pipeline()\n\n            # then\n            outputs1 = dsl_pipeline.ops[\"node1\"].file_outputs\n            self.assertEqual(len(outputs1), 1)\n            assert \"B\" in outputs1\n            self.assertEqual(outputs1[\"B\"], \"/home/kedro/data/02_intermediate/b.csv\")\n            outputs2 = dsl_pipeline.ops[\"node2\"].file_outputs\n            self.assertEqual(len(outputs2), 0)  # output \"C\" is missing in the catalog)\n\n    def test_should_skip_artifact_registration_if_requested(self):\n        # given\n        self.create_generator(\n            catalog={\n                \"B\": {\n                    \"type\": \"pandas.CSVDataSet\",\n                    \"filepath\": \"data/02_intermediate/b.csv\",\n                }\n            },\n            config={\"store_kedro_outputs_as_kfp_artifacts\": False},\n        )\n\n        # when\n        with patch(\n            \"kedro.framework.project.pipelines\",\n            new=self.pipelines_under_test,\n        ):\n            pipeline = self.generator_under_test.generate_pipeline(\"pipeline\", \"unittest-image\", \"Always\")\n            with kfp.dsl.Pipeline(None) as dsl_pipeline:\n                pipeline()\n\n            # then\n            outputs1 = dsl_pipeline.ops[\"node1\"].file_outputs\n            self.assertEqual(len(outputs1), 0)\n\n    def test_should_skip_volume_removal_if_requested(self):\n        # given\n        self.create_generator(config={\"volume\": {\"keep\": True}})\n\n        # when\n        with patch(\n            \"kedro.framework.project.pipelines\",\n            new=self.pipelines_under_test,\n        ):\n            pipeline = self.generator_under_test.generate_pipeline(\"pipeline\", \"unittest-image\", \"Always\")\n            with kfp.dsl.Pipeline(None) as dsl_pipeline:\n                pipeline()\n\n            # then\n            assert \"schedule-volume-termination\" not in dsl_pipeline.ops\n\n    def test_should_pass_kedro_config_env_to_nodes(self):\n        # given\n        self.create_generator()\n        os.environ[\"KEDRO_CONFIG_MY_KEY\"] = \"42\"\n        os.environ[\"SOME_VALUE\"] = \"100\"\n\n        try:\n            # when\n            with patch(\n                \"kedro.framework.project.pipelines\",\n                new=self.pipelines_under_test,\n            ):\n                pipeline = self.generator_under_test.generate_pipeline(\"pipeline\", \"unittest-image\", \"Always\")\n                with kfp.dsl.Pipeline(None) as dsl_pipeline:\n                    pipeline()\n\n                # then\n                for node_name in [\"node1\", \"node2\"]:\n                    env_values = {e.name: e.value for e in dsl_pipeline.ops[node_name].container.env}\n                    assert \"KEDRO_CONFIG_MY_KEY\" in env_values\n                    self.assertEqual(env_values[\"KEDRO_CONFIG_MY_KEY\"], \"42\")\n                    assert \"SOME_VALUE\" not in env_values\n        finally:\n            del os.environ[\"KEDRO_CONFIG_MY_KEY\"]\n            del os.environ[\"SOME_VALUE\"]\n\n    def create_generator(self, config=None, params=None, catalog=None):\n        project_name = \"my-awesome-project\"\n        config_loader = MagicMock()\n        config_loader.get.return_value = catalog or {}\n        context = type(\n            \"obj\",\n            (object,),\n            {\n                \"env\": \"unittests\",\n                \"params\": params or {},\n                \"config_loader\": config_loader,\n            },\n        )\n        self.pipelines_under_test = {\n            \"pipeline\": Pipeline(\n                [\n                    node(identity, \"A\", \"B\", name=\"node1\"),\n                    node(identity, \"B\", \"C\", name=\"node2\"),\n                    node(identity, \"B\", \"D\", name=\"node3\"),\n                ]\n            )\n        }\n        self.generator_under_test = PodPerNodePipelineGenerator(\n            PluginConfig(**self.minimal_config({\"host\": \"http://unittest\", \"run_config\": config or {}})),\n            project_name,\n            context,\n        )\n\n    def mock_mlflow(self, enabled=False):\n        def fakeimport(name, *args, **kw):\n            if not enabled and name == \"mlflow\":\n                raise ImportError\n            return self.realimport(name, *args, **kw)\n\n        __builtins__[\"__import__\"] = fakeimport\n\n    def setUp(self):\n        self.realimport = __builtins__[\"__import__\"]\n        self.mock_mlflow(False)\n\n    def tearDown(self):\n        __builtins__[\"__import__\"] = self.realimport\n"
  },
  {
    "repo": "getindata/kedro-kubeflow",
    "file_path": "kedro_kubeflow/generators/one_pod_pipeline_generator.py",
    "raw_url": "https://raw.githubusercontent.com/getindata/kedro-kubeflow/develop/kedro_kubeflow/generators/one_pod_pipeline_generator.py",
    "content": "import logging\n\nfrom kedro.framework.context import KedroContext\nfrom kfp import dsl\n\nfrom ..utils import clean_name\nfrom .utils import (\n    create_arguments_from_parameters,\n    create_command_using_params_dumper,\n    create_container_environment,\n    create_pipeline_exit_handler,\n    customize_op,\n    is_local_fs,\n    maybe_add_params,\n    merge_namespaced_params_to_dict,\n)\n\n\nclass OnePodPipelineGenerator(object):\n    log = logging.getLogger(__name__)\n\n    def __init__(self, config, project_name, context):\n        self.project_name = project_name\n        self.context: KedroContext = context\n        dsl.ContainerOp._DISABLE_REUSABLE_COMPONENT_WARNING = True\n        self.run_config = config.run_config\n        self.catalog = context.config_loader.get(\"catalog\")\n\n    def generate_pipeline(self, pipeline, image, image_pull_policy):\n        merged_params = merge_namespaced_params_to_dict(self.context.params)\n\n        @dsl.pipeline(self.project_name, self.run_config.description)\n        @maybe_add_params(merged_params)\n        def convert_kedro_pipeline_to_kfp() -> None:\n            dsl.get_pipeline_conf().set_ttl_seconds_after_finished(self.run_config.ttl)\n            with create_pipeline_exit_handler(\n                pipeline,\n                image,\n                image_pull_policy,\n                self.run_config,\n                self.context,\n            ):\n                self._build_kfp_op(pipeline, merged_params, image, image_pull_policy)\n\n        return convert_kedro_pipeline_to_kfp\n\n    def _build_kfp_op(\n        self,\n        pipeline,\n        params,\n        image,\n        image_pull_policy,\n    ) -> dsl.ContainerOp:\n        container_op = dsl.ContainerOp(\n            name=clean_name(pipeline),\n            image=image,\n            command=create_command_using_params_dumper(\n                \"kedro \" \"run \" f\"--env {self.context.env} \" f\"--pipeline {pipeline} \" f\"--config config.yaml\"\n            ),\n            arguments=create_arguments_from_parameters(params.keys()),\n            container_kwargs={\"env\": create_container_environment()},\n            file_outputs={\n                output: f\"/home/kedro/{self.catalog[output]['filepath']}\"\n                for output in self.catalog\n                if \"filepath\" in self.catalog[output]\n                and is_local_fs(self.catalog[output][\"filepath\"])\n                and self.run_config.store_kedro_outputs_as_kfp_artifacts\n            },\n        )\n\n        container_op.execution_options.caching_strategy.max_cache_staleness = self.run_config.max_cache_staleness\n\n        return customize_op(container_op, image_pull_policy, self.run_config)\n"
  },
  {
    "repo": "getindata/kedro-kubeflow",
    "file_path": "kedro_kubeflow/generators/pod_per_node_pipeline_generator.py",
    "raw_url": "https://raw.githubusercontent.com/getindata/kedro-kubeflow/develop/kedro_kubeflow/generators/pod_per_node_pipeline_generator.py",
    "content": "import logging\nfrom typing import Dict, Set\n\nimport kubernetes.client as k8s\nfrom kedro.framework.context import KedroContext\nfrom kedro.pipeline.node import Node\nfrom kfp import dsl\n\nfrom ..utils import clean_name, is_mlflow_enabled\nfrom .utils import (\n    create_arguments_from_parameters,\n    create_command_using_params_dumper,\n    create_container_environment,\n    create_pipeline_exit_handler,\n    customize_op,\n    is_local_fs,\n    maybe_add_params,\n    merge_namespaced_params_to_dict,\n)\n\n\nclass PodPerNodePipelineGenerator(object):\n    log = logging.getLogger(__name__)\n\n    def __init__(self, config, project_name, context):\n        self.project_name = project_name\n        self.context: KedroContext = context\n        dsl.ContainerOp._DISABLE_REUSABLE_COMPONENT_WARNING = True\n        self.run_config = config.run_config\n        self.catalog = context.config_loader.get(\"catalog\")\n\n    def configure_max_cache_staleness(self, kfp_ops):\n        if self.run_config.max_cache_staleness not in [None, \"\"]:\n            for _, op in kfp_ops.items():\n                op.execution_options.caching_strategy.max_cache_staleness = self.run_config.max_cache_staleness\n\n    def generate_pipeline(self, pipeline, image, image_pull_policy):\n        merged_params = merge_namespaced_params_to_dict(self.context.params)\n\n        @dsl.pipeline(\n            name=self.project_name,\n            description=self.run_config.description,\n        )\n        @maybe_add_params(merged_params)\n        def convert_kedro_pipeline_to_kfp() -> None:\n            \"\"\"Convert from a Kedro pipeline into a kfp container graph.\"\"\"\n\n            from kedro.framework.project import pipelines  # NOQA\n\n            dsl.get_pipeline_conf().set_ttl_seconds_after_finished(self.run_config.ttl)\n            node_dependencies = pipelines[pipeline].node_dependencies\n            with create_pipeline_exit_handler(\n                pipeline,\n                image,\n                image_pull_policy,\n                self.run_config,\n                self.context,\n            ):\n                kfp_ops = self._build_kfp_ops(\n                    pipeline,\n                    merged_params,\n                    node_dependencies,\n                    image,\n                    image_pull_policy,\n                )\n\n                self.configure_max_cache_staleness(kfp_ops)\n                for node, dependencies in node_dependencies.items():\n                    for dependency in dependencies:\n                        kfp_ops[node.name].after(kfp_ops[dependency.name])\n\n        return convert_kedro_pipeline_to_kfp\n\n    def _build_kfp_ops(\n        self,\n        pipeline,\n        params,\n        node_dependencies: Dict[Node, Set[Node]],\n        image,\n        image_pull_policy,\n    ) -> Dict[str, dsl.ContainerOp]:\n        \"\"\"Build kfp container graph from Kedro node dependencies.\"\"\"\n        kfp_ops = {}\n\n        node_volumes = (\n            self._setup_volumes(f\"{pipeline}-data-volume\", image, image_pull_policy)\n            if self.run_config.volume is not None\n            else {}\n        )\n\n        nodes_env = create_container_environment()\n\n        if is_mlflow_enabled():\n            kfp_ops[\"mlflow-start-run\"] = customize_op(\n                dsl.ContainerOp(\n                    name=\"mlflow-start-run\",\n                    image=image,\n                    command=[\"kedro\"],\n                    arguments=[\n                        \"kubeflow\",\n                        \"--env\",\n                        self.context.env,\n                        \"mlflow-start\",\n                        dsl.RUN_ID_PLACEHOLDER,\n                    ],\n                    container_kwargs={\"env\": nodes_env.copy()},\n                    file_outputs={\"mlflow_run_id\": \"/tmp/mlflow_run_id\"},\n                ),\n                image_pull_policy,\n                self.run_config,\n            )\n\n            nodes_env.append(\n                k8s.V1EnvVar(\n                    name=\"MLFLOW_RUN_ID\",\n                    value=kfp_ops[\"mlflow-start-run\"].output,\n                )\n            )\n\n        for node in node_dependencies:\n            name = clean_name(node.name)\n            kfp_ops[node.name] = customize_op(\n                dsl.ContainerOp(\n                    name=name,\n                    image=image,\n                    command=create_command_using_params_dumper(\n                        \"kedro \"\n                        \"run \"\n                        f\"--env {self.context.env} \"\n                        f\"--pipeline {pipeline} \"\n                        f\"--nodes {node.name} \"\n                        f\"--config config.yaml\"\n                    ),\n                    arguments=create_arguments_from_parameters(params.keys()),\n                    pvolumes=node_volumes,\n                    container_kwargs={\"env\": nodes_env},\n                    file_outputs={\n                        output: \"/home/kedro/\" + self.catalog[output][\"filepath\"]\n                        for output in node.outputs\n                        if output in self.catalog\n                        and \"filepath\" in self.catalog[output]\n                        and is_local_fs(self.catalog[output][\"filepath\"])\n                        and self.run_config.store_kedro_outputs_as_kfp_artifacts\n                    },\n                ),\n                image_pull_policy,\n                self.run_config,\n            )\n\n        return kfp_ops\n\n    def _setup_volumes(self, volume_name, image, image_pull_policy):\n        vop = dsl.VolumeOp(\n            name=\"data-volume-create\",\n            resource_name=volume_name,\n            size=self.run_config.volume.size,\n            modes=self.run_config.volume.access_modes,\n            storage_class=self.run_config.volume.storageclass,\n        )\n\n        if self.run_config.max_cache_staleness not in [None, \"\"]:\n            vop.add_pod_annotation(\n                \"pipelines.kubeflow.org/max_cache_staleness\",\n                self.run_config.max_cache_staleness,\n            )\n\n        if self.run_config.volume.skip_init:\n            return {\"/home/kedro/data\": vop.volume}\n        else:\n            volume_init = customize_op(\n                dsl.ContainerOp(\n                    name=\"data-volume-init\",\n                    image=image,\n                    command=[\"sh\", \"-c\"],\n                    arguments=[\n                        \" \".join(\n                            [\n                                \"cp\",\n                                \"--verbose\",\n                                \"-r\",\n                                \"/home/kedro/data/*\",\n                                \"/home/kedro/datavolume\",\n                            ]\n                        )\n                    ],\n                    pvolumes={\"/home/kedro/datavolume\": vop.volume},\n                ),\n                image_pull_policy,\n                self.run_config,\n            )\n            return {\"/home/kedro/data\": volume_init.pvolume}\n"
  },
  {
    "repo": "sbakiu/kubeflow-spark",
    "file_path": "kubeflow_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/sbakiu/kubeflow-spark/main/kubeflow_pipeline.py",
    "content": "import json\nimport time\nimport yaml\n\nimport kfp.components as comp\nimport kfp.dsl as dsl\n\nSPARK_COMPLETED_STATE = \"COMPLETED\"\nSPARK_APPLICATION_KIND = \"sparkapplications\"\n\n\ndef get_spark_job_definition():\n    \"\"\"\n    Read Spark Operator job manifest file and return the corresponding dictionary and\n    add some randomness in the job name\n    :return: dictionary defining the spark job\n    \"\"\"\n    # Read manifest file\n    with open(\"spark-job-python.yaml\", \"r\") as stream:\n        spark_job_manifest = yaml.safe_load(stream)\n\n    # Add epoch time in the job name\n    epoch = int(time.time())\n    spark_job_manifest[\"metadata\"][\"name\"] = spark_job_manifest[\"metadata\"][\"name\"].format(epoch=epoch)\n\n    return spark_job_manifest\n\n\ndef print_op(msg):\n    \"\"\"\n    Op to print a message.\n    \"\"\"\n    return dsl.ContainerOp(\n        name=\"Print message.\",\n        image=\"alpine:3.6\",\n        command=[\"echo\", msg],\n    )\n\n\n@dsl.graph_component  # Graph component decorator is used to annotate recursive functions\ndef graph_component_spark_app_status(input_application_name):\n    k8s_get_op = comp.load_component_from_file(\"k8s-get-component.yaml\")\n    check_spark_application_status_op = k8s_get_op(\n        name=input_application_name,\n        kind=SPARK_APPLICATION_KIND\n    )\n    # Remove cache\n    check_spark_application_status_op.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n\n    time.sleep(5)\n    with dsl.Condition(check_spark_application_status_op.outputs[\"applicationstate\"] != SPARK_COMPLETED_STATE):\n        graph_component_spark_app_status(check_spark_application_status_op.outputs[\"name\"])\n\n\n@dsl.pipeline(\n    name=\"Spark Operator job pipeline\",\n    description=\"Spark Operator job pipeline\"\n)\ndef spark_job_pipeline():\n\n    # Load spark job manifest\n    spark_job_definition = get_spark_job_definition()\n\n    # Load the kubernetes apply component\n    k8s_apply_op = comp.load_component_from_file(\"k8s-apply-component.yaml\")\n\n    # Execute the apply command\n    spark_job_op = k8s_apply_op(object=json.dumps(spark_job_definition))\n\n    # Fetch spark job name\n    spark_job_name = spark_job_op.outputs[\"name\"]\n\n    # Remove cache for the apply operator\n    spark_job_op.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n\n    spark_application_status_op = graph_component_spark_app_status(spark_job_op.outputs[\"name\"])\n    spark_application_status_op.after(spark_job_op)\n\n    print_message = print_op(f\"Job {spark_job_name} is completed.\")\n    print_message.after(spark_application_status_op)\n    print_message.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n\n\nif __name__ == \"__main__\":\n    # Compile the pipeline\n    import kfp.compiler as compiler\n    import logging\n    logging.basicConfig(level=logging.INFO)\n    pipeline_func = spark_job_pipeline\n    pipeline_filename = pipeline_func.__name__ + \".yaml\"\n    compiler.Compiler().compile(pipeline_func, pipeline_filename)\n    logging.info(f\"Generated pipeline file: {pipeline_filename}.\")\n"
  },
  {
    "repo": "FernandoLpz/Kubeflow_Pipelines",
    "file_path": "pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/FernandoLpz/Kubeflow_Pipelines/master/pipeline.py",
    "content": "import kfp\nfrom kfp import dsl\nfrom kfp.components import func_to_container_op\n\n@func_to_container_op\ndef show_results(decision_tree : float, logistic_regression : float) -> None:\n    # Given the outputs from decision_tree and logistic regression components\n    # the results are shown.\n\n    print(f\"Decision tree (accuracy): {decision_tree}\")\n    print(f\"Logistic regression (accuracy): {logistic_regression}\")\n\n\n@dsl.pipeline(name='First Pipeline', description='Applies Decision Tree and Logistic Regression for classification problem.')\ndef first_pipeline():\n\n    # Loads the yaml manifest for each component\n    download = kfp.components.load_component_from_file('download_data/download_data.yaml')\n    decision_tree = kfp.components.load_component_from_file('decision_tree/decision_tree.yaml')\n    logistic_regression = kfp.components.load_component_from_file('logistic_regression/logistic_regression.yaml')\n\n    # Run download_data task\n    download_task = download()\n\n    # Run tasks \"decison_tree\" and \"logistic_regression\" given\n    # the output generated by \"download_task\".\n    decision_tree_task = decision_tree(download_task.output)\n    logistic_regression_task = logistic_regression(download_task.output)\n\n    # Given the outputs from \"decision_tree\" and \"logistic_regression\"\n    # the component \"show_results\" is called to print the results.\n    show_results(decision_tree_task.output, logistic_regression_task.output)\n\n\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(first_pipeline, 'FirstPipeline.yaml')\n    # kfp.Client().create_run_from_pipeline_func(basic_pipeline, arguments={})"
  },
  {
    "repo": "ciandt-d1/chicago-taxi-forecast",
    "file_path": "code/pipeline/build_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/ciandt-d1/chicago-taxi-forecast/master/code/pipeline/build_pipeline.py",
    "content": "# -*- coding: utf-8 -*-\n\nimport kfp.dsl as dsl\nimport kfp.gcp as gcp\nimport os\n\n\n@dsl.pipeline(\n    name='Time-Series-Forecast for Chicago Taxi dataset',\n    description='A pipeline to preprocess, train, deploy and measure metrics for a time series forecast problem.'\n)\ndef chicago_taxi_pipeline(\n        artifacts_dir=\"gs://ciandt-cognitive-sandbox-chicago-taxi-demo-bucket/{{workflow.uid}}/artifacts\",\n        model_dir=\"gs://ciandt-cognitive-sandbox-chicago-taxi-demo-bucket/{{workflow.uid}}/models\",\n        project_id=\"ciandt-cognitive-sandbox\",\n        start_date=\"2019-04-01\",\n        end_date=\"2019-04-30\",\n        split_date=\"2019-04-20\",\n        window_size=24,  # hours\n        model_name=\"rnn_v1\",\n        deployed_model_name=\"chicago_taxi_forecast\",\n        dataflow_runner=\"DirectRunner\",\n        epochs=10,\n        train_batch_size=128,\n        prediction_batch_size=512,\n        gpu_mem_usage=\"0.9\"):\n    \"\"\"\n    Pipeline with 7 stages:\n      1. Extract and transform input data from BigQuery into tfrecords\n      2. Data Validation\n      3. Train NN\n      4. Deploy NN on CMLE\n      5. Make predictions\n      6. Evaluation\n      7. Plot time series\n    \"\"\"\n\n    temp_dir = os.path.join(str(artifacts_dir), \"temp\")\n\n    read_metadata = dsl.ContainerOp(name='read_metadata',\n                                    image='gcr.io/ciandt-cognitive-sandbox/chicago-taxi-forecast/preproc:latest',\n                                    command=[\n                                        \"python\", \"/app/read_metadata.py\"],\n                                    arguments=[\n                                        \"--tfx-artifacts-dir\", artifacts_dir,\n                                        \"--project\", project_id,\n                                        \"--start-date\", start_date,\n                                        \"--end-date\", end_date,\n                                        \"--split-date\", split_date,\n                                        \"--temp-dir\", temp_dir,\n                                        \"--runner\", dataflow_runner\n                                    ],\n                                    file_outputs={\n                                        \"community_area_list_path\": \"/community_area_list_path.txt\",\n                                        \"znorm_stats_path\": \"/znorm_stats_path.txt\",\n                                    }\n                                    ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n\n    bq2tfrecord = dsl.ContainerOp(name='bq2tfrecord',\n                                  image='gcr.io/ciandt-cognitive-sandbox/chicago-taxi-forecast/preproc:latest',\n                                  command=[\"python\", \"/app/bq2tfrecord.py\"],\n                                  arguments=[\n                                      \"--tfrecord-dir\", artifacts_dir,\n                                      \"--tfx-artifacts-dir\", artifacts_dir,\n                                      \"--project\", project_id,\n                                      \"--window-size\", window_size,\n                                      \"--start-date\", start_date,\n                                      \"--end-date\", end_date,\n                                      \"--split-date\", split_date,\n                                      \"--community-area-list-path\", read_metadata.outputs[\"community_area_list_path\"],\n                                      \"--znorm-stats-path\", read_metadata.outputs[\"znorm_stats_path\"],\n                                      \"--temp-dir\", temp_dir,\n                                      \"--runner\", dataflow_runner\n                                  ],\n                                  file_outputs={\n                                      \"train_tfrecord_path\": \"/train_tfrecord_path.txt\",\n                                      \"eval_tfrecord_path\": \"/eval_tfrecord_path.txt\",\n                                      \"eval_raw_tfrecord_path\": \"/eval_raw_tfrecord_path.txt\",\n                                      \"znorm_stats\": \"/znorm_stats.txt\",\n                                      \"n_areas\": \"/n_areas.txt\",\n                                      \"n_windows_train\": \"/n_windows_train.txt\",\n                                      \"n_windows_eval\": \"/n_windows_eval.txt\",\n                                      \"tft_artifacts_dir\": \"/tft_artifacts_dir.txt\"\n                                  }\n                                  ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n\n    data_validation = dsl.ContainerOp(\n        name='data_validation',\n        image='gcr.io/ciandt-cognitive-sandbox/chicago-taxi-forecast/data-validation:latest',\n        command=[\"python3\", \"/app/data_validation.py\"],\n        arguments=[\n            \"--input-data-path\", bq2tfrecord.outputs[\"eval_raw_tfrecord_path\"],\n            \"--output-dir\", artifacts_dir\n        ],\n        file_outputs={\n            \"schema\": \"/schema.txt\"\n        },\n        output_artifact_paths={\n            'mlpipeline-ui-metadata': '/mlpipeline-ui-metadata.json'\n        }\n    ).apply(gcp.use_gcp_secret('user-gcp-sa')).after(bq2tfrecord)\n\n    train = dsl.ContainerOp(\n        name='train',\n        image='gcr.io/ciandt-cognitive-sandbox/chicago-taxi-forecast/train:latest',\n        command=[\"python3\", \"/app/train.py\"],\n        arguments=[\n            \"--tfrecord-file-train\", bq2tfrecord.outputs[\"train_tfrecord_path\"],\n            \"--tfrecord-file-eval\", bq2tfrecord.outputs[\"eval_tfrecord_path\"],\n            \"--tft-artifacts-dir\", bq2tfrecord.outputs[\"tft_artifacts_dir\"],\n            \"--model-name\", model_name,\n            \"--n-windows-train\", bq2tfrecord.outputs[\"n_windows_train\"],\n            \"--n-windows-eval\", bq2tfrecord.outputs[\"n_windows_eval\"],\n            \"--window-size\", window_size,\n            \"--n-areas\", bq2tfrecord.outputs[\"n_areas\"],\n            \"--epochs\", epochs,\n            \"--batch-size\", train_batch_size,\n            \"--output-dir\", model_dir,\n            \"--gpu-memory-fraction\", gpu_mem_usage\n        ],\n        file_outputs={\n            \"saved_model_path\": \"/saved_model_path.txt\"\n        },\n        output_artifact_paths={\n            'mlpipeline-ui-metadata': '/mlpipeline-ui-metadata.json'\n        }\n    ).apply(gcp.use_gcp_secret('user-gcp-sa')).after(bq2tfrecord)  # .set_gpu_limit(1)\n\n    deploy = dsl.ContainerOp(\n        name='deploy',\n        image='gcr.io/ciandt-cognitive-sandbox/chicago-taxi-forecast/deploy:latest',\n        command=[\"python\", \"/app/deploy_cmle.py\"],\n        arguments=[\n            \"--project\", project_id,\n            \"--gcs-path\", train.outputs[\"saved_model_path\"],\n            \"--model-name\", deployed_model_name\n        ]\n    ).apply(gcp.use_gcp_secret('user-gcp-sa')).after(train)  # .set_gpu_limit(1)\n\n    predictions = dsl.ContainerOp(\n        name='predictions',\n        image='gcr.io/ciandt-cognitive-sandbox/chicago-taxi-forecast/evaluate:latest',\n        command=[\"python\", \"/app/make_predictions.py\"],\n        arguments=[\n            \"--model-name\", deployed_model_name,\n            \"--project\", project_id,\n            \"--window-size\", window_size,\n            \"--start-date\", split_date,\n            \"--end-date\", end_date,\n            # bq2tfrecord.outputs['znorm_stats'],\n            \"--znorm-stats-json\", read_metadata.outputs[\"znorm_stats_path\"],\n            \"--batch-size\", prediction_batch_size,\n            \"--output-path\", \"gs://ciandt-cognitive-sandbox-chicago-taxi-demo-bucket/{{workflow.uid}}/predictions/forecast.csv\"\n        ],\n        file_outputs={\n            \"prediction_csv_path\": \"/prediction_csv_path.txt\"\n        }\n    ).apply(gcp.use_gcp_secret('user-gcp-sa')).after(deploy)\n\n    metrics = dsl.ContainerOp(\n        name='metrics',\n        image='gcr.io/ciandt-cognitive-sandbox/chicago-taxi-forecast/evaluate:latest',\n        command=[\"python\", \"/app/evaluate.py\"],\n        arguments=[\n            \"--prediction-csv\", predictions.outputs['prediction_csv_path']\n        ],\n        output_artifact_paths={\n            \"mlpipeline-metrics\": \"/mlpipeline-metrics.json\"\n        }\n    ).apply(gcp.use_gcp_secret('user-gcp-sa')).after(predictions)\n\n    plot = dsl.ContainerOp(\n        name='plot_time_series',\n        image='gcr.io/ciandt-cognitive-sandbox/chicago-taxi-forecast/evaluate:latest',\n        command=[\"python\", \"/app/plot_series.py\"],\n        arguments=[\n            \"--prediction-csv\", predictions.outputs['prediction_csv_path'],\n            \"--output-dir\", \"gs://ciandt-cognitive-sandbox-chicago-taxi-demo-bucket/{{workflow.uid}}/plots\"\n        ],\n        output_artifact_paths={\n            'mlpipeline-ui-metadata': '/mlpipeline-ui-metadata.json'\n        }\n    ).apply(gcp.use_gcp_secret('user-gcp-sa')).after(predictions)\n\n\nif __name__ == '__main__':\n    import kfp.compiler as compiler\n    compiler.Compiler().compile(chicago_taxi_pipeline,  'chicago_taxi_pipeline.tar.gz')\n"
  },
  {
    "repo": "gnovack/kubeflow-pipelines",
    "file_path": "boston_housing/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/gnovack/kubeflow-pipelines/master/boston_housing/pipeline.py",
    "content": "import kfp\nfrom kfp import dsl\n\ndef preprocess_op():\n\n    return dsl.ContainerOp(\n        name='Preprocess Data',\n        image='gnovack/boston_pipeline_preprocessing:latest',\n        arguments=[],\n        file_outputs={\n            'x_train': '/app/x_train.npy',\n            'x_test': '/app/x_test.npy',\n            'y_train': '/app/y_train.npy',\n            'y_test': '/app/y_test.npy',\n        }\n    )\n\ndef train_op(x_train, y_train):\n\n    return dsl.ContainerOp(\n        name='Train Model',\n        image='gnovack/boston_pipeline_train:latest',\n        arguments=[\n            '--x_train', x_train,\n            '--y_train', y_train\n        ],\n        file_outputs={\n            'model': '/app/model.pkl'\n        }\n    )\n\ndef test_op(x_test, y_test, model):\n\n    return dsl.ContainerOp(\n        name='Test Model',\n        image='gnovack/boston_pipeline_test:latest',\n        arguments=[\n            '--x_test', x_test,\n            '--y_test', y_test,\n            '--model', model\n        ],\n        file_outputs={\n            'mean_squared_error': '/app/output.txt'\n        }\n    )\n\ndef deploy_model_op(model):\n\n    return dsl.ContainerOp(\n        name='Deploy Model',\n        image='gnovack/boston_pipeline_deploy_model:latest',\n        arguments=[\n            '--model', model\n        ]\n    )\n\n@dsl.pipeline(\n   name='Boston Housing Pipeline',\n   description='An example pipeline that trains and logs a regression model.'\n)\ndef boston_pipeline():\n    _preprocess_op = preprocess_op()\n    \n    _train_op = train_op(\n        dsl.InputArgumentPath(_preprocess_op.outputs['x_train']),\n        dsl.InputArgumentPath(_preprocess_op.outputs['y_train'])\n    ).after(_preprocess_op)\n\n    _test_op = test_op(\n        dsl.InputArgumentPath(_preprocess_op.outputs['x_test']),\n        dsl.InputArgumentPath(_preprocess_op.outputs['y_test']),\n        dsl.InputArgumentPath(_train_op.outputs['model'])\n    ).after(_train_op)\n\n    deploy_model_op(\n        dsl.InputArgumentPath(_train_op.outputs['model'])\n    ).after(_test_op)\n\nclient = kfp.Client()\nclient.create_run_from_pipeline_func(boston_pipeline, arguments={})"
  },
  {
    "repo": "dermatologist/kedro-multimodal",
    "file_path": "build_kubeflow_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/dermatologist/kedro-multimodal/develop/build_kubeflow_pipeline.py",
    "content": "# python build_kubeflow_pipeline.py <project_image>\n\nimport re\nfrom pathlib import Path\nfrom typing import Dict, Set\n\nimport click\n\nfrom kfp import aws, dsl\nfrom kfp.compiler.compiler import Compiler\n\nfrom kedro.framework.project import pipelines\nfrom kedro.framework.startup import bootstrap_project\nfrom kedro.pipeline.node import Node\n\n_PIPELINE = None\n_IMAGE = None\n\n\n@click.command()\n@click.argument(\"image\", required=True)\n@click.option(\"-p\", \"--pipeline\", \"pipeline_name\", default=None)\n@click.option(\"--env\", \"-e\", type=str, default=None)\ndef generate_kfp(image: str, pipeline_name: str, env: str) -> None:\n    \"\"\"Generates a workflow spec yaml file from a Kedro pipeline.\n\n    Args:\n        image: container image name.\n        pipeline_name: pipeline name to build a workflow spec.\n        env: Kedro configuration environment name.\n\n    \"\"\"\n    global _PIPELINE\n    global _IMAGE\n    _IMAGE = image\n\n    project_path = Path.cwd()\n    metadata = bootstrap_project(project_path)\n    package_name = metadata.package_name\n\n    pipeline_name = pipeline_name or \"__default__\"\n    _PIPELINE = pipelines.get(pipeline_name)\n\n    Compiler().compile(convert_kedro_pipeline_to_kfp, package_name + \".yaml\")\n\n\n@dsl.pipeline(name=\"Kedro pipeline\", description=\"Kubeflow pipeline for Kedro project\")\ndef convert_kedro_pipeline_to_kfp() -> None:\n    \"\"\"Convert from a Kedro pipeline into a kfp container graph.\"\"\"\n    node_dependencies = _PIPELINE.node_dependencies\n    kfp_ops = _build_kfp_ops(node_dependencies)\n    for node, dependencies in node_dependencies.items():\n        for dependency in dependencies:\n            kfp_ops[node.name].after(kfp_ops[dependency.name])\n\n\ndef _build_kfp_ops(\n    node_dependencies: Dict[Node, Set[Node]]\n) -> Dict[str, dsl.ContainerOp]:\n    \"\"\"Build kfp container graph from Kedro node dependencies.\"\"\"\n    kfp_ops = {}\n\n    for node in node_dependencies:\n        name = clean_name(node.name)\n        kfp_ops[node.name] = dsl.ContainerOp(\n            name=name,\n            image=_IMAGE,\n            command=[\"kedro\"],\n            arguments=[\"run\", \"--node\", node.name],\n        ).apply(\n            # Configure the container to use AWS credentials.\n            aws.use_aws_secret(\n                \"aws-secrets\", \"AWS_ACCESS_KEY_ID\", \"AWS_SECRET_ACCESS_KEY\"\n            )\n        )\n    return kfp_ops\n\n\ndef clean_name(name: str) -> str:\n    \"\"\"Reformat a name.\n\n    Returns:\n        name: formatted name.\n\n    \"\"\"\n    return re.sub(r\"[\\W_]+\", \"-\", name).strip(\"-\")\n\n\nif __name__ == \"__main__\":\n    generate_kfp()\n"
  },
  {
    "repo": "Mastercard/mastercard-labs-ml-pipeline",
    "file_path": "santander-trnx-classification.py",
    "raw_url": "https://raw.githubusercontent.com/Mastercard/mastercard-labs-ml-pipeline/master/santander-trnx-classification.py",
    "content": "# !/usr/bin/env python3\n\nimport kfp\nfrom kfp import components\nfrom kfp import dsl\nfrom kfp import gcp\nfrom kfp import onprem\n\nplatform = 'GCP'\n\ndataflow_tf_transform_op = components.load_component_from_file('pipeline_steps/preprocessing/tft/component.yaml')\ntf_train_op = components.load_component_from_file('pipeline_steps/training/dnntrainer/component.yaml')\ndataflow_tf_predict_op = components.load_component_from_file('pipeline_steps/training/predict/component.yaml')\n\nconfusion_matrix_op = components.load_component_from_file('pipeline_steps/metrics/confusion_matrix/component.yaml')\nroc_op = components.load_component_from_file('pipeline_steps/metrics/roc/component.yaml')\n\nkubeflow_deploy_op = components.load_component_from_file('pipeline_steps/kubeflow/deployer/component.yaml')\n\n\n@dsl.pipeline(\n    name='Santander Customer Transaction Prediction',\n    description='Example pipeline that does classification with model analysis based on Santander customer transaction dataset.'\n)\ndef santander_transaction_classification(\n        output,\n        project,\n        train='gs://kubeflow-pipelines-demo/dataset/train.csv',\n        evaluation='gs://kubeflow-pipelines-demo/dataset/test.csv',\n        mode='local',\n        preprocess_module='gs://kubeflow-pipelines-demo/dataset/preprocessing.py',\n        learning_rate=0.1,\n        hidden_layer_size='1500',\n        steps=3000\n):\n    output_template = str(output) + '/{{workflow.uid}}/{{pod.name}}/data'\n    target_class_lambda = \"\"\"lambda x: x['target']\"\"\"\n\n    tf_server_name = 'kfdemo-service'\n\n    if platform != 'GCP':\n        vop = dsl.VolumeOp(\n            name=\"create_pvc\",\n            resource_name=\"pipeline-pvc\",\n            modes=dsl.VOLUME_MODE_RWM,\n            size=\"1Gi\"\n        )\n\n        checkout = dsl.ContainerOp(\n            name=\"checkout\",\n            image=\"alpine/git:latest\",\n            command=[\"git\", \"clone\", \"https://github.com/kubeflow/pipelines.git\", str(output) + \"/pipelines\"],\n        ).apply(onprem.mount_pvc(vop.outputs[\"name\"], 'local-storage', output))\n        checkout.after(vop)\n\n    preprocess = dataflow_tf_transform_op(\n        training_data_file_pattern=train,\n        evaluation_data_file_pattern=evaluation,\n        schema=\"not.txt\",\n        gcp_project=project,\n        run_mode=mode,\n        preprocessing_module=preprocess_module,\n        transformed_data_dir=output_template\n    )\n\n    training = tf_train_op(\n        transformed_data_dir=preprocess.output,\n        schema='not.txt',\n        learning_rate=learning_rate,\n        hidden_layer_size=hidden_layer_size,\n        steps=steps,\n        target='tips',\n        preprocessing_module=preprocess_module,\n        training_output_dir=output_template\n    )\n\n    prediction = dataflow_tf_predict_op(\n        data_file_pattern=evaluation,\n        schema='not.txt',\n        target_column='tips',\n        model=training.outputs['training_output_dir'],\n        run_mode=mode,\n        gcp_project=project,\n        predictions_dir=output_template\n    )\n\n    cm = confusion_matrix_op(\n        predictions=prediction.outputs['predictions_dir'],\n        output_dir=output_template\n    )\n\n    roc = roc_op(\n        predictions_dir=prediction.outputs['predictions_dir'],\n        target_lambda=target_class_lambda,\n        output_dir=output_template\n    )\n\n    steps = [training, prediction, cm, roc]\n    for step in steps:\n        if platform == 'GCP':\n            step.apply(gcp.use_gcp_secret('user-gcp-sa'))\n        else:\n            step.apply(onprem.mount_pvc(vop.outputs[\"name\"], 'local-storage', output))\n\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(santander_transaction_classification, __file__ + '.zip')\n"
  },
  {
    "repo": "Mastercard/mastercard-labs-ml-pipeline",
    "file_path": "santander-trnx-classification_release.py",
    "raw_url": "https://raw.githubusercontent.com/Mastercard/mastercard-labs-ml-pipeline/master/santander-trnx-classification_release.py",
    "content": "# !/usr/bin/env python3\n\nimport kfp\nfrom kfp import components\nfrom kfp import dsl\nfrom kfp import gcp\nfrom kfp import onprem\n\nplatform = 'GCP'\n\nkubeflow_deploy_op = components.load_component_from_file('pipeline_steps/serving/deployer/component.yaml')\n\n\n@dsl.pipeline(\n    name='Santander Customer Transaction Prediction Release Pipeline',\n    description='Example pipeline that releases the trained classification model for Santander customer transaction.'\n)\ndef santander_transaction_classification(\n        output,\n        project,\n):\n    tf_server_name = 'kfdemo-service'\n\n    if platform != 'GCP':\n        vop = dsl.VolumeOp(\n            name=\"create_pvc\",\n            resource_name=\"pipeline-pvc\",\n            modes=dsl.VOLUME_MODE_RWM,\n            size=\"1Gi\"\n        )\n\n        checkout = dsl.ContainerOp(\n            name=\"checkout\",\n            image=\"alpine/git:latest\",\n            command=[\"git\", \"clone\", \"https://github.com/kubeflow/pipelines.git\", str(output) + \"/pipelines\"],\n        ).apply(onprem.mount_pvc(vop.outputs[\"name\"], 'local-storage', output))\n        checkout.after(vop)\n\n    if platform == 'GCP':\n        deploy = kubeflow_deploy_op(\n            model_dir=str(\n                'gs://kubeflow-pipelines-demo/tfx/0b22081a-ed94-11e9-81fb-42010a800160/santander-customer-transaction-prediction-95qxr-268134926/data') + '/export/export',\n            server_name=tf_server_name\n        )\n    else:\n        deploy = kubeflow_deploy_op(\n            cluster_name=project,\n            model_dir=str(\n                'gs://kubeflow-pipelines-demo/tfx/0b22081a-ed94-11e9-81fb-42010a800160/santander-customer-transaction-prediction-95qxr-268134926/data') + '/export/export',\n            pvc_name=vop.outputs[\"name\"],\n            server_name=tf_server_name\n        )\n\n    webapp = dsl.ContainerOp(\n        name='webapp',\n        image='us.gcr.io/kf-pipelines/ml-pipeline-webapp-launcher:v0.3',\n        arguments=[\"--model_name\", 'santanderapp']\n\n    )\n    webapp.after(deploy)\n\n    steps = [deploy, webapp]\n\n    for step in steps:\n        if platform == 'GCP':\n            step.apply(gcp.use_gcp_secret('user-gcp-sa'))\n        else:\n            step.apply(onprem.mount_pvc(vop.outputs[\"name\"], 'local-storage', output))\n\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(santander_transaction_classification, __file__ + '.zip')\n"
  },
  {
    "repo": "Mastercard/mastercard-labs-ml-pipeline",
    "file_path": "pipeline_steps/training/katib-launcher/kubeflow_katib_launcher_op.py",
    "raw_url": "https://raw.githubusercontent.com/Mastercard/mastercard-labs-ml-pipeline/master/pipeline_steps/training/katib-launcher/kubeflow_katib_launcher_op.py",
    "content": "# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp.dsl as dsl\n\ndef kubeflow_studyjob_launcher_op(name, namespace, optimizationtype, objectivevaluename, optimizationgoal, requestcount, metricsnames,\n                                  parameterconfigs, nasConfig, workertemplatepath, mcollectortemplatepath, suggestionspec,\n                                  studyjob_timeout_minutes, delete=True, output_file='/output.txt', step_name='StudyJob-Launcher'):\n    return dsl.ContainerOp(\n        name = step_name,\n        image = 'liuhougangxa/ml-pipeline-kubeflow-studyjob:latest',\n        arguments = [\n            '--name', name,\n            '--namespace', namespace,\n            \"--optimizationtype\", optimizationtype,\n            \"--objectivevaluename\", objectivevaluename,\n            \"--optimizationgoal\", optimizationgoal,\n            \"--requestcount\", requestcount,\n            \"--metricsnames\", metricsnames,\n            \"--parameterconfigs\", parameterconfigs,\n            \"--nasConfig\", nasConfig,\n            \"--workertemplatepath\", workertemplatepath,\n            \"--mcollectortemplatepath\", mcollectortemplatepath,\n            \"--suggestionspec\", suggestionspec,\n            \"--outputfile\", output_file,\n            \"--deleteAfterDone\", delete,\n            '--studyjobtimeoutminutes', studyjob_timeout_minutes,\n        ],\n        file_outputs = {'hyperparameter': output_file}\n    )\n"
  },
  {
    "repo": "dvdbisong/kubeflow-for-poets",
    "file_path": "kubeflow-pipelines/crypto_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/dvdbisong/kubeflow-for-poets/master/kubeflow-pipelines/crypto_pipeline.py",
    "content": "#!/usr/bin/env python3\n# Copyright (c) 2019 Ekaba Bisong\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n\nimport kfp.dsl as dsl\n\nclass ObjectDict(dict):\n  def __getattr__(self, name):\n    if name in self:\n      return self[name]\n    else:\n      raise AttributeError(\"No such attribute: \" + name)\n\n\n@dsl.pipeline(\n  name='crypto',\n  description='Train Bitcoin closing prediction model'\n)\ndef train_and_deploy(\n    project='oceanic-sky-230504',\n    source_bucket='kubeflow-ekaba-bucket',\n    target_bucket='kubeflow-ekaba-bucket'\n):\n  \"\"\"Pipeline to train crypto model\"\"\"\n\n  # Step 1: transfer the raw dataset from github to gcs\n  raw_transfer = dsl.ContainerOp(\n    name='raw_data_transfer',\n    # image needs to be a compile-time string\n    image='gcr.io/oceanic-sky-230504/crypto-move-raw-to-gcs:latest',\n    arguments=[\n      target_bucket\n    ],\n    file_outputs={'target_bucket': '/output.txt'}\n  )\n\n  # Step 2: create training dataset using Apache Beam on Cloud Dataflow\n  preprocess = dsl.ContainerOp(\n    name='preprocess',\n    # image needs to be a compile-time string\n    image='gcr.io/oceanic-sky-230504/crypto-pipeline-dataflow-transform:latest',\n    arguments=[\n      '--project', project,\n      '--source_bucket', source_bucket,\n      '--target_bucket', raw_transfer.outputs['target_bucket']\n    ],\n    file_outputs={'bucket': '/output.txt'}\n  )\n\n  # Step 3: Do hyperparameter tuning of the model on Cloud ML Engine\n  hparam_train = dsl.ContainerOp(\n    name='hypertrain',\n    # image needs to be a compile-time string\n    image='gcr.io/oceanic-sky-230504/crypto-pipeline-cloud-hypertune-train:latest',\n    arguments=[\n      preprocess.outputs['bucket']\n    ],\n    file_outputs={'jobname': '/output.txt'}\n  )\n\n  # Step 4: Train the model with the optimized hyper-parameters\n  train_optimized_hyperparams = dsl.ContainerOp(\n    name='train_optimized_hyperparams',\n    # image needs to be a compile-time string\n    image='gcr.io/oceanic-sky-230504/crypto-pipeline-train-best-hyperparam:latest',\n    arguments=[\n      hparam_train.outputs['jobname'],\n      target_bucket\n    ],\n    file_outputs={'train': '/output.txt'}\n  )\n\n  # Step 5: Deploy the trained model to Cloud ML Engine\n  deploy_cloud_mle = dsl.ContainerOp(\n    name='deploy-model-cloud-mle',\n    # image needs to be a compile-time string\n    image='gcr.io/oceanic-sky-230504/crypto-deploy-cloudmle:latest',\n    arguments=[\n      train_optimized_hyperparams.outputs['train'],\n      'crypto',\n      'v1'\n    ],\n    file_outputs={\n      'model': '/model.txt',\n      'version': '/version.txt'\n    }\n  )\n\nif __name__ == '__main__':\n  import kfp.compiler as compiler\n  import sys\n  if len(sys.argv) != 2:\n    print(\"Usage: mlp_crypto  pipeline-output-name\")\n    sys.exit(-1)\n  \n  filename = sys.argv[1]\n  compiler.Compiler().compile(train_and_deploy, filename)"
  },
  {
    "repo": "Ark-kun/pipeline_components",
    "file_path": "components/XGBoost/_samples/recursive_training.py",
    "raw_url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/XGBoost/_samples/recursive_training.py",
    "content": "#!/usr/bin/env python3\n\n# This sample demonstrates continuous training using a train-eval-check recursive loop.\n# The main pipeline trains the initial model and then gradually trains the model\n# some more until the model evaluation metrics are good enough.\n\nimport kfp\nfrom kfp import components\n\n\nchicago_taxi_dataset_op = components.load_component_from_url('https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/datasets/Chicago_Taxi_Trips/component.yaml')\nxgboost_train_on_csv_op = components.load_component_from_url('https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/XGBoost/Train/component.yaml')\nxgboost_predict_on_csv_op = components.load_component_from_url('https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/XGBoost/Predict/component.yaml')\n\npandas_transform_csv_op = components.load_component_from_url('https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/pandas/Transform_DataFrame/in_CSV_format/component.yaml')\ndrop_header_op = kfp.components.load_component_from_url('https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/tables/Remove_header/component.yaml')\ncalculate_regression_metrics_from_csv_op = kfp.components.load_component_from_url('https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/ml_metrics/Calculate_regression_metrics/from_CSV/component.yaml')\n\n\n# This recursive sub-pipeline trains a model, evaluates it, calculates the metrics and checks them.\n# If the model error is too high, then more training is performed until the model is good.\n@kfp.dsl.graph_component\ndef train_until_low_error(starting_model, training_data, true_values):\n    # Training\n    model = xgboost_train_on_csv_op(\n        training_data=training_data,\n        starting_model=starting_model,\n        label_column=0,\n        objective='reg:squarederror',\n        num_iterations=50,\n    ).outputs['model']\n\n    # Predicting\n    predictions = xgboost_predict_on_csv_op(\n        data=training_data,\n        model=model,\n        label_column=0,\n    ).output\n\n    # Calculating the regression metrics\n    metrics_task = calculate_regression_metrics_from_csv_op(\n        true_values=true_values,\n        predicted_values=predictions,\n    )\n\n    # Checking the metrics\n    with kfp.dsl.Condition(metrics_task.outputs['mean_squared_error'] > 0.01):\n        # Training some more\n        train_until_low_error(\n            starting_model=model,\n            training_data=training_data,\n            true_values=true_values,\n        )\n\n\n# The main pipleine trains the initial model and then gradually trains the model some more until the model evaluation metrics are good enough.\ndef train_until_good_pipeline():\n    # Preparing the training data\n    training_data = chicago_taxi_dataset_op(\n        where='trip_start_timestamp >= \"2019-01-01\" AND trip_start_timestamp < \"2019-02-01\"',\n        select='tips,trip_seconds,trip_miles,pickup_community_area,dropoff_community_area,fare,tolls,extras,trip_total',\n        limit=10000,\n    ).output\n\n    # Preparing the true values\n    true_values_table = pandas_transform_csv_op(\n        table=training_data,\n        transform_code='df = df[[\"tips\"]]',\n    ).output\n\n    true_values = drop_header_op(true_values_table).output\n\n    # Initial model training\n    first_model = xgboost_train_on_csv_op(\n        training_data=training_data,\n        label_column=0,\n        objective='reg:squarederror',\n        num_iterations=100,\n    ).outputs['model']\n\n    # Recursively training until the error becomes low\n    train_until_low_error(\n        starting_model=first_model,\n        training_data=training_data,\n        true_values=true_values,\n    )\n\n\nif __name__ == '__main__':\n    kfp_endpoint=None\n    kfp.Client(host=kfp_endpoint).create_run_from_pipeline_func(train_until_good_pipeline, arguments={})\n"
  },
  {
    "repo": "Ark-kun/pipeline_components",
    "file_path": "samples/core/continue_training_from_prod/continue_training_from_prod.py",
    "raw_url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/samples/core/continue_training_from_prod/continue_training_from_prod.py",
    "content": "# This sample demonstrates a common training scenario.\n# New models are being trained strarting from the production model (if it exists).\n# This sample produces two runs:\n# 1. The trainer will train the model from scratch and set as prod after testing it\n# 2. Exact same configuration, but the pipeline will discover the existing prod model (published by the 1st run) and warm-start the training from it.\n\n\n# GCS URI of a directory where the models and the model pointers should be be stored.\nmodel_dir_uri='gs://<bucket>/<path>'\nkfp_endpoint=None\n\n\nimport kfp\nfrom kfp import components\n\n\nchicago_taxi_dataset_op = components.load_component_from_url('https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/datasets/Chicago_Taxi_Trips/component.yaml')\nxgboost_train_on_csv_op = components.load_component_from_url('https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/XGBoost/Train/component.yaml')\nxgboost_predict_on_csv_op = components.load_component_from_url('https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/XGBoost/Predict/component.yaml')\n\npandas_transform_csv_op = components.load_component_from_url('https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/pandas/Transform_DataFrame/in_CSV_format/component.yaml')\ndrop_header_op = kfp.components.load_component_from_url('https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/tables/Remove_header/component.yaml')\ncalculate_regression_metrics_from_csv_op = kfp.components.load_component_from_url('https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/ml_metrics/Calculate_regression_metrics/from_CSV/component.yaml')\n\ndownload_from_gcs_op = components.load_component_from_url('https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/google-cloud/storage/download/component.yaml')\nupload_to_gcs_op = components.load_component_from_url('https://raw.githubusercontent.com/Ark-kun/pipeline_components/6210648f30b2b3a8c01cc10be338da98300efb6b/components/google-cloud/storage/upload_to_explicit_uri/component.yaml')\nupload_to_gcs_unique_op = components.load_component_from_url('https://raw.githubusercontent.com/Ark-kun/pipeline_components/6210648f30b2b3a8c01cc10be338da98300efb6b/components/google-cloud/storage/upload_to_unique_uri/component.yaml')\n\n\ndef continuous_training_pipeline(\n    model_dir_uri,\n    training_start_date: str = '2019-02-01',\n    training_end_date: str = '2019-03-01',\n    testing_start_date: str = '2019-01-01',\n    testing_end_date: str = '2019-02-01',\n):\n    # Preparing the training and testing data\n    training_data = chicago_taxi_dataset_op(\n        where='trip_start_timestamp >= \"{}\" AND trip_start_timestamp < \"{}\"'.format(str(training_start_date), str(training_end_date)),\n        select='tips,trip_seconds,trip_miles,pickup_community_area,dropoff_community_area,fare,tolls,extras,trip_total',\n        limit=10000,\n    ).set_display_name('Training data').output\n\n    testing_data = chicago_taxi_dataset_op(\n        where='trip_start_timestamp >= \"{}\" AND trip_start_timestamp < \"{}\"'.format(str(testing_start_date), str(testing_end_date)),\n        select='tips,trip_seconds,trip_miles,pickup_community_area,dropoff_community_area,fare,tolls,extras,trip_total',\n        limit=10000,\n    ).set_display_name('Testing data').output\n\n    # Preparing the true values for the testing data\n    true_values_table = pandas_transform_csv_op(\n        table=testing_data,\n        transform_code='''df = df[[\"tips\"]]''',\n    ).set_display_name('True values').output\n\n    true_values = drop_header_op(true_values_table).output\n\n    # Getting the active prod model\n    prod_model_pointer_uri = str(model_dir_uri) + 'prod'\n    get_prod_model_uri_task = download_from_gcs_op(\n        gcs_path=prod_model_pointer_uri,\n        default_data='',\n    ).set_display_name('Get prod model')\n    # Disabling cache reuse to always get new data\n    get_prod_model_uri_task.execution_options.caching_strategy.max_cache_staleness = 'P0D'\n    prod_model_uri = get_prod_model_uri_task.output\n\n    # Training new model from scratch\n    with kfp.dsl.Condition(prod_model_uri == \"\"):\n        # Training\n        model = xgboost_train_on_csv_op(\n            training_data=training_data,\n            label_column=0,\n            objective='reg:squarederror',\n            num_iterations=400,\n        ).outputs['model']\n\n        # Predicting\n        predictions = xgboost_predict_on_csv_op(\n            data=testing_data,\n            model=model,\n            label_column=0,\n        ).output\n\n        # Calculating the regression metrics\n        metrics_task = calculate_regression_metrics_from_csv_op(\n            true_values=true_values,\n            predicted_values=predictions,\n        )\n\n        # Checking the metrics\n        with kfp.dsl.Condition(metrics_task.outputs['mean_squared_error'] < 2.0):\n            # Uploading the model\n            model_uri = upload_to_gcs_unique_op(\n                data=model,\n                gcs_path_prefix=model_dir_uri,\n            ).set_display_name('Upload model').output\n\n            # Setting the model as prod\n            upload_to_gcs_op(\n                data=model_uri,\n                gcs_path=prod_model_pointer_uri,\n            ).set_display_name('Set prod model')\n\n    # Training new model starting from the prod model\n    with kfp.dsl.Condition(prod_model_uri != \"\"):\n        # Downloading the model\n        prod_model = download_from_gcs_op(prod_model_uri).output\n\n        # Training\n        model = xgboost_train_on_csv_op(\n            training_data=training_data,\n            starting_model=prod_model,\n            label_column=0,\n            objective='reg:squarederror',\n            num_iterations=100,\n        ).outputs['model']\n\n        # Predicting\n        predictions = xgboost_predict_on_csv_op(\n            data=testing_data,\n            model=model,\n            label_column=0,\n        ).output\n\n        # Calculating the regression metrics\n        metrics_task = calculate_regression_metrics_from_csv_op(\n            true_values=true_values,\n            predicted_values=predictions,\n        )\n\n        # Checking the metrics\n        with kfp.dsl.Condition(metrics_task.outputs['mean_squared_error'] < 2.0):\n            # Uploading the model\n            model_uri = upload_to_gcs_unique_op(\n                data=model,\n                gcs_path_prefix=model_dir_uri,\n            ).set_display_name('Upload model').output\n\n            # Setting the model as prod\n            upload_to_gcs_op(\n                data=model_uri,\n                gcs_path=prod_model_pointer_uri,\n            ).set_display_name('Set prod model')\n\n\nif __name__ == '__main__':\n    # Running the first time. The trainer will train the model from scratch and set as prod after testing it\n    pipelin_run = kfp.Client(host=kfp_endpoint).create_run_from_pipeline_func(\n        continuous_training_pipeline,\n        arguments=dict(\n            model_dir_uri=model_dir_uri,\n            training_start_date='2019-02-01',\n            training_end_date='2019-03-01',\n        ),\n    )\n    pipelin_run.wait_for_run_completion()\n\n    # Running the second time. The trainer should warm-start the training from the prod model and set the new model as prod after testing it\n    kfp.Client(host=kfp_endpoint).create_run_from_pipeline_func(\n        continuous_training_pipeline,\n        arguments=dict(\n            model_dir_uri=model_dir_uri,\n            training_start_date='2019-02-01',\n            training_end_date='2019-03-01',\n        ),\n    )\n"
  },
  {
    "repo": "Ark-kun/pipeline_components",
    "file_path": "samples/core/train_until_good/train_until_good.py",
    "raw_url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/samples/core/train_until_good/train_until_good.py",
    "content": "#!/usr/bin/env python3\n# Copyright 2020 The Kubeflow Pipleines authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# This sample demonstrates continuous training using a train-eval-check recursive loop.\n# The main pipeline trains the initial model and then gradually trains the model\n# some more until the model evaluation metrics are good enough.\n\nimport kfp\nfrom kfp import components\n\n\nchicago_taxi_dataset_op = components.load_component_from_url('https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/datasets/Chicago_Taxi_Trips/component.yaml')\nxgboost_train_on_csv_op = components.load_component_from_url('https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/XGBoost/Train/component.yaml')\nxgboost_predict_on_csv_op = components.load_component_from_url('https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/XGBoost/Predict/component.yaml')\n\npandas_transform_csv_op = components.load_component_from_url('https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/pandas/Transform_DataFrame/in_CSV_format/component.yaml')\ndrop_header_op = kfp.components.load_component_from_url('https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/tables/Remove_header/component.yaml')\ncalculate_regression_metrics_from_csv_op = kfp.components.load_component_from_url('https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/ml_metrics/Calculate_regression_metrics/from_CSV/component.yaml')\n\n\n# This recursive sub-pipeline trains a model, evaluates it, calculates the metrics and checks them.\n# If the model error is too high, then more training is performed until the model is good.\n@kfp.dsl.graph_component\ndef train_until_low_error(starting_model, training_data, true_values):\n    # Training\n    model = xgboost_train_on_csv_op(\n        training_data=training_data,\n        starting_model=starting_model,\n        label_column=0,\n        objective='reg:squarederror',\n        num_iterations=50,\n    ).outputs['model']\n\n    # Predicting\n    predictions = xgboost_predict_on_csv_op(\n        data=training_data,\n        model=model,\n        label_column=0,\n    ).output\n\n    # Calculating the regression metrics\n    metrics_task = calculate_regression_metrics_from_csv_op(\n        true_values=true_values,\n        predicted_values=predictions,\n    )\n\n    # Checking the metrics\n    with kfp.dsl.Condition(metrics_task.outputs['mean_squared_error'] > 0.01):\n        # Training some more\n        train_until_low_error(\n            starting_model=model,\n            training_data=training_data,\n            true_values=true_values,\n        )\n\n\n# The main pipleine trains the initial model and then gradually trains the model some more until the model evaluation metrics are good enough.\ndef train_until_good_pipeline():\n    # Preparing the training data\n    training_data = chicago_taxi_dataset_op(\n        where='trip_start_timestamp >= \"2019-01-01\" AND trip_start_timestamp < \"2019-02-01\"',\n        select='tips,trip_seconds,trip_miles,pickup_community_area,dropoff_community_area,fare,tolls,extras,trip_total',\n        limit=10000,\n    ).output\n\n    # Preparing the true values\n    true_values_table = pandas_transform_csv_op(\n        table=training_data,\n        transform_code='df = df[[\"tips\"]]',\n    ).output\n\n    true_values = drop_header_op(true_values_table).output\n\n    # Initial model training\n    first_model = xgboost_train_on_csv_op(\n        training_data=training_data,\n        label_column=0,\n        objective='reg:squarederror',\n        num_iterations=100,\n    ).outputs['model']\n\n    # Recursively training until the error becomes low\n    train_until_low_error(\n        starting_model=first_model,\n        training_data=training_data,\n        true_values=true_values,\n    )\n\n\nif __name__ == '__main__':\n    kfp_endpoint=None\n    kfp.Client(host=kfp_endpoint).create_run_from_pipeline_func(train_until_good_pipeline, arguments={})\n"
  },
  {
    "repo": "StatCan/aaw-kubeflow-mlops",
    "file_path": "pipeline/train/cnn.py",
    "raw_url": "https://raw.githubusercontent.com/StatCan/aaw-kubeflow-mlops/master/pipeline/train/cnn.py",
    "content": "\"\"\"Convolutional Neural Network (CNN) Pipeline\"\"\"\nfrom kubernetes import client as k8s_client\nimport kfp.dsl as dsl\nimport kfp.compiler as compiler\nfrom kfp.azure import use_azure_secret\nimport json\nimport os\nfrom kubernetes.client.models import V1EnvVar\nfrom utils.kubernetes.secret import use_azstorage_secret  # noqa: E501\n\n# Initially derived from https://github.com/kaizentm/kubemlops\n\n\nTRAIN_START_EVENT = \"Training Started\"\nTRAIN_FINISH_EVENT = \"Training Finished\"\n\n\n@dsl.pipeline(\n    name='Tacos vs. Burritos',\n    description='Simple TF CNN'\n)\ndef get_callback_payload(event_type):\n    payload = {}\n    payload['event_type'] = event_type\n    payload['sha'] = os.getenv('GITHUB_SHA')\n    payload['pr_num'] = os.getenv('PR_NUM')\n    payload['run_id'] = dsl.RUN_ID_PLACEHOLDER\n    if (event_type == TRAIN_FINISH_EVENT):\n        payload['status'] = '{{workflow.status}}'\n    return json.dumps(payload)\n\n\ndef cnn_train(\n    resource_group,\n    workspace,\n    dataset,\n    token\n):\n    \"\"\"Pipeline steps\"\"\"\n\n    persistent_volume_path = '/mnt/azure'\n    data_download = dataset  # noqa: E501\n    batch = 32\n    model_name = 'cnnmodel'\n    operations = {}\n    image_size = 160\n    training_folder = 'train'\n    training_dataset = 'train.txt'\n    model_folder = 'model'\n    image_repo_name = \"k8scc01covidmlopsacr.azurecr.io/mlops\"\n    callback_url = 'kubemlopsbot-svc.kubeflow.svc.cluster.local:8080'\n    mlflow_url = 'http://mlflow.mlflow:5000'\n\n    exit_op = dsl.ContainerOp(\n        name='Exit Handler',\n        image=\"curlimages/curl\",\n        command=['curl'],\n        arguments=[\n            '-d', get_callback_payload(TRAIN_FINISH_EVENT),\n            callback_url\n        ]\n    )\n\n    with dsl.ExitHandler(exit_op):\n        start_callback = \\\n            dsl.UserContainer('callback',\n                              'curlimages/curl',\n                              command=['curl'],\n                              args=['-d',\n                                    get_callback_payload(TRAIN_START_EVENT), callback_url])  # noqa: E501\n\n        operations['tensorflow preprocess'] = dsl.ContainerOp(\n            name='tensorflow preprocess',\n            init_containers=[start_callback],\n            image=image_repo_name + '/tensorflow-preprocess:latest',\n            command=['python'],\n            arguments=[\n                '/scripts/data.py',\n                '--base_path', persistent_volume_path,\n                '--data', training_folder,\n                '--target', training_dataset,\n                '--img_size', image_size,\n                '--zipfile', data_download\n            ]\n        )\n\n        operations['tensorflow training'] = dsl.ContainerOp(\n            name=\"tensorflow training\",\n            image=image_repo_name + '/tensorflow-training:latest',\n            command=['python'],\n            arguments=[\n                '/scripts/train.py',\n                '--base_path', persistent_volume_path,\n                '--data', training_folder,\n                '--epochs', 2,\n                '--batch', batch,\n                '--image_size', image_size,\n                '--lr', 0.0001,\n                '--outputs', model_folder,\n                '--dataset', training_dataset\n            ],\n            output_artifact_paths={\n                'mlpipeline-metrics': '/mlpipeline-metrics.json',\n                'mlpipeline-ui-metadata': '/mlpipeline-ui-metadata.json'\n            }\n            ).apply(use_azstorage_secret()).add_env_variable(V1EnvVar(name=\"RUN_ID\", value=dsl.RUN_ID_PLACEHOLDER)).add_env_variable(V1EnvVar(name=\"MLFLOW_TRACKING_TOKEN\", value=token)).add_env_variable(V1EnvVar(name=\"MLFLOW_TRACKING_URI\", value=mlflow_url)).add_env_variable(V1EnvVar(name=\"GIT_PYTHON_REFRESH\", value='quiet'))  # noqa: E501\n\n        operations['tensorflow training'].after(operations['tensorflow preprocess'])  # noqa: E501\n\n        operations['evaluate'] = dsl.ContainerOp(\n            name='evaluate',\n            image=\"busybox\",\n            command=['sh', '-c'],\n            arguments=[\n                'echo',\n                'Life is Good!'\n            ]\n\n        )\n        operations['evaluate'].after(operations['tensorflow training'])\n\n        operations['register kubeflow'] = dsl.ContainerOp(\n            name='register kubeflow',\n            image=image_repo_name + '/register-kubeflow-artifacts:latest',\n            command=['python'],\n            arguments=[\n                '/scripts/register.py',\n                '--base_path', persistent_volume_path,\n                '--model', 'latest.h5',\n                '--model_name', model_name,\n                '--data', training_folder,\n                '--dataset', training_dataset,\n                '--run_id', dsl.RUN_ID_PLACEHOLDER\n            ]\n        ).apply(use_azure_secret())\n        operations['register kubeflow'].after(operations['evaluate'])\n\n        operations['register AML'] = dsl.ContainerOp(\n            name='register AML',\n            image=image_repo_name + '/register-aml:latest',\n            command=['python'],\n            arguments=[\n                '/scripts/register.py',\n                '--base_path', persistent_volume_path,\n                '--model', 'latest.h5',\n                '--model_name', model_name,\n                '--tenant_id', \"$(AZ_TENANT_ID)\",\n                '--service_principal_id', \"$(AZ_CLIENT_ID)\",\n                '--service_principal_password', \"$(AZ_CLIENT_SECRET)\",\n                '--subscription_id', \"$(AZ_SUBSCRIPTION_ID)\",\n                '--resource_group', resource_group,\n                '--workspace', workspace,\n                '--run_id', dsl.RUN_ID_PLACEHOLDER\n            ]\n        ).apply(use_azure_secret())\n        operations['register AML'].after(operations['register kubeflow'])\n\n        operations['register mlflow'] = dsl.ContainerOp(\n            name='register mlflow',\n            image=image_repo_name + '/register-mlflow:latest',\n            command=['python'],\n            arguments=[\n                '/scripts/register.py',\n                '--model', 'model',\n                '--model_name', model_name,\n                '--experiment_name', 'kubeflow-mlops',\n                '--run_id', dsl.RUN_ID_PLACEHOLDER\n            ]\n        ).apply(use_azure_secret()).add_env_variable(V1EnvVar(name=\"MLFLOW_TRACKING_URI\", value=mlflow_url)).add_env_variable(V1EnvVar(name=\"MLFLOW_TRACKING_TOKEN\", value=token))  # noqa: E501\n        operations['register mlflow'].after(operations['register AML'])\n\n        operations['finalize'] = dsl.ContainerOp(\n            name='Finalize',\n            image=\"curlimages/curl\",\n            command=['curl'],\n            arguments=[\n                '-d', get_callback_payload(\"Model is registered\"),\n                callback_url\n            ]\n        )\n        operations['finalize'].after(operations['register mlflow'])\n\n    for _, op_1 in operations.items():\n        op_1.container.set_image_pull_policy(\"Always\")\n        op_1.add_volume(\n            k8s_client.V1Volume(\n              name='azure',\n              persistent_volume_claim=k8s_client.V1PersistentVolumeClaimVolumeSource(  # noqa: E501\n                claim_name='azure-managed-file')\n            )\n        ).add_volume_mount(k8s_client.V1VolumeMount(\n            mount_path='/mnt/azure', name='azure'))\n\n\nif __name__ == '__main__':\n    compiler.Compiler().compile(cnn_train, __file__ + '.tar.gz')\n"
  },
  {
    "repo": "StatCan/aaw-kubeflow-mlops",
    "file_path": "pipeline/train/cnn_databricks.py",
    "raw_url": "https://raw.githubusercontent.com/StatCan/aaw-kubeflow-mlops/master/pipeline/train/cnn_databricks.py",
    "content": "\"\"\"Convolutional Neural Network (CNN) Pipeline\"\"\"\nfrom kubernetes import client as k8s_client\nimport kfp.dsl as dsl\nimport kfp.compiler as compiler\nfrom kfp.azure import use_azure_secret\nimport json\nimport os\nfrom kubernetes.client.models import V1EnvVar\nfrom utils.kubernetes.secret import use_databricks_secret, use_azstorage_secret  # noqa: E501\n\n# Initially derived from https://github.com/kaizentm/kubemlops\n\n\nTRAIN_START_EVENT = \"Training Started\"\nTRAIN_FINISH_EVENT = \"Training Finished\"\n\n\n@dsl.pipeline(\n    name='Tacos vs. Burritos',\n    description='Simple TF CNN'\n)\ndef get_callback_payload(event_type):\n    payload = {}\n    payload['event_type'] = event_type\n    payload['sha'] = os.getenv('GITHUB_SHA')\n    payload['pr_num'] = os.getenv('PR_NUM')\n    payload['run_id'] = dsl.RUN_ID_PLACEHOLDER\n    if (event_type == TRAIN_FINISH_EVENT):\n        payload['status'] = '{{workflow.status}}'\n    return json.dumps(payload)\n\n\ndef cnn_train(\n    resource_group,\n    workspace,\n    dataset,\n    token\n):\n    \"\"\"Pipeline steps\"\"\"\n\n    persistent_volume_path = '/mnt/azure'\n    data_download = dataset  # noqa: E501\n    batch = 32\n    model_name = 'cnnmodel'\n    operations = {}\n    image_size = 160\n    training_folder = 'train'\n    training_dataset = 'train.txt'\n    model_folder = 'model'\n    image_repo_name = \"k8scc01covidmlopsacr.azurecr.io/mlops\"\n    callback_url = 'kubemlopsbot-svc.kubeflow.svc.cluster.local:8080'\n    mlflow_url = 'http://mlflow.mlflow:5000'\n\n    exit_op = dsl.ContainerOp(\n        name='Exit Handler',\n        image=\"curlimages/curl\",\n        command=['curl'],\n        arguments=[\n            '-d', get_callback_payload(TRAIN_FINISH_EVENT),\n            callback_url\n        ]\n    )\n\n    with dsl.ExitHandler(exit_op):\n        start_callback = \\\n            dsl.UserContainer('callback',\n                              'curlimages/curl',\n                              command=['curl'],\n                              args=['-d',\n                                    get_callback_payload(TRAIN_START_EVENT), callback_url])  # noqa: E501\n\n        operations['databricks data processing'] = dsl.ContainerOp(\n            name='databricks data processing',\n            init_containers=[start_callback],\n            image=image_repo_name + '/databricks-notebook:latest',\n            arguments=[\n                '-r', dsl.RUN_ID_PLACEHOLDER,\n                '-p', '{\"argument_one\":\"param one\",\"argument_two\":\"param two\"}'\n            ]\n        ).apply(use_databricks_secret())\n\n        operations['tensorflow preprocess'] = dsl.ContainerOp(\n            name='tensorflow preprocess',\n            image=image_repo_name + '/tensorflow-preprocess:latest',\n            command=['python'],\n            arguments=[\n                '/scripts/data.py',\n                '--base_path', persistent_volume_path,\n                '--data', training_folder,\n                '--target', training_dataset,\n                '--img_size', image_size,\n                '--zipfile', data_download\n            ]\n        )\n\n        operations['tensorflow preprocess'].after(operations['databricks data processing'])  # noqa: E501\n\n        operations['tensorflow training'] = dsl.ContainerOp(\n            name=\"tensorflow training\",\n            image=image_repo_name + '/tensorflow-training:latest',\n            command=['python'],\n            arguments=[\n                '/scripts/train.py',\n                '--base_path', persistent_volume_path,\n                '--data', training_folder,\n                '--epochs', 2,\n                '--batch', batch,\n                '--image_size', image_size,\n                '--lr', 0.0001,\n                '--outputs', model_folder,\n                '--dataset', training_dataset\n            ],\n            output_artifact_paths={\n                'mlpipeline-metrics': '/mlpipeline-metrics.json',\n                'mlpipeline-ui-metadata': '/mlpipeline-ui-metadata.json'\n            }\n            ).apply(use_azstorage_secret()).add_env_variable(V1EnvVar(name=\"RUN_ID\", value=dsl.RUN_ID_PLACEHOLDER)).add_env_variable(V1EnvVar(name=\"MLFLOW_TRACKING_TOKEN\", value=token)).add_env_variable(V1EnvVar(name=\"MLFLOW_TRACKING_URI\", value=mlflow_url)).add_env_variable(V1EnvVar(name=\"GIT_PYTHON_REFRESH\", value='quiet'))  # noqa: E501\n\n        operations['tensorflow training'].after(operations['tensorflow preprocess'])  # noqa: E501\n\n        operations['evaluate'] = dsl.ContainerOp(\n            name='evaluate',\n            image=\"busybox\",\n            command=['sh', '-c'],\n            arguments=[\n                'echo',\n                'Life is Good!'\n            ]\n\n        )\n        operations['evaluate'].after(operations['tensorflow training'])\n\n        operations['register kubeflow'] = dsl.ContainerOp(\n            name='register kubeflow',\n            image=image_repo_name + '/register-kubeflow-artifacts:latest',\n            command=['python'],\n            arguments=[\n                '/scripts/register.py',\n                '--base_path', persistent_volume_path,\n                '--model', 'latest.h5',\n                '--model_name', model_name,\n                '--data', training_folder,\n                '--dataset', training_dataset,\n                '--run_id', dsl.RUN_ID_PLACEHOLDER\n            ]\n        ).apply(use_azure_secret())\n        operations['register kubeflow'].after(operations['evaluate'])\n\n        operations['register AML'] = dsl.ContainerOp(\n            name='register AML',\n            image=image_repo_name + '/register-aml:latest',\n            command=['python'],\n            arguments=[\n                '/scripts/register.py',\n                '--base_path', persistent_volume_path,\n                '--model', 'latest.h5',\n                '--model_name', model_name,\n                '--tenant_id', \"$(AZ_TENANT_ID)\",\n                '--service_principal_id', \"$(AZ_CLIENT_ID)\",\n                '--service_principal_password', \"$(AZ_CLIENT_SECRET)\",\n                '--subscription_id', \"$(AZ_SUBSCRIPTION_ID)\",\n                '--resource_group', resource_group,\n                '--workspace', workspace,\n                '--run_id', dsl.RUN_ID_PLACEHOLDER\n            ]\n        ).apply(use_azure_secret())\n        operations['register AML'].after(operations['register kubeflow'])\n\n        operations['register mlflow'] = dsl.ContainerOp(\n            name='register mlflow',\n            image=image_repo_name + '/register-mlflow:latest',\n            command=['python'],\n            arguments=[\n                '/scripts/register.py',\n                '--model', 'model',\n                '--model_name', model_name,\n                '--experiment_name', 'mlops',\n                '--run_id', dsl.RUN_ID_PLACEHOLDER\n            ]\n        ).apply(use_azure_secret()).add_env_variable(V1EnvVar(name=\"MLFLOW_TRACKING_URI\", value=mlflow_url)).add_env_variable(V1EnvVar(name=\"MLFLOW_TRACKING_TOKEN\", value=token))  # noqa: E501\n        operations['register mlflow'].after(operations['register AML'])\n\n        operations['finalize'] = dsl.ContainerOp(\n            name='Finalize',\n            image=\"curlimages/curl\",\n            command=['curl'],\n            arguments=[\n                '-d', get_callback_payload(\"Model is registered\"),\n                callback_url\n            ]\n        )\n        operations['finalize'].after(operations['register mlflow'])\n\n    for _, op_1 in operations.items():\n        op_1.container.set_image_pull_policy(\"Always\")\n        op_1.add_volume(\n            k8s_client.V1Volume(\n              name='azure',\n              persistent_volume_claim=k8s_client.V1PersistentVolumeClaimVolumeSource(  # noqa: E501\n                claim_name='azure-managed-file')\n            )\n        ).add_volume_mount(k8s_client.V1VolumeMount(\n            mount_path='/mnt/azure', name='azure'))\n\n\nif __name__ == '__main__':\n    compiler.Compiler().compile(cnn_train, __file__ + '.tar.gz')\n"
  },
  {
    "repo": "StatCan/aaw-kubeflow-mlops",
    "file_path": "pipeline/train/default.py",
    "raw_url": "https://raw.githubusercontent.com/StatCan/aaw-kubeflow-mlops/master/pipeline/train/default.py",
    "content": "\"\"\"Default Pipeline\"\"\"\nfrom kubernetes import client as k8s_client\nimport kfp.dsl as dsl\nimport kfp.compiler as compiler\nimport json\nimport os\n\n# Initially derived from https://github.com/kaizentm/kubemlops\n\n\nTRAIN_START_EVENT = \"Training Started\"\nTRAIN_FINISH_EVENT = \"Training Finished\"\n\n\n@dsl.pipeline(\n    name='Default',\n    description='Simple default pipeline to test functionality'\n)\ndef get_callback_payload(event_type):\n    payload = {}\n    payload['event_type'] = event_type\n    payload['sha'] = os.getenv('GITHUB_SHA')\n    payload['pr_num'] = os.getenv('PR_NUM')\n    payload['run_id'] = dsl.RUN_ID_PLACEHOLDER\n    if (event_type == TRAIN_FINISH_EVENT):\n        payload['status'] = '{{workflow.status}}'\n    return json.dumps(payload)\n\n\ndef default_train(\n    resource_group,\n    workspace,\n    dataset\n):\n    \"\"\"Pipeline steps\"\"\"\n\n    operations = {}\n    callback_url = 'kubemlopsbot-svc.kubeflow.svc.cluster.local:8080'\n\n    exit_op = dsl.ContainerOp(\n        name='Exit Handler',\n        image=\"curlimages/curl\",\n        command=['curl'],\n        arguments=[\n            '-d', get_callback_payload(TRAIN_FINISH_EVENT),\n            callback_url\n        ]\n    )\n\n    with dsl.ExitHandler(exit_op):\n        start_callback = \\\n            dsl.UserContainer('callback',\n                              'curlimages/curl',\n                              command=['curl'],\n                              args=['-d',\n                                    get_callback_payload(TRAIN_START_EVENT), callback_url])  # noqa: E501\n\n        operations['start'] = dsl.ContainerOp(\n            name='start',\n            init_containers=[start_callback],\n            image=\"busybox\",\n            command=['sh', '-c'],\n            arguments=[\n                'echo',\n                'Pipeline starting'\n            ]\n        )\n\n        operations['end'] = dsl.ContainerOp(\n            name='End',\n            image=\"curlimages/curl\",\n            command=['curl'],\n            arguments=[\n                '-d', get_callback_payload(\"Model is registered\"),\n                callback_url\n            ]\n        )\n        operations['end'].after(operations['start'])\n\n    for _, op_1 in operations.items():\n        op_1.container.set_image_pull_policy(\"Always\")\n        op_1.add_volume(\n            k8s_client.V1Volume(\n              name='azure',\n              persistent_volume_claim=k8s_client.V1PersistentVolumeClaimVolumeSource(  # noqa: E501\n                claim_name='azure-managed-file')\n            )\n        ).add_volume_mount(k8s_client.V1VolumeMount(\n            mount_path='/mnt/azure', name='azure'))\n\n\nif __name__ == '__main__':\n    compiler.Compiler().compile(default_train, __file__ + '.tar.gz')\n"
  },
  {
    "repo": "redhat-ai-services/kubeflow-pipelines-examples",
    "file_path": "pipelines/00_compiled_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/redhat-ai-services/kubeflow-pipelines-examples/main/pipelines/00_compiled_pipeline.py",
    "content": "\"\"\"Example of a pipeline built from inline functions with kfp and compiled to yaml.\"\"\"\n\nimport kfp.compiler\nfrom kfp import dsl\n\n\n@dsl.component(base_image=\"image-registry.openshift-image-registry.svc:5000/openshift/python:latest\")\ndef add(a: float, b: float) -> float:\n    \"\"\"Calculate the sum of the two arguments.\"\"\"\n    return a + b\n\n\n@dsl.pipeline()\ndef add_pipeline(a: float = 1.0, b: float = 7.0):\n    \"\"\"Pipeline to add values.\n\n    Pipeline to take the value of a, add 4 to it and then\n    perform a second task to take the put of the first task and add b.\n    \"\"\"\n    first_add_task = add(a=a, b=4.0)\n    second_add_task = add(a=first_add_task.output, b=b)  # noqa: F841\n\n\nif __name__ == \"__main__\":\n    kfp.compiler.Compiler().compile(add_pipeline, package_path=__file__.replace(\".py\", \".yaml\"))\n"
  },
  {
    "repo": "redhat-ai-services/kubeflow-pipelines-examples",
    "file_path": "pipelines/02_submitted_pipeline_via_route.py",
    "raw_url": "https://raw.githubusercontent.com/redhat-ai-services/kubeflow-pipelines-examples/main/pipelines/02_submitted_pipeline_via_route.py",
    "content": "\"\"\"Example of a pipeline submitted directly to kfp.\"\"\"\n\nimport os\n\nimport kfp.compiler\nfrom dotenv import load_dotenv\nfrom kfp import dsl\n\nload_dotenv(override=True)\n\nkubeflow_endpoint = os.environ[\"KUBEFLOW_ENDPOINT\"]\nbearer_token = os.environ[\"BEARER_TOKEN\"]\n\n\n@dsl.component(base_image=\"image-registry.openshift-image-registry.svc:5000/openshift/python:latest\")\ndef add(a: float, b: float) -> float:\n    \"\"\"Calculate the sum of the two arguments.\"\"\"\n    return a + b\n\n\n@dsl.pipeline()\ndef add_pipeline(a: float = 1.0, b: float = 7.0):\n    \"\"\"Pipeline to add values.\n\n    Pipeline to take the value of a, add 4 to it and then\n    perform a second task to take the put of the first task and add b.\n    \"\"\"\n    first_add_task = add(a=a, b=4.0)\n    second_add_task = add(a=first_add_task.output, b=b)  # noqa: F841\n\n\nif __name__ == \"__main__\":\n    client = kfp.Client(\n        host=kubeflow_endpoint,\n        existing_token=bearer_token,\n    )\n    arguments = {\"a\": 7.0, \"b\": 8.0}\n    client.create_run_from_pipeline_func(add_pipeline, arguments=arguments, experiment_name=\"submitted-example\")\n"
  },
  {
    "repo": "redhat-ai-services/kubeflow-pipelines-examples",
    "file_path": "pipelines/02_submitted_pipeline_via_service.py",
    "raw_url": "https://raw.githubusercontent.com/redhat-ai-services/kubeflow-pipelines-examples/main/pipelines/02_submitted_pipeline_via_service.py",
    "content": "\"\"\"Example of a pipeline submitted directly to kfp.\"\"\"\n\nimport os\n\nimport kfp.compiler\nfrom dotenv import load_dotenv\nfrom kfp import dsl\n\nload_dotenv(override=True)\n\nkubeflow_endpoint = \"https://ds-pipeline-dspa:8443\"\n\n\n@dsl.component(base_image=\"image-registry.openshift-image-registry.svc:5000/openshift/python:latest\")\ndef add(a: float, b: float) -> float:\n    \"\"\"Calculate the sum of the two arguments.\"\"\"\n    return a + b\n\n\n@dsl.pipeline()\ndef add_pipeline(a: float = 1.0, b: float = 7.0):\n    \"\"\"Pipeline to add values.\n\n    Pipeline to take the value of a, add 4 to it and then\n    perform a second task to take the put of the first task and add b.\n    \"\"\"\n    first_add_task = add(a=a, b=4.0)\n    second_add_task = add(a=first_add_task.output, b=b)  # noqa: F841\n\n\nif __name__ == \"__main__\":\n    # Check if the script is running in a k8s pod\n    # Read the service account token if it is\n    # Get the bearer token from an env var if it is not\n    # Note: The service account needs permission to access DSP instance in RBAC.\n    sa_token_path = \"/run/secrets/kubernetes.io/serviceaccount/token\"  # noqa: S105\n    if os.path.isfile(sa_token_path):\n        with open(sa_token_path) as f:\n            bearer_token = f.read().rstrip()\n    else:\n        bearer_token = os.environ[\"BEARER_TOKEN\"]\n\n    # Check if the script is running in a k8s pod\n    # Get the CA from the service account if it is\n    # Skip the CA if it is not\n    sa_ca_cert = \"/run/secrets/kubernetes.io/serviceaccount/service-ca.crt\"\n    if os.path.isfile(sa_ca_cert):\n        ssl_ca_cert = sa_ca_cert\n    else:\n        ssl_ca_cert = None\n\n    client = kfp.Client(\n        host=kubeflow_endpoint,\n        existing_token=bearer_token,\n        ssl_ca_cert=ssl_ca_cert,\n    )\n\n    arguments = {\"a\": 7.0, \"b\": 8.0}\n    client.create_run_from_pipeline_func(add_pipeline, arguments=arguments, experiment_name=\"submitted-example\")\n"
  },
  {
    "repo": "redhat-ai-services/kubeflow-pipelines-examples",
    "file_path": "pipelines/03_outputs_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/redhat-ai-services/kubeflow-pipelines-examples/main/pipelines/03_outputs_pipeline.py",
    "content": "\"\"\"Example of a pipeline returning multiple values.\"\"\"\n\nimport os\nfrom typing import NamedTuple\n\nimport kfp.compiler\nfrom dotenv import load_dotenv\nfrom kfp import dsl\n\nload_dotenv(override=True)\n\nkubeflow_endpoint = os.environ[\"KUBEFLOW_ENDPOINT\"]\nbearer_token = os.environ[\"BEARER_TOKEN\"]\n\n\n@dsl.component(base_image=\"image-registry.openshift-image-registry.svc:5000/openshift/python:latest\")\ndef return_multiple_values(a: float, b: float) -> NamedTuple(\"outputs\", [(\"sum\", float), (\"product\", float)]):\n    from collections import namedtuple\n\n    sum_result = a + b\n    product_result = a * b\n\n    outputs = namedtuple(\"outputs\", [\"sum\", \"product\"])\n    return outputs(sum_result, product_result)\n\n\n@kfp.dsl.pipeline(\n    name=\"Submitted Pipeline\",\n)\ndef multiple_values_pipeline(a: float = 1.0, b: float = 7.0):\n    first_task = return_multiple_values(a=a, b=b)\n    second_task = return_multiple_values(  # noqa: F841\n        a=first_task.outputs[\"sum\"], b=first_task.outputs[\"product\"]\n    )\n\n\nif __name__ == \"__main__\":\n    client = kfp.Client(\n        host=kubeflow_endpoint,\n        existing_token=bearer_token,\n    )\n\n    arguments = {\"a\": 7.0, \"b\": 8.0}\n    client.create_run_from_pipeline_func(\n        multiple_values_pipeline,\n        arguments=arguments,\n        experiment_name=\"outputs-example\",\n    )\n"
  },
  {
    "repo": "redhat-ai-services/kubeflow-pipelines-examples",
    "file_path": "pipelines/04_artifact_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/redhat-ai-services/kubeflow-pipelines-examples/main/pipelines/04_artifact_pipeline.py",
    "content": "\"\"\"Example of a pipeline to demonstrate accessing secrets/config maps in a pipeline.\"\"\"\n\nimport os\n\nimport kfp.compiler\nfrom dotenv import load_dotenv\nfrom kfp import dsl\n\nload_dotenv(override=True)\n\nkubeflow_endpoint = os.environ[\"KUBEFLOW_ENDPOINT\"]\nbearer_token = os.environ[\"BEARER_TOKEN\"]\n\n\n@dsl.component(base_image=\"image-registry.openshift-image-registry.svc:5000/openshift/python:latest\")\ndef create_artifact(my_artifact: dsl.Output[dsl.Artifact]):\n    import pickle\n\n    artifact = \"1, 2, 3, 4\"\n\n    with open(my_artifact.path, \"bw\") as f:\n        pickle.dump(artifact, f)\n\n\n@dsl.component(base_image=\"image-registry.openshift-image-registry.svc:5000/openshift/python:latest\")\ndef consume_artifact(my_artifact: dsl.Input[dsl.Artifact]):\n    import pickle\n\n    with open(my_artifact.path, \"br\") as f:\n        artifact = pickle.load(f)  # noqa: S301\n\n    print(artifact)\n\n\n@kfp.dsl.pipeline(\n    name=\"Artifact Pipeline\",\n)\ndef artifact_pipeline():\n    create_artifact_task = create_artifact()\n    consume_artifact_task = consume_artifact(  # noqa: F841\n        my_artifact=create_artifact_task.outputs[\"my_artifact\"]\n    )\n\n\nif __name__ == \"__main__\":\n    client = kfp.Client(\n        host=kubeflow_endpoint,\n        existing_token=bearer_token,\n    )\n    client.create_run_from_pipeline_func(artifact_pipeline, arguments={}, experiment_name=\"artifact-example\")\n"
  },
  {
    "repo": "redhat-ai-services/kubeflow-pipelines-examples",
    "file_path": "pipelines/05_metrics_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/redhat-ai-services/kubeflow-pipelines-examples/main/pipelines/05_metrics_pipeline.py",
    "content": "\"\"\"Example of a pipeline to demonstrate saving metrics from a pipeline.\n\nrunMetrics appear to be depreciated in kfp v2 api so implement\nthis feature at your own risk.\n\"\"\"\n\nimport os\n\nimport kfp.compiler\nfrom dotenv import load_dotenv\nfrom kfp import dsl\n\nload_dotenv(override=True)\n\nkubeflow_endpoint = os.environ[\"KUBEFLOW_ENDPOINT\"]\nbearer_token = os.environ[\"BEARER_TOKEN\"]\n\n\n@dsl.component(base_image=\"image-registry.openshift-image-registry.svc:5000/openshift/python:latest\")\ndef produce_metrics(\n    mlpipeline_metrics_path: dsl.OutputPath(\"Metrics\"),\n):\n    import json\n\n    accuracy = 0.9\n    mse = 0.1\n    metrics = {\n        \"metrics\": [\n            {\n                # The name of the metric. Visualized as the column name in the runs table.\n                \"name\": \"accuracy-score\",\n                # The value of the metric. Must be a numeric value.\n                \"numberValue\": accuracy,\n                # The optional format of the metric. Supported values are \"RAW\" (displayed in raw format) and\n                #  \"PERCENTAGE\" (displayed in percentage format).\n                \"format\": \"PERCENTAGE\",\n            },\n            {\n                # The name of the metric. Visualized as the column name in the runs table.\n                \"name\": \"mse-score\",\n                # The value of the metric. Must be a numeric value.\n                \"numberValue\": mse,\n                # The optional format of the metric. Supported values are \"RAW\" (displayed in raw format) and\n                #  \"PERCENTAGE\" (displayed in percentage format).\n                \"format\": \"RAW\",\n            },\n        ]\n    }\n\n    with open(mlpipeline_metrics_path, \"w\") as f:\n        json.dump(metrics, f)\n\n\n@kfp.dsl.pipeline(\n    name=\"metrics pipeline\",\n)\ndef metrics_pipeline():\n    produce_metrics_task = produce_metrics()  # noqa: F841\n\n\nif __name__ == \"__main__\":\n    client = kfp.Client(\n        host=kubeflow_endpoint,\n        existing_token=bearer_token,\n    )\n    arguments = {}\n    client.create_run_from_pipeline_func(metrics_pipeline, arguments=arguments, experiment_name=\"metrics-example\")\n"
  },
  {
    "repo": "redhat-ai-services/kubeflow-pipelines-examples",
    "file_path": "pipelines/06_visualization_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/redhat-ai-services/kubeflow-pipelines-examples/main/pipelines/06_visualization_pipeline.py",
    "content": "\"\"\"Example of a pipeline to demonstrate visualizations from a pipeline.\n\nThe visualization doesn't work for this.  PR's are welcome to help\nget a visualization functioning.\n\nThis pipeline example is currently broken.\n\"\"\"\n\nimport os\n\nimport kfp.compiler\nfrom dotenv import load_dotenv\nfrom kfp import dsl\n\nload_dotenv(override=True)\n\nkubeflow_endpoint = os.environ[\"KUBEFLOW_ENDPOINT\"]\nbearer_token = os.environ[\"BEARER_TOKEN\"]\n\n\n@dsl.component(base_image=\"image-registry.openshift-image-registry.svc:5000/openshift/python:latest\")\ndef confusion_matrix_viz(\n    mlpipeline_ui_metadata_path: dsl.OutputPath(),\n    confusion_matrix_path: dsl.OutputPath(),\n):\n    import json\n\n    cf = \"\"\"\n    16,0,0\n    0,11,0\n    0,0,18\n    \"\"\"\n\n    with open(confusion_matrix_path, \"w\") as text_file:\n        text_file.write(cf)\n\n    metadata = {\n        \"outputs\": [\n            {\n                \"type\": \"confusion_matrix\",\n                \"format\": \"csv\",\n                \"schema\": [\n                    {\"name\": \"target\", \"type\": \"CATEGORY\"},\n                    {\"name\": \"predicted\", \"type\": \"CATEGORY\"},\n                    {\"name\": \"count\", \"type\": \"NUMBER\"},\n                ],\n                \"source\": cf,\n                \"storage\": \"inline\",\n                \"labels\": [\"one\", \"two\", \"three\"],\n            }\n        ]\n    }\n\n    with open(mlpipeline_ui_metadata_path, \"w\") as metadata_file:\n        json.dump(metadata, metadata_file)\n\n    return cf\n\n\n@kfp.dsl.pipeline(\n    name=\"Metadata Example Pipeline\",\n    description=\"A pipeline that is built from inline functions\",\n)\ndef visualization_pipeline():\n    \"\"\"Pipeline to add values.\n\n    Pipeline to take the value of a, add 4 to it and then\n    perform a second task to take the put of the first task and add b.\n    \"\"\"\n    confusion_matrix_task = confusion_matrix_viz()  # noqa: F841\n\n\nif __name__ == \"__main__\":\n    client = kfp.Client(\n        host=kubeflow_endpoint,\n        existing_token=bearer_token,\n    )\n    client.create_run_from_pipeline_func(visualization_pipeline, arguments={}, experiment_name=\"visualization-example\")\n"
  },
  {
    "repo": "redhat-ai-services/kubeflow-pipelines-examples",
    "file_path": "pipelines/07_container_components_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/redhat-ai-services/kubeflow-pipelines-examples/main/pipelines/07_container_components_pipeline.py",
    "content": "\"\"\"Example of a pipeline to demonstrate running code built into the container image.\n\nThis pipeline uses the kfp.dsl.ContainerOp() function which throws some warnings.\nWould be nice to find a better way to run code build into the container image.\n\nThis pipeline example is currently broken.\n\"\"\"\n\nimport os\n\nimport kfp.compiler\nfrom dotenv import load_dotenv\n\nload_dotenv(override=True)\n\nkubeflow_endpoint = os.environ[\"KUBEFLOW_ENDPOINT\"]\nbearer_token = os.environ[\"BEARER_TOKEN\"]\n\n\n@kfp.dsl.pipeline(\n    name=\"container-pipeline\",\n)\ndef add_pipeline(a: float = 1.0, b: float = 7.0):\n    \"\"\"Pipeline to add values.\n\n    Pipeline to take the value of a, add 4 to it and then\n    perform a second task to take the put of the first task and add b.\n    \"\"\"\n    add_op = kfp.dsl.ContainerOp(  # noqa: F841\n        name=\"add\",\n        image=\"quay.io/rhiap/kubeflow-example:latest\",\n        command=[\"sh\", \"-c\"],\n        arguments=[\"python components/add.py\"],\n    )\n\n\nif __name__ == \"__main__\":\n    client = kfp.Client(\n        host=kubeflow_endpoint,\n        existing_token=bearer_token,\n    )\n    arguments = {\"a\": 7.0, \"b\": 8.0}\n    client.create_run_from_pipeline_func(add_pipeline, arguments=arguments, experiment_name=\"submitted-example\")\n"
  },
  {
    "repo": "redhat-ai-services/kubeflow-pipelines-examples",
    "file_path": "pipelines/08_additional_packages_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/redhat-ai-services/kubeflow-pipelines-examples/main/pipelines/08_additional_packages_pipeline.py",
    "content": "\"\"\"Example of a pipeline to demonstrate installing additional packages in the pipeline.\"\"\"\n\nimport os\n\nimport kfp.compiler\nfrom dotenv import load_dotenv\nfrom kfp import dsl\n\nload_dotenv(override=True)\n\nkubeflow_endpoint = os.environ[\"KUBEFLOW_ENDPOINT\"]\nbearer_token = os.environ[\"BEARER_TOKEN\"]\n\n\n@dsl.component(\n    base_image=\"image-registry.openshift-image-registry.svc:5000/openshift/python:latest\",\n    packages_to_install=[\"pandas\", \"scikit-learn\"],\n)\ndef get_iris_data():\n    import pandas as pd\n    from sklearn import datasets\n\n    iris = datasets.load_iris()\n    data = pd.DataFrame(\n        {\n            \"sepalLength\": iris.data[:, 0],\n            \"sepalWidth\": iris.data[:, 1],\n            \"petalLength\": iris.data[:, 2],\n            \"petalWidth\": iris.data[:, 3],\n            \"species\": iris.target,\n        }\n    )\n\n    print(data.head())\n\n\n@kfp.dsl.pipeline(\n    name=\"Additional Packages Pipeline\",\n)\ndef additional_packages_pipeline():\n    get_iris_data_task = get_iris_data()  # noqa: F841\n\n\nif __name__ == \"__main__\":\n    client = kfp.Client(\n        host=kubeflow_endpoint,\n        existing_token=bearer_token,\n    )\n\n    client.create_run_from_pipeline_func(\n        additional_packages_pipeline,\n        arguments={},\n        experiment_name=\"additional-packages-example\",\n    )\n"
  },
  {
    "repo": "redhat-ai-services/kubeflow-pipelines-examples",
    "file_path": "pipelines/09_secrets_cm_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/redhat-ai-services/kubeflow-pipelines-examples/main/pipelines/09_secrets_cm_pipeline.py",
    "content": "\"\"\"Example of a pipeline to demonstrate accessing secrets/config maps in a pipeline.\"\"\"\n\nimport os\n\nimport kfp.compiler\nfrom dotenv import load_dotenv\nfrom kfp import dsl\nfrom kfp import kubernetes\n\nload_dotenv(override=True)\n\nkubeflow_endpoint = os.environ[\"KUBEFLOW_ENDPOINT\"]\nbearer_token = os.environ[\"BEARER_TOKEN\"]\n\n\n@dsl.component(base_image=\"image-registry.openshift-image-registry.svc:5000/openshift/python:latest\")\ndef print_envvar(env_var: str):\n    import os\n\n    var_value = os.environ[env_var]\n    print(f\"my var value: {var_value}\")\n\n\n@kfp.dsl.pipeline(\n    name=\"Env Vars Pipeline\",\n)\ndef env_vars_pipeline():\n    \"\"\"Pipeline to add values.\n\n    Pipeline to take the value of a, add 4 to it and then\n    perform a second task to take the put of the first task and add b.\n    \"\"\"\n    secret_print_task = print_envvar(env_var=\"my-secret-env-var\")\n    secret_print_task.set_caching_options(False)\n    kubernetes.use_secret_as_env(\n        secret_print_task, secret_name=\"my-secret\", secret_key_to_env={\"my-secret-data\": \"my-secret-env-var\"}\n    )\n\n    cm_print_task = print_envvar(env_var=\"my-cm-env-var\")\n    cm_print_task.set_caching_options(False)\n    kubernetes.use_config_map_as_env(\n        cm_print_task, config_map_name=\"my-configmap\", config_map_key_to_env={\"my-configmap-data\": \"my-cm-env-var\"}\n    )\n\n\nif __name__ == \"__main__\":\n    client = kfp.Client(\n        host=kubeflow_endpoint,\n        existing_token=bearer_token,\n    )\n    client.create_run_from_pipeline_func(env_vars_pipeline, arguments={}, experiment_name=\"secrets-configmap-example\")\n"
  },
  {
    "repo": "redhat-ai-services/kubeflow-pipelines-examples",
    "file_path": "pipelines/10_mount_pvc_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/redhat-ai-services/kubeflow-pipelines-examples/main/pipelines/10_mount_pvc_pipeline.py",
    "content": "\"\"\"Example of a pipeline to demonstrate mounting a pvc to a task in a pipeline.\"\"\"\n\nimport os\n\nimport kfp.compiler\nfrom dotenv import load_dotenv\nfrom kfp import dsl\nfrom kfp import kubernetes\n\nload_dotenv(override=True)\n\nkubeflow_endpoint = os.environ[\"KUBEFLOW_ENDPOINT\"]\nbearer_token = os.environ[\"BEARER_TOKEN\"]\n\n\n@dsl.component(base_image=\"image-registry.openshift-image-registry.svc:5000/openshift/python:latest\")\ndef add(a: float, b: float) -> float:\n    \"\"\"Calculate the sum of the two arguments.\"\"\"\n    return a + b\n\n\n@kfp.dsl.pipeline(\n    name=\"PVC Pipeline\",\n)\ndef add_pipeline(a: float = 1.0, b: float = 7.0):\n    \"\"\"Pipeline to add values.\n\n    Pipeline to take the value of a, add 4 to it and then\n    perform a second task to take the put of the first task and add b.\n    \"\"\"\n    first_add_task = add(a=a, b=4.0)\n    kubernetes.mount_pvc(first_add_task, pvc_name=\"my-data\", mount_path=\"/opt/data\")\n\n    second_add_task = add(a=first_add_task.output, b=b)  # noqa: F841\n\n\nif __name__ == \"__main__\":\n    client = kfp.Client(\n        host=kubeflow_endpoint,\n        existing_token=bearer_token,\n    )\n    arguments = {\"a\": 7.0, \"b\": 8.0}\n    client.create_run_from_pipeline_func(add_pipeline, arguments=arguments, experiment_name=\"pvc-example\")\n"
  },
  {
    "repo": "redhat-ai-services/kubeflow-pipelines-examples",
    "file_path": "pipelines/11_iris_training_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/redhat-ai-services/kubeflow-pipelines-examples/main/pipelines/11_iris_training_pipeline.py",
    "content": "\"\"\"Example of a pipeline to demonstrate a simple real world data science workflow.\"\"\"\n\nimport os\n\nimport kfp.compiler\nfrom dotenv import load_dotenv\nfrom kfp import dsl\n\nload_dotenv(override=True)\n\nkubeflow_endpoint = os.environ[\"KUBEFLOW_ENDPOINT\"]\nbase_image = os.getenv(\"BASE_IMAGE\", \"image-registry.openshift-image-registry.svc:5000/openshift/python:latest\")\n\n\n@dsl.component(\n    base_image=base_image,\n    packages_to_install=[\"pandas\", \"scikit-learn\"],\n)\ndef data_prep(\n    x_train_file: dsl.Output[dsl.Dataset],\n    x_test_file: dsl.Output[dsl.Dataset],\n    y_train_file: dsl.Output[dsl.Dataset],\n    y_test_file: dsl.Output[dsl.Dataset],\n):\n    import pickle\n\n    import pandas as pd\n    from sklearn import datasets\n    from sklearn.model_selection import train_test_split\n\n    def get_iris_data() -> pd.DataFrame:\n        iris = datasets.load_iris()\n        data = pd.DataFrame(\n            {\n                \"sepalLength\": iris.data[:, 0],\n                \"sepalWidth\": iris.data[:, 1],\n                \"petalLength\": iris.data[:, 2],\n                \"petalWidth\": iris.data[:, 3],\n                \"species\": iris.target,\n            }\n        )\n\n        print(\"Initial Dataset:\")\n        print(data.head())\n\n        return data\n\n    def create_training_set(dataset: pd.DataFrame, test_size: float = 0.3):\n        # Features\n        x = dataset[[\"sepalLength\", \"sepalWidth\", \"petalLength\", \"petalWidth\"]]\n        # Labels\n        y = dataset[\"species\"]\n\n        # Split dataset into training set and test set\n        x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_size, random_state=11)\n\n        return x_train, x_test, y_train, y_test\n\n    def save_pickle(object_file, target_object):\n        with open(object_file, \"wb\") as f:\n            pickle.dump(target_object, f)\n\n    dataset = get_iris_data()\n    x_train, x_test, y_train, y_test = create_training_set(dataset)\n\n    save_pickle(x_train_file.path, x_train)\n    save_pickle(x_test_file.path, x_test)\n    save_pickle(y_train_file.path, y_train)\n    save_pickle(y_test_file.path, y_test)\n\n\n@dsl.component(\n    base_image=base_image,\n    packages_to_install=[\"pandas\", \"scikit-learn\"],\n)\ndef validate_data():\n    pass\n\n\n@dsl.component(\n    base_image=base_image,\n    packages_to_install=[\"pandas\", \"scikit-learn\"],\n)\ndef train_model(\n    x_train_file: dsl.Input[dsl.Dataset],\n    y_train_file: dsl.Input[dsl.Dataset],\n    model_file: dsl.Output[dsl.Model],\n):\n    import pickle\n\n    import pandas as pd\n    from sklearn.ensemble import RandomForestClassifier\n\n    def load_pickle(object_file):\n        with open(object_file, \"rb\") as f:\n            target_object = pickle.load(f)  # noqa: S301\n\n        return target_object\n\n    def save_pickle(object_file, target_object):\n        with open(object_file, \"wb\") as f:\n            pickle.dump(target_object, f)\n\n    def train_iris(x_train: pd.DataFrame, y_train: pd.DataFrame):\n        model = RandomForestClassifier(n_estimators=100)\n        model.fit(x_train, y_train)\n\n        return model\n\n    x_train = load_pickle(x_train_file.path)\n    y_train = load_pickle(y_train_file.path)\n\n    model = train_iris(x_train, y_train)\n\n    save_pickle(model_file.path, model)\n\n\n@dsl.component(\n    base_image=base_image,\n    packages_to_install=[\"pandas\", \"scikit-learn\"],\n)\ndef evaluate_model(\n    x_test_file: dsl.Input[dsl.Dataset],\n    y_test_file: dsl.Input[dsl.Dataset],\n    model_file: dsl.Input[dsl.Model],\n    mlpipeline_metrics_file: dsl.Output[dsl.Metrics],\n):\n    import json\n    import pickle\n\n    from sklearn.metrics import accuracy_score\n\n    def load_pickle(object_file):\n        with open(object_file, \"rb\") as f:\n            target_object = pickle.load(f)  # noqa: S301\n\n        return target_object\n\n    x_test = load_pickle(x_test_file.path)\n    y_test = load_pickle(y_test_file.path)\n    model = load_pickle(model_file.path)\n\n    y_pred = model.predict(x_test)\n\n    accuracy_score_metric = accuracy_score(y_test, y_pred)\n    print(f\"Accuracy: {accuracy_score_metric}\")\n\n    metrics = {\n        \"metrics\": [\n            {\n                \"name\": \"accuracy-score\",\n                \"numberValue\": accuracy_score_metric,\n                \"format\": \"PERCENTAGE\",\n            },\n        ]\n    }\n\n    with open(mlpipeline_metrics_file.path, \"w\") as f:\n        json.dump(metrics, f)\n\n\n@dsl.component(\n    base_image=base_image,\n    packages_to_install=[\"pandas\", \"skl2onnx\"],\n)\ndef model_to_onnx(model_file: dsl.Input[dsl.Model], onnx_model_file: dsl.Output[dsl.Model]):\n    import pickle\n\n    from skl2onnx import to_onnx\n    from skl2onnx.common.data_types import FloatTensorType\n\n    def load_pickle(object_file):\n        with open(object_file, \"rb\") as f:\n            target_object = pickle.load(f)  # noqa: S301\n\n        return target_object\n\n    model = load_pickle(model_file.path)\n\n    initial_type = [(\"float_input\", FloatTensorType([None, 4]))]\n    onnx_model = to_onnx(model, initial_types=initial_type)\n\n    with open(onnx_model_file.path, \"wb\") as f:\n        f.write(onnx_model.SerializeToString())\n\n\n@dsl.component(\n    base_image=base_image,\n    packages_to_install=[\"pandas\", \"onnxruntime\"],\n)\ndef validate_model(onnx_model_file: dsl.Input[dsl.Model]):\n    import onnxruntime\n\n    session = onnxruntime.InferenceSession(onnx_model_file.path)\n\n    input_name = session.get_inputs()[0].name\n    label_name = session.get_outputs()[0].name\n\n    input_values = [[5, 3, 1.6, 0.2]]\n\n    print(f\"Performing test prediction on {input_values}\")\n    result = session.run([label_name], {input_name: input_values})[0]\n\n    print(f\"Response: {result}\")\n\n\n@kfp.dsl.pipeline(\n    name=\"Iris Pipeline\",\n)\ndef iris_pipeline(model_obc: str = \"iris-model\"):\n    data_prep_task = data_prep()\n\n    train_model_task = train_model(\n        x_train_file=data_prep_task.outputs[\"x_train_file\"],\n        y_train_file=data_prep_task.outputs[\"y_train_file\"],\n    )\n\n    evaluate_model_task = evaluate_model(  # noqa: F841\n        x_test_file=data_prep_task.outputs[\"x_test_file\"],\n        y_test_file=data_prep_task.outputs[\"y_test_file\"],\n        model_file=train_model_task.output,\n    )\n\n    model_to_onnx_task = model_to_onnx(  # noqa: F841\n        model_file=train_model_task.output,\n    )\n\n    validate_model_task = validate_model(onnx_model_file=model_to_onnx_task.output)  # noqa: F841\n\n\nif __name__ == \"__main__\":\n    print(f\"Connecting to kfp: {kubeflow_endpoint}\")\n\n    sa_token_path = \"/run/secrets/kubernetes.io/serviceaccount/token\"  # noqa: S105\n    if \"BEARER_TOKEN\" in os.environ:\n        bearer_token = os.environ[\"BEARER_TOKEN\"]\n    elif os.path.isfile(sa_token_path):\n        with open(sa_token_path) as f:\n            bearer_token = f.read().rstrip()\n\n    # Check if the script is running in a k8s pod\n    # Get the CA from the service account if it is\n    # Skip the CA if it is not\n    sa_ca_cert = \"/run/secrets/kubernetes.io/serviceaccount/service-ca.crt\"\n    if os.path.isfile(sa_ca_cert) and \"svc\" in kubeflow_endpoint:\n        ssl_ca_cert = sa_ca_cert\n    else:\n        ssl_ca_cert = None\n\n    client = kfp.Client(\n        host=kubeflow_endpoint,\n        existing_token=bearer_token,\n        ssl_ca_cert=ssl_ca_cert,\n    )\n    result = client.create_run_from_pipeline_func(iris_pipeline, arguments={}, experiment_name=\"iris\")\n    print(f\"Starting pipeline run with run_id: {result.run_id}\")\n"
  },
  {
    "repo": "redhat-ai-services/kubeflow-pipelines-examples",
    "file_path": "pipelines/12_gpu_task_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/redhat-ai-services/kubeflow-pipelines-examples/main/pipelines/12_gpu_task_pipeline.py",
    "content": "\"\"\"Example of a pipeline using a GPU.\"\"\"\n\nimport os\n\nimport kfp.compiler\nfrom kfp import dsl, kubernetes\n\nfrom dotenv import load_dotenv\n\nload_dotenv(override=True)\n\nkubeflow_endpoint = os.environ[\"KUBEFLOW_ENDPOINT\"]\nbearer_token = os.environ[\"BEARER_TOKEN\"]\n\n\n@dsl.component(base_image=\"quay.io/modh/cuda-notebooks:cuda-jupyter-minimal-ubi9-python-3.11-20250326\")\ndef nvidia_smi():\n    \"\"\"Use the nvidia-smi command to \"\"\"\n    import os\n\n    os.system(\"nvidia-smi\")\n\n\n@dsl.pipeline()\ndef nvidia_smi_pipeline():\n    \"\"\"Pipeline to execute a task using GPUs\n    \"\"\"\n    nvidia_smi_task = nvidia_smi()\n\n    # Set the accelerator type and set the request/limit to 1\n    # Note: You cannot request different values for the request/limit\n    nvidia_smi_task.set_accelerator_type(\"nvidia.com/gpu\").set_accelerator_limit(1)\n\n    # Set the toleration for the GPU node\n    kubernetes.add_toleration(nvidia_smi_task, key=\"nvidia.com/gpu\", operator=\"Exists\", effect=\"NoSchedule\")\n\n\nif __name__ == \"__main__\":\n    print(f\"Connecting to kfp: {kubeflow_endpoint}\")\n\n    sa_token_path = \"/run/secrets/kubernetes.io/serviceaccount/token\"  # noqa: S105\n    if \"BEARER_TOKEN\" in os.environ:\n        bearer_token = os.environ[\"BEARER_TOKEN\"]\n    elif os.path.isfile(sa_token_path):\n        with open(sa_token_path) as f:\n            bearer_token = f.read().rstrip()\n\n    # Check if the script is running in a k8s pod\n    # Get the CA from the service account if it is\n    # Skip the CA if it is not\n    sa_ca_cert = \"/run/secrets/kubernetes.io/serviceaccount/service-ca.crt\"\n    if os.path.isfile(sa_ca_cert) and \"svc\" in kubeflow_endpoint:\n        ssl_ca_cert = sa_ca_cert\n    else:\n        ssl_ca_cert = None\n\n    client = kfp.Client(\n        host=kubeflow_endpoint,\n        existing_token=bearer_token,\n        ssl_ca_cert=ssl_ca_cert,\n    )\n    result = client.create_run_from_pipeline_func(nvidia_smi_pipeline, arguments={}, experiment_name=\"iris\")\n    print(f\"Starting pipeline run with run_id: {result.run_id}\")\n"
  },
  {
    "repo": "google/vertex-pipelines-boilerplate",
    "file_path": "src/pipelines/sample_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/google/vertex-pipelines-boilerplate/main/src/pipelines/sample_pipeline.py",
    "content": "# Copyright 2022 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Sample Kubeflow pipeline.\"\"\"\n\n\nimport kfp\nfrom kfp.v2 import dsl\n\n\n@dsl.component(base_image=\"python:3.10\", packages_to_install=[\"cloudpathlib==0.10.0\"])\ndef _save_message_to_file(message: str, gcs_filepath: str) -> None:\n    \"\"\"Saves a given message to a given file in GCS.\"\"\"\n    import cloudpathlib as cpl\n\n    with cpl.CloudPath(gcs_filepath).open(\"w\") as fp:\n        fp.write(message)\n\n\n@kfp.dsl.pipeline(name=\"sample-pipeline\")\ndef pipeline(message: str, gcs_filepath: str) -> None:\n    \"\"\"Sample Kubeflow pipeline definition.\"\"\"\n    _save_message_to_file(message, gcs_filepath)\n"
  },
  {
    "repo": "lynnmatrix/kfp-local",
    "file_path": "kfp_local/local_client_test.py",
    "raw_url": "https://raw.githubusercontent.com/lynnmatrix/kfp-local/master/kfp_local/local_client_test.py",
    "content": "import unittest\nfrom typing import Union\n\nfrom kfp.v2 import dsl\n\nfrom kfp_local import LocalClient\n\n\n@dsl.component\ndef hello(name: str) -> str:\n    return f\"hello {name}\"\n\n\n@dsl.component\ndef local_loader(src: str, dst: dsl.OutputPath(str)):\n    import os\n    import shutil\n\n    if os.path.exists(src):\n        shutil.copyfile(src, dst)\n\n\n@dsl.component\ndef flip_coin() -> str:\n    import random\n\n    return \"head\" if random.randint(0, 1) == 0 else \"tail\"\n\n\n@dsl.component\ndef component_with_inputpath(src: dsl.InputPath()) -> str:\n    with open(src, \"r\") as f:\n        return f.read()\n\n\n@dsl.component\ndef component_return_artifact(content: str) -> dsl.Artifact:\n    return content\n\n\n@dsl.component\ndef component_consume_artifact(artifact: dsl.Input[dsl.Artifact]) -> str:\n    with open(artifact.path, \"r\") as f:\n        return f.read()\n\n\nclass LocalRunnerTest(unittest.TestCase):\n    def setUp(self):\n        self.local_client = LocalClient()\n        import tempfile\n\n        with tempfile.NamedTemporaryFile(\"w\", delete=False) as f:\n            self.temp_file_path = f.name\n            f.write(\"hello world\")\n\n    def get_param_output(self, result, task_name: str, output_name: str = None) -> str:\n        with open(result.get_output_file(task_name, output_name), \"r\") as f:\n            return f.read()\n\n    def get_param_return(self, result, task_name: str, output_name: str = None):\n        with open(result.get_output_file(task_name, \"output_metadata.json\"), \"r\") as f:\n            from google.protobuf import json_format\n            from kfp.pipeline_spec import pipeline_spec_pb2\n\n            metadata = pipeline_spec_pb2.ExecutorOutput()\n            json_format.Parse(f.read(), metadata)\n            output = metadata.parameters[output_name or \"Output\"]\n            value_type = output.WhichOneof(\"value\")\n            if value_type == \"string_value\":\n                output_value = output.string_value\n            elif value_type == \"int_value\":\n                output_value = output.int_value\n            elif value_type == \"double_value\":\n                output_value = output.double_value\n            else:\n                output_value = None\n            return output_value\n\n    def assert_func_param_return_equal(\n        self,\n        content: Union[str, int, float, bool, dict, list],\n        result,\n        task_name: str,\n        output_name: str = None,\n    ):\n        output_value = self.get_param_return(result, task_name, output_name)\n\n        if type(output_value) == str and type(content) != str:\n            import json\n\n            content = json.dumps(content)\n\n        self.assertEqual(content, output_value)\n\n    def test_run_local(self):\n        @dsl.pipeline(name=\"test-run-local-pipeline\")\n        def _pipeline(name: str):\n            hello(name)\n\n        result = self.local_client.create_run_from_pipeline_func(\n            _pipeline,\n            {\"name\": \"world\"},\n            execution_mode=LocalClient.ExecutionMode(\"local\"),\n        )\n        self.assertTrue(result.success)\n        self.assert_func_param_return_equal(\"hello world\", result, \"hello\")\n\n    def test_local_file(self):\n        @dsl.pipeline(name=\"test-local-file-pipeline\")\n        def _pipeline(file_path: str):\n            local_loader(file_path)\n\n        run_result = self.local_client.create_run_from_pipeline_func(\n            _pipeline,\n            {\"file_path\": self.temp_file_path},\n            execution_mode=LocalClient.ExecutionMode(\"local\"),\n        )\n        self.assertEqual(\n            \"hello world\", self.get_param_output(run_result, \"local-loader\", \"dst\")\n        )\n\n    def test_condition(self):\n        @dsl.pipeline(name=\"test-condition\")\n        def _pipeline():\n            _flip = flip_coin()\n            with dsl.Condition(_flip.output == \"head\"):\n                hello(\"head\")\n\n            with dsl.Condition(_flip.output == \"tail\"):\n                hello(\"tail\")\n\n        run_result = self.local_client.create_run_from_pipeline_func(\n            _pipeline, {}, execution_mode=LocalClient.ExecutionMode(\"local\")\n        )\n        self.assertTrue(run_result.success)\n        coin_result = self.get_param_return(run_result, \"flip-coin\")\n        if coin_result == \"head\":\n            self.assert_func_param_return_equal(\"hello head\", run_result, \"hello\")\n        else:\n            self.assert_func_param_return_equal(\"hello tail\", run_result, \"hello-2\")\n\n    def test_loop(self):\n        @dsl.pipeline(name=\"for-pipeline\")\n        def _pipeline():\n            with dsl.ParallelFor([\"hello\", \"world\"]) as item:\n                hello(item)\n\n        run_result = self.local_client.create_run_from_pipeline_func(\n            _pipeline, {}, execution_mode=LocalClient.ExecutionMode(\"local\")\n        )\n        self.assertTrue(run_result.success)\n        self.assertEqual(\n            \"hello world\", self.get_param_return(result=run_result, task_name=\"hello\")\n        )\n\n    def test_nest_loop(self):\n        @dsl.pipeline(name=\"test-nest-loop\")\n        def _pipeline():\n            loop_parameter = [[1, 2], [3, 4]]\n            with dsl.ParallelFor(loop_parameter) as item:\n                with dsl.ParallelFor(item) as item_a:\n                    hello(item)\n                    hello(item_a)\n\n        run_result = self.local_client.create_run_from_pipeline_func(\n            _pipeline, {}, execution_mode=LocalClient.ExecutionMode(\"local\")\n        )\n        self.assertTrue(run_result.success)\n        self.assertEqual(\"hello 4\", self.get_param_return(run_result, \"hello-2\"))\n\n    def test_exit_handler(self):\n        @dsl.pipeline(\"test-exit-handler\")\n        def _pipeline():\n            with dsl.ExitHandler(exit_op=hello(\"exit\")):\n                hello(\"inner\")\n\n        run_result = self.local_client.create_run_from_pipeline_func(\n            _pipeline, {}, execution_mode=LocalClient.ExecutionMode(\"local\")\n        )\n\n        self.assertTrue(run_result.success)\n        self.assertEqual(\"hello exit\", self.get_param_return(run_result, \"hello\"))\n        self.assertEqual(\"hello inner\", self.get_param_return(run_result, \"hello-2\"))\n\n    @unittest.skip(\"docker is not installed in CI environment.\")\n    def test_connect_artifact(self):\n        @dsl.pipeline(name=\"test-connect-artifact\")\n        def _pipeline():\n            input_component = component_return_artifact(\"hello world\")\n            component_with_inputpath(input_component.output)\n\n        run_result = self.local_client.create_run_from_pipeline_func(\n            _pipeline,\n            {},\n            execution_mode=LocalClient.ExecutionMode(\"docker\"),\n        )\n        self.assertTrue(run_result.success)\n        self.assertEqual(\n            \"hello world\",\n            self.get_param_return(run_result, \"component-with-inputpath\"),\n        )\n\n    @unittest.skip(\"docker is not installed in CI environment.\")\n    def test_output_artifact(self):\n        @dsl.pipeline(name=\"test-output-artifact\")\n        def _pipeline():\n            component1 = component_return_artifact(\"artifact content\")\n            component_consume_artifact(component1.output)\n\n        run_result = self.local_client.create_run_from_pipeline_func(\n            _pipeline, {}, execution_mode=LocalClient.ExecutionMode(\"docker\")\n        )\n        self.assertTrue(run_result.success)\n        self.assertEqual(\n            \"artifact content\",\n            self.get_param_return(run_result, \"component-consume-artifact\"),\n        )\n\n    @unittest.skip(\"docker is not installed in CI environment.\")\n    def test_execution_mode_exclude_op(self):\n        @dsl.component(base_image=\"image_not_exist\")\n        def cat_on_image_not_exist(name: str) -> str:\n            return name\n\n        @dsl.pipeline(name=\"test-execution-mode-exclude-op\")\n        def _pipeline():\n            cat_on_image_not_exist(\"exclude tasks\")\n\n        run_result = self.local_client.create_run_from_pipeline_func(\n            _pipeline,\n            {},\n            execution_mode=LocalClient.ExecutionMode(mode=\"docker\"),\n        )\n        self.assertFalse(run_result.success)\n\n        run_result = self.local_client.create_run_from_pipeline_func(\n            _pipeline,\n            {},\n            execution_mode=LocalClient.ExecutionMode(\n                mode=\"docker\", tasks_to_exclude=[\"cat-on-image-not-exist\"]\n            ),\n        )\n        self.assertTrue(run_result.success)\n        self.assertEqual(\n            \"exclude tasks\",\n            self.get_param_return(run_result, \"cat-on-image-not-exist\"),\n        )\n\n    @unittest.skip(\"docker is not installed in CI environment.\")\n    def test_docker_options(self):\n        @dsl.component\n        def check_option(env_name: str) -> str:\n            import os\n\n            return os.environ[env_name]\n\n        @dsl.pipeline(name=\"test-docker-options\")\n        def _pipeline():\n            check_option(\"foo\")\n\n        run_result = self.local_client.create_run_from_pipeline_func(\n            _pipeline,\n            {},\n            execution_mode=LocalClient.ExecutionMode(\n                mode=\"docker\", docker_options=[\"-e\", \"foo=bar\"]\n            ),\n        )\n        assert run_result.success\n        self.assertEqual(\"bar\", self.get_param_return(run_result, \"check-option\"))\n"
  },
  {
    "repo": "deployKF/kubeflow-pipelines-gitops",
    "file_path": "step-1--render-pipelines/example_pipeline_1/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/deployKF/kubeflow-pipelines-gitops/main/step-1--render-pipelines/example_pipeline_1/pipeline.py",
    "content": "import argparse\nimport logging\nimport os\nimport sys\nfrom typing import List, NamedTuple\n\nfrom kfp import dsl, compiler, components\n\n#########################################################################################\n# Logging\n#########################################################################################\nlogger = logging.getLogger(__name__)\nlogger.propagate = False\nlogger.setLevel(logging.INFO)\nhandler = logging.StreamHandler()\nformatter = logging.Formatter(\n    fmt=\"%(asctime)s %(levelname)-8s %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\"\n)\nhandler.setFormatter(formatter)\nlogger.addHandler(handler)\n\n\n#########################################################################################\n# Arguments\n#########################################################################################\ndef _parse_args(args: List[str]) -> argparse.Namespace:\n    parser = argparse.ArgumentParser(\n        description=\"Render Kubeflow Pipeline into a folder containing a 'workflow.yaml' file and 'params/' folder.\"\n    )\n    parser.add_argument(\n        \"--output-folder\",\n        help=\"The path to render the pipeline into (must NOT already exist)\",\n        required=True,\n    )\n    return parser.parse_args(args)\n\n\n####################################################################################################\n# Functions\n####################################################################################################\ndef step_0__func() -> NamedTuple(\"Outputs\", utc_epoch=int, day_of_week=str):\n    import datetime\n    from collections import namedtuple\n\n    # get the current time\n    now_utc = datetime.datetime.utcnow()\n    utc_epoch = int(now_utc.timestamp())\n    day_of_week = now_utc.strftime(\"%A\")\n\n    # return namedtuple() with KFP component outputs\n    # https://www.kubeflow.org/docs/components/pipelines/v1/sdk/python-function-components/#building-python-function-based-components\n    step_outputs = namedtuple(\"Outputs\", [\"utc_epoch\", \"day_of_week\"])\n    return step_outputs(utc_epoch=utc_epoch, day_of_week=day_of_week)\n\n\ndef step_1__func(message: components.InputPath(str)):\n    print(message)\n\n\n####################################################################################################\n# Main\n####################################################################################################\ndef main(args: List[str]):\n    # parse CLI arguments\n    args = _parse_args(args)\n\n    # ensure the output folder does not already exist, or is empty\n    if os.path.exists(args.output_folder):\n        if os.listdir(args.output_folder):\n            logger.error(\n                f\"The output folder already exists, but is not empty: {args.output_folder}\"\n            )\n            sys.exit(1)\n\n    ################################\n    # pipeline components\n    ################################\n    step_0__op = components.create_component_from_func(\n        func=step_0__func, base_image=\"python:3.10\"\n    )\n\n    step_1__op = components.create_component_from_func(\n        func=step_1__func, base_image=\"python:3.10\"\n    )\n\n    step_2__op = components.load_component_from_file(filename=\"example_component.yaml\")\n\n    ################################\n    # pipeline definition\n    ################################\n    @dsl.pipeline(name=\"pipeline_1\", description=\"pipeline_1 description\")\n    def pipeline(custom_message: str):\n        ################################\n        # pipeline step 0\n        ################################\n        step_0 = step_0__op()\n        step_0.set_display_name(\"STEP 0: Get Current Date\")\n\n        # disable caching for this step\n        step_0.set_caching_options(False)\n        step_0.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n\n        # unpack outputs from the step\n        current_utc_epoch__ref = step_0.outputs[\"utc_epoch\"]\n        day_of_week__ref = step_0.outputs[\"day_of_week\"]\n\n        # build message string\n        message_string = \"\\n\".join(\n            [\n                f\"current_utc_epoch: {current_utc_epoch__ref}\",\n                f\"day_of_week: {day_of_week__ref}\",\n                f\"custom_message: {custom_message}\",\n            ]\n        )\n\n        ################################\n        # pipeline step 1\n        ################################\n        step_1 = step_1__op(message=message_string)\n        step_1.set_display_name(\"STEP 1: Print Message (Python Function Component)\")\n\n        ################################\n        # pipeline step 2\n        ################################\n        step_2 = step_2__op(message=message_string)\n        step_2.set_display_name(\"STEP 2: Print Message (YAML Component)\")\n\n    ################################\n    # pipeline parameters\n    ################################\n    pipeline_params = {\n        \"custom_message\": \"Hello world!\\n\" * 10,\n    }\n\n    ################################\n    # render pipeline\n    ################################\n    output_folder_path = args.output_folder\n    workflow_yaml_path = os.path.join(output_folder_path, \"workflow.yaml\")\n    params_folder_path = os.path.join(output_folder_path, \"params\")\n\n    # create the output folder\n    os.makedirs(output_folder_path, exist_ok=True)\n\n    # write the 'workflow.yaml' file\n    compiler.Compiler().compile(pipeline_func=pipeline, package_path=workflow_yaml_path)\n\n    # create the 'params/' folder\n    os.makedirs(params_folder_path, exist_ok=True)\n\n    # write parameters to files in the 'params/' folder\n    for param_name, param_value in pipeline_params.items():\n        param_file_path = os.path.join(params_folder_path, param_name)\n        with open(param_file_path, \"w\") as f:\n            f.write(param_value)\n\n\nif __name__ == \"__main__\":\n    main(args=sys.argv[1:])\n"
  },
  {
    "repo": "muhammadyaseen/kubeflow-on-linode",
    "file_path": "kfp-examples/01_markdown-visualization-pipeline/md_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/muhammadyaseen/kubeflow-on-linode/main/kfp-examples/01_markdown-visualization-pipeline/md_pipeline.py",
    "content": "\"\"\"\nA simple pipeline to show some Markdown visualizations\n\nFor some reason, Kubeflow Pipelines UI can't render Markdown tables correctly when \npython's triple quoted strings are used. This is the reason you are seeing these long strings.\n\nFor details, see this issue: https://github.com/kubeflow/pipelines/issues/10182\n\"\"\"\n\nimport kfp\nfrom kfp.dsl import Output, Dataset, Input, Markdown\n\n\n@kfp.dsl.component\ndef write_simple_markdown_table(markdown_artifact: Output[Markdown]):\n\n    markdown_content = \"| Num   | animal1   | animal_2   | \\n |---|-----------|-----------| \\n |  0 | elk        | dog        | \\n |  1 | pig        | quetzal    | \"\n\n    with open(markdown_artifact.path, 'w') as f:\n        f.write(markdown_content)\n\n@kfp.dsl.component\ndef write_simple_markdown_heading(markdown_artifact: Output[Markdown]):\n    \n    markdown_content = '## Hello world \\n\\n Markdown content'\n    with open(markdown_artifact.path, 'w') as f:\n        f.write(markdown_content)\n\n@kfp.dsl.component\ndef vertex_ai_markdown_example(md_artifact: Output[Markdown]):\n\n    table_md = \"## Table Visualization \\n | ID \t| Name  \t| Type   \t| Correctness \t| Count \t|  \\n  |----\t|:-------:\t|:--------:\t|:-------------:\t|:-------:\t|  \\n | 1  \t| Apple \t| Fruit  \t| 60%         \t| 40    \t|  \\n  | 2  \t| Cat   \t| Animal \t| 30%         \t| 156   \t|  \\n  | 3  \t| Dog   \t| Animal \t| 90%         \t| 592   \t|\"\n\n    with open(md_artifact.path, 'w') as f:\n        f.write(table_md)\n\n\n@kfp.dsl.component(packages_to_install=['pandas'])\ndef write_pandas_dataframe_as_markdown(df_as_md: Output[Markdown]):\n\n    import pandas as pd\n    \n    df = pd.DataFrame(\n        data={\"animal_1\": [\"elk\", \"pig\"], \"animal_2\": [\"dog\", \"quetzal\"]}\n    )\n    \n    df_as_md_str = df.to_markdown()\n    \n    with open(df_as_md.path,'w') as f:\n        f.write(df_as_md_str)\n\n\n@kfp.dsl.pipeline(\n    name='md-pipeline',\n    description='Markdown Pipeline.'\n)\ndef markdown_pipeline():\n    \n    component1 = write_simple_markdown_table()\n    component2 = write_simple_markdown_heading()\n    component3 = vertex_ai_markdown_example()\n    component3 = write_pandas_dataframe_as_markdown()\n\n\nif __name__ == \"__main__\":\n\n    kfp.compiler.Compiler().compile(\n        pipeline_func=markdown_pipeline,\n        package_path=\"md-pipeline.yaml\"\n    )\n"
  },
  {
    "repo": "muhammadyaseen/kubeflow-on-linode",
    "file_path": "kfp-examples/02_simple-minio-pipeline/minio_census_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/muhammadyaseen/kubeflow-on-linode/main/kfp-examples/02_simple-minio-pipeline/minio_census_pipeline.py",
    "content": "\"\"\"\nAdapted from: https://blog.min.io/building-an-ml-data-pipeline-with-minio-and-kubeflow-v2-0/\n\"\"\"\n\nimport kfp\nimport minio\n\nfrom kfp.dsl import Output, Dataset, Input\n\n\n@kfp.dsl.component(packages_to_install=['minio==7.1.14'])\ndef check_if_table_data_exists_already(bucket: str, table_code: str, year: int) -> bool:\n    '''\n    Check for the existence of Census table data in the given MinIO bucket.\n    '''\n    from minio import Minio\n    from minio.error import S3Error\n    import logging\n\n    object_name=f'{table_code}-{year}.csv'\n\n    logger = logging.getLogger('kfp_logger')\n    logger.setLevel(logging.INFO)\n    logger.info(bucket)\n    logger.info(table_code)\n    logger.info(year)\n    logger.info(object_name)\n  \n    try:\n        # Create client with access and secret key.\n        client = Minio('minio-service-ip-here:9000',\n                    'minio',\n                    'minio123',\n                    secure=False)\n\n        # First, check if the bucket exists\n        bucket_found = client.bucket_exists(bucket)\n        if not bucket_found:\n            return False\n\n        # If so, check if the file / object exists that contains the data\n        objects = client.list_objects(bucket)\n        found = False\n        \n        for obj in objects:\n            \n            logger.info(obj.object_name)\n            \n            if object_name == obj.object_name: \n                found = True\n\n    except S3Error as s3_err:\n        logger.error(f'S3 Error occurred: {s3_err}.')\n    except Exception as err:\n        logger.error(f'Error occurred: {err}.')\n\n    return found\n\n\n@kfp.dsl.component(packages_to_install=['pandas==1.3.5', 'requests'])\ndef download_table_data(dataset: str, table_code: str, year: int, table_df: Output[Dataset]):\n    '''\n    Returns all fields for the specified table. The output is a DataFrame saved to csv.\n    '''\n    import logging\n    import pandas as pd\n    import requests\n\n    logger = logging.getLogger('kfp_logger')\n    logger.setLevel(logging.INFO)\n\n    # e.g. dataset = acs\n    # https://www.census.gov/data/developers/data-sets/acs-5year/2021.html\n    census_endpoint = f'https://api.census.gov/data/{year}/{dataset}/acs5'\n    census_key = 'us-census-key-here'\n\n    # Setup a simple dictionary for the requests parameters.\n    get_token = f'group({table_code})'\n    params = {'key': census_key, 'get': get_token, 'for': 'county:*'}\n\n    # sending get request and saving the response as response object\n    response = requests.get(url=census_endpoint, params=params)\n\n    # Extract the data in json format.\n    # The first row of our matrix contains the column names. The remaining rows\n    # are the data.\n    survey_data = response.json()\n    \n    # Create a pandas df with the response data and write it as CSV\n    df = pd.DataFrame(survey_data[1:], columns = survey_data[0])\n    df.to_csv(table_df.path, index=False)\n\n    logger.info(f'Table {table_code} for {year} has been downloaded.')\n\n\n@kfp.dsl.component(packages_to_install=['pandas==1.3.5', 'minio==7.1.14'])\ndef save_table_data(bucket: str, table_code: str, year: int, table_df: Input[Dataset]):\n    '''\n    Save the data in the diven MinIO Bucket.\n    The input param `table_df` refers to the Dataset written by `download_table_data` func to an \n    intermediate location in pipeline root\n    '''\n    \n    import io\n    import logging\n    from minio import Minio\n    from minio.error import S3Error\n    import pandas as pd\n\n    object_name=f'{table_code}-{year}.csv'\n\n    logger = logging.getLogger('kfp_logger')\n    logger.setLevel(logging.INFO)\n    logger.info(bucket)\n    logger.info(table_code)\n    logger.info(year)\n    logger.info(object_name)\n\n    # Read in the data that was written by `download_table_data` func\n    df = pd.read_csv(table_df.path)\n\n    try:\n        # Create client with access and secret key\n        client = Minio('minio-service-ip-here:9000',\n                    'minio',\n                    'minio123',\n                    secure=False)\n\n        # Make the bucket if it does not exist.\n        found = client.bucket_exists(bucket)\n        if not found:\n            logger.info(f'Creating bucket: {bucket}.')\n            client.make_bucket(bucket)\n\n        # Upload the dataframe as an object.\n        encoded_df = df.to_csv(index=False).encode('utf-8')\n        client.put_object(bucket, object_name, data=io.BytesIO(encoded_df), length=len(encoded_df), content_type='application/csv')\n        logger.info(f'{object_name} successfully uploaded to bucket {bucket}.')\n        logger.info(f'Object length: {len(df)}.')\n\n    except S3Error as s3_err:\n        logger.error(f'S3 Error occurred: {s3_err}.')\n    except Exception as err:\n        logger.error(f'Error occurred: {err}.')\n\n\n@kfp.dsl.component(packages_to_install=['pandas==1.3.5', 'minio==7.1.14'])\ndef get_table_data(bucket: str, table_code: str, year: int, table_df: Output[Dataset]):\n    '''\n    Read CSV data from MinIO into a Pandas df\n    '''\n    \n    import io\n    import logging\n    from minio import Minio\n    from minio.error import S3Error\n    import pandas as pd\n\n    object_name=f'{table_code}-{year}.csv'\n\n    logger = logging.getLogger('kfp_logger')\n    logger.setLevel(logging.INFO)\n    logger.info(bucket)\n    logger.info(table_code)\n    logger.info(year)\n    logger.info(object_name)\n\n    # Get data of an object.\n    try:\n        # Create client with access and secret key\n        client = Minio('minio-service-ip-here:9000',\n                    'minio',\n                    'minio123',\n                    secure=False)\n\n        response = client.get_object(bucket, object_name)\n        df = pd.read_csv(io.BytesIO(response.data))\n        df.to_csv(table_df.path, index=False)\n        logger.info(f'Object: {object_name} has been retrieved from bucket: {bucket} in MinIO object storage.')\n        logger.info(f'Object length: {len(df)}.')\n\n    except S3Error as s3_err:\n        logger.error(f'S3 Error occurred: {s3_err}.')\n    except Exception as err:\n        logger.error(f'Error occurred: {err}.')\n\n    finally:\n        response.close()\n        response.release_conn()\n\n\n@kfp.dsl.pipeline(\n   name='census-pipeline',\n   description='Pipeline that will download Census data and save to MinIO.'\n)\ndef census_pipeline(bucket: str, dataset: str, table_code: str, year: int) -> Dataset:\n    # Positional arguments are not allowed.\n    # When I set the name parameter of the condition that task in the DAG fails.\n\n    exists = check_if_table_data_exists_already(bucket=bucket, table_code=table_code, year=year)\n\n    with kfp.dsl.Condition(exists.output == False):\n        \n        table_data = download_table_data(dataset=dataset, table_code=table_code, year=year)\n        \n        save_table_data(\n            bucket=bucket,\n            table_code=table_code,\n            year=year,\n            table_df=table_data.outputs['table_df']\n        )\n\n    with kfp.dsl.Condition(exists.output == True):\n        table_data = get_table_data(\n            bucket=bucket,\n            table_code=table_code,\n            year=year\n        )\n\n    return table_data.outputs['table_df']\n\n\nif __name__ == \"__main__\":\n\n    kfp.compiler.Compiler().compile(\n        pipeline_func=census_pipeline,\n        package_path=\"minio-census-pipeline.yaml\",\n        pipeline_parameters={\n            'bucket': 'census-data',\n            'table_code': 'B01001',\n            'year': 2021\n        }\n    )\n"
  },
  {
    "repo": "muhammadyaseen/kubeflow-on-linode",
    "file_path": "kfp-examples/03_data-cleaning-pipeline/cleaning_and_prep_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/muhammadyaseen/kubeflow-on-linode/main/kfp-examples/03_data-cleaning-pipeline/cleaning_and_prep_pipeline.py",
    "content": "import kfp\nfrom kfp.dsl import Output, Dataset, Input, Markdown\n\n@kfp.dsl.component(packages_to_install=['minio==7.1.14'])\ndef check_if_raw_data_exists_already(\n    bucket_name: str,\n    object_name: str\n) -> bool:\n    \n    from minio import Minio\n    from minio.error import S3Error\n    import logging\n\n\n    logger = logging.getLogger('kfp_logger')\n    logger.setLevel(logging.INFO)\n    logger.info(f\"Looking for {object_name} in Bucket: {bucket_name}\")\n      \n    try:\n    \n        # Create client with access and secret key.\n        client = Minio(\n            '10.110.111.225:9000',\n            'minio',\n            'minio123',\n            secure=False\n        )\n\n        # First, check if the bucket exists\n        bucket_found = client.bucket_exists(bucket_name)\n        if not bucket_found:\n            return False\n\n        # If so, check if the file / object exists that contains the data\n        available_objects = client.list_objects(bucket_name)\n        found = False\n        \n        for current_object in available_objects:\n            \n            logger.info(current_object.object_name)\n            \n            if object_name == current_object.object_name: \n                found = True\n\n    except S3Error as s3_err:\n        logger.error(f'S3 Error occurred: {s3_err}.')\n    except Exception as err:\n        logger.error(f'Error occurred: {err}.')\n\n    return found\n\n@kfp.dsl.component\ndef download_raw_data_for_pipeline(raw_data_location: Output[Dataset]):\n    \n    import logging\n    import urllib.request\n\n    logger = logging.getLogger('kfp_logger')\n    logger.setLevel(logging.INFO)\n\n    CSV_PATH = \"https://raw.githubusercontent.com/muhammadyaseen/kubeflow-on-linode/main/kfp-examples/practical-car-tier-prediction/data/kfp-practical-product-tier-prediction-data.csv\"\n    \n    with urllib.request.urlopen(CSV_PATH) as raw_data_csv:\n        raw_data_content = raw_data_csv.read().decode('utf-8')\n       \n        with open(raw_data_location.path, 'w') as f:       \n            f.write(raw_data_content)\n\n    logger.info(f'Raw data downloaded and saved at: {raw_data_location.path}')\n\n@kfp.dsl.component(packages_to_install=['pandas', 'minio==7.1.14'])\ndef save_raw_data_to_bucket(\n    bucket_name: str, \n    object_name: str, \n    raw_data: Input[Dataset]\n):\n    \n    import logging\n    import io\n\n    from minio import Minio\n    from minio.error import S3Error\n    import pandas as pd\n\n\n    logger = logging.getLogger('kfp_logger')\n    logger.setLevel(logging.INFO)\n\n    logger.info(f'Reading data from: {raw_data.path}')\n    df = pd.read_csv(raw_data.path, sep=';')\n    logger.info('Columns: ' + ', '.join(df.columns))\n\n    try:\n        # Create client with access and secret key\n        client = Minio(\n            '10.110.111.225:9000',\n            'minio',\n            'minio123',\n            secure=False\n        )\n\n        # Make the bucket if it does not exist.\n        if not client.bucket_exists(bucket_name):\n            logger.info(f'Creating bucket: {bucket_name}.')\n            client.make_bucket(bucket_name)\n\n        # Upload the dataframe as an object.\n        encoded_df = df.to_csv(index=False).encode('utf-8')\n        client.put_object(\n            bucket_name, \n            object_name, \n            data=io.BytesIO(encoded_df), \n            length=len(encoded_df), \n            content_type='application/csv'\n        )\n        \n        logger.info(f'{object_name} successfully uploaded to bucket {bucket_name}.')\n        logger.info(f'Object length: {len(df)}.')\n\n    except S3Error as s3_err:\n        logger.error(f'S3 Error occurred: {s3_err}.')\n    except Exception as err:\n        logger.error(f'Error occurred: {err}.')\n\n@kfp.dsl.component(packages_to_install=['pandas', 'minio==7.1.14'])\ndef prepare_cleaned_dataset(\n    bucket: str,\n    object: str,\n    cleaned_data_bucket: str,\n    cleaned_data_object: str\n):\n    \"\"\"\n    \n    \"\"\"\n    \n    # TODO: cleaning should happen after making derived features\n\n    import io\n    import logging\n    import pandas as pd\n    from minio import Minio\n    from minio.error import S3Error\n\n    try:\n\n        logger = logging.getLogger('kfp_logger')\n        logger.setLevel(logging.INFO)\n\n        # Create client with access and secret key\n        client = Minio('10.110.111.225:9000',\n                    'minio',\n                    'minio123',\n                    secure=False)\n\n        logger.info(f'Attempting to read data')\n        response = client.get_object(bucket, object)\n        df = pd.read_csv(io.BytesIO(response.data))\n        logger.info('Columns: ' + ', '.join(df.columns))\n\n        cleaned_df = df.copy(deep=True)\n\n        # These limits were taken from the quantile dataframes. Ideally we should pass-in the dataframe\n        # and extract the limits automatically.\n        cleaned_df = df[\n            ((df['price'] >= 1100) & (df['price'] <= 51950)) & \\\n            ((df['ctr_cleaned'] >= 0.006450) & (df['ctr_cleaned'] <= 0.130435)) & \\\n            ((df['search_views'] >= 63) & (df['search_views'] <= 11119.00)) & \\\n            ((df['detail_views'] >= 1) & (df['detail_views'] <= 900)) & \\\n            ((df['stock_days'] >= 1) & (df['stock_days'] <= 112)) & \\\n            ((df['first_registration_year'] >= 1989) & (df['first_registration_year'] <= 2023)) & \\\n            (df['detail_views_per_day_in_stock'] <= 25)  | \\\n            ((df['product_tier'] == 'Premium') | (df['product_tier'] == 'Plus')) # we don't want to loose precious examples of minority classes\n        ]\n\n        # Write this df as CSV in MinIO\n        # Upload the dataframe adfs an object.\n        encoded_df = cleaned_df.to_csv(index=False).encode('utf-8')\n        client.put_object(\n            cleaned_data_bucket, \n            cleaned_data_object, \n            data=io.BytesIO(encoded_df), \n            length=len(encoded_df), \n            content_type='application/csv'\n        )\n        \n        logger.info(f'{cleaned_data_object} successfully uploaded to bucket {cleaned_data_bucket}.')\n        logger.info(f'Object length: {len(cleaned_df)}.')\n    \n    except S3Error as s3_err:\n        logger.error(f'S3 Error occurred: {s3_err}.')\n    except Exception as err:\n        logger.error(f'Error occurred: {err}.')\n\n    finally:\n        response.close()\n        response.release_conn()\n\n\n@kfp.dsl.component(packages_to_install=['pandas', 'minio==7.1.14'])\ndef product_tier_counts_and_percentages(\n    cleaned_dataset_bucket: str, \n    cleaned_dataset_object: str, \n    product_percentage_md: Output[Markdown], \n    product_counts_md: Output[Markdown]\n): \n\n    import io\n    import logging\n    import pandas as pd\n\n    from minio import Minio\n    from minio.error import S3Error\n    \n    logger = logging.getLogger('kfp_logger')\n    logger.setLevel(logging.INFO)\n    \n    # read data\n    logger.info(f'Reading data')\n    client = Minio(\n        '10.110.111.225:9000',\n        'minio',\n        'minio123',\n        secure=False\n    )\n    logger.info(f'Attempting to read data')\n    response = client.get_object(cleaned_dataset_bucket, cleaned_dataset_object)\n    \n    # create df\n    df = pd.read_csv(io.BytesIO(response.data))\n    logger.info('Columns: ' + ', '.join(df.columns))\n\n    # crunch numbers\n    product_tier_counts_df = df[['product_tier', 'article_id']].groupby(['product_tier']).count()\n\n    product_tier_percentages_df = product_tier_counts_df * 100 / len(df)\n\n    # produce markdown visuals \n    logger.info('Writing counts')\n    with open(product_counts_md.path, 'w') as f:\n        f.write(product_tier_counts_df.to_markdown())\n\n    logger.info('Writing percentages')\n    with open(product_percentage_md.path, 'w') as f:\n        f.write(product_tier_percentages_df.to_markdown())\n\n@kfp.dsl.component(packages_to_install=['pandas', 'minio==7.1.14'])\ndef make_derived_features(\n    input_bucket_name: str,\n    input_object_name: str, \n    final_bucket_name: str,\n    final_object_name: str\n):\n    \"\"\"\n    \n    \"\"\"\n\n    import logging\n    import io\n\n    from datetime import date, datetime\n\n    import pandas as pd\n    from minio import Minio\n    from minio.error import S3Error\n\n    def _get_days(date_str_future, date_str_past):\n\n        f = datetime.strptime(date_str_future, \"%d.%m.%y\").date()\n        p = datetime.strptime(date_str_past, \"%d.%m.%y\").date()\n        return (f - p).days + 1\n\n    def _get_ctr(search_views, detail_views):\n\n        if search_views == 0:\n            return 0.\n        else:\n            return detail_views / search_views\n\n    try:\n\n        logger = logging.getLogger('kfp_logger')\n        logger.setLevel(logging.INFO)\n\n        # Read-in CSV from Kubeflow\n         # Create client with access and secret key\n        client = Minio('10.110.111.225:9000',\n                    'minio',\n                    'minio123',\n                    secure=False)\n\n        logger.info(f'Attempting to read data')\n        response = client.get_object(input_bucket_name, input_object_name)\n        derived_data_df = pd.read_csv(io.BytesIO(response.data))\n        logger.info('Columns: ' + ', '.join(derived_data_df.columns))\n\n        # Start making derived features\n        derived_data_df['ctr_cleaned'] = derived_data_df.apply(\n            lambda x: _get_ctr(\n                x['search_views'],\n                x['detail_views']\n            ), axis=1\n        )\n        derived_data_df['stock_days_cleaned'] = derived_data_df.apply(\n            lambda x: _get_days(\n                x['deleted_date'],\n                x['created_date']\n            ), axis=1\n        )\n\n        # If an entry has been up throughout the cleaned_data_df, it may have been viewed / searched for more times.\n        derived_data_df['weekends_while_in_stock'] = derived_data_df['stock_days_cleaned'] // 7\n\n        derived_data_df['age'] = derived_data_df['first_registration_year'].max() - derived_data_df['first_registration_year']\n        derived_data_df['price_k'] = derived_data_df['price'] / 1000\n\n        # Cars are not compared in isolation, visitors compare them with what's available\n        derived_data_df['price_relative'] = derived_data_df['price_k'] / derived_data_df['price_k'].mean()\n\n        # Normalizing by in stock days\n        derived_data_df['search_views_per_day_in_stock'] = derived_data_df['search_views'] / derived_data_df['stock_days_cleaned']\n        derived_data_df['detail_views_per_day_in_stock'] = derived_data_df['detail_views'] / derived_data_df['stock_days_cleaned']\n\n        # Write this df as CSV in MinIO\n        # Make the bucket if it does not exist.\n        if not client.bucket_exists(final_bucket_name):\n            logger.info(f'Creating bucket: {final_bucket_name}.')\n            client.make_bucket(final_bucket_name)\n\n        # Upload the dataframe adfs an object.\n        encoded_df = derived_data_df.to_csv(index=False).encode('utf-8')\n        client.put_object(\n            final_bucket_name, \n            final_object_name, \n            data=io.BytesIO(encoded_df), \n            length=len(encoded_df), \n            content_type='application/csv'\n        )\n        \n        logger.info(f'{final_object_name} successfully uploaded to bucket {final_bucket_name}.')\n        logger.info(f'Object length: {len(derived_data_df)}.')\n\n    except Exception as err:\n    \n        logger.error(f'Error occurred: {err}.')\n\n@kfp.dsl.pipeline(\n   name='data-preparation-cleaning-pipeline',\n   description='Pipeline that will download & clean raw data and save it to MinIO.'\n)\ndef data_preparation_pipeline(bucket: str, object: str):\n\n    exists = check_if_raw_data_exists_already(bucket_name=bucket, object_name=object)\n\n    with kfp.dsl.Condition(\n        condition=exists.output == False,\n        name='Data-Does-Not-Exist'\n    ):\n        \n        raw_data = download_raw_data_for_pipeline()\n        \n        save_data_to_bucket = save_raw_data_to_bucket(\n            bucket_name=bucket,\n            object_name=object,\n            raw_data=raw_data.outputs['raw_data_location']\n        )\n\n        # Should happen after `save_data_to_bucket`\n        with_derived_features = make_derived_features(\n            input_bucket_name=bucket,\n            input_object_name=object,\n            final_bucket_name='car-tier-prediction-data',\n            final_object_name='data-with-features.csv'\n        ).after(save_data_to_bucket)\n\n        # Should happen after `with_derived_features`\n        cleaned_data = prepare_cleaned_dataset(\n            bucket='car-tier-prediction-data',\n            object='data-with-features.csv',\n            cleaned_data_bucket='car-tier-prediction-data',\n            cleaned_data_object='data-with-features-cleaned.csv',\n        ).after(with_derived_features)\n\n        counts_and_percent_visual = product_tier_counts_and_percentages(\n            cleaned_dataset_bucket='car-tier-prediction-data',\n            cleaned_dataset_object='data-with-features-cleaned.csv',\n        ).after(cleaned_data)\n\n    with kfp.dsl.Condition(\n        condition=exists.output == True,\n        name='Data-Already-Exists'\n    ):\n\n        # TODO: cleaning should happen after making derived features\n        cleaned_data = prepare_cleaned_dataset(\n            bucket=bucket,\n            object=object,\n            cleaned_data_bucket='car-tier-prediction-data',\n            cleaned_data_object='data-with-features-cleaned.csv',\n        )\n    \n        counts_and_percent_visual = product_tier_counts_and_percentages(\n            cleaned_dataset_bucket='car-tier-prediction-data',\n            cleaned_dataset_object='data-with-features-cleaned.csv',\n        ).after(cleaned_data)\n\n    # return cleaned_data.outputs['cleaned_data']\n\n\nif __name__ == \"__main__\":\n\n    kfp.compiler.Compiler().compile(\n        pipeline_func=data_preparation_pipeline,\n        package_path=\"car-tier-pred-data-prep-pipeline.yaml\",\n        pipeline_parameters={\n            'bucket': 'car-tier-prediction-data',\n            'object': 'car-tier-pred-study.csv'\n        }\n    )\n    "
  },
  {
    "repo": "muhammadyaseen/kubeflow-on-linode",
    "file_path": "kfp-examples/04_model-train-eval-pipeline/train_eval_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/muhammadyaseen/kubeflow-on-linode/main/kfp-examples/04_model-train-eval-pipeline/train_eval_pipeline.py",
    "content": "import json\nfrom typing import Dict\nimport kfp\nfrom kfp.dsl import Output, Dataset, Input, Markdown\n\n\n@kfp.dsl.container_component\ndef train_eval_baseline_model(\n    model_name: str,\n    script: str,\n    ip: str,\n    port: str,\n    bucket_name: str,\n    object_name: str\n):\n    \"\"\"\n    This component loads our main training container image and runs the code to train the models\n    \"\"\"\n\n    return kfp.dsl.ContainerSpec(\n        image='myaseende/my-scikit:latest', \n        command=['python'], \n        args=[\n            script,\n            model_name,\n            ip,\n            port,\n            bucket_name,\n            object_name\n        ]\n    )\n\n@kfp.dsl.component(packages_to_install=['pandas'])\ndef find_best_model_on_full_data(\n    baseline_metric: float,\n    lr_metric: float,\n    lr_resampled_metric: float,\n    gbt_metric: float,\n    gbt_resampled_metric: float,\n    dtree_metric: float,\n    dtree_resampled_metric: float,\n    experiment_summary: Output[Markdown]\n):\n    \"\"\"\n    Given the evaluation scores, find out which model did the best\n    \"\"\"\n\n    import logging\n    import pandas as pd\n\n    logger = logging.getLogger('kfp_logger')\n    logger.setLevel(logging.INFO)\n    \n    # TODO: there should be a better way to do this\n    model_names = [\n        'baseline',\n        'lr',\n        'lr_resampled',\n        'gbt',\n        'gbt_resampled',\n        'dtree',\n        'dtree_resampled'\n    ]\n\n    model_metrics = [\n        baseline_metric,\n        lr_metric,\n        lr_resampled_metric,\n        gbt_metric,\n        gbt_resampled_metric,\n        dtree_metric,\n        dtree_resampled_metric\n    ]\n    \n    models_and_metrics = dict(\n        zip(\n            model_names,\n            model_metrics\n        )\n    )\n\n    best_model_name = max(models_and_metrics, key=lambda key: models_and_metrics[key])\n    best_model_metric = models_and_metrics[best_model_name]\n\n    # write experiments summary as a markdown table\n\n    logger.info('Writing experiment summary')\n    \n    with open(experiment_summary.path, 'w') as f:\n        \n        experiment_summary_df = pd.DataFrame({\n            # We replace the underscore so that Mardown is rendered properly\n            \"Models\": list(map(lambda name: name.replace(\"_\", \" \"), model_names)),\n            \"Metrics\": model_metrics,\n        })\n\n        summaries_table_md = experiment_summary_df.to_markdown()\n        summaries_table_md += \"\\n\\n\"\n        summaries_table_md += f\"The best model is **{best_model_name}** with evaluation metric value: **{best_model_metric}.**\"\n        \n        f.write(summaries_table_md)\n    \n\n@kfp.dsl.component(packages_to_install=['pandas', 'minio==7.1.14'])\ndef show_best_model_info(\n    model_name: str,\n    ip: str,\n    port: str,\n    bucket_name: str, \n    object_name: str,\n) -> float:\n    \"\"\"\n    Retrieve the saved training metrics\n    \"\"\"\n\n    import logging\n    import io\n\n    from minio import Minio\n    from minio.error import S3Error\n    import pandas as pd\n\n    try:\n\n        logger = logging.getLogger('kfp_logger')\n        logger.setLevel(logging.INFO)\n\n        # Create client with access and secret key\n        client = Minio(\n            f'{ip}:{port}',\n            'minio',\n            'minio123',\n            secure=False\n        )\n\n        logger.info(f'Attempting to read data from MinIO.')\n\n        # read the ranks and hparams info df from MinIO\n        response = client.get_object(bucket_name, object_name)\n\n        pandas_df = pd.read_csv(io.BytesIO(response.data))\n\n        logger.info('Columns: ' + ', '.join(pandas_df.columns))  \n        \n        metric = pandas_df.loc[0, 'mean_test_balanced_accuracy'].item()\n        logger.info(f\"Metric (mean_test_balanced_accuracy) on {model_name}: {metric}\")\n\n        return metric\n    \n    except Exception as err:\n    \n        logger.error(f'Error occurred: {err}.')\n    \n    # output the main metric (this can be used for comparison)\n    return 0.0\n\n@kfp.dsl.pipeline(\n   name='base-train-eval-pipeline',\n   description='Pipeline that will train a baseline model and run eval'\n)\ndef model_train_eval_pipeline(\n    ip: str,\n    port: str,\n    bucket_name: str,\n    object_name: str\n):\n\n    # Note: the `.set_caching_options(False)` call is used so that we don't reuse results from previous runs.\n    # This was required during developmenet because sometimes I change the container and push it to DockerHub, but \n    # Kubeflow wasn't pulling the new container and was instead using old results.\n\n    ############################\n    # Baseline model\n    ############################\n\n    # This will train, eval, and save the model as well as hparams\n    base_line_train_eval_task = train_eval_baseline_model(\n        model_name='baseline',\n        script='baseline_model.py',\n        ip=ip,\n        port=port,\n        bucket_name=bucket_name,\n        object_name=object_name\n    )\n\n    base_line_train_eval_task.set_caching_options(False)\n\n    base_line_metric = show_best_model_info(\n        model_name='baseline',\n        ip=ip,\n        port=port,\n        bucket_name=bucket_name,\n        object_name=\"baseline_model_ranks.pandas_df\"\n    ).after(base_line_train_eval_task)\n\n    base_line_metric.set_caching_options(False)\n\n\n    ############################\n    # Full models\n    ############################\n    \n    # 1. Logistic regression + with resampling\n    lr_train_eval_task = train_eval_baseline_model(\n        model_name='logistic_regression',\n        script='full_models.py',\n        ip=ip,\n        port=port,\n        bucket_name=bucket_name,\n        object_name=object_name\n    )\n    lr_train_eval_task.set_caching_options(False)\n\n    lr_metric = show_best_model_info(\n        model_name='logistic_regression',\n        ip=ip,\n        port=port,\n        bucket_name=bucket_name,\n        object_name=\"logistic_regression_ranks.pandas_df\"\n    ).after(lr_train_eval_task)\n\n    lr_resampled_metric = show_best_model_info(\n        model_name='logistic_regression_resampled',\n        ip=ip,\n        port=port,\n        bucket_name=bucket_name,\n        object_name=\"logistic_regression_ranks_resampled.pandas_df\"\n    ).after(lr_train_eval_task)\n\n    lr_metric.set_caching_options(False)\n    lr_resampled_metric.set_caching_options(False)\n\n    # 2. Gradient Boosted Trees regression + with resampling\n    gbt_train_eval_task = train_eval_baseline_model(\n        model_name='gbt',\n        script='full_models.py',\n        ip=ip,\n        port=port,\n        bucket_name=bucket_name,\n        object_name=object_name\n    )\n    gbt_train_eval_task.set_caching_options(False)\n\n    gbt_metric = show_best_model_info(\n        model_name='gbt',\n        ip=ip,\n        port=port,\n        bucket_name=bucket_name,\n        object_name=\"gbt_ranks.pandas_df\"\n    ).after(gbt_train_eval_task)\n    \n    gbt_resampeld_metric = show_best_model_info(\n        model_name='gbt_resampled',\n        ip=ip,\n        port=port,\n        bucket_name=bucket_name,\n        object_name=\"gbt_ranks_resampled.pandas_df\"\n    ).after(gbt_train_eval_task)\n\n    gbt_metric.set_caching_options(False)\n    gbt_resampeld_metric.set_caching_options(False)\n\n    # Decision Tree regression + with resampling\n    dtree_train_eval_task = train_eval_baseline_model(\n        model_name='decision_tree',\n        script='full_models.py',\n        ip=ip,\n        port=port,\n        bucket_name=bucket_name,\n        object_name=object_name\n    )\n    dtree_train_eval_task.set_caching_options(False)\n\n    dtree_metric = show_best_model_info(\n        model_name='decision_tree',\n        ip=ip,\n        port=port,\n        bucket_name=bucket_name,\n        object_name=\"decision_tree_ranks.pandas_df\"\n    ).after(dtree_train_eval_task)\n\n    dtree_resampled_metric = show_best_model_info(\n        model_name='decision_tree_resampled',\n        ip=ip,\n        port=port,\n        bucket_name=bucket_name,\n        object_name=\"decision_tree_ranks_resampled.pandas_df\"\n    ).after(dtree_train_eval_task)\n\n    dtree_metric.set_caching_options(False)\n    dtree_resampled_metric.set_caching_options(False)\n\n    # ############################\n    # # Compare models\n    # ############################\n    # TODO: For some reason, collecting the results into a dictionary like this doesn't seem to work\n\n    # model_and_metric_dict={\n    #     \"baseline\":         base_line_metric.output,\n    #     \"lr\":               lr_metric.output,\n    #     \"lr_resampled\":     lr_resampled_metric.output,\n    #     \"gbt\":              gbt_metric.output,\n    #     \"gbt_resampled\":    gbt_resampeld_metric.output,\n    #     \"dtree\":            dtree_metric.output,\n    #     \"dtree_resampled\":  dtree_resampled_metric.output\n    # }\n    \n    # TODO: There has to be a better way of doing this, but for now we go with this (honestly) ugly solution\n    find_best_model_on_full_data(\n        baseline_metric=base_line_metric.output,\n        lr_metric=lr_metric.output,\n        lr_resampled_metric=lr_resampled_metric.output,\n        gbt_metric=gbt_metric.output,\n        gbt_resampled_metric=gbt_resampeld_metric.output,\n        dtree_metric=dtree_metric.output,\n        dtree_resampled_metric=dtree_resampled_metric.output\n    )\n\n\nif __name__ == \"__main__\":\n\n    kfp.compiler.Compiler().compile(\n        pipeline_func=model_train_eval_pipeline,\n        package_path=\"model_train_eval_pipeline.yaml\",\n        pipeline_parameters={\n            'bucket_name': 'car-tier-prediction-data',\n            'object_name': 'data-with-features-cleaned.csv',\n            'port': '9000'\n        }\n    )\n"
  },
  {
    "repo": "e2fyi/kfx",
    "file_path": "kfx/dsl/__init__.py",
    "raw_url": "https://raw.githubusercontent.com/e2fyi/kfx/master/kfx/dsl/__init__.py",
    "content": "\"\"\"Extension to kfp dsl.\n\nUsing `kfx.dsl.ContainerOpTransform` to modify ContainerOp internal properties\n::\n\n    import kfp.components\n    import kfp.dsl\n    import kfx.dsl\n\n    transforms = (\n        kfx.dsl.ContainerOpTransform()\n        .set_resources(cpu=\"500m\", memory=(\"1G\", \"4G\"))\n        .set_image_pull_policy(\"Always\")\n        .set_annotations({\"iam.amazonaws.com/role\": \"some-arn\"})\n        .add_env_vars({\"ENV\": \"production\"})\n        .add_env_var_from_secret(\"AWS_ACCESS_KEY\", secret_name=\"aws\", secret_key=\"access_key\")\n    )\n\n\n    @kfp.dsl.components.func_to_container_op\n    def echo(text: str) -> str:\n        print(text)\n        return text\n\n\n    @kfp.dsl.pipeline(name=\"demo\")\n    def pipeline(text: str):\n        op1 = echo(text)\n        op2 = echo(\"%s-%s\" % text)\n\n        # u can apply the transform on op1 only\n        # op1.apply(transforms)\n\n        # or apply on all ops in the pipeline\n        kfp.dsl.get_pipeline_conf().add_op_transformer(transforms)\n\n\nUsing `kfx.dsl.ArtifactLocationHelper` to get the path to an artifact generated\nby a kfp task.\n::\n\n    import kfp.components\n    import kfp.dsl\n    import kfx.dsl\n\n\n    helper = kfx.dsl.ArtifactLocationHelper(\n        scheme=\"minio\", bucket=\"mlpipeline\", key_prefix=\"artifacts/\"\n    )\n\n    @kfp.components.func_to_container_op\n    def test_op(\n        mlpipeline_ui_metadata: OutputTextFile(str), markdown_data_file: OutputTextFile(str)\n    ):\n        \"A test kubeflow pipeline task.\"\n\n        import json\n\n        import kfx.dsl\n        import kfx.vis\n        import kfx.vis.vega\n\n        data = [\n            {\"a\": \"A\", \"b\": 28},\n            {\"a\": \"B\", \"b\": 55},\n            {\"a\": \"C\", \"b\": 43},\n            {\"a\": \"D\", \"b\": 91},\n            {\"a\": \"E\", \"b\": 81},\n            {\"a\": \"F\", \"b\": 53},\n            {\"a\": \"G\", \"b\": 19},\n            {\"a\": \"H\", \"b\": 87},\n            {\"a\": \"I\", \"b\": 52},\n        ]\n\n        # `KfpArtifact` provides the reference to data artifact created\n        # inside this task\n        spec = {\n            \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.json\",\n            \"description\": \"A simple bar chart\",\n            \"data\": {\n                \"values\": data,\n            },\n            \"mark\": \"bar\",\n            \"encoding\": {\n                \"x\": {\"field\": \"a\", \"type\": \"ordinal\"},\n                \"y\": {\"field\": \"b\", \"type\": \"quantitative\"},\n            },\n        }\n\n        # write the markdown to the `markdown-data` artifact\n        markdown_data_file.write(\"### hello world\")\n\n        # creates an ui metadata object\n        ui_metadata = kfx.vis.kfp_ui_metadata(\n            # Describes the vis to generate in the kubeflow pipeline UI.\n            [\n                # markdown vis from a markdown artifact.\n                # `KfpArtifact` provides the reference to data artifact created\n                # inside this task\n                kfx.vis.markdown(kfx.dsl.KfpArtifact(\"markdown_data_file\")),\n                # a vega web app from the vega data artifact.\n                kfx.vis.vega.vega_web_app(spec),\n            ]\n        )\n\n        # writes the ui metadata object as the `mlpipeline-ui-metadata` artifact\n        mlpipeline_ui_metadata.write(kfx.vis.asjson(ui_metadata))\n\n        # prints the uri to the markdown artifact\n        print(ui_metadata.outputs[0].source)\n\n\n    @kfp.dsl.pipeline()\n    def test_pipeline():\n        \"A test kubeflow pipeline\"\n\n        op: kfp.dsl.ContainerOp = test_op()\n\n        # modify kfp operator with artifact location metadata through env vars\n        op.apply(helper.set_envs())\n\n\"\"\"\nfrom kfx.dsl._artifact_location import (\n    ArtifactLocationHelper,\n    KfpArtifact,\n    WorkflowVars,\n    set_pod_metadata_envs,\n    set_workflow_env,\n)\nfrom kfx.dsl._transformers import ContainerOpTransform\n"
  },
  {
    "repo": "e2fyi/kfx",
    "file_path": "kfx/dsl/_artifact_location.py",
    "raw_url": "https://raw.githubusercontent.com/e2fyi/kfx/master/kfx/dsl/_artifact_location.py",
    "content": "\"\"\"Utils.\"\"\"\nimport os\nimport os.path\nfrom typing import Callable, NamedTuple\n\nimport kfp.dsl\nfrom kubernetes import client as k8s_client\n\nfrom kfx.dsl._compat import sanitize_k8s_name\n\nDEFAULT_KEY_FORMAT = \"{{workflow.name}}/{{pod.name}}\"\n\n\nclass WorkflowVars(NamedTuple):\n    \"\"\"Describes a templated workflow environment variable.\"\"\"\n\n    name: str\n    template: str\n\n\ndef set_workflow_env(\n    workflow_vars: WorkflowVars = WorkflowVars(\n        name=\"WORKFLOW_DEFAULT_KEY_FORMAT\", template=\"{{workflow.name}}/{{pod.name}}\"\n    )\n) -> Callable[[kfp.dsl.ContainerOp], kfp.dsl.ContainerOp]:\n    \"\"\"Modifier for kubeflow pipelines tasks.\n\n    Setup a kfp op to pass in the workflow variables as an environment var.\n\n    See https://github.com/argoproj/argo/blob/master/docs/variables.md\n\n    Example::\n\n        import kfp\n        import kfx.lib.utils as kfxutils\n\n        from kfp.components import func_to_container_op\n\n        @func_to_container_op\n        def echo_workflow_vars():\n            import os\n\n            print(os.environ.get(\"WORKFLOW_NAME\"))\n\n\n        @kfp.dsl.pipeline()\n        def simple_pipeline():\n            op = echo_workflow_vars()\n\n            # op will have an environment variable \"WORKFLOW_NAME\"\n            # that provides the name of the workflow\n            op.apply(kfxutils.set_workflow_env(\n                kfxutils.WorkflowVars(\"WORKFLOW_NAME\", template=\"{{workflow.name}}\")\n            ))\n\n    Args:\n        name (str, optional): Env var name to pass in.\n            Defaults to \"WORKFLOW_DEFAULT_KEY_FORMAT\".\n        value (str, optional): Template string with argo workflow variables.\n            Defaults to \"{{workflow.name}}/{{pod.name}}\".\n\n    Returns:\n        Callable[[kfp.dsl.ContainerOp], kfp.dsl.ContainerOp]: kfp op.\n    \"\"\"\n\n    def apply_workflow_name_env(task: kfp.dsl.ContainerOp) -> kfp.dsl.ContainerOp:\n        task.container.add_env_variable(\n            k8s_client.V1EnvVar(name=workflow_vars.name, value=workflow_vars.template)\n        )\n        return task\n\n    return apply_workflow_name_env\n\n\ndef set_pod_metadata_envs(\n    pod_name: str = \"POD_NAME\",\n    namespace: str = \"NAMESPACE\",\n    node_name: str = \"NODE_NAME\",\n) -> Callable[[kfp.dsl.ContainerOp], kfp.dsl.ContainerOp]:\n    \"\"\"Modifier for kubeflow pipelines tasks.\n\n    Setup a kfp op to pass in the pod name, namespace, and node name as env\n    var.\n\n    Example::\n\n        import kfp\n        import kfx.lib.utils as kfxutils\n\n        from kfp.components import func_to_container_op\n\n        @func_to_container_op\n        def echo_podname():\n            import os\n\n            print(os.environ.get(\"POD_NAME\"))\n\n\n        @kfp.dsl.pipeline()\n        def simple_pipeline():\n            op = echo_podname()\n            op.apply(kfxutils.set_pod_metadata_envs())\n\n    Args:\n        pod_name (str, optional): Env var name for the pod name.\n            Defaults to \"POD_NAME\".\n        namespace (str, optional): Env var name for the namespace.\n            Defaults to \"NAMESPACE\".\n        node_name (str, optional): Env var name for the node name.\n            Defaults to \"NODE_NAME\".\n\n    Returns:\n        Callable[[kfp.dsl.ContainerOp], kfp.dsl.ContainerOp]: kfp op.\n    \"\"\"\n\n    def apply_pod_metadata_envs(task: kfp.dsl.ContainerOp) -> kfp.dsl.ContainerOp:\n        for name, field_path in [\n            (pod_name, \"metadata.name\"),\n            (namespace, \"metadata.namespace\"),\n            (node_name, \"spec.nodeName\"),\n        ]:\n\n            task.container.add_env_variable(\n                k8s_client.V1EnvVar(\n                    name=name,\n                    value_from=k8s_client.V1EnvVarSource(\n                        field_ref=k8s_client.V1ObjectFieldSelector(\n                            field_path=field_path\n                        )\n                    ),\n                )\n            )\n        return task\n\n    return apply_pod_metadata_envs\n\n\nclass ArtifactLocationHelper:\n    \"\"\"Helper class to generate artifact location based on provided argo config.\n\n    See an example of an `Argo configmap <https://github.com/argoproj/argo/blob/master/docs/workflow-controller-configmap.yaml>`_.\n\n    ::\n\n        import kfp.components\n        import kfp.dsl\n        import kfx.dsl\n\n\n        helper = kfx.dsl.ArtifactLocationHelper(\n            scheme=\"minio\", bucket=\"mlpipeline\", key_prefix=\"artifacts/\"\n        )\n\n        @kfp.components.func_to_container_op\n        def test_op(\n            mlpipeline_ui_metadata: OutputTextFile(str), markdown_data_file: OutputTextFile(str)\n        ):\n            \"A test kubeflow pipeline task.\"\n\n            import kfx.dsl\n            import kfx.vis\n\n            # write the markdown to the `markdown-data` artifact\n            markdown_data_file.write(\"### hello world\")\n\n            # creates an ui metadata object\n            ui_metadata = kfx.vis.kfp_ui_metadata(\n                # Describes the vis to generate in the kubeflow pipeline UI.\n                # In this case, a markdown vis from a markdown artifact.\n                [kfx.vis.markdown(kfx.dsl.KfpArtifact(\"markdown_data_file\"))]\n                # `KfpArtifact` provides the reference to data artifact created\n                # inside this task\n            )\n\n            # writes the ui metadata object as the `mlpipeline-ui-metadata` artifact\n            mlpipeline_ui_metadata.write(kfx.vis.asjson(ui_metadata))\n\n            # prints the uri to the markdown artifact\n            print(ui_metadata.outputs[0].source)\n\n\n        @kfp.dsl.pipeline()\n        def test_pipeline():\n            \"A test kubeflow pipeline\"\n\n            op: kfp.dsl.ContainerOp = test_op()\n\n            # modify kfp operator with artifact location metadata through env vars\n            op.apply(helper.set_envs())\n    \"\"\"\n\n    artifact_prefix_env: str = \"WORKFLOW_ARTIFACT_PREFIX\"\n    artifact_storage_env: str = \"WORKFLOW_ARTIFACT_STORAGE\"\n    artifact_bucket_env: str = \"WORKFLOW_ARTIFACT_BUCKET\"\n    artifact_key_prefix_env: str = \"WORKFLOW_ARTIFACT_KEY_PREFIX\"\n\n    def __init__(\n        self, scheme: str, bucket: str, key_prefix: str = \"\", key_format: str = \"\"\n    ):\n        \"\"\"Creates a new instance of ArtifactLocationHelper object.\n\n        Args:\n            scheme (str): Storage scheme, e.g. s3, minio, gcs.\n            bucket (str): Name of the bucket.\n            key_prefix (str, optional): Key prefix set inside argo artifactory\n                configmap. Will not be used if key_format is provided.\n                Defaults to \"\".\n            key_format (str, optional): Key format set inside argo artifactory\n                configmap. Defaults to \"\".\n        \"\"\"\n\n        self.scheme = scheme\n        self.bucket = bucket\n        self.key_prefix = key_prefix\n        self.key_format = key_format\n\n    # ref\n    # https://github.com/argoproj/argo/blob/f25a45deb4a7179044034da890884432e750d98a/workflow/controller/workflowpod.go#L859\n    # artifact location is defaulted using the following formula:\n    #   <worflow_name>/<pod_name>/<artifact_name>.tgz\n    #   (e.g. myworkflowartifacts/argo-wf-fhljp/argo-wf-fhljp-123291312382/src.tgz)\n    def _get_key_prefix(self) -> str:\n        \"\"\"Returns artifact key prefix.\n\n        Returns:\n            str: artifact key prefix\n        \"\"\"\n        if self.key_format:\n            return self.key_format\n\n        return os.path.join(self.key_prefix, DEFAULT_KEY_FORMAT)\n\n    def set_envs(\n        self, image: str = \"e2fyi/kfx:latest\"\n    ) -> Callable[[kfp.dsl.ContainerOp], kfp.dsl.ContainerOp]:\n        \"\"\"A kfp task modifier.\n\n        This task modifier appends 2 env variables to the task, which\n        can be subsequently used to determine the artifact location and prefix.\n\n        Args:\n            image (str, optional): image to use for the task. Defaults to\n                \"e2fyi/kfx:latest\".\n\n        Returns:\n            Callable[[kfp.dsl.ContainerOp], kfp.dsl.ContainerOp]: modified task.\n        \"\"\"\n\n        def set_workflow_envs(task: kfp.dsl.ContainerOp):\n            task.container.image = image\n            artifact_prefix = sanitize_k8s_name(task.name)\n\n            for name, value in [\n                (self.artifact_storage_env, self.scheme),\n                (self.artifact_bucket_env, self.bucket),\n                (self.artifact_key_prefix_env, self._get_key_prefix()),\n                (self.artifact_prefix_env, artifact_prefix),\n            ]:\n                task.container.add_env_variable(\n                    k8s_client.V1EnvVar(name=name, value=value)\n                )\n\n        return set_workflow_envs\n\n\ndef _handle_special_artifact_names(name: str) -> str:\n    \"\"\"Always sanitize special artifact names (e.g. mlpipeline_ui_metadata)\"\"\"\n    sanitized: str = sanitize_k8s_name(name)\n    return (\n        sanitized\n        if sanitized in {\"mlpipeline-ui-metadata\", \"mlpipeline-metrics\"}\n        else name\n    )\n\n\ndef _sanitize_artifact_name(name: str, sanitize: bool = False) -> str:\n    \"\"\"Sanitize the artifact name based on k8s resource naming convention.\n\n    Also remove suffixes \"_path\" and \"_file\". (See this `comment <https://github.com/kubeflow/pipelines/blob/4cb81ea047361ddce7ce8b0b68133b0a92724588/sdk/python/kfp/components/_python_op.py#L327>'_.)\n\n\n    Args:\n        name (str): [description]\n        sanitize (bool, optional): Whether to sanitize the name. Defaults to False.\n\n    Returns:\n        str: [description]\n    \"\"\"\n    if name.endswith(\"_path\"):\n        name = name[0 : -len(\"_path\")]\n    elif name.endswith(\"_file\"):\n        name = name[0 : -len(\"_file\")]\n\n    return (  # type: ignore\n        sanitize_k8s_name(name) if sanitize else _handle_special_artifact_names(name)\n    )\n\n\nclass KfpArtifact:\n    \"\"\"Class to represent a kubeflow pipeline artifact created inside the pipeline task.\"\"\"\n\n    def __init__(self, name: str, ext: str = \".tgz\", sanitize_name: bool = False):\n        \"\"\"Reference to a kfp artifact that is created within the kubeflow pipeline task.\n\n        This function should be used inside the kfp task. It returns the artifact uri,\n        which then can be provided to the kubeflow pipeline UI - i.e. as the\n        `source` field inside kubeflow pipeline ui metadata.\n\n        ::\n\n            import kfp.components\n            import kfp.dsl\n            import kfx.dsl\n\n\n            helper = kfx.dsl.ArtifactLocationHelper(\n                scheme=\"minio\", bucket=\"mlpipeline\", key_prefix=\"artifacts/\"\n            )\n\n            @kfp.components.func_to_container_op\n            def test_op(\n                mlpipeline_ui_metadata: OutputTextFile(str), markdown_data_file: OutputTextFile(str)\n            ):\n                \"A test kubeflow pipeline task.\"\n\n                import json\n\n                import kfx.dsl\n                import kfx.vis\n                import kfx.vis.vega\n\n                data = [\n                    {\"a\": \"A\", \"b\": 28},\n                    {\"a\": \"B\", \"b\": 55},\n                    {\"a\": \"C\", \"b\": 43},\n                    {\"a\": \"D\", \"b\": 91},\n                    {\"a\": \"E\", \"b\": 81},\n                    {\"a\": \"F\", \"b\": 53},\n                    {\"a\": \"G\", \"b\": 19},\n                    {\"a\": \"H\", \"b\": 87},\n                    {\"a\": \"I\", \"b\": 52},\n                ]\n\n                # `KfpArtifact` provides the reference to data artifact created\n                # inside this task\n                spec = {\n                    \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.json\",\n                    \"description\": \"A simple bar chart\",\n                    \"data\": {\n                        \"values\": data,\n                    },\n                    \"mark\": \"bar\",\n                    \"encoding\": {\n                        \"x\": {\"field\": \"a\", \"type\": \"ordinal\"},\n                        \"y\": {\"field\": \"b\", \"type\": \"quantitative\"},\n                    },\n                }\n\n                # write the markdown to the `markdown-data` artifact\n                markdown_data_file.write(\"### hello world\")\n\n                # creates an ui metadata object\n                ui_metadata = kfx.vis.kfp_ui_metadata(\n                    # Describes the vis to generate in the kubeflow pipeline UI.\n                    [\n                        # markdown vis from a markdown artifact.\n                        # `KfpArtifact` provides the reference to data artifact created\n                        # inside this task\n                        kfx.vis.markdown(kfx.dsl.KfpArtifact(\"markdown_data_file\")),\n                        # a vega web app from the vega data artifact.\n                        kfx.vis.vega.vega_web_app(spec),\n                    ]\n                )\n\n                # writes the ui metadata object as the `mlpipeline-ui-metadata` artifact\n                mlpipeline_ui_metadata.write(kfx.vis.asjson(ui_metadata))\n\n                # prints the uri to the markdown artifact\n                print(ui_metadata.outputs[0].source)\n\n\n            @kfp.dsl.pipeline()\n            def test_pipeline():\n                \"A test kubeflow pipeline\"\n\n                op: kfp.dsl.ContainerOp = test_op()\n\n                # modify kfp operator with artifact location metadata through env vars\n                op.apply(helper.set_envs())\n\n\n        Args:\n            name (str): name of the artifact.\n            ext (str, optional): extension for the artifact. Defaults to \".tgz\".\n            sanitize_name (bool, optional): whether to sanitize the artifact name. Defaults to False.\n\n        Returns:\n            str: uri to the artifact which can be provided to kfp ui.\n        \"\"\"\n        self.storage = os.environ[ArtifactLocationHelper.artifact_storage_env]\n        self.bucket = os.environ[ArtifactLocationHelper.artifact_bucket_env]\n        self.key_prefix = os.environ[ArtifactLocationHelper.artifact_key_prefix_env]\n        self.prefix = os.environ[ArtifactLocationHelper.artifact_prefix_env]\n        self.name = _sanitize_artifact_name(name, sanitize_name)\n        self.ext = ext\n        self.key = os.path.join(\n            self.key_prefix, \"%s-%s%s\" % (self.prefix, self.name, self.ext)\n        )\n\n    @property\n    def source(self) -> str:\n        \"\"\"Url to the artifact source.\"\"\"\n        path = os.path.join(self.bucket, self.key)\n        return \"%s://%s\" % (self.storage, path)\n\n    def __str__(self):\n        \"\"\"Url to the artifact source.\"\"\"\n        return self.source\n"
  },
  {
    "repo": "e2fyi/kfx",
    "file_path": "kfx/dsl/_artifact_location_test.py",
    "raw_url": "https://raw.githubusercontent.com/e2fyi/kfx/master/kfx/dsl/_artifact_location_test.py",
    "content": "\"\"\"Tests for kfx.lib.utils.\"\"\"\nimport os\nfrom typing import List\n\nimport kfp.compiler\nimport kfp.components\nimport kfp.dsl\nimport kubernetes.client as k8s_client\nfrom kfp.compiler import Compiler\nfrom kfp.components import OutputTextFile\n\nimport kfx.dsl._artifact_location\n\n\ndef is_envs_similar(\n    envs: List[k8s_client.V1EnvVar], expected_envs: List[k8s_client.V1EnvVar]\n) -> bool:\n\n    return [env.to_dict() for env in envs] == expected_envs\n\n\ndef test_set_workflow_name(tmp_path):\n    expected = [\n        {\"name\": \"WORKFLOW_NAME\", \"value\": \"{{workflow.name}}\", \"value_from\": None}\n    ]\n\n    @kfp.components.func_to_container_op\n    def test_op():\n        import os\n\n        print(os.environ.get(\"WORKFLOW_NAME\"))\n\n    @kfp.dsl.pipeline()\n    def test_pipeline():\n        op: kfp.dsl.ContainerOp = test_op()\n        op.apply(\n            kfx.dsl._artifact_location.set_workflow_env(\n                kfx.dsl._artifact_location.WorkflowVars(\n                    \"WORKFLOW_NAME\", template=\"{{workflow.name}}\"\n                )\n            )\n        )\n\n        assert is_envs_similar(op.container.env, expected)\n\n    outfile = tmp_path / \"pipeline.yaml\"\n    Compiler().compile(test_pipeline, str(outfile))\n\n\ndef test_set_pod_metadata(tmp_path):\n    expected = [\n        {\n            \"name\": \"POD_NAME\",\n            \"value\": None,\n            \"value_from\": {\n                \"config_map_key_ref\": None,\n                \"field_ref\": {\"api_version\": None, \"field_path\": \"metadata.name\"},\n                \"resource_field_ref\": None,\n                \"secret_key_ref\": None,\n            },\n        },\n        {\n            \"name\": \"NAMESPACE\",\n            \"value\": None,\n            \"value_from\": {\n                \"config_map_key_ref\": None,\n                \"field_ref\": {\"api_version\": None, \"field_path\": \"metadata.namespace\"},\n                \"resource_field_ref\": None,\n                \"secret_key_ref\": None,\n            },\n        },\n        {\n            \"name\": \"NODE_NAME\",\n            \"value\": None,\n            \"value_from\": {\n                \"config_map_key_ref\": None,\n                \"field_ref\": {\"api_version\": None, \"field_path\": \"spec.nodeName\"},\n                \"resource_field_ref\": None,\n                \"secret_key_ref\": None,\n            },\n        },\n    ]\n\n    @kfp.components.func_to_container_op\n    def test_op():\n        import os\n\n        print(os.environ.get(\"WORKFLOW_NAME\"))\n\n    @kfp.dsl.pipeline()\n    def test_pipeline():\n        op: kfp.dsl.ContainerOp = test_op()\n        op.apply(kfx.dsl._artifact_location.set_pod_metadata_envs())\n\n        assert is_envs_similar(op.container.env, expected)\n\n    outfile = tmp_path / \"pipeline.yaml\"\n    Compiler().compile(test_pipeline, str(outfile))\n\n\ndef test_artifact_location_helper(tmp_path):\n    expected_envs = [\n        {\"name\": \"WORKFLOW_ARTIFACT_STORAGE\", \"value\": \"minio\", \"value_from\": None},\n        {\"name\": \"WORKFLOW_ARTIFACT_BUCKET\", \"value\": \"mlpipeline\", \"value_from\": None},\n        {\n            \"name\": \"WORKFLOW_ARTIFACT_KEY_PREFIX\",\n            \"value\": \"artifacts/{{workflow.name}}/{{pod.name}}\",\n            \"value_from\": None,\n        },\n        {\"name\": \"WORKFLOW_ARTIFACT_PREFIX\", \"value\": \"test-op\", \"value_from\": None},\n    ]\n\n    helper = kfx.dsl._artifact_location.ArtifactLocationHelper(\n        scheme=\"minio\", bucket=\"mlpipeline\", key_prefix=\"artifacts/\"\n    )\n\n    @kfp.components.func_to_container_op\n    def test_op(\n        mlpipeline_ui_metadata: OutputTextFile(str),\n        markdown_data_file: OutputTextFile(str),\n        vega_data_file: OutputTextFile(str),\n    ):\n        import json\n\n        import kfx.dsl\n        import kfx.vis\n        import kfx.vis.vega\n\n        data = {\n            \"data\": [\n                {\"a\": \"A\", \"b\": 28},\n                {\"a\": \"B\", \"b\": 55},\n                {\"a\": \"C\", \"b\": 43},\n                {\"a\": \"D\", \"b\": 91},\n                {\"a\": \"E\", \"b\": 81},\n                {\"a\": \"F\", \"b\": 53},\n                {\"a\": \"G\", \"b\": 19},\n                {\"a\": \"H\", \"b\": 87},\n                {\"a\": \"I\", \"b\": 52},\n            ]\n        }\n        vega_data_file.write(json.dumps(data))\n\n        spec = {\n            \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.json\",\n            \"description\": \"A simple bar chart\",\n            \"data\": {\n                \"url\": kfx.dsl.KfpArtifact(\"vega_data_file\"),\n                \"format\": {\"type\": \"json\", \"property\": \"data\"},\n            },\n            \"mark\": \"bar\",\n            \"encoding\": {\n                \"x\": {\"field\": \"a\", \"type\": \"ordinal\"},\n                \"y\": {\"field\": \"b\", \"type\": \"quantitative\"},\n            },\n        }\n\n        markdown_data_file.write(\"### hello world\")\n        ui_metadata = kfx.vis.kfp_ui_metadata(\n            [\n                kfx.vis.markdown(kfx.dsl.KfpArtifact(\"markdown_data_file\")),\n                kfx.vis.vega.vega_web_app(spec),\n            ]\n        )\n        mlpipeline_ui_metadata.write(kfx.vis.asjson(ui_metadata))\n        print(ui_metadata.outputs[0].source)\n\n    @kfp.dsl.pipeline()\n    def test_pipeline():\n        op: kfp.dsl.ContainerOp = test_op()\n        op.apply(helper.set_envs())\n\n        assert is_envs_similar(op.container.env, expected_envs)\n\n    outfile = tmp_path / \"pipeline.yaml\"\n    Compiler().compile(test_pipeline, str(outfile))\n    # print(outfile.read_text())\n    # assert False\n\n\ndef test_kfp_artifact():\n\n    os.environ[\n        kfx.dsl._artifact_location.ArtifactLocationHelper.artifact_storage_env\n    ] = \"gcs\"\n    os.environ[\n        kfx.dsl._artifact_location.ArtifactLocationHelper.artifact_bucket_env\n    ] = \"your_bucket\"\n    os.environ[\n        kfx.dsl._artifact_location.ArtifactLocationHelper.artifact_key_prefix_env\n    ] = \"pipelines/artifact\"\n    os.environ[\n        kfx.dsl._artifact_location.ArtifactLocationHelper.artifact_prefix_env\n    ] = \"test-task\"\n\n    assert (\n        str(kfx.dsl.KfpArtifact(\"some_artifact_file\"))\n        == \"gcs://your_bucket/pipelines/artifact/test-task-some_artifact.tgz\"\n    )\n\n    assert (\n        str(kfx.dsl.KfpArtifact(\"some_artifact_path\"))\n        == \"gcs://your_bucket/pipelines/artifact/test-task-some_artifact.tgz\"\n    )\n"
  },
  {
    "repo": "e2fyi/kfx",
    "file_path": "kfx/dsl/_transformers.py",
    "raw_url": "https://raw.githubusercontent.com/e2fyi/kfx/master/kfx/dsl/_transformers.py",
    "content": "\"\"\"Transform functions that modify containerOp.\"\"\"\nfrom fnmatch import fnmatch\nfrom typing import Callable, Dict, List, Tuple, Union\n\nimport kfp.dsl\nimport kubernetes.client as k8s\n\nTransformFunc = Callable[[kfp.dsl.ContainerOp], kfp.dsl.ContainerOp]\n\n\nclass ContainerOpTransform:\n    \"\"\"Helper class to manipulate some common internal properties of ContainerOp.\n\n    Please refer to documentation for full set of transforms available.\n\n    ::\n\n        import kfp.components\n        import kfp.dsl\n        import kfx.dsl\n\n        transforms = (\n            kfx.dsl.ContainerOpTransform()\n            .set_resources(cpu=\"500m\", memory=(\"1G\", \"4G\"))\n            .set_image_pull_policy(\"Always\")\n            .set_env_vars({\"ENV\": \"production\"})\n            .set_env_var_from_secret(\"AWS_ACCESS_KEY\", secret_name=\"aws\", secret_key=\"access_key\")\n            .set_annotations({\"iam.amazonaws.com/role\": \"some-arn\"})\n        )\n\n\n        @kfp.dsl.components.func_to_container_op\n        def echo(text: str) -> str:\n            print(text)\n            return text\n\n\n        @kfp.dsl.pipeline(name=\"demo\")\n        def pipeline(text: str):\n            op1 = echo(text)\n            op2 = echo(\"%s-%s\" % text)\n\n            # u can apply the transform on op1 only\n            # op1.apply(transforms)\n\n            # or apply on all ops in the pipeline\n            kfp.dsl.get_pipeline_conf().add_op_transformer(transforms)\n\n    \"\"\"\n\n    def __init__(self, transforms: List[TransformFunc] = None):\n        \"\"\"Creates a new instance of ContainerOpTransform object.\n\n        Args:\n            transforms (List[TransformFunc], optional): Optional list of custom transform functions. Defaults to None.\n        \"\"\"\n        self._transforms: List[TransformFunc] = transforms or []\n\n    def __call__(self, op: kfp.dsl.ContainerOp) -> kfp.dsl.ContainerOp:\n        \"\"\"In-place transform of the provided ContainerOp.\n\n        Args:\n            op (kfp.dsl.ContainerOp): ContainerOp obj.\n\n        Returns:\n            kfp.dsl.ContainerOp: ContainerOp obj.\n        \"\"\"\n        [  # pylint: disable=expression-not-assigned\n            transform(op) for transform in self._transforms\n        ]\n        return op\n\n    def set_annotations(self, annotations: Dict[str, str]) -> \"ContainerOpTransform\":\n        \"\"\"Update the transform function to set the provided annotations to the ContainerOp.\n\n        Args:\n            annotations (Dict[str, str]): dict of annotation keys and values.\n        \"\"\"\n\n        def set_annotations_transform(op: kfp.dsl.ContainerOp) -> kfp.dsl.ContainerOp:\n            [  # pylint: disable=expression-not-assigned\n                op.add_pod_annotation(name, value)\n                for name, value in annotations.items()\n            ]\n            return op\n\n        self._transforms.append(set_annotations_transform)\n        return self\n\n    def set_labels(self, labels: Dict[str, str]) -> \"ContainerOpTransform\":\n        \"\"\"Update the transform function to set the provided labels to the ContainerOp.\n\n        Args:\n            labels (Dict[str, str]): dict of labels keys and values.\n        \"\"\"\n\n        def set_labels_transform(op: kfp.dsl.ContainerOp) -> kfp.dsl.ContainerOp:\n            [  # pylint: disable=expression-not-assigned\n                op.add_pod_label(name, value) for name, value in labels.items()\n            ]\n            return op\n\n        self._transforms.append(set_labels_transform)\n        return self\n\n    def add_env_vars(self, env_vars: Dict[str, str]) -> \"ContainerOpTransform\":\n        \"\"\"Update the transform function to set the provided env vars to the ContainerOp.\n\n        Args:\n            env_vars (Dict[str, str]): dict of env vars keys and values.\n        \"\"\"\n\n        def set_env_vars_transform(op: kfp.dsl.ContainerOp) -> kfp.dsl.ContainerOp:\n            [  # pylint: disable=expression-not-assigned\n                op.add_env_variable(k8s.V1EnvVar(name, value))\n                for name, value in env_vars.items()\n            ]\n            return op\n\n        self._transforms.append(set_env_vars_transform)\n        return self\n\n    def add_env_var(self, name: str, value: str) -> \"ContainerOpTransform\":\n        \"\"\"Update the transform function to set the provided env var to the ContainerOp.\n\n        Args:\n            name (str): name of the env var.\n            value (str): value of the env var.\n\n        Returns:\n            ContainerOpTransform: updated ContainerOpTransform object.\n        \"\"\"\n        self._transforms.append(\n            lambda op: op.container.add_env_variable(k8s.V1EnvVar(name, value))\n        )\n        return self\n\n    def add_env_var_from_secret(\n        self, name: str, secret_name: str, secret_key: str\n    ) -> \"ContainerOpTransform\":\n        \"\"\"Update the transform function to set the provided env var from a k8s secret.\n\n        Args:\n            name (str): name of the env var.\n            secret_name (str): name of the k8s secret.\n            secret_key (str): key to retrieve from the k8s secret.\n\n        Returns:\n            ContainerOpTransform: updated ContainerOpTransform object.\n        \"\"\"\n        self._transforms.append(\n            lambda op: op.container.add_env_variable(\n                k8s.V1EnvVar(\n                    name,\n                    value_from=k8s.V1EnvVarSource(\n                        secret_key_ref=k8s.V1SecretKeySelector(\n                            key=secret_key, name=secret_name\n                        )\n                    ),\n                )\n            )\n        )\n        return self\n\n    def add_env_var_from_configmap(self, configmap_name: str) -> \"ContainerOpTransform\":\n        \"\"\"Update the transform function to set env vars from a configmap.\n\n        Args:\n            configmap_name (str): name of the configmap.\n\n        Returns:\n            ContainerOpTransform: updated ContainerOpTransform object.\n        \"\"\"\n        self._transforms.append(\n            lambda op: op.container.add_env_from(\n                k8s.V1EnvFromSource(\n                    config_map_ref=k8s.V1ConfigMapEnvSource(name=configmap_name)\n                )\n            )\n        )\n        return self\n\n    def set_cpu_resources(\n        self, request: Union[int, str], limit: Union[int, str] = None\n    ) -> \"ContainerOpTransform\":\n        \"\"\"Update the transform function to set the cpu resources for the main container.\n\n        Args:\n            request (Union[int, str]): How much cpu to request.\n            limit (Union[int, str], optional): Max cpu load before throttling. Defaults to None.\n\n        Returns:\n            ContainerOpTransform: updated ContainerOpTransform object.\n        \"\"\"\n\n        self._transforms.append(\n            lambda op: op.container.set_cpu_request(str(request)).set_cpu_limit(\n                str(limit or request)\n            )\n        )\n\n        return self\n\n    def set_memory_resources(\n        self, request: Union[int, str], limit: Union[int, str] = None\n    ) -> \"ContainerOpTransform\":\n        \"\"\"Update the transform function to set the memory resources for the main container.\n\n        Args:\n            request (Union[int, str]): How much memory to request.\n            limit (Union[int, str], optional): Max memory before killing the pod. Defaults to None.\n\n        Returns:\n            ContainerOpTransform: updated ContainerOpTransform object.\n        \"\"\"\n        self._transforms.append(\n            lambda op: op.container.set_memory_request(str(request)).set_memory_limit(\n                str(limit or request)\n            )\n        )\n\n        return self\n\n    def set_gpu_limit(\n        self, value: Union[int, str], vendor: str = \"nvidia\"\n    ) -> \"ContainerOpTransform\":\n        \"\"\"Update the transform function to set the cpu limit for the main container.\n\n        Args:\n            value (Union[int, str]): GPU limit for the main container.\n            vendor (str, optional): Either \"nvidia\" or \"amd\". Defaults to \"nvidia\".\n\n        Returns:\n            ContainerOpTransform: updated ContainerOpTransform object.\n        \"\"\"\n        self._transforms.append(\n            lambda op: op.container.set_gpu_limit(str(value), vendor)\n        )\n        return self\n\n    def set_image_pull_policy(self, policy: str) -> \"ContainerOpTransform\":\n        \"\"\"Update the transform function to set the image pull policy for the main container.\n\n        Args:\n            policy (str): One of \"Always\", \"Never\", \"IfNotPresent\".\n\n        Returns:\n            ContainerOpTransform: updated ContainerOpTransform object.\n        \"\"\"\n        self._transforms.append(lambda op: op.container.set_image_pull_policy(policy))\n        return self\n\n    def set_sidecar_image_pull_policy(\n        self, policy: str, sidecar_name: str = \"*\"\n    ) -> \"ContainerOpTransform\":\n        \"\"\"Update the transform function to set the image pull policy for the sidecars.\n\n        Args:\n            policy (str): One of \"Always\", \"Never\", \"IfNotPresent\".\n            sidecar_name (str, optional): Glob pattern for sidecar name. Defaults to \"*\".\n\n        Returns:\n            ContainerOpTransform: updated ContainerOpTransform object.\n        \"\"\"\n\n        def set_sidecar_transform(op: kfp.dsl.ContainerOp) -> kfp.dsl.ContainerOp:\n            [  # pylint: disable=expression-not-assigned\n                sidecar.set_image_pull_policy(policy)\n                for sidecar in op.sidecars\n                if fnmatch(sidecar.name, sidecar_name)\n            ]\n\n        self._transforms.append(set_sidecar_transform)\n        return self\n\n    def set_resources(\n        self,\n        cpu: Union[int, str, Tuple[Union[int, str], Union[int, str]]] = None,\n        memory: Union[str, Tuple[str, str]] = None,\n    ) -> \"ContainerOpTransform\":\n        \"\"\"Update the transform function to set the cpu and memory resources for the main container.\n\n        If a int or str is provided, the resource request will equal the limit (i.e. QoS is Guaranteed).\n        Otherwise, a tuple should be provided specifying the request and the limit.\n\n        Args:\n            cpu (Union[int, str, Tuple[Union[int, str], Union[int, str]]], optional): A str or tuple representing the cpu request and limit.\n            memory (Union[str, Tuple[str, str]], optional): A str or tuple representing the memory request and limit.\n\n        Returns:\n            ContainerOpTransform: updated ContainerOpTransform object.\n        \"\"\"\n        if isinstance(cpu, (tuple, list)):\n            cpu_request, cpu_limit = cpu\n        else:\n            cpu_request = cpu  # type: ignore\n            cpu_limit = cpu  # type: ignore\n\n        if isinstance(memory, (tuple, list)):\n            memory_request, memory_limit = memory\n        else:\n            memory_request = memory  # type: ignore\n            memory_limit = memory  # type: ignore\n\n        def set_resources_transform(op: kfp.dsl.ContainerOp) -> kfp.dsl.ContainerOp:\n\n            if cpu_request:\n                op.container.set_cpu_request(str(cpu_request))\n            if cpu_limit:\n                op.container.set_cpu_limit(str(cpu_limit))\n            if memory_request:\n                op.container.set_memory_request(memory_request)\n            if memory_limit:\n                op.container.set_memory_limit(memory_limit)\n            return op\n\n        self._transforms.append(set_resources_transform)\n        return self\n\n    def set_sidecar_resources(\n        self,\n        cpu: Union[int, str, Tuple[Union[int, str], Union[int, str]]] = None,\n        memory: Union[str, Tuple[str, str]] = None,\n        sidecar_name: str = \"*\",\n    ) -> \"ContainerOpTransform\":\n        \"\"\"Update the transform function to set the cpu and memory resources for the sidecars.\n\n        If a int or str is provided, the resource request will equal the limit (i.e. QoS is Guaranteed).\n        Otherwise, a tuple should be provided specifying the request and the limit.\n\n        Args:\n            cpu (Union[int, str, Tuple[Union[int, str], Union[int, str]]], optional): A str or tuple representing the cpu request and limit.\n            memory (Union[str, Tuple[str, str]], optional): A str or tuple representing the memory request and limit.\n            sidecar_name (str, optional): Glob pattern matching the sidecar name. Defaults to \"*\".\n\n        Returns:\n            ContainerOpTransform: updated ContainerOpTransform object.\n        \"\"\"\n        if isinstance(cpu, (tuple, list)):\n            cpu_request, cpu_limit = cpu\n        else:\n            cpu_request = cpu  # type: ignore\n            cpu_limit = cpu  # type: ignore\n\n        if isinstance(memory, (tuple, list)):\n            memory_request, memory_limit = memory\n        else:\n            memory_request = memory  # type: ignore\n            memory_limit = memory  # type: ignore\n\n        def set_sidecar_resources_transform(\n            op: kfp.dsl.ContainerOp,\n        ) -> kfp.dsl.ContainerOp:\n            for sidecar in op.sidecars:\n                if fnmatch(sidecar.name, sidecar_name):\n                    if cpu_request:\n                        sidecar.set_cpu_request(str(cpu_request))\n                    if cpu_limit:\n                        sidecar.set_cpu_limit(str(cpu_limit))\n                    if memory_request:\n                        sidecar.set_memory_request(memory_request)\n                    if memory_limit:\n                        sidecar.set_memory_limit(memory_limit)\n            return op\n\n        self._transforms.append(set_sidecar_resources_transform)\n        return self\n"
  },
  {
    "repo": "e2fyi/kfx",
    "file_path": "kfx/dsl/_transformers_test.py",
    "raw_url": "https://raw.githubusercontent.com/e2fyi/kfx/master/kfx/dsl/_transformers_test.py",
    "content": "\"\"\"Test for ContainerOp transformers.\"\"\"\nimport kfp.dsl\nimport pytest\n\nfrom kfx.dsl._transformers import ContainerOpTransform\n\n\n@pytest.fixture\ndef op() -> kfp.dsl.ContainerOp:\n    return kfp.dsl.ContainerOp(\n        name=\"hello\",\n        image=\"bash\",\n        sidecars=[\n            kfp.dsl.Sidecar(name=\"foo\", image=\"bash\"),\n            kfp.dsl.Sidecar(name=\"bar\", image=\"bash\"),\n        ],\n    )\n\n\ndef test_containerop_transform_image_pull_policy(op: kfp.dsl.ContainerOp):\n\n    transform = (\n        ContainerOpTransform()\n        .set_image_pull_policy(\"Always\")\n        .set_sidecar_image_pull_policy(\"Always\", \"f*\")\n    )\n\n    op.apply(transform)\n    assert op.container.image_pull_policy == \"Always\"\n    assert [sidecar.image_pull_policy for sidecar in op.sidecars] == [\"Always\", None]\n\n\ndef test_containerop_transform_set_resources(op: kfp.dsl.ContainerOp):\n    transform = ContainerOpTransform().set_resources(cpu=(1, 2), memory=(\"1G\", \"2G\"))\n    op.apply(transform)\n    assert op.container.resources.requests == {\"cpu\": \"1\", \"memory\": \"1G\"}\n    assert op.container.resources.limits == {\"cpu\": \"2\", \"memory\": \"2G\"}\n\n    transform = ContainerOpTransform().set_resources(cpu=\"500m\", memory=\"4G\")\n    op.apply(transform)\n    assert op.container.resources.requests == {\"cpu\": \"500m\", \"memory\": \"4G\"}\n    assert op.container.resources.limits == {\"cpu\": \"500m\", \"memory\": \"4G\"}\n\n\ndef test_containerop_transform_set_sidecar_resources(op: kfp.dsl.ContainerOp):\n\n    transform = ContainerOpTransform().set_sidecar_resources(cpu=\"500m\", memory=\"4G\")\n    op.apply(transform)\n    assert [sidecar.resources.requests for sidecar in op.sidecars] == [\n        {\"cpu\": \"500m\", \"memory\": \"4G\"},\n        {\"cpu\": \"500m\", \"memory\": \"4G\"},\n    ]\n    assert [sidecar.resources.limits for sidecar in op.sidecars] == [\n        {\"cpu\": \"500m\", \"memory\": \"4G\"},\n        {\"cpu\": \"500m\", \"memory\": \"4G\"},\n    ]\n\n    transform = (\n        ContainerOpTransform()\n        .set_sidecar_resources(cpu=(1, 2), memory=(\"1G\", \"2G\"), sidecar_name=\"f*\")\n        .set_sidecar_resources(cpu=\"200m\", memory=\"3G\", sidecar_name=\"bar\")\n    )\n    op.apply(transform)\n    assert [sidecar.resources.requests for sidecar in op.sidecars] == [\n        {\"cpu\": \"1\", \"memory\": \"1G\"},\n        {\"cpu\": \"200m\", \"memory\": \"3G\"},\n    ]\n    assert [sidecar.resources.limits for sidecar in op.sidecars] == [\n        {\"cpu\": \"2\", \"memory\": \"2G\"},\n        {\"cpu\": \"200m\", \"memory\": \"3G\"},\n    ]\n\n\ndef test_containerop_transform_set_annotation_labels(op: kfp.dsl.ContainerOp):\n    transform = (\n        ContainerOpTransform()\n        .set_annotations({\"foo\": \"bar\"})\n        .set_labels({\"hello\": \"world\"})\n    )\n    op.apply(transform)\n\n    assert op.pod_annotations == {\"foo\": \"bar\"}\n    assert op.pod_labels == {\"hello\": \"world\"}\n\n\ndef test_containerop_transform_add_envs(op: kfp.dsl.ContainerOp):\n    transform = (\n        ContainerOpTransform()\n        .add_env_var(\"foo\", \"bar\")\n        .add_env_vars({\"hello\": \"world\"})\n        .add_env_var_from_secret(\n            \"creds\", secret_name=\"k8s_secret\", secret_key=\"access_key\"\n        )\n        .add_env_var_from_configmap(\"some_configmap\")\n    )\n    op.apply(transform)\n\n    assert [obj.to_dict() for obj in op.container.env] == [\n        {\"name\": \"foo\", \"value\": \"bar\", \"value_from\": None},\n        {\"name\": \"hello\", \"value\": \"world\", \"value_from\": None},\n        {\n            \"name\": \"creds\",\n            \"value\": None,\n            \"value_from\": {\n                \"config_map_key_ref\": None,\n                \"field_ref\": None,\n                \"resource_field_ref\": None,\n                \"secret_key_ref\": {\n                    \"key\": \"access_key\",\n                    \"name\": \"k8s_secret\",\n                    \"optional\": None,\n                },\n            },\n        },\n    ]\n\n    assert [obj.to_dict() for obj in op.container.env_from] == [\n        {\n            \"config_map_ref\": {\"name\": \"some_configmap\", \"optional\": None},\n            \"prefix\": None,\n            \"secret_ref\": None,\n        }\n    ]\n"
  },
  {
    "repo": "glukicov/llm_pipelines_demo",
    "file_path": "pipelines/demo.py",
    "raw_url": "https://raw.githubusercontent.com/glukicov/llm_pipelines_demo/main/pipelines/demo.py",
    "content": "import logging\n\nimport typer\nfrom kfp.dsl import Metrics, Output, component, pipeline\n\nfrom pipelines.utils import job_constants, job_params, run_pipeline_on_vertex_or_locally\n\nlogging.basicConfig(level=logging.INFO)\n\n\n@component(base_image=job_constants.BASE_IMAGE)\ndef get_data(data_source: str) -> str:\n    import logging\n\n    logging.info(f\"Getting data from: {data_source}\")\n    return \"data\"\n\n\n@component(base_image=job_constants.BASE_IMAGE)\ndef call_llm(model_name: str, prompt: str) -> str:\n    import logging\n\n    logging.info(f\"Calling LLM model {model_name} with prompt: {prompt}\")\n    return \"results\"\n\n\n@component(base_image=job_constants.BASE_IMAGE)\ndef evaluate_results(results: str, metrics_output: Output[Metrics]):\n    metrics_output.metadata = {\"accuracy\": float(results == \"results\")}\n\n\n@pipeline(name=job_params.pipeline_name)\ndef demo_pipeline():\n    # Step 1: Get data\n    data_task = get_data(data_source=job_params.data_source)\n\n    # Step 2: Call LLM with the data\n    llm_task = call_llm(model_name=job_params.model_name, prompt=data_task.output)\n\n    # Step 3: Evaluate results\n    evaluate_results(results=llm_task.output)\n\n\nif __name__ == \"__main__\":\n\n    def main(\n        local: bool = typer.Option(\n            False, help=\"Run the pipeline locally instead of on Vertex AI\"\n        ),\n    ):\n        run_pipeline_on_vertex_or_locally(\n            pipeline_function=demo_pipeline,\n            params=job_params,\n            constants=job_constants,\n            local=local,\n        )\n\n    typer.run(main)\n"
  },
  {
    "repo": "canonical/kfp-operators",
    "file_path": "tests/integration/pipelines/pipeline_container_no_input.py",
    "raw_url": "https://raw.githubusercontent.com/canonical/kfp-operators/main/tests/integration/pipelines/pipeline_container_no_input.py",
    "content": "# Copyright 2022 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport os\n\nfrom kfp import compiler\nfrom kfp import dsl\n\n\n@dsl.container_component\ndef container_no_input():\n    return dsl.ContainerSpec(\n        image=\"python:3.7\",\n        command=[\"echo\", \"hello world\"],\n        args=[],\n    )\n\n\n@dsl.pipeline(name=\"v2-container-component-no-input\")\ndef pipeline_container_no_input():\n    container_no_input()\n\n\nif __name__ == \"__main__\":\n    # execute only if run as a script\n    compiler.Compiler().compile(\n        pipeline_func=pipeline_container_no_input, package_path=\"pipeline_container_no_input.yaml\"\n    )\n"
  },
  {
    "repo": "Anvil-Late/Kubeflow_advanced_pipeline",
    "file_path": "pipeline/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/Anvil-Late/Kubeflow_advanced_pipeline/main/pipeline/pipeline.py",
    "content": "\nimport kfp\n\nmerge_and_split_op = kfp.components.load_component_from_file(\"./kf_utils/merge_and_split_op.yaml\")\npreprocess_dataset_op = kfp.components.load_component_from_file(\"./kf_utils/preprocess_dataset_op.yaml\")\nprepare_data_op = kfp.components.load_component_from_file(\"./kf_utils/prepare_data_op.yaml\")\ntrain_svm_op = kfp.components.load_component_from_file(\"./kf_utils/train_svm_op.yaml\")\ntrain_randomforest_op = kfp.components.load_component_from_file(\"./kf_utils/train_randomforest_op.yaml\")\ntrain_xgb_op = kfp.components.load_component_from_file(\"./kf_utils/train_xgb_op.yaml\")\nevaluate_models_op = kfp.components.load_component_from_file(\"./kf_utils/evaluate_models_op.yaml\")\ntrain_best_model_op = kfp.components.load_component_from_file(\"./kf_utils/train_best_model_op.yaml\")\nmodel_predict_op = kfp.components.load_component_from_file(\"./kf_utils/model_predict_op.yaml\")\n\n@kfp.dsl.pipeline(\n   name='Emission prediction pipeline',\n   description='An example pipeline.'\n)\ndef emission_pipeline(\n    bucket,\n    data_2015,\n    data_2016,\n    hyperopt_iterations,\n    subfolder\n):\n    merge_and_split_task = merge_and_split_op(bucket, data_2015, data_2016)\n    preprocess_task = preprocess_dataset_op(merge_and_split_task.outputs['output_edfcsv'])\n    preparation_task = prepare_data_op(preprocess_task.outputs['output_cleandatacsv'])\n    \n    rf_train_task = train_randomforest_op(preparation_task.outputs['output_xtraincsv'],\n                                         preparation_task.outputs['output_ytraincsv'],\n                                         preparation_task.outputs['output_xtestcsv'],\n                                         preparation_task.outputs['output_ytestcsv'],\n                                         hyperopt_iterations)\n    \n    xgb_train_task = train_xgb_op(preparation_task.outputs['output_xtraincsv'],\n                                 preparation_task.outputs['output_ytraincsv'],\n                                 preparation_task.outputs['output_xtestcsv'],\n                                 preparation_task.outputs['output_ytestcsv'],\n                                 hyperopt_iterations)\n    \n    svm_train_task = train_svm_op(preparation_task.outputs['output_xtraincsv'],\n                                 preparation_task.outputs['output_ytraincsv'],\n                                 preparation_task.outputs['output_xtestcsv'],\n                                 preparation_task.outputs['output_ytestcsv'],\n                                 hyperopt_iterations)\n    \n    evaluate_models_task = evaluate_models_op(bucket,\n                                              subfolder,\n                                              svm_train_task.outputs['MSE'],\n                                              svm_train_task.outputs['R2'],\n                                              svm_train_task.outputs['hyperparams'],\n                                              xgb_train_task.outputs['MSE'],\n                                              xgb_train_task.outputs['R2'],\n                                              xgb_train_task.outputs['hyperparams'],\n                                              rf_train_task.outputs['MSE'],\n                                              rf_train_task.outputs['R2'],\n                                              rf_train_task.outputs['hyperparams']\n                                             )\n    \n    train_best_model_task = train_best_model_op(evaluate_models_task.outputs['best_model'],\n                                               evaluate_models_task.outputs['hyperparams'],\n                                               preparation_task.outputs['output_xtraincsv'],\n                                               preparation_task.outputs['output_ytraincsv'])\n    \n    model_predict_task = model_predict_op(train_best_model_task.outputs['output_pickle_model'],\n                                          preparation_task.outputs['output_xtestcsv'])\n"
  },
  {
    "repo": "jhammarstedt/MLOps-Kubeflow_in_GCP",
    "file_path": "pipeline/main.py",
    "raw_url": "https://raw.githubusercontent.com/jhammarstedt/MLOps-Kubeflow_in_GCP/master/pipeline/main.py",
    "content": "import kfp\nimport os\nfrom kfp.gcp import use_gcp_secret\nimport kfp.dsl as dsl\nimport argparse\n\n\n@dsl.pipeline(\n    name='ml-demo',\n)\ndef preprocess_train_deploy(\n        project='ml-pipeline-309409',\n        bucket='ml-pipeline-309409_bucket',\n):\n    # 1 load data\n    data = dsl.ContainerOp(\n        name='preprocess',\n        # image needs to be a compile-time string\n        image='gcr.io/ml-pipeline-309409/ml-demo-data:latest',\n        arguments=[\n            '--project', project,\n            '--mode', 'cloud',\n            '--bucket', bucket,\n          ],\n        file_outputs = {'bucket':'/output.txt'}\n    ).apply(use_gcp_secret('user-gcp-sa'))\n    data.container.set_image_pull_policy('Always')\n\n\n    # 2 train\n    train = dsl.ContainerOp(\n      name='train',\n      # image needs to be a compile-time string\n      image='gcr.io/ml-pipeline-309409/ml-demo-train:latest',\n      arguments=[\n        data.outputs['bucket']\n      ],\n      file_outputs={'model': '/modelOutput.txt'}\n    ).apply(use_gcp_secret('user-gcp-sa'))\n    train.container.set_image_pull_policy('Always')\n\n\n    # 3 deploy the trained model to Cloud ML Engine\n    deploymodel = dsl.ContainerOp(\n      name='deploymodel',\n      # image needs to be a compile-time string\n      image='gcr.io/ml-pipeline-309409/ml-demo-deploy-toai:latest',\n      arguments=[\n        train.outputs['model'],\n      ],\n      file_outputs={\n        'model': '/model.txt',\n        'version': '/version.txt'\n      }\n    ).apply(use_gcp_secret('user-gcp-sa'))\n    deploymodel.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n    deploymodel.container.set_image_pull_policy('Always')\n\n\ndef handle_newfile(data, context):\n    PIPELINES_HOST = os.environ.get('PIPELINES_HOST', \"Environment variable PIPELINES_HOST not set\")\n    PROJECT = os.environ.get('PROJECT', \"Environment variable PROJECT not set\")\n    BUCKET = os.environ.get('BUCKET', \"Environment variable BUCKET not set\")\n    client = kfp.Client(host=PIPELINES_HOST)\n    client.create_run_from_pipeline_func(preprocess_train_deploy, {'project': PROJECT, 'bucket': BUCKET})\n"
  },
  {
    "repo": "ferneutron/mlops",
    "file_path": "src/pipelines/iris_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/ferneutron/mlops/main/src/pipelines/iris_pipeline.py",
    "content": "import sys\n\nimport kfp\n\nsys.path.append(\"src\")\n\nPIPELIE_NAME = \"The-Iris-Pipeline-v1\"\nPIPELINE_ROOT = \"gs://mlops-demo-youtube/pipeline_root\"\n\n\n@kfp.dsl.pipeline(name=PIPELIE_NAME, pipeline_root=PIPELINE_ROOT)\ndef pipeline(project_id: str, location: str, bq_dataset: str, bq_table: str):\n    from components.data import load_data\n    from components.evaluation import choose_best_model\n    from components.models import decision_tree, random_forest\n    from components.register import upload_model\n\n    data_op = load_data(\n        project_id=project_id, bq_dataset=bq_dataset, bq_table=bq_table\n    ).set_display_name(\"Load data from BigQuery\")\n\n    dt_op = decision_tree(\n        train_dataset=data_op.outputs[\"train_dataset\"]\n    ).set_display_name(\"Decision Tree\")\n\n    rf_op = random_forest(\n        train_dataset=data_op.outputs[\"train_dataset\"]\n    ).set_display_name(\"Random Forest\")\n\n    choose_model_op = choose_best_model(\n        test_dataset=data_op.outputs[\"test_dataset\"],\n        decision_tree_model=dt_op.outputs[\"output_model\"],\n        random_forest_model=rf_op.outputs[\"output_model\"],\n    ).set_display_name(\"Select best Model\")\n\n    upload_model(\n        project_id=project_id,\n        location=location,\n        model=choose_model_op.outputs[\"best_model\"],\n    ).set_display_name(\"Register Model\")\n\n\nif __name__ == \"__main__\":\n    kfp.compiler.Compiler().compile(\n        pipeline_func=pipeline, package_path=f\"pipeline.yaml\"\n    )\n"
  },
  {
    "repo": "ferneutron/mlops",
    "file_path": "src/pipelines/components/data.py",
    "raw_url": "https://raw.githubusercontent.com/ferneutron/mlops/main/src/pipelines/components/data.py",
    "content": "from kfp.dsl import Dataset, Output, component\n\n\n@component(\n    base_image=\"gcr.io/deeplearning-platform-release/tf2-cpu.2-6:latest\",\n    packages_to_install=[\n        \"pandas\",\n        \"google-cloud-bigquery\",\n    ],\n)\ndef load_data(\n    project_id: str,\n    bq_dataset: str,\n    bq_table: str,\n    train_dataset: Output[Dataset],\n    test_dataset: Output[Dataset],\n):\n    import pandas as pd\n    from google.cloud import bigquery\n    from sklearn.model_selection import train_test_split\n\n    client = bigquery.Client()\n\n    dataset_ref = bigquery.DatasetReference(project_id, bq_dataset)\n    table_ref = dataset_ref.table(bq_table)\n    table = bigquery.Table(table_ref)\n    iterable_table = client.list_rows(table).to_dataframe_iterable()\n\n    dfs = []\n    for row in iterable_table:\n        dfs.append(row)\n\n    df = pd.concat(dfs, ignore_index=True)\n    del dfs\n\n    df[\"Species\"].replace(\n        {\n            \"Iris-versicolor\": 0,\n            \"Iris-virginica\": 1,\n            \"Iris-setosa\": 2,\n        },\n        inplace=True,\n    )\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        df.drop(\"Species\", axis=1),\n        df[\"Species\"],\n        test_size=0.2,\n        random_state=42,\n    )\n\n    X_train[\"Species\"] = y_train\n    X_test[\"Species\"] = y_test\n\n    X_train.to_csv(f\"{train_dataset.path}\", index=False)\n    X_test.to_csv(f\"{test_dataset.path}\", index=False)\n"
  },
  {
    "repo": "ferneutron/mlops",
    "file_path": "src/pipelines/components/evaluation.py",
    "raw_url": "https://raw.githubusercontent.com/ferneutron/mlops/main/src/pipelines/components/evaluation.py",
    "content": "from kfp.dsl import Dataset, Input, Metrics, Model, Output, component\n\n\n@component(\n    base_image=\"gcr.io/deeplearning-platform-release/tf2-cpu.2-6:latest\",\n    packages_to_install=[\n        \"pandas==1.3.5\",\n        \"joblib==1.1.0\",\n    ],\n)\ndef choose_best_model(\n    test_dataset: Input[Dataset],\n    decision_tree_model: Input[Model],\n    random_forest_model: Input[Model],\n    metrics: Output[Metrics],\n    best_model: Output[Model],\n):\n    import joblib\n    import pandas as pd\n    from sklearn.metrics import accuracy_score\n\n    test_data = pd.read_csv(test_dataset.path)\n\n    dt = joblib.load(decision_tree_model.path)\n    rf = joblib.load(random_forest_model.path)\n\n    dt_pred = dt.predict(test_data.drop(\"Species\", axis=1))\n    rf_pred = rf.predict(test_data.drop(\"Species\", axis=1))\n\n    dt_accuracy = accuracy_score(test_data[\"Species\"], dt_pred)\n    rf_accuracy = accuracy_score(test_data[\"Species\"], rf_pred)\n\n    metrics.log_metric(\"Decision Tree (Accuracy)\", (dt_accuracy))\n    metrics.log_metric(\"Random Forest (Accuracy)\", (rf_accuracy))\n\n    if dt_accuracy > rf_accuracy:\n        joblib.dump(dt, best_model.path)\n    else:\n        joblib.dump(rf, best_model.path)\n"
  },
  {
    "repo": "ferneutron/mlops",
    "file_path": "src/pipelines/components/models.py",
    "raw_url": "https://raw.githubusercontent.com/ferneutron/mlops/main/src/pipelines/components/models.py",
    "content": "from kfp.dsl import Dataset, Input, Metrics, Model, Output, component\n\n\n@component(\n    base_image=\"gcr.io/deeplearning-platform-release/tf2-cpu.2-6:latest\",\n    packages_to_install=[\n        \"pandas==1.3.5\",\n        \"joblib==1.1.0\",\n    ],\n)\ndef decision_tree(\n    train_dataset: Input[Dataset],\n    metrics: Output[Metrics],\n    output_model: Output[Model],\n):\n    import joblib\n    import pandas as pd\n    from sklearn.metrics import accuracy_score\n    from sklearn.model_selection import train_test_split\n    from sklearn.tree import DecisionTreeClassifier\n\n    train = pd.read_csv(train_dataset.path)\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        train.drop(\"Species\", axis=1),\n        train[\"Species\"],\n        test_size=0.2,\n        random_state=42,\n    )\n\n    model = DecisionTreeClassifier()\n    model.fit(X_train, y_train)\n    pred = model.predict(X_test)\n    acc = accuracy_score(y_test, pred)\n\n    metrics.log_metric(\"accuracy\", (acc))\n\n    joblib.dump(model, output_model.path)\n\n\n@component(\n    base_image=\"gcr.io/deeplearning-platform-release/tf2-cpu.2-6:latest\",\n    packages_to_install=[\n        \"pandas==1.3.5\",\n        \"joblib==1.1.0\",\n    ],\n)\ndef random_forest(\n    train_dataset: Input[Dataset],\n    metrics: Output[Metrics],\n    output_model: Output[Model],\n):\n    import joblib\n    import pandas as pd\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.metrics import accuracy_score, roc_auc_score\n    from sklearn.model_selection import train_test_split\n\n    train = pd.read_csv(train_dataset.path)\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        train.drop(\"Species\", axis=1),\n        train[\"Species\"],\n        test_size=0.2,\n        random_state=42,\n    )\n\n    model = RandomForestClassifier()\n    model.fit(X_train, y_train)\n    pred = model.predict(X_test)\n    acc = accuracy_score(y_test, pred)\n\n    metrics.log_metric(\"accuracy\", (acc))\n\n    joblib.dump(model, output_model.path)\n"
  },
  {
    "repo": "ferneutron/mlops",
    "file_path": "src/pipelines/components/register.py",
    "raw_url": "https://raw.githubusercontent.com/ferneutron/mlops/main/src/pipelines/components/register.py",
    "content": "from kfp.dsl import Input, Model, component\n\n\n@component(\n    base_image=\"gcr.io/deeplearning-platform-release/tf2-cpu.2-6:latest\",\n    packages_to_install=[\"google-cloud-aiplatform\"],\n)\ndef upload_model(\n    project_id: str,\n    location: str,\n    model: Input[Model],\n):\n    from google.cloud import aiplatform\n\n    aiplatform.init(project=project_id, location=location)\n\n    aiplatform.Model.upload_scikit_learn_model_file(\n        model_file_path=model.path,\n        display_name=\"IrisModelv3\",\n        project=project_id,\n    )\n"
  },
  {
    "repo": "ferneutron/mlops",
    "file_path": "src/pipelines/components/utils.py",
    "raw_url": "https://raw.githubusercontent.com/ferneutron/mlops/main/src/pipelines/components/utils.py",
    "content": "from kfp.dsl import Input, Model, component\n\n\n@component(\n    base_image=\"gcr.io/deeplearning-platform-release/tf2-cpu.2-6:latest\",\n    packages_to_install=[\"google-cloud-aiplatform\"],\n)\ndef upload_model(\n    model: Input[Model],\n):\n    from pathlib import Path\n\n    from google.cloud import aiplatform\n\n    aiplatform.init(project=\"gsd-ai-mx-ferneutron\", location=\"us-central1\")\n\n    aiplatform.Model.upload_scikit_learn_model_file(\n        model_file_path=model.path,\n        display_name=\"IrisModelv3\",\n        project=\"gsd-ai-mx-ferneutron\",\n    )\n"
  },
  {
    "repo": "iQuantC/Kubeflow-pipeline",
    "file_path": "kubeflow_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/iQuantC/Kubeflow-pipeline/main/kubeflow_pipeline.py",
    "content": "from typing import Dict, List\nfrom kfp import dsl\nfrom kfp import compiler\nimport kfp\nfrom kfp.dsl import Input, Output, Dataset, Model, component\n\n# Step 1: Load Dataset\n@dsl.component(base_image=\"python:3.9\")\ndef load_data(output_csv: Output[Dataset]):\n    import subprocess\n    subprocess.run([\"pip\", \"install\", \"pandas\", \"scikit-learn\"], check=True)\n\n    from sklearn.datasets import load_iris\n    import pandas as pd\n    iris = load_iris()\n    df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n    df['target'] = iris.target\n\n    # Save the dataset to the output artifact path\n    df.to_csv(output_csv.path, index=False)\n\n# Step 2: Preprocess Data\n@dsl.component(base_image=\"python:3.9\")\ndef preprocess_data(input_csv: Input[Dataset], output_train: Output[Dataset], output_test: Output[Dataset], \n                    output_ytrain: Output[Dataset], output_ytest: Output[Dataset]):\n    import subprocess\n    subprocess.run([\"pip\", \"install\", \"pandas\", \"scikit-learn\"], check=True)\n\n    import pandas as pd\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.model_selection import train_test_split\n\n    # Load dataset\n    df = pd.read_csv(input_csv.path)\n\n    # Debug: Check for NaN values\n    print(\"Initial dataset shape:\", df.shape)\n    print(\"Missing values before preprocessing:\\n\", df.isnull().sum())\n\n    # Handle missing values\n    if df.isnull().values.any():\n        print(\"Missing values detected. Handling them...\")\n        df = df.dropna()  # Drop rows with any NaN values\n    \n    # Validate that there are no NaNs in the target column\n    assert not df['target'].isnull().any(), \"Target column contains NaN values after handling missing values.\"\n\n    features = df.drop(columns=['target'])\n    target = df['target']\n\n    # Standardize features\n    scaler = StandardScaler()\n    scaled_features = scaler.fit_transform(features)\n\n    # Train-test split\n    X_train, X_test, y_train, y_test = train_test_split(scaled_features, target, test_size=0.2, random_state=42)\n\n    # Debug: Validate splits\n    print(\"Shapes after train-test split:\")\n    print(\"X_train:\", X_train.shape, \"X_test:\", X_test.shape) \n    print(\"y_train:\", y_train.shape, \"y_test:\", y_test.shape)\n    print(\"Missing values in y_train:\", y_train.isnull().sum())\n\n    # Ensure no NaNs in the split data\n    assert not y_train.isnull().any(), \"y_train contains NaN values.\"\n    assert not y_test.isnull().any(), \"y_test contains NaN values.\"\n\n    # Create DataFrames for train and test sets\n    X_train_df = pd.DataFrame(X_train, columns=features.columns)\n    print(\"X_train_df:\", X_train_df) \n\n    y_train_df = pd.DataFrame(y_train) \n    print(\"y_train_df: \", y_train_df)  \n\n    X_test_df = pd.DataFrame(X_test, columns=features.columns)\n    print(\"X_test_df:\", X_test_df) \n\n    y_test_df = pd.DataFrame(y_test) \n    print(\"y_test_df: \", y_test_df) \n\n    # Save processed train and test data\n    X_train_df.to_csv(output_train.path, index=False)  \n    X_test_df.to_csv(output_test.path, index=False)\n\n    y_train_df.to_csv(output_ytrain.path, index=False)  \n    y_test_df.to_csv(output_ytest.path, index=False) \n\n# Step 3: Train Model\n@dsl.component(base_image=\"python:3.9\")\ndef train_model(train_data: Input[Dataset], ytrain_data: Input[Dataset], model_output: Output[Model]):\n\n    import subprocess\n    subprocess.run([\"pip\", \"install\", \"pandas\", \"scikit-learn\", \"joblib\"], check=True)\n\n    import pandas as pd\n    from sklearn.linear_model import LogisticRegression\n    from joblib import dump\n\n    # Load training data\n    train_df = pd.read_csv(train_data.path)\n    print(\"Shape of train_df:\", train_df.shape)\n    print(\"train_df:\", train_df)\n    X_train = train_df \n\n    y_train = pd.read_csv(ytrain_data.path)\n    print(\"Shape of ytrain_df:\", y_train.shape)\n    print(\"y_train_df:\", y_train)\n\n    # Debug: Validate splits\n    print(\"Shapes of X_train and y_train: \")\n    print(\"X_train:\", X_train.shape)\n    print(\"y_train:\", y_train.shape) \n    print(\"Missing values in X_train:\", X_train.isnull().sum())\n    print(\"Missing values in y_train:\", y_train.isnull().sum()) \n\n    # Ensure no NaN values\n    assert not X_train.isnull().values.any(), \"X_train contains NaN values.\"\n    assert not y_train.isnull().values.any(), \"y_train contains NaN values.\" \n\n    # Train model\n    model = LogisticRegression()\n    model.fit(X_train, y_train)\n\n    # Save model\n    dump(model, model_output.path)\n\n# Step 4: Evaluate Model\n@dsl.component(base_image=\"python:3.9\")\ndef evaluate_model(test_data: Input[Dataset], ytest_data: Input[Dataset], model: Input[Model], metrics_output: Output[Dataset]):\n    import subprocess\n    subprocess.run([\"pip\", \"install\", \"pandas\", \"scikit-learn\", \"matplotlib\", \"joblib\"], check=True)\n\n    import pandas as pd\n    from sklearn.metrics import classification_report, confusion_matrix\n    import matplotlib.pyplot as plt\n    from joblib import load\n\n    # Load test data\n    X_test = pd.read_csv(test_data.path)\n\n    y_test = pd.read_csv(ytest_data.path)  \n\n    # Load model\n    model = load(model.path)\n\n    # Predict\n    y_pred = model.predict(X_test)\n\n    # Generate metrics\n    report = classification_report(y_test, y_pred, output_dict=True)\n    cm = confusion_matrix(y_test, y_pred)\n\n    # Save metrics to a file\n    metrics_path = metrics_output.path\n    with open(metrics_path, 'w') as f:\n        f.write(str(report))\n\n    # Plot confusion matrix\n    plt.figure(figsize=(8, 6))\n    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n    plt.title('Confusion Matrix')\n    plt.colorbar()\n    plt.xlabel('Predicted Label')\n    plt.ylabel('True Label')\n    plt.savefig(metrics_path.replace('.txt', '.png'))\n\n# Define the pipeline\n@dsl.pipeline(name=\"ml-pipeline\")\ndef ml_pipeline():\n    # Step 1: Load Dataset\n    load_op = load_data()\n\n    # Step 2: Preprocess Data\n    preprocess_op = preprocess_data(input_csv=load_op.outputs[\"output_csv\"])\n\n    # Step 3: Train Model\n    train_op = train_model(train_data=preprocess_op.outputs[\"output_train\"], ytrain_data=preprocess_op.outputs[\"output_ytrain\"])\n\n    # Step 4: Evaluate Model\n    evaluate_op = evaluate_model(test_data=preprocess_op.outputs[\"output_test\"], ytest_data=preprocess_op.outputs[\"output_ytest\"], model=train_op.outputs[\"model_output\"]) \n\n# Compile the pipeline\nif __name__ == \"__main__\":\n    compiler.Compiler().compile(pipeline_func=ml_pipeline, package_path=\"kubeflow_pipeline.yaml\")\n"
  },
  {
    "repo": "DanielAvdar/protocol-task",
    "file_path": "protocol_task_kfp/executors.py",
    "raw_url": "https://raw.githubusercontent.com/DanielAvdar/protocol-task/main/protocol_task_kfp/executors.py",
    "content": "# flake8: noqa: F403, F405, B006\nfrom typing import *\n\nfrom kfp.dsl import *\n\n\ndef artifacttaskexecutor(\n    task_params: dict,\n    task_module: str,\n    input_artifact: Input[Artifact],\n    output_artifact: Output[Artifact],\n):\n    from protocol_task.task_executors import ArtifactTaskExecutor\n\n    comp = ArtifactTaskExecutor(\n        task_params=task_params,\n        task_module=task_module,\n        input_artifact=input_artifact,\n        output_artifact=output_artifact,\n    )\n    comp.execute()\n\n\ndef artifacttaskinitexecutor(\n    task_params: dict,\n    task_module: str,\n    output_artifact: Output[Artifact],\n):\n    from protocol_task.task_executors import ArtifactTaskInitExecutor\n\n    comp = ArtifactTaskInitExecutor(\n        task_params=task_params,\n        task_module=task_module,\n        output_artifact=output_artifact,\n    )\n    comp.execute()\n"
  },
  {
    "repo": "kfous/kubeflow-pipeline",
    "file_path": "mlflow_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/kfous/kubeflow-pipeline/main/mlflow_pipeline.py",
    "content": "from kfp import dsl\nfrom kfp.components import create_component_from_func\n\ndef mlflow_logging():\n    import subprocess\n    # Dynamically install MLflow\n    subprocess.run([\"pip\", \"install\", \"--no-cache-dir\", \"mlflow\"])\n\n    import mlflow\n    mlflow.set_tracking_uri(\"http://host.docker.internal:5000\")\n    mlflow.start_run()\n    mlflow.log_param(\"param\", 42)\n    mlflow.log_metric(\"accuracy\", 0.95)\n    mlflow.end_run()\n\nmlflow_logging_op = create_component_from_func(\n    mlflow_logging,\n    base_image='python:3.8-slim'  # base Docker image with Python installed\n)\n\n@dsl.pipeline(\n    name='MLflow Logging Pipeline',\n    description='A pipeline that logs parameters and metrics to MLflow'\n)\ndef mlflow_pipeline():\n    mlflow_logging_op()\n\nif __name__ == '__main__':\n    from kfp.compiler import Compiler\n    Compiler().compile(\n        pipeline_func=mlflow_pipeline,\n        package_path='mlflow_pipeline.yaml'\n    )\n"
  },
  {
    "repo": "pharmbio/kubeflow-pipelines",
    "file_path": "kensert_CNN/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/pharmbio/kubeflow-pipelines/master/kensert_CNN/pipeline.py",
    "content": "import kfp.dsl as dsl\nimport kfp.gcp as gcp\nfrom kubernetes.client.models import V1SecretKeySelector, V1EnvVar\nfrom kubernetes import client as k8s_client\nfrom json import loads\n\ndef mount_pvc(pvc_name='pipeline-claim', volume_name='pipeline', volume_mount_path='/mnt/pipeline', volume_sub_path=None):\n    \"\"\"\n        Modifier function to apply to a Container Op to simplify volume, volume mount addition and\n        enable better reuse of volumes, volume claims across container ops.\n        Usage:\n            train = train_op(...)\n            train.apply(mount_pvc('claim-name', 'pipeline', '/mnt/pipeline'))\n    \"\"\"\n    def _mount_pvc(task):\n        from kubernetes import client as k8s_client\n        local_pvc = k8s_client.V1PersistentVolumeClaimVolumeSource(claim_name=pvc_name)\n        return (\n            task\n                .add_volume(\n                    k8s_client.V1Volume(name=volume_name, persistent_volume_claim=local_pvc)\n                )\n                .add_volume_mount(\n                    k8s_client.V1VolumeMount(mount_path=volume_mount_path, name=volume_name, sub_path=volume_sub_path)\n                )\n        )\n    return _mount_pvc\n\n\ndef set_resources(memory_req=None, memory_lim=None, cpu_req=None, cpu_lim=None, gpus=\"0\", container:dsl.ContainerOp.container=None):\n    if container:\n        container.set_memory_request(memory_req)\n        container.set_memory_limit(memory_lim)\n        container.set_cpu_request(cpu_req)\n        container.set_cpu_limit(cpu_lim)\n        if int(gpus) > 0:\n            container.set_gpu_limit(gpus)\n        return container\n    else:\n        return None\n\n@dsl.pipeline(\n  name='Kensert_CNN_test',\n  description='Testing a CNN workflow in kfpipelines'\n)\ndef cnn_workflow(\n    ### inputs to pipeline\n    model_type: dsl.PipelineParam=dsl.PipelineParam(name=\"model_type\", value=\"Inception_v3\"),\n    artifact_bucket: dsl.PipelineParam=dsl.PipelineParam(name=\"artifact_bucket\", value=\"kensert_CNN\"),\n    checkpoint_preprocess: dsl.PipelineParam=dsl.PipelineParam(name=\"checkpoint_preprocess\", value=\"false\"),\n    checkpoint_training: dsl.PipelineParam=dsl.PipelineParam(name=\"checkpoint_training\", value=\"false\"),\n    checkpoint_evaluation: dsl.PipelineParam=dsl.PipelineParam(name=\"checkpoint_evaluation\", value=\"false\"),\n    workspace_name: dsl.PipelineParam=dsl.PipelineParam(name=\"workspace_name\", value=\"kensert_CNN\"),\n    model_repo: dsl.PipelineParam=dsl.PipelineParam(name=\"model_repo\", value=\"\"),\n):\n\n    ### preprocessing step\n    preprocessing = dsl.ContainerOp(\n        name=\"preprocessing\",\n        image=\"pharmbio/pipelines-kensert-preprocess:test\",\n        arguments=[ \"--model-type\" , model_type, \"--checkpoint\", checkpoint_preprocess],\n        # NOTE: arguments from pipeline not utilized in training script yet\n        container_kwargs={\"image_pull_policy\": \"Always\", \"env\":[V1EnvVar(name=\"WORKFLOW_NAME\",value=workspace_name)]},\n        file_outputs={\"labels\":\"/home/output/bbbc014_labels.npy\"}\n    )\n    ### add resource definitions, pvc mounts etc.\n    preprocessing.apply(mount_pvc(pvc_name=\"external-images-kubeflow-pvc\",volume_name=\"external-images-kubeflow-pv\",volume_mount_path=\"/mnt/data\"))\n    preprocessing.apply(mount_pvc(pvc_name=\"kubeflow-workdir-pvc\",volume_name=\"kubeflow-workdir-pv\",volume_mount_path=\"/mnt/data/workdir\",volume_sub_path=workspace_name))\n    set_resources(memory_req=\"4Gi\", memory_lim=\"8Gi\", cpu_req=\"2\", cpu_lim=\"4\", container=preprocessing.container)\n\n    ### training step\n    training = dsl.ContainerOp(\n        name=\"training\",\n        image=\"pharmbio/pipelines-kensert-training:test\",\n        arguments=[ \"--model-type\" , model_type,  \"--checkpoint\", checkpoint_training],\n        container_kwargs={\"image_pull_policy\": \"Always\", \"env\":[V1EnvVar(name=\"WORKFLOW_NAME\",value=workspace_name)]}\n    )\n    ### set order (after preprocessing), add pvc and resource definitions\n    training.apply(mount_pvc(pvc_name=\"kubeflow-workdir-pvc\",volume_name=\"kubeflow-workdir-pv\",volume_mount_path=\"/mnt/data/workdir\",volume_sub_path=workspace_name))\n    set_resources(memory_req=\"4Gi\", memory_lim=\"8Gi\", cpu_req=\"2\", cpu_lim=\"4\", gpus=\"1\", container=training.container)\n    training.after(preprocessing)\n\n     ### evaluation step\n    evaluation = dsl.ContainerOp(\n        name=\"evaluation\",\n        image=\"pharmbio/pipelines-kensert-evaluation:test\",\n        # NOTE: arguments from pipeline not utilized in training script yet\n        arguments=[ \"--model-type\" , model_type,  \"--checkpoint\", checkpoint_evaluation],\n        container_kwargs={\"image_pull_policy\": \"Always\", \"env\":[V1EnvVar(name=\"WORKFLOW_NAME\",value=workspace_name)]}\n        #file_outputs={\"prediction_accuracy\":\"/home/output/kensert_CNN/predictions_bbbc014\"}\n    )\n    ### set order (after training), add pvc and resource definitions\n    evaluation.apply(mount_pvc(pvc_name=\"kubeflow-workdir-pvc\",volume_name=\"kubeflow-workdir-pv\",volume_mount_path=\"/mnt/data/workdir\",volume_sub_path=workspace_name))\n    set_resources(memory_req=\"1Gi\", memory_lim=\"4Gi\", cpu_req=\"1\", cpu_lim=\"2\", gpus=\"1\", container=evaluation.container)\n    evaluation.after(training)\n\n    ### model building step\n    model_building = dsl.ContainerOp(\n        name=\"model_building\",\n        image=\"pharmbio/pipelines-kensert-building:test\",\n        arguments=[ ],\n        container_kwargs={\"image_pull_policy\": \"Always\", \"env\":[V1EnvVar(name=\"WORKFLOW_NAME\",value=workspace_name), V1EnvVar(name=\"MODEL_REPO\",value=model_repo)]}\n    )\n    ### set order (after evaluation), add secrets, socket (hostpath pv mount), pvc and resource definitions\n    model_building.apply(mount_pvc(pvc_name=\"kubeflow-workdir-pvc\",volume_name=\"kubeflow-workdir-pv\",volume_mount_path=\"/home/models\",volume_sub_path=(str(workspace_name) + \"/models\")))\n    docker_socket_volume = k8s_client.V1Volume(name='dockervol-pv', host_path=k8s_client.V1HostPathVolumeSource(path='/var/run/docker.sock'))\n    docker_credentials_volume = k8s_client.V1Volume(name=\"dockercreds\", secret=k8s_client.V1SecretVolumeSource(secret_name=\"dockercreds\"))\n    model_building.add_pvolumes({\"/var/run/docker.sock\":docker_socket_volume, \"/root/.docker/\":docker_credentials_volume})\n    set_resources(memory_req=\"1Gi\", memory_lim=\"2Gi\", cpu_req=\"1\", cpu_lim=\"2\", container=model_building.container)\n    model_building.after(evaluation)\n\n### build pipeline when executing python script\nif __name__ == '__main__':\n    import kfp.compiler as compiler\n    compiler.Compiler().compile(cnn_workflow, __file__ + '.tar.gz')"
  },
  {
    "repo": "NusretOzates/autonlp-classification-kubeflow",
    "file_path": "pipeline/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/NusretOzates/autonlp-classification-kubeflow/master/pipeline/pipeline.py",
    "content": "\"\"\"\nThis is where magic happens. We define the pipeline using the @dsl.pipeline decorator.\nThe pipeline is composed of 6 steps:\n\n1. Download the dataset\n2. Preprocess the dataset\n3. Split the dataset\n4. Train the model(s)\n5. Evaluate the model(s)\n6. Print the best model\n\"\"\"\n\nfrom typing import NamedTuple\n\nimport kfp\nimport kfp_server_api\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import component\nfrom kfp.v2.dsl import Input, Output, Dataset, Metrics\nfrom kfp.compiler.compiler import Compiler\n\n\n@component(\n    packages_to_install=[\"datasets\"],\n    output_component_file=\"upload_data_component.yaml\",\n)\ndef upload_data(\n        dataset_name: str,\n        dataset_subset: str,\n        dataset_object: Output[Dataset],\n) -> None:\n    \"\"\"Uploads the dataset and preprocesses it\n\n    It will download the dataset from Huggingface.\n    Then it will save the dataset inside Kubeflow's minIO object storage.\n\n    Args:\n        dataset_name: Name of the dataset from Huggingface\n        dataset_subset: Name of the subset if any\n        dataset_object: Output dataset object\n\n    Returns:\n        Nothing\n    \"\"\"\n    from datasets import load_dataset, DatasetDict\n\n    if dataset_subset:\n        dataset: DatasetDict = load_dataset(dataset_name, dataset_subset)\n    else:\n        dataset: DatasetDict = load_dataset(dataset_name)\n\n    dataset.save_to_disk(dataset_object.path)\n\n\n@component(\n    packages_to_install=[\"datasets\", \"transformers\"],\n    output_component_file=\"upload_data_component.yaml\",\n)\ndef preprocess(\n        model_name: str,\n        dataset_object: Input[Dataset],\n        tokenized_dataset_object: Output[Dataset],\n) -> None:\n    \"\"\"Preprocess and save the tokenized dataset\n\n    The preprocessing is:\n\n    - Tokenization\n    - Padding\n    - Truncation\n    - Changing the label column to labels\n\n    Args:\n        model_name: Name of the model from Huggingface\n        dataset_object: Uploaded dataset object\n        tokenized_dataset_object: Output tokenized dataset object\n\n    Returns:\n        Nothing\n    \"\"\"\n\n    from datasets import load_from_disk\n    from transformers import AutoTokenizer\n\n    dataset = load_from_disk(dataset_object.path)\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n    dataset = dataset.map(lambda examples: {\"labels\": examples[\"label\"]}, batched=True)\n\n    dataset = dataset.map(\n        function=lambda examples: tokenizer(\n            examples[\"text\"], truncation=True, padding=\"max_length\"\n        ),\n        batched=True,\n    )\n\n    dataset.save_to_disk(tokenized_dataset_object.path)\n\n\n@component(\n    packages_to_install=[\"datasets\", \"transformers\"],\n    output_component_file=\"split_data_component.yaml\",\n)\ndef split_data(\n        dataset_object: Input[Dataset],\n        training_dataset: Output[Dataset],\n        validation_dataset: Output[Dataset],\n        test_dataset: Output[Dataset],\n) -> None:\n    \"\"\"Splits the dataset into training, validation and test\n\n    Split the dataset into training, validation and test.\n    The split is 80%, 10% and 10% respectively.\n    If the dataset is already split, it will use the split.\n\n    Args:\n        dataset_object: Input dataset object\n        training_dataset: Output training dataset object\n        validation_dataset: Output validation dataset object\n        test_dataset: Output test dataset object\n\n    Returns:\n        Nothing\n    \"\"\"\n    from datasets import load_from_disk, DatasetDict\n    from datasets import Dataset as HFDataset\n\n    dataset: DatasetDict = load_from_disk(dataset_object.path)\n\n    splits = len(dataset)\n\n    # We have only the train split\n    if splits == 1:\n        dataset: DatasetDict = dataset[\"train\"].train_test_split(\n            test_size=0.2, stratify_by_column=\"labels\"\n        )\n\n        train: Dataset = dataset[\"train\"]\n        val = dataset[\"test\"].train_test_split(\n            test_size=0.5, stratify_by_column=\"labels\"\n        )\n        val, test = val[\"train\"], val[\"test\"]\n\n    # We have train and validation (maybe test) split\n    if splits == 2:\n        split_name = \"test\" if \"test\" in dataset else \"validation\"\n\n        train: HFDataset = dataset[\"train\"]\n        val = dataset[split_name].train_test_split(\n            test_size=0.5, stratify_by_column=\"labels\"\n        )\n        val, test = val[\"train\"], val[\"test\"]\n\n    # We have train, validation and test split\n    if splits == 3:\n        train, val, test = dataset[\"train\"], dataset[\"validation\"], dataset[\"test\"]\n\n    if not train or not val or not test:\n        raise ValueError(\"Something went wrong while splitting the dataset\")\n\n    train.save_to_disk(training_dataset.path)\n    val.save_to_disk(validation_dataset.path)\n    test.save_to_disk(test_dataset.path)\n\n\n@component(\n    packages_to_install=[\n        \"datasets\",\n        \"transformers\",\n        \"tensorflow\",\n        \"keras-tuner\",\n        \"scipy\",\n    ],\n    output_component_file=\"training_component.yaml\",\n)\ndef model_training(\n        model_name: str,\n        best_hyperparams: dict,\n        training_dataset: Input[Dataset],\n        validation_dataset: Input[Dataset],\n        test_dataset: Input[Dataset],\n        metric: Output[Metrics],\n) -> NamedTuple(\"Output\", [(\"accuracy\", float), (\"loss\", float), (\"best_hp\", dict)]):\n    \"\"\"Trains the given model with the dataset\n\n    Trains the model with the dataset, if best_hyperparams is an empty dict,\n    this component will do hyperparameter tuning and return the best hyperparameters.\n    Otherwise, it will concatenate train and validation data and train\n    the model.\n\n    Lastly, it will evaluate the data with the test set and return the accuracy and loss.\n\n    Args:\n        model_name: Name of the model from Huggingface\n        best_hyperparams: the best hyperparameter values. If empty, it will do hyperparameter tuning\n        training_dataset: Input training dataset object\n        validation_dataset: Input validation dataset object\n        test_dataset: Input test dataset object\n        metric: Output metric object\n\n    Returns:\n        NamedTuple with the accuracy, loss and best hyperparameters\n    \"\"\"\n\n    from typing import NamedTuple\n\n    from datasets import load_from_disk\n    from datasets import Dataset as HFDataset\n    from transformers import AutoTokenizer, TFAutoModel, DataCollatorWithPadding\n    from transformers.modeling_tf_outputs import TFBaseModelOutput\n\n    import tensorflow as tf\n\n    import keras\n    from keras import Model\n    from keras import Input as KerasInput\n    from keras_tuner import HyperParameters, BayesianOptimization\n    from keras.layers import Dense\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n    train: HFDataset = load_from_disk(training_dataset.path)\n    val: HFDataset = load_from_disk(validation_dataset.path)\n    test: HFDataset = load_from_disk(test_dataset.path)\n\n    num_labels: int = val.features[\"label\"].num_classes\n\n    # For now, this real training code is not activated because it uses too many resources\n    def model_builder(hp: HyperParameters) -> Model:\n\n        input_ids = Input(\n            name=\"input_ids\",\n            shape=tokenizer.init_kwargs[\"model_max_length\"],\n            dtype=\"int32\",\n        )\n        attention_mask = Input(\n            name=\"attention_mask\",\n            shape=tokenizer.init_kwargs[\"model_max_length\"],\n            dtype=\"int32\",\n        )\n\n        # I could just use num_labels but I want to use more dense layers\n        base_model_output: TFBaseModelOutput = TFAutoModel.from_pretrained(model_name)(\n            {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n        )\n\n        last_hidden_state = base_model_output.last_hidden_state\n\n        # Min is 1 because I don't want to deal with 0. Otherwise, I had to open an extra case for 0\n        dense_count = hp.Int(name=\"Dense Layer Count\", min_value=1, max_value=3)\n\n        for i in range(dense_count):\n            neuron_count = hp.Int(\n                name=f\"{i}. Dense Neuron Count\", min_value=8, max_value=256\n            )\n\n            if i == 0:\n                hidden_layer = Dense(neuron_count, activation=\"relu\")(\n                    last_hidden_state[:, :, 0]\n                )\n                continue\n\n            hidden_layer = Dense(neuron_count, activation=\"relu\")(hidden_layer)\n\n        classification_layer = Dense(num_labels, \"softmax\")(hidden_layer)\n\n        model = Model(\n            inputs=[input_ids, attention_mask], outputs=[classification_layer]\n        )\n        model.compile(\n            optimizer=\"adam\",\n            loss=\"sparse_categorical_crossentropy\",\n            metrics=[\"accuracy\"],\n        )\n\n        return model\n\n    def model_builder_small(hp: HyperParameters) -> Model:\n        input_ids = KerasInput(\n            name=\"input_ids\",\n            shape=tokenizer.init_kwargs[\"model_max_length\"],\n            dtype=\"int32\",\n        )\n        attention_mask = KerasInput(\n            name=\"attention_mask\",\n            shape=tokenizer.init_kwargs[\"model_max_length\"],\n            dtype=\"int32\",\n        )\n\n        embedding = keras.layers.Embedding(input_dim=40000, output_dim=64)(input_ids)\n        sentence_embedding = keras.layers.GlobalAveragePooling1D()(embedding)\n\n        dense_count = hp.Int(name=\"Dense Layer Count\", min_value=1, max_value=3)\n\n        for i in range(dense_count):\n            neuron_count = hp.Int(\n                name=f\"{i}. Dense Neuron Count\", min_value=8, max_value=256\n            )\n\n            if i == 0:\n                hidden_layer = Dense(neuron_count, activation=\"relu\")(\n                    sentence_embedding\n                )\n                continue\n\n            hidden_layer = Dense(neuron_count, activation=\"relu\")(hidden_layer)\n\n        classification_layer = Dense(num_labels, \"softmax\")(hidden_layer)\n\n        model = Model(\n            inputs=[input_ids, attention_mask], outputs=[classification_layer]\n        )\n        model.compile(\n            optimizer=\"adam\",\n            loss=\"sparse_categorical_crossentropy\",\n            metrics=[\"accuracy\"],\n        )\n\n        return model\n\n    def hf_to_tf(dataset: HFDataset) -> tf.data.Dataset:\n        \"\"\"Converts HuggingFace Dataset object into a TF Dataset.\n\n        Args:\n            dataset:  HuggingFace Dataset object\n\n        Returns:\n            TF Dataset object\n        \"\"\"\n\n        data_collator = DataCollatorWithPadding(\n            tokenizer=tokenizer, return_tensors=\"tf\"\n        )\n\n        return dataset.to_tf_dataset(\n            columns=[\"input_ids\", \"attention_mask\"],\n            label_cols=[\"labels\"],\n            batch_size=32,\n            collate_fn=data_collator,\n            shuffle=True,\n        )\n\n    tf_train = hf_to_tf(train)\n    tf_val = hf_to_tf(val)\n    tf_test = hf_to_tf(test)\n\n    OutputTuple = NamedTuple(\n        \"Output\", [(\"accuracy\", float), (\"loss\", float), (\"best_hp\", dict)]\n    )\n\n    if not best_hyperparams:\n\n        tuner = BayesianOptimization(\n            model_builder_small, objective=\"val_accuracy\", max_trials=15\n        )\n\n        tuner.search(tf_train, epochs=5, validation_data=tf_val)\n        best_hp: HyperParameters = tuner.get_best_hyperparameters()[0]\n\n        for key, value in best_hp.values.items():\n            metric.log_metric(key, value)\n\n        \"\"\"\n        We cannot send 'HyperParameters' object to the next component so we need to convert it to a dictionary\n        The reason is, we cannot import this object in the type hints.... \n        But this object has get_config() and from_config() methods\n        \"\"\"\n\n        return OutputTuple(0.0, 0.0, best_hp.get_config())\n\n    model = model_builder_small(HyperParameters.from_config(best_hyperparams))\n\n    train_dataset = tf_train.concatenate(tf_val)\n    model.fit(train_dataset, epochs=5)\n\n    result = model.evaluate(tf_test, return_dict=True)\n\n    for key, value in result.items():\n        metric.log_metric(key, value)\n\n    return OutputTuple(result[\"accuracy\"], result[\"loss\"], {})\n\n\n@dsl.component(output_component_file=\"print_best.yaml\")\ndef print_best(all_results: list, metrics: Output[Metrics]) -> None:\n    \"\"\"Prints the best accuracy\n\n    Yes, literally just prints the best accuracy. You can do whatever you want here. Change the code a little,\n    and you can send an email, create a Slack message, etc. or upload the best model to a model registry.\n\n    Args:\n        best_accuracy: Best accuracy\n\n    Returns:\n        None\n    \"\"\"\n    all_results.sort(key=lambda x: x.outputs[\"accuracy\"], reverse=True)\n    metrics.log_metric(\"best_accuracy\", all_results[0].outputs[\"accuracy\"])\n\n\n@dsl.pipeline(\n    name=\"text-classification\",\n)\ndef classification_training_pipeline(\n        dataset_name: str, dataset_subset: str\n) -> None:\n    \"\"\"Pipeline for training a text classification model\n\n    Args:\n        dataset_name: Name of the dataset\n        dataset_subset: Subset of the dataset\n        model_names: Name of the models, separated by commas\n\n    Returns:\n        None\n    \"\"\"\n\n    upload_op = upload_data(dataset_name, dataset_subset)\n\n    all_results = []\n\n    with dsl.ParallelFor([\"?model_names?\"]) as model_name:\n        dataset_path = upload_op.outputs[\"dataset_object\"]\n        preprocess_op = preprocess(model_name, dataset_path)\n\n        tokenized_dataset_path = preprocess_op.outputs[\"tokenized_dataset_object\"]\n        split_op = split_data(tokenized_dataset_path)\n\n        training_dataset = split_op.outputs[\"training_dataset\"]\n        validation_dataset = split_op.outputs[\"validation_dataset\"]\n        test_dataset = split_op.outputs[\"test_dataset\"]\n        hp_tune_op = model_training(\n            model_name=model_name,\n            best_hyperparams={},\n            training_dataset=training_dataset,\n            validation_dataset=validation_dataset,\n            test_dataset=test_dataset,\n        )\n\n        best_hp: dict = hp_tune_op.outputs[\"best_hp\"]\n        training_op = model_training(\n            model_name=model_name,\n            best_hyperparams=best_hp,\n            training_dataset=training_dataset,\n            validation_dataset=validation_dataset,\n            test_dataset=test_dataset,\n        )\n\n        all_results.append(training_op)\n\n\n    print_best(all_results)\n\n\n\n"
  },
  {
    "repo": "NusretOzates/autonlp-classification-kubeflow",
    "file_path": "pipeline/pipeline_runner.py",
    "raw_url": "https://raw.githubusercontent.com/NusretOzates/autonlp-classification-kubeflow/master/pipeline/pipeline_runner.py",
    "content": "import kfp_server_api\nimport kfp\nfrom kfp.compiler.compiler import Compiler\nimport os\nfrom importlib import invalidate_caches\n\n\ndef run_pipeline(\n        dataset_name: str = \"tweet_eval\",\n        dataset_subset: str = \"emotion\",\n        model_names: str = \"google/electra-small-discriminator\",\n) -> kfp_server_api.ApiRun:\n    \"\"\"Runs the pipeline\n\n    Args:\n        dataset_name: Name of the dataset\n        dataset_subset: Subset of the dataset\n        model_name: Name of the model\n\n    Returns:\n        None\n    \"\"\"\n\n    # Connect to KFP, this command is used to connect to the KFP UI:\n    # kubectl port-forward -n kubeflow svc/ml-pipeline-ui 8080:80\n    client = kfp.Client(host=\"http://127.0.0.1:8080/pipeline\")\n\n    models = model_names.split(\",\")\n    models = [f'\\\"{model}\\\"' for model in models]\n    model_names = \",\".join(models)\n\n    # ParallerFor takes static arguments, it is not possible to give it as pipeline parameter so we need to override the\n    # pipeline function code to pass the arguments.\n\n    # Open the pipeline code\n    with open(\"pipeline/pipeline.py\", \"r\") as f:\n        pipeline_yaml = f.read()\n\n    # Replace the template with the actual arguments\n    pipeline_yaml = pipeline_yaml.replace('\"?model_names?\"', model_names)\n\n    # Save the pipeline code\n    with open(\"pipeline/pipeline_modified.py\", \"w\") as f:\n        f.write(pipeline_yaml)\n\n    # Invalidate the cache to make sure the modified pipeline is loaded\n    invalidate_caches()\n\n    # Import the pipeline function\n    from pipeline.pipeline_modified import classification_training_pipeline\n\n    # Compile the pipeline\n    Compiler(mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE).compile(pipeline_func=classification_training_pipeline, package_path=\"pipeline.yaml\")\n\n    # Create an experiment\n    experiment = client.create_experiment(name=\"text-classification\", description=\"Text classification pipeline\")\n\n    # Run the pipeline\n    job = client.run_pipeline(\n        experiment_id=experiment.id,\n        job_name=\"text-classification\",\n        pipeline_package_path=\"pipeline.yaml\",\n        enable_caching=False,\n        params={\n            \"dataset_name\": dataset_name,\n            \"dataset_subset\": dataset_subset\n        },\n    )\n\n    # Delete the modified pipeline code\n    os.remove(\"pipeline/pipeline_modified.py\")\n\n    components = filter(lambda x: x.endswith(\".yaml\"), os.listdir())\n\n    for file in components:\n        os.remove(file)\n\n    return job\n"
  },
  {
    "repo": "sbakiu/ml-kf-pipeline",
    "file_path": "tokenize_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/sbakiu/ml-kf-pipeline/main/tokenize_pipeline.py",
    "content": "import kfp.dsl as dsl\nimport kfp.aws as aws\nimport logging\nimport kfp.compiler as compiler\n\nlogging.basicConfig(level=logging.INFO)\n\nTAG = \"1.0.0\"\nREGISTRY = \"<ID>.dkr.ecr.eu-central-1.amazonaws.com/<registry>\"\n\n\n@dsl.pipeline(\n    name='Pipeline',\n    description='A pipeline for training with Reddit response dataset'\n)\ndef kf_pipeline(\n        input_data='reddit_train.csv',\n):\n    \"\"\"\n    Pipeline\n    \"\"\"\n\n    tokenize_training_step = dsl.ContainerOp(\n        name='tokenize',\n        image=f\"{REGISTRY}/tokenize:{TAG}\",\n        command=\"python\",\n        arguments=[\n            \"-m\",\n            \"src.steps.tokenize.pipeline_step\"\n        ],\n        file_outputs={\"tokenize_location\": \"/tokenized_location.txt\",\n                      \"labels_location\": \"/labels_location.txt\"},\n        pvolumes={}\n    ).apply(aws.use_aws_secret('aws-secret', 'AWS_ACCESS_KEY_ID', 'AWS_SECRET_ACCESS_KEY')) \\\n        .set_image_pull_policy('Always')\n\n    vectorize_training_step = dsl.ContainerOp(\n        name='vectorize',\n        image=f\"{REGISTRY}/tokenize:{TAG}\",\n        command=\"python\",\n        arguments=[\n            \"-m\",\n            \"src.steps.tfidftransformer.pipeline_step\",\n            \"--input-data\", tokenize_training_step.outputs['tokenize_location']\n        ],\n        file_outputs={\"tfidftransformer_location\": \"/vectorizer_location.txt\",\n                      \"tfidfvectors_location\": \"/vectors_location.txt\"},\n        pvolumes={}\n    ).apply(aws.use_aws_secret('aws-secret', 'AWS_ACCESS_KEY_ID', 'AWS_SECRET_ACCESS_KEY')) \\\n        .set_image_pull_policy('Always')\n\n    lr_training_step = dsl.ContainerOp(\n        name='logical regression',\n        image=f\"{REGISTRY}/tokenize:{TAG}\",\n        command=\"python\",\n        arguments=[\n            \"-m\",\n            \"src.steps.lrclassifier.pipeline_step\",\n            \"--labels-data\", tokenize_training_step.outputs['labels_location'],\n            \"--vectors-data\", vectorize_training_step.outputs['tfidfvectors_location']\n        ],\n        file_outputs={\"lr_model_location\": \"/lr_model_location.txt\"},\n        pvolumes={}\n    ).apply(aws.use_aws_secret('aws-secret', 'AWS_ACCESS_KEY_ID', 'AWS_SECRET_ACCESS_KEY')) \\\n        .set_image_pull_policy('Always')\n\n    tokenize_build_step = dsl.ContainerOp(\n        name='Build tokenize Serving',\n        image=f\"{REGISTRY}/kaniko-executor:{TAG}\",\n        arguments=[\n            \"--dockerfile=Dockerfile\",\n            f\"--build-arg=TOKENIZE_MODEL={tokenize_training_step.outputs['tokenize_location']}\",\n            \"--context=dir:///workspace\",\n            f\"--destination={REGISTRY}/tokenizeserving:{TAG}\"\n        ],\n        pvolumes={}\n    ).apply(aws.use_aws_secret('aws-secret', 'AWS_ACCESS_KEY_ID', 'AWS_SECRET_ACCESS_KEY')) \\\n        .set_image_pull_policy('Always') \\\n        .after(lr_training_step)\n\n    vectorize_build_step = dsl.ContainerOp(\n        name='Build vectorize Serving',\n        image=f\"{REGISTRY}/kaniko-executor:{TAG}\",\n        arguments=[\n            \"--dockerfile=Dockerfile\",\n            f\"--build-arg=VECTORIZE_MODEL={vectorize_training_step.outputs['tfidftransformer_location']}\",\n            \"--context=dir:///workspace\",\n            f\"--destination={REGISTRY}/vecotrizeserving:{TAG}\"\n        ],\n        pvolumes={}\n    ).apply(aws.use_aws_secret('aws-secret', 'AWS_ACCESS_KEY_ID', 'AWS_SECRET_ACCESS_KEY')) \\\n        .set_image_pull_policy('Always') \\\n        .after(tokenize_build_step)\n\n    lr_model_build_step = dsl.ContainerOp(\n        name='Build LR Serving',\n        image=f\"{REGISTRY}/kaniko-executor:{TAG}\",\n        arguments=[\n            \"--dockerfile=Dockerfile\",\n            f\"--build-arg=LR_MODEL={lr_training_step.outputs['lr_model_location']}\",\n            \"--context=dir:///workspace\",\n            f\"--destination={REGISTRY}/lrserving:{TAG}\"\n        ],\n        pvolumes={}\n    ).apply(aws.use_aws_secret('aws-secret', 'AWS_ACCESS_KEY_ID', 'AWS_SECRET_ACCESS_KEY')) \\\n        .set_image_pull_policy('Always') \\\n        .after(vectorize_build_step)\n\n\nif __name__ == '__main__':\n\n    pipeline_file_path = __file__ + '.yaml'\n    compiler.Compiler().compile(kf_pipeline, pipeline_file_path)\n\n"
  },
  {
    "repo": "Ark-kun/kfp_sdk_components",
    "file_path": "components/_python_op.py",
    "raw_url": "https://raw.githubusercontent.com/Ark-kun/kfp_sdk_components/master/components/_python_op.py",
    "content": "# Copyright 2018 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n__all__ = [\n    'create_component_from_func',\n    'func_to_container_op',\n    'func_to_component_text',\n    'default_base_image_or_builder',\n    'InputPath',\n    'InputTextFile',\n    'InputBinaryFile',\n    'OutputPath',\n    'OutputTextFile',\n    'OutputBinaryFile',\n]\n\nfrom ._yaml_utils import dump_yaml\nfrom ._components import _create_task_factory_from_component_spec\nfrom ._data_passing import serialize_value, get_deserializer_code_for_type_struct, get_serializer_func_for_type_struct, get_canonical_type_struct_for_type\nfrom ._naming import _make_name_unique_by_adding_index\nfrom .structures import *\nfrom . import _structures as structures\n\nimport inspect\nfrom pathlib import Path\nimport textwrap\nfrom typing import Callable, List, Mapping, Optional, TypeVar\nimport warnings\n\nimport docstring_parser\n\nT = TypeVar('T')\n\n\n# InputPath(list) or InputPath('JsonObject')\n\nclass InputPath:\n    '''When creating component from function, :class:`.InputPath` should be used as function parameter annotation to tell the system to pass the *data file path* to the function instead of passing the actual data.'''\n    def __init__(self, type=None):\n        self.type = type\n\n\nclass InputTextFile:\n    '''When creating component from function, :class:`.InputTextFile` should be used as function parameter annotation to tell the system to pass the *text data stream* object (`io.TextIOWrapper`) to the function instead of passing the actual data.'''\n    def __init__(self, type=None):\n        self.type = type\n\n\nclass InputBinaryFile:\n    '''When creating component from function, :class:`.InputBinaryFile` should be used as function parameter annotation to tell the system to pass the *binary data stream* object (`io.BytesIO`) to the function instead of passing the actual data.'''\n    def __init__(self, type=None):\n        self.type = type\n\n\nclass OutputPath:\n    '''When creating component from function, :class:`.OutputPath` should be used as function parameter annotation to tell the system that the function wants to output data by writing it into a file with the given path instead of returning the data from the function.'''\n    def __init__(self, type=None):\n        self.type = type\n\n\nclass OutputTextFile:\n    '''When creating component from function, :class:`.OutputTextFile` should be used as function parameter annotation to tell the system that the function wants to output data by writing it into a given text file stream (`io.TextIOWrapper`) instead of returning the data from the function.'''\n    def __init__(self, type=None):\n        self.type = type\n\n\nclass OutputBinaryFile:\n    '''When creating component from function, :class:`.OutputBinaryFile` should be used as function parameter annotation to tell the system that the function wants to output data by writing it into a given binary file stream (:code:`io.BytesIO`) instead of returning the data from the function.'''\n    def __init__(self, type=None):\n        self.type = type\n\n\ndef _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\n\ndef _parent_dirs_maker_that_returns_open_file(mode: str, encoding: str = None):\n    def make_parent_dirs_and_return_path(file_path: str):\n        import os\n        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n        return open(file_path, mode=mode, encoding=encoding)\n    return make_parent_dirs_and_return_path\n\n\ndefault_base_image_or_builder='python:3.7'\n\n\ndef _python_function_name_to_component_name(name):\n    import re\n    name_with_spaces = re.sub(' +', ' ', name.replace('_', ' ')).strip(' ')\n    return name_with_spaces[0].upper() + name_with_spaces[1:]\n\n\ndef _capture_function_code_using_cloudpickle(func, modules_to_capture: List[str] = None) -> str:\n    import base64\n    import sys\n    import cloudpickle\n    import pickle\n\n    if modules_to_capture is None:\n        modules_to_capture = [func.__module__]\n\n    # Hack to force cloudpickle to capture the whole function instead of just referencing the code file. See https://github.com/cloudpipe/cloudpickle/blob/74d69d759185edaeeac7bdcb7015cfc0c652f204/cloudpickle/cloudpickle.py#L490\n    old_modules = {}\n    old_sig = getattr(func, '__signature__', None)\n    try: # Try is needed to restore the state if something goes wrong\n        for module_name in modules_to_capture:\n            if module_name in sys.modules:\n                old_modules[module_name] = sys.modules.pop(module_name)\n        # Hack to prevent cloudpickle from trying to pickle generic types that might be present in the signature. See https://github.com/cloudpipe/cloudpickle/issues/196 \n        # Currently the __signature__ is only set by Airflow components as a means to spoof/pass the function signature to _func_to_component_spec\n        if hasattr(func, '__signature__'):\n            del func.__signature__\n        func_pickle = base64.b64encode(cloudpickle.dumps(func, pickle.DEFAULT_PROTOCOL))\n    finally:\n        sys.modules.update(old_modules)\n        if old_sig:\n            func.__signature__ = old_sig\n\n    function_loading_code = '''\\\nimport sys\ntry:\n    import cloudpickle as _cloudpickle\nexcept ImportError:\n    import subprocess\n    try:\n        print(\"cloudpickle is not installed. Installing it globally\", file=sys.stderr)\n        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"cloudpickle==1.1.1\", \"--quiet\"], env={\"PIP_DISABLE_PIP_VERSION_CHECK\": \"1\"}, check=True)\n        print(\"Installed cloudpickle globally\", file=sys.stderr)\n    except:\n        print(\"Failed to install cloudpickle globally. Installing for the current user.\", file=sys.stderr)\n        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"cloudpickle==1.1.1\", \"--user\", \"--quiet\"], env={\"PIP_DISABLE_PIP_VERSION_CHECK\": \"1\"}, check=True)\n        print(\"Installed cloudpickle for the current user\", file=sys.stderr)\n        # Enable loading from user-installed package directory. Python does not add it to sys.path if it was empty at start. Running pip does not refresh `sys.path`.\n        import site\n        sys.path.append(site.getusersitepackages())\n    import cloudpickle as _cloudpickle\n    print(\"cloudpickle loaded successfully after installing.\", file=sys.stderr)\n''' + '''\npickler_python_version = {pickler_python_version}\ncurrent_python_version = tuple(sys.version_info)\nif (\n    current_python_version[0] != pickler_python_version[0] or\n    current_python_version[1] < pickler_python_version[1] or\n    current_python_version[0] == 3 and ((pickler_python_version[1] < 6) != (current_python_version[1] < 6))\n    ):\n    raise RuntimeError(\"Incompatible python versions: \" + str(current_python_version) + \" instead of \" + str(pickler_python_version))\n\nif current_python_version != pickler_python_version:\n    print(\"Warning!: Different python versions. The code may crash! Current environment python version: \" + str(current_python_version) + \". Component code python version: \" + str(pickler_python_version), file=sys.stderr)\n\nimport base64\nimport pickle\n\n{func_name} = pickle.loads(base64.b64decode({func_pickle}))\n'''.format(\n        func_name=func.__name__,\n        func_pickle=repr(func_pickle),\n        pickler_python_version=repr(tuple(sys.version_info)),\n    )\n\n    return function_loading_code\n\n\ndef strip_type_hints(source_code: str) -> str:\n    try:\n        return _strip_type_hints_using_lib2to3(source_code)\n    except Exception as ex:\n        print('Error when stripping type annotations: ' + str(ex))\n\n    try:\n        return _strip_type_hints_using_strip_hints(source_code)\n    except Exception as ex:\n        print('Error when stripping type annotations: ' + str(ex))\n\n    return source_code\n\n\ndef _strip_type_hints_using_strip_hints(source_code: str) -> str:\n    from strip_hints import strip_string_to_string\n\n    # Workaround for https://github.com/abarker/strip-hints/issues/4 , https://bugs.python.org/issue35107\n    # I could not repro it though\n    if source_code[-1] != '\\n':\n        source_code += '\\n'\n\n    return strip_string_to_string(source_code, to_empty=True)\n\n\ndef _strip_type_hints_using_lib2to3(source_code: str) -> str:\n    \"\"\"Strips type annotations from the function definitions in the provided source code.\"\"\"\n\n    # Using the standard lib2to3 library to strip type annotations.\n    # Switch to another library like strip-hints if issues are found.\n    from lib2to3 import fixer_base, refactor, fixer_util\n\n    class StripAnnotations(fixer_base.BaseFix):\n        PATTERN = r'''\n            typed_func_parameter=tname\n            |\n            typed_func_return_value=funcdef< any+ '->' any+ >\n        '''\n\n        def transform(self, node, results):\n            if 'typed_func_parameter' in results:\n                # Delete the annotation part of the function parameter declaration\n                del node.children[1:]\n            elif 'typed_func_return_value' in results:\n                # Delete the return annotation part of the function declaration\n                del node.children[-4:-2]\n            return node\n\n    class Refactor(refactor.RefactoringTool):\n        def __init__(self, fixers):\n            self._fixers = [cls(None, None) for cls in fixers]\n            super().__init__(None, {'print_function': True})\n\n        def get_fixers(self):\n            return self._fixers, []\n\n    stripped_code = str(Refactor([StripAnnotations]).refactor_string(source_code, ''))\n    return stripped_code\n\n\ndef _capture_function_code_using_source_copy(func) -> str:\t\n    func_code = inspect.getsource(func)\n\n    #Function might be defined in some indented scope (e.g. in another function).\n    #We need to handle this and properly dedent the function source code\n    func_code = textwrap.dedent(func_code)\n    func_code_lines = func_code.split('\\n')\n\n    # Removing possible decorators (can be multiline) until the function definition is found\n    while func_code_lines and not func_code_lines[0].startswith('def '):\n        del func_code_lines[0]\n\n    if not func_code_lines:\n        raise ValueError('Failed to dedent and clean up the source of function \"{}\". It is probably not properly indented.'.format(func.__name__))\n\n    func_code = '\\n'.join(func_code_lines)\n\n    # Stripping type annotations to prevent import errors.\n    # The most common cases are InputPath/OutputPath and typing.NamedTuple annotations\n    func_code = strip_type_hints(func_code)\n\n    return func_code\n\n\ndef _extract_component_interface(func: Callable) -> ComponentSpec:\n    single_output_name_const = 'Output'\n\n    signature = inspect.signature(func)\n    parameters = list(signature.parameters.values())\n\n    parsed_docstring = docstring_parser.parse(inspect.getdoc(func))\n    doc_dict = {p.arg_name: p.description for p in parsed_docstring.params}\n\n    inputs = []\n    outputs = []\n\n    def annotation_to_type_struct(annotation):\n        if not annotation or annotation == inspect.Parameter.empty:\n            return None\n        if hasattr(annotation, 'to_dict'):\n            annotation = annotation.to_dict()\n        if isinstance(annotation, dict):\n            return annotation\n        if isinstance(annotation, type):\n            type_struct = get_canonical_type_struct_for_type(annotation)\n            if type_struct:\n                return type_struct\n            type_name = str(annotation.__name__)\n        elif hasattr(annotation, '__forward_arg__'): # Handling typing.ForwardRef('Type_name') (the name was _ForwardRef in python 3.5-3.6)\n            type_name = str(annotation.__forward_arg__)\n        else:\n            type_name = str(annotation)\n\n        # It's also possible to get the converter by type name\n        type_struct = get_canonical_type_struct_for_type(type_name)\n        if type_struct:\n            return type_struct\n        return type_name\n\n    input_names = set()\n    output_names = set()\n    for parameter in parameters:\n        parameter_annotation = parameter.annotation\n        passing_style = None\n        io_name = parameter.name\n        if isinstance(parameter_annotation, (InputPath, InputTextFile, InputBinaryFile, OutputPath, OutputTextFile, OutputBinaryFile)):\n            passing_style = type(parameter_annotation)\n            parameter_annotation = parameter_annotation.type\n            if parameter.default is not inspect.Parameter.empty and not (passing_style == InputPath and parameter.default is None):\n                raise ValueError('Path inputs only support default values of None. Default values for outputs are not supported.')\n            # Removing the \"_path\" and \"_file\" suffixes from the input/output names as the argument passed to the component needs to be the data itself, not local file path.\n            # Problem: When accepting file inputs (outputs), the function inside the component receives file paths (or file streams), so it's natural to call the function parameter \"something_file_path\" (e.g. model_file_path or number_file_path).\n            # But from the outside perspective, there are no files or paths - the actual data objects (or references to them) are passed in.\n            # It looks very strange when argument passing code looks like this: `component(number_file_path=42)`. This looks like an error since 42 is not a path. It's not even a string.\n            # It's much more natural to strip the names of file inputs and outputs of \"_file\" or \"_path\" suffixes. Then the argument passing code will look natural: \"component(number=42)\".\n            if isinstance(parameter.annotation, (InputPath, OutputPath)) and io_name.endswith('_path'):\n                io_name = io_name[0:-len('_path')]\n            if io_name.endswith('_file'):\n                io_name = io_name[0:-len('_file')]\n        type_struct = annotation_to_type_struct(parameter_annotation)\n        #TODO: Humanize the input/output names\n\n        if isinstance(parameter.annotation, (OutputPath, OutputTextFile, OutputBinaryFile)):\n            io_name = _make_name_unique_by_adding_index(io_name, output_names, '_')\n            output_names.add(io_name)\n            output_spec = OutputSpec(\n                name=io_name,\n                type=type_struct,\n                description=doc_dict.get(parameter.name)\n            )\n            output_spec._passing_style = passing_style\n            output_spec._parameter_name = parameter.name\n            outputs.append(output_spec)\n        else:\n            io_name = _make_name_unique_by_adding_index(io_name, input_names, '_')\n            input_names.add(io_name)\n            input_spec = InputSpec(\n                name=io_name,\n                type=type_struct,\n                description=doc_dict.get(parameter.name)\n            )\n            if parameter.default is not inspect.Parameter.empty:\n                input_spec.optional = True\n                if parameter.default is not None:\n                    outer_type_name = list(type_struct.keys())[0] if isinstance(type_struct, dict) else type_struct\n                    try:\n                        input_spec.default = serialize_value(parameter.default, outer_type_name)\n                    except Exception as ex:\n                        warnings.warn('Could not serialize the default value of the parameter \"{}\". {}'.format(parameter.name, ex))\n            input_spec._passing_style = passing_style\n            input_spec._parameter_name = parameter.name\n            inputs.append(input_spec)\n\n    #Analyzing the return type annotations.\n    return_ann = signature.return_annotation\n    if hasattr(return_ann, '_fields'): #NamedTuple\n        # Getting field type annotations.\n        # __annotations__ does not exist in python 3.5 and earlier\n        # _field_types does not exist in python 3.9 and later\n        field_annotations = getattr(return_ann, '__annotations__', None) or getattr(return_ann, '_field_types', None)\n        for field_name in return_ann._fields:\n            type_struct = None\n            if field_annotations:\n                type_struct = annotation_to_type_struct(field_annotations.get(field_name, None))\n\n            output_name = _make_name_unique_by_adding_index(field_name, output_names, '_')\n            output_names.add(output_name)\n            output_spec = OutputSpec(\n                name=output_name,\n                type=type_struct,\n            )\n            output_spec._passing_style = None\n            output_spec._return_tuple_field_name = field_name\n            outputs.append(output_spec)\n    # Deprecated dict-based way of declaring multiple outputs. Was only used by the @component decorator\n    elif isinstance(return_ann, dict):\n        warnings.warn(\n            \"The ability to specify multiple outputs using the dict syntax has been deprecated.\"\n            \"It will be removed soon after release 0.1.32.\"\n            \"Please use typing.NamedTuple to declare multiple outputs.\"\n        )\n        for output_name, output_type_annotation in return_ann.items():\n            output_type_struct = annotation_to_type_struct(output_type_annotation)\n            output_spec = OutputSpec(\n                name=output_name,\n                type=output_type_struct,\n            )\n            outputs.append(output_spec)\n    elif signature.return_annotation is not None and signature.return_annotation != inspect.Parameter.empty:\n        output_name = _make_name_unique_by_adding_index(single_output_name_const, output_names, '_') # Fixes exotic, but possible collision: `def func(output_path: OutputPath()) -> str: ...`\n        output_names.add(output_name)\n        type_struct = annotation_to_type_struct(signature.return_annotation)\n        output_spec = OutputSpec(\n            name=output_name,\n            type=type_struct,\n        )\n        output_spec._passing_style = None\n        outputs.append(output_spec)\n\n    # Component name and description are derived from the function's name and docstring.\n    # The name can be overridden by setting setting func.__name__ attribute (of the legacy func._component_human_name attribute).\n    # The description can be overridden by setting the func.__doc__ attribute (or the legacy func._component_description attribute).\n    component_name = getattr(func, '_component_human_name', None) or _python_function_name_to_component_name(func.__name__)\n    description = getattr(func, '_component_description', None) or parsed_docstring.short_description\n    if description:\n        description = description.strip()\n\n    component_spec = ComponentSpec(\n        name=component_name,\n        description=description,\n        inputs=inputs if inputs else None,\n        outputs=outputs if outputs else None,\n    )\n    return component_spec\n\n\ndef _func_to_component_spec(func, extra_code='', base_image : str = None, packages_to_install: List[str] = None, modules_to_capture: List[str] = None, use_code_pickling=False) -> ComponentSpec:\n    '''Takes a self-contained python function and converts it to component.\n\n    Args:\n        func: Required. The function to be converted\n        base_image: Optional. Docker image to be used as a base image for the python component. Must have python 3.5+ installed. Default is python:3.7\n                    Note: The image can also be specified by decorating the function with the @python_component decorator. If different base images are explicitly specified in both places, an error is raised.\n        extra_code: Optional. Python source code that gets placed before the function code. Can be used as workaround to define types used in function signature.\n        packages_to_install: Optional. List of [versioned] python packages to pip install before executing the user function.\n        modules_to_capture: Optional. List of module names that will be captured (instead of just referencing) during the dependency scan. By default the :code:`func.__module__` is captured.\n        use_code_pickling: Specifies whether the function code should be captured using pickling as opposed to source code manipulation. Pickling has better support for capturing dependencies, but is sensitive to version mismatch between python in component creation environment and runtime image.\n\n    Returns:\n        A :py:class:`kfp.components.structures.ComponentSpec` instance.\n    '''\n    decorator_base_image = getattr(func, '_component_base_image', None)\n    if decorator_base_image is not None:\n        if base_image is not None and decorator_base_image != base_image:\n            raise ValueError('base_image ({}) conflicts with the decorator-specified base image metadata ({})'.format(base_image, decorator_base_image))\n        else:\n            base_image = decorator_base_image\n    else:\n        if base_image is None:\n            base_image = default_base_image_or_builder\n            if isinstance(base_image, Callable):\n                base_image = base_image()\n\n    packages_to_install = packages_to_install or []\n\n    component_spec = _extract_component_interface(func)\n\n    component_inputs = component_spec.inputs or []\n    component_outputs = component_spec.outputs or []\n\n    arguments = []\n    arguments.extend(InputValuePlaceholder(input.name) for input in component_inputs)\n    arguments.extend(OutputPathPlaceholder(output.name) for output in component_outputs)\n\n    if use_code_pickling:\n        func_code = _capture_function_code_using_cloudpickle(func, modules_to_capture)\n        # pip startup is quite slow. TODO: Remove the special cloudpickle installation code in favor of the the following line once a way to speed up pip startup is discovered.\n        #packages_to_install.append('cloudpickle==1.1.1')\n    else:\n        func_code = _capture_function_code_using_source_copy(func)\n\n    definitions = set()\n    def get_deserializer_and_register_definitions(type_name):\n        deserializer_code = get_deserializer_code_for_type_struct(type_name)\n        if deserializer_code:\n            (deserializer_code_str, definition_str) = deserializer_code\n            if definition_str:\n                definitions.add(definition_str)\n            return deserializer_code_str\n        return 'str'\n\n    pre_func_definitions = set()\n    def get_argparse_type_for_input_file(passing_style):\n        if passing_style is None:\n            return None\n\n        if passing_style is InputPath:\n            return 'str'\n        elif passing_style is InputTextFile:\n            return \"argparse.FileType('rt')\"\n        elif passing_style is InputBinaryFile:\n            return \"argparse.FileType('rb')\"\n        # For Output* we cannot use the build-in argparse.FileType objects since they do not create parent directories.\n        elif passing_style is OutputPath:\n            # ~= return 'str'\n            pre_func_definitions.add(inspect.getsource(_make_parent_dirs_and_return_path))\n            return _make_parent_dirs_and_return_path.__name__\n        elif passing_style is OutputTextFile:\n            # ~= return \"argparse.FileType('wt')\"\n            pre_func_definitions.add(inspect.getsource(_parent_dirs_maker_that_returns_open_file))\n            return _parent_dirs_maker_that_returns_open_file.__name__ + \"('wt')\"\n        elif passing_style is OutputBinaryFile:\n            # ~= return \"argparse.FileType('wb')\"\n            pre_func_definitions.add(inspect.getsource(_parent_dirs_maker_that_returns_open_file))\n            return _parent_dirs_maker_that_returns_open_file.__name__ + \"('wb')\"\n        raise NotImplementedError('Unexpected data passing style: \"{}\".'.format(str(passing_style)))\n\n    def get_serializer_and_register_definitions(type_name) -> str:\n        serializer_func = get_serializer_func_for_type_struct(type_name)\n        if serializer_func:\n            # If serializer is not part of the standard python library, then include its code in the generated program\n            if hasattr(serializer_func, '__module__') and not _module_is_builtin_or_standard(serializer_func.__module__):\n                import inspect\n                serializer_code_str = inspect.getsource(serializer_func)\n                definitions.add(serializer_code_str)\n            return serializer_func.__name__\n        return 'str'\n\n    arg_parse_code_lines = [\n        'import argparse',\n        '_parser = argparse.ArgumentParser(prog={prog_repr}, description={description_repr})'.format(\n            prog_repr=repr(component_spec.name or ''),\n            description_repr=repr(component_spec.description or ''),\n        ),\n    ]\n    outputs_passed_through_func_return_tuple = [output for output in component_outputs if output._passing_style is None]\n    file_outputs_passed_using_func_parameters = [output for output in component_outputs if output._passing_style is not None]\n    arguments = []\n    for input in component_inputs + file_outputs_passed_using_func_parameters:\n        param_flag = \"--\" + input.name.replace(\"_\", \"-\")\n        is_required = isinstance(input, OutputSpec) or not input.optional\n        line = '_parser.add_argument(\"{param_flag}\", dest=\"{param_var}\", type={param_type}, required={is_required}, default=argparse.SUPPRESS)'.format(\n            param_flag=param_flag,\n            param_var=input._parameter_name, # Not input.name, since the inputs could have been renamed\n            param_type=get_argparse_type_for_input_file(input._passing_style) or get_deserializer_and_register_definitions(input.type),\n            is_required=str(is_required),\n        )\n        arg_parse_code_lines.append(line)\n\n        if input._passing_style in [InputPath, InputTextFile, InputBinaryFile]:\n            arguments_for_input = [param_flag, InputPathPlaceholder(input.name)]\n        elif input._passing_style in [OutputPath, OutputTextFile, OutputBinaryFile]:\n            arguments_for_input = [param_flag, OutputPathPlaceholder(input.name)]\n        else:\n            arguments_for_input = [param_flag, InputValuePlaceholder(input.name)]\n\n        if is_required:\n            arguments.extend(arguments_for_input)\n        else:\n            arguments.append(\n                IfPlaceholder(\n                    IfPlaceholderStructure(\n                        condition=IsPresentPlaceholder(input.name),\n                        then_value=arguments_for_input,\n                    )\n                )\n            )\n\n    if outputs_passed_through_func_return_tuple:\n        param_flag=\"----output-paths\"\n        output_param_var=\"_output_paths\"\n        line = '_parser.add_argument(\"{param_flag}\", dest=\"{param_var}\", type=str, nargs={nargs})'.format(\n            param_flag=param_flag,\n            param_var=output_param_var,\n            nargs=len(outputs_passed_through_func_return_tuple),\n        )\n        arg_parse_code_lines.append(line)\n        arguments.append(param_flag)\n        arguments.extend(OutputPathPlaceholder(output.name) for output in outputs_passed_through_func_return_tuple)\n\n    output_serialization_expression_strings = []\n    for output in outputs_passed_through_func_return_tuple:\n        serializer_call_str = get_serializer_and_register_definitions(output.type)\n        output_serialization_expression_strings.append(serializer_call_str)\n\n    pre_func_code = '\\n'.join(list(pre_func_definitions))\n\n    arg_parse_code_lines = sorted(list(definitions)) + arg_parse_code_lines\n\n    arg_parse_code_lines.append(\n        '_parsed_args = vars(_parser.parse_args())',\n    )\n    if outputs_passed_through_func_return_tuple:\n        arg_parse_code_lines.append(\n            '_output_files = _parsed_args.pop(\"_output_paths\", [])',\n        )\n\n    # Putting singular return values in a list to be \"zipped\" with the serializers and output paths\n    outputs_to_list_code = ''\n    return_ann = inspect.signature(func).return_annotation\n    if ( # The return type is singular, not sequence\n        return_ann is not None\n        and return_ann != inspect.Parameter.empty\n        and not isinstance(return_ann, dict)\n        and not hasattr(return_ann, '_fields') # namedtuple\n    ):\n        outputs_to_list_code = '_outputs = [_outputs]'\n\n    output_serialization_code = ''.join('    {},\\n'.format(s) for s in output_serialization_expression_strings)\n\n    full_output_handling_code = '''\n\n{outputs_to_list_code}\n\n_output_serializers = [\n{output_serialization_code}\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except OSError:\n        pass\n    with open(output_file, 'w') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n'''.format(\n        output_serialization_code=output_serialization_code,\n        outputs_to_list_code=outputs_to_list_code,\n    )\n\n    full_source = \\\n'''\\\n{pre_func_code}\n\n{extra_code}\n\n{func_code}\n\n{arg_parse_code}\n\n_outputs = {func_name}(**_parsed_args)\n'''.format(\n        func_name=func.__name__,\n        func_code=func_code,\n        pre_func_code=pre_func_code,\n        extra_code=extra_code,\n        arg_parse_code='\\n'.join(arg_parse_code_lines),\n    )\n\n    if outputs_passed_through_func_return_tuple:\n        full_source += full_output_handling_code\n\n    #Removing consecutive blank lines\n    import re\n    full_source = re.sub('\\n\\n\\n+', '\\n\\n', full_source).strip('\\n') + '\\n'\n\n    package_preinstallation_command = []\n    if packages_to_install:\n        package_install_command_line = 'PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location {}'.format(' '.join([repr(str(package)) for package in packages_to_install]))\n        package_preinstallation_command = ['sh', '-c', '({pip_install} || {pip_install} --user) && \"$0\" \"$@\"'.format(pip_install=package_install_command_line)]\n\n    component_spec.implementation=ContainerImplementation(\n        container=ContainerSpec(\n            image=base_image,\n            command=package_preinstallation_command + [\n                'sh',\n                '-ec',\n                # Writing the program code to a file.\n                # This is needed for Python to show stack traces and for `inspect.getsource` to work (used by PyTorch JIT and this module for example).\n                textwrap.dedent('''\\\n                    program_path=$(mktemp)\n                    printf \"%s\" \"$0\" > \"$program_path\"\n                    python3 -u \"$program_path\" \"$@\"\n                '''),\n                full_source,\n            ],\n            args=arguments,\n        )\n    )\n\n    return component_spec\n\n\ndef _func_to_component_dict(func, extra_code='', base_image: str = None, packages_to_install: List[str] = None, modules_to_capture: List[str] = None, use_code_pickling=False):\n    return _func_to_component_spec(\n        func=func,\n        extra_code=extra_code,\n        base_image=base_image,\n        packages_to_install=packages_to_install,\n        modules_to_capture=modules_to_capture,\n        use_code_pickling=use_code_pickling,\n    ).to_dict()\n\n\ndef func_to_component_text(func, extra_code='', base_image: str = None, packages_to_install: List[str] = None, modules_to_capture: List[str] = None, use_code_pickling=False):\n    '''Converts a Python function to a component definition and returns its textual representation.\n\n    Function docstring is used as component description. Argument and return annotations are used as component input/output types.\n\n    To declare a function with multiple return values, use the NamedTuple return annotation syntax::\n\n        from typing import NamedTuple\n        def add_multiply_two_numbers(a: float, b: float) -> NamedTuple('DummyName', [('sum', float), ('product', float)]):\n            \"\"\"Returns sum and product of two arguments\"\"\"\n            return (a + b, a * b)\n\n    Args:\n        func: The python function to convert\n        base_image: Optional. Specify a custom Docker container image to use in the component. For lightweight components, the image needs to have python 3.5+. Default is python:3.7\n        extra_code: Optional. Extra code to add before the function code. Can be used as workaround to define types used in function signature.\n        packages_to_install: Optional. List of [versioned] python packages to pip install before executing the user function.\n        modules_to_capture: Optional. List of module names that will be captured (instead of just referencing) during the dependency scan. By default the :code:`func.__module__` is captured. The actual algorithm: Starting with the initial function, start traversing dependencies. If the dependency.__module__ is in the modules_to_capture list then it's captured and it's dependencies are traversed. Otherwise the dependency is only referenced instead of capturing and its dependencies are not traversed.\n        use_code_pickling: Specifies whether the function code should be captured using pickling as opposed to source code manipulation. Pickling has better support for capturing dependencies, but is sensitive to version mismatch between python in component creation environment and runtime image.\n    \n    Returns:\n        Textual representation of a component definition\n    '''\n    component_dict = _func_to_component_dict(\n        func=func,\n        extra_code=extra_code,\n        base_image=base_image,\n        packages_to_install=packages_to_install,\n        modules_to_capture=modules_to_capture,\n        use_code_pickling=use_code_pickling,\n    )\n    return dump_yaml(component_dict)\n\n\ndef func_to_component_file(func, output_component_file, base_image: str = None, extra_code='', packages_to_install: List[str] = None, modules_to_capture: List[str] = None, use_code_pickling=False) -> None:\n    '''Converts a Python function to a component definition and writes it to a file.\n\n    Function docstring is used as component description. Argument and return annotations are used as component input/output types.\n\n    To declare a function with multiple return values, use the NamedTuple return annotation syntax::\n\n        from typing import NamedTuple\n        def add_multiply_two_numbers(a: float, b: float) -> NamedTuple('DummyName', [('sum', float), ('product', float)]):\n            \"\"\"Returns sum and product of two arguments\"\"\"\n            return (a + b, a * b)\n\n    Args:\n        func: The python function to convert\n        output_component_file: Write a component definition to a local file. Can be used for sharing.\n        base_image: Optional. Specify a custom Docker container image to use in the component. For lightweight components, the image needs to have python 3.5+. Default is tensorflow/tensorflow:1.13.2-py3\n        extra_code: Optional. Extra code to add before the function code. Can be used as workaround to define types used in function signature.\n        packages_to_install: Optional. List of [versioned] python packages to pip install before executing the user function.\n        modules_to_capture: Optional. List of module names that will be captured (instead of just referencing) during the dependency scan. By default the :code:`func.__module__` is captured. The actual algorithm: Starting with the initial function, start traversing dependencies. If the :code:`dependency.__module__` is in the :code:`modules_to_capture` list then it's captured and it's dependencies are traversed. Otherwise the dependency is only referenced instead of capturing and its dependencies are not traversed.\n        use_code_pickling: Specifies whether the function code should be captured using pickling as opposed to source code manipulation. Pickling has better support for capturing dependencies, but is sensitive to version mismatch between python in component creation environment and runtime image.\n    '''\n\n    component_yaml = func_to_component_text(\n        func=func,\n        extra_code=extra_code,\n        base_image=base_image,\n        packages_to_install=packages_to_install,\n        modules_to_capture=modules_to_capture,\n        use_code_pickling=use_code_pickling,\n    )\n    \n    Path(output_component_file).write_text(component_yaml)\n\n\ndef func_to_container_op(\n    func: Callable,\n    output_component_file: Optional[str] = None,\n    base_image: Optional[str] = None,\n    extra_code: Optional[str] = '',\n    packages_to_install: List[str] = None,\n    modules_to_capture: List[str] = None,\n    use_code_pickling: bool = False,\n    annotations: Optional[Mapping[str, str]] = None,\n):\n    '''Converts a Python function to a component and returns a task\n      (:class:`kfp.dsl.ContainerOp`) factory.\n\n    Function docstring is used as component description. Argument and return annotations are used as component input/output types.\n\n    To declare a function with multiple return values, use the :code:`NamedTuple` return annotation syntax::\n\n        from typing import NamedTuple\n        def add_multiply_two_numbers(a: float, b: float) -> NamedTuple('DummyName', [('sum', float), ('product', float)]):\n            \"\"\"Returns sum and product of two arguments\"\"\"\n            return (a + b, a * b)\n\n    Args:\n        func: The python function to convert\n        base_image: Optional. Specify a custom Docker container image to use in the component. For lightweight components, the image needs to have python 3.5+. Default is tensorflow/tensorflow:1.13.2-py3\n        output_component_file: Optional. Write a component definition to a local file. Can be used for sharing.\n        extra_code: Optional. Extra code to add before the function code. Can be used as workaround to define types used in function signature.\n        packages_to_install: Optional. List of [versioned] python packages to pip install before executing the user function.\n        modules_to_capture: Optional. List of module names that will be captured (instead of just referencing) during the dependency scan. By default the :code:`func.__module__` is captured. The actual algorithm: Starting with the initial function, start traversing dependencies. If the :code:`dependency.__module__` is in the :code:`modules_to_capture` list then it's captured and it's dependencies are traversed. Otherwise the dependency is only referenced instead of capturing and its dependencies are not traversed.\n        use_code_pickling: Specifies whether the function code should be captured using pickling as opposed to source code manipulation. Pickling has better support for capturing dependencies, but is sensitive to version mismatch between python in component creation environment and runtime image.\n        annotations: Optional. Allows adding arbitrary key-value data to the component specification.\n\n    Returns:\n        A factory function with a strongly-typed signature taken from the python function.\n        Once called with the required arguments, the factory constructs a pipeline task instance (:class:`kfp.dsl.ContainerOp`) that can run the original function in a container.\n    '''\n\n    component_spec = _func_to_component_spec(\n        func=func,\n        extra_code=extra_code,\n        base_image=base_image,\n        packages_to_install=packages_to_install,\n        modules_to_capture=modules_to_capture,\n        use_code_pickling=use_code_pickling,\n    )\n    if annotations:\n        component_spec.metadata = structures.MetadataSpec(\n            annotations=annotations,\n        )\n\n    output_component_file = output_component_file or getattr(func, '_component_target_component_file', None)\n    if output_component_file:\n        component_spec.save(output_component_file)\n        #TODO: assert ComponentSpec.from_dict(load_yaml(output_component_file)) == component_spec\n\n    return _create_task_factory_from_component_spec(component_spec)\n\n\ndef create_component_from_func(\n    func: Callable,\n    output_component_file: str=None,\n    base_image: str = None,\n    packages_to_install: List[str] = None,\n    annotations: Optional[Mapping[str, str]] = None,\n):\n    '''Converts a Python function to a component and returns a task factory\n    (a function that accepts arguments and returns a task object).\n\n    Args:\n        func: The python function to convert\n        base_image: Optional. Specify a custom Docker container image to use in the component. For lightweight components, the image needs to have python 3.5+. Default is the python image corresponding to the current python environment.\n        output_component_file: Optional. Write a component definition to a local file. The produced component file can be loaded back by calling :code:`load_component_from_file` or :code:`load_component_from_uri`.\n        packages_to_install: Optional. List of [versioned] python packages to pip install before executing the user function.\n        annotations: Optional. Allows adding arbitrary key-value data to the component specification.\n\n    Returns:\n        A factory function with a strongly-typed signature taken from the python function.\n        Once called with the required arguments, the factory constructs a task instance that can run the original function in a container.\n\n    Examples:\n        The function name and docstring are used as component name and description. Argument and return annotations are used as component input/output types::\n\n            def add(a: float, b: float) -> float:\n                \"\"\"Returns sum of two arguments\"\"\"\n                return a + b\n\n            # add_op is a task factory function that creates a task object when given arguments\n            add_op = create_component_from_func(\n                func=add,\n                base_image='python:3.7', # Optional\n                output_component_file='add.component.yaml', # Optional\n                packages_to_install=['pandas==0.24'], # Optional\n            )\n\n            # The component spec can be accessed through the .component_spec attribute:\n            add_op.component_spec.save('add.component.yaml')\n\n            # The component function can be called with arguments to create a task:\n            add_task = add_op(1, 3)\n\n            # The resulting task has output references, corresponding to the component outputs.\n            # When the function only has a single anonymous return value, the output name is \"Output\":\n            sum_output_ref = add_task.outputs['Output']\n\n            # These task output references can be passed to other component functions, constructing a computation graph:\n            task2 = add_op(sum_output_ref, 5)\n\n\n        :code:`create_component_from_func` function can also be used as decorator::\n\n            @create_component_from_func\n            def add_op(a: float, b: float) -> float:\n                \"\"\"Returns sum of two arguments\"\"\"\n                return a + b\n\n        To declare a function with multiple return values, use the :code:`NamedTuple` return annotation syntax::\n\n            from typing import NamedTuple\n\n            def add_multiply_two_numbers(a: float, b: float) -> NamedTuple('Outputs', [('sum', float), ('product', float)]):\n                \"\"\"Returns sum and product of two arguments\"\"\"\n                return (a + b, a * b)\n\n            add_multiply_op = create_component_from_func(add_multiply_two_numbers)\n\n            # The component function can be called with arguments to create a task:\n            add_multiply_task = add_multiply_op(1, 3)\n\n            # The resulting task has output references, corresponding to the component outputs:\n            sum_output_ref = add_multiply_task.outputs['sum']\n\n            # These task output references can be passed to other component functions, constructing a computation graph:\n            task2 = add_multiply_op(sum_output_ref, 5)\n\n        Bigger data should be read from files and written to files.\n        Use the :py:class:`kfp.components.InputPath` parameter annotation to tell the system that the function wants to consume the corresponding input data as a file. The system will download the data, write it to a local file and then pass the **path** of that file to the function.\n        Use the :py:class:`kfp.components.OutputPath` parameter annotation to tell the system that the function wants to produce the corresponding output data as a file. The system will prepare and pass the **path** of a file where the function should write the output data. After the function exits, the system will upload the data to the storage system so that it can be passed to downstream components.\n\n        You can specify the type of the consumed/produced data by specifying the type argument to :py:class:`kfp.components.InputPath` and :py:class:`kfp.components.OutputPath`. The type can be a python type or an arbitrary type name string. :code:`OutputPath('CatBoostModel')` means that the function states that the data it has written to a file has type :code:`CatBoostModel`. :code:`InputPath('CatBoostModel')` means that the function states that it expect the data it reads from a file to have type 'CatBoostModel'. When the pipeline author connects inputs to outputs the system checks whether the types match.\n        Every kind of data can be consumed as a file input. Conversely, bigger data should not be consumed by value as all value inputs pass through the command line.\n\n        Example of a component function declaring file input and output::\n\n            def catboost_train_classifier(\n                training_data_path: InputPath('CSV'),            # Path to input data file of type \"CSV\"\n                trained_model_path: OutputPath('CatBoostModel'), # Path to output data file of type \"CatBoostModel\"\n                number_of_trees: int = 100,                      # Small output of type \"Integer\"\n            ) -> NamedTuple('Outputs', [\n                ('Accuracy', float),  # Small output of type \"Float\"\n                ('Precision', float), # Small output of type \"Float\"\n                ('JobUri', 'URI'),    # Small output of type \"URI\"\n            ]):\n                \"\"\"Trains CatBoost classification model\"\"\"\n                ...\n\n                return (accuracy, precision, recall)\n    '''\n\n    component_spec = _func_to_component_spec(\n        func=func,\n        base_image=base_image,\n        packages_to_install=packages_to_install,\n    )\n    if annotations:\n        component_spec.metadata = structures.MetadataSpec(\n            annotations=annotations,\n        )\n\n    if output_component_file:\n        component_spec.save(output_component_file)\n\n    return _create_task_factory_from_component_spec(component_spec)\n\n\ndef _module_is_builtin_or_standard(module_name: str) -> bool:\n    import sys\n    if module_name in sys.builtin_module_names:\n        return True\n    import distutils.sysconfig as sysconfig\n    import os\n    std_lib_dir = sysconfig.get_python_lib(standard_lib=True)\n    module_name_parts = module_name.split('.')\n    expected_module_path = os.path.join(std_lib_dir, *module_name_parts)\n    return os.path.exists(expected_module_path) or os.path.exists(expected_module_path + '.py')\n"
  },
  {
    "repo": "s102401002/kubeflowPipeline0722",
    "file_path": "src/HeartDisease_prediction.py",
    "raw_url": "https://raw.githubusercontent.com/s102401002/kubeflowPipeline0722/main/src/HeartDisease_prediction.py",
    "content": "from typing import NewType\n\nimport kfp\nimport kfp.compiler\n\nfrom kfp import dsl\nfrom kfp.dsl import OutputPath, InputPath, Input, Output, Artifact, Model, Dataset \n\nfrom pandas import DataFrame\n# from kfp.components import func_to_container_op\n\nDF = NewType('DF', DataFrame)\n\n@dsl.component(\n    base_image='python:3.9',\n    packages_to_install=['pandas==2.2.2']\n)\ndef load_data(data_output: Output[Artifact]):\n    import pandas as pd\n    \n    # \u8b80\u53d6\u8207\u7a0b\u5f0f\u78bc\u4f4d\u65bc\u540c\u4e00\u500b\u8cc7\u6599\u593e\u4e2d\u7684 stroke.csv\n    # urls = [\n    #     \"https://raw.githubusercontent.com/s102401002/kubeflowPipeline0722/main/heart_2020_cleaned.csv\"\n    # ]\n    df_data = pd.read_csv(\"https://raw.githubusercontent.com/s102401002/kubeflowPipeline0722/main/heart_2020_cleaned.csv\")\n    # \u79fb\u9664\u4e0d\u9700\u8981\u7684\u6b04\u4f4d\n    df_data = df_data.drop(columns=['PhysicalHealth', 'MentalHealth', 'Race' , 'GenHealth'])\n\n\n    # \u5b9a\u7fa9\u6620\u5c04\n    HeartDisease_map = {'Yes': 1, 'No': 0}\n    Smoking_map = {'Yes': 1, 'No': 0}\n    AlcoholDrinking_map = {'Yes': 1, 'No': 0}\n    Stroke_map = {'Yes': 1, 'No': 0}\n    DiffWalking_map = {'Yes': 1, 'No': 0}\n    Sex_map = {'Male': 0, 'Female': 1}\n    AgeCategory_map = {\n                        '0-4': 0,\n                        '5-9': 1,\n                        '10-14': 2,\n                        '15-17': 3,\n                        '18-24': 4,\n                        '25-29': 5,\n                        '30-34': 6,\n                        '35-39': 7,\n                        '40-44': 8,\n                        '45-49': 9,\n                        '50-54': 10,\n                        '55-59': 11,\n                        '60-64': 12,\n                        '65-69': 13,\n                        '70-74': 14,\n                        '75-79': 15,\n                        '80 or older': 16\n                    }\n    Diabetic_map = {'Yes (during pregnancy)':1 ,'Yes': 1, 'No': 0, 'No, borderline diabetes':0 }\n    PhysicalActivity_map = {'Yes': 1, 'No': 0}\n    Asthma_map = {'Yes': 1, 'No': 0}\n    KidneyDisease_map = {'Yes': 1, 'No': 0}\n    SkinCancer_map = {'Yes': 1, 'No': 0} \n\n    # \u88dc\u9f4a\u8cc7\u6599\n    df_data['HeartDisease'] = df_data['HeartDisease'].map(HeartDisease_map)\n    df_data['Smoking'] = df_data['Smoking'].map(Smoking_map) \n    df_data['AlcoholDrinking'] = df_data['AlcoholDrinking'].map(AlcoholDrinking_map) \n    df_data['Stroke'] = df_data['Stroke'].map(Stroke_map) \n    df_data['DiffWalking'] = df_data['DiffWalking'].map(DiffWalking_map) \n    df_data['Sex'] = df_data['Sex'].map(Sex_map) \n    df_data['AgeCategory'] = df_data['AgeCategory'].map(AgeCategory_map) \n    df_data['Diabetic'] = df_data['Diabetic'].map(Diabetic_map) \n    df_data['PhysicalActivity'] = df_data['PhysicalActivity'].map(PhysicalActivity_map)\n    df_data['Asthma'] = df_data['Asthma'].map(Asthma_map) \n    df_data['KidneyDisease'] = df_data['KidneyDisease'].map(KidneyDisease_map) \n    df_data['SkinCancer'] = df_data['SkinCancer'].map(SkinCancer_map) \n\n    # \u5c07 'Sex' \u548c 'AgeCategory' \u6b04\u4f4d\u5206\u5225\u79fb\u5230 DataFrame \u7684\u7b2c\u4e00\u548c\u7b2c\u4e8c\u6b04\n    columns_order = ['Sex', 'AgeCategory'] + [col for col in df_data.columns if col not in ['Sex', 'AgeCategory']]\n    df_data = df_data[columns_order]\n    df_data.to_csv(data_output.path)\n\n@dsl.component(\n    base_image='python:3.9',\n    packages_to_install=['pandas==2.2.2', 'scikit-learn==1.5.1']\n)\ndef prepare_data(\n    data_input: Input[Artifact], \n    X_train_output: Output[Artifact], X_test_output: Output[Artifact],\n    Y_train_output: Output[Artifact], Y_test_output: Output[Artifact],\n    X_val_output: Output[Artifact], Y_val_output: Output[Artifact]\n):\n    import pandas as pd\n    from sklearn.model_selection import train_test_split\n\n    df_data = pd.read_csv(data_input.path)\n\n    X = df_data.drop(labels=['HeartDisease'], axis=1)\n    Y = df_data[['HeartDisease']]\n    \n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n    X_test, X_val, Y_test, Y_val = train_test_split(X_test, Y_test, test_size=0.5, random_state=42)\n    X_train.to_csv(X_train_output.path, index=False)\n    X_test.to_csv(X_test_output.path, index=False)\n    Y_train.to_csv(Y_train_output.path, index=False)\n    Y_test.to_csv(Y_test_output.path, index=False)\n    X_val.to_csv(X_val_output.path, index=False)\n    Y_val.to_csv(Y_val_output.path, index=False)\n\n\n@dsl.component(\n    base_image='python:3.9',\n    packages_to_install=['pandas==2.2.2', 'scikit-learn==1.5.1', 'joblib==1.4.2']\n)\ndef train_model_LogisticRegression(\n    X_train: Input[Artifact], \n    Y_train: Input[Artifact], \n    X_val: Input[Artifact], \n    Y_val: Input[Artifact],\n    X_test: Input[Artifact], \n    Y_test: Input[Artifact], \n    model: Output[Artifact],\n    file: Output[Artifact]\n):\n    import pandas as pd\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.metrics import accuracy_score\n    import joblib\n    import json\n    \n    X_train = pd.read_csv(X_train.path)\n    Y_train = pd.read_csv(Y_train.path)\n    X_val = pd.read_csv(X_val.path)\n    Y_val = pd.read_csv(Y_val.path)\n    X_test = pd.read_csv(X_test.path)\n    Y_test = pd.read_csv(Y_test.path)\n\n    # Logistic Regression\n    print('Logistic Regression')\n    lr_model = LogisticRegression(random_state=0, max_iter=10000)\n    lr_model.fit(X_train, Y_train.values.ravel())\n    print('Training accuracy:', lr_model.score(X_train, Y_train))\n    lr_accuracy = lr_model.score(X_test, Y_test)\n    print('Test accuracy:', lr_accuracy)\n    \n    # Save the model\n    joblib.dump(lr_model, model.path)\n    # Save the accuracy\n    jsonFile = open(file.path,'w')\n    data = {}\n    data['accuracy'] = lr_accuracy\n    data['model_path'] = model.path\n    json.dump(data, jsonFile, indent=2)\n@dsl.component(\n    base_image='python:3.9',\n    packages_to_install=['pandas==2.2.2', 'scikit-learn==1.5.1', 'joblib==1.4.2', 'xgboost==2.0.3']\n)\ndef train_model_xgboost(\n    X_train: Input[Artifact], \n    Y_train: Input[Artifact],\n    X_val: Input[Artifact], \n    Y_val: Input[Artifact],\n    X_test: Input[Artifact], \n    Y_test: Input[Artifact], \n    model: Output[Artifact],\n    file: Output[Artifact]\n):\n    import pandas as pd\n    import xgboost as xgb\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.metrics import accuracy_score\n    import joblib\n    import json\n    \n    X_train = pd.read_csv(X_train.path)\n    Y_train = pd.read_csv(Y_train.path)\n    X_val = pd.read_csv(X_val.path)\n    Y_val = pd.read_csv(Y_val.path)\n    X_test = pd.read_csv(X_test.path)\n    Y_test = pd.read_csv(Y_test.path)\n\n    dtrain = xgb.DMatrix(X_train, label=Y_train)\n    dval = xgb.DMatrix(X_val, label=Y_val)\n    dtest = xgb.DMatrix(X_test, label=Y_test)\n\n    scale_pos_weight = len(Y_train[Y_train == 0]) / len(Y_train[Y_train == 1])\n    param = {\n        'max_depth': 3,\n        'eta': 0.3,\n        'objective': 'binary:logistic',\n        'eval_metric': 'logloss',\n        'scale_pos_weight': scale_pos_weight\n    }\n    evallist = [(dtrain, 'train'), (dval, 'eval')]\n    num_round = 1000\n    xgb_model = xgb.train(param, dtrain, num_round, evallist, early_stopping_rounds=10)\n    preds = xgb_model.predict(dtest)\n    predictions = [round(value) for value in preds]\n    global xgb_accuracy \n    xgb_accuracy = accuracy_score(Y_test, predictions)\n    print('XGBoost Test accuracy:', xgb_accuracy)\n    \n    # Save the model\n    joblib.dump(xgb_model, model.path)\n\n     # Save the accuracy\n    jsonFile = open(file.path,'w')\n    data = {}\n    data['accuracy'] = xgb_accuracy\n    data['model_path'] = model.path\n    json.dump(data, jsonFile, indent=2)\n    # with open(accuracy.path, 'w') as f:\n    #     f.write(str(xgb_accuracy))\n\n# @dsl.component(\n#     base_image='python:3.9',\n#     packages_to_install=['pandas==2.2.2', 'scikit-learn==1.5.1', 'joblib==1.4.2']\n# )\n# def train_model_svm(\n#     X_train: Input[Artifact], \n#     Y_train: Input[Artifact],\n#     X_val: Input[Artifact], \n#     Y_val: Input[Artifact],\n#     X_test: Input[Artifact], \n#     Y_test: Input[Artifact], \n#     model: Output[Artifact],\n#     accuracy: Output[Artifact]\n# ):\n#     import pandas as pd\n#     from sklearn.metrics import accuracy_score\n#     from sklearn import svm\n#     import joblib\n    \n#     X_train = pd.read_csv(X_train.path)\n#     Y_train = pd.read_csv(Y_train.path)\n#     X_val = pd.read_csv(X_val.path)\n#     Y_val = pd.read_csv(Y_val.path)\n#     X_test = pd.read_csv(X_test.path)\n#     Y_test = pd.read_csv(Y_test.path)\n\n#     clf=svm.SVC(kernel='poly',gamma='auto',C=100)\n#     clf.fit(X_train,Y_train.values.ravel())\n#     clf.predict(X_test)\n#     svm_accuracy = clf.score(X_test, Y_test)\n#     # Save the model\n#     joblib.dump(model, clf.path)\n\n#      # Save the accuracy\n#     with open(accuracy.path, 'w') as f:\n#         f.write(str(svm_accuracy))\n\n@dsl.component(\n    base_image='python:3.9',\n    packages_to_install=['pandas==2.2.2', 'scikit-learn==1.5.1', 'joblib==1.4.2']\n)\ndef train_model_RandomForest(\n    X_train: Input[Artifact], \n    Y_train: Input[Artifact],\n    X_val: Input[Artifact], \n    Y_val: Input[Artifact],\n    X_test: Input[Artifact], \n    Y_test: Input[Artifact], \n    model: Output[Artifact],\n    file: Output[Artifact]\n):\n    import pandas as pd\n    from sklearn.metrics import accuracy_score\n    from sklearn.ensemble import RandomForestClassifier\n    import joblib\n    import json\n    \n    X_train = pd.read_csv(X_train.path)\n    Y_train = pd.read_csv(Y_train.path)\n    X_val = pd.read_csv(X_val.path)\n    Y_val = pd.read_csv(Y_val.path)\n    X_test = pd.read_csv(X_test.path)\n    Y_test = pd.read_csv(Y_test.path)\n\n    rfc=RandomForestClassifier(n_estimators=5)\n    rfc.fit(X_train,Y_train.values.ravel())    \n    y_predict=rfc.predict(X_test)\n    rfc.predict(X_test)\n    rf_accuracy = rfc.score(X_test,Y_test)\n    # Save the model\n    joblib.dump(rfc, model.path)\n\n     # Save the accuracy\n    jsonFile = open(file.path,'w')\n    data = {}\n    data['accuracy'] = rf_accuracy\n    data['model_path'] = model.path\n    json.dump(data, jsonFile, indent=2)\n\n@dsl.component(\n    base_image='python:3.9',\n    packages_to_install=['pandas==2.2.2', 'scikit-learn==1.5.1', 'joblib==1.4.2']\n)\ndef train_model_KNN(\n    X_train: Input[Artifact], \n    Y_train: Input[Artifact],\n    X_val: Input[Artifact], \n    Y_val: Input[Artifact],\n    X_test: Input[Artifact], \n    Y_test: Input[Artifact], \n    model: Output[Artifact],\n    file: Output[Artifact]\n):\n    import pandas as pd\n    from sklearn.neighbors import KNeighborsClassifier\n    import joblib\n    import json\n\n    X_train = pd.read_csv(X_train.path)\n    Y_train = pd.read_csv(Y_train.path)\n    X_val = pd.read_csv(X_val.path)\n    Y_val = pd.read_csv(Y_val.path)\n    X_test = pd.read_csv(X_test.path)\n    Y_test = pd.read_csv(Y_test.path)\n\n    knc = KNeighborsClassifier(n_neighbors=3)\n    knc.fit(X_train,Y_train.values.ravel())\n    knn_accuracy = knc.score(X_test,Y_test)\n    # Save the model\n    joblib.dump(knc, model.path)\n\n     # Save the accuracy\n    jsonFile = open(file.path,'w')\n    data = {}\n    data['accuracy'] = knn_accuracy\n    data['model_path'] = model.path\n    json.dump(data, jsonFile, indent=2)\n\n@dsl.component(\n    base_image='python:3.9',\n    packages_to_install=['joblib==1.4.2', 'scikit-learn==1.5.1', 'xgboost==2.0.3']# \n)\ndef choose_model(\n    LogisticRegression_model: Input[Artifact],\n    XGBoost_model: Input[Artifact],\n    # SVM_model: Input[Artifact],\n    RandomForest_model: Input[Artifact],\n    KNN_model: Input[Artifact],\n    lr_file: Input[Artifact],\n    xgb_file: Input[Artifact],\n    # svm_accuracy: Input[Artifact],\n    rf_file: Input[Artifact],\n    knn_file: Input[Artifact],\n    final_model: Output[Model],\n    result: Output[Artifact]\n) -> None:\n    import joblib\n    import json\n\n    # Define a dictionary to store model artifacts and their corresponding JSON files\n    models = {\n        'LogisticRegression': lr_file,\n        'XGBoost': xgb_file,\n        'RandomForest': rf_file,\n        'KNN': knn_file\n    }\n\n    accuracy = {}\n    model_paths = {}\n\n    # Read accuracies and model paths\n    for model_name, json_file in models.items():\n        with open(json_file.path, 'r') as f:\n            data = json.load(f)\n        accuracy[model_name] = data['accuracy']\n        model_paths[model_name] = data['model_path']\n\n    # Find the best model\n    best_model_name = max(accuracy, key=accuracy.get)\n    best_model = joblib.load(model_paths[best_model_name])\n    \n    # Save the best model\n    joblib.dump(best_model, final_model.path)\n\n    # Prepare result string\n    result_string = f'Best Model is {best_model_name} : {accuracy[best_model_name]}'\n    result_string += f'\\nAccuracy:\\n'\n    for model_name, acc in accuracy.items():\n        result_string += f'{model_name:17} : {acc}\\n'\n    print(result_string)\n\n    # Write the result to a file\n    with open(result.path, 'w') as f:\n        f.write(result_string)\n@dsl.pipeline(\n    name='HeartDisease Prediction Pipeline',\n    description='Using Kubeflow pipeline to train and evaluate a HeartDisease prediction model'\n)\ndef HeartDisease_prediction_pipeline():\n    # Load data\n    load_data_task = load_data()\n\n    # Prepare data\n    prepare_data_task = prepare_data(data_input=load_data_task.outputs['data_output'])\n    \n    # Train models\n    train_model_LogisticRegression_task = train_model_LogisticRegression(\n        X_train=prepare_data_task.outputs['X_train_output'], \n        Y_train=prepare_data_task.outputs['Y_train_output'],\n        X_val=prepare_data_task.outputs['X_val_output'],\n        Y_val=prepare_data_task.outputs['Y_val_output'],\n        X_test=prepare_data_task.outputs['X_test_output'], \n        Y_test=prepare_data_task.outputs['Y_test_output']\n    )\n\n    train_model_xgboost_task = train_model_xgboost(\n        X_train=prepare_data_task.outputs['X_train_output'], \n        Y_train=prepare_data_task.outputs['Y_train_output'],\n        X_val=prepare_data_task.outputs['X_val_output'],\n        Y_val=prepare_data_task.outputs['Y_val_output'],\n        X_test=prepare_data_task.outputs['X_test_output'], \n        Y_test=prepare_data_task.outputs['Y_test_output']\n    )\n    \n    # train_model_svm_task = train_model_svm(\n    #     X_train=prepare_data_task.outputs['X_train_output'], \n    #     Y_train=prepare_data_task.outputs['Y_train_output'],\n    #     X_val=prepare_data_task.outputs['X_val_output'],\n    #     Y_val=prepare_data_task.outputs['Y_val_output'],\n    #     X_test=prepare_data_task.outputs['X_test_output'], \n    #     Y_test=prepare_data_task.outputs['Y_test_output']\n    # )\n\n    train_model_RandomForest_task = train_model_RandomForest(\n        X_train=prepare_data_task.outputs['X_train_output'], \n        Y_train=prepare_data_task.outputs['Y_train_output'],\n        X_val=prepare_data_task.outputs['X_val_output'],\n        Y_val=prepare_data_task.outputs['Y_val_output'],\n        X_test=prepare_data_task.outputs['X_test_output'], \n        Y_test=prepare_data_task.outputs['Y_test_output']\n    )\n\n    train_model_KNN_task = train_model_KNN(\n        X_train=prepare_data_task.outputs['X_train_output'], \n        Y_train=prepare_data_task.outputs['Y_train_output'],\n        X_val=prepare_data_task.outputs['X_val_output'],\n        Y_val=prepare_data_task.outputs['Y_val_output'],\n        X_test=prepare_data_task.outputs['X_test_output'], \n        Y_test=prepare_data_task.outputs['Y_test_output']\n    )\n\n    choose_model_task = choose_model(\n        LogisticRegression_model=train_model_LogisticRegression_task.outputs['model'],\n        XGBoost_model=train_model_xgboost_task.outputs['model'],\n        # SVM_model=train_model_svm_task.outputs['model'],\n        RandomForest_model=train_model_RandomForest_task.outputs['model'],\n        KNN_model=train_model_KNN_task.outputs['model'],\n        lr_file=train_model_LogisticRegression_task.outputs['file'],\n        xgb_file=train_model_xgboost_task.outputs['file'],\n        # svm_file=train_model_svm_task.outputs['file'],\n        rf_file=train_model_RandomForest_task.outputs['file'],\n        knn_file=train_model_KNN_task.outputs['file']\n    )\n    \n    # The pipeline doesn't need to return anything explicitly now\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(HeartDisease_prediction_pipeline, '../HeartDisease_prediction_pipeline.yaml')"
  },
  {
    "repo": "s102401002/kubeflowPipeline0722",
    "file_path": "src/compose.py",
    "raw_url": "https://raw.githubusercontent.com/s102401002/kubeflowPipeline0722/main/src/compose.py",
    "content": "from kfp import dsl, compiler, kubernetes, components\nfrom kfp.dsl import Input, Output, Metrics, Dataset, Model, Artifact, component, ContainerSpec,  OutputPath, InputPath\n\nimport json\n\n\ndef get_spark_job_definition():\n    import yaml\n    import time\n    # Read manifest file\n    with open('spark-job-python-10kprocess.yaml', \"r\") as stream:\n        spark_job_manifest = yaml.safe_load(stream)\n\n    # Add epoch time in the job name\n    epoch = int(time.time())\n    spark_job_manifest[\"metadata\"][\"name\"] = spark_job_manifest[\"metadata\"][\"name\"].format(epoch=epoch)\n    return spark_job_manifest\n\n\n@component(\n    base_image='python:3.10-slim', \n    packages_to_install=['pandas']\n)\ndef load_file_from_nas_to_minio(\n    x_train_input_path: str, \n    x_test_input_path: str, \n    y_train_input_path: str, \n    y_test_input_path: str, \n    x_train_output: Output[Dataset], \n    x_test_output: Output[Dataset], \n    y_train_output: Output[Dataset], \n    y_test_output: Output[Dataset]\n):\n    import pandas as pd\n\n    df = pd.read_csv(x_train_input_path)\n    df.to_csv(x_train_output.path, index=False)\n\n    df = pd.read_csv(x_test_input_path)\n    df.to_csv(x_test_output.path, index=False)\n\n    df = pd.read_csv(y_train_input_path)\n    df.to_csv(y_train_output.path, index=False)\n\n    df = pd.read_csv(y_test_input_path)\n    df.to_csv(y_test_output.path, index=False)\n\n@component(base_image='python:3.10-slim')\ndef parse_input_json(\n    json_file_path: str, \n    xgboost_input_metrics: Output[Metrics], \n    random_forest_input_metrics: Output[Metrics], \n    knn_input_metrics: Output[Metrics],\n    lr_input_metrics: Output[Metrics]\n):\n    import json\n\n    def log_metric(metrics: Metrics, input_dict: dict):\n        for key in input_dict:\n            if key == \"method\":\n                continue\n            else:\n                metrics.log_metric(key, input_dict.get(key))\n\n    with open(file=json_file_path, mode='r', encoding='utf8') as file:\n        input_dict_arr: list[dict] = json.load(file)\n    \n    for input_dict in input_dict_arr:\n        if input_dict[\"method\"] == \"xgboost\":\n            log_metric(xgboost_input_metrics, input_dict)\n        elif input_dict[\"method\"] == \"random_forest\":\n            log_metric(random_forest_input_metrics, input_dict)\n        elif input_dict[\"method\"] == \"knn\":\n            log_metric(knn_input_metrics, input_dict)\n        elif input_dict[\"method\"] == \"lr\":\n            log_metric(lr_input_metrics, input_dict)\n        else:\n            continue\n\n#xgboost_katib\n@dsl.component(\n    base_image='python:3.10-slim', \n    packages_to_install=[\n        'kubeflow-katib==0.17.0'\n    ]\n)\ndef run_xgboost_katib_experiment(\n    input_params_metrics: Input[Metrics], \n    best_params_metrics: Output[Metrics]\n):\n    from kubeflow.katib import KatibClient\n    from kubernetes.client import V1ObjectMeta\n    from kubeflow.katib import V1beta1Experiment\n    from kubeflow.katib import V1beta1AlgorithmSpec\n    from kubeflow.katib import V1beta1ObjectiveSpec\n    from kubeflow.katib import V1beta1FeasibleSpace\n    from kubeflow.katib import V1beta1ExperimentSpec\n    from kubeflow.katib import V1beta1ObjectiveSpec\n    from kubeflow.katib import V1beta1ParameterSpec\n    from kubeflow.katib import V1beta1TrialTemplate\n    from kubeflow.katib import V1beta1TrialParameterSpec\n\n    from datetime import datetime, timezone, timedelta\n\n    dt_str = datetime.now(timezone(timedelta(hours=8))).strftime(\"%-Y-%m-%d-%H-%M-%S\")\n\n    experiment_name = \"xgboost-\" + dt_str.replace(\"_\", \"-\")\n    experiment_namespace = input_params_metrics.metadata.get(\"experiment_namespace\")\n\n    if experiment_name is None or experiment_namespace is None:\n        raise ValueError(\"Both experiment_name and experiment namespace needs to be a string!\")\n\n    metadata = V1ObjectMeta(\n        name=experiment_name, \n        namespace=experiment_namespace\n    )\n\n    algorithm_spec = V1beta1AlgorithmSpec(\n        algorithm_name=\"random\"\n    )\n\n    objective_spec = V1beta1ObjectiveSpec(\n        type=\"maximize\",\n        goal= 0.99,\n        objective_metric_name=\"accuracy\",\n    )\n\n    learning_rate_min = input_params_metrics.metadata.get(\"learning_rate_min\")\n    learning_rate_max = input_params_metrics.metadata.get(\"learning_rate_max\")\n    learning_rate_step = input_params_metrics.metadata.get(\"learning_rate_step\")\n\n    if learning_rate_min is None or learning_rate_max is None or learning_rate_step is None:\n        raise ValueError(\"All learning_rate_min, learning_rate_max and learning_rate_step cannot be null!\")\n\n    try:\n        learning_rate_min = float(learning_rate_min)\n        learning_rate_max = float(learning_rate_max)\n        learning_rate_step = float(learning_rate_step)\n    except ValueError:\n        raise ValueError(\"All learning_rate_min, learning_rate_max and learning_rate_step needs to be a float!\")\n\n    n_estimators_min = input_params_metrics.metadata.get(\"n_estimators_min\")\n    n_estimators_max = input_params_metrics.metadata.get(\"n_estimators_max\")\n    n_estimators_step = input_params_metrics.metadata.get(\"n_estimators_step\")\n\n    if n_estimators_min is None or n_estimators_max is None or n_estimators_step is None:\n        raise ValueError(\"All n_estimators_min, n_estimators_max and n_estimators_step cannot be null!\")\n\n    try:\n        n_estimators_min = int(n_estimators_min)\n        n_estimators_max = int(n_estimators_max)\n        n_estimators_step = int(n_estimators_step)\n    except ValueError:\n        raise ValueError(\"All n_estimators_min, n_estimators_max and n_estimators_step needs to be a float!\")\n\n    parameters = [\n        V1beta1ParameterSpec(\n            name=\"lr\",\n            parameter_type=\"double\",\n            feasible_space=V1beta1FeasibleSpace(\n                min=str(learning_rate_min),\n                max=str(learning_rate_max), \n                step=str(learning_rate_step)\n            ),\n        ), \n        V1beta1ParameterSpec(\n            name=\"ne\",\n            parameter_type=\"int\",\n            feasible_space=V1beta1FeasibleSpace(\n                min=str(n_estimators_min),\n                max=str(n_estimators_max), \n                step=str(n_estimators_step)\n            ),\n        )\n    ]\n\n    docker_image_name = input_params_metrics.metadata.get(\"docker_image_name\")\n    if docker_image_name is None:\n        raise ValueError(\"Docker image name cannot be null!\")\n\n    random_state = input_params_metrics.metadata.get(\"random_state\")\n    if random_state is None:\n        random_state = 42\n    else:\n        try:\n            random_state = int(random_state)\n        except ValueError:\n            raise ValueError(\"Random state needs to be an int!\")\n        \n    x_train_path = input_params_metrics.metadata.get(\"x_train_path\")\n    x_test_path = input_params_metrics.metadata.get(\"x_test_path\")\n    y_train_path = input_params_metrics.metadata.get(\"y_train_path\")\n    y_test_path = input_params_metrics.metadata.get(\"y_test_path\")\n\n    train_container = {\n        \"name\": \"training-container\",\n        \"image\": f\"docker.io/{docker_image_name}\",\n        \"command\": [\n            \"python3\",\n            \"/opt/xgboost/train.py\",\n            \"--lr=${trialParameters.learningRate}\",\n            \"--ne=${trialParameters.nEstimators}\",\n            f\"--rs={random_state}\",\n            f\"--esp=100000\",\n            f\"--booster=gbtree\",\n            f\"--x_train_path={x_train_path}\",\n            f\"--x_test_path={x_test_path}\",\n            f\"--y_train_path={y_train_path}\",\n            f\"--y_test_path={y_test_path}\",\n            f\"--save_model=false\",\n            f\"--model_folder_path=models\"\n        ]\n    }\n\n    template_spec = {\n        \"containers\": [\n            train_container\n        ],\n        \"restartPolicy\": \"Never\"\n    }\n\n    volumes = []\n    volumeMounts = []\n\n    datasets_from_pvc = input_params_metrics.metadata.get(\"datasets_from_pvc\")\n    datasets_pvc_name = input_params_metrics.metadata.get(\"datasets_pvc_name\")\n    datasets_pvc_mount_path = input_params_metrics.metadata.get(\"datasets_pvc_mount_path\")\n\n    datasets_from_pvc = bool(datasets_from_pvc)\n    \n    if datasets_from_pvc is True:\n        if datasets_pvc_name is None or datasets_pvc_mount_path is None:\n            raise ValueError(\"Both datasets_pvc_name and datasets_pvc_mount_path cannot be null\")\n\n        volumes.append({\n            \"name\": \"datasets\", \n            \"persistentVolumeClaim\": {\n                \"claimName\": datasets_pvc_name\n            }\n        })\n        volumeMounts.append({\n            \"name\": \"datasets\", \n            \"mountPath\": datasets_pvc_mount_path\n        })\n\n    '''\n    if save_model is True:\n        volumes.append({\n            \"name\": \"models\", \n            \"persistentVolumeClaim\": {\n                \"claimName\": models_pvc_name\n            }\n        })\n        volumeMounts.append({\n            \"name\": \"models\", \n            \"mountPath\": \"/opt/xgboost/models\"\n        })\n\n    if datasets_from_pvc is True or save_model is True:\n        train_container[\"volumeMounts\"] = volumeMounts\n        template_spec[\"volumes\"] = volumes\n    '''\n    if datasets_from_pvc is True:\n        train_container[\"volumeMounts\"] = volumeMounts\n        template_spec[\"volumes\"] = volumes\n\n    trial_spec={\n        \"apiVersion\": \"batch/v1\",\n        \"kind\": \"Job\",\n        \"spec\": {\n            \"template\": {\n                \"metadata\": {\n                    \"annotations\": {\n                        \"sidecar.istio.io/inject\": \"false\"\n                    }\n                },\n                \"spec\": template_spec\n            }\n        }\n    }\n\n    trial_template=V1beta1TrialTemplate(\n        primary_container_name=\"training-container\",\n        trial_parameters=[\n            V1beta1TrialParameterSpec(\n                name=\"learningRate\",\n                description=\"Learning rate for the training model\",\n                reference=\"lr\"\n            ), \n            V1beta1TrialParameterSpec(\n                name=\"nEstimators\",\n                description=\"N estimators for the training model\",\n                reference=\"ne\"\n            )\n        ],\n        trial_spec=trial_spec,\n        retain=True\n    )\n\n    max_trial_counts = input_params_metrics.metadata.get(\"max_trial_counts\")\n    max_failed_trial_counts = input_params_metrics.metadata.get(\"max_failed_trial_counts\")\n    parallel_trial_counts = input_params_metrics.metadata.get(\"parallel_trial_counts\")\n\n    if max_failed_trial_counts is None or max_failed_trial_counts is None or parallel_trial_counts is None:\n        raise ValueError(\"All max_trial_counts, max_failed_trial_counts and parallel_trial_counts cannot be null!\")\n    \n    try:\n        max_trial_counts = int(max_trial_counts)\n        max_failed_trial_counts = int(max_failed_trial_counts)\n        parallel_trial_counts = int(parallel_trial_counts)\n    except ValueError:\n        raise ValueError(\"All max_trial_counts, max_failed_trial_counts and needs to be an int!\")\n\n    experiment = V1beta1Experiment(\n        api_version=\"kubeflow.org/v1beta1\",\n        kind=\"Experiment\",\n        metadata=metadata,\n        spec=V1beta1ExperimentSpec(\n            max_trial_count=max_trial_counts,\n            parallel_trial_count=parallel_trial_counts,\n            max_failed_trial_count=max_failed_trial_counts,\n            algorithm=algorithm_spec,\n            objective=objective_spec,\n            parameters=parameters,\n            trial_template=trial_template,\n        )\n    )\n\n    client_namespace = input_params_metrics.metadata.get(\"client_namespace\")\n    if client_namespace is None:\n        raise ValueError(\"Client namespace cannot be null!\")\n\n    client = KatibClient(namespace=client_namespace)\n    client.create_experiment(experiment=experiment)\n    client.wait_for_experiment_condition(name=experiment_name, namespace=experiment_namespace, timeout=3600)\n\n    result = client.get_optimal_hyperparameters(name=experiment_name, namespace=experiment_namespace).to_dict()\n\n    best_params_list = result[\"parameter_assignments\"]\n\n    for params in best_params_list:\n        name = params[\"name\"]\n        value = params[\"value\"]\n\n        if name == \"lr\":\n            value = float(value)\n        elif name == \"ne\":\n            value = int(value)\n            \n        best_params_metrics.log_metric(metric=name, value=value)\n\n'''\n==================================\nThis is for seperating the code.\nDon't remove it.\nThx!\n\nRandom forest katib below.\n==================================\n'''\n\n#random_forest_katib\n@dsl.component(\n    base_image='python:3.10-slim', \n    packages_to_install=[\n        'kubeflow-katib==0.17.0'\n    ]\n)\ndef run_random_forest_katib_experiment(\n    input_params_metrics: Input[Metrics], \n    best_params_metrics: Output[Metrics]\n):\n    from kubeflow.katib import KatibClient\n    from kubernetes.client import V1ObjectMeta\n    from kubeflow.katib import V1beta1Experiment\n    from kubeflow.katib import V1beta1AlgorithmSpec\n    from kubeflow.katib import V1beta1ObjectiveSpec\n    from kubeflow.katib import V1beta1FeasibleSpace\n    from kubeflow.katib import V1beta1ExperimentSpec\n    from kubeflow.katib import V1beta1ObjectiveSpec\n    from kubeflow.katib import V1beta1ParameterSpec\n    from kubeflow.katib import V1beta1TrialTemplate\n    from kubeflow.katib import V1beta1TrialParameterSpec\n\n    from datetime import datetime, timezone, timedelta\n\n    dt_str = datetime.now(timezone(timedelta(hours=8))).strftime(\"%-Y-%m-%d-%H-%M-%S\")\n\n    experiment_name = \"random-forest-\" + dt_str.replace(\"_\", \"-\")\n    experiment_namespace = input_params_metrics.metadata.get(\"experiment_namespace\")\n\n    if experiment_name is None or experiment_namespace is None:\n        raise ValueError(\"Both experiment_name and experiment namespace needs to be a string!\")\n\n    metadata = V1ObjectMeta(\n        name=experiment_name, \n        namespace=experiment_namespace\n    )\n\n    algorithm_spec = V1beta1AlgorithmSpec(\n        algorithm_name=\"random\"\n    )\n\n    objective_spec = V1beta1ObjectiveSpec(\n        type=\"maximize\",\n        goal= 0.99,\n        objective_metric_name=\"accuracy\",\n    )\n\n    n_estimators_min = input_params_metrics.metadata.get(\"n_estimators_min\")\n    n_estimators_max = input_params_metrics.metadata.get(\"n_estimators_max\")\n    n_estimators_step = input_params_metrics.metadata.get(\"n_estimators_step\")\n\n    if n_estimators_min is None or n_estimators_max is None or n_estimators_step is None:\n        raise ValueError(\"All n_estimators_min, n_estimators_max and n_estimators_step cannot be null!\")\n\n    try:\n        n_estimators_min = int(n_estimators_min)\n        n_estimators_max = int(n_estimators_max)\n        n_estimators_step = int(n_estimators_step)\n    except ValueError:\n        raise ValueError(\"All n_estimators_min, n_estimators_max and n_estimators_step needs to be a float!\")\n\n    parameters = [\n        V1beta1ParameterSpec(\n            name=\"ne\",\n            parameter_type=\"int\",\n            feasible_space=V1beta1FeasibleSpace(\n                min=str(n_estimators_min),\n                max=str(n_estimators_max), \n                step=str(n_estimators_step)\n            ),\n        )\n    ]\n\n    docker_image_name = input_params_metrics.metadata.get(\"docker_image_name\")\n    if docker_image_name is None:\n        raise ValueError(\"Docker image name cannot be null!\")\n\n    random_state = input_params_metrics.metadata.get(\"random_state\")\n    if random_state is None:\n        random_state = 42\n    else:\n        try:\n            random_state = int(random_state)\n        except ValueError:\n            raise ValueError(\"Random state needs to be an int!\")\n        \n    x_train_path = input_params_metrics.metadata.get(\"x_train_path\")\n    x_test_path = input_params_metrics.metadata.get(\"x_test_path\")\n    y_train_path = input_params_metrics.metadata.get(\"y_train_path\")\n    y_test_path = input_params_metrics.metadata.get(\"y_test_path\")\n\n    train_container = {\n        \"name\": \"training-container\",\n        \"image\": f\"docker.io/{docker_image_name}\",\n        \"command\": [\n            \"python3\",\n            \"/opt/rfc/train.py\",\n            \"--ne=${trialParameters.nEstimators}\",\n            f\"--rs={random_state}\",\n            f\"--x_train_path={x_train_path}\",\n            f\"--x_test_path={x_test_path}\",\n            f\"--y_train_path={y_train_path}\",\n            f\"--y_test_path={y_test_path}\",\n            f\"--save_model=false\",\n            f\"--model_folder_path=models\"\n        ]\n    }\n\n    template_spec = {\n        \"containers\": [\n            train_container\n        ],\n        \"restartPolicy\": \"Never\"\n    }\n\n    volumes = []\n    volumeMounts = []\n\n    datasets_from_pvc = input_params_metrics.metadata.get(\"datasets_from_pvc\")\n    datasets_pvc_name = input_params_metrics.metadata.get(\"datasets_pvc_name\")\n    datasets_pvc_mount_path = input_params_metrics.metadata.get(\"datasets_pvc_mount_path\")\n\n    datasets_from_pvc = bool(datasets_from_pvc)\n    \n    if datasets_from_pvc is True:\n        if datasets_pvc_name is None or datasets_pvc_mount_path is None:\n            raise ValueError(\"Both datasets_pvc_name and datasets_pvc_mount_path cannot be null\")\n\n        volumes.append({\n            \"name\": \"datasets\", \n            \"persistentVolumeClaim\": {\n                \"claimName\": datasets_pvc_name\n            }\n        })\n        volumeMounts.append({\n            \"name\": \"datasets\", \n            \"mountPath\": datasets_pvc_mount_path\n        })\n\n    '''\n    if save_model is True:\n        volumes.append({\n            \"name\": \"models\", \n            \"persistentVolumeClaim\": {\n                \"claimName\": models_pvc_name\n            }\n        })\n        volumeMounts.append({\n            \"name\": \"models\", \n            \"mountPath\": \"/opt/rfc/models\"\n        })\n\n    if datasets_from_pvc is True or save_model is True:\n        train_container[\"volumeMounts\"] = volumeMounts\n        template_spec[\"volumes\"] = volumes\n    '''\n    if datasets_from_pvc is True:\n        train_container[\"volumeMounts\"] = volumeMounts\n        template_spec[\"volumes\"] = volumes\n\n    trial_spec={\n        \"apiVersion\": \"batch/v1\",\n        \"kind\": \"Job\",\n        \"spec\": {\n            \"template\": {\n                \"metadata\": {\n                    \"annotations\": {\n                        \"sidecar.istio.io/inject\": \"false\"\n                    }\n                },\n                \"spec\": template_spec\n            }\n        }\n    }\n\n    trial_template=V1beta1TrialTemplate(\n        primary_container_name=\"training-container\",\n        trial_parameters=[\n            V1beta1TrialParameterSpec(\n                name=\"nEstimators\",\n                description=\"N estimators for the training model\",\n                reference=\"ne\"\n            )\n        ],\n        trial_spec=trial_spec,\n        retain=True\n    )\n\n    max_trial_counts = input_params_metrics.metadata.get(\"max_trial_counts\")\n    max_failed_trial_counts = input_params_metrics.metadata.get(\"max_failed_trial_counts\")\n    parallel_trial_counts = input_params_metrics.metadata.get(\"parallel_trial_counts\")\n\n    if max_failed_trial_counts is None or max_failed_trial_counts is None or parallel_trial_counts is None:\n        raise ValueError(\"All max_trial_counts, max_failed_trial_counts and parallel_trial_counts cannot be null!\")\n    \n    try:\n        max_trial_counts = int(max_trial_counts)\n        max_failed_trial_counts = int(max_failed_trial_counts)\n        parallel_trial_counts = int(parallel_trial_counts)\n    except ValueError:\n        raise ValueError(\"All max_trial_counts, max_failed_trial_counts and needs to be an int!\")\n\n    experiment = V1beta1Experiment(\n        api_version=\"kubeflow.org/v1beta1\",\n        kind=\"Experiment\",\n        metadata=metadata,\n        spec=V1beta1ExperimentSpec(\n            max_trial_count=max_trial_counts,\n            parallel_trial_count=parallel_trial_counts,\n            max_failed_trial_count=max_failed_trial_counts,\n            algorithm=algorithm_spec,\n            objective=objective_spec,\n            parameters=parameters,\n            trial_template=trial_template,\n        )\n    )\n\n    client_namespace = input_params_metrics.metadata.get(\"client_namespace\")\n    if client_namespace is None:\n        raise ValueError(\"Client namespace cannot be null!\")\n\n    client = KatibClient(namespace=client_namespace)\n    client.create_experiment(experiment=experiment)\n    client.wait_for_experiment_condition(name=experiment_name, namespace=experiment_namespace, timeout=3600)\n\n    result = client.get_optimal_hyperparameters(name=experiment_name, namespace=experiment_namespace).to_dict()\n\n    best_params_list = result[\"parameter_assignments\"]\n\n    for params in best_params_list:\n        name = params[\"name\"]\n        value = params[\"value\"]\n\n        if name == \"ne\":\n            value = int(value)\n            \n        best_params_metrics.log_metric(metric=name, value=value)\n\n'''\n==================================\nThis is for seperating the code.\nDon't remove it.\nThx!\n\nKNN katib below.\n==================================\n'''\n\n#knn_katib\n@dsl.component(\n    base_image='python:3.10-slim', \n    packages_to_install=[\n        'kubeflow-katib==0.17.0'\n    ]\n)\ndef run_knn_katib_experiment(\n    input_params_metrics: Input[Metrics], \n    best_params_metrics: Output[Metrics]\n):\n    from kubeflow.katib import KatibClient\n    from kubernetes.client import V1ObjectMeta\n    from kubeflow.katib import V1beta1Experiment\n    from kubeflow.katib import V1beta1AlgorithmSpec\n    from kubeflow.katib import V1beta1ObjectiveSpec\n    from kubeflow.katib import V1beta1FeasibleSpace\n    from kubeflow.katib import V1beta1ExperimentSpec\n    from kubeflow.katib import V1beta1ObjectiveSpec\n    from kubeflow.katib import V1beta1ParameterSpec\n    from kubeflow.katib import V1beta1TrialTemplate\n    from kubeflow.katib import V1beta1TrialParameterSpec\n\n    from datetime import datetime, timezone, timedelta\n\n    dt_str = datetime.now(timezone(timedelta(hours=8))).strftime(\"%-Y-%m-%d-%H-%M-%S\")\n\n    experiment_name = \"knn-\" + dt_str.replace(\"_\", \"-\")\n    experiment_namespace = input_params_metrics.metadata.get(\"experiment_namespace\")\n\n    if experiment_name is None or experiment_namespace is None:\n        raise ValueError(\"Both experiment_name and experiment namespace needs to be a string!\")\n\n    metadata = V1ObjectMeta(\n        name=experiment_name, \n        namespace=experiment_namespace\n    )\n\n    algorithm_spec = V1beta1AlgorithmSpec(\n        algorithm_name=\"random\"\n    )\n\n    objective_spec = V1beta1ObjectiveSpec(\n        type=\"maximize\",\n        goal= 0.99,\n        objective_metric_name=\"accuracy\",\n    )\n\n    n_neighbors_min = input_params_metrics.metadata.get(\"n_neighbors_min\")\n    n_neighbors_max = input_params_metrics.metadata.get(\"n_neighbors_max\")\n    n_neighbors_step = input_params_metrics.metadata.get(\"n_neighbors_step\")\n\n    if n_neighbors_min is None or n_neighbors_max is None or n_neighbors_step is None:\n        raise ValueError(\"All n_neighbors_min, n_neighbors_max and n_neighbors_step cannot be null!\")\n\n    try:\n        n_neighbors_min = int(n_neighbors_min)\n        n_neighbors_max = int(n_neighbors_max)\n        n_neighbors_step = int(n_neighbors_step)\n    except ValueError:\n        raise ValueError(\"All n_neighbors_min, n_neighbors_max and n_neighbors_step needs to be a int!\")\n    \n    if n_neighbors_min % 2 != 1 or n_neighbors_max % 2 != 1 or n_neighbors_step % 2 != 0:\n        raise ValueError(\"N neighbors needs to be an odd number!\")\n\n    parameters = [\n        V1beta1ParameterSpec(\n            name=\"nn\",\n            parameter_type=\"int\",\n            feasible_space=V1beta1FeasibleSpace(\n                min=str(n_neighbors_min),\n                max=str(n_neighbors_max), \n                step=str(n_neighbors_step)\n            )\n        )\n    ]\n\n    docker_image_name = input_params_metrics.metadata.get(\"docker_image_name\")\n    if docker_image_name is None:\n        raise ValueError(\"Docker image name cannot be null!\")\n\n    random_state = input_params_metrics.metadata.get(\"random_state\")\n    if random_state is None:\n        random_state = 42\n    else:\n        try:\n            random_state = int(random_state)\n        except ValueError:\n            raise ValueError(\"Random state needs to be an int!\")\n        \n    x_train_path = input_params_metrics.metadata.get(\"x_train_path\")\n    x_test_path = input_params_metrics.metadata.get(\"x_test_path\")\n    y_train_path = input_params_metrics.metadata.get(\"y_train_path\")\n    y_test_path = input_params_metrics.metadata.get(\"y_test_path\")\n\n    train_container = {\n        \"name\": \"training-container\",\n        \"image\": f\"docker.io/{docker_image_name}\",\n        \"command\": [\n            \"python3\",\n            \"/opt/knn/train.py\",\n            \"--nn=${trialParameters.nNeighbors}\",\n            f\"--rs={random_state}\",\n            f\"--x_train_path={x_train_path}\",\n            f\"--x_test_path={x_test_path}\",\n            f\"--y_train_path={y_train_path}\",\n            f\"--y_test_path={y_test_path}\",\n            f\"--save_model=false\",\n            f\"--model_folder_path=models\"\n        ]\n    }\n\n    template_spec = {\n        \"containers\": [\n            train_container\n        ],\n        \"restartPolicy\": \"Never\"\n    }\n\n    volumes = []\n    volumeMounts = []\n\n    datasets_from_pvc = input_params_metrics.metadata.get(\"datasets_from_pvc\")\n    datasets_pvc_name = input_params_metrics.metadata.get(\"datasets_pvc_name\")\n    datasets_pvc_mount_path = input_params_metrics.metadata.get(\"datasets_pvc_mount_path\")\n\n    datasets_from_pvc = bool(datasets_from_pvc)\n    \n    if datasets_from_pvc is True:\n        if datasets_pvc_name is None or datasets_pvc_mount_path is None:\n            raise ValueError(\"Both datasets_pvc_name and datasets_pvc_mount_path cannot be null\")\n\n        volumes.append({\n            \"name\": \"datasets\", \n            \"persistentVolumeClaim\": {\n                \"claimName\": datasets_pvc_name\n            }\n        })\n        volumeMounts.append({\n            \"name\": \"datasets\", \n            \"mountPath\": datasets_pvc_mount_path\n        })\n\n    '''\n    if save_model is True:\n        volumes.append({\n            \"name\": \"models\", \n            \"persistentVolumeClaim\": {\n                \"claimName\": models_pvc_name\n            }\n        })\n        volumeMounts.append({\n            \"name\": \"models\", \n            \"mountPath\": \"/opt/rfc/models\"\n        })\n\n    if datasets_from_pvc is True or save_model is True:\n        train_container[\"volumeMounts\"] = volumeMounts\n        template_spec[\"volumes\"] = volumes\n    '''\n    if datasets_from_pvc is True:\n        train_container[\"volumeMounts\"] = volumeMounts\n        template_spec[\"volumes\"] = volumes\n\n    trial_spec={\n        \"apiVersion\": \"batch/v1\",\n        \"kind\": \"Job\",\n        \"spec\": {\n            \"template\": {\n                \"metadata\": {\n                    \"annotations\": {\n                        \"sidecar.istio.io/inject\": \"false\"\n                    }\n                },\n                \"spec\": template_spec\n            }\n        }\n    }\n\n    trial_template=V1beta1TrialTemplate(\n        primary_container_name=\"training-container\",\n        trial_parameters=[\n            V1beta1TrialParameterSpec(\n                name=\"nNeighbors\",\n                description=\"N neighbors for the training model\",\n                reference=\"nn\"\n            )\n        ],\n        trial_spec=trial_spec,\n        retain=True\n    )\n\n    max_trial_counts = input_params_metrics.metadata.get(\"max_trial_counts\")\n    max_failed_trial_counts = input_params_metrics.metadata.get(\"max_failed_trial_counts\")\n    parallel_trial_counts = input_params_metrics.metadata.get(\"parallel_trial_counts\")\n\n    if max_failed_trial_counts is None or max_failed_trial_counts is None or parallel_trial_counts is None:\n        raise ValueError(\"All max_trial_counts, max_failed_trial_counts and parallel_trial_counts cannot be null!\")\n    \n    try:\n        max_trial_counts = int(max_trial_counts)\n        max_failed_trial_counts = int(max_failed_trial_counts)\n        parallel_trial_counts = int(parallel_trial_counts)\n    except ValueError:\n        raise ValueError(\"All max_trial_counts, max_failed_trial_counts and needs to be an int!\")\n\n    experiment = V1beta1Experiment(\n        api_version=\"kubeflow.org/v1beta1\",\n        kind=\"Experiment\",\n        metadata=metadata,\n        spec=V1beta1ExperimentSpec(\n            max_trial_count=max_trial_counts,\n            parallel_trial_count=parallel_trial_counts,\n            max_failed_trial_count=max_failed_trial_counts,\n            algorithm=algorithm_spec,\n            objective=objective_spec,\n            parameters=parameters,\n            trial_template=trial_template,\n        )\n    )\n\n    client_namespace = input_params_metrics.metadata.get(\"client_namespace\")\n    if client_namespace is None:\n        raise ValueError(\"Client namespace cannot be null!\")\n\n    client = KatibClient(namespace=client_namespace)\n    client.create_experiment(experiment=experiment)\n    client.wait_for_experiment_condition(name=experiment_name, namespace=experiment_namespace, timeout=3600)\n\n    result = client.get_optimal_hyperparameters(name=experiment_name, namespace=experiment_namespace).to_dict()\n\n    best_params_list = result[\"parameter_assignments\"]\n\n    for params in best_params_list:\n        name = params[\"name\"]\n        value = params[\"value\"]\n\n        if name == \"nn\":\n            value = int(value)\n            \n        best_params_metrics.log_metric(metric=name, value=value)\n\n'''\n==================================\nThis is for seperating the code.\nDon't remove it.\nThx!\n\nLogistic Regression katib below.\n==================================\n'''\n\n#Logistic Regression_katib\n@dsl.component(\n    base_image='python:3.10-slim', \n    packages_to_install=[\n        'kubeflow-katib==0.17.0'\n    ]\n)\ndef run_lr_katib_experiment(\n    input_params_metrics: Input[Metrics], \n    best_params_metrics: Output[Metrics]\n):\n    from kubeflow.katib import KatibClient\n    from kubernetes.client import V1ObjectMeta\n    from kubeflow.katib import V1beta1Experiment\n    from kubeflow.katib import V1beta1AlgorithmSpec\n    from kubeflow.katib import V1beta1ObjectiveSpec\n    from kubeflow.katib import V1beta1FeasibleSpace\n    from kubeflow.katib import V1beta1ExperimentSpec\n    from kubeflow.katib import V1beta1ObjectiveSpec\n    from kubeflow.katib import V1beta1ParameterSpec\n    from kubeflow.katib import V1beta1TrialTemplate\n    from kubeflow.katib import V1beta1TrialParameterSpec\n\n    from datetime import datetime, timezone, timedelta\n\n    dt_str = datetime.now(timezone(timedelta(hours=8))).strftime(\"%-Y-%m-%d-%H-%M-%S\")\n\n    experiment_name = \"lr-\" + dt_str.replace(\"_\", \"-\")\n    experiment_namespace = input_params_metrics.metadata.get(\"experiment_namespace\")\n\n    if experiment_name is None or experiment_namespace is None:\n        raise ValueError(\"Both experiment_name and experiment namespace needs to be a string!\")\n\n    metadata = V1ObjectMeta(\n        name=experiment_name, \n        namespace=experiment_namespace\n    )\n\n    algorithm_spec = V1beta1AlgorithmSpec(\n        algorithm_name=\"random\"\n    )\n\n    objective_spec = V1beta1ObjectiveSpec(\n        type=\"maximize\",\n        goal= 0.99,\n        objective_metric_name=\"accuracy\",\n    )\n\n    iterators_min = input_params_metrics.metadata.get(\"iterators_min\")\n    iterators_max = input_params_metrics.metadata.get(\"iterators_max\")\n    iterators_step = input_params_metrics.metadata.get(\"iterators_step\")\n\n    if iterators_min is None or iterators_max is None or iterators_step is None:\n        raise ValueError(\"All iterators_min, iterators_max and iterators_step cannot be null!\")\n\n    try:\n        iterators_min = int(iterators_min)\n        iterators_max = int(iterators_max)\n        iterators_step = int(iterators_step)\n    except ValueError:\n        raise ValueError(\"All iterators_min, iterators_max and iterators_step needs to be a int!\")\n\n    parameters = [\n        V1beta1ParameterSpec(\n            name=\"it\",\n            parameter_type=\"int\",\n            feasible_space=V1beta1FeasibleSpace(\n                min=str(iterators_min),\n                max=str(iterators_max), \n                step=str(iterators_step)\n            )\n        )\n    ]\n\n    docker_image_name = input_params_metrics.metadata.get(\"docker_image_name\")\n    if docker_image_name is None:\n        raise ValueError(\"Docker image name cannot be null!\")\n\n    random_state = input_params_metrics.metadata.get(\"random_state\")\n    if random_state is None:\n        random_state = 42\n    else:\n        try:\n            random_state = int(random_state)\n        except ValueError:\n            raise ValueError(\"Random state needs to be an int!\")\n        \n    x_train_path = input_params_metrics.metadata.get(\"x_train_path\")\n    x_test_path = input_params_metrics.metadata.get(\"x_test_path\")\n    y_train_path = input_params_metrics.metadata.get(\"y_train_path\")\n    y_test_path = input_params_metrics.metadata.get(\"y_test_path\")\n\n    train_container = {\n        \"name\": \"training-container\",\n        \"image\": f\"docker.io/{docker_image_name}\",\n        \"command\": [\n            \"python3\",\n            \"/opt/lr/train.py\",\n            \"--it=${trialParameters.iterators}\",\n            f\"--rs={random_state}\",\n            f\"--x_train_path={x_train_path}\",\n            f\"--x_test_path={x_test_path}\",\n            f\"--y_train_path={y_train_path}\",\n            f\"--y_test_path={y_test_path}\",\n            f\"--save_model=false\",\n            f\"--model_folder_path=models\"\n        ]\n    }\n\n    template_spec = {\n        \"containers\": [\n            train_container\n        ],\n        \"restartPolicy\": \"Never\"\n    }\n\n    volumes = []\n    volumeMounts = []\n\n    datasets_from_pvc = input_params_metrics.metadata.get(\"datasets_from_pvc\")\n    datasets_pvc_name = input_params_metrics.metadata.get(\"datasets_pvc_name\")\n    datasets_pvc_mount_path = input_params_metrics.metadata.get(\"datasets_pvc_mount_path\")\n\n    datasets_from_pvc = bool(datasets_from_pvc)\n    \n    if datasets_from_pvc is True:\n        if datasets_pvc_name is None or datasets_pvc_mount_path is None:\n            raise ValueError(\"Both datasets_pvc_name and datasets_pvc_mount_path cannot be null\")\n\n        volumes.append({\n            \"name\": \"datasets\", \n            \"persistentVolumeClaim\": {\n                \"claimName\": datasets_pvc_name\n            }\n        })\n        volumeMounts.append({\n            \"name\": \"datasets\", \n            \"mountPath\": datasets_pvc_mount_path\n        })\n\n    '''\n    if save_model is True:\n        volumes.append({\n            \"name\": \"models\", \n            \"persistentVolumeClaim\": {\n                \"claimName\": models_pvc_name\n            }\n        })\n        volumeMounts.append({\n            \"name\": \"models\", \n            \"mountPath\": \"/opt/lr/models\"\n        })\n    '''\n\n    if datasets_from_pvc is True:\n        train_container[\"volumeMounts\"] = volumeMounts\n        template_spec[\"volumes\"] = volumes\n\n    trial_spec={\n        \"apiVersion\": \"batch/v1\",\n        \"kind\": \"Job\",\n        \"spec\": {\n            \"template\": {\n                \"metadata\": {\n                    \"annotations\": {\n                        \"sidecar.istio.io/inject\": \"false\"\n                    }\n                },\n                \"spec\": template_spec\n            }\n        }\n    }\n\n    trial_template=V1beta1TrialTemplate(\n        primary_container_name=\"training-container\",\n        trial_parameters=[\n            V1beta1TrialParameterSpec(\n                name=\"iterators\",\n                description=\"iterators for the training model\",\n                reference=\"it\"\n            )\n        ],\n        trial_spec=trial_spec,\n        retain=True\n    )\n\n    max_trial_counts = input_params_metrics.metadata.get(\"max_trial_counts\")\n    max_failed_trial_counts = input_params_metrics.metadata.get(\"max_failed_trial_counts\")\n    parallel_trial_counts = input_params_metrics.metadata.get(\"parallel_trial_counts\")\n\n    if max_failed_trial_counts is None or max_failed_trial_counts is None or parallel_trial_counts is None:\n        raise ValueError(\"All max_trial_counts, max_failed_trial_counts and parallel_trial_counts cannot be null!\")\n    \n    try:\n        max_trial_counts = int(max_trial_counts)\n        max_failed_trial_counts = int(max_failed_trial_counts)\n        parallel_trial_counts = int(parallel_trial_counts)\n    except ValueError:\n        raise ValueError(\"All max_trial_counts, max_failed_trial_counts and needs to be an int!\")\n\n    experiment = V1beta1Experiment(\n        api_version=\"kubeflow.org/v1beta1\",\n        kind=\"Experiment\",\n        metadata=metadata,\n        spec=V1beta1ExperimentSpec(\n            max_trial_count=max_trial_counts,\n            parallel_trial_count=parallel_trial_counts,\n            max_failed_trial_count=max_failed_trial_counts,\n            algorithm=algorithm_spec,\n            objective=objective_spec,\n            parameters=parameters,\n            trial_template=trial_template,\n        )\n    )\n\n    client_namespace = input_params_metrics.metadata.get(\"client_namespace\")\n    if client_namespace is None:\n        raise ValueError(\"Client namespace cannot be null!\")\n\n    client = KatibClient(namespace=client_namespace)\n    client.create_experiment(experiment=experiment)\n    client.wait_for_experiment_condition(name=experiment_name, namespace=experiment_namespace, timeout=3600)\n\n    result = client.get_optimal_hyperparameters(name=experiment_name, namespace=experiment_namespace).to_dict()\n\n    best_params_list = result[\"parameter_assignments\"]\n\n    for params in best_params_list:\n        name = params[\"name\"]\n        value = params[\"value\"]\n\n        if name == \"it\":\n            value = int(value)\n            \n        best_params_metrics.log_metric(metric=name, value=value)\n\n@dsl.component(\n    base_image='python:3.10-slim', \n    packages_to_install=['pandas', 'xgboost', 'scikit-learn', 'joblib']\n)\ndef run_xgboost_train(\n    best_params_metrics: Input[Metrics], \n    x_train: Input[Dataset], \n    x_test: Input[Dataset], \n    y_train: Input[Dataset], \n    y_test: Input[Dataset], \n    model: Output[Model], \n    file: Output[Artifact]\n):\n    import pandas as pd\n    import xgboost as xgb\n    from xgboost import XGBClassifier\n    import joblib\n    import json\n\n    from sklearn.metrics import accuracy_score\n\n    learning_rate = best_params_metrics.metadata.get(\"lr\")\n    n_estimators = best_params_metrics.metadata.get(\"ne\")\n\n    x_train_df = pd.read_csv(x_train.path)\n    y_train_df = pd.read_csv(y_train.path)\n    x_test_df = pd.read_csv(x_test.path)\n    y_test_df = pd.read_csv(y_test.path)\n\n    dtrain = xgb.DMatrix(x_train_df.values, label=y_train_df.values)\n    dtest = xgb.DMatrix(x_test_df.values, label=y_test_df.values)\n  \n    xgb_model = XGBClassifier(\n        n_estimators=n_estimators, \n        learning_rate= learning_rate\n    )\n    \n    xgb_model.fit(x_train_df, y_train_df.values.ravel())\n    \n    preds = xgb_model.predict(dtest)\n\n    predictions = [round(value) for value in preds]\n    xgb_accuracy = accuracy_score(y_test_df.values, predictions)\n    print('XGBoost Test accuracy:', xgb_accuracy)\n    \n    # Save the model\n    joblib.dump(xgb_model, model.path)\n\n     # Save the accuracy\n    data = {}\n    data['accuracy'] = xgb_accuracy\n    data['model_path'] = model.path\n\n    with open(file=file.path, mode='w', encoding='utf8') as file:\n        json.dump(data, file, indent=4)\n\n@dsl.component(\n    base_image='python:3.10-slim', \n    packages_to_install=['pandas', 'scikit-learn', 'joblib']\n)\ndef run_random_forest_train(\n    best_params_metrics: Input[Metrics], \n    x_train: Input[Dataset], \n    x_test: Input[Dataset], \n    y_train: Input[Dataset], \n    y_test: Input[Dataset], \n    model: Output[Model], \n    file: Output[Artifact]\n):\n    import pandas as pd\n    import joblib\n    import json\n\n    from sklearn.metrics import accuracy_score\n    from sklearn.ensemble import RandomForestClassifier\n\n    n_estimators = best_params_metrics.metadata.get(\"ne\")\n\n    x_train_df = pd.read_csv(x_train.path)\n    y_train_df = pd.read_csv(y_train.path)\n    x_test_df = pd.read_csv(x_test.path)\n    y_test_df = pd.read_csv(y_test.path)\n\n    rfc = RandomForestClassifier(n_estimators=n_estimators)\n    rfc.fit(x_train_df.values, y_train_df.values.ravel())\n\n    rfc.predict(x_test_df.values)\n    rfc_accuracy = rfc.score(x_test_df.values, y_test_df.values)\n\n    # Save the model\n    joblib.dump(rfc, model.path)\n\n    data = {}\n    data['accuracy'] = rfc_accuracy\n    data['model_path'] = model.path\n\n    with open(file=file.path, mode='w', encoding='utf8') as file:\n        json.dump(data, file, indent=4)\n\n@dsl.component(\n    base_image='python:3.10-slim', \n    packages_to_install=['pandas', 'scikit-learn', 'joblib']\n)\ndef run_knn_train(\n    best_params_metrics: Input[Metrics], \n    x_train: Input[Dataset], \n    x_test: Input[Dataset], \n    y_train: Input[Dataset], \n    y_test: Input[Dataset], \n    model: Output[Model], \n    file: Output[Artifact]\n):\n    import pandas as pd\n    import joblib\n    import json\n\n    from sklearn.metrics import accuracy_score\n    from sklearn.neighbors import KNeighborsClassifier\n\n    n_neighbors = best_params_metrics.metadata.get(\"nn\")\n\n    x_train_df = pd.read_csv(x_train.path)\n    y_train_df = pd.read_csv(y_train.path)\n    x_test_df = pd.read_csv(x_test.path)\n    y_test_df = pd.read_csv(y_test.path)\n\n    knn_model = KNeighborsClassifier(\n        n_neighbors=n_neighbors\n    )\n    knn_model.fit(x_train_df.values, y_train_df.values.ravel())\n\n    y_pred = knn_model.predict(x_test_df.values)\n    accuracy = accuracy_score(y_test_df.values, y_pred)\n\n    # Save the model\n    joblib.dump(knn_model, model.path)\n\n    data = {}\n    data['accuracy'] = accuracy\n    data['model_path'] = model.path\n\n    with open(file=file.path, mode='w', encoding='utf8') as file:\n        json.dump(data, file, indent=4)\n\n@dsl.component(\n    base_image='python:3.10-slim', \n    packages_to_install=['pandas', 'scikit-learn', 'joblib']\n)\ndef run_lr_train(\n    best_params_metrics: Input[Metrics], \n    x_train: Input[Dataset], \n    x_test: Input[Dataset], \n    y_train: Input[Dataset], \n    y_test: Input[Dataset], \n    model: Output[Model], \n    file: Output[Artifact]\n):\n    import pandas as pd\n    import joblib\n    import json\n\n    from sklearn.metrics import accuracy_score\n    from sklearn.linear_model import LogisticRegression\n\n    iterators = best_params_metrics.metadata.get(\"it\")\n\n    x_train_df = pd.read_csv(x_train.path)\n    y_train_df = pd.read_csv(y_train.path)\n    x_test_df = pd.read_csv(x_test.path)\n    y_test_df = pd.read_csv(y_test.path)\n\n    lr_model = LogisticRegression(\n        random_state=0, \n        max_iter=iterators\n    )\n    lr_model.fit(x_train_df.values, y_train_df.values.ravel())\n\n    y_pred = lr_model.predict(x_test_df.values)\n    accuracy = accuracy_score(y_test_df.values, y_pred)\n\n    # Save the model\n    joblib.dump(lr_model, model.path)\n\n    data = {}\n    data['accuracy'] = accuracy\n    data['model_path'] = model.path\n\n    with open(file=file.path, mode='w', encoding='utf8') as file:\n        json.dump(data, file, indent=4)\n\n@dsl.component(\n    base_image='python:3.10-slim',\n    packages_to_install=['joblib==1.4.2', 'scikit-learn==1.5.1', 'xgboost==2.0.3']# \n)\ndef choose_model(\n    LogisticRegression_model: Input[Model],\n    XGBoost_model: Input[Model],\n    RandomForest_model: Input[Model],\n    KNN_model: Input[Model],\n    lr_file: Input[Artifact],\n    xgb_file: Input[Artifact],\n    rf_file: Input[Artifact],\n    knn_file: Input[Artifact],\n    final_model: Output[Model],\n    result: Output[Artifact]\n) -> None:\n    import joblib\n    import json\n\n    # Define a dictionary to store model artifacts and their corresponding JSON files\n    models = {\n        'LogisticRegression': lr_file,\n        'XGBoost': xgb_file,\n        'RandomForest': rf_file,\n        'KNN': knn_file\n    }\n\n    accuracy = {}\n    model_paths = {}\n\n    # Read accuracies and model paths\n    for model_name, json_file in models.items():\n        with open(json_file.path, 'r') as f:\n            data = json.load(f)\n        accuracy[model_name] = data['accuracy']\n        model_paths[model_name] = data['model_path']\n\n    # Find the best model\n    best_model_name = max(accuracy, key=accuracy.get)\n    best_model = joblib.load(model_paths[best_model_name])\n    \n    # Save the best model\n    joblib.dump(best_model, final_model.path)\n\n    # Prepare result string\n    result_string = f'Best Model is {best_model_name} : {accuracy[best_model_name]}'\n    result_string += f'\\nAccuracy:\\n'\n    for model_name, acc in accuracy.items():\n        result_string += f'{model_name:17} : {acc}\\n'\n    print(result_string)\n\n    # Write the result to a file\n    data = {}\n    data['accuracy'] = accuracy[best_model_name]\n    data['model_path'] = final_model.path\n\n    with open(file=result.path, mode='w', encoding='utf8') as file:\n        json.dump(data, file, indent=4)\n\n@dsl.component(\n    base_image='python:3.10-slim',\n    packages_to_install=['joblib==1.4.2', 'scikit-learn==1.5.1', 'xgboost==2.0.3']\n)\ndef change_model(\n    old_model_path: str, \n    old_model_file_path: str, \n    new_model: Input[Model],\n    new_model_file: Input[Artifact], \n):\n    import joblib\n    import json\n    import os\n\n    with open(new_model_file.path, 'r') as f:\n        data_new = json.load(f)\n    new_model_accuracy = data_new['accuracy']\n\n    new_model_model = joblib.load(new_model.path)\n\n    if not (os.path.exists(old_model_path) and os.path.exists(old_model_file_path)):\n        joblib.dump(new_model_model, old_model_path)\n        with open(old_model_file_path, 'w', encoding='utf-8') as file:\n            json.dump(data_new, file, indent=4)\n        result_message = f\"New model saved to NAS. Accuracy: {new_model_accuracy}\"\n        return None\n    \n    try:\n        with open(old_model_file_path, 'r') as f:\n            data_old = json.load(f)\n        old_model_accuracy = data_old['accuracy']\n\n        if new_model_accuracy > old_model_accuracy:\n            joblib.dump(new_model_model, old_model_path)\n            with open(old_model_file_path, 'w', encoding='utf-8') as file:\n                json.dump(data_new, file, indent=4)\n            result_message = f\"Model updated. New accuracy: {new_model_accuracy}, Old accuracy: {old_model_accuracy}\"\n        else:\n            result_message = f\"Existing model retained. Existing accuracy: {old_model_accuracy}, New accuracy: {new_model_accuracy}\"\n    except (json.JSONDecodeError, UnicodeDecodeError):\n        joblib.dump(new_model_model, old_model_path)\n        with open(old_model_file_path, 'w', encoding='utf-8') as file:\n            json.dump(data_new, file, indent=4)\n        result_message = f\"New model saved to NAS. Accuracy: {new_model_accuracy}\"\n\n    print(result_message)\n\n@dsl.pipeline(\n    name=\"compose\", \n    description=\"Compose of kubeflow, katib and spark\"\n)\ndef compose_pipeline(\n    params_pvc_name: str = \"params-pvc\", \n    params_json_file_path: str = \"/mnt/params/params_heart_disease.json\", \n    models_pvc_name: str = \"models-pvc\"\n):\n    sparkapplication_dict = get_spark_job_definition()\n\n    k8s_apply_op = components.load_component_from_file(\"k8s-apply-component.yaml\")\n    apply_sparkapplication_task = k8s_apply_op(object=json.dumps(sparkapplication_dict))\n    apply_sparkapplication_task.set_caching_options(enable_caching=False)\n\n    check_sparkapplication_status_op = components.load_component_from_file(\"checkSparkapplication.yaml\")\n    check_sparkapplication_status_task = check_sparkapplication_status_op(\n        name=sparkapplication_dict[\"metadata\"][\"name\"],\n        namespace=sparkapplication_dict[\"metadata\"][\"namespace\"]\n    ).after(apply_sparkapplication_task)\n    check_sparkapplication_status_task.set_caching_options(enable_caching=False)\n\n    load_datasets_task = load_file_from_nas_to_minio(\n        x_train_input_path=\"/mnt/datasets/heart_disease/x_train.csv\", \n        x_test_input_path=\"/mnt/datasets/heart_disease/x_test.csv\", \n        y_train_input_path=\"/mnt/datasets/heart_disease/y_train.csv\", \n        y_test_input_path=\"/mnt/datasets/heart_disease/y_test.csv\", \n    ).after(check_sparkapplication_status_task)\n    load_datasets_task.set_caching_options(enable_caching=False)\n\n    kubernetes.mount_pvc(\n        task=load_datasets_task, \n        pvc_name=\"datasets-pvc\", \n        mount_path=\"/mnt/datasets\"\n    )\n\n    parse_input_json_task = parse_input_json(\n        json_file_path=params_json_file_path\n    ).after(load_datasets_task)\n    parse_input_json_task.set_caching_options(enable_caching=False)\n\n    kubernetes.mount_pvc(\n        task=parse_input_json_task, \n        pvc_name=params_pvc_name, \n        mount_path=\"/mnt/params\"\n    )\n\n    xgboost_katib_experiment_task = run_xgboost_katib_experiment(\n        input_params_metrics=parse_input_json_task.outputs[\"xgboost_input_metrics\"]\n    )\n\n    random_forest_katib_experiment_task = run_random_forest_katib_experiment(\n        input_params_metrics=parse_input_json_task.outputs[\"random_forest_input_metrics\"]\n    )\n\n    knn_katib_experiment_task = run_knn_katib_experiment(\n        input_params_metrics=parse_input_json_task.outputs[\"knn_input_metrics\"]\n    )\n\n    lr_katib_experiment_task = run_lr_katib_experiment(\n        input_params_metrics=parse_input_json_task.outputs[\"lr_input_metrics\"]\n    )\n\n    xgboost_train_task = run_xgboost_train(\n        best_params_metrics=xgboost_katib_experiment_task.outputs['best_params_metrics'], \n        x_train=load_datasets_task.outputs['x_train_output'], \n        x_test=load_datasets_task.outputs['x_test_output'], \n        y_train=load_datasets_task.outputs['y_train_output'], \n        y_test=load_datasets_task.outputs['y_test_output']\n    )\n\n    random_forest_train_task = run_random_forest_train(\n        best_params_metrics=random_forest_katib_experiment_task.outputs['best_params_metrics'], \n        x_train=load_datasets_task.outputs['x_train_output'], \n        x_test=load_datasets_task.outputs['x_test_output'], \n        y_train=load_datasets_task.outputs['y_train_output'], \n        y_test=load_datasets_task.outputs['y_test_output']\n    )\n\n    knn_train_task = run_knn_train(\n        best_params_metrics=knn_katib_experiment_task.outputs['best_params_metrics'], \n        x_train=load_datasets_task.outputs['x_train_output'], \n        x_test=load_datasets_task.outputs['x_test_output'], \n        y_train=load_datasets_task.outputs['y_train_output'], \n        y_test=load_datasets_task.outputs['y_test_output']\n    )\n\n    lr_train_task = run_lr_train(\n        best_params_metrics=lr_katib_experiment_task.outputs['best_params_metrics'], \n        x_train=load_datasets_task.outputs['x_train_output'], \n        x_test=load_datasets_task.outputs['x_test_output'], \n        y_train=load_datasets_task.outputs['y_train_output'], \n        y_test=load_datasets_task.outputs['y_test_output']\n    )\n\n    choose_model_task = choose_model(\n        LogisticRegression_model=lr_train_task.outputs['model'],\n        XGBoost_model=xgboost_train_task.outputs['model'],\n        RandomForest_model=random_forest_train_task.outputs['model'],\n        KNN_model=knn_train_task.outputs['model'],\n        lr_file=lr_train_task.outputs['file'],\n        xgb_file=xgboost_train_task.outputs['file'],\n        rf_file=random_forest_train_task.outputs['file'],\n        knn_file=knn_train_task.outputs['file']\n    )\n\n    change_model_task = change_model(\n        old_model_path=\"/mnt/models/heart_disease_model.pkl\", \n        old_model_file_path=\"/mnt/models/heart_disease_model.json\", \n        new_model=choose_model_task.outputs[\"final_model\"], \n        new_model_file=choose_model_task.outputs[\"result\"]\n    )\n\n    kubernetes.mount_pvc(\n        task=change_model_task, \n        pvc_name=models_pvc_name, \n        mount_path=\"/mnt/models\"\n    )\n\nif __name__ == \"__main__\":\n    compiler.Compiler().compile(compose_pipeline, \"../compose_pipeline.yaml\")"
  },
  {
    "repo": "s102401002/kubeflowPipeline0722",
    "file_path": "src/diabetes_prediction.py",
    "raw_url": "https://raw.githubusercontent.com/s102401002/kubeflowPipeline0722/main/src/diabetes_prediction.py",
    "content": "# test\n\nfrom typing import NewType\n\nimport kfp\nimport kfp.compiler\n\nfrom kfp import dsl\nfrom kfp.dsl import OutputPath, InputPath, Input, Output, Artifact, Model, Dataset\n\nfrom pandas import DataFrame\n# from kfp.components import func_to_container_op\n\nDF = NewType('DF', DataFrame)\n\n@dsl.component(\n    base_image='python:3.9',\n    packages_to_install=['pandas==2.2.2']\n)\ndef load_data(data_output: Output[Artifact]):\n    import pandas as pd\n    \n    urls = [\n        \"https://raw.githubusercontent.com/daniel88516/diabetes-data/main/10k.csv\",\n        \"https://raw.githubusercontent.com/s102401002/kubeflowPipeline/main/data1.csv\"\n    ]\n    \n    standard_name_mapping = {\n        'gender': ['gender', 'gen', 'Gender', 'sex', 'Sex'],\n        'age': ['age', 'Age', 'AGE'],\n        'bmi': ['bmi', 'BMI', 'Bmi'],\n        'HbA1c_level': ['HbA1c_level', 'HbA1c', 'hba1c'],\n        'blood_glucose_level': ['blood_glucose_level', 'glucose', 'BloodGlucose'],\n        'diabetes': ['diabetes', 'Diabetes']\n    }\n\n    datas = [] # download all the csv in urls as a array\n    for url in urls:\n        df = pd.read_csv(url)\n        for standard_name, variants in standard_name_mapping.items():\n            for variant in variants:\n                if variant in df.columns:\n                    df.rename(columns={variant: standard_name}, inplace=True) # inplace=True: changing directly instead of creating a new column\n                    break\n        \n        datas.append(df)\n\n    df_data = pd.concat(datas, ignore_index=True)\n    \n    df_data = df_data.drop(df_data[df_data['diabetes'] == 'No Info'].index)\n    df_data = df_data[['gender','age', 'bmi', 'HbA1c_level', 'blood_glucose_level', 'diabetes']]\n    df_data = df_data.dropna(thresh=4)\n    \n    gender_map = {'Male': 0 , 'Female': 1  , 'Other': 2}\n    df_data['gender'] = df_data['gender'].map(gender_map)\n    df_data = df_data[df_data['gender'] != 2]\n    df_data['age'] = df_data['age'].replace('No Info', df_data['age'].mean())\n    df_data['bmi'] = df_data['bmi'].replace('No Info', df_data['bmi'].mean())\n    df_data['HbA1c_level'] = df_data['HbA1c_level'].replace('No Info', df_data['HbA1c_level'].mean())\n    df_data['blood_glucose_level'] = df_data['blood_glucose_level'].replace('No Info', df_data['blood_glucose_level'].mean())\n\n    df_data.to_csv(data_output.path)\n\n@dsl.component(\n    base_image='python:3.9',\n    packages_to_install=['pandas==2.2.2', 'scikit-learn==1.5.1']\n)\ndef prepare_data(\n    data_input: Input[Artifact], \n    X_train_output: Output[Artifact], X_test_output: Output[Artifact],\n    Y_train_output: Output[Artifact], Y_test_output: Output[Artifact],\n    X_val_output: Output[Artifact], Y_val_output: Output[Artifact]\n):\n    import pandas as pd\n    from sklearn.model_selection import train_test_split\n\n    df_data = pd.read_csv(data_input.path)\n\n    X = df_data.drop(labels=['diabetes'], axis=1)\n    Y = df_data[['diabetes']]\n    \n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n    X_test, X_val, Y_test, Y_val = train_test_split(X_test, Y_test, test_size=0.5, random_state=42)\n    X_train.to_csv(X_train_output.path, index=False)\n    X_test.to_csv(X_test_output.path, index=False)\n    Y_train.to_csv(Y_train_output.path, index=False)\n    Y_test.to_csv(Y_test_output.path, index=False)\n    X_val.to_csv(X_val_output.path, index=False)\n    Y_val.to_csv(Y_val_output.path, index=False)\n\n@dsl.component(\n    base_image='python:3.9',\n    packages_to_install=['pandas==2.2.2', 'scikit-learn==1.5.1', 'joblib==1.4.2']\n)\ndef train_model_LogisticRegression(\n    X_train: Input[Artifact], \n    Y_train: Input[Artifact], \n    X_val: Input[Artifact], \n    Y_val: Input[Artifact],\n    X_test: Input[Artifact], \n    Y_test: Input[Artifact], \n    model: Output[Artifact],\n    file: Output[Artifact]\n):\n    import pandas as pd\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.metrics import accuracy_score\n    import joblib\n    import json\n    \n    X_train = pd.read_csv(X_train.path)\n    Y_train = pd.read_csv(Y_train.path)\n    X_val = pd.read_csv(X_val.path)\n    Y_val = pd.read_csv(Y_val.path)\n    X_test = pd.read_csv(X_test.path)\n    Y_test = pd.read_csv(Y_test.path)\n\n    # Logistic Regression\n    print('Logistic Regression')\n    lr_model = LogisticRegression(random_state=0, max_iter=10000)\n    lr_model.fit(X_train, Y_train.values.ravel())\n    print('Training accuracy:', lr_model.score(X_train, Y_train))\n    lr_accuracy = lr_model.score(X_test, Y_test)\n    print('Test accuracy:', lr_accuracy)\n    \n    # Save the model\n    joblib.dump(lr_model, model.path)\n    # Save the accuracy\n    jsonFile = open(file.path,'w')\n    data = {}\n    data['accuracy'] = lr_accuracy\n    data['model_path'] = model.path\n    json.dump(data, jsonFile, indent=2)\n@dsl.component(\n    base_image='python:3.9',\n    packages_to_install=['pandas==2.2.2', 'scikit-learn==1.5.1', 'joblib==1.4.2', 'xgboost==2.0.3']\n)\ndef train_model_xgboost(\n    X_train: Input[Artifact], \n    Y_train: Input[Artifact],\n    X_val: Input[Artifact], \n    Y_val: Input[Artifact],\n    X_test: Input[Artifact], \n    Y_test: Input[Artifact], \n    model: Output[Artifact],\n    file: Output[Artifact]\n):\n    import pandas as pd\n    import xgboost as xgb\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.metrics import accuracy_score\n    import joblib\n    import json\n    \n    X_train = pd.read_csv(X_train.path)\n    Y_train = pd.read_csv(Y_train.path)\n    X_val = pd.read_csv(X_val.path)\n    Y_val = pd.read_csv(Y_val.path)\n    X_test = pd.read_csv(X_test.path)\n    Y_test = pd.read_csv(Y_test.path)\n\n    dtrain = xgb.DMatrix(X_train, label=Y_train)\n    dval = xgb.DMatrix(X_val, label=Y_val)\n    dtest = xgb.DMatrix(X_test, label=Y_test)\n\n    scale_pos_weight = len(Y_train[Y_train == 0]) / len(Y_train[Y_train == 1])\n    param = {\n        'max_depth': 3,\n        'eta': 0.3,\n        'objective': 'binary:logistic',\n        'eval_metric': 'logloss',\n        'scale_pos_weight': scale_pos_weight\n    }\n    evallist = [(dtrain, 'train'), (dval, 'eval')]\n    num_round = 1000\n    xgb_model = xgb.train(param, dtrain, num_round, evallist, early_stopping_rounds=10)\n    preds = xgb_model.predict(dtest)\n    predictions = [round(value) for value in preds]\n    global xgb_accuracy \n    xgb_accuracy = accuracy_score(Y_test, predictions)\n    print('XGBoost Test accuracy:', xgb_accuracy)\n    \n    # Save the model\n    joblib.dump(xgb_model, model.path)\n\n     # Save the accuracy\n    jsonFile = open(file.path,'w')\n    data = {}\n    data['accuracy'] = xgb_accuracy\n    data['model_path'] = model.path\n    json.dump(data, jsonFile, indent=2)\n    # with open(accuracy.path, 'w') as f:\n    #     f.write(str(xgb_accuracy))\n\n# @dsl.component(\n#     base_image='python:3.9',\n#     packages_to_install=['pandas==2.2.2', 'scikit-learn==1.5.1', 'joblib==1.4.2']\n# )\n# def train_model_svm(\n#     X_train: Input[Artifact], \n#     Y_train: Input[Artifact],\n#     X_val: Input[Artifact], \n#     Y_val: Input[Artifact],\n#     X_test: Input[Artifact], \n#     Y_test: Input[Artifact], \n#     model: Output[Artifact],\n#     accuracy: Output[Artifact]\n# ):\n#     import pandas as pd\n#     from sklearn.metrics import accuracy_score\n#     from sklearn import svm\n#     import joblib\n    \n#     X_train = pd.read_csv(X_train.path)\n#     Y_train = pd.read_csv(Y_train.path)\n#     X_val = pd.read_csv(X_val.path)\n#     Y_val = pd.read_csv(Y_val.path)\n#     X_test = pd.read_csv(X_test.path)\n#     Y_test = pd.read_csv(Y_test.path)\n\n#     clf=svm.SVC(kernel='poly',gamma='auto',C=100)\n#     clf.fit(X_train,Y_train.values.ravel())\n#     clf.predict(X_test)\n#     svm_accuracy = clf.score(X_test, Y_test)\n#     # Save the model\n#     joblib.dump(model, clf.path)\n\n#      # Save the accuracy\n#     with open(accuracy.path, 'w') as f:\n#         f.write(str(svm_accuracy))\n\n@dsl.component(\n    base_image='python:3.9',\n    packages_to_install=['pandas==2.2.2', 'scikit-learn==1.5.1', 'joblib==1.4.2']\n)\ndef train_model_RandomForest(\n    X_train: Input[Artifact], \n    Y_train: Input[Artifact],\n    X_val: Input[Artifact], \n    Y_val: Input[Artifact],\n    X_test: Input[Artifact], \n    Y_test: Input[Artifact], \n    model: Output[Artifact],\n    file: Output[Artifact]\n):\n    import pandas as pd\n    from sklearn.metrics import accuracy_score\n    from sklearn.ensemble import RandomForestClassifier\n    import joblib\n    import json\n    \n    X_train = pd.read_csv(X_train.path)\n    Y_train = pd.read_csv(Y_train.path)\n    X_val = pd.read_csv(X_val.path)\n    Y_val = pd.read_csv(Y_val.path)\n    X_test = pd.read_csv(X_test.path)\n    Y_test = pd.read_csv(Y_test.path)\n\n    rfc=RandomForestClassifier(n_estimators=5)\n    rfc.fit(X_train,Y_train.values.ravel())    \n    y_predict=rfc.predict(X_test)\n    rfc.predict(X_test)\n    rf_accuracy = rfc.score(X_test,Y_test)\n    # Save the model\n    joblib.dump(rfc, model.path)\n\n     # Save the accuracy\n    jsonFile = open(file.path,'w')\n    data = {}\n    data['accuracy'] = rf_accuracy\n    data['model_path'] = model.path\n    json.dump(data, jsonFile, indent=2)\n\n@dsl.component(\n    base_image='python:3.9',\n    packages_to_install=['pandas==2.2.2', 'scikit-learn==1.5.1', 'joblib==1.4.2']\n)\ndef train_model_KNN(\n    X_train: Input[Artifact], \n    Y_train: Input[Artifact],\n    X_val: Input[Artifact], \n    Y_val: Input[Artifact],\n    X_test: Input[Artifact], \n    Y_test: Input[Artifact], \n    model: Output[Artifact],\n    file: Output[Artifact]\n):\n    import pandas as pd\n    from sklearn.neighbors import KNeighborsClassifier\n    import joblib\n    import json\n\n    X_train = pd.read_csv(X_train.path)\n    Y_train = pd.read_csv(Y_train.path)\n    X_val = pd.read_csv(X_val.path)\n    Y_val = pd.read_csv(Y_val.path)\n    X_test = pd.read_csv(X_test.path)\n    Y_test = pd.read_csv(Y_test.path)\n\n    knc = KNeighborsClassifier(n_neighbors=3)\n    knc.fit(X_train,Y_train.values.ravel())\n    knn_accuracy = knc.score(X_test,Y_test)\n    # Save the model\n    joblib.dump(knc, model.path)\n\n     # Save the accuracy\n    jsonFile = open(file.path,'w')\n    data = {}\n    data['accuracy'] = knn_accuracy\n    data['model_path'] = model.path\n    json.dump(data, jsonFile, indent=2)\n\n@dsl.component(\n    base_image='python:3.9',\n    packages_to_install=['joblib==1.4.2', 'scikit-learn==1.5.1', 'xgboost==2.0.3']# \n)\ndef choose_model(\n    LogisticRegression_model: Input[Artifact],\n    XGBoost_model: Input[Artifact],\n    # SVM_model: Input[Artifact],\n    RandomForest_model: Input[Artifact],\n    KNN_model: Input[Artifact],\n    lr_file: Input[Artifact],\n    xgb_file: Input[Artifact],\n    # svm_accuracy: Input[Artifact],\n    rf_file: Input[Artifact],\n    knn_file: Input[Artifact],\n    final_model: Output[Model],\n    result: Output[Artifact]\n) -> None:\n    import joblib\n    import json\n\n    # Define a dictionary to store model artifacts and their corresponding JSON files\n    models = {\n        'LogisticRegression': lr_file,\n        'XGBoost': xgb_file,\n        'RandomForest': rf_file,\n        'KNN': knn_file\n    }\n\n    accuracy = {}\n    model_paths = {}\n\n    # Read accuracies and model paths\n    for model_name, json_file in models.items():\n        with open(json_file.path, 'r') as f:\n            data = json.load(f)\n        accuracy[model_name] = data['accuracy']\n        model_paths[model_name] = data['model_path']\n\n    # Find the best model\n    best_model_name = max(accuracy, key=accuracy.get)\n    best_model = joblib.load(model_paths[best_model_name])\n    \n    # Save the best model\n    joblib.dump(best_model, final_model.path)\n\n    # Prepare result string\n    result_string = f'Best Model is {best_model_name} : {accuracy[best_model_name]}'\n    result_string += f'\\nAccuracy:\\n'\n    for model_name, acc in accuracy.items():\n        result_string += f'{model_name:17} : {acc}\\n'\n    print(result_string)\n\n    # Write the result to a file\n    with open(result.path, 'w') as f:\n        f.write(result_string)\n@dsl.pipeline(\n    name='Diabete Prediction Pipeline',\n    description='Using Kubeflow pipeline to train and evaluate a Diabete prediction model'\n)\ndef Diabete_prediction_pipeline():\n    # Load data\n    load_data_task = load_data()\n\n    # Prepare data\n    prepare_data_task = prepare_data(data_input=load_data_task.outputs['data_output'])\n    \n    # Train models\n    train_model_LogisticRegression_task = train_model_LogisticRegression(\n        X_train=prepare_data_task.outputs['X_train_output'], \n        Y_train=prepare_data_task.outputs['Y_train_output'],\n        X_val=prepare_data_task.outputs['X_val_output'],\n        Y_val=prepare_data_task.outputs['Y_val_output'],\n        X_test=prepare_data_task.outputs['X_test_output'], \n        Y_test=prepare_data_task.outputs['Y_test_output']\n    )\n\n    train_model_xgboost_task = train_model_xgboost(\n        X_train=prepare_data_task.outputs['X_train_output'], \n        Y_train=prepare_data_task.outputs['Y_train_output'],\n        X_val=prepare_data_task.outputs['X_val_output'],\n        Y_val=prepare_data_task.outputs['Y_val_output'],\n        X_test=prepare_data_task.outputs['X_test_output'], \n        Y_test=prepare_data_task.outputs['Y_test_output']\n    )\n    \n    # train_model_svm_task = train_model_svm(\n    #     X_train=prepare_data_task.outputs['X_train_output'], \n    #     Y_train=prepare_data_task.outputs['Y_train_output'],\n    #     X_val=prepare_data_task.outputs['X_val_output'],\n    #     Y_val=prepare_data_task.outputs['Y_val_output'],\n    #     X_test=prepare_data_task.outputs['X_test_output'], \n    #     Y_test=prepare_data_task.outputs['Y_test_output']\n    # )\n\n    train_model_RandomForest_task = train_model_RandomForest(\n        X_train=prepare_data_task.outputs['X_train_output'], \n        Y_train=prepare_data_task.outputs['Y_train_output'],\n        X_val=prepare_data_task.outputs['X_val_output'],\n        Y_val=prepare_data_task.outputs['Y_val_output'],\n        X_test=prepare_data_task.outputs['X_test_output'], \n        Y_test=prepare_data_task.outputs['Y_test_output']\n    )\n\n    train_model_KNN_task = train_model_KNN(\n        X_train=prepare_data_task.outputs['X_train_output'], \n        Y_train=prepare_data_task.outputs['Y_train_output'],\n        X_val=prepare_data_task.outputs['X_val_output'],\n        Y_val=prepare_data_task.outputs['Y_val_output'],\n        X_test=prepare_data_task.outputs['X_test_output'], \n        Y_test=prepare_data_task.outputs['Y_test_output']\n    )\n\n    choose_model_task = choose_model(\n        LogisticRegression_model=train_model_LogisticRegression_task.outputs['model'],\n        XGBoost_model=train_model_xgboost_task.outputs['model'],\n        # SVM_model=train_model_svm_task.outputs['model'],\n        RandomForest_model=train_model_RandomForest_task.outputs['model'],\n        KNN_model=train_model_KNN_task.outputs['model'],\n        lr_file=train_model_LogisticRegression_task.outputs['file'],\n        xgb_file=train_model_xgboost_task.outputs['file'],\n        # svm_file=train_model_svm_task.outputs['file'],\n        rf_file=train_model_RandomForest_task.outputs['file'],\n        knn_file=train_model_KNN_task.outputs['file']\n    )\n    \n    # The pipeline doesn't need to return anything explicitly now\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(Diabete_prediction_pipeline, '../Diabete_prediction_pipeline.yaml')"
  },
  {
    "repo": "s102401002/kubeflowPipeline0722",
    "file_path": "src/hypertension_prediction.py",
    "raw_url": "https://raw.githubusercontent.com/s102401002/kubeflowPipeline0722/main/src/hypertension_prediction.py",
    "content": "from typing import NewType\n\nimport kfp\nimport kfp.compiler\n\nfrom kfp import dsl\nfrom kfp.dsl import OutputPath, InputPath, Input, Output, Artifact, Model, Dataset \n\nfrom pandas import DataFrame\n# from kfp.components import func_to_container_op\n\nDF = NewType('DF', DataFrame)\n\n@dsl.component(\n    base_image='python:3.9',\n    packages_to_install=['pandas==2.2.2']\n)\ndef load_data(data_output: Output[Artifact]):\n    import pandas as pd\n\n    # \u8b80\u53d6\u8207\u7a0b\u5f0f\u78bc\u4f4d\u65bc\u540c\u4e00\u500b\u8cc7\u6599\u593e\u4e2d\u7684 hypertension.csv\n    df_data = pd.read_csv('https://raw.githubusercontent.com/s102401002/kubeflowPipeline0722/main/hypertension_data.csv')\n\n    # \u79fb\u9664\u4e0d\u9700\u8981\u7684\u6b04\u4f4d\n    df_data = df_data.drop(columns=['cp','thal'])\n\n\n    # \u88dc\u9f4a\u8cc7\u6599\n    df_data = df_data[(df_data['age'] != 'N/A') & (~df_data['age'].isna())]\n    df_data = df_data[(df_data['sex'] != 'N/A') & (~df_data['sex'].isna())]\n    df_data = df_data[(df_data['trestbps'] != 'N/A') & (~df_data['trestbps'].isna())] #\u975c\u6b62\u8840\u58d3\n    df_data = df_data[(df_data['chol'] != 'N/A') & (~df_data['chol'].isna())] #\u8840\u6e05\u81bd\u56fa\u9187\n    df_data = df_data[(df_data['fbs'] != 'N/A') & (~df_data['fbs'].isna())] #\u662f\u5426\u60a3\u8005\u7684\u7a7a\u8179\u8840\u7cd6 > 120\n    df_data = df_data[(df_data['restecg'] != 'N/A') & (~df_data['restecg'].isna())] #ST-T \u6ce2\u662f\u5426\u7570\u5e38\n    df_data = df_data[df_data['restecg'] != 2]\n    df_data = df_data[(df_data['thalach'] != 'N/A') & (~df_data['thalach'].isna())] #\u6700\u5927\u5fc3\u7387\n    df_data = df_data[(df_data['exang'] != 'N/A') & (~df_data['exang'].isna())] #\u662f\u5426\u904b\u52d5\u8a98\u767c\u5fc3\u7d5e\u75db\n    df_data = df_data[(df_data['oldpeak'] != 'N/A') & (~df_data['oldpeak'].isna())] #\u904b\u52d5\u76f8\u5c0d\u65bc\u4f11\u606f\u5f15\u8d77\u7684 ST \u3002\n    df_data = df_data[(df_data['slope'] != 'N/A') & (~df_data['slope'].isna())] #\u904b\u52d5\u5cf0\u503c ST \u6bb5\u659c\u7387\uff1a0\uff1a\u4e0a\u5761 1\uff1a\u5e73\u5766 2\uff1a\u4e0b\u5761\n    df_data = df_data[(df_data['ca'] != 'N/A') & (~df_data['ca'].isna())]#\u87a2\u5149\u6aa2\u67e5\u8457\u8272\u7684\u4e3b\u8981\u8840\u7ba1\u6578\u91cf (0\u20134)\n    df_data = df_data[(df_data['target'] != 'N/A') & (~df_data['target'].isna())]\n    df_data.to_csv(data_output.path)\n\n@dsl.component(\n    base_image='python:3.9',\n    packages_to_install=['pandas==2.2.2', 'scikit-learn==1.5.1']\n)\ndef prepare_data(\n    data_input: Input[Artifact], \n    X_train_output: Output[Artifact], X_test_output: Output[Artifact],\n    Y_train_output: Output[Artifact], Y_test_output: Output[Artifact],\n    X_val_output: Output[Artifact], Y_val_output: Output[Artifact]\n):\n    import pandas as pd\n    from sklearn.model_selection import train_test_split\n\n    df_data = pd.read_csv(data_input.path)\n\n    X = df_data.drop(labels=['target'], axis=1)\n    Y = df_data[['target']]\n    \n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n    X_test, X_val, Y_test, Y_val = train_test_split(X_test, Y_test, test_size=0.5, random_state=42)\n    X_train.to_csv(X_train_output.path, index=False)\n    X_test.to_csv(X_test_output.path, index=False)\n    Y_train.to_csv(Y_train_output.path, index=False)\n    Y_test.to_csv(Y_test_output.path, index=False)\n    X_val.to_csv(X_val_output.path, index=False)\n    Y_val.to_csv(Y_val_output.path, index=False)\n\n\n@dsl.component(\n    base_image='python:3.9',\n    packages_to_install=['pandas==2.2.2', 'scikit-learn==1.5.1', 'joblib==1.4.2']\n)\ndef train_model_LogisticRegression(\n    X_train: Input[Artifact], \n    Y_train: Input[Artifact], \n    X_val: Input[Artifact], \n    Y_val: Input[Artifact],\n    X_test: Input[Artifact], \n    Y_test: Input[Artifact], \n    model: Output[Artifact],\n    file: Output[Artifact]\n):\n    import pandas as pd\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.metrics import accuracy_score\n    import joblib\n    import json\n    \n    X_train = pd.read_csv(X_train.path)\n    Y_train = pd.read_csv(Y_train.path)\n    X_val = pd.read_csv(X_val.path)\n    Y_val = pd.read_csv(Y_val.path)\n    X_test = pd.read_csv(X_test.path)\n    Y_test = pd.read_csv(Y_test.path)\n\n    # Logistic Regression\n    print('Logistic Regression')\n    lr_model = LogisticRegression(random_state=0, max_iter=10000)\n    lr_model.fit(X_train, Y_train.values.ravel())\n    print('Training accuracy:', lr_model.score(X_train, Y_train))\n    lr_accuracy = lr_model.score(X_test, Y_test)\n    print('Test accuracy:', lr_accuracy)\n    \n    # Save the model\n    joblib.dump(lr_model, model.path)\n    # Save the accuracy\n    jsonFile = open(file.path,'w')\n    data = {}\n    data['accuracy'] = lr_accuracy\n    data['model_path'] = model.path\n    json.dump(data, jsonFile, indent=2)\n@dsl.component(\n    base_image='python:3.9',\n    packages_to_install=['pandas==2.2.2', 'scikit-learn==1.5.1', 'joblib==1.4.2', 'xgboost==2.0.3']\n)\ndef train_model_xgboost(\n    X_train: Input[Artifact], \n    Y_train: Input[Artifact],\n    X_val: Input[Artifact], \n    Y_val: Input[Artifact],\n    X_test: Input[Artifact], \n    Y_test: Input[Artifact], \n    model: Output[Artifact],\n    file: Output[Artifact]\n):\n    import pandas as pd\n    import xgboost as xgb\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.metrics import accuracy_score\n    import joblib\n    import json\n    \n    X_train = pd.read_csv(X_train.path)\n    Y_train = pd.read_csv(Y_train.path)\n    X_val = pd.read_csv(X_val.path)\n    Y_val = pd.read_csv(Y_val.path)\n    X_test = pd.read_csv(X_test.path)\n    Y_test = pd.read_csv(Y_test.path)\n\n    dtrain = xgb.DMatrix(X_train, label=Y_train)\n    dval = xgb.DMatrix(X_val, label=Y_val)\n    dtest = xgb.DMatrix(X_test, label=Y_test)\n\n    scale_pos_weight = len(Y_train[Y_train == 0]) / len(Y_train[Y_train == 1])\n    param = {\n        'max_depth': 3,\n        'eta': 0.3,\n        'objective': 'binary:logistic',\n        'eval_metric': 'logloss',\n        'scale_pos_weight': scale_pos_weight\n    }\n    evallist = [(dtrain, 'train'), (dval, 'eval')]\n    num_round = 1000\n    xgb_model = xgb.train(param, dtrain, num_round, evallist, early_stopping_rounds=10)\n    preds = xgb_model.predict(dtest)\n    predictions = [round(value) for value in preds]\n    global xgb_accuracy \n    xgb_accuracy = accuracy_score(Y_test, predictions)\n    print('XGBoost Test accuracy:', xgb_accuracy)\n    \n    # Save the model\n    joblib.dump(xgb_model, model.path)\n\n     # Save the accuracy\n    jsonFile = open(file.path,'w')\n    data = {}\n    data['accuracy'] = xgb_accuracy\n    data['model_path'] = model.path\n    json.dump(data, jsonFile, indent=2)\n    # with open(accuracy.path, 'w') as f:\n    #     f.write(str(xgb_accuracy))\n\n# @dsl.component(\n#     base_image='python:3.9',\n#     packages_to_install=['pandas==2.2.2', 'scikit-learn==1.5.1', 'joblib==1.4.2']\n# )\n# def train_model_svm(\n#     X_train: Input[Artifact], \n#     Y_train: Input[Artifact],\n#     X_val: Input[Artifact], \n#     Y_val: Input[Artifact],\n#     X_test: Input[Artifact], \n#     Y_test: Input[Artifact], \n#     model: Output[Artifact],\n#     accuracy: Output[Artifact]\n# ):\n#     import pandas as pd\n#     from sklearn.metrics import accuracy_score\n#     from sklearn import svm\n#     import joblib\n    \n#     X_train = pd.read_csv(X_train.path)\n#     Y_train = pd.read_csv(Y_train.path)\n#     X_val = pd.read_csv(X_val.path)\n#     Y_val = pd.read_csv(Y_val.path)\n#     X_test = pd.read_csv(X_test.path)\n#     Y_test = pd.read_csv(Y_test.path)\n\n#     clf=svm.SVC(kernel='poly',gamma='auto',C=100)\n#     clf.fit(X_train,Y_train.values.ravel())\n#     clf.predict(X_test)\n#     svm_accuracy = clf.score(X_test, Y_test)\n#     # Save the model\n#     joblib.dump(model, clf.path)\n\n#      # Save the accuracy\n#     with open(accuracy.path, 'w') as f:\n#         f.write(str(svm_accuracy))\n\n@dsl.component(\n    base_image='python:3.9',\n    packages_to_install=['pandas==2.2.2', 'scikit-learn==1.5.1', 'joblib==1.4.2']\n)\ndef train_model_RandomForest(\n    X_train: Input[Artifact], \n    Y_train: Input[Artifact],\n    X_val: Input[Artifact], \n    Y_val: Input[Artifact],\n    X_test: Input[Artifact], \n    Y_test: Input[Artifact], \n    model: Output[Artifact],\n    file: Output[Artifact]\n):\n    import pandas as pd\n    from sklearn.metrics import accuracy_score\n    from sklearn.ensemble import RandomForestClassifier\n    import joblib\n    import json\n    \n    X_train = pd.read_csv(X_train.path)\n    Y_train = pd.read_csv(Y_train.path)\n    X_val = pd.read_csv(X_val.path)\n    Y_val = pd.read_csv(Y_val.path)\n    X_test = pd.read_csv(X_test.path)\n    Y_test = pd.read_csv(Y_test.path)\n\n    rfc=RandomForestClassifier(n_estimators=5)\n    rfc.fit(X_train,Y_train.values.ravel())    \n    y_predict=rfc.predict(X_test)\n    rfc.predict(X_test)\n    rf_accuracy = rfc.score(X_test,Y_test)\n    # Save the model\n    joblib.dump(rfc, model.path)\n\n     # Save the accuracy\n    jsonFile = open(file.path,'w')\n    data = {}\n    data['accuracy'] = rf_accuracy\n    data['model_path'] = model.path\n    json.dump(data, jsonFile, indent=2)\n\n@dsl.component(\n    base_image='python:3.9',\n    packages_to_install=['pandas==2.2.2', 'scikit-learn==1.5.1', 'joblib==1.4.2']\n)\ndef train_model_KNN(\n    X_train: Input[Artifact], \n    Y_train: Input[Artifact],\n    X_val: Input[Artifact], \n    Y_val: Input[Artifact],\n    X_test: Input[Artifact], \n    Y_test: Input[Artifact], \n    model: Output[Artifact],\n    file: Output[Artifact]\n):\n    import pandas as pd\n    from sklearn.neighbors import KNeighborsClassifier\n    import joblib\n    import json\n\n    X_train = pd.read_csv(X_train.path)\n    Y_train = pd.read_csv(Y_train.path)\n    X_val = pd.read_csv(X_val.path)\n    Y_val = pd.read_csv(Y_val.path)\n    X_test = pd.read_csv(X_test.path)\n    Y_test = pd.read_csv(Y_test.path)\n\n    knc = KNeighborsClassifier(n_neighbors=3)\n    knc.fit(X_train,Y_train.values.ravel())\n    knn_accuracy = knc.score(X_test,Y_test)\n    # Save the model\n    joblib.dump(knc, model.path)\n\n     # Save the accuracy\n    jsonFile = open(file.path,'w')\n    data = {}\n    data['accuracy'] = knn_accuracy\n    data['model_path'] = model.path\n    json.dump(data, jsonFile, indent=2)\n\n@dsl.component(\n    base_image='python:3.9',\n    packages_to_install=['joblib==1.4.2', 'scikit-learn==1.5.1', 'xgboost==2.0.3']# \n)\ndef choose_model(\n    LogisticRegression_model: Input[Artifact],\n    XGBoost_model: Input[Artifact],\n    # SVM_model: Input[Artifact],\n    RandomForest_model: Input[Artifact],\n    KNN_model: Input[Artifact],\n    lr_file: Input[Artifact],\n    xgb_file: Input[Artifact],\n    # svm_accuracy: Input[Artifact],\n    rf_file: Input[Artifact],\n    knn_file: Input[Artifact],\n    final_model: Output[Model],\n    result: Output[Artifact]\n) -> None:\n    import joblib\n    import json\n\n    # Define a dictionary to store model artifacts and their corresponding JSON files\n    models = {\n        'LogisticRegression': lr_file,\n        'XGBoost': xgb_file,\n        'RandomForest': rf_file,\n        'KNN': knn_file\n    }\n\n    accuracy = {}\n    model_paths = {}\n\n    # Read accuracies and model paths\n    for model_name, json_file in models.items():\n        with open(json_file.path, 'r') as f:\n            data = json.load(f)\n        accuracy[model_name] = data['accuracy']\n        model_paths[model_name] = data['model_path']\n\n    # Find the best model\n    best_model_name = max(accuracy, key=accuracy.get)\n    best_model = joblib.load(model_paths[best_model_name])\n    \n    # Save the best model\n    joblib.dump(best_model, final_model.path)\n\n    # Prepare result string\n    result_string = f'Best Model is {best_model_name} : {accuracy[best_model_name]}'\n    result_string += f'\\nAccuracy:\\n'\n    for model_name, acc in accuracy.items():\n        result_string += f'{model_name:17} : {acc}\\n'\n    print(result_string)\n\n\n    # Write the result to a file\n    with open(result.path, 'w') as f:\n        f.write(result_string)\n@dsl.pipeline(\n    name='Hypertension Prediction Pipeline',\n    description='Using Kubeflow pipeline to train and evaluate a Hypertension prediction model'\n)\ndef hypertension_prediction_pipeline():\n    # Load data\n    load_data_task = load_data()\n\n    # Prepare data\n    prepare_data_task = prepare_data(data_input=load_data_task.outputs['data_output'])\n    \n    # Train models\n    train_model_LogisticRegression_task = train_model_LogisticRegression(\n        X_train=prepare_data_task.outputs['X_train_output'], \n        Y_train=prepare_data_task.outputs['Y_train_output'],\n        X_val=prepare_data_task.outputs['X_val_output'],\n        Y_val=prepare_data_task.outputs['Y_val_output'],\n        X_test=prepare_data_task.outputs['X_test_output'], \n        Y_test=prepare_data_task.outputs['Y_test_output']\n    )\n\n    train_model_xgboost_task = train_model_xgboost(\n        X_train=prepare_data_task.outputs['X_train_output'], \n        Y_train=prepare_data_task.outputs['Y_train_output'],\n        X_val=prepare_data_task.outputs['X_val_output'],\n        Y_val=prepare_data_task.outputs['Y_val_output'],\n        X_test=prepare_data_task.outputs['X_test_output'], \n        Y_test=prepare_data_task.outputs['Y_test_output']\n    )\n    \n    # train_model_svm_task = train_model_svm(\n    #     X_train=prepare_data_task.outputs['X_train_output'], \n    #     Y_train=prepare_data_task.outputs['Y_train_output'],\n    #     X_val=prepare_data_task.outputs['X_val_output'],\n    #     Y_val=prepare_data_task.outputs['Y_val_output'],\n    #     X_test=prepare_data_task.outputs['X_test_output'], \n    #     Y_test=prepare_data_task.outputs['Y_test_output']\n    # )\n\n    train_model_RandomForest_task = train_model_RandomForest(\n        X_train=prepare_data_task.outputs['X_train_output'], \n        Y_train=prepare_data_task.outputs['Y_train_output'],\n        X_val=prepare_data_task.outputs['X_val_output'],\n        Y_val=prepare_data_task.outputs['Y_val_output'],\n        X_test=prepare_data_task.outputs['X_test_output'], \n        Y_test=prepare_data_task.outputs['Y_test_output']\n    )\n\n    train_model_KNN_task = train_model_KNN(\n        X_train=prepare_data_task.outputs['X_train_output'], \n        Y_train=prepare_data_task.outputs['Y_train_output'],\n        X_val=prepare_data_task.outputs['X_val_output'],\n        Y_val=prepare_data_task.outputs['Y_val_output'],\n        X_test=prepare_data_task.outputs['X_test_output'], \n        Y_test=prepare_data_task.outputs['Y_test_output']\n    )\n\n    choose_model_task = choose_model(\n        LogisticRegression_model=train_model_LogisticRegression_task.outputs['model'],\n        XGBoost_model=train_model_xgboost_task.outputs['model'],\n        # SVM_model=train_model_svm_task.outputs['model'],\n        RandomForest_model=train_model_RandomForest_task.outputs['model'],\n        KNN_model=train_model_KNN_task.outputs['model'],\n        lr_file=train_model_LogisticRegression_task.outputs['file'],\n        xgb_file=train_model_xgboost_task.outputs['file'],\n        # svm_file=train_model_svm_task.outputs['file'],\n        rf_file=train_model_RandomForest_task.outputs['file'],\n        knn_file=train_model_KNN_task.outputs['file']\n    )\n    \n    # The pipeline doesn't need to return anything explicitly now\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(hypertension_prediction_pipeline, '../Hypertension_prediction_pipeline.yaml')"
  },
  {
    "repo": "s102401002/kubeflowPipeline0722",
    "file_path": "src/kubeflowPipeline0722.py",
    "raw_url": "https://raw.githubusercontent.com/s102401002/kubeflowPipeline0722/main/src/kubeflowPipeline0722.py",
    "content": "from typing import NewType\n\nimport kfp\nimport kfp.compiler\n\nfrom kfp import dsl\nfrom kfp.dsl import OutputPath, InputPath, Input, Output, Artifact\n\nfrom pandas import DataFrame\n# from kfp.components import func_to_container_op\n\nDF = NewType('DF', DataFrame)\n\n@dsl.component(\n    base_image='python:3.9',\n    packages_to_install=['pandas==2.2.2']\n)\ndef load_data(data_output: Output[Artifact]):\n    import pandas as pd\n    \n    urls = [\n        \"https://raw.githubusercontent.com/daniel88516/diabetes-data/main/10k.csv\",\n        \"https://raw.githubusercontent.com/s102401002/kubeflowPipeline/main/data1.csv\"\n    ]\n    \n    standard_name_mapping = {\n        'gender': ['gender', 'gen', 'Gender', 'sex', 'Sex'],\n        'age': ['age', 'Age', 'AGE'],\n        'bmi': ['bmi', 'BMI', 'Bmi', 'bodymassindex'],\n        'HbA1c_level': ['HbA1c_level', 'HbA1c', 'hba1c'],\n        'blood_glucose_level': ['blood_glucose_level', 'glucose', 'BloodGlucose'],\n        'diabetes': ['diabetes', 'Diabetes']\n    }\n\n    datas = [] # download all the csv in urls as a array\n    for url in urls:\n        df = pd.read_csv(url)\n        for standard_name, variants in standard_name_mapping.items():\n            for variant in variants:\n                if variant in df.columns:\n                    df.rename(columns={variant: standard_name}, inplace=True) # inplace=True: changing directly instead of creating a new column\n                    break\n        \n        datas.append(df)\n\n    df_data = pd.concat(datas, ignore_index=True)\n    \n    df_data = df_data.drop(df_data[df_data['diabetes'] == 'No Info'].index)\n    df_data = df_data[['gender','age', 'bmi', 'HbA1c_level', 'blood_glucose_level', 'diabetes']]\n    df_data = df_data.dropna(thresh=4)\n    \n    gender_map = {'Male': 0 , 'Female': 1  , 'Other': 2}\n    df_data['gender'] = df_data['gender'].map(gender_map)\n    df_data = df_data[df_data['gender'] != 2]\n    df_data['age'] = df_data['age'].replace('No Info', df_data['age'].mean())\n    df_data['bmi'] = df_data['bmi'].replace('No Info', df_data['bmi'].mean())\n    df_data['HbA1c_level'] = df_data['HbA1c_level'].replace('No Info', df_data['HbA1c_level'].mean())\n    df_data['blood_glucose_level'] = df_data['blood_glucose_level'].replace('No Info', df_data['blood_glucose_level'].mean())\n\n    df_data.to_csv(data_output.path)\n\n@dsl.component(\n    base_image='python:3.9',\n    packages_to_install=['pandas==2.2.2', 'scikit-learn==1.5.1']\n)\ndef prepare_data(\n    data_input: Input[Artifact], \n    x_train_output: Output[Artifact], x_test_output: Output[Artifact],\n    y_train_output: Output[Artifact], y_test_output: Output[Artifact]\n):\n    import pandas as pd\n    from sklearn.model_selection import train_test_split\n\n    df_data = pd.read_csv(data_input.path)\n\n    x = df_data.drop(labels=['diabetes'], axis=1)\n    y = df_data[['diabetes']]\n    \n    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n    \n    x_train_df = pd.DataFrame(x_train)\n    x_test_df = pd.DataFrame(x_test)\n    y_train_df = pd.DataFrame(y_train)\n    y_test_df = pd.DataFrame(y_test)\n\n    x_train_df.to_csv(x_train_output.path, index=False)\n    x_test_df.to_csv(x_test_output.path, index=False)\n    y_train_df.to_csv(y_train_output.path, index=False)\n    y_test_df.to_csv(y_test_output.path, index=False)\n\n@dsl.component(\n    base_image='python:3.9',\n    packages_to_install=['pandas==2.2.2', 'scikit-learn==1.5.1', 'joblib==1.4.2']\n)\ndef train_model(x_train: Input[Artifact], y_train: Input[Artifact], train_model_output: Output[Artifact]):\n    import pandas as pd\n    from sklearn.linear_model import LogisticRegression\n    import joblib\n    \n    x_train = pd.read_csv(x_train.path)\n    y_train = pd.read_csv(y_train.path)\n    \n    model = LogisticRegression(random_state=0, max_iter=10000) # 100 times for test p.s. it is 10000 times in beginning\n    model.fit(x_train, y_train)\n    \n    #model_path = './diabete_prediction_model.pkl'\n    joblib.dump(model, train_model_output.path)\n\n@dsl.component(\n    base_image='python:3.9',\n    packages_to_install=['pandas==2.2.2', 'scikit-learn==1.5.1', 'joblib==1.4.2']\n)\ndef evaluate_model(model_path: Input[Artifact], x_test: Input[Artifact], y_test: Input[Artifact]) -> str:\n    import pandas as pd\n    import sklearn\n    import joblib\n\n    model = joblib.load(filename=model_path.path)\n\n    x_test_df = pd.read_csv(x_test.path)\n    y_test_df = pd.read_csv(y_test.path)\n    \n    accuracy = model.score(x_test_df, y_test_df)\n    \n    return f'Test accuracy: {accuracy}'\n\n@dsl.pipeline(\n    name='Diabetes Prediction Pipeline',\n    description='Using kubeflow pipeline to train and evaluate a diabetes prediction model'\n)\ndef diabetes_prediction_pipeline() -> str:\n    load_data_task = load_data()\n\n    prepare_data_task = prepare_data(data_input=load_data_task.outputs['data_output'])\n    \n    train_model_task = train_model(\n        x_train = prepare_data_task.outputs['x_train_output'], \n        y_train = prepare_data_task.outputs['y_train_output']\n    )\n    \n    evaluate_model_task = evaluate_model(\n        model_path = train_model_task.outputs['train_model_output'], \n        x_test = prepare_data_task.outputs['x_test_output'], \n        y_test = prepare_data_task.outputs['y_test_output']\n    )\n    \n    return evaluate_model_task.output\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(diabetes_prediction_pipeline, 'diabetes_prediction_pipeline.yaml')"
  },
  {
    "repo": "s102401002/kubeflowPipeline0722",
    "file_path": "src/kubeflowPipeline_xgboost.py",
    "raw_url": "https://raw.githubusercontent.com/s102401002/kubeflowPipeline0722/main/src/kubeflowPipeline_xgboost.py",
    "content": "# test\n\nfrom typing import NewType\n\nimport kfp\nimport kfp.compiler\n\nfrom kfp import dsl\nfrom kfp.dsl import OutputPath, InputPath, Input, Output, Artifact\n\nfrom pandas import DataFrame\n# from kfp.components import func_to_container_op\n\nDF = NewType('DF', DataFrame)\n\n@dsl.component(\n    base_image='python:3.9',\n    packages_to_install=['pandas==2.2.2']\n)\ndef load_data(data_output: Output[Artifact]):\n    import pandas as pd\n    \n    urls = [\n        \"https://raw.githubusercontent.com/daniel88516/diabetes-data/main/10k.csv\",\n        \"https://raw.githubusercontent.com/s102401002/kubeflowPipeline/main/data1.csv\"\n    ]\n    \n    standard_name_mapping = {\n        'gender': ['gender', 'gen', 'Gender', 'sex', 'Sex'],\n        'age': ['age', 'Age', 'AGE'],\n        'bmi': ['bmi', 'BMI', 'Bmi'],\n        'HbA1c_level': ['HbA1c_level', 'HbA1c', 'hba1c'],\n        'blood_glucose_level': ['blood_glucose_level', 'glucose', 'BloodGlucose'],\n        'diabetes': ['diabetes', 'Diabetes']\n    }\n\n    datas = [] # download all the csv in urls as a array\n    for url in urls:\n        df = pd.read_csv(url)\n        for standard_name, variants in standard_name_mapping.items():\n            for variant in variants:\n                if variant in df.columns:\n                    df.rename(columns={variant: standard_name}, inplace=True) # inplace=True: changing directly instead of creating a new column\n                    break\n        \n        datas.append(df)\n\n    df_data = pd.concat(datas, ignore_index=True)\n    \n    df_data = df_data.drop(df_data[df_data['diabetes'] == 'No Info'].index)\n    df_data = df_data[['gender','age', 'bmi', 'HbA1c_level', 'blood_glucose_level', 'diabetes']]\n    df_data = df_data.dropna(thresh=4)\n    \n    gender_map = {'Male': 0 , 'Female': 1  , 'Other': 2}\n    df_data['gender'] = df_data['gender'].map(gender_map)\n    df_data = df_data[df_data['gender'] != 2]\n    df_data['age'] = df_data['age'].replace('No Info', df_data['age'].mean())\n    df_data['bmi'] = df_data['bmi'].replace('No Info', df_data['bmi'].mean())\n    df_data['HbA1c_level'] = df_data['HbA1c_level'].replace('No Info', df_data['HbA1c_level'].mean())\n    df_data['blood_glucose_level'] = df_data['blood_glucose_level'].replace('No Info', df_data['blood_glucose_level'].mean())\n\n    df_data.to_csv(data_output.path)\n\n@dsl.component(\n    base_image='python:3.9',\n    packages_to_install=['pandas==2.2.2', 'scikit-learn==1.5.1']\n)\ndef prepare_data(\n    data_input: Input[Artifact], \n    x_train_output: Output[Artifact], x_test_output: Output[Artifact],\n    y_train_output: Output[Artifact], y_test_output: Output[Artifact]\n):\n    import pandas as pd\n    from sklearn.model_selection import train_test_split\n\n    df_data = pd.read_csv(data_input.path)\n\n    x = df_data.drop(labels=['diabetes'], axis=1)\n    y = df_data[['diabetes']]\n    \n    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n    \n    x_train_df = pd.DataFrame(x_train)\n    x_test_df = pd.DataFrame(x_test)\n    y_train_df = pd.DataFrame(y_train)\n    y_test_df = pd.DataFrame(y_test)\n\n    x_train_df.to_csv(x_train_output.path, index=False)\n    x_test_df.to_csv(x_test_output.path, index=False)\n    y_train_df.to_csv(y_train_output.path, index=False)\n    y_test_df.to_csv(y_test_output.path, index=False)\n\n@dsl.component(\n    base_image='python:3.9',\n    packages_to_install=['pandas==2.2.2', 'scikit-learn==1.5.1', 'joblib==1.4.2', 'xgboost==2.0.3']\n)\ndef train_model(x_train: Input[Artifact], y_train: Input[Artifact], train_model_output: Output[Artifact]):\n    import pandas as pd\n    from xgboost import XGBClassifier\n    import joblib\n    \n    x_train = pd.read_csv(x_train.path)\n    y_train = pd.read_csv(y_train.path)\n    \n    model = XGBClassifier(n_estimators=1000, learning_rate= 0.01)\n    model.fit(x_train, y_train.values.ravel())\n    \n    joblib.dump(model, train_model_output.path)\n\n@dsl.component(\n    base_image='python:3.9',\n    packages_to_install=['pandas==2.2.2', 'scikit-learn==1.5.1', 'joblib==1.4.2', 'xgboost==2.0.3']\n)\ndef evaluate_model(model_path: Input[Artifact], x_test: Input[Artifact], y_test: Input[Artifact]) -> str:\n    import pandas as pd\n    from sklearn.metrics import accuracy_score\n    import joblib\n\n    model = joblib.load(filename=model_path.path)\n\n    x_test_df = pd.read_csv(x_test.path)\n    y_test_df = pd.read_csv(y_test.path)\n    \n    y_pred = model.predict(x_test_df)\n    accuracy = accuracy_score(y_test_df, y_pred)\n    \n    return f'Test accuracy: {accuracy}'\n\n@dsl.pipeline(\n    name='Diabetes Prediction Pipeline',\n    description='Using kubeflow pipeline to train and evaluate a diabetes prediction model'\n)\ndef diabetes_prediction_pipeline() -> str:\n    load_data_task = load_data()\n\n    prepare_data_task = prepare_data(data_input=load_data_task.outputs['data_output'])\n    \n    train_model_task = train_model(\n        x_train = prepare_data_task.outputs['x_train_output'], \n        y_train = prepare_data_task.outputs['y_train_output']\n    )\n    \n    evaluate_model_task = evaluate_model(\n        model_path = train_model_task.outputs['train_model_output'], \n        x_test = prepare_data_task.outputs['x_test_output'], \n        y_test = prepare_data_task.outputs['y_test_output']\n    )\n    \n    return evaluate_model_task.output\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(diabetes_prediction_pipeline, 'diabetes_prediction_pipeline_xgboost.yaml')"
  },
  {
    "repo": "s102401002/kubeflowPipeline0722",
    "file_path": "src/kubeflow_NAS.py",
    "raw_url": "https://raw.githubusercontent.com/s102401002/kubeflowPipeline0722/main/src/kubeflow_NAS.py",
    "content": "from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, when, mean\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nimport kfp\nimport kfp.components as comp\nfrom kfp import dsl\nfrom kfp.dsl import Output, Input, Artifact, Model\n\n@dsl.component(\n    base_image='python:3.9',\n    packages_to_install=['pyspark==3.3.1']\n)\ndef load_data(nas_mount_path: str, data_output: Output[Artifact]):\n    from pyspark.sql import SparkSession\n    \n    spark = SparkSession.builder.appName(\"DiabetesPrediction\").getOrCreate()\n    \n    # Read CSV files from the directory\n    df = spark.read.csv(nas_mount_path + '/*.csv', header=True, inferSchema=True)\n    \n    # Define standard name mapping\n    standard_name_mapping = {\n        'gender': ['gender', 'gen', 'Gender', 'sex', 'Sex'],\n        'age': ['age', 'Age', 'AGE'],\n        'bmi': ['bmi', 'BMI', 'Bmi'],\n        'HbA1c_level': ['HbA1c_level', 'HbA1c', 'hba1c'],\n        'blood_glucose_level': ['blood_glucose_level', 'glucose', 'BloodGlucose'],\n        'diabetes': ['diabetes', 'Diabetes']\n    }\n\n    # Rename columns based on the standard name mapping\n    for standard_name, variants in standard_name_mapping.items():\n        for variant in variants:\n            if variant in df.columns:\n                df = df.withColumnRenamed(variant, standard_name)\n                break\n\n    # Drop rows where 'diabetes' is 'No Info'\n    df = df.filter(df['diabetes'] != 'No Info')\n\n    # Drop rows with missing values\n    df = df.dropna(thresh=4)\n\n    # Map gender values to numerical\n    df = df.withColumn('gender', when(col('gender') == 'Male', 0)\n                                  .when(col('gender') == 'Female', 1)\n                                  .otherwise(None))\n\n    # Fill missing values\n    df = df.na.fill({\n        'age': df.agg(mean('age')).first()[0],\n        'bmi': df.agg(mean('bmi')).first()[0],\n        'HbA1c_level': df.agg(mean('HbA1c_level')).first()[0],\n        'blood_glucose_level': df.agg(mean('blood_glucose_level')).first()[0]\n    })\n\n    # Save to CSV\n    df.toPandas().to_csv(data_output.path, index=False)\n@dsl.component(\n    base_image='python:3.9',\n    packages_to_install=['pyspark==3.3.1']\n)\ndef prepare_data(\n    data_input: Input[Artifact], \n    x_train_output: Output[Artifact], x_test_output: Output[Artifact],\n    y_train_output: Output[Artifact], y_test_output: Output[Artifact]\n):\n    from pyspark.sql import SparkSession\n    from pyspark.ml.feature import VectorAssembler\n    from pyspark.ml import Pipeline\n    from pyspark.ml.linalg import Vectors\n    from pyspark.ml.classification import RandomForestClassifier\n    from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n    \n    spark = SparkSession.builder.appName(\"DiabetesPrediction\").getOrCreate()\n\n    # Load data\n    df_data = spark.read.csv(data_input.path, header=True, inferSchema=True)\n\n    # Prepare features and labels\n    feature_columns = ['gender', 'age', 'bmi', 'HbA1c_level', 'blood_glucose_level']\n    assembler = VectorAssembler(inputCols=feature_columns, outputCol='features')\n    df_data = assembler.transform(df_data)\n\n    # Split data\n    train_data, test_data = df_data.randomSplit([0.8, 0.2], seed=42)\n\n    # Separate features and labels\n    train_data = train_data.select('features', 'diabetes')\n    test_data = test_data.select('features', 'diabetes')\n\n    # Save to CSV\n    train_data.toPandas().to_csv(x_train_output.path, index=False)\n    test_data.toPandas().to_csv(x_test_output.path, index=False)\n    # Save labels separately\n    train_data.select('diabetes').toPandas().to_csv(y_train_output.path, index=False)\n    test_data.select('diabetes').toPandas().to_csv(y_test_output.path, index=False)\n\n@dsl.component(\n    base_image='python:3.9',\n    packages_to_install=['pyspark==3.3.1']\n)\ndef train_model(x_train: Input[Artifact], y_train: Input[Artifact], train_model_output: Output[Model]):\n    from pyspark.sql import SparkSession\n    from pyspark.ml.classification import RandomForestClassifier\n    from pyspark.ml import Pipeline\n    import joblib\n    \n    spark = SparkSession.builder.appName(\"DiabetesPrediction\").getOrCreate()\n\n    # Load data\n    x_train_df = spark.read.csv(x_train.path, header=True, inferSchema=True)\n    y_train_df = spark.read.csv(y_train.path, header=True, inferSchema=True)\n\n    # Combine features and labels\n    train_data = x_train_df.withColumn('label', y_train_df['diabetes'])\n\n    # Train the model\n    rf = RandomForestClassifier(featuresCol='features', labelCol='label')\n    model = rf.fit(train_data)\n\n    # Save the model\n    joblib.dump(model, train_model_output.path)\n\n@dsl.component(\n    base_image='python:3.9',\n    packages_to_install=['pyspark==3.3.1']\n)\ndef evaluate_model(model_path: Input[Model], x_test: Input[Artifact], y_test: Input[Artifact], result_output: Output[Artifact]):\n    from pyspark.sql import SparkSession\n    from pyspark.ml.classification import RandomForestClassificationModel\n    from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n    import joblib\n    import pandas as pd\n\n    spark = SparkSession.builder.appName(\"DiabetesPrediction\").getOrCreate()\n\n    # Load model\n    model = joblib.load(filename=model_path.path)\n\n    # Load test data\n    x_test_df = spark.read.csv(x_test.path, header=True, inferSchema=True)\n    y_test_df = spark.read.csv(y_test.path, header=True, inferSchema=True)\n\n    # Combine features and labels\n    test_data = x_test_df.withColumn('label', y_test_df['diabetes'])\n\n    # Predict and evaluate\n    predictions = model.transform(test_data)\n    evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='accuracy')\n    accuracy = evaluator.evaluate(predictions)\n\n    # Write the result to a file\n    result_df = pd.DataFrame({'accuracy': [accuracy]})\n    result_df.to_csv(result_output.path, index=False)\n\n@dsl.pipeline(\n    name='Diabetes Prediction Pipeline with PySpark',\n    description='Using PySpark to train and evaluate a diabetes prediction model'\n)\ndef diabetes_prediction_pipeline(nfs_mount_path: str = '/mnt/datasets') -> Output[Artifact]:\n    load_data_task = load_data(nas_mount_path=nfs_mount_path)\n    \n    prepare_data_task = prepare_data(data_input=load_data_task.outputs['data_output'])\n    \n    train_model_task = train_model(\n        x_train=prepare_data_task.outputs['x_train_output'], \n        y_train=prepare_data_task.outputs['y_train_output']\n    )\n    \n    evaluate_task = evaluate_model(\n        model_path=train_model_task.outputs['train_model_output'], \n        x_test=prepare_data_task.outputs['x_test_output'], \n        y_test=prepare_data_task.outputs['y_test_output']\n    )\n\n    return evaluate_task.outputs['result_output']\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(diabetes_prediction_pipeline, 'diabetes_prediction_pipeline_pyspark.yaml')\n"
  },
  {
    "repo": "s102401002/kubeflowPipeline0722",
    "file_path": "src/stroke_prediction.py",
    "raw_url": "https://raw.githubusercontent.com/s102401002/kubeflowPipeline0722/main/src/stroke_prediction.py",
    "content": "from typing import NewType\n\nimport kfp\nimport kfp.compiler\n\nfrom kfp import dsl\nfrom kfp.dsl import OutputPath, InputPath, Input, Output, Artifact, Model, Dataset \n\nfrom pandas import DataFrame\n# from kfp.components import func_to_container_op\n\nDF = NewType('DF', DataFrame)\n\n@dsl.component(\n    base_image='python:3.9',\n    packages_to_install=['pandas==2.2.2']\n)\ndef load_data(data_output: Output[Artifact]):\n    import pandas as pd\n\n    # \u8b80\u53d6\u8207\u7a0b\u5f0f\u78bc\u4f4d\u65bc\u540c\u4e00\u500b\u8cc7\u6599\u593e\u4e2d\u7684 stroke.csv\n    df_data_1 = pd.read_csv('https://raw.githubusercontent.com/s102401002/kubeflowPipeline0722/main/stroke.csv')\n\n    # \u79fb\u9664\u4e0d\u9700\u8981\u7684\u6b04\u4f4d\n    df_data_1 = df_data_1.drop(columns=['id', 'ever_married', 'work_type'])\n\n    # \u5b9a\u7fa9\u6620\u5c04\n    gender_map = {'Male': 0, 'Female': 1}\n    smoking_status_map = {'Unknown': 0, 'never smoked': 0, 'formerly smoked': 1, 'smokes': 1}\n    Residence_type_map = {'Urban': 1, 'Rural': 0}\n\n    # \u88dc\u9f4a\u8cc7\u6599\n    # gender\n    df_data_1 = df_data_1[(df_data_1['gender'] != 'N/A') & (~df_data_1['gender'].isna())]\n    df_data_1['gender'] = df_data_1['gender'].map(gender_map)  # map\n\n    # age\n    df_data_1 = df_data_1[(df_data_1['age'] != 'N/A') & (~df_data_1['age'].isna())]\n\n    # hypertension\n    df_data_1 = df_data_1[(df_data_1['hypertension'] != 'N/A') & (~df_data_1['hypertension'].isna())]\n\n    # heart_disease\n    df_data_1 = df_data_1[(df_data_1['heart_disease'] != 'N/A') & (~df_data_1['heart_disease'].isna())]\n\n    # Residence_type\n    df_data_1 = df_data_1[(df_data_1['Residence_type'] != 'N/A') & (~df_data_1['Residence_type'].isna())]\n    df_data_1['Residence_type'] = df_data_1['Residence_type'].map(Residence_type_map)  # map\n\n    # avg_glucose_level\n    df_data_1 = df_data_1[(df_data_1['avg_glucose_level'] != 'N/A') & (~df_data_1['avg_glucose_level'].isna())]\n\n    # bmi\n    df_data_1 = df_data_1[(df_data_1['bmi'] != 'N/A') & (~df_data_1['bmi'].isna())]\n\n    # smoking_status\n    df_data_1 = df_data_1[(df_data_1['smoking_status'] != 'N/A') & (~df_data_1['smoking_status'].isna())]\n    df_data_1['smoking_status'] = df_data_1['smoking_status'].map(smoking_status_map)  # map\n\n    df_data_1 = df_data_1.drop(3116)#\u7279\u6b8a\u8655\u7406\n    df_data_1 = df_data_1.sample(frac=1).reset_index(drop=True)\n\n    df_data_2 = pd.read_csv('https://raw.githubusercontent.com/s102401002/kubeflowPipeline0722/main/stroke_2.csv')\n    df_data_2 = df_data_2.drop(columns=['ever_married', 'work_type'])\n    df_data_2.rename(columns={'sex': 'gender'}, inplace=True)\n    #\u5408\u4f75\n    df_data = pd.concat([df_data_1, df_data_2], ignore_index=True)\n\n    # \u5220\u9664\u6307\u5b9a\u7684\u884c\n    rows_to_delete = [27386, 33816, 40092]\n    df_data = df_data.drop(index=rows_to_delete)\n    df_data.to_csv(data_output.path)\n\n@dsl.component(\n    base_image='python:3.9',\n    packages_to_install=['pandas==2.2.2', 'scikit-learn==1.5.1']\n)\ndef prepare_data(\n    data_input: Input[Artifact], \n    X_train_output: Output[Artifact], X_test_output: Output[Artifact],\n    Y_train_output: Output[Artifact], Y_test_output: Output[Artifact],\n    X_val_output: Output[Artifact], Y_val_output: Output[Artifact]\n):\n    import pandas as pd\n    from sklearn.model_selection import train_test_split\n\n    df_data = pd.read_csv(data_input.path)\n\n    X = df_data.drop(labels=['stroke'], axis=1)\n    Y = df_data[['stroke']]\n    \n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n    X_test, X_val, Y_test, Y_val = train_test_split(X_test, Y_test, test_size=0.5, random_state=42)\n    X_train.to_csv(X_train_output.path, index=False)\n    X_test.to_csv(X_test_output.path, index=False)\n    Y_train.to_csv(Y_train_output.path, index=False)\n    Y_test.to_csv(Y_test_output.path, index=False)\n    X_val.to_csv(X_val_output.path, index=False)\n    Y_val.to_csv(Y_val_output.path, index=False)\n\n\n@dsl.component(\n    base_image='python:3.9',\n    packages_to_install=['pandas==2.2.2', 'scikit-learn==1.5.1', 'joblib==1.4.2']\n)\ndef train_model_LogisticRegression(\n    X_train: Input[Artifact], \n    Y_train: Input[Artifact], \n    X_val: Input[Artifact], \n    Y_val: Input[Artifact],\n    X_test: Input[Artifact], \n    Y_test: Input[Artifact], \n    model: Output[Artifact],\n    file: Output[Artifact]\n):\n    import pandas as pd\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.metrics import accuracy_score\n    import joblib\n    import json\n    \n    X_train = pd.read_csv(X_train.path)\n    Y_train = pd.read_csv(Y_train.path)\n    X_val = pd.read_csv(X_val.path)\n    Y_val = pd.read_csv(Y_val.path)\n    X_test = pd.read_csv(X_test.path)\n    Y_test = pd.read_csv(Y_test.path)\n\n    # Logistic Regression\n    print('Logistic Regression')\n    lr_model = LogisticRegression(random_state=0, max_iter=10000)\n    lr_model.fit(X_train, Y_train.values.ravel())\n    print('Training accuracy:', lr_model.score(X_train, Y_train))\n    lr_accuracy = lr_model.score(X_test, Y_test)\n    print('Test accuracy:', lr_accuracy)\n    \n    # Save the model\n    joblib.dump(lr_model, model.path)\n    # Save the accuracy\n    jsonFile = open(file.path,'w')\n    data = {}\n    data['accuracy'] = lr_accuracy\n    data['model_path'] = model.path\n    json.dump(data, jsonFile, indent=2)\n@dsl.component(\n    base_image='python:3.9',\n    packages_to_install=['pandas==2.2.2', 'scikit-learn==1.5.1', 'joblib==1.4.2', 'xgboost==2.0.3']\n)\ndef train_model_xgboost(\n    X_train: Input[Artifact], \n    Y_train: Input[Artifact],\n    X_val: Input[Artifact], \n    Y_val: Input[Artifact],\n    X_test: Input[Artifact], \n    Y_test: Input[Artifact], \n    model: Output[Artifact],\n    file: Output[Artifact]\n):\n    import pandas as pd\n    import xgboost as xgb\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.metrics import accuracy_score\n    import joblib\n    import json\n    \n    X_train = pd.read_csv(X_train.path)\n    Y_train = pd.read_csv(Y_train.path)\n    X_val = pd.read_csv(X_val.path)\n    Y_val = pd.read_csv(Y_val.path)\n    X_test = pd.read_csv(X_test.path)\n    Y_test = pd.read_csv(Y_test.path)\n\n    dtrain = xgb.DMatrix(X_train, label=Y_train)\n    dval = xgb.DMatrix(X_val, label=Y_val)\n    dtest = xgb.DMatrix(X_test, label=Y_test)\n\n    scale_pos_weight = len(Y_train[Y_train == 0]) / len(Y_train[Y_train == 1])\n    param = {\n        'max_depth': 3,\n        'eta': 0.3,\n        'objective': 'binary:logistic',\n        'eval_metric': 'logloss',\n        'scale_pos_weight': scale_pos_weight\n    }\n    evallist = [(dtrain, 'train'), (dval, 'eval')]\n    num_round = 1000\n    xgb_model = xgb.train(param, dtrain, num_round, evallist, early_stopping_rounds=10)\n    preds = xgb_model.predict(dtest)\n    predictions = [round(value) for value in preds]\n    global xgb_accuracy \n    xgb_accuracy = accuracy_score(Y_test, predictions)\n    print('XGBoost Test accuracy:', xgb_accuracy)\n    \n    # Save the model\n    joblib.dump(xgb_model, model.path)\n\n     # Save the accuracy\n    jsonFile = open(file.path,'w')\n    data = {}\n    data['accuracy'] = xgb_accuracy\n    data['model_path'] = model.path\n    json.dump(data, jsonFile, indent=2)\n    # with open(accuracy.path, 'w') as f:\n    #     f.write(str(xgb_accuracy))\n\n# @dsl.component(\n#     base_image='python:3.9',\n#     packages_to_install=['pandas==2.2.2', 'scikit-learn==1.5.1', 'joblib==1.4.2']\n# )\n# def train_model_svm(\n#     X_train: Input[Artifact], \n#     Y_train: Input[Artifact],\n#     X_val: Input[Artifact], \n#     Y_val: Input[Artifact],\n#     X_test: Input[Artifact], \n#     Y_test: Input[Artifact], \n#     model: Output[Artifact],\n#     accuracy: Output[Artifact]\n# ):\n#     import pandas as pd\n#     from sklearn.metrics import accuracy_score\n#     from sklearn import svm\n#     import joblib\n    \n#     X_train = pd.read_csv(X_train.path)\n#     Y_train = pd.read_csv(Y_train.path)\n#     X_val = pd.read_csv(X_val.path)\n#     Y_val = pd.read_csv(Y_val.path)\n#     X_test = pd.read_csv(X_test.path)\n#     Y_test = pd.read_csv(Y_test.path)\n\n#     clf=svm.SVC(kernel='poly',gamma='auto',C=100)\n#     clf.fit(X_train,Y_train.values.ravel())\n#     clf.predict(X_test)\n#     svm_accuracy = clf.score(X_test, Y_test)\n#     # Save the model\n#     joblib.dump(model, clf.path)\n\n#      # Save the accuracy\n#     with open(accuracy.path, 'w') as f:\n#         f.write(str(svm_accuracy))\n\n@dsl.component(\n    base_image='python:3.9',\n    packages_to_install=['pandas==2.2.2', 'scikit-learn==1.5.1', 'joblib==1.4.2']\n)\ndef train_model_RandomForest(\n    X_train: Input[Artifact], \n    Y_train: Input[Artifact],\n    X_val: Input[Artifact], \n    Y_val: Input[Artifact],\n    X_test: Input[Artifact], \n    Y_test: Input[Artifact], \n    model: Output[Artifact],\n    file: Output[Artifact]\n):\n    import pandas as pd\n    from sklearn.metrics import accuracy_score\n    from sklearn.ensemble import RandomForestClassifier\n    import joblib\n    import json\n    \n    X_train = pd.read_csv(X_train.path)\n    Y_train = pd.read_csv(Y_train.path)\n    X_val = pd.read_csv(X_val.path)\n    Y_val = pd.read_csv(Y_val.path)\n    X_test = pd.read_csv(X_test.path)\n    Y_test = pd.read_csv(Y_test.path)\n\n    rfc=RandomForestClassifier(n_estimators=5)\n    rfc.fit(X_train,Y_train.values.ravel())    \n    y_predict=rfc.predict(X_test)\n    rfc.predict(X_test)\n    rf_accuracy = rfc.score(X_test,Y_test)\n    # Save the model\n    joblib.dump(rfc, model.path)\n\n     # Save the accuracy\n    jsonFile = open(file.path,'w')\n    data = {}\n    data['accuracy'] = rf_accuracy\n    data['model_path'] = model.path\n    json.dump(data, jsonFile, indent=2)\n\n@dsl.component(\n    base_image='python:3.9',\n    packages_to_install=['pandas==2.2.2', 'scikit-learn==1.5.1', 'joblib==1.4.2']\n)\ndef train_model_KNN(\n    X_train: Input[Artifact], \n    Y_train: Input[Artifact],\n    X_val: Input[Artifact], \n    Y_val: Input[Artifact],\n    X_test: Input[Artifact], \n    Y_test: Input[Artifact], \n    model: Output[Artifact],\n    file: Output[Artifact]\n):\n    import pandas as pd\n    from sklearn.neighbors import KNeighborsClassifier\n    import joblib\n    import json\n\n    X_train = pd.read_csv(X_train.path)\n    Y_train = pd.read_csv(Y_train.path)\n    X_val = pd.read_csv(X_val.path)\n    Y_val = pd.read_csv(Y_val.path)\n    X_test = pd.read_csv(X_test.path)\n    Y_test = pd.read_csv(Y_test.path)\n\n    knc = KNeighborsClassifier(n_neighbors=3)\n    knc.fit(X_train,Y_train.values.ravel())\n    knn_accuracy = knc.score(X_test,Y_test)\n    # Save the model\n    joblib.dump(knc, model.path)\n\n     # Save the accuracy\n    jsonFile = open(file.path,'w')\n    data = {}\n    data['accuracy'] = knn_accuracy\n    data['model_path'] = model.path\n    json.dump(data, jsonFile, indent=2)\n\n@dsl.component(\n    base_image='python:3.9',\n    packages_to_install=['joblib==1.4.2', 'scikit-learn==1.5.1', 'xgboost==2.0.3']# \n)\ndef choose_model(\n    LogisticRegression_model: Input[Artifact],\n    XGBoost_model: Input[Artifact],\n    # SVM_model: Input[Artifact],\n    RandomForest_model: Input[Artifact],\n    KNN_model: Input[Artifact],\n    lr_file: Input[Artifact],\n    xgb_file: Input[Artifact],\n    # svm_accuracy: Input[Artifact],\n    rf_file: Input[Artifact],\n    knn_file: Input[Artifact],\n    final_model: Output[Model],\n    result: Output[Artifact]\n) -> None:\n    import joblib\n    import json\n\n    # Define a dictionary to store model artifacts and their corresponding JSON files\n    models = {\n        'LogisticRegression': lr_file,\n        'XGBoost': xgb_file,\n        'RandomForest': rf_file,\n        'KNN': knn_file\n    }\n\n    accuracy = {}\n    model_paths = {}\n\n    # Read accuracies and model paths\n    for model_name, json_file in models.items():\n        with open(json_file.path, 'r') as f:\n            data = json.load(f)\n        accuracy[model_name] = data['accuracy']\n        model_paths[model_name] = data['model_path']\n\n    # Find the best model\n    best_model_name = max(accuracy, key=accuracy.get)\n    best_model = joblib.load(model_paths[best_model_name])\n    \n    # Save the best model\n    joblib.dump(best_model, final_model.path)\n\n    # Prepare result string\n    result_string = f'Best Model is {best_model_name} : {accuracy[best_model_name]}'\n    result_string += f'\\nAccuracy:\\n'\n    for model_name, acc in accuracy.items():\n        result_string += f'{model_name:17} : {acc}\\n'\n    print(result_string)\n\n\n    # Write the result to a file\n    with open(result.path, 'w') as f:\n        f.write(result_string)\n@dsl.pipeline(\n    name='Stroke Prediction Pipeline',\n    description='Using Kubeflow pipeline to train and evaluate a Stroke prediction model'\n)\ndef Stroke_prediction_pipeline():\n    # Load data\n    load_data_task = load_data()\n\n    # Prepare data\n    prepare_data_task = prepare_data(data_input=load_data_task.outputs['data_output'])\n    \n    # Train models\n    train_model_LogisticRegression_task = train_model_LogisticRegression(\n        X_train=prepare_data_task.outputs['X_train_output'], \n        Y_train=prepare_data_task.outputs['Y_train_output'],\n        X_val=prepare_data_task.outputs['X_val_output'],\n        Y_val=prepare_data_task.outputs['Y_val_output'],\n        X_test=prepare_data_task.outputs['X_test_output'], \n        Y_test=prepare_data_task.outputs['Y_test_output']\n    )\n\n    train_model_xgboost_task = train_model_xgboost(\n        X_train=prepare_data_task.outputs['X_train_output'], \n        Y_train=prepare_data_task.outputs['Y_train_output'],\n        X_val=prepare_data_task.outputs['X_val_output'],\n        Y_val=prepare_data_task.outputs['Y_val_output'],\n        X_test=prepare_data_task.outputs['X_test_output'], \n        Y_test=prepare_data_task.outputs['Y_test_output']\n    )\n    \n    # train_model_svm_task = train_model_svm(\n    #     X_train=prepare_data_task.outputs['X_train_output'], \n    #     Y_train=prepare_data_task.outputs['Y_train_output'],\n    #     X_val=prepare_data_task.outputs['X_val_output'],\n    #     Y_val=prepare_data_task.outputs['Y_val_output'],\n    #     X_test=prepare_data_task.outputs['X_test_output'], \n    #     Y_test=prepare_data_task.outputs['Y_test_output']\n    # )\n\n    train_model_RandomForest_task = train_model_RandomForest(\n        X_train=prepare_data_task.outputs['X_train_output'], \n        Y_train=prepare_data_task.outputs['Y_train_output'],\n        X_val=prepare_data_task.outputs['X_val_output'],\n        Y_val=prepare_data_task.outputs['Y_val_output'],\n        X_test=prepare_data_task.outputs['X_test_output'], \n        Y_test=prepare_data_task.outputs['Y_test_output']\n    )\n\n    train_model_KNN_task = train_model_KNN(\n        X_train=prepare_data_task.outputs['X_train_output'], \n        Y_train=prepare_data_task.outputs['Y_train_output'],\n        X_val=prepare_data_task.outputs['X_val_output'],\n        Y_val=prepare_data_task.outputs['Y_val_output'],\n        X_test=prepare_data_task.outputs['X_test_output'], \n        Y_test=prepare_data_task.outputs['Y_test_output']\n    )\n\n    choose_model_task = choose_model(\n        LogisticRegression_model=train_model_LogisticRegression_task.outputs['model'],\n        XGBoost_model=train_model_xgboost_task.outputs['model'],\n        # SVM_model=train_model_svm_task.outputs['model'],\n        RandomForest_model=train_model_RandomForest_task.outputs['model'],\n        KNN_model=train_model_KNN_task.outputs['model'],\n        lr_file=train_model_LogisticRegression_task.outputs['file'],\n        xgb_file=train_model_xgboost_task.outputs['file'],\n        # svm_file=train_model_svm_task.outputs['file'],\n        rf_file=train_model_RandomForest_task.outputs['file'],\n        knn_file=train_model_KNN_task.outputs['file']\n    )\n    \n    # The pipeline doesn't need to return anything explicitly now\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(Stroke_prediction_pipeline, '../Stroke_prediction_pipeline.yaml')"
  },
  {
    "repo": "paul-sud/demo-pipeline",
    "file_path": "kubeflow/toy.py",
    "raw_url": "https://raw.githubusercontent.com/paul-sud/demo-pipeline/master/kubeflow/toy.py",
    "content": "from kfp import dsl\nfrom kfp.components import InputPath, OutputPath, func_to_container_op\n\n\ndef trim(\n    fastq: str,\n    leading: int,\n    trailing: int,\n    minlen: int,\n    sliding_window: str,\n    trimmed_fastq: OutputPath(\"TrimmedFastq\"),\n) -> None:\n    \"\"\"\n    In the function signature, `InputPath(\"Fastq\")` is used to indicate a path to some\n    fastq, and at runtime the actual path of the data gets passed. The `Fastq` argument\n    is typechecked by the DSL compiler to make sure that types of data being passed from\n    component to component match up.\n\n    Because the trimmed fastq output path is controlled by Kubeflow, it does not have a\n    .gz extension, and as such Trimmomatic thinks it's OK to not gzip the output file\n    if we just pass that path directly. Instead, we pass a dummy filename to Trimmomatic\n    and then just write the contents of that to the output path.\n    \"\"\"\n    import os\n    import subprocess\n\n    subprocess.run(\n        [\n            \"java\",\n            \"-jar\",\n            \"/software/Trimmomatic-0.38/trimmomatic-0.38.jar\",\n            \"SE\",\n            \"-phred33\",\n            fastq,\n            \"trimmed.fastq.gz\",\n            \"LEADING:{}\".format(leading),\n            \"TRAILING:{}\".format(trailing),\n            \"SLIDINGWINDOW:{}\".format(sliding_window),\n            \"MINLEN:{}\".format(minlen),\n        ],\n        stderr=subprocess.STDOUT\n    )\n\n    os.rename(\"trimmed.fastq.gz\", trimmed_fastq)\n\n\ndef plot(\n    fastq: str,\n    trimmed_fastq: InputPath(\"TrimmedFastq\"),\n    bar_color: str,\n    flier_color: str,\n    plot_color: str,\n    plot: OutputPath(\"Plot\")\n) -> None:\n    \"\"\"\n    This is a rather odd Pythonception. Don't do this in production! One of the nice\n    things about the Kubeflow Pipelines SDK is that you can define your components\n    in Python. Calling the script as a subprocess here is a legacy of the original WDL\n    pipeline, WDL forces you to wrap all code with shell scripts. With KFP the compiler\n    autogenerates CLI wrappers for you.\n\n    Like in the trim task, tools often make unfortunate assumptions about the names of\n    files. Here we pull a similar trick to get the plot to not complain due to the\n    unexpected input filename.\n    \"\"\"\n    import glob\n    import os\n    from shutil import copyfile\n    import subprocess\n\n    copyfile(trimmed_fastq, \"trimmed.fastq.gz\")\n\n    subprocess.run(\n        [\n            \"python3\",\n            \"/software/demo-pipeline/src/plot_fastq_scores.py\",\n            \"--untrimmed\",\n            fastq,\n            \"--trimmed\",\n            \"trimmed.fastq.gz\",\n            \"--bar-color\",\n            bar_color,\n            \"--flier-color\",\n            flier_color,\n            \"--plot-color\",\n            plot_color,\n        ],\n        stderr=subprocess.STDOUT\n    )\n\n    os.rename(glob.glob(\"*quality_scores.png\")[0], plot)\n\n\n\ntrim_op = func_to_container_op(trim, base_image=\"quay.io/encode-dcc/demo-pipeline:template\")\nplot_op = func_to_container_op(plot, base_image=\"quay.io/encode-dcc/demo-pipeline:template\")\n\n\n@dsl.pipeline(\n    name='ENCODE Demo Pipeline',\n    description='A Kubeflow Pipelines implementation of the ENCODE DCC demo pipeline'\n)\ndef demo_pipeline(\n    fastqs=[\"/mnt/data/file1.fastq.gz\", \"/mnt/data/file2.fastq.gz\"],\n    leading: int = 5,\n    trailing: int = 5,\n    minlen: int = 80,\n    sliding_window: str = \"4:25\",\n    bar_color: str = \"white\",\n    flier_color: str = \"grey\",\n    plot_color: str = \"darkgrid\",\n):\n    \"\"\"\n    func_to_container_op simply converts the function into a factory that produces ops\n    when called. add_pvolumes is a method of the op itself, so we need to apply it here\n    when the op is actually generated, NOT above where the trim_op factory is created.\n    \"\"\"\n    with dsl.ParallelFor(fastqs) as fastq:\n        trim_task = trim_op(\n            fastq=fastq,\n            leading=leading,\n            trailing=trailing,\n            minlen=minlen,\n            sliding_window=sliding_window,\n        ).add_pvolumes({\"/mnt/data\": dsl.PipelineVolume(pvc=\"test-data-pv-claim\")})\n\n        _ = plot_op(\n            fastq=fastq,\n            trimmed_fastq=trim_task.outputs[\"trimmed_fastq\"],\n            bar_color=bar_color,\n            flier_color=flier_color,\n            plot_color=plot_color\n        ).add_pvolumes({\"/mnt/data\": dsl.PipelineVolume(pvc=\"test-data-pv-claim\")})\n"
  },
  {
    "repo": "tonouchi510/kfp-project",
    "file_path": "pipelines/head-pose-dataset-pipeline/head-pose-dataset-pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/tonouchi510/kfp-project/main/pipelines/head-pose-dataset-pipeline/head-pose-dataset-pipeline.py",
    "content": "import kfp\nfrom kfp import dsl\nfrom kfp import gcp\n\n# Initialize component store\ncomponent_store = kfp.components.ComponentStore(\n    local_search_paths=[\"pipelines/head-pose-dataset-pipeline\", \"components\"])\n\n# Create component factories\npose_annotation_op = component_store.load_component(\"pose-annotation\")\ndata_chunk_spliter_op = component_store.load_component(\"data-chunk-spliter\")\nslack_notification_op = component_store.load_component(\"slack-notification\")\n\n\n# Define pipeline\n@dsl.pipeline(\n    name=\"head-pose-dataset pipeline\",\n    description=\"Create tfrecord dataset for head-pose-pipeline.\"\n)\ndef pipeline(\n    pipeline_name: str = \"head-pose-dataset-pipeline\",\n    bucket_name: str = \"kfp-project\",\n    job_id: str = \"{{JOB_ID}}\",\n    dataset: str = \"\",\n    chunk_size: int = 1000,\n    valid_ratio: float = 0.1,\n):\n\n    with dsl.ExitHandler(\n        exit_op=slack_notification_op(\n            pipeline_name=pipeline_name,\n            job_id=job_id,\n            message=\"Status: {{workflow.status}}\"\n        )\n    ):\n        split_task = data_chunk_spliter_op(\n            pipeline=pipeline_name,\n            bucket_name=bucket_name,\n            job_id=job_id,\n            dataset=dataset,\n            chunk_size=chunk_size,\n        ).apply(gcp.use_preemptible_nodepool()) \\\n            .set_retry(num_retries=2)\n\n        with dsl.ParallelFor(split_task.output) as item:\n            pose_annotation_op(\n                pipeline=pipeline_name,\n                bucket_name=bucket_name,\n                job_id=job_id,\n                valid_ratio=valid_ratio,\n                chunk_file=item\n            ).apply(gcp.use_preemptible_nodepool()) \\\n                .set_retry(num_retries=2)\n\n\nif __name__ == \"__main__\":\n    kfp.compiler.Compiler().compile(pipeline, \"head-pose-dataset-pipeline.yaml\")\n"
  },
  {
    "repo": "tonouchi510/kfp-project",
    "file_path": "pipelines/head-pose-pipeline/head-pose-pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/tonouchi510/kfp-project/main/pipelines/head-pose-pipeline/head-pose-pipeline.py",
    "content": "import kfp\nfrom kfp import dsl\nfrom kfp import gcp\n\n# Initialize component store\ncomponent_store = kfp.components.ComponentStore(\n    local_search_paths=[\"pipelines/head-pose-pipeline\", \"components\"])\n\n# Create component factories\ntrain_op = component_store.load_component(\"training\")\neval_op = component_store.load_component(\"evaluation\")\ntensorboard_op = component_store.load_component(\"tb-observer\")\nslack_notification_op = component_store.load_component(\"slack-notification\")\n\n\n# Define pipeline\n@dsl.pipeline(\n    name=\"head-pose-pipeline\",\n    description=\"training pipeline for head-pose-estimation\"\n)\ndef pipeline(\n    pipeline_name: str = \"head-pose-pipeline\",\n    bucket_name: str = \"kfp-project\",\n    job_id: str = \"{{JOB_ID}}\",\n    model_type: int = 7,\n    global_batch_size: int = 1024,\n    epochs: int = 30,\n    lr: float = 0.001,\n    image_size: int = 64,\n    dataset: str = \"gs://kfp-project/datasets/300W-LP\",\n    test_dataset: str = \"gs://kfp-project/datasets/300W-LP\",\n):\n    with dsl.ExitHandler(\n        exit_op=slack_notification_op(\n            pipeline_name=pipeline_name,\n            job_id=job_id,\n            message=\"Status: {{workflow.status}}\"\n        )\n    ):\n        train_task = train_op(\n            pipeline_name=pipeline_name,\n            bucket_name=bucket_name,\n            job_id=job_id,\n            global_batch_size=global_batch_size,\n            epochs=epochs,\n            learning_rate=lr,\n            dataset=dataset,\n            model_type=model_type,\n            image_size=image_size,\n        ).set_display_name(\"training\")\\\n            .apply(gcp.use_preemptible_nodepool())\\\n            .apply(gcp.use_tpu(\n                tpu_cores=8,\n                tpu_resource=\"v3\",\n                tf_version=\"2.8.0\"))\\\n            .set_retry(num_retries=2)\n\n        eval_op(\n            pipeline_name=pipeline_name,\n            bucket_name=bucket_name,\n            job_id=job_id,\n            model_type=model_type,\n            image_size=image_size,\n            test_dataset=test_dataset,\n        ).set_display_name(\"evaluation\")\\\n            .apply(gcp.use_preemptible_nodepool())\\\n            .after(train_task)\\\n            .set_retry(num_retries=2)\n\n        tensorboard_op(\n            pipeline_name=pipeline_name,\n            bucket_name=bucket_name,\n            job_id=job_id,\n            tblog_dir=\"training/logs\"\n        ).set_display_name(\"tboard\")\\\n            .apply(gcp.use_preemptible_nodepool())\n\n\nif __name__ == \"__main__\":\n    kfp.compiler.Compiler().compile(pipeline, \"head-pose-pipeline.yaml\")\n"
  },
  {
    "repo": "tonouchi510/kfp-project",
    "file_path": "pipelines/hello-world-pipeline/hello-world-pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/tonouchi510/kfp-project/main/pipelines/hello-world-pipeline/hello-world-pipeline.py",
    "content": "import kfp\nfrom kfp.v2 import dsl\nfrom kfp import gcp\n\nPIPELINE_NAME = \"hello-world-pipeline\"\n\n# Initialize component store\ncomponent_store = kfp.components.ComponentStore(\n    local_search_paths=[\"pipelines/hello-world-pipeline\", \"components\"])\n\n# Create component factories\nhello_op = component_store.load_component(\"hello\")\n\n\n# Define pipeline\n@dsl.pipeline(\n    name=\"hello-world-pipeline\",\n    description=\"Output messages\"\n)\ndef pipeline(\n    job_id: str = \"xxxx\",\n    message: str = \"hello world\",\n):\n    hello_op(\n        message=message,\n    ).apply(gcp.use_preemptible_nodepool()) \\\n        .set_retry(num_retries=2)\n\n\nif __name__ == \"__main__\":\n    kfp.v2.compiler.Compiler().compile(\n        pipeline_func=pipeline, \n        package_path=\"hello-world-pipeline.json\"\n    )\n"
  },
  {
    "repo": "tonouchi510/kfp-project",
    "file_path": "pipelines/optuna-pipeline/optuna-pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/tonouchi510/kfp-project/main/pipelines/optuna-pipeline/optuna-pipeline.py",
    "content": "import kfp\nfrom kfp import dsl\nimport os\n\n# Initialize component store\ncomponent_store = kfp.components.ComponentStore(\n    local_search_paths=[\"pipelines/optuna-pipeline\", \"components\"])\n\n# Create component factories\noptuna_op = component_store.load_component(\"optuna-worker\")\nslack_notification_op = component_store.load_component(\"slack-notification\")\n\nsidecar = kfp.dsl.Sidecar(\n    name=\"cloudsqlproxy\",\n    image=\"gcr.io/cloudsql-docker/gce-proxy:1.14\",\n    command=[\n        \"/cloud_sql_proxy\",\n        f\"-instances={os.environ.get('GCP_PROJECT')}:{os.environ.get('GCP_REGION')}:{os.environ.get('DB_NAME')}=tcp:3306\",\n    ],\n)\n\n# Define pipeline\n@dsl.pipeline(\n    name=\"optuna pipeline\",\n    description=\"optuna pipeline\"\n)\ndef pipeline(\n    pipeline_name: str = \"optuna-pipeline\",\n    bucket_name: str = \"\",\n    job_id: str = \"{{JOB_ID}}\",\n    n_trials: int = 100,\n    n_jobs: int = 5,\n    training_pipeline_name: str = \"head-pose-pipeline\",\n    dataset: str = \"\",\n    epochs: int = 5\n):\n    with dsl.ExitHandler(\n        exit_op=slack_notification_op(\n            pipeline_name=pipeline_name,\n            job_id=job_id,\n            message=\"Status: {{workflow.status}}\"\n        ).add_node_selector_constraint(\"cloud.google.com/gke-nodepool\", \"main-pool\")\n    ):\n        optuna_op(\n            pipeline_name=pipeline_name,\n            bucket_name=bucket_name,\n            job_id=job_id,\n            n_trials=n_trials,\n            n_jobs=n_jobs,\n            training_pipeline_name=training_pipeline_name,\n            dataset=dataset,\n            epochs=epochs\n        ).set_display_name(\"optuna-worker\")\\\n            .add_node_selector_constraint(\"cloud.google.com/gke-nodepool\", \"cpu-pool\")\\\n            .add_sidecar(sidecar)\\\n            .set_retry(num_retries=2)\n\n\nif __name__ == \"__main__\":\n    kfp.compiler.Compiler().compile(pipeline, \"optuna-pipeline.yaml\")\n"
  },
  {
    "repo": "tonouchi510/kfp-project",
    "file_path": "pipelines/simple-training-pipeline/simple-training-pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/tonouchi510/kfp-project/main/pipelines/simple-training-pipeline/simple-training-pipeline.py",
    "content": "import kfp\nfrom kfp import dsl\nfrom kfp import gcp\n\n# Initialize component store\ncomponent_store = kfp.components.ComponentStore(\n    local_search_paths=[\"pipelines/simple-training-pipeline\", \"components\"])\n\n# Create component factories\ntrain_op = component_store.load_component(\"training\")\ntensorboard_op = component_store.load_component(\"tb-observer\")\nslack_notification_op = component_store.load_component(\"slack-notification\")\n\n\n# Define pipeline\n@dsl.pipeline(\n    name=\"simple-training-pipeline\",\n    description=\"training pipeline for image classification\"\n)\ndef pipeline(\n    pipeline_name: str = \"simple-training-pipeline\",\n    bucket_name: str = \"kfp-project\",\n    job_id: str = \"{{JOB_ID}}\",\n    model_type: str = \"resnet\",\n    global_batch_size: int = 1024,\n    epochs: int = 30,\n    lr: float = 0.001,\n    image_size: int = 64,\n    num_classes: int = 100,\n    dataset: str = \"gs://kfp-project/datasets/mnist\",\n):\n    with dsl.ExitHandler(\n        exit_op=slack_notification_op(\n            pipeline_name=pipeline_name,\n            job_id=job_id,\n            message=\"Status: {{workflow.status}}\"\n        )\n    ):\n        train_op(\n            pipeline_name=pipeline_name,\n            bucket_name=bucket_name,\n            job_id=job_id,\n            global_batch_size=global_batch_size,\n            epochs=epochs,\n            learning_rate=lr,\n            dataset=dataset,\n            model_type=model_type,\n            image_size=image_size,\n            num_classes=num_classes,\n        ).set_display_name(\"training\")\\\n            .apply(gcp.use_preemptible_nodepool())\\\n            .apply(gcp.use_tpu(\n                tpu_cores=8,\n                tpu_resource=\"v3\",\n                tf_version=\"2.8.0\"))\\\n            .set_retry(num_retries=2)\n\n        tensorboard_op(\n            pipeline_name=pipeline_name,\n            bucket_name=bucket_name,\n            job_id=job_id,\n            tblog_dir=\"training/logs\"\n        ).set_display_name(\"tboard\")\\\n            .apply(gcp.use_preemptible_nodepool())\n\n\nif __name__ == \"__main__\":\n    kfp.compiler.Compiler().compile(pipeline, \"simple-training-pipeline.yaml\")\n"
  },
  {
    "repo": "hiruna72/miniKF_example_pipeline",
    "file_path": "small_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/hiruna72/miniKF_example_pipeline/master/small_pipeline.py",
    "content": "import kfp\n# import os\n# component_root=\"/home/jovyan\"\n# Load the component by calling load_component_from_file or load_component_from_url\n# To load the component, the pipeline author only needs to have access to the component.yaml file.\n# The Kubernetes cluster executing the pipeline needs access to the container image specified in the component.\n# dummy_op = kfp.components.load_component_from_file(os.path.join(component_root, 'component.yaml')) \n# dummy_op = kfp.components.load_component_from_url('http://....../component.yaml')\n\n# dummy_op is now a \"factory function\" that accepts the arguments for the component's inputs\n# and produces a task object (e.g. ContainerOp instance).\n# Inspect the dummy_op function in Jupyter Notebook by typing \"dummy_op(\" and pressing Shift+Tab\n# You can also get help by writing help(dummy_op) or dummy_op? or dummy_op??\n# The signature of the dummy_op function corresponds to the inputs section of the component.\n# Some tweaks are performed to make the signature valid and pythonic:\n# 1) All inputs with default values will come after the inputs without default values\n# 2) The input names are converted to pythonic names (spaces and symbols replaced\n#    with underscores and letters lowercased).\n\ndef multiply(input_file, multiplier,\n               output_uri, output_uri_in_file,\n               volume,\n               step_name='multiply',\n              mount_output_to='/data'):\n    return kfp.dsl.ContainerOp(\n        name=step_name,\n        image='hiruna72/multiplier@sha256:3016c55dcb8015ef9b457dce839206b5704afacd71a42a688132569d97684f99',\n        arguments=[\n            '--input1-path', input_file,\n            '--param1', multiplier,\n            '--output1-path', output_uri,\n            '--output1-path-file', output_uri_in_file,\n        ],\n        command=['python3', '/pipelines/component/src/multiplier.py'],\n        file_outputs={\n            'output_file': output_uri,\n            'output_uri_in_file': output_uri_in_file,\n        },\n        pvolumes={mount_output_to: volume}\n    )\n\ndef concatenate(input_file1, input_fi1e2,\n               output_uri, output_uri_in_file,\n               volume,\n               step_name='concat',\n              mount_output_to='/data'):\n    return kfp.dsl.ContainerOp(\n        name=step_name,\n        image='hiruna72/concatenate@sha256:2119c2f95d5b65eb02cfca29dbbe6d8d9c1e61d900498ae45381ed9e28b0e48c',\n        arguments=[\n            '--input1-path', input_file1,\n            '--input2-path', input_fi1e2,\n            '--output1-path', output_uri,\n            '--output1-path-file', output_uri_in_file,\n        ],\n        command=['python3', '/pipelines/component/src/concat.py'],\n        file_outputs={\n            'output_file': output_uri,\n            'output_uri_in_file': output_uri_in_file,\n        },\n        pvolumes={mount_output_to: volume}\n    )\n\n\n# Define a pipeline and create a task from a component:\n@kfp.dsl.pipeline(name='My pipeline', description='')\ndef my_pipeline(\n        rok_url,\n        pvc_size='1Gi'):\n    \n    vop = kfp.dsl.VolumeOp(\n        name='create-volume',\n        resource_name='helloPipeline_data',\n        annotations={\"rok/origin\": rok_url},\n        size=pvc_size\n    )\n    \n    compo1 = multiply(\n        input_file='/data/input_compo1.txt',\n        multiplier=7,\n        output_uri='/data/output_compo1.txt',\n        output_uri_in_file='/data/output_compo1_uri.txt',\n        volume=vop.volume\n    )\n    \n    compo2 = multiply(\n        input_file='/data/input_compo2.txt',\n        multiplier=7,\n        output_uri='/data/output_compo2.txt',\n        output_uri_in_file='/data/output_compo2_uri.txt',\n        volume=vop.volume\n    )\n    \n#     compo3 = concatenate(\n#         input_file1='/data/input_compo1.txt',\n#         input_file2='/data/input_compo2.txt',\n#         output_uri='/data/output_compo3.txt',\n#         output_uri_in_file='/data/output_compo3_uri.txt',\n#         volume=vop.volume\n#     )\n    \n    compo3 = concatenate(\n        input_file1=compo1.outputs['output_uri_in_file'],\n        input_fi1e2=compo2.outputs['output_uri_in_file'],\n        output_uri='/data/output_compo3.txt',\n        output_uri_in_file='/data/output_compo3_uri.txt',\n        volume=vop.volume\n    )\n    \nif __name__ == '__main__':\n    import kfp.compiler as compiler\n    compiler.Compiler().compile(my_pipeline, 'small_pipeline.tar.gz')"
  },
  {
    "repo": "hiruna72/miniKF_example_pipeline",
    "file_path": "components/noRok/norok_reusable_compo_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/hiruna72/miniKF_example_pipeline/master/components/noRok/norok_reusable_compo_pipeline.py",
    "content": "import kfp\nimport os\ncomponent_root=\"/home/jovyan/src\"\n# Load the component by calling load_component_from_file or load_component_from_url\n# To load the component, the pipeline author only needs to have access to the component.yaml file.\n# The Kubernetes cluster executing the pipeline needs access to the container image specified in the component.\necho = kfp.components.load_component_from_file(os.path.join(component_root, 'component.yaml')) \n# dummy_op = kfp.components.load_component_from_url('http://....../component.yaml')\n\n# Define a pipeline and create a task from a component:\n@kfp.dsl.pipeline(name='My pipeline', description='')\ndef my_pipeline():\n        \n    compo1 = echo(\n        input_1_uri='https://www.w3.org/TR/PNG/iso_8859-1.txt')\n    \nif __name__ == '__main__':\n    import kfp.compiler as compiler\n    compiler.Compiler().compile(my_pipeline, 'norok_reusable_compo_pipeline.tar.gz')"
  },
  {
    "repo": "shashnavad/wine-quality-mlops",
    "file_path": "pipelines/test_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/shashnavad/wine-quality-mlops/main/pipelines/test_pipeline.py",
    "content": "import os\nimport sys\n\nimport logging\n# Configure root logger and specific loggers to suppress debug messages\nlogging.getLogger().setLevel(logging.ERROR)  # Set root logger to ERROR\nlogging.getLogger('great_expectations').setLevel(logging.ERROR)\nlogging.getLogger('great_expectations.expectations.registry').setLevel(logging.CRITICAL)  # Even stricter for registry\n# Set environment variables first\nos.environ[\"GE_USAGE_STATISTICS_ENABLED\"] = \"False\"\nos.environ[\"GE_HOME\"] = \"/tmp/great_expectations_home\"\nos.environ[\"TESTING\"] = \"True\"\nos.environ[\"GE_CONFIG_VERSION\"] = \"3\"  # Force using V3 config\nos.environ[\"GE_UNCOMMITTED_DIRECTORIES\"] = \"True\"  # Skip Git checks\nos.environ[\"GX_ASSUME_MISSING_LIBRARIES\"] = \"git\"\n\n# Mock out git before anything tries to import it\nclass MockGit:\n    class Repo:\n        @staticmethod\n        def init(*args, **kwargs):\n            pass\n    \n    class NoSuchPathError(Exception):\n        pass\n\n# Add the mock to sys.modules\nsys.modules['git'] = MockGit\n\nimport pytest\nimport numpy as np\nfrom sklearn.ensemble import RandomForestRegressor\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom kfp import local\nimport pickle\nimport json \nimport joblib\nfrom kfp.dsl import Dataset, Model, Metrics\n\n# Import components directly\nfrom wine_quality_pipeline import validate_data, preprocess, train\n\n# Initialize local runner\nlocal.init(runner=local.SubprocessRunner())\n\n# Helper class to mock KFP artifacts\nclass MockArtifact:\n    def __init__(self, path):\n        self.path = path\n\nfrom unittest.mock import MagicMock\n\nclass MockPrometheusClient:\n    Gauge = MagicMock()\n    start_http_server = MagicMock()\n\nsys.modules['prometheus_client'] = MockPrometheusClient\n\ndef test_validate_data(tmp_path):\n    original_cwd = os.getcwd()\n    os.chdir(tmp_path)  # Work in temporary directory\n    \n    try:\n        # Set environment variables\n        os.environ[\"GE_USAGE_STATISTICS_ENABLED\"] = \"False\"\n        os.environ[\"GE_UNCOMMITTED_DIRECTORIES\"] = \"True\"\n        os.environ[\"GX_ASSUME_MISSING_LIBRARIES\"] = \"git\"\n        \n        # Configure logging to reduce verbosity\n        import logging\n        logging.getLogger('great_expectations').setLevel(logging.ERROR)\n        \n        # Create sample data\n        data_dir = os.path.join(tmp_path, \"data\")\n        os.makedirs(data_dir, exist_ok=True)\n        data_path = os.path.join(data_dir, \"sample_wine_data.csv\")\n        \n        with open(data_path, \"w\") as f:\n            f.write(\"fixed acidity;volatile acidity;citric acid;residual sugar;chlorides;free sulfur dioxide;total sulfur dioxide;density;pH;sulphates;alcohol;quality\\n\")\n            f.write(\"7.4;0.7;0;1.9;0.076;11;34;0.9978;3.51;0.56;9.4;5\\n\")\n            f.write(\"7.8;0.88;0;2.6;0.098;25;67;0.9968;3.2;0.68;9.8;5\\n\")\n        \n        metrics_path = os.path.join(tmp_path, \"validation_metrics.json\")\n        \n        # Import Great Expectations components\n        from great_expectations.data_context.types.base import DataContextConfig\n        from great_expectations.data_context import BaseDataContext\n        from great_expectations.core.batch import RuntimeBatchRequest\n        import pandas as pd\n        import json\n        \n        # Load data\n        df = pd.read_csv(data_path, sep=\";\")\n        \n        # Create minimal context\n        context_config = DataContextConfig(\n            store_backend_defaults=None,\n            expectations_store_name=\"expectations_store\",  # Specify store names\n            validations_store_name=\"validations_store\",\n            evaluation_parameter_store_name=\"evaluation_parameter_store\",\n            checkpoint_store_name=None,\n            stores={\n                \"expectations_store\": {\n                    \"class_name\": \"ExpectationsStore\",\n                    \"store_backend\": {\n                        \"class_name\": \"InMemoryStoreBackend\"\n                    }\n                },\n                \"validations_store\": {\n                    \"class_name\": \"ValidationsStore\",\n                    \"store_backend\": {\n                        \"class_name\": \"InMemoryStoreBackend\"\n                    }\n                },\n                \"evaluation_parameter_store\": {\n                    \"class_name\": \"EvaluationParameterStore\"\n                }\n            }\n        )\n        \n        context = BaseDataContext(project_config=context_config)\n        \n        # Add datasource\n        context.add_datasource(\n            name=\"pandas_datasource\",\n            class_name=\"Datasource\",\n            module_name=\"great_expectations.datasource\",\n            execution_engine={\n                \"class_name\": \"PandasExecutionEngine\",\n                \"module_name\": \"great_expectations.execution_engine\"\n            },\n            data_connectors={\n                \"runtime_connector\": {\n                    \"class_name\": \"RuntimeDataConnector\",\n                    \"module_name\": \"great_expectations.datasource.data_connector\",\n                    \"batch_identifiers\": [\"batch_id\"]\n                }\n            }\n        )\n        \n        # Create expectation suite - ensure this is a string\n        suite_name = \"wine_quality_suite\"\n        print(f\"Suite name type: {type(suite_name)}\")\n        context.create_expectation_suite(suite_name, overwrite_existing=True)\n        \n        # Create batch request using RuntimeBatchRequest\n        batch_request = RuntimeBatchRequest(\n            datasource_name=\"pandas_datasource\",\n            data_connector_name=\"runtime_connector\",\n            data_asset_name=\"wine_data\",\n            runtime_parameters={\"batch_data\": df},\n            batch_identifiers={\"batch_id\": \"default_identifier\"},\n        )\n        \n        # Get validator with explicit expectation_suite_name\n        validator = context.get_validator(\n            batch_request=batch_request,\n            expectation_suite_name=suite_name\n        )\n        \n        # Add expectations and validate\n        validator.expect_table_columns_to_match_ordered_list(list(df.columns))\n        results = validator.validate()\n        \n        # Save metrics\n        with open(metrics_path, 'w') as f:\n            json.dump({\"validation_success\": results.success}, f)\n        \n        assert os.path.exists(metrics_path)\n    \n    finally:\n        os.chdir(original_cwd)  # Restore original directory\n\ndef test_preprocess(tmp_path):\n    os.makedirs(tmp_path, exist_ok=True)\n    features_path = os.path.join(tmp_path, \"features.npy\")\n    labels_path = os.path.join(tmp_path, \"labels.npy\")\n    scaler_path = os.path.join(tmp_path, \"scaler.pkl\")\n    \n    preprocess.python_func(\n        data_path=\"data/sample_wine_data.csv\",\n        features=MockArtifact(features_path),\n        labels=MockArtifact(labels_path),\n        scaler=MockArtifact(scaler_path)\n    )\n    \n    assert os.path.exists(features_path)\n    assert os.path.exists(labels_path)\n    assert os.path.exists(scaler_path)\n\ndef test_train(tmp_path):\n    os.makedirs(tmp_path, exist_ok=True)\n    features_path = os.path.join(tmp_path, \"features.npy\")\n    labels_path = os.path.join(tmp_path, \"labels.npy\")\n    model_path = os.path.join(tmp_path, \"model.pkl\")\n    metrics_path = os.path.join(tmp_path, \"metrics.json\")\n    scaler_path = os.path.join(tmp_path, \"scaler.pkl\")\n    \n    import joblib\n    from sklearn.preprocessing import StandardScaler\n    \n    joblib.dump(StandardScaler(), scaler_path)\n    \n    # Create dummy input data\n    np.save(features_path, np.random.rand(10, 11))\n    np.save(labels_path, np.random.rand(10))\n    \n    # Disable MLflow for testing\n    os.environ[\"TESTING\"] = \"True\"\n    \n    # Test RandomForest model type\n    train.python_func(\n        features=MockArtifact(features_path),\n        labels=MockArtifact(labels_path),\n        hyperparameters={\"n_estimators\": 100},\n        model_type=\"RandomForest\",\n        model=MockArtifact(model_path),\n        metrics=MockArtifact(metrics_path),\n        scaler=MockArtifact(scaler_path)\n    )\n    \n    assert os.path.exists(model_path)\n    assert os.path.exists(metrics_path)\n    \n    # Test XGBoost model type\n    train.python_func(\n        features=MockArtifact(features_path),\n        labels=MockArtifact(labels_path),\n        hyperparameters={\"n_estimators\": 100, \"max_depth\": 6},\n        model_type=\"XGBoost\",\n        model=MockArtifact(model_path),\n        metrics=MockArtifact(metrics_path),\n        scaler=MockArtifact(scaler_path)\n    )\n    \n    assert os.path.exists(model_path)\n    assert os.path.exists(metrics_path)\n    # Verify Prometheus metrics setup\n    from prometheus_client import Gauge, start_http_server\n    Gauge.assert_any_call('train_r2_score', 'Training R\u00b2 score', ['model_type'])\n    start_http_server.assert_called()\n\n\ndef test_deploy_model(tmp_path):\n    os.makedirs(tmp_path, exist_ok=True)\n    \n    # Create paths for all required artifacts\n    model_path = os.path.join(tmp_path, \"model.pkl\")\n    metrics_path = os.path.join(tmp_path, \"metrics.json\")\n    scaler_path = os.path.join(tmp_path, \"scaler.pkl\")\n    \n    # Create mock model\n    from sklearn.ensemble import RandomForestRegressor\n    import pickle\n    model_obj = RandomForestRegressor(n_estimators=10)\n    with open(model_path, 'wb') as f:\n        pickle.dump(model_obj, f)\n    \n    # Create mock metrics\n    import json\n    metrics_data = {\n        \"train_r2\": 0.85,\n        \"test_r2\": 0.82,\n        \"mean_absolute_error\": 0.35\n    }\n    with open(metrics_path, 'w') as f:\n        json.dump(metrics_data, f)\n    \n    # Create mock scaler\n    from sklearn.preprocessing import StandardScaler\n    import joblib\n    scaler_obj = StandardScaler()\n    joblib.dump(scaler_obj, scaler_path)\n    \n    # Import the deploy_model component\n    from wine_quality_pipeline import deploy_model\n    \n    # Mock kubernetes and kserve modules\n    import sys\n    from unittest.mock import MagicMock\n    \n    # Create mock modules\n    mock_kserve = MagicMock()\n    mock_kubernetes = MagicMock()\n    \n    # Add mocks to sys.modules\n    sys.modules['kserve'] = mock_kserve\n    sys.modules['kubernetes'] = mock_kubernetes\n    sys.modules['kubernetes.client'] = MagicMock()\n    sys.modules['kubernetes.config'] = MagicMock()\n    \n    # Create a mock KServeClient\n    mock_kserve_client = MagicMock()\n    mock_kserve.KServeClient.return_value = mock_kserve_client\n    \n    # Set environment variable for testing\n    os.environ[\"TESTING\"] = \"True\"\n    \n    # Call the deploy_model function\n    service_url = deploy_model.python_func(\n        model=MockArtifact(model_path),\n        metrics=MockArtifact(metrics_path),\n        scaler=MockArtifact(scaler_path),\n        service_name=\"wine-quality-test\",\n        namespace=\"kubeflow-test\"\n    )\n    \n    # Verify that the KServe client was called to create the inference service\n    mock_kserve_client.create.assert_called_once()\n    \n    # Assert that the function returns a service URL\n    assert isinstance(service_url, str)\n    assert \"wine-quality-test\" in service_url\n\ndef test_select_best_model(tmp_path):\n    \"\"\"Test select_best_model with individual model inputs.\"\"\"\n    os.makedirs(tmp_path, exist_ok=True)\n    \n    # Create paths for multiple models and metrics\n    model_paths = [\n        os.path.join(tmp_path, f\"model_{i}.pkl\") for i in range(3)\n    ]\n    \n    metrics_paths = [\n        os.path.join(tmp_path, f\"metrics_{i}.json\") for i in range(3)\n    ]\n    \n    best_model_path = os.path.join(tmp_path, \"best_model.pkl\")\n    best_metrics_path = os.path.join(tmp_path, \"best_metrics.json\")\n    \n    X = np.random.rand(10, 4)\n    y = np.random.rand(10)\n    \n    # Create and fit models\n    models = [\n        RandomForestRegressor(n_estimators=10).fit(X, y),\n        xgb.XGBRegressor(n_estimators=10).fit(X, y),\n        lgb.LGBMRegressor(n_estimators=10).fit(X, y)\n    ]\n    \n    model_types = [\"RandomForest\", \"XGBoost\", \"LightGBM\"]\n    test_scores = [0.82, 0.85, 0.80]  # XGBoost should be selected as best\n    \n    # Save models and metrics\n    for i, (model, model_type, test_score) in enumerate(zip(models, model_types, test_scores)):\n        joblib.dump(model, model_paths[i])\n        \n        metrics_data = {\n            \"model_type\": model_type,\n            \"train_r2\": float(test_score + 0.05),\n            \"test_r2\": float(test_score)\n        }\n        \n        with open(metrics_paths[i], 'w') as f:\n            json.dump(metrics_data, f)\n    \n    # Import the select_best_model component\n    from wine_quality_pipeline import select_best_model\n    \n    # Create mock artifacts\n    model_artifacts = [MockArtifact(path) for path in model_paths]\n    metrics_artifacts = [MockArtifact(path) for path in metrics_paths]\n    best_model_artifact = MockArtifact(best_model_path)\n    best_metrics_artifact = MockArtifact(best_metrics_path)\n    \n    # Call the function with the updated parameter structure\n    select_best_model.python_func(\n        best_model=best_model_artifact,\n        best_metrics=best_metrics_artifact,\n        model1=model_artifacts[0],\n        model2=model_artifacts[1],\n        model3=model_artifacts[2],\n        metrics1=metrics_artifacts[0],\n        metrics2=metrics_artifacts[1],\n        metrics3=metrics_artifacts[2]\n    )\n    \n    # Verify results\n    assert os.path.exists(best_model_path)\n    assert os.path.exists(best_metrics_path)\n    \n    # Check that XGBoost was selected as the best model\n    with open(best_metrics_path, 'r') as f:\n        best_metrics_data = json.load(f)\n    assert best_metrics_data[\"model_type\"] == \"XGBoost\"\n\n\n"
  },
  {
    "repo": "shashnavad/wine-quality-mlops",
    "file_path": "pipelines/wine_quality_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/shashnavad/wine-quality-mlops/main/pipelines/wine_quality_pipeline.py",
    "content": "import os\nimport sys\n\n# Set environment variables to disable Git functionality\nos.environ[\"GE_USAGE_STATISTICS_ENABLED\"] = \"False\"\nos.environ[\"GE_HOME\"] = \"/tmp/great_expectations_home\"\nos.environ[\"GE_CONFIG_VERSION\"] = \"3\"  # Force using V3 config\nos.environ[\"GE_UNCOMMITTED_DIRECTORIES\"] = \"True\"  # Skip Git checks\nos.environ[\"GX_ASSUME_MISSING_LIBRARIES\"] = \"git\"\n\n# Mock out git before anything tries to import it\nclass MockGit:\n    class Repo:\n        @staticmethod\n        def init(*args, **kwargs):\n            pass\n    \n    class NoSuchPathError(Exception):\n        pass\n\n# Add the mock to sys.modules\nsys.modules['git'] = MockGit\n\n# Now continue with the regular imports\nfrom kfp import dsl\nfrom kfp import compiler\nfrom kfp.dsl import Dataset, Input, Output, Model, Metrics, component\nimport json\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler\nimport joblib\nimport mlflow\nfrom typing import List\n@component(\n    base_image='pes1ug19cs601/wine-quality-mlops:latest'\n)\ndef preprocess(\n    data_path: str,\n    features: Output[Dataset],\n    labels: Output[Dataset],\n    scaler: Output[Model]\n):\n    \"\"\"Preprocess the wine quality data.\"\"\"\n    # Load data\n    df = pd.read_csv(data_path, sep=\";\")\n\n    # Split features and labels\n    X = df.drop('quality', axis=1)\n    y = df['quality']\n\n    # Scale features\n    scaler_obj = StandardScaler()\n    X_scaled = scaler_obj.fit_transform(X)\n\n    # Save processed data\n    np.save(features.path, X_scaled)\n    np.save(labels.path, y.values)\n    joblib.dump(scaler_obj, scaler.path)\n\n    print(f\"Preprocessed features saved to {features.path}\")\n    print(f\"Preprocessed labels saved to {labels.path}\")\n    print(f\"Scaler saved to {scaler.path}\")\n\n@component(\n    base_image='pes1ug19cs601/wine-quality-mlops:latest',\n    packages_to_install=['xgboost', 'lightgbm']\n)\ndef train(\n    features: Input[Dataset],\n    labels: Input[Dataset],\n    hyperparameters: dict,\n    model: Output[Model],\n    metrics: Output[Metrics],\n    scaler: Input[Model],\n    model_type: str = \"RandomForest\"  # Default argument moved to the end\n):\n    \"\"\"Train a model with support for multiple algorithms.\"\"\"\n    import os\n    import joblib\n    import json\n    import numpy as np\n    from sklearn.model_selection import train_test_split\n    from sklearn.ensemble import RandomForestRegressor\n    import xgboost as xgb\n    import lightgbm as lgb\n    import mlflow\n    import time\n    from prometheus_client import start_http_server, Gauge\n\n    # Load data\n    X = np.load(features.path)\n    y = np.load(labels.path)\n\n    # Split data\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n    # Initialize model based on model_type\n    print(f\"Training {model_type} model with hyperparameters: {hyperparameters}\")\n    if model_type == \"RandomForest\":\n        model_obj = RandomForestRegressor(**hyperparameters)\n    elif model_type == \"XGBoost\":\n        model_obj = xgb.XGBRegressor(**hyperparameters)\n    elif model_type == \"LightGBM\":\n        model_obj = lgb.LGBMRegressor(**hyperparameters)\n    else:\n        raise ValueError(f\"Unsupported model type: {model_type}\")\n    \n    start_time = time.time()\n    # Train model\n    model_obj.fit(X_train, y_train)\n    \n    # Evaluate model\n    train_score = model_obj.score(X_train, y_train)\n    test_score = model_obj.score(X_test, y_test)\n    \n    # Save metrics\n    metrics_dict = {\n        'model_type': model_type,\n        'train_r2': float(train_score),\n        'test_r2': float(test_score)\n    }\n    \n    with open(metrics.path, 'w') as f:\n        json.dump(metrics_dict, f, indent=2)\n    \n    # Save model\n    joblib.dump(model_obj, model.path)\n    \n    # Only log to MLflow if not in testing mode and if MLflow server is available\n    if not os.environ.get(\"TESTING\", \"False\").lower() == \"true\":\n        try:\n            # Set a timeout for MLflow connection attempts\n            import socket\n            socket.setdefaulttimeout(5)  # 5 second timeout\n            \n            # Try to connect to MLflow server\n            mlflow.set_tracking_uri('http://mlflow-service.mlops.svc.cluster.local:5000')\n            with mlflow.start_run():\n                mlflow.log_param(\"model_type\", model_type)\n                mlflow.log_params(hyperparameters)\n                mlflow.log_metric(\"train_r2\", train_score)\n                mlflow.log_metric(\"test_r2\", test_score)\n                mlflow.sklearn.log_model(model_obj, \"model\")\n                mlflow.log_artifact(scaler.path, \"preprocessor\")\n                print(\"Successfully logged metrics to MLflow\")\n        except Exception as e:\n            print(f\"MLflow logging failed (this is expected in local environments): {e}\")\n            print(\"Continuing without MLflow logging\")\n    else:\n        print(\"Testing mode active, skipping MLflow logging\")\n    \n    print(f\"Model saved to {model.path}\")\n    print(f\"Metrics saved to {metrics.path}\")\n    print(f\"Training metrics: {metrics_dict}\")\n    # Add Prometheus metrics server (after model evaluation)\n    prom_port = int(os.getenv(\"PROMETHEUS_PORT\", 8000))\n    start_http_server(prom_port)\n    \n    # Create Prometheus gauges\n    train_r2_gauge = Gauge('train_r2_score', 'Training R\u00b2 score', ['model_type'])\n    test_r2_gauge = Gauge('test_r2_score', 'Test R\u00b2 score', ['model_type'])\n    training_time_gauge = Gauge('training_time_seconds', 'Training duration')\n    \n    # Set metric values\n    train_r2_gauge.labels(model_type=model_type).set(train_score)\n    test_r2_gauge.labels(model_type=model_type).set(test_score)\n    training_time_gauge.set(time.time() - start_time)\n\n@component(\n    base_image='pes1ug19cs601/wine-quality-mlops:latest'\n)\ndef select_best_model(\n    best_model: Output[Model],  \n    best_metrics: Output[Metrics],  \n    model1: Input[Model] = None,  \n    model2: Input[Model] = None, \n    model3: Input[Model] = None,  \n    metrics1: Input[Metrics] = None,  \n    metrics2: Input[Metrics] = None,  \n    metrics3: Input[Metrics] = None  \n):\n    \"\"\"Select the best model based on test R\u00b2 score.\"\"\"\n    import json\n    import shutil\n    import copy\n    \n    best_score = -float('inf')\n    best_model_idx = -1\n    all_metrics_summary = []\n    \n    # Create lists of non-None inputs\n    models = [m for m in [model1, model2, model3] if m is not None]\n    metrics_files = [m for m in [metrics1, metrics2, metrics3] if m is not None]\n    \n    # Load all metrics and find the best model\n    for i, metric_file in enumerate(metrics_files):\n        with open(metric_file.path, 'r') as f:\n            metric_data = json.load(f)\n            \n        # Create a simplified summary to avoid circular references\n        metric_summary = {\n            \"model_type\": metric_data.get(\"model_type\", \"Unknown\"),\n            \"test_r2\": float(metric_data.get(\"test_r2\", -1.0)),\n            \"train_r2\": float(metric_data.get(\"train_r2\", -1.0))\n        }\n        \n        all_metrics_summary.append(metric_summary)\n        test_r2 = metric_summary[\"test_r2\"]\n        \n        if test_r2 > best_score:\n            best_score = test_r2\n            best_model_idx = i\n    \n    # Copy the best model and its metrics\n    if best_model_idx >= 0:\n        shutil.copy(models[best_model_idx].path, best_model.path)\n        \n        # Create a new dictionary for best metrics to avoid reference issues\n        best_metric_data = copy.deepcopy(all_metrics_summary[best_model_idx])\n        best_metric_data[\"model_comparison\"] = [\n            {\"model\": m[\"model_type\"], \"test_r2\": m[\"test_r2\"]}\n            for m in all_metrics_summary\n        ]\n        \n        with open(best_metrics.path, 'w') as f:\n            json.dump(best_metric_data, f, indent=2)\n            \n        print(f\"Selected best model: {best_metric_data['model_type']}\")\n        print(f\"Best test R\u00b2: {best_score:.4f}\")\n    else:\n        raise ValueError(\"No valid models found\")\n\n\n@component(\n    base_image='pes1ug19cs601/wine-quality-mlops:latest'\n)\ndef validate_data(\n    data_path: str,\n    metrics: Output[Metrics],\n    validation_success: Output[bool]\n):\n    \"\"\"Validate wine data for drift using Great Expectations with Git functionality disabled.\"\"\"\n    import os\n    import sys\n    import json\n    import pandas as pd\n\n    import logging\n    # Configure root logger and specific loggers to suppress debug messages\n    logging.getLogger().setLevel(logging.ERROR)  # Set root logger to ERROR\n    logging.getLogger('great_expectations').setLevel(logging.ERROR)\n    logging.getLogger('great_expectations.expectations.registry').setLevel(logging.CRITICAL)  # Even stricter for registry\n\n    \n    # Set environment variables\n    os.environ[\"GE_USAGE_STATISTICS_ENABLED\"] = \"False\"\n    os.environ[\"GE_UNCOMMITTED_DIRECTORIES\"] = \"True\"\n    os.environ[\"GX_ASSUME_MISSING_LIBRARIES\"] = \"git\"\n    \n    \n    # Import Great Expectations components\n    from great_expectations.data_context.types.base import DataContextConfig\n    from great_expectations.data_context import BaseDataContext\n    from great_expectations.core.batch import RuntimeBatchRequest\n    \n    try:\n        # Load data\n        print(\"Loading data...\")\n        df = pd.read_csv(data_path)\n        \n        # Create context configuration\n        print(\"Creating context configuration...\")\n        context_config = DataContextConfig(\n            store_backend_defaults=None,\n            checkpoint_store_name=None,\n            datasources={\n                \"pandas_datasource\": {\n                    \"class_name\": \"Datasource\",\n                    \"module_name\": \"great_expectations.datasource\",\n                    \"execution_engine\": {\n                        \"class_name\": \"PandasExecutionEngine\",\n                        \"module_name\": \"great_expectations.execution_engine\"\n                    },\n                    \"data_connectors\": {\n                        \"runtime_connector\": {\n                            \"class_name\": \"RuntimeDataConnector\",\n                            \"module_name\": \"great_expectations.datasource.data_connector\",\n                            \"batch_identifiers\": [\"batch_id\"]\n                        }\n                    }\n                }\n            }\n        )\n        \n        # Create context\n        print(\"Creating context...\")\n        context = BaseDataContext(project_config=context_config)\n        \n        # Create expectation suite\n        print(\"Creating expectation suite...\")\n        suite_name = \"wine_quality_suite\"\n        context.create_expectation_suite(suite_name, overwrite_existing=True)\n        \n        # Create batch request\n        print(\"Creating batch request...\")\n        batch_request = RuntimeBatchRequest(\n            datasource_name=\"pandas_datasource\",\n            data_connector_name=\"runtime_connector\",\n            data_asset_name=\"wine_data\",\n            runtime_parameters={\"batch_data\": df},\n            batch_identifiers={\"batch_id\": \"default_identifier\"},\n        )\n        \n        # Get validator\n        print(\"Getting validator...\")\n        validator = context.get_validator(\n            batch_request=batch_request,\n            expectation_suite_name=suite_name\n        )\n        \n        # Add expectations\n        print(\"Adding expectations...\")\n        expectations = []\n        \n        # Check that columns match expected list\n        expectations.append(validator.expect_table_columns_to_match_ordered_list(list(df.columns)))\n        \n        # Check data types\n        expectations.append(validator.expect_column_values_to_be_of_type(\"fixed acidity\", \"float64\"))\n        expectations.append(validator.expect_column_values_to_be_of_type(\"volatile acidity\", \"float64\"))\n        expectations.append(validator.expect_column_values_to_be_of_type(\"citric acid\", \"float64\"))\n        expectations.append(validator.expect_column_values_to_be_of_type(\"residual sugar\", \"float64\"))\n        expectations.append(validator.expect_column_values_to_be_of_type(\"chlorides\", \"float64\"))\n        expectations.append(validator.expect_column_values_to_be_of_type(\"free sulfur dioxide\", \"float64\"))\n        expectations.append(validator.expect_column_values_to_be_of_type(\"total sulfur dioxide\", \"float64\"))\n        expectations.append(validator.expect_column_values_to_be_of_type(\"density\", \"float64\"))\n        expectations.append(validator.expect_column_values_to_be_of_type(\"pH\", \"float64\"))\n        expectations.append(validator.expect_column_values_to_be_of_type(\"sulphates\", \"float64\"))\n        expectations.append(validator.expect_column_values_to_be_of_type(\"alcohol\", \"float64\"))\n        expectations.append(validator.expect_column_values_to_be_of_type(\"quality\", \"int64\"))\n        \n        # Check value ranges\n        expectations.append(validator.expect_column_values_to_be_between(\"fixed acidity\", min_value=3.8, max_value=15.9))\n        expectations.append(validator.expect_column_values_to_be_between(\"volatile acidity\", min_value=0.08, max_value=1.58))\n        expectations.append(validator.expect_column_values_to_be_between(\"citric acid\", min_value=0, max_value=1.66))\n        expectations.append(validator.expect_column_values_to_be_between(\"residual sugar\", min_value=0.6, max_value=65.8))\n        expectations.append(validator.expect_column_values_to_be_between(\"chlorides\", min_value=0.009, max_value=0.611))\n        expectations.append(validator.expect_column_values_to_be_between(\"free sulfur dioxide\", min_value=1, max_value=289))\n        expectations.append(validator.expect_column_values_to_be_between(\"total sulfur dioxide\", min_value=6, max_value=440))\n        expectations.append(validator.expect_column_values_to_be_between(\"density\", min_value=0.98711, max_value=1.03898))\n        expectations.append(validator.expect_column_values_to_be_between(\"pH\", min_value=2.72, max_value=4.01))\n        expectations.append(validator.expect_column_values_to_be_between(\"sulphates\", min_value=0.22, max_value=2))\n        expectations.append(validator.expect_column_values_to_be_between(\"alcohol\", min_value=8, max_value=14.9))\n        expectations.append(validator.expect_column_values_to_be_between(\"quality\", min_value=3, max_value=9))\n        \n        # Check for missing values\n        for column in df.columns:\n            expectations.append(validator.expect_column_values_to_not_be_null(column))\n        \n        # Run validation\n        print(\"Running validation...\")\n        validation_results = validator.validate()\n        \n        # Process results\n        print(\"Processing results...\")\n        validation_passed = validation_results.success\n        \n        # Prepare metrics\n        validation_metrics = {\n            \"validation_success\": validation_passed,\n            \"evaluated_expectations\": validation_results.statistics[\"evaluated_expectations\"],\n            \"successful_expectations\": validation_results.statistics[\"successful_expectations\"],\n            \"unsuccessful_expectations\": validation_results.statistics[\"unsuccessful_expectations\"],\n        }\n        \n        # Log metrics\n        print(\"Logging metrics...\")\n        metrics.log_metric(\"validation_success\", float(validation_passed))\n        metrics.log_metric(\"evaluated_expectations\", float(validation_results.statistics[\"evaluated_expectations\"]))\n        metrics.log_metric(\"successful_expectations\", float(validation_results.statistics[\"successful_expectations\"]))\n        metrics.log_metric(\"unsuccessful_expectations\", float(validation_results.statistics[\"unsuccessful_expectations\"]))\n        \n        print(f\"Validation {'passed' if validation_passed else 'failed'}\")\n        print(f\"Metrics: {validation_metrics}\")\n        \n        validation_success = validation_passed\n        with open(validation_success.path, 'w') as f:\n            f.write(str(validation_passed).lower())\n        return validation_metrics\n        \n    except Exception as e:\n        print(f\"Error in validate_data: {type(e).__name__}: {str(e)}\")\n        import traceback\n        print(traceback.format_exc())\n        # Log failure in metrics\n        metrics.log_metric(\"validation_success\", 0.0)\n        metrics.log_metric(\"error\", 1.0)\n        with open(validation_success.path, 'w') as f:\n            f.write(\"False\")\n        raise\n\n@component(\n    base_image='pes1ug19cs601/wine-quality-mlops:latest',\n    packages_to_install=['kserve==0.10.1']\n)\ndef deploy_model(\n    model: Input[Model],\n    metrics: Input[Metrics],\n    scaler: Input[Model],\n    service_name: str,\n    namespace: str = \"kubeflow\"\n) -> str:\n    import os\n    import json\n    import pickle\n    import tempfile\n    from kubernetes import client\n    from kubernetes import config\n    from kserve import KServeClient\n    from kserve import constants\n    from kserve import V1beta1InferenceService\n    from kserve import V1beta1InferenceServiceSpec\n    from kserve import V1beta1PredictorSpec\n    from kserve import V1beta1SKLearnSpec\n    \n    # Create a temporary directory to prepare the model\n    model_dir = tempfile.mkdtemp()\n    os.makedirs(os.path.join(model_dir, \"model\"), exist_ok=True)\n    \n    # Load and save the model and scaler\n    with open(model.path, 'rb') as f:\n        model_obj = pickle.load(f)\n        \n    with open(scaler.path, 'rb') as f:\n        scaler_obj = pickle.load(f)\n    \n    # Save model and scaler to the temporary directory\n    with open(os.path.join(model_dir, \"model\", \"model.pkl\"), 'wb') as f:\n        pickle.dump(model_obj, f)\n        \n    with open(os.path.join(model_dir, \"model\", \"scaler.pkl\"), 'wb') as f:\n        pickle.dump(scaler_obj, f)\n    \n    # Load metrics to include in model metadata\n    with open(metrics.path, 'r') as f:\n        metrics_data = json.load(f)\n    \n    # Create a metadata file\n    metadata = {\n        \"name\": service_name,\n        \"version\": \"v1\",\n        \"metrics\": metrics_data\n    }\n    \n    with open(os.path.join(model_dir, \"model\", \"metadata.json\"), 'w') as f:\n        json.dump(metadata, f)\n    \n    # Create a simple inference script\n    inference_script = \"\"\"\nimport os\nimport pickle\nimport json\nimport numpy as np\n\nclass WineQualityModel(object):\n    def __init__(self):\n        self.model = None\n        self.scaler = None\n        self.ready = False\n        \n    def load(self):\n        model_dir = os.path.join(os.getcwd(), \"model\")\n        with open(os.path.join(model_dir, \"model.pkl\"), \"rb\") as f:\n            self.model = pickle.load(f)\n        with open(os.path.join(model_dir, \"scaler.pkl\"), \"rb\") as f:\n            self.scaler = pickle.load(f)\n        self.ready = True\n        \n    def predict(self, X, feature_names=None):\n        if not self.ready:\n            self.load()\n        X_scaled = self.scaler.transform(X)\n        predictions = self.model.predict(X_scaled)\n        return predictions.tolist()\n\"\"\"\n    \n    with open(os.path.join(model_dir, \"model\", \"WineQualityModel.py\"), 'w') as f:\n        f.write(inference_script)\n    \n    # Create a simple requirements.txt\n    with open(os.path.join(model_dir, \"model\", \"requirements.txt\"), 'w') as f:\n        f.write(\"scikit-learn==1.0.2\\nnumpy==1.22.3\\n\")\n    \n    # Upload the model to a storage location (MinIO, S3, etc.)\n    # Assuming you have a PVC for model storage\n    model_uri = f\"pvc://{service_name}-models\"\n    \n    # In a real implementation, you would upload the model to your storage\n    # For now, printing the path and assuming it's accessible\n    print(f\"Model prepared at: {model_dir}\")\n    print(f\"Model would be deployed from: {model_uri}\")\n    \n    try:\n        # Initialize KServe client\n        config.load_incluster_config()\n        kserve_client = KServeClient()\n        \n        # Define the inference service\n        isvc = V1beta1InferenceService(\n            api_version=constants.KSERVE_V1BETA1,\n            kind=constants.KSERVE_KIND,\n            metadata=client.V1ObjectMeta(\n                name=service_name,\n                namespace=namespace,\n                annotations={\"sidecar.istio.io/inject\": \"false\"}\n            ),\n            spec=V1beta1InferenceServiceSpec(\n                predictor=V1beta1PredictorSpec(\n                    sklearn=V1beta1SKLearnSpec(\n                        storage_uri=model_uri,\n                        resources=client.V1ResourceRequirements(\n                            requests={\"cpu\": \"100m\", \"memory\": \"1Gi\"},\n                            limits={\"cpu\": \"1\", \"memory\": \"2Gi\"}\n                        )\n                    )\n                )\n            )\n        )\n        \n        # Create the inference service\n        kserve_client.create(isvc)\n        print(f\"Inference service '{service_name}' created in namespace '{namespace}'\")\n        \n        # Get the URL of the deployed service\n        service_url = f\"http://{service_name}.{namespace}.svc.cluster.local/v1/models/{service_name}:predict\"\n        return service_url\n        \n    except Exception as e:\n        print(f\"Error deploying model: {str(e)}\")\n        return f\"Deployment failed: {str(e)}\"\n\nif os.environ.get(\"TESTING\", \"False\").lower() != \"true\":\n    @dsl.pipeline(\n    name='wine-quality-pipeline',\n    description='End-to-end ML pipeline for wine quality prediction with model selection'\n)\n    def wine_quality_pipeline(\n        data_path: str = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv',\n        # Model types to train\n        use_random_forest: bool = True,\n        use_xgboost: bool = True,\n        use_lightgbm: bool = True,  \n        # RandomForest parameters\n        rf_n_estimators: int = 100,\n        rf_max_depth: int = None,\n        rf_min_samples_split: int = 2,\n        # XGBoost parameters\n        xgb_n_estimators: int = 100,\n        xgb_max_depth: int = 6,\n        xgb_learning_rate: float = 0.1,\n        # LightGBM parameters\n        lgbm_n_estimators: int = 100,\n        lgbm_max_depth: int = -1,\n        lgbm_learning_rate: float = 0.1,\n        # Service parameters\n        service_name: str = 'wine-quality-predictor'\n    ):\n        \"\"\"Define the wine quality prediction pipeline with model selection.\"\"\"\n        \n        # Construct hyperparameters dictionaries from pipeline parameters\n        hyperparameters = {\n            \"RandomForest\": {\n                'n_estimators': rf_n_estimators,\n                'max_depth': rf_max_depth,\n                'min_samples_split': rf_min_samples_split,\n                'random_state': 42\n            },\n            \"XGBoost\": {\n                'n_estimators': xgb_n_estimators,\n                'max_depth': xgb_max_depth,\n                'learning_rate': xgb_learning_rate,\n                'random_state': 42\n            },\n            \"LightGBM\": {\n                'n_estimators': lgbm_n_estimators,\n                'max_depth': lgbm_max_depth,\n                'learning_rate': lgbm_learning_rate,\n                'random_state': 42\n            }\n        }\n\n        # Validate data for drift\n        validation_task = validate_data(data_path=data_path)\n        \n        with dsl.Condition(validation_task.outputs[\"validation_success\"] == True):\n            # Preprocess data (only if validation passes)\n            preprocess_task = preprocess(data_path=data_path).after(validation_task)\n            \n            # Create separate tasks for each model type to avoid using ParallelFor\n            rf_task = None\n            xgb_task = None\n            lgbm_task = None\n            \n            # Create model training tasks individually\n            if use_random_forest:\n                rf_task = train(\n                    features=preprocess_task.outputs['features'],\n                    labels=preprocess_task.outputs['labels'],\n                    hyperparameters=hyperparameters[\"RandomForest\"],\n                    model_type=\"RandomForest\",\n                    scaler=preprocess_task.outputs['scaler']\n                ).set_env_variable(name=\"PROMETHEUS_PORT\", value=\"8000\")\n                \n            if use_xgboost:\n                xgb_task = train(\n                    features=preprocess_task.outputs['features'],\n                    labels=preprocess_task.outputs['labels'],\n                    hyperparameters=hyperparameters[\"XGBoost\"],\n                    model_type=\"XGBoost\",\n                    scaler=preprocess_task.outputs['scaler']\n                ).set_env_variable(name=\"PROMETHEUS_PORT\", value=\"8000\")\n            \n            if use_lightgbm:\n                lgbm_task = train(\n                    features=preprocess_task.outputs['features'],\n                    labels=preprocess_task.outputs['labels'],\n                    hyperparameters=hyperparameters[\"LightGBM\"],\n                    model_type=\"LightGBM\",\n                    scaler=preprocess_task.outputs['scaler']\n                ).set_env_variable(name=\"PROMETHEUS_PORT\", value=\"8000\")\n            \n            # Collect models and metrics from all active tasks\n            model_list = []\n            metrics_list = []\n            \n            # Add outputs from each model training task if it exists\n            for task in [rf_task, xgb_task, lgbm_task]:\n                if task is not None:\n                    model_list.append(task.outputs['model'])\n                    metrics_list.append(task.outputs['metrics'])\n            \n            # Select the best model using individual inputs\n            select_model_task = select_best_model(\n            model1=rf_task.outputs['model'] if rf_task else None,\n            model2=xgb_task.outputs['model'] if xgb_task else None,\n            model3=lgbm_task.outputs['model'] if lgbm_task else None,\n            metrics1=rf_task.outputs['metrics'] if rf_task else None,\n            metrics2=xgb_task.outputs['metrics'] if xgb_task else None,\n            metrics3=lgbm_task.outputs['metrics'] if lgbm_task else None\n        )\n\n            \n            # Deploy only the best model\n            deploy_task = deploy_model(\n                model=select_model_task.outputs['best_model'],\n                metrics=select_model_task.outputs['best_metrics'],\n                scaler=preprocess_task.outputs['scaler'],\n                service_name=service_name\n            )\n\n    if __name__ == '__main__':\n        # Compile the pipeline\n        compiler.Compiler().compile(\n            pipeline_func=wine_quality_pipeline,\n            package_path='wine_quality_pipeline.yaml'\n        ) "
  },
  {
    "repo": "shashnavad/wine-quality-mlops",
    "file_path": "pipelines/.ipynb_checkpoints/wine_quality_pipeline-checkpoint.py",
    "raw_url": "https://raw.githubusercontent.com/shashnavad/wine-quality-mlops/main/pipelines/.ipynb_checkpoints/wine_quality_pipeline-checkpoint.py",
    "content": "import kfp\nfrom kfp import dsl\nfrom kfp.components import create_component_from_func\nfrom typing import NamedTuple\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport mlflow\nimport os\n\n\n@create_component_from_func\ndef download_data() -> NamedTuple('Outputs', [('dataset_path', str)]):\n    \"\"\"Download wine quality dataset and perform initial validation.\"\"\"\n    import pandas as pd\n    import requests\n    import os\n    from collections import namedtuple\n    \n    # Create data directory\n    os.makedirs(\"/tmp/data\", exist_ok=True)\n    \n    # Download dataset\n    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n    response = requests.get(url)\n    \n    # Save dataset\n    dataset_path = \"/tmp/data/winequality-red.csv\"\n    with open(dataset_path, \"wb\") as f:\n        f.write(response.content)\n    \n    print(f\"Dataset downloaded to {dataset_path}\")\n    \n    Outputs = namedtuple('Outputs', ['dataset_path'])\n    return Outputs(dataset_path)\n\n\n@create_component_from_func\ndef validate_data(dataset_path: str) -> NamedTuple('Outputs', [('validation_status', str), ('validation_report', str)]):\n    \"\"\"Validate data quality using basic checks.\"\"\"\n    import pandas as pd\n    import json\n    import os\n    from collections import namedtuple\n    \n    # Load data\n    df = pd.read_csv(dataset_path, sep=\";\")\n    \n    # Create validation report\n    validation_report = {\n        \"num_rows\": len(df),\n        \"num_columns\": len(df.columns),\n        \"columns\": list(df.columns),\n        \"missing_values\": df.isnull().sum().to_dict(),\n        \"data_types\": {col: str(dtype) for col, dtype in df.dtypes.items()},\n        \"summary_statistics\": {\n            col: {\n                \"min\": float(df[col].min()),\n                \"max\": float(df[col].max()),\n                \"mean\": float(df[col].mean()),\n                \"median\": float(df[col].median()),\n                \"std\": float(df[col].std())\n            } for col in df.columns\n        }\n    }\n    \n    # Check for validation issues\n    validation_issues = []\n    \n    # Check for missing values\n    if df.isnull().any().any():\n        validation_issues.append(\"Dataset contains missing values\")\n    \n    # Check for quality range\n    if df[\"quality\"].min() < 0 or df[\"quality\"].max() > 10:\n        validation_issues.append(\"Quality values outside expected range (0-10)\")\n    \n    # Determine validation status\n    if validation_issues:\n        validation_status = \"failed: \" + \"; \".join(validation_issues)\n    else:\n        validation_status = \"passed\"\n    \n    # Save validation report\n    os.makedirs(\"/tmp/reports\", exist_ok=True)\n    report_path = \"/tmp/reports/validation_report.json\"\n    with open(report_path, \"w\") as f:\n        json.dump(validation_report, f, indent=2)\n    \n    print(f\"Validation status: {validation_status}\")\n    print(f\"Validation report saved to {report_path}\")\n    \n    Outputs = namedtuple('Outputs', ['validation_status', 'validation_report'])\n    return Outputs(validation_status, json.dumps(validation_report))\n\n\n@create_component_from_func\ndef preprocess_data(dataset_path: str) -> NamedTuple('Outputs', [('features_path', str), ('labels_path', str), ('scaler_path', str)]):\n    \"\"\"Preprocess the wine quality dataset.\"\"\"\n    import pandas as pd\n    import numpy as np\n    from sklearn.preprocessing import StandardScaler\n    import joblib\n    import os\n    from collections import namedtuple\n    \n    # Load data\n    df = pd.read_csv(dataset_path, sep=\";\")\n    \n    # Split features and labels\n    X = df.drop('quality', axis=1)\n    y = df['quality']\n    \n    # Scale features\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n    \n    # Create output directory\n    os.makedirs(\"/tmp/processed\", exist_ok=True)\n    \n    # Save processed data\n    features_path = \"/tmp/processed/features.npy\"\n    labels_path = \"/tmp/processed/labels.npy\"\n    scaler_path = \"/tmp/processed/scaler.joblib\"\n    \n    np.save(features_path, X_scaled)\n    np.save(labels_path, y.values)\n    joblib.dump(scaler, scaler_path)\n    \n    print(f\"Preprocessed features saved to {features_path}\")\n    print(f\"Preprocessed labels saved to {labels_path}\")\n    print(f\"Scaler saved to {scaler_path}\")\n    \n    Outputs = namedtuple('Outputs', ['features_path', 'labels_path', 'scaler_path'])\n    return Outputs(features_path, labels_path, scaler_path)\n\n\n@create_component_from_func\ndef train_model(features_path: str, labels_path: str, hyperparameters: str) -> NamedTuple('Outputs', [('model_path', str)]):\n    \"\"\"Train a machine learning model.\"\"\"\n    import numpy as np\n    from sklearn.ensemble import RandomForestRegressor\n    from sklearn.model_selection import train_test_split\n    import joblib\n    import json\n    import os\n    from collections import namedtuple\n    \n    # Load data\n    X = np.load(features_path)\n    y = np.load(labels_path)\n    \n    # Parse hyperparameters\n    params = json.loads(hyperparameters)\n    \n    # Split data\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n    # Train model\n    model = RandomForestRegressor(**params)\n    model.fit(X_train, y_train)\n    \n    # Create output directory\n    os.makedirs(\"/tmp/models\", exist_ok=True)\n    \n    # Save model\n    model_path = \"/tmp/models/model.joblib\"\n    joblib.dump(model, model_path)\n    \n    print(f\"Model saved to {model_path}\")\n    \n    Outputs = namedtuple('Outputs', ['model_path'])\n    return Outputs(model_path)\n\n\n@create_component_from_func\ndef evaluate_model(model_path: str, features_path: str, labels_path: str) -> NamedTuple('Outputs', [('metrics_path', str)]):\n    \"\"\"Evaluate the trained model.\"\"\"\n    import numpy as np\n    import joblib\n    from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n    import json\n    import os\n    from collections import namedtuple\n    \n    # Load data and model\n    X = np.load(features_path)\n    y = np.load(labels_path)\n    model = joblib.load(model_path)\n    \n    # Split data\n    from sklearn.model_selection import train_test_split\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n    # Make predictions\n    y_pred = model.predict(X_test)\n    \n    # Calculate metrics\n    metrics = {\n        'mse': float(mean_squared_error(y_test, y_pred)),\n        'rmse': float(np.sqrt(mean_squared_error(y_test, y_pred))),\n        'mae': float(mean_absolute_error(y_test, y_pred)),\n        'r2': float(r2_score(y_test, y_pred))\n    }\n    \n    # Create output directory\n    os.makedirs(\"/tmp/metrics\", exist_ok=True)\n    \n    # Save metrics\n    metrics_path = \"/tmp/metrics/metrics.json\"\n    with open(metrics_path, 'w') as f:\n        json.dump(metrics, f, indent=2)\n    \n    print(f\"Metrics saved to {metrics_path}\")\n    print(f\"Model evaluation metrics: {metrics}\")\n    \n    Outputs = namedtuple('Outputs', ['metrics_path'])\n    return Outputs(metrics_path)\n\n\n@create_component_from_func\ndef deploy_model(model_path: str, scaler_path: str, metrics_path: str) -> NamedTuple('Outputs', [('deployment_path', str)]):\n    \"\"\"Package the model for deployment.\"\"\"\n    import joblib\n    import json\n    import os\n    import shutil\n    from collections import namedtuple\n    \n    # Create deployment directory\n    deployment_dir = \"/tmp/deployment\"\n    os.makedirs(deployment_dir, exist_ok=True)\n    \n    # Copy model and scaler\n    shutil.copy(model_path, os.path.join(deployment_dir, \"model.joblib\"))\n    shutil.copy(scaler_path, os.path.join(deployment_dir, \"scaler.joblib\"))\n    \n    # Load and save metrics\n    with open(metrics_path, 'r') as f:\n        metrics = json.load(f)\n    \n    with open(os.path.join(deployment_dir, \"metrics.json\"), 'w') as f:\n        json.dump(metrics, f, indent=2)\n    \n    # Create deployment info\n    deployment_info = {\n        \"model_file\": \"model.joblib\",\n        \"scaler_file\": \"scaler.joblib\",\n        \"metrics_file\": \"metrics.json\",\n        \"api_endpoint\": \"/predict\",\n        \"input_format\": {\n            \"fixed_acidity\": \"float\",\n            \"volatile_acidity\": \"float\",\n            \"citric_acid\": \"float\",\n            \"residual_sugar\": \"float\",\n            \"chlorides\": \"float\",\n            \"free_sulfur_dioxide\": \"float\",\n            \"total_sulfur_dioxide\": \"float\",\n            \"density\": \"float\",\n            \"pH\": \"float\",\n            \"sulphates\": \"float\",\n            \"alcohol\": \"float\"\n        },\n        \"output_format\": {\n            \"quality\": \"float\",\n            \"confidence\": \"float\"\n        }\n    }\n    \n    # Save deployment info\n    with open(os.path.join(deployment_dir, \"deployment_info.json\"), 'w') as f:\n        json.dump(deployment_info, f, indent=2)\n    \n    print(f\"Model packaged for deployment in {deployment_dir}\")\n    \n    Outputs = namedtuple('Outputs', ['deployment_path'])\n    return Outputs(deployment_dir)\n\n\n@dsl.pipeline(\n    name='Wine Quality Pipeline',\n    description='End-to-end ML pipeline for wine quality prediction'\n)\ndef wine_quality_pipeline(\n    n_estimators: int = 100,\n    max_depth: str = \"None\",\n    min_samples_split: int = 2,\n    min_samples_leaf: int = 1,\n    random_state: int = 42\n):\n    \"\"\"Define the wine quality ML pipeline.\"\"\"\n    \n    # Convert max_depth to proper format for JSON\n    max_depth_value = None if max_depth == \"None\" else int(max_depth)\n    \n    # Create hyperparameters JSON\n    hyperparameters = {\n        \"n_estimators\": n_estimators,\n        \"max_depth\": max_depth_value,\n        \"min_samples_split\": min_samples_split,\n        \"min_samples_leaf\": min_samples_leaf,\n        \"random_state\": random_state\n    }\n    \n    # Download and validate data\n    download_task = download_data()\n    validate_task = validate_data(download_task.outputs[\"dataset_path\"])\n    \n    # Add validation check\n    with dsl.Condition(validate_task.outputs[\"validation_status\"] == \"passed\"):\n        # Preprocess data\n        preprocess_task = preprocess_data(download_task.outputs[\"dataset_path\"])\n        \n        # Train model\n        train_task = train_model(\n            preprocess_task.outputs[\"features_path\"],\n            preprocess_task.outputs[\"labels_path\"],\n            hyperparameters=str(hyperparameters).replace(\"'\", \"\\\"\").replace(\"None\", \"null\")\n        )\n        \n        # Evaluate model\n        evaluate_task = evaluate_model(\n            train_task.outputs[\"model_path\"],\n            preprocess_task.outputs[\"features_path\"],\n            preprocess_task.outputs[\"labels_path\"]\n        )\n        \n        # Deploy model\n        deploy_task = deploy_model(\n            train_task.outputs[\"model_path\"],\n            preprocess_task.outputs[\"scaler_path\"],\n            evaluate_task.outputs[\"metrics_path\"]\n        )\n\n\nif __name__ == '__main__':\n    # Compile the pipeline\n    kfp.compiler.Compiler().compile(wine_quality_pipeline, 'wine_quality_pipeline.yaml') "
  },
  {
    "repo": "levitomer/kubeflow-pipeline-demo",
    "file_path": "pipeline/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/levitomer/kubeflow-pipeline-demo/master/pipeline/pipeline.py",
    "content": "import kfp\nfrom kfp import dsl\nfrom kfp import compiler\n\ndef preprocess_op():\n\n    # It would be much cleaner to have our component accept the file paths as command-line arguments.\n    return dsl.ContainerOp(\n        name='Preprocess Data',\n        image='tomerlev/pipeline_preprocessing:latest',\n        arguments=[],\n        file_outputs={\n            'x_train': '/app/x_train.npy',\n            'x_test': '/app/x_test.npy',\n            'y_train': '/app/y_train.npy',\n            'y_test': '/app/y_test.npy',\n        }\n    )\n\ndef train_op(x_train, y_train):\n\n    return dsl.ContainerOp(\n        name='Train Model',\n        image='tomerlev/pipeline_train:latest',\n        arguments=[\n            '--x_train', x_train,\n            '--y_train', y_train\n        ],\n        file_outputs={\n            'model': '/app/model.pkl'\n        }\n    )\n\ndef test_op(x_test, y_test, model):\n\n    return dsl.ContainerOp(\n        name='Test Model',\n        image='tomerlev/pipeline_test:latest',\n        arguments=[\n            '--x_test', x_test,\n            '--y_test', y_test,\n            '--model', model\n        ],\n        file_outputs={\n            'mean_squared_error': '/app/output.txt'\n        }\n    )\n\ndef deploy_model_op(model):\n\n    return dsl.ContainerOp(\n        name='Deploy Model',\n        image='tomerlev/pipeline_deploy_model:latest',\n        arguments=[\n            '--model', model\n        ]\n    )\n\n@dsl.pipeline(\n   name='Pipeline',\n   description='An example pipeline that trains and logs a model.'\n)\ndef pipeline():\n    _preprocess_op = preprocess_op()\n\n    _train_op = train_op(\n        dsl.InputArgumentPath(_preprocess_op.outputs['x_train']),\n        dsl.InputArgumentPath(_preprocess_op.outputs['y_train'])\n    ).after(_preprocess_op)\n\n    _test_op = test_op(\n        dsl.InputArgumentPath(_preprocess_op.outputs['x_test']),\n        dsl.InputArgumentPath(_preprocess_op.outputs['y_test']),\n        dsl.InputArgumentPath(_train_op.outputs['model'])\n    ).after(_train_op)\n\n    deploy_model_op(\n        dsl.InputArgumentPath(_train_op.outputs['model'])\n    ).after(_test_op)\n\n\ncompiler.Compiler().compile(pipeline, './pipeline.yaml')\n\nclient = kfp.Client()\nclient.upload_pipeline('./pipeline.yaml', \"pipeline\")"
  },
  {
    "repo": "anifort/kubeflow-pipelines-mlops",
    "file_path": "pipeline/src/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/anifort/kubeflow-pipelines-mlops/master/pipeline/src/pipeline.py",
    "content": "import kfp\nimport kfp.dsl as dsl\nimport kfp.gcp as gcp\nimport os\n\n\ndname = os.path.dirname( os.path.abspath(__file__))\n\n# Initialize component store\ncomponent_store = kfp.components.ComponentStore(local_search_paths=[dname+'/../../component'])\n\n# Create component factories\ntfidfvec_op = component_store.load_component('tfidf-vectoriser')\n\n# Define pipeline\n@dsl.pipeline(\n  name='A Simple CI pipeline',\n  description=''\n)\ndef preprocessing_pl(\n  data_path: str=\"gs://kubeflow_pipelines_sentiment/data/test.csv\",\n  vectorizer_gcs_location: str=\"gs://kubeflow_pipelines_sentiment/assets/x.pkl\"\n):\n  tfidfvec_step = tfidfvec_op(data_path=data_path, vectorizer_gcs_location=vectorizer_gcs_location).apply(gcp.use_gcp_secret('user-gcp-sa')) # We do not define output local file, that is auto-generated.\n  tfidfvec_step.set_display_name('vectorizing')\n  tfidfvec_step.outputs['local_output']\n\n\n  '''with kfp.dsl.Condition(sum_value != 0):\n    divide_step = divide_op(x_value=sum_value, y_value=z_value)\n    divide_step.set_display_name('Divide sum by z')\n    add_step2 = add_op(\n      x_value=divide_step.outputs['quotient'],\n      y_value=divide_step.outputs['remainder'])\n    add_step2.set_display_name('Add quotient and remainder')\n  '''\n\n\n'''\npipeline_func = preprocessing_pl\npipeline_filename = pipeline_func.__name__ + '.zip'\n\n\nimport kfp.compiler as compiler\ncompiler.Compiler().compile(pipeline_func, pipeline_filename)\n'''\n# dsl-compile --py pipeline.py --output pipeline_build.tar.gz\n"
  },
  {
    "repo": "AnsealArt/MLOps-Kubeflow",
    "file_path": "kubeflow-breast-cancer-pipeline/src/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/AnsealArt/MLOps-Kubeflow/master/kubeflow-breast-cancer-pipeline/src/pipeline.py",
    "content": "import kfp\nfrom kfp import dsl\nfrom kfp.components import func_to_container_op\n\n@func_to_container_op\ndef show_results(decision_tree : float, logistic_regression : float, random_forest : float) -> None:\n    \n    # Show metrics summary for all the models\n    print('=== Accuracy for chosen models ===')\n    print(f'Decision Tree: {decision_tree}')\n    print(f'Logistic Regression: {logistic_regression}')\n    print(f'Random Forest: {random_forest}')\n\n@dsl.pipeline(name='Breast Cancer Classification Pipeline', description='Applies ML models to classify breast cancer type')\ndef breast_cancer_classification_pipeline():\n\n    # Load YAML files for each component\n    download_data = kfp.components.load_component_from_file('download_data/download_data.yaml')\n    decision_tree = kfp.components.load_component_from_file('decision_tree/decision_tree.yaml')\n    logistic_regression = kfp.components.load_component_from_file('logistic_regression/logistic_regression.yaml')\n    random_forest = kfp.components.load_component_from_file('random_forest/random_forest.yaml')\n\n    # Set pipeline tasks and pass data to models\n    download_task = download_data()\n    decision_tree_task = decision_tree(download_task.output)\n    logistic_regression_task = logistic_regression(download_task.output)\n    random_forest_task = random_forest(download_task.output)\n\n    # Show results of ML models\n    show_results(\n        decision_tree_task.output,\n        logistic_regression_task.output,\n        random_forest_task.output\n    )\n\n# Compile the pipeline so it can be imported into Kubeflow\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(breast_cancer_classification_pipeline, 'BreastCancerClassificationPipeline.yaml')"
  },
  {
    "repo": "AnsealArt/MLOps-Kubeflow",
    "file_path": "kubeflow-california-housing-pipeline/src/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/AnsealArt/MLOps-Kubeflow/master/kubeflow-california-housing-pipeline/src/pipeline.py",
    "content": "import kfp\nfrom kfp import dsl\n\nimport argparse\n\nfrom datetime import datetime\n\n# Define components: preprocess, train, test, deploy\n\ndef preprocess_op(output_path, run_date, test_size):\n\n    return dsl.ContainerOp(\n        name = \"Download and preprocess data\",\n        image = \"dzieciolfilipit/kf_ch_preprocess_data:1.1\",\n        arguments = [\n            '--output_path', output_path,\n            '--run_date', run_date,\n            '--test_size', test_size\n        ],\n        file_outputs = {\n              'x_train': '{}{}/x_train.npy'.format(output_path, run_date)\n            , 'x_test' : '{}{}/x_test.npy'.format(output_path, run_date)\n            , 'y_train': '{}{}/y_train.npy'.format(output_path, run_date)\n            , 'y_test' : '{}{}/y_test.npy'.format(output_path, run_date)\n        }\n    )\n\ndef train_op(x_train, y_train, output_path, run_date):\n\n    return dsl.ContainerOp(\n        name = \"Train SGD Regressor\",\n        image = \"dzieciolfilipit/kf_ch_train_model:1.1\",\n        arguments = [\n            '--x_train', x_train,\n            '--y_train', y_train,\n            '--output_path', output_path,\n            '--run_date', run_date\n        ], \n        file_outputs = {\n            'model_path': '{}{}/model.pkl'.format(output_path, run_date)\n        }\n    )\n\ndef test_op(x_test, y_test, model_path, output_path, run_date):\n\n    return dsl.ContainerOp(\n        name = \"Test model and get MSE metric\",\n        image = \"dzieciolfilipit/kf_ch_test_model:1.1\",\n        arguments = [\n            '--x_test', x_test,\n            '--y_test', y_test,\n            '--model_path', model_path,\n            '--output_path', output_path,\n            '--run_date', run_date\n        ],\n        file_outputs = {\n            'mean_squared_error': '{}{}/output.txt'.format(output_path, run_date)\n        }\n    )\n\ndef deploy_model_op(model_path, mse_path):\n\n    return dsl.ContainerOp(\n        name = \"Deploy model for inference\",\n        image = \"dzieciolfilipit/kf_ch_deploy_model:1.1\",\n        arguments = [\n            '--model_path', model_path,\n            '--mse_path', mse_path\n        ]\n    )\n\n\n# Describe Pipeline\n@dsl.pipeline(\n    name = \"California Housing prediction Pipeline\",\n    description = \"Sample Pipeline for California Housing predictions using SGDRegressor regression model\"\n)\n# Define Pipeline steps\ndef pipeline(test_size, output_path, deployment_threshhold_mse):\n\n    # Use current date without separators\n    # Sample date: 20220529160603\n    run_date = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n\n    _preprocess_op = preprocess_op(\n        dsl.InputArgumentPath(output_path),\n        dsl.InputArgumentPath(run_date),\n        dsl.InputArgumentPath(test_size)\n    )\n\n    _train_op = train_op(\n        dsl.InputArgumentPath(_preprocess_op.outputs['x_train']),\n        dsl.InputArgumentPath(_preprocess_op.outputs['y_train']),\n        dsl.InputArgumentPath(output_path),\n        dsl.InputArgumentPath(run_date)\n    ).after(_preprocess_op)\n\n    _test_op = test_op(\n        dsl.InputArgumentPath(_preprocess_op.outputs['x_test']),\n        dsl.InputArgumentPath(_preprocess_op.outputs['y_test']),\n        dsl.InputArgumentPath(_train_op.outputs['model_path']),\n        dsl.InputArgumentPath(output_path),\n        dsl.InputArgumentPath(run_date)\n    ).after(_test_op)\n\n    # Define Production deployment condition, retrain otherwise\n    with dsl.Condition(_test_op.outputs['mean_squared_error'] < deployment_threshhold_mse):\n\n        deploy_model_op(\n            dsl.InputArgumentPath(_train_op.outputs['model_path']),\n            dsl.InputArgumentPath(_test_op.outputs['mean_squared_error'])\n        ).after(_test_op)\n\n    with dsl.Condition(_test_op.outputs['mean_squared_error'] >= deployment_threshhold_mse):\n\n        client = kfp.Client(host='http://ml-pipeline-ui:80')\n        client.create_run_from_pipeline_func(\n            pipeline, \n            arguments = {\n                'test_size': args.test_size,\n                'output_path': args.output_path,\n                'deployment_threshhold_mse': args.deployment_threshhold_mse\n        }\n    )\n\n\n\n# Add arguments to main run\nparser = argparse.ArgumentParser()\n\nparser.add_argument('--test_size', type=float)\nparser.add_argument('--output_path')\nparser.add_argument('--deployment_threshhold_mse', type=float)\n\nargs = parser.parse_args()\n\n# Create Kubeflow Client object and run Pipeline Function\nclient = kfp.Client(host='http://ml-pipeline-ui:80')\nclient.create_run_from_pipeline_func(\n    pipeline, \n    arguments = {\n        'test_size': args.test_size,\n        'output_path': args.output_path,\n        'deployment_threshhold_mse': args.deployment_threshhold_mse\n    }\n)"
  },
  {
    "repo": "s102401002/kubeflowPipeline",
    "file_path": "kubeflowPipeline0720.py",
    "raw_url": "https://raw.githubusercontent.com/s102401002/kubeflowPipeline/main/kubeflowPipeline0720.py",
    "content": "import kfp\n\nfrom kfp import dsl\nimport kfp.compiler\nfrom kfp.dsl import OutputPath, InputPath\n# from kfp.components import func_to_container_op\n\n@dsl.component(\n    base_image='python:3.9',\n    packages_to_install=['pandas==2.2.2'])\ndef load_data(load_data_output: OutputPath(str)):\n    import pandas as pd\n    \n    url = \"https://raw.githubusercontent.com/daniel88516/diabetes-data/main/10k.csv\"\n    df_data = pd.read_csv(url)\n    \n    df_data = df_data.drop(df_data[df_data['diabetes'] == 'No Info'].index)\n    df_data = df_data[['gender','age', 'bmi', 'HbA1c_level', 'blood_glucose_level', 'diabetes']]\n    df_data = df_data.dropna(thresh=4)\n    \n    gender_map = {'Male': 0 , 'Female': 1  , 'Other': 2}\n    df_data['gender'] = df_data['gender'].map(gender_map)\n    df_data = df_data[df_data['gender'] != 2]\n    df_data['age'] = df_data['age'].replace('No Info', df_data['age'].mean())\n    df_data['bmi'] = df_data['bmi'].replace('No Info', df_data['bmi'].mean())\n    df_data['HbA1c_level'] = df_data['HbA1c_level'].replace('No Info', df_data['HbA1c_level'].mean())\n    df_data['blood_glucose_level'] = df_data['blood_glucose_level'].replace('No Info', df_data['blood_glucose_level'].mean())\n\n    df_data.to_csv(load_data_output, index=False)\n\n@dsl.component(\n    base_image='python:3.9',\n    packages_to_install=['pandas==2.2.2', 'scikit-learn==1.5.1']\n)\ndef prepare_data(\n    data: InputPath(str), \n    x_train_output: OutputPath(str), x_test_output: OutputPath(str),\n    y_train_output: OutputPath(str), y_test_output: OutputPath(str)\n):\n    import pandas as pd\n    from sklearn.model_selection import train_test_split\n    df_data = pd.read_csv(data)\n    x = df_data.drop(labels=['diabetes'], axis=1)\n    y = df_data[['diabetes']]\n    \n    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n    \n    x_train_df = pd.DataFrame(x_train)\n    x_test_df = pd.DataFrame(x_test)\n    y_train_df = pd.DataFrame(y_train)\n    y_test_df = pd.DataFrame(y_test)\n\n    x_train_df.to_csv(x_train_output, index=False)\n    x_test_df.to_csv(x_test_output, index=False)\n    y_train_df.to_csv(y_train_output, index=False)\n    y_test_df.to_csv(y_test_output, index=False)\n\n@dsl.component(\n    base_image='python:3.9',\n    packages_to_install=['pandas==2.2.2', 'scikit-learn==1.5.1', 'joblib==1.4.2']\n)\ndef train_model(x_train: InputPath(str), y_train: InputPath(str), train_model_output: OutputPath(str)):\n    import pandas as pd\n    from sklearn.linear_model import LogisticRegression\n    import joblib\n    \n    x_train = pd.read_csv(x_train)\n    y_train = pd.read_csv(y_train)\n    \n    model = LogisticRegression(random_state=0, max_iter=100) # 100 times for test p.s. it is 10000 times in beginning\n    model.fit(x_train, y_train)\n    \n    #model_path = './diabete_prediction_model.pkl'\n    joblib.dump(model, train_model_output)\n\n@dsl.component(\n    base_image='python:3.9',\n    packages_to_install=['pandas==2.2.2', 'joblib==1.4.2']\n)\ndef evaluate_model(model_path: InputPath(str), x_test: InputPath(str), y_test: InputPath(str)) -> str:\n    import pandas as pd\n    import joblib\n\n    model = joblib.load(filename=model_path)\n\n    x_test_df = pd.read_csv(x_test)\n    y_test_df = pd.read_csv(y_test)\n    \n    accuracy = model.score(x_test_df, y_test_df)\n    \n    return f'Test accuracy: {accuracy}'\n\n@dsl.pipeline(\n    name='Diabetes Prediction Pipeline',\n    description='Using kubeflow pipeline to train and evaluate a diabetes prediction model'\n)\ndef diabetes_prediction_pipeline() -> str:\n    load_data_task = load_data()\n\n    prepare_data_task = prepare_data(data=load_data_task.outputs['load_data_output'])\n    \n    train_model_task = train_model(\n        x_train = prepare_data_task.outputs['x_train_output'], \n        y_train = prepare_data_task.outputs['y_train_output']\n    )\n    \n    evaluate_model_task = evaluate_model(\n        model_path = train_model_task.outputs['train_model_output'], \n        x_test = prepare_data_task.outputs['x_test_output'], \n        y_test = prepare_data_task.outputs['y_test_output']\n    )\n    \n    return evaluate_model_task.output\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(diabetes_prediction_pipeline, 'diabetes_prediction_pipeline.yaml')"
  },
  {
    "repo": "s102401002/kubeflowPipeline",
    "file_path": "kubeflowPipeline0722.py",
    "raw_url": "https://raw.githubusercontent.com/s102401002/kubeflowPipeline/main/kubeflowPipeline0722.py",
    "content": "from typing import NewType\r\n\r\nimport kfp\r\nimport kfp.compiler\r\n\r\nfrom kfp import dsl\r\nfrom kfp.dsl import OutputPath, InputPath, Input, Output, Artifact\r\n\r\nfrom pandas import DataFrame\r\n# from kfp.components import func_to_container_op\r\n\r\nDF = NewType('DF', DataFrame)\r\n\r\n@dsl.component(\r\n    base_image='python:3.9',\r\n    packages_to_install=['pandas==2.2.2']\r\n)\r\ndef load_data(data_output: Output[Artifact]):\r\n    import pandas as pd\r\n    \r\n    urls = [\r\n        \"https://raw.githubusercontent.com/daniel88516/diabetes-data/main/10k.csv\",\r\n        \"https://raw.githubusercontent.com/s102401002/kubeflowPipeline/main/data1.csv\"\r\n    ]\r\n    \r\n    standard_name_mapping = {\r\n        'gender': ['gender', 'gen', 'Gender', 'sex', 'Sex'],\r\n        'age': ['age', 'Age', 'AGE'],\r\n        'bmi': ['bmi', 'BMI', 'Bmi'],\r\n        'HbA1c_level': ['HbA1c_level', 'HbA1c', 'hba1c'],\r\n        'blood_glucose_level': ['blood_glucose_level', 'glucose', 'BloodGlucose'],\r\n        'diabetes': ['diabetes', 'Diabetes']\r\n    }\r\n\r\n    datas = [] # download all the csv in urls as a array\r\n    for url in urls:\r\n        df = pd.read_csv(url)\r\n        for standard_name, variants in standard_name_mapping.items():\r\n            for variant in variants:\r\n                if variant in df.columns:\r\n                    df.rename(columns={variant: standard_name}, inplace=True) # inplace=True: changing directly instead of creating a new column\r\n                    break\r\n        \r\n        datas.append(df)\r\n\r\n    df_data = pd.concat(datas, ignore_index=True)\r\n    \r\n    df_data = df_data.drop(df_data[df_data['diabetes'] == 'No Info'].index)\r\n    df_data = df_data[['gender','age', 'bmi', 'HbA1c_level', 'blood_glucose_level', 'diabetes']]\r\n    df_data = df_data.dropna(thresh=4)\r\n    \r\n    gender_map = {'Male': 0 , 'Female': 1  , 'Other': 2}\r\n    df_data['gender'] = df_data['gender'].map(gender_map)\r\n    df_data = df_data[df_data['gender'] != 2]\r\n    df_data['age'] = df_data['age'].replace('No Info', df_data['age'].mean())\r\n    df_data['bmi'] = df_data['bmi'].replace('No Info', df_data['bmi'].mean())\r\n    df_data['HbA1c_level'] = df_data['HbA1c_level'].replace('No Info', df_data['HbA1c_level'].mean())\r\n    df_data['blood_glucose_level'] = df_data['blood_glucose_level'].replace('No Info', df_data['blood_glucose_level'].mean())\r\n\r\n    df_data.to_csv(data_output.path)\r\n\r\n@dsl.component(\r\n    base_image='python:3.9',\r\n    packages_to_install=['pandas==2.2.2', 'scikit-learn==1.5.1']\r\n)\r\ndef prepare_data(\r\n    data_input: Input[Artifact], \r\n    x_train_output: Output[Artifact], x_test_output: Output[Artifact],\r\n    y_train_output: Output[Artifact], y_test_output: Output[Artifact]\r\n):\r\n    import pandas as pd\r\n    from sklearn.model_selection import train_test_split\r\n\r\n    df_data = pd.read_csv(data_input.path)\r\n\r\n    x = df_data.drop(labels=['diabetes'], axis=1)\r\n    y = df_data[['diabetes']]\r\n    \r\n    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\r\n    \r\n    x_train_df = pd.DataFrame(x_train)\r\n    x_test_df = pd.DataFrame(x_test)\r\n    y_train_df = pd.DataFrame(y_train)\r\n    y_test_df = pd.DataFrame(y_test)\r\n\r\n    x_train_df.to_csv(x_train_output.path, index=False)\r\n    x_test_df.to_csv(x_test_output.path, index=False)\r\n    y_train_df.to_csv(y_train_output.path, index=False)\r\n    y_test_df.to_csv(y_test_output.path, index=False)\r\n\r\n@dsl.component(\r\n    base_image='python:3.9',\r\n    packages_to_install=['pandas==2.2.2', 'scikit-learn==1.5.1', 'joblib==1.4.2']\r\n)\r\ndef train_model(x_train: Input[Artifact], y_train: Input[Artifact], train_model_output: Output[Artifact]):\r\n    import pandas as pd\r\n    from sklearn.linear_model import LogisticRegression\r\n    import joblib\r\n    \r\n    x_train = pd.read_csv(x_train.path)\r\n    y_train = pd.read_csv(y_train.path)\r\n    \r\n    model = LogisticRegression(random_state=0, max_iter=10000) # 100 times for test p.s. it is 10000 times in beginning\r\n    model.fit(x_train, y_train)\r\n    \r\n    #model_path = './diabete_prediction_model.pkl'\r\n    joblib.dump(model, train_model_output.path)\r\n\r\n@dsl.component(\r\n    base_image='python:3.9',\r\n    packages_to_install=['pandas==2.2.2', 'scikit-learn==1.5.1', 'joblib==1.4.2']\r\n)\r\ndef evaluate_model(model_path: Input[Artifact], x_test: Input[Artifact], y_test: Input[Artifact]) -> str:\r\n    import pandas as pd\r\n    import sklearn\r\n    import joblib\r\n\r\n    model = joblib.load(filename=model_path.path)\r\n\r\n    x_test_df = pd.read_csv(x_test.path)\r\n    y_test_df = pd.read_csv(y_test.path)\r\n    \r\n    accuracy = model.score(x_test_df, y_test_df)\r\n    \r\n    return f'Test accuracy: {accuracy}'\r\n\r\n@dsl.pipeline(\r\n    name='Diabetes Prediction Pipeline',\r\n    description='Using kubeflow pipeline to train and evaluate a diabetes prediction model'\r\n)\r\ndef diabetes_prediction_pipeline() -> str:\r\n    load_data_task = load_data()\r\n\r\n    prepare_data_task = prepare_data(data_input=load_data_task.outputs['data_output'])\r\n    \r\n    train_model_task = train_model(\r\n        x_train = prepare_data_task.outputs['x_train_output'], \r\n        y_train = prepare_data_task.outputs['y_train_output']\r\n    )\r\n    \r\n    evaluate_model_task = evaluate_model(\r\n        model_path = train_model_task.outputs['train_model_output'], \r\n        x_test = prepare_data_task.outputs['x_test_output'], \r\n        y_test = prepare_data_task.outputs['y_test_output']\r\n    )\r\n    \r\n    return evaluate_model_task.output\r\n\r\nif __name__ == '__main__':\r\n    kfp.compiler.Compiler().compile(diabetes_prediction_pipeline, 'diabetes_prediction_pipeline.yaml')"
  },
  {
    "repo": "s102401002/kubeflowPipeline",
    "file_path": "kubeflowPipeline_parquet.py",
    "raw_url": "https://raw.githubusercontent.com/s102401002/kubeflowPipeline/main/kubeflowPipeline_parquet.py",
    "content": "import kfp\r\n\r\nfrom kfp import dsl\r\nimport kfp.compiler\r\nfrom kfp.dsl import OutputPath, InputPath\r\n# from kfp.components import func_to_container_op\r\n\r\n@dsl.component(\r\n    base_image='python:3.9',\r\n    packages_to_install=['pandas==2.2.2', 'numpy==2.0.0', 'requests==2.32.3', 'pyarrow==15.0.0']\r\n)\r\ndef load_data(load_data_output: OutputPath()):\r\n    import pandas as pd\r\n    import numpy as np\r\n    import io\r\n    import requests\r\n    import pyarrow as pa\r\n    import pyarrow.parquet as pq\r\n    \r\n    url = \"https://raw.githubusercontent.com/daniel88516/diabetes-data/main/10k.csv\"\r\n    s = requests.get(url).content\r\n    df_data = pd.read_csv(io.StringIO(s.decode('utf-8')))\r\n    df_data.drop(df_data[df_data['diabetes'] == 'No Info'].index, inplace=True)\r\n    df_data = df_data[['gender','age', 'bmi', 'HbA1c_level', 'blood_glucose_level', 'diabetes']]\r\n    df_data.dropna(thresh=4, inplace=True)\r\n    \r\n    gender_map = {'Male':0 , 'Female':1  , 'Other':2}\r\n    df_data['gender'] = df_data['gender'].map(gender_map)\r\n    df_data = df_data[df_data['gender'] != 2]\r\n    df_data['age'] = pd.to_numeric(df_data['age'].replace('No Info', np.nan), errors='coerce')\r\n    df_data['bmi'] = pd.to_numeric(df_data['bmi'].replace('No Info', np.nan), errors='coerce')\r\n    df_data['HbA1c_level'] = pd.to_numeric(df_data['HbA1c_level'].replace('No Info', np.nan), errors='coerce')\r\n    df_data['blood_glucose_level'] = pd.to_numeric(df_data['blood_glucose_level'].replace('No Info', np.nan), errors='coerce')\r\n    \r\n    df_data = df_data.fillna(df_data.mean())\r\n    \r\n    # \u8f49\u63db\u70ba Parquet \u683c\u5f0f\r\n    table = pa.Table.from_pandas(df_data)\r\n    pq.write_table(table, load_data_output)\r\n\r\n@dsl.component(\r\n    base_image='python:3.9',\r\n    packages_to_install=['pandas==2.2.2', 'scikit-learn==1.5.1', 'pyarrow==15.0.0']\r\n)\r\ndef prepare_data(data: InputPath(), x_train_output: OutputPath(), x_test_output: OutputPath(), y_train_output: OutputPath(), y_test_output: OutputPath()):\r\n    import pandas as pd\r\n    import pyarrow.parquet as pq\r\n    from sklearn.model_selection import train_test_split\r\n    \r\n    df_data = pq.read_table(data).to_pandas()\r\n    x = df_data.drop(labels=['diabetes'], axis=1)\r\n    y = df_data[['diabetes']]\r\n    \r\n    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\r\n    \r\n    pq.write_table(pa.Table.from_pandas(x_train), x_train_output)\r\n    pq.write_table(pa.Table.from_pandas(x_test), x_test_output)\r\n    pq.write_table(pa.Table.from_pandas(y_train), y_train_output)\r\n    pq.write_table(pa.Table.from_pandas(y_test), y_test_output)\r\n\r\n@dsl.component(\r\n    base_image='python:3.9',\r\n    packages_to_install=['pandas==2.2.2', 'scikit-learn==1.5.1', 'joblib==1.4.2', 'pyarrow==15.0.0']\r\n)\r\ndef train_model(x_train: InputPath(), y_train: InputPath(), train_model_output: OutputPath()):\r\n    import pandas as pd\r\n    import pyarrow.parquet as pq\r\n    from sklearn.linear_model import LogisticRegression\r\n    import joblib\r\n    \r\n    x_train = pq.read_table(x_train).to_pandas()\r\n    y_train = pq.read_table(y_train).to_pandas()\r\n    \r\n    model = LogisticRegression(random_state=0, max_iter=100) # 100 times for test p.s. it is 10000 times in beginning\r\n    model.fit(x_train, y_train.values.ravel())\r\n    \r\n    joblib.dump(model, train_model_output)\r\n\r\n@dsl.component(\r\n    base_image='python:3.9',\r\n    packages_to_install=['pandas==2.2.2', 'joblib==1.4.2', 'pyarrow==15.0.0']\r\n)\r\ndef evaluate_model(model_path: InputPath(), x_test: InputPath(), y_test: InputPath()) -> str:\r\n    import pandas as pd\r\n    import pyarrow.parquet as pq\r\n    import joblib\r\n    \r\n    model = joblib.load(filename=model_path)\r\n    x_test_df = pq.read_table(x_test).to_pandas()\r\n    y_test_df = pq.read_table(y_test).to_pandas()\r\n    \r\n    accuracy = model.score(x_test_df, y_test_df)\r\n    \r\n    return f'Test accuracy: {accuracy}'\r\n\r\n@dsl.pipeline(\r\n    name='Diabetes Prediction Pipeline',\r\n    description='Using kubeflow pipeline to train and evaluate a diabetes prediction model'\r\n)\r\ndef diabetes_prediction_pipeline() -> str:\r\n    load_data_task = load_data()\r\n    prepare_data_task = prepare_data(data=load_data_task.outputs['load_data_output'])\r\n    \r\n    train_model_task = train_model(\r\n        x_train = prepare_data_task.outputs['x_train_output'], \r\n        y_train = prepare_data_task.outputs['y_train_output']\r\n    )\r\n    \r\n    evaluate_model_task = evaluate_model(\r\n        model_path = train_model_task.outputs['train_model_output'], \r\n        x_test = prepare_data_task.outputs['x_test_output'], \r\n        y_test = prepare_data_task.outputs['y_test_output']\r\n    )\r\n    \r\n    return evaluate_model_task.output\r\n\r\nif __name__ == '__main__':\r\n    kfp.compiler.Compiler().compile(diabetes_prediction_pipeline, 'diabetes_prediction_pipeline_parquet.yaml')"
  },
  {
    "repo": "s102401002/kubeflowPipeline",
    "file_path": "kubeflowPipeline_xgboost.py",
    "raw_url": "https://raw.githubusercontent.com/s102401002/kubeflowPipeline/main/kubeflowPipeline_xgboost.py",
    "content": "from typing import NewType\r\n\r\nimport kfp\r\nimport kfp.compiler\r\n\r\nfrom kfp import dsl\r\nfrom kfp.dsl import OutputPath, InputPath, Input, Output, Artifact\r\n\r\nfrom pandas import DataFrame\r\n# from kfp.components import func_to_container_op\r\n\r\nDF = NewType('DF', DataFrame)\r\n\r\n@dsl.component(\r\n    base_image='python:3.9',\r\n    packages_to_install=['pandas==2.2.2']\r\n)\r\ndef load_data(data_output: Output[Artifact]):\r\n    import pandas as pd\r\n    \r\n    urls = [\r\n        \"https://raw.githubusercontent.com/daniel88516/diabetes-data/main/10k.csv\",\r\n        \"https://raw.githubusercontent.com/s102401002/kubeflowPipeline/main/data1.csv\"\r\n    ]\r\n    \r\n    standard_name_mapping = {\r\n        'gender': ['gender', 'gen', 'Gender', 'sex', 'Sex'],\r\n        'age': ['age', 'Age', 'AGE'],\r\n        'bmi': ['bmi', 'BMI', 'Bmi'],\r\n        'HbA1c_level': ['HbA1c_level', 'HbA1c', 'hba1c'],\r\n        'blood_glucose_level': ['blood_glucose_level', 'glucose', 'BloodGlucose'],\r\n        'diabetes': ['diabetes', 'Diabetes']\r\n    }\r\n\r\n    datas = [] # download all the csv in urls as a array\r\n    for url in urls:\r\n        df = pd.read_csv(url)\r\n        for standard_name, variants in standard_name_mapping.items():\r\n            for variant in variants:\r\n                if variant in df.columns:\r\n                    df.rename(columns={variant: standard_name}, inplace=True) # inplace=True: changing directly instead of creating a new column\r\n                    break\r\n        \r\n        datas.append(df)\r\n\r\n    df_data = pd.concat(datas, ignore_index=True)\r\n    \r\n    df_data = df_data.drop(df_data[df_data['diabetes'] == 'No Info'].index)\r\n    df_data = df_data[['gender','age', 'bmi', 'HbA1c_level', 'blood_glucose_level', 'diabetes']]\r\n    df_data = df_data.dropna(thresh=4)\r\n    \r\n    gender_map = {'Male': 0 , 'Female': 1  , 'Other': 2}\r\n    df_data['gender'] = df_data['gender'].map(gender_map)\r\n    df_data = df_data[df_data['gender'] != 2]\r\n    df_data['age'] = df_data['age'].replace('No Info', df_data['age'].mean())\r\n    df_data['bmi'] = df_data['bmi'].replace('No Info', df_data['bmi'].mean())\r\n    df_data['HbA1c_level'] = df_data['HbA1c_level'].replace('No Info', df_data['HbA1c_level'].mean())\r\n    df_data['blood_glucose_level'] = df_data['blood_glucose_level'].replace('No Info', df_data['blood_glucose_level'].mean())\r\n\r\n    df_data.to_csv(data_output.path)\r\n\r\n@dsl.component(\r\n    base_image='python:3.9',\r\n    packages_to_install=['pandas==2.2.2', 'scikit-learn==1.5.1']\r\n)\r\ndef prepare_data(\r\n    data_input: Input[Artifact], \r\n    x_train_output: Output[Artifact], x_test_output: Output[Artifact],\r\n    y_train_output: Output[Artifact], y_test_output: Output[Artifact]\r\n):\r\n    import pandas as pd\r\n    from sklearn.model_selection import train_test_split\r\n\r\n    df_data = pd.read_csv(data_input.path)\r\n\r\n    x = df_data.drop(labels=['diabetes'], axis=1)\r\n    y = df_data[['diabetes']]\r\n    \r\n    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\r\n    \r\n    x_train_df = pd.DataFrame(x_train)\r\n    x_test_df = pd.DataFrame(x_test)\r\n    y_train_df = pd.DataFrame(y_train)\r\n    y_test_df = pd.DataFrame(y_test)\r\n\r\n    x_train_df.to_csv(x_train_output.path, index=False)\r\n    x_test_df.to_csv(x_test_output.path, index=False)\r\n    y_train_df.to_csv(y_train_output.path, index=False)\r\n    y_test_df.to_csv(y_test_output.path, index=False)\r\n\r\n@dsl.component(\r\n    base_image='python:3.9',\r\n    packages_to_install=['pandas==2.2.2', 'scikit-learn==1.5.1', 'joblib==1.4.2', 'xgboost==2.0.3']\r\n)\r\ndef train_model(x_train: Input[Artifact], y_train: Input[Artifact], train_model_output: Output[Artifact]):\r\n    import pandas as pd\r\n    from xgboost import XGBClassifier\r\n    import joblib\r\n    \r\n    x_train = pd.read_csv(x_train.path)\r\n    y_train = pd.read_csv(y_train.path)\r\n    \r\n    model = XGBClassifier(n_estimators=1000, learning_rate= 0.01)\r\n    model.fit(x_train, y_train.values.ravel())\r\n    \r\n    joblib.dump(model, train_model_output.path)\r\n\r\n@dsl.component(\r\n    base_image='python:3.9',\r\n    packages_to_install=['pandas==2.2.2', 'scikit-learn==1.5.1', 'joblib==1.4.2', 'xgboost==2.0.3']\r\n)\r\ndef evaluate_model(model_path: Input[Artifact], x_test: Input[Artifact], y_test: Input[Artifact]) -> str:\r\n    import pandas as pd\r\n    from sklearn.metrics import accuracy_score\r\n    import joblib\r\n\r\n    model = joblib.load(filename=model_path.path)\r\n\r\n    x_test_df = pd.read_csv(x_test.path)\r\n    y_test_df = pd.read_csv(y_test.path)\r\n    \r\n    y_pred = model.predict(x_test_df)\r\n    accuracy = accuracy_score(y_test_df, y_pred)\r\n    \r\n    return f'Test accuracy: {accuracy}'\r\n\r\n@dsl.pipeline(\r\n    name='Diabetes Prediction Pipeline',\r\n    description='Using kubeflow pipeline to train and evaluate a diabetes prediction model'\r\n)\r\ndef diabetes_prediction_pipeline() -> str:\r\n    load_data_task = load_data()\r\n\r\n    prepare_data_task = prepare_data(data_input=load_data_task.outputs['data_output'])\r\n    \r\n    train_model_task = train_model(\r\n        x_train = prepare_data_task.outputs['x_train_output'], \r\n        y_train = prepare_data_task.outputs['y_train_output']\r\n    )\r\n    \r\n    evaluate_model_task = evaluate_model(\r\n        model_path = train_model_task.outputs['train_model_output'], \r\n        x_test = prepare_data_task.outputs['x_test_output'], \r\n        y_test = prepare_data_task.outputs['y_test_output']\r\n    )\r\n    \r\n    return evaluate_model_task.output\r\n\r\nif __name__ == '__main__':\r\n    kfp.compiler.Compiler().compile(diabetes_prediction_pipeline, 'diabetes_prediction_pipeline_xgboost.yaml')"
  },
  {
    "repo": "stackdemos/yolo4",
    "file_path": "components/golang/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/yolo4/master/components/golang/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef processing_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('GOLANG_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=[script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n"
  },
  {
    "repo": "stackdemos/yolo4",
    "file_path": "components/training/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/yolo4/master/components/training/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef training_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('TRAINING_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=['/usr/local/bin/python', script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n\n"
  },
  {
    "repo": "aiffel-socar-cv/kubeflow-pipeline",
    "file_path": "pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/aiffel-socar-cv/kubeflow-pipeline/master/pipeline.py",
    "content": "import kfp\nimport kfp.components as comp\nfrom kfp import dsl\n\n\n@dsl.pipeline(name=\"viai-retrain\", description=\"viai retrain pipeline\")\ndef retrain_pipeline():\n    check_files = dsl.ContainerOp(\n        name=\"Check files\",\n        image=\"tseo/check_bucket:0.3\",\n        file_outputs={\"file_num\": \"/file_nums.json\"},\n    )\n\n    mv_files = dsl.ContainerOp(\n        name=\"Move files\",\n        image=\"tseo/mv_files:0.6\",\n        arguments=[\"--json_file\", check_files.outputs[\"file_num\"]],\n    )\n\n    mv_files.after(check_files)\n\n\nif __name__ == \"__main__\":\n    import kfp.compiler as compiler\n\n    compiler.Compiler().compile(retrain_pipeline, __file__ + \".tar.gz\")\n"
  },
  {
    "repo": "aiffel-socar-cv/kubeflow-pipeline",
    "file_path": "baseline/retrain_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/aiffel-socar-cv/kubeflow-pipeline/master/baseline/retrain_pipeline.py",
    "content": "from typing import NamedTuple\nimport kfp\nimport kfp.dsl as dsl\nimport kfp.gcp as gcp\nfrom kfp import components as comp\nfrom typing import NamedTuple\n\n\ndef check_cnt(\n    bucket_name: str, data_type: str\n) -> NamedTuple(\"output\", [(\"count\", int), (\"type\", str)]):\n    \"\"\"\n        count the number of newly added images\n    \"\"\"\n    from google.cloud import storage\n\n    client = storage.Client()\n    blob = client.list_blobs(\"images-original\", prefix=\"originals\")\n    cnt = -1  # root directory \uc81c\uc678\n    for b in blob:\n        cnt += 1\n\n\ndef retrain_op():\n    return dsl.ContainerOp(\n        name=\"retrain\",\n        image=\"us-west1-docker.pkg.dev/viai/retrain:v1.0\",\n        arguments=[\n            \"/opt/retrain.py\",\n            \"--data-dir\",\n            data_dir,\n            \"--torch-export-dir\",\n            model_export_dir,\n        ],\n    )\n\n\n@dsl.pipeline(name=\"retrain\", description=\"retrain demo\")\ndef retrain_pipeline(\n    name=\"retrain\",\n    training_image=\"gcr.io/aiffel-gn-3/retrain:latest\",\n    training_namespace=\"kubeflow\",\n    model_export_dir=\"gs://model-cpt/\",\n):\n    comp.func_to_container_op(check_cnt)\n    check_cnt = check_op()\n    with dsl.Condition(check_cnt.output > 100):\n        retrain = retrain_op()\n\n\nsteps = [retrain]\nfor step in steps:\n    if platform == \"GCP\":\n        step.apply(gcp.use_gcp_secret(\"user-gcp-sa\"))\nif __name__ == \"__main__\":\n    import kfp.compiler as compiler\n\n    compiler.Compiler().compile(retrain_pipeline, __file__ + \".tar.gz\")\n"
  },
  {
    "repo": "lauramorillo/kubeflow-example",
    "file_path": "taxi-on-prem.py",
    "raw_url": "https://raw.githubusercontent.com/lauramorillo/kubeflow-example/master/taxi-on-prem.py",
    "content": "#!/usr/bin/env python3\n# Copyright 2018 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp\nfrom kfp import dsl\nfrom kfp import onprem\n\ndef dataflow_tf_data_validation_op(inference_data, validation_data,\n                                   column_names, key_columns, project, mode,\n                                   validation_output, volume,\n                                   step_name='validation'):\n    return dsl.ContainerOp(\n        name=step_name,\n        image='gcr.io/ml-pipeline/ml-pipeline-dataflow-tfdv:6ad2601ec7d04e842c212c50d5c78e548e12ddea',\n        arguments=[\n            '--csv-data-for-inference', inference_data,\n            '--csv-data-to-validate', validation_data,\n            '--column-names', column_names,\n            '--key-columns', key_columns,\n            '--project', project,\n            '--mode', mode,\n            '--output', '%s/{{workflow.name}}/validation' % validation_output,\n        ],\n        file_outputs={\n            'schema': '/schema.txt',\n            'validation': '/output_validation_result.txt',\n        },\n        pvolumes={validation_output: volume}\n    )\n\n\ndef dataflow_tf_transform_op(train_data, evaluation_data, schema,\n                             project, preprocess_mode, preprocess_module,\n                             transform_output, volume,\n                             step_name='preprocess'):\n    return dsl.ContainerOp(\n        name=step_name,\n        image='gcr.io/ml-pipeline/ml-pipeline-dataflow-tft:6ad2601ec7d04e842c212c50d5c78e548e12ddea',\n        arguments=[\n            '--train', train_data,\n            '--eval', evaluation_data,\n            '--schema', schema,\n            '--project', project,\n            '--mode', preprocess_mode,\n            '--preprocessing-module', preprocess_module,\n            '--output', '%s/{{workflow.name}}/transformed' % transform_output,\n        ],\n        file_outputs={'transformed': '/output.txt'},\n        pvolumes={transform_output: volume}\n    )\n\n\ndef tf_train_op(transformed_data_dir, schema, learning_rate: float,\n                hidden_layer_size: int, steps: int, target: str,\n                preprocess_module, training_output, volume,\n                step_name='training'):\n    return dsl.ContainerOp(\n        name=step_name,\n        image='gcr.io/ml-pipeline/ml-pipeline-kubeflow-tf-trainer:5df2cdc1ed145320204e8bc73b59cdbd7b3da28f',\n        arguments=[\n            '--transformed-data-dir', transformed_data_dir,\n            '--schema', schema,\n            '--learning-rate', learning_rate,\n            '--hidden-layer-size', hidden_layer_size,\n            '--steps', steps,\n            '--target', target,\n            '--preprocessing-module', preprocess_module,\n            '--job-dir', '%s/{{workflow.name}}/train' % training_output,\n        ],\n        file_outputs={'train': '/output.txt'},\n        pvolumes={training_output: volume}\n    )\n\n\ndef dataflow_tf_model_analyze_op(model: 'TensorFlow model', evaluation_data,\n                                 schema, project, analyze_mode,\n                                 analyze_slice_column, analysis_output,\n                                 volume, step_name='analysis'):\n    return dsl.ContainerOp(\n        name=step_name,\n        image='gcr.io/ml-pipeline/ml-pipeline-dataflow-tfma:6ad2601ec7d04e842c212c50d5c78e548e12ddea',\n        arguments=[\n            '--model', model,\n            '--eval', evaluation_data,\n            '--schema', schema,\n            '--project', project,\n            '--mode', analyze_mode,\n            '--slice-columns', analyze_slice_column,\n            '--output', '%s/{{workflow.name}}/analysis' % analysis_output,\n        ],\n        file_outputs={'analysis': '/output.txt'},\n        pvolumes={analysis_output: volume}\n    )\n\n\ndef dataflow_tf_predict_op(evaluation_data, schema, target: str,\n                           model: 'TensorFlow model', predict_mode, project,\n                           prediction_output, volume,\n                           step_name='prediction'):\n    return dsl.ContainerOp(\n        name=step_name,\n        image='gcr.io/ml-pipeline/ml-pipeline-dataflow-tf-predict:6ad2601ec7d04e842c212c50d5c78e548e12ddea',\n        arguments=[\n            '--data', evaluation_data,\n            '--schema', schema,\n            '--target', target,\n            '--model', model,\n            '--mode', predict_mode,\n            '--project', project,\n            '--output', '%s/{{workflow.name}}/predict' % prediction_output,\n        ],\n        file_outputs={'prediction': '/output.txt'},\n        pvolumes={prediction_output: volume}\n    )\n\n\ndef confusion_matrix_op(predictions, output, volume,\n                        step_name='confusion_matrix'):\n    return dsl.ContainerOp(\n        name=step_name,\n        image='gcr.io/ml-pipeline/ml-pipeline-local-confusion-matrix:5df2cdc1ed145320204e8bc73b59cdbd7b3da28f',\n        arguments=[\n            '--output', '%s/{{workflow.name}}/confusionmatrix' % output,\n            '--predictions', predictions,\n            '--target_lambda', \"\"\"lambda x: (x['target'] > x['fare'] * 0.2)\"\"\",\n        ],\n        pvolumes={output: volume}\n    )\n\n\ndef roc_op(predictions, output, volume, step_name='roc'):\n    return dsl.ContainerOp(\n        name=step_name,\n        image='gcr.io/ml-pipeline/ml-pipeline-local-roc:5df2cdc1ed145320204e8bc73b59cdbd7b3da28f',\n        arguments=[\n            '--output', '%s/{{workflow.name}}/roc' % output,\n            '--predictions', predictions,\n            '--target_lambda', \"\"\"lambda x: 1 if (x['target'] > x['fare'] * 0.2) else 0\"\"\",\n        ],\n        pvolumes={output: volume}\n    )\n\n\ndef kubeflow_deploy_op(model: 'TensorFlow model', tf_server_name, pvc_name,\n                       pvolumes, step_name='deploy'):\n    return dsl.ContainerOp(\n        name=step_name,\n        image='gcr.io/ml-pipeline/ml-pipeline-kubeflow-deployer:727c48c690c081b505c1f0979d11930bf1ef07c0',\n        arguments=[\n            '--cluster-name', 'tfx-taxi-pipeline-on-prem',\n            '--model-export-path', model,\n            '--server-name', tf_server_name,\n            '--pvc-name', pvc_name,\n        ],\n        pvolumes=pvolumes\n    )\n\n\n@dsl.pipeline(\n    name='Taxi Cab on-prem',\n    description='Example pipeline that does classification with model analysis based on a public BigQuery dataset for on-prem cluster.'\n)\ndef taxi_cab_classification(\n        pvc_size='1Gi',\n        project='tfx-taxi-pipeline-on-prem',\n        column_names='pipelines/samples/tfx/taxi-cab-classification/column-names.json',\n        key_columns='trip_start_timestamp',\n        train='pipelines/samples/tfx/taxi-cab-classification/train.csv',\n        evaluation='pipelines/samples/tfx/taxi-cab-classification/eval.csv',\n        mode='local',\n        preprocess_module='pipelines/samples/tfx/taxi-cab-classification/preprocessing.py',\n        learning_rate=0.1,\n        hidden_layer_size=1500,\n        steps=3000,\n        analyze_slice_column='trip_start_hour'):\n\n    tf_server_name = 'taxi-cab-classification-model-{{workflow.name}}'\n\n    vop = dsl.VolumeOp(\n        name='create-volume',\n        resource_name='taxi-cab-data',\n        modes=dsl.VOLUME_MODE_RWM,\n        size=pvc_size\n    )\n\n    checkout = dsl.ContainerOp(\n        name=\"checkout\",\n        image=\"alpine/git:latest\",\n        command=[\"git\", \"clone\", \"https://github.com/kubeflow/pipelines.git\", \"/mnt/pipelines\"],\n    ).apply(onprem.mount_pvc(vop.outputs[\"name\"], 'local-storage', \"/mnt\"))\n    checkout.after(vop)\n\n    validation = dataflow_tf_data_validation_op(\n        '/mnt/%s' % train,\n        '/mnt/%s' % evaluation,\n        '/mnt/%s' % column_names,\n        key_columns,\n        project,\n        mode,\n        '/mnt',\n        vop.volume\n    )\n    validation.after(checkout)\n\n    preprocess = dataflow_tf_transform_op(\n        '/mnt/%s' % train,\n        '/mnt/%s' % evaluation,\n        validation.outputs['schema'],\n        project, mode,\n        '/mnt/%s' % preprocess_module,\n        '/mnt',\n        vop.volume\n    )\n\n    training = tf_train_op(\n        preprocess.output,\n        validation.outputs['schema'],\n        learning_rate,\n        hidden_layer_size,\n        steps,\n        'tips',\n        '/mnt/%s' % preprocess_module,\n        '/mnt',\n        vop.volume\n    )\n\n    analysis = dataflow_tf_model_analyze_op(\n        training.output,\n        '/mnt/%s' % evaluation,\n        validation.outputs['schema'],\n        project,\n        mode,\n        analyze_slice_column,\n        '/mnt',\n        vop.volume\n    )\n\n    prediction = dataflow_tf_predict_op(\n        '/mnt/%s' % evaluation,\n        validation.outputs['schema'],\n        'tips',\n        training.output,\n        mode,\n        project,\n        '/mnt',\n        vop.volume\n    )\n\n    cm = confusion_matrix_op(\n        prediction.output,\n        '/mnt',\n        vop.volume\n    )\n\n    roc = roc_op(\n        prediction.output,\n        '/mnt',\n        vop.volume\n    )\n\n    deploy = kubeflow_deploy_op(\n        training.output,\n        tf_server_name,\n        vop.output,\n        {'/mnt': vop.volume}\n    )\n\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(taxi_cab_classification, __file__ + '.tar.gz')"
  },
  {
    "repo": "pragadeeshraju/kubeflow-sample-pipeline",
    "file_path": "pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/pragadeeshraju/kubeflow-sample-pipeline/main/pipeline.py",
    "content": "import kfp\nfrom kfp import dsl\nimport kfp.components as components\n                                           \n\n@dsl.pipeline(name='Docker test', description='Applies Decision Tree and Logistic Regression for classification problem.')\ndef first_pipeline():\n\n    # Loads the yaml manifest for each component\n    getdata = kfp.components.load_component_from_file('getdata/getdata.yaml')\n    reshapedata = kfp.components.load_component_from_file('reshapedata/reshapedata.yaml')\n    modelbuilding = kfp.components.load_component_from_file('modelbuilding/modelbuilding.yaml')\n    kserve_op = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/'\n                                                'master/components/kserve/component.yaml') \n \n    # pipeline steps\n    step1 = getdata()\n    step2 = reshapedata()\n    step2.after(step1)\n\n    step3 = modelbuilding()\n    step3.after(step2)\n\n\n    kserve_op(action='apply',\n            model_name='tensorflow-sample',\n            model_uri='s3://mlpipeline/mnistdocker/models/detect-digits/',\n            namespace='kubeflow-user-example-com',\n            framework='tensorflow',\n            service_account='sa-minio-kserve').after(step3)\n\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(first_pipeline, 'dockerten1Pipeline.yaml')\n    # kfp.Client().create_run_from_pipeline_func(basic_pipeline, arguments={})"
  },
  {
    "repo": "pragadeeshraju/kubeflow-sample-pipeline",
    "file_path": "pipeline2.py",
    "raw_url": "https://raw.githubusercontent.com/pragadeeshraju/kubeflow-sample-pipeline/main/pipeline2.py",
    "content": "import kfp.compiler as compiler\nimport kfp.dsl as dsl\nfrom kfp import components\n\n# kfserving_op = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/'\n#                                                  'master/components/kubeflow/kfserving/component.yaml')\nkserve_op = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/'\n                                               'master/components/kserve/component.yaml')\n\n\n@dsl.pipeline(\n    name='KServe pipeline',\n    description='A pipeline for KServe.'\n)\ndef kservePipeline(\n        action='apply',\n        model_name='tensorflow-sample',\n        model_uri='s3://mlpipeline/mnistdocker/models/detect-digits/',\n        namespace='kubeflow-user-example-com',\n        framework='tensorflow',\n        service_account='sa-minio-kserve'):\n    kserve_op(action=action,\n            model_name=model_name,\n            model_uri=model_uri,\n            namespace=namespace,\n            framework=framework,\n            service_account=service_account)\n\n\n\nif __name__ == '__main__':\n    compiler.Compiler().compile(kservePipeline, 'pip.yaml')"
  },
  {
    "repo": "dedmari/kubeflow_trident_pipeline",
    "file_path": "pipeline/src/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/dedmari/kubeflow_trident_pipeline/master/pipeline/src/pipeline.py",
    "content": "# Copyright (c) 2019, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport kfp.dsl as dsl\nimport datetime\nimport os\nfrom kubernetes import client as k8s_client\n\n\n# Modify image='<image>' in each op to match IMAGE in the build.sh of its corresponding component\n# Modified by Muneer\n# Defining Images as variable.\npreprocess_image='muneer7589/k_pipeline_preprocess:latest'\ntrain_image='muneer7589/k_pipeline_train:latest'\ninference_server_launcher_image='muneer7589/k_pipeline_inference:latest'\nwebapp_launcher_image='muneer7589/k_pipeline_webapp_launcher:latest'\n \n# TODO: Create configuration file for storing images and make that as general template for generating different pipelines\n\n\ndef PreprocessOp(name, input_dir, output_dir):\n    return dsl.ContainerOp(\n        name=name,\n        image=preprocess_image,\n        arguments=[\n            '--input_dir', input_dir,\n            '--output_dir', output_dir,\n        ],\n        file_outputs={'output': '/output.txt'}\n    )\n\n\ndef TrainOp(name, input_dir, output_dir, model_name, model_version, epochs):\n    return dsl.ContainerOp(\n        name=name,\n        image=train_image,\n        arguments=[\n            '--input_dir', input_dir,\n            '--output_dir', output_dir,\n            '--model_name', model_name,\n            '--model_version', model_version,\n            '--epochs', epochs\n        ],\n        file_outputs={'output': '/output.txt'}\n    )\n\n\ndef InferenceServerLauncherOp(name, input_dir, trtserver_name):\n    print(\"inside inference server ContainerOp and value of name is: \", name)\n    return dsl.ContainerOp(\n        name=name,\n        image=inference_server_launcher_image,\n        arguments=[\n            '--trtserver_name', trtserver_name,\n            '--model_path', input_dir,\n        ],\n        file_outputs={'output': '/output.txt'}\n    )\n\n\ndef WebappLauncherOp(name, trtserver_name, model_name, model_version, webapp_prefix, webapp_port):\n    print(\"Inside WeappLauncherOp and the value of name is: \", name)\n    return dsl.ContainerOp(\n        name=name,\n        image=webapp_launcher_image,\n        arguments=[\n            '--workflow_name', '{{workflow.name}}',\n            '--trtserver_name', trtserver_name,\n            '--model_name', model_name,\n            '--model_version', str(model_version),\n            '--webapp_prefix', webapp_prefix,\n            '--webapp_port', str(webapp_port)\n        ],\n        file_outputs={}\n    )\n\n\n@dsl.pipeline(\n    name='resnet_cifar10_pipeline',\n    description='Demonstrate an end-to-end training & serving pipeline using ResNet and CIFAR-10'\n)\ndef resnet_pipeline(\n    raw_data_dir='/mnt/workspace/raw_data',\n    processed_data_dir='/mnt/workspace/processed_data',\n    model_dir='/mnt/workspace/saved_model',\n    epochs=50,\n    trtserver_name='trtis',\n    model_name='resnet_graphdef',\n    model_version=1,\n    webapp_prefix='webapp',\n    webapp_port=80\n):\n\n    persistent_volume_name = 'pipeline-workspace'\n    persistent_volume_path = '/mnt/workspace'\n\n    op_dict = {}\n\n    op_dict['preprocess'] = PreprocessOp(\n        'preprocess', raw_data_dir, processed_data_dir)\n\n    op_dict['train'] = TrainOp(\n        'train', op_dict['preprocess'].output, model_dir, model_name, model_version, epochs)\n\n    op_dict['deploy_inference_server'] = InferenceServerLauncherOp(\n        'deploy_inference_server', op_dict['train'].output, trtserver_name)\n\n    op_dict['deploy_webapp'] = WebappLauncherOp(\n        'deploy_webapp', op_dict['deploy_inference_server'].output, model_name, model_version, webapp_prefix, webapp_port)\n\n    for _, container_op in op_dict.items():\n        container_op.add_volume(k8s_client.V1Volume(\n            persistent_volume_claim=k8s_client.V1PersistentVolumeClaimVolumeSource(\n                claim_name=persistent_volume_name),\n            name=persistent_volume_name))\n        container_op.add_volume_mount(k8s_client.V1VolumeMount(\n            mount_path=persistent_volume_path,\n            name=persistent_volume_name))\n\n\nif __name__ == '__main__':\n    import kfp.compiler as compiler\n    compiler.Compiler().compile(resnet_pipeline, __file__ + '.tar.gz')\n"
  },
  {
    "repo": "Alexander6463/Kubeflow_MNIST",
    "file_path": "pipeline_dev.py",
    "raw_url": "https://raw.githubusercontent.com/Alexander6463/Kubeflow_MNIST/master/pipeline_dev.py",
    "content": "import kfp\nimport kfp.components as components\nimport kfp.dsl as dsl\n\nfrom components.download_component import download_dataset\nfrom components.train_component import train_model\nfrom components.evaluate_component import evaluate_model\nfrom components.export_component import export_model\n\nBASE_IMAGE = \"drobnov1994/example:kubeflow_tensorflow_v1\"\nINPUT_BUCKET = \"pipelines-tutorial-data\"\nNAMESPACE = \"default\"\n\nkfserving = components.load_component_from_url(\n    \"https://raw.githubusercontent.com/kubeflow/pipelines/master/components/kubeflow/kfserving/component.yaml\"\n)\n\n\ndef train_and_serve(\n    input_bucket: str,\n    dataset_name: str,\n    export_bucket: str,\n    model_name: str,\n    model_version: str,\n):\n    downloadOp = components.func_to_container_op(\n        download_dataset, base_image=BASE_IMAGE\n    )(input_bucket, dataset_name)\n    downloadOp.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n    trainOp = components.func_to_container_op(train_model, base_image=BASE_IMAGE)(\n        downloadOp.output\n    )\n    trainOp.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n    evaluateOp = components.func_to_container_op(evaluate_model, base_image=BASE_IMAGE)(\n        downloadOp.output, trainOp.output\n    )\n    evaluateOp.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n    exportOp = components.func_to_container_op(export_model, base_image=BASE_IMAGE)(\n        trainOp.output, evaluateOp.output, export_bucket, model_name, model_version\n    )\n    exportOp.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n    kfservingOp = kfserving(\n        action=\"apply\",\n        model_uri=f\"s3://{export_bucket}/{model_name}\",\n        model_name=\"mnist\",\n        namespace=NAMESPACE,\n        framework=\"tensorflow\",\n        watch_timeout=\"300\",\n    )\n    kfservingOp.after(exportOp)\n    kfservingOp.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n\n\ndef op_transformer(op):\n    op.add_pod_annotation(name=\"sidecar.istio.io/inject\", value=\"false\")\n    return op\n\n\n@dsl.pipeline(\n    name=\"End-to-End MNIST Pipeline\",\n    description=\"A sample pipeline to demonstrate \"\n    \"multi-step model training,\"\n    \"evaluation, export, and serving\",\n)\ndef mnist_pipeline(\n    input_bucket: str = \"pipelines-tutorial-data\",\n    dataset_name: str = \"datasets.tar.gz\",\n    export_bucket: str = \"models\",\n    model_name: str = \"mnist\",\n    model_version: str = \"1\",\n):\n    train_and_serve(\n        input_bucket=input_bucket,\n        dataset_name=dataset_name,\n        export_bucket=export_bucket,\n        model_name=model_name,\n        model_version=model_version,\n    )\n    dsl.get_pipeline_conf().add_op_transformer(op_transformer)\n\n\narguments = {\n    \"input_bucket\": INPUT_BUCKET,\n    \"dataset_name\": \"datasets.tar.gz\",\n    \"export_bucket\": \"models\",\n    \"model_name\": \"mnist\",\n    \"model_version\": \"1\",\n}\n\nclient = kfp.Client(host=\"http://127.0.0.1:8080\")\nclient.create_run_from_pipeline_func(train_and_serve, arguments=arguments)\n"
  },
  {
    "repo": "jonrossclaytor/kubeflow-mlb",
    "file_path": "pipelines/baseball-pipeline-branch.py",
    "raw_url": "https://raw.githubusercontent.com/jonrossclaytor/kubeflow-mlb/master/pipelines/baseball-pipeline-branch.py",
    "content": "#!/usr/bin/env python3\n\nimport kfp\nfrom kfp import dsl\nimport kfp.gcp as gcp\n\n\ndef collect_stats_op(): #symbol\n    return dsl.ContainerOp(\n        name='Collect Stats',\n        image='gcr.io/ross-kubeflow/collect-stats:latest'       \n        \n    ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n\n\ndef feature_eng_op(): \n    return dsl.ContainerOp(\n        name='Feature Engineering',\n        image='gcr.io/ross-kubeflow/feature-eng:latest',   \n        \n    ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n\n\ndef train_test_val_op(pitch_type): \n    return dsl.ContainerOp(\n        name='Split Train Test Val',\n        image='gcr.io/ross-kubeflow/train-test-val:latest',\n        arguments=[\n            '--pitch_type', pitch_type\n        ]    \n        \n    ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n\n\ndef tune_hp_op(pitch_type): \n    return dsl.ContainerOp(\n        name='Tune Hyperparameters',\n        image='gcr.io/ross-kubeflow/tune-hp:latest',\n        arguments=[\n            '--pitch_type', pitch_type\n        ]    \n        \n    ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n\n\ndef train_xgboost_op(pitch_type): \n    return dsl.ContainerOp(\n        name='Train XGBoost',\n        image='gcr.io/ross-kubeflow/train-xgboost:latest',\n        arguments=[\n            '--pitch_type', pitch_type\n        ]    \n        \n    ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n\n\ndef host_xgboost_op(pitch_type): \n    return dsl.ContainerOp(\n        name='Host Model',\n        image='gcr.io/ross-kubeflow/host-xgboost:latest',\n        arguments=[\n            '--pitch_type', pitch_type\n        ] \n        \n    ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n\n\ndef find_threshold_op(pitch_type): \n    return dsl.ContainerOp(\n        name='Find Threshold',\n        image='gcr.io/ross-kubeflow/find-threshold:latest',\n        arguments=[\n            '--pitch_type', pitch_type\n        ]    \n        \n    ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n\n\ndef evaluate_model_op(pitch_type, dummy1=None): \n    return dsl.ContainerOp(\n        name='Evaluate Models',\n        image='gcr.io/ross-kubeflow/evaluate-model:latest',\n        arguments=[\n            '--pitch_type', pitch_type\n        ],\n        file_outputs={\n            'data': '/root/dummy.txt',\n        } \n        \n    ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n\n\n\n@dsl.pipeline(\n    name='Sequential pipeline',\n    description='A pipeline with sequential steps.'\n)\ndef sequential_pipeline():  \n    \"\"\"A pipeline with sequential steps.\"\"\" \n    \n    refresh_data_pipeline = feature_eng_op().after(collect_stats_op())\n\n    FT_task = evaluate_model_op('FT').after(find_threshold_op('FT').after(host_xgboost_op('FT').after(train_xgboost_op('FT').after(tune_hp_op('FT').after(train_test_val_op('FT').after(refresh_data_pipeline)))))) \n    FS_task = evaluate_model_op('FS').after(find_threshold_op('FS').after(host_xgboost_op('FS').after(train_xgboost_op('FS').after(tune_hp_op('FS').after(train_test_val_op('FS').after(refresh_data_pipeline)))))) \n    CH_task = evaluate_model_op('CH').after(find_threshold_op('CH').after(host_xgboost_op('CH').after(train_xgboost_op('CH').after(tune_hp_op('CH').after(train_test_val_op('CH').after(refresh_data_pipeline))))))\n    FF_task = evaluate_model_op('FF').after(find_threshold_op('FF').after(host_xgboost_op('FF').after(train_xgboost_op('FF').after(tune_hp_op('FF').after(train_test_val_op('FF').after(refresh_data_pipeline))))))\n    SL_task = evaluate_model_op('SL').after(find_threshold_op('SL').after(host_xgboost_op('SL').after(train_xgboost_op('SL').after(tune_hp_op('SL').after(train_test_val_op('SL').after(refresh_data_pipeline))))))\n    CU_task = evaluate_model_op('CU').after(find_threshold_op('CU').after(host_xgboost_op('CU').after(train_xgboost_op('CU').after(tune_hp_op('CU').after(train_test_val_op('CU').after(refresh_data_pipeline))))))\n    FC_task = evaluate_model_op('FC').after(find_threshold_op('FC').after(host_xgboost_op('FC').after(train_xgboost_op('FC').after(tune_hp_op('FC').after(train_test_val_op('FC').after(refresh_data_pipeline))))))\n    SI_task = evaluate_model_op('SI').after(find_threshold_op('SI').after(host_xgboost_op('SI').after(train_xgboost_op('SI').after(tune_hp_op('SI').after(train_test_val_op('SI').after(refresh_data_pipeline))))))\n    KC_task = evaluate_model_op('KC').after(find_threshold_op('KC').after(host_xgboost_op('KC').after(train_xgboost_op('KC').after(tune_hp_op('KC').after(train_test_val_op('KC').after(refresh_data_pipeline)))))) \n    EP_task = evaluate_model_op('EP').after(find_threshold_op('EP').after(host_xgboost_op('EP').after(train_xgboost_op('EP').after(tune_hp_op('EP').after(train_test_val_op('EP').after(refresh_data_pipeline))))))\n    KN_task = evaluate_model_op('KN').after(find_threshold_op('KN').after(host_xgboost_op('KN').after(train_xgboost_op('KN').after(tune_hp_op('KN').after(train_test_val_op('KN').after(refresh_data_pipeline))))))\n    FO_task = evaluate_model_op('FO').after(find_threshold_op('FO').after(host_xgboost_op('FO').after(train_xgboost_op('FO').after(tune_hp_op('FO').after(train_test_val_op('FO').after(refresh_data_pipeline))))))\n\n    \n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(sequential_pipeline, __file__ + '.zip')"
  },
  {
    "repo": "jonrossclaytor/kubeflow-mlb",
    "file_path": "pipelines/baseball-pipeline-enhance.py",
    "raw_url": "https://raw.githubusercontent.com/jonrossclaytor/kubeflow-mlb/master/pipelines/baseball-pipeline-enhance.py",
    "content": "#!/usr/bin/env python3\n\nimport kfp\nfrom kfp import dsl\nimport kfp.gcp as gcp\n\n\ndef collect_stats_op(): #symbol\n    return dsl.ContainerOp(\n        name='Collect Stats',\n        image='gcr.io/ross-kubeflow/collect-stats:latest'       \n        \n    ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n\n\ndef feature_eng_op(): \n    return dsl.ContainerOp(\n        name='Feature Engineering',\n        image='gcr.io/ross-kubeflow/feature-eng:latest',   \n        \n    ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n\n\ndef train_test_val_op(pitch_type): \n    return dsl.ContainerOp(\n        name='Split Train Test Val',\n        image='gcr.io/ross-kubeflow/train-test-val:latest',\n        arguments=[\n            '--pitch_type', pitch_type\n        ]    \n        \n    ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n\n\ndef tune_hp_op(pitch_type): \n    return dsl.ContainerOp(\n        name='Tune Hyperparameters',\n        image='gcr.io/ross-kubeflow/tune-hp:latest',\n        arguments=[\n            '--pitch_type', pitch_type\n        ]    \n        \n    ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n\n\ndef train_xgboost_op(pitch_type): \n    return dsl.ContainerOp(\n        name='Train XGBoost',\n        image='gcr.io/ross-kubeflow/train-xgboost:latest',\n        arguments=[\n            '--pitch_type', pitch_type\n        ]    \n        \n    ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n\n\ndef host_xgboost_op(pitch_type): \n    return dsl.ContainerOp(\n        name='Host Model',\n        image='gcr.io/ross-kubeflow/host-xgboost:latest',\n        arguments=[\n            '--pitch_type', pitch_type\n        ] \n        \n    ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n\n\ndef find_threshold_op(pitch_type): \n    return dsl.ContainerOp(\n        name='Find Threshold',\n        image='gcr.io/ross-kubeflow/find-threshold:latest',\n        arguments=[\n            '--pitch_type', pitch_type\n        ]    \n        \n    ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n\n\ndef evaluate_model_op(pitch_type, dummy1=None): \n    return dsl.ContainerOp(\n        name='Evaluate Models',\n        image='gcr.io/ross-kubeflow/evaluate-model:latest',\n        arguments=[\n            '--pitch_type', pitch_type\n        ],\n        file_outputs={\n            'data': '/root/dummy.txt',\n        } \n        \n    ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n\n\ndef enhance_features_op(dummy_1=None, dummy_2=None, dummy_3=None, dummy_4=None, dummy_5=None, dummy_6=None, dummy_7=None, dummy_8=None, dummy_9=None, dummy_10=None, dummy_11=None, dummy_12=None): \n    return dsl.ContainerOp(\n        name='Enhance Features',\n        image='gcr.io/ross-kubeflow/enhance-features:latest',\n        arguments=[\n            '--dummy_1', dummy_1,\n            '--dummy_2', dummy_2,\n            '--dummy_3', dummy_3,\n            '--dummy_4', dummy_4,\n            '--dummy_5', dummy_5,\n            '--dummy_6', dummy_6,\n            '--dummy_7', dummy_7,\n            '--dummy_8', dummy_8,\n            '--dummy_9', dummy_9,\n            '--dummy_10', dummy_10,\n            '--dummy_11', dummy_11,\n            '--dummy_12', dummy_12\n        ]\n        \n    ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n\n\ndef train_rf_op(): #symbol\n    return dsl.ContainerOp(\n        name='Train RF',\n        image='gcr.io/ross-kubeflow/train-rf:latest'       \n        \n    ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n\n\ndef host_rf_op(): #symbol\n    return dsl.ContainerOp(\n        name='Host RF',\n        image='gcr.io/ross-kubeflow/host-rf:latest'       \n        \n    ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n\n\n@dsl.pipeline(\n    name='Sequential pipeline',\n    description='A pipeline with sequential steps.'\n)\ndef sequential_pipeline():  \n    \"\"\"A pipeline with sequential steps.\"\"\" \n    \n    refresh_data_pipeline = feature_eng_op().after(collect_stats_op())\n\n    FT_task = evaluate_model_op('FT').after(find_threshold_op('FT').after(host_xgboost_op('FT').after(train_xgboost_op('FT').after(tune_hp_op('FT').after(train_test_val_op('FT').after(refresh_data_pipeline)))))) \n    FS_task = evaluate_model_op('FS').after(find_threshold_op('FS').after(host_xgboost_op('FS').after(train_xgboost_op('FS').after(tune_hp_op('FS').after(train_test_val_op('FS').after(refresh_data_pipeline)))))) \n    CH_task = evaluate_model_op('CH').after(find_threshold_op('CH').after(host_xgboost_op('CH').after(train_xgboost_op('CH').after(tune_hp_op('CH').after(train_test_val_op('CH').after(refresh_data_pipeline))))))\n    FF_task = evaluate_model_op('FF').after(find_threshold_op('FF').after(host_xgboost_op('FF').after(train_xgboost_op('FF').after(tune_hp_op('FF').after(train_test_val_op('FF').after(refresh_data_pipeline))))))\n    SL_task = evaluate_model_op('SL').after(find_threshold_op('SL').after(host_xgboost_op('SL').after(train_xgboost_op('SL').after(tune_hp_op('SL').after(train_test_val_op('SL').after(refresh_data_pipeline))))))\n    CU_task = evaluate_model_op('CU').after(find_threshold_op('CU').after(host_xgboost_op('CU').after(train_xgboost_op('CU').after(tune_hp_op('CU').after(train_test_val_op('CU').after(refresh_data_pipeline))))))\n    FC_task = evaluate_model_op('FC').after(find_threshold_op('FC').after(host_xgboost_op('FC').after(train_xgboost_op('FC').after(tune_hp_op('FC').after(train_test_val_op('FC').after(refresh_data_pipeline))))))\n    SI_task = evaluate_model_op('SI').after(find_threshold_op('SI').after(host_xgboost_op('SI').after(train_xgboost_op('SI').after(tune_hp_op('SI').after(train_test_val_op('SI').after(refresh_data_pipeline))))))\n    KC_task = evaluate_model_op('KC').after(find_threshold_op('KC').after(host_xgboost_op('KC').after(train_xgboost_op('KC').after(tune_hp_op('KC').after(train_test_val_op('KC').after(refresh_data_pipeline)))))) \n    EP_task = evaluate_model_op('EP').after(find_threshold_op('EP').after(host_xgboost_op('EP').after(train_xgboost_op('EP').after(tune_hp_op('EP').after(train_test_val_op('EP').after(refresh_data_pipeline))))))\n    KN_task = evaluate_model_op('KN').after(find_threshold_op('KN').after(host_xgboost_op('KN').after(train_xgboost_op('KN').after(tune_hp_op('KN').after(train_test_val_op('KN').after(refresh_data_pipeline))))))\n    FO_task = evaluate_model_op('FO').after(find_threshold_op('FO').after(host_xgboost_op('FO').after(train_xgboost_op('FO').after(tune_hp_op('FO').after(train_test_val_op('FO').after(refresh_data_pipeline))))))\n\n    enhance_features_task = enhance_features_op(FT_task.output,FS_task.output,CH_task.output,FF_task.output,SL_task.output,CU_task.output,FC_task.output,SI_task.output,KC_task.output,EP_task.output,KN_task.output,FO_task.output)\n\n    rf_ensemble_task = host_rf_op().after(train_rf_op().after(enhance_features_task))\n    \n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(sequential_pipeline, __file__ + '.zip')"
  },
  {
    "repo": "jonrossclaytor/kubeflow-mlb",
    "file_path": "pipelines/baseball-pipeline-single.py",
    "raw_url": "https://raw.githubusercontent.com/jonrossclaytor/kubeflow-mlb/master/pipelines/baseball-pipeline-single.py",
    "content": "#!/usr/bin/env python3\n\nimport kfp\nfrom kfp import dsl\nimport kfp.gcp as gcp\n\n\ndef collect_stats_op(): #symbol\n    return dsl.ContainerOp(\n        name='Collect Stats',\n        image='gcr.io/ross-kubeflow/collect-stats:latest'       \n        \n    ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n\n\ndef feature_eng_op(): \n    return dsl.ContainerOp(\n        name='Feature Engineering',\n        image='gcr.io/ross-kubeflow/feature-eng:latest',   \n        \n    ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n\n\ndef train_test_val_op(pitch_type): \n    return dsl.ContainerOp(\n        name='Split Train Test Val',\n        image='gcr.io/ross-kubeflow/train-test-val:latest',\n        arguments=[\n            '--pitch_type', pitch_type\n        ]    \n        \n    ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n\n\ndef tune_hp_op(pitch_type): \n    return dsl.ContainerOp(\n        name='Tune Hyperparameters',\n        image='gcr.io/ross-kubeflow/tune-hp:latest',\n        arguments=[\n            '--pitch_type', pitch_type\n        ]    \n        \n    ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n\n\ndef train_xgboost_op(pitch_type): \n    return dsl.ContainerOp(\n        name='Train XGBoost',\n        image='gcr.io/ross-kubeflow/train-xgboost:latest',\n        arguments=[\n            '--pitch_type', pitch_type\n        ]    \n        \n    ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n\n\ndef host_xgboost_op(pitch_type): \n    return dsl.ContainerOp(\n        name='Host Model',\n        image='gcr.io/ross-kubeflow/host-xgboost:latest',\n        arguments=[\n            '--pitch_type', pitch_type\n        ] \n        \n    ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n\n\ndef find_threshold_op(pitch_type): \n    return dsl.ContainerOp(\n        name='Find Threshold',\n        image='gcr.io/ross-kubeflow/find-threshold:latest',\n        arguments=[\n            '--pitch_type', pitch_type\n        ]    \n        \n    ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n\n\ndef evaluate_model_op(pitch_type, dummy1=None): \n    return dsl.ContainerOp(\n        name='Evaluate Models',\n        image='gcr.io/ross-kubeflow/evaluate-model:latest',\n        arguments=[\n            '--pitch_type', pitch_type\n        ],\n        file_outputs={\n            'data': '/root/dummy.txt',\n        } \n        \n    ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n\n\n\n@dsl.pipeline(\n    name='Sequential pipeline',\n    description='A pipeline with sequential steps.'\n)\ndef sequential_pipeline():  \n    \"\"\"A pipeline with sequential steps.\"\"\" \n    \n    refresh_data_pipeline = feature_eng_op().after(collect_stats_op())\n\n    FT_task = evaluate_model_op('FT').after(find_threshold_op('FT').after(host_xgboost_op('FT').after(train_xgboost_op('FT').after(tune_hp_op('FT').after(train_test_val_op('FT').after(refresh_data_pipeline)))))) \n    \n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(sequential_pipeline, __file__ + '.zip')"
  },
  {
    "repo": "xxxtrillionarie/KubeFlow_MLOps_Pipelines",
    "file_path": "samples/test/after.py",
    "raw_url": "https://raw.githubusercontent.com/xxxtrillionarie/KubeFlow_MLOps_Pipelines/master/samples/test/after.py",
    "content": "# Copyright 2020 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom kfp import components\nfrom kfp import dsl\nfrom kfp import compiler\n\ncomponent_op = components.load_component_from_text(\"\"\"\nname: Print Text\ninputs:\n- {name: text, type: String}\nimplementation:\n  container:\n    image: alpine\n    command:\n    - sh\n    - -c\n    - |\n      set -e -x\n      echo \"$0\"\n    - {inputValue: text}\n\"\"\")\n\n\n@dsl.pipeline(name='pipeline-with-after')\ndef my_pipeline():\n    task1 = component_op(text='1st task')\n    task2 = component_op(text='2nd task').after(task1)\n    task3 = component_op(text='3rd task').after(task1, task2)\n\n\nif __name__ == '__main__':\n    compiler.Compiler().compile(\n        pipeline_func=my_pipeline,\n        pipeline_root='dummy_root',\n        output_path=__file__ + '.json')\n"
  },
  {
    "repo": "xxxtrillionarie/KubeFlow_MLOps_Pipelines",
    "file_path": "samples/test/after_test.py",
    "raw_url": "https://raw.githubusercontent.com/xxxtrillionarie/KubeFlow_MLOps_Pipelines/master/samples/test/after_test.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp.deprecated as kfp\nfrom .after import my_pipeline\nfrom kfp.samples.test.utils import run_pipeline_func, TestCase\n\nrun_pipeline_func([\n    TestCase(\n        pipeline_func=my_pipeline,\n        mode=kfp.dsl.PipelineExecutionMode.V2_ENGINE),\n])\n"
  },
  {
    "repo": "xxxtrillionarie/KubeFlow_MLOps_Pipelines",
    "file_path": "samples/test/cache_v2_compatible_test.py",
    "raw_url": "https://raw.githubusercontent.com/xxxtrillionarie/KubeFlow_MLOps_Pipelines/master/samples/test/cache_v2_compatible_test.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Two step v2-compatible pipeline.\"\"\"\n\n# %%\n\nfrom __future__ import annotations\n\nimport random\nimport string\nimport unittest\nimport functools\n\nimport kfp_server_api\n\nfrom .two_step import two_step_pipeline\nfrom kfp.samples.test.utils import run_pipeline_func, TestCase, KfpMlmdClient, KfpTask\nfrom ml_metadata.proto import Execution\n\n\ndef verify_tasks(t: unittest.TestCase, tasks: dict[str, KfpTask], task_state,\n                 uri: str, some_int: int):\n    task_names = [*tasks.keys()]\n    t.assertCountEqual(task_names, ['train-op', 'preprocess'], 'task names')\n\n    preprocess = tasks['preprocess']\n    train = tasks['train-op']\n\n    t.assertEqual(\n        {\n            'name': 'preprocess',\n            'inputs': {\n                'artifacts': [],\n                'parameters': {\n                    'some_int': some_int,\n                    'uri': uri\n                }\n            },\n            'outputs': {\n                'artifacts': [{\n                    'metadata': {\n                        'display_name': 'output_dataset_one',\n                    },\n                    'name': 'output_dataset_one',\n                    'type': 'system.Dataset'\n                }],\n                'parameters': {\n                    'output_parameter_one': some_int\n                }\n            },\n            'type': 'system.ContainerExecution',\n            'state': task_state,\n        }, preprocess.get_dict())\n    t.assertEqual(\n        {\n            'name': 'train-op',\n            'inputs': {\n                'artifacts': [{\n                    'metadata': {\n                        'display_name': 'output_dataset_one',\n                    },\n                    'name': 'dataset',\n                    'type': 'system.Dataset',\n                }],\n                'parameters': {\n                    'num_steps': some_int\n                }\n            },\n            'outputs': {\n                'artifacts': [{\n                    'metadata': {\n                        'display_name': 'model',\n                    },\n                    'name': 'model',\n                    'type': 'system.Model',\n                }],\n                'parameters': {}\n            },\n            'type': 'system.ContainerExecution',\n            'state': task_state,\n        }, train.get_dict())\n\n\ndef verify(run: kfp_server_api.ApiRun, mlmd_connection_config, uri: str,\n           some_int, state: int, **kwargs):\n    t = unittest.TestCase()\n    t.maxDiff = None  # we always want to see full diff\n    t.assertEqual(run.status, 'Succeeded')\n    client = KfpMlmdClient(mlmd_connection_config=mlmd_connection_config)\n    tasks = client.get_tasks(run_id=run.id)\n    verify_tasks(t, tasks, state, uri, some_int)\n\n\nif __name__ == '__main__':\n    letters = string.ascii_lowercase\n    random_uri = 'http://' + ''.join(random.choice(letters) for i in range(5))\n    random_int = random.randint(0, 10000)\n    # TODO: update to v2 engine test\n    # run_pipeline_func([\n    #     TestCase(\n    #         pipeline_func=two_step_pipeline,\n    #         arguments={\n    #             'uri': f'{random_uri}',\n    #             'some_int': f'{random_int}'\n    #         },\n    #         verify_func=functools.partial(\n    #             verify,\n    #             uri=random_uri,\n    #             some_int=random_int,\n    #             state=Execution.State.COMPLETE,\n    #         ),\n    #         mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE,\n    #         enable_caching=True\n    #     ),\n    # ]),\n    # run_pipeline_func([\n    #     TestCase(\n    #         pipeline_func=two_step_pipeline,\n    #         arguments={\n    #             'uri': f'{random_uri}',\n    #             'some_int': f'{random_int}'\n    #         },\n    #         verify_func=functools.partial(\n    #             verify,\n    #             uri=random_uri,\n    #             some_int=random_int,\n    #             state=Execution.State.CACHED\n    #         ),\n    #         mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE,\n    #         enable_caching=True\n    #     ),\n    # ])\n\n# %%\n"
  },
  {
    "repo": "xxxtrillionarie/KubeFlow_MLOps_Pipelines",
    "file_path": "samples/test/fail.py",
    "raw_url": "https://raw.githubusercontent.com/xxxtrillionarie/KubeFlow_MLOps_Pipelines/master/samples/test/fail.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Fail pipeline.\"\"\"\n\nfrom kfp.deprecated import components, dsl\n\n\ndef fail():\n    '''Fails'''\n    import sys\n    sys.exit(1)\n\n\nfail_op = components.create_component_from_func(\n    fail, base_image='alpine:latest')\n\n\n@dsl.pipeline(name='fail-pipeline')\ndef fail_pipeline():\n    fail_task = fail_op()\n"
  },
  {
    "repo": "xxxtrillionarie/KubeFlow_MLOps_Pipelines",
    "file_path": "samples/test/fail_parameter_value_missing.py",
    "raw_url": "https://raw.githubusercontent.com/xxxtrillionarie/KubeFlow_MLOps_Pipelines/master/samples/test/fail_parameter_value_missing.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom kfp.deprecated import dsl, components\n\necho = components.load_component_from_text(\n    \"\"\"\nname: Echo\ninputs:\n- {name: text, type: String}\nimplementation:\n  container:\n    image: alpine\n    command:\n    - echo\n    - {inputValue: text}\n\"\"\"\n)\n\n\n@dsl.pipeline(name='parameter_value_missing')\ndef pipeline(\n    parameter:\n    str  # parameter should be specified when submitting, but we are missing it in the test\n):\n    echo_op = echo(text=parameter)\n"
  },
  {
    "repo": "xxxtrillionarie/KubeFlow_MLOps_Pipelines",
    "file_path": "samples/test/fail_parameter_value_missing_test.py",
    "raw_url": "https://raw.githubusercontent.com/xxxtrillionarie/KubeFlow_MLOps_Pipelines/master/samples/test/fail_parameter_value_missing_test.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp.deprecated as kfp\nfrom .fail_parameter_value_missing import pipeline\nfrom kfp.samples.test.utils import run_pipeline_func, TestCase\n\n\ndef verify(run, run_id: str, **kwargs):\n    assert run.status == 'Succeeded'\n    # TODO(Bobgy): should a pipeline fail when it is missing a required input?\n    # assert run.status == 'Failed'\n\n\nrun_pipeline_func([\n    TestCase(\n        pipeline_func=pipeline,\n        verify_func=verify,\n        mode=kfp.dsl.PipelineExecutionMode.V1_LEGACY)\n])\n"
  },
  {
    "repo": "xxxtrillionarie/KubeFlow_MLOps_Pipelines",
    "file_path": "samples/test/fail_test.py",
    "raw_url": "https://raw.githubusercontent.com/xxxtrillionarie/KubeFlow_MLOps_Pipelines/master/samples/test/fail_test.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Fail pipeline.\"\"\"\n\nfrom __future__ import annotations\nimport unittest\nimport kfp.deprecated as kfp\nimport kfp_server_api\nfrom ml_metadata.proto import Execution\nfrom .fail import fail_pipeline\nfrom .fail_v2 import fail_pipeline as fail_v2_pipeline\nfrom kfp.samples.test.utils import TaskInputs, TaskOutputs, run_pipeline_func, TestCase, KfpTask\n\n\ndef verify(run, **kwargs):\n    assert run.status == 'Failed'\n\n\ndef verify_v2(t: unittest.TestCase, run: kfp_server_api.ApiRun,\n              tasks: dict[str, KfpTask], **kwargs):\n    t.assertEqual(run.status, 'Failed')\n    t.assertEqual(\n        {\n            'fail':\n                KfpTask(\n                    name='fail',\n                    type='system.ContainerExecution',\n                    # TODO(Bobgy): fix v2 engine to properly publish FAILED state.\n                    state=Execution.State.RUNNING,\n                    inputs=TaskInputs(parameters={}, artifacts=[]),\n                    outputs=TaskOutputs(parameters={}, artifacts=[]),\n                )\n        },\n        tasks,\n    )\n\n\nrun_pipeline_func([\n    TestCase(\n        pipeline_func=fail_v2_pipeline,\n        verify_func=verify_v2,\n        mode=kfp.dsl.PipelineExecutionMode.V2_ENGINE),\n    TestCase(\n        pipeline_func=fail_pipeline,\n        verify_func=verify,\n        mode=kfp.dsl.PipelineExecutionMode.V1_LEGACY),\n])\n"
  },
  {
    "repo": "xxxtrillionarie/KubeFlow_MLOps_Pipelines",
    "file_path": "samples/test/fail_v2.py",
    "raw_url": "https://raw.githubusercontent.com/xxxtrillionarie/KubeFlow_MLOps_Pipelines/master/samples/test/fail_v2.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Fail pipeline.\"\"\"\n\nfrom kfp import dsl\n\n\n@dsl.component\ndef fail():\n    '''Fails'''\n    import sys\n    sys.exit(1)\n\n\n@dsl.pipeline(name='fail-pipeline')\ndef fail_pipeline():\n    fail_task = fail()\n"
  },
  {
    "repo": "xxxtrillionarie/KubeFlow_MLOps_Pipelines",
    "file_path": "samples/test/legacy_data_passing.py",
    "raw_url": "https://raw.githubusercontent.com/xxxtrillionarie/KubeFlow_MLOps_Pipelines/master/samples/test/legacy_data_passing.py",
    "content": "# This pipeline demonstrates and verifies all ways of data passing supported by KFP.\n\n# %% [markdown]\n# KFP has simple data passing model. There are three different kinds of arguments and tw different ways to consume an input argument. All combinations are supported.\n\n# Any input can be consumed:\n# * As value (`inputValue` placeholder)\n# * As file (`inputPath` placeholder).\n\n# Input argument can come from:\n# * Constant value\n# * Pipeline parameter\n# * Upstream component output\n\n# Combining these options there are 6 end-to-end data passing cases, each of which works regardless of type:\n\n# 1. Pass constant value which gets consumed as value. (Common)\n# 2. Pass constant value which gets consumed as file. (Often used in test pipelines)\n# 3. Pass pipeline parameter that gets consumed as value. (Common)\n# 4. Pass pipeline parameter that gets consumed as file. (Rare. Sometimes used with JSON parameters)\n# 5. Pass task output that gets consumed as value. (Common)\n# 6. Pass task output that gets consumed as file. (Common)\n\n# The only restriction on types is that when both upstream output and downstream input have types, the types must match.\n\n# %%\nimport kfp.deprecated as kfp\nfrom kfp.deprecated.components import create_component_from_func, InputPath, OutputPath\n\n\n# Components\n# Produce\n@create_component_from_func\ndef produce_anything(data_path: OutputPath()):\n    with open(data_path, \"w\") as f:\n        f.write(\"produce_anything\")\n\n\n@create_component_from_func\ndef produce_something(data_path: OutputPath(\"Something\")):\n    with open(data_path, \"w\") as f:\n        f.write(\"produce_something\")\n\n\n@create_component_from_func\ndef produce_something2() -> 'Something':\n    return \"produce_something2\"\n\n\n@create_component_from_func\ndef produce_string() -> str:\n    return \"produce_string\"\n\n\n# Consume as value\n@create_component_from_func\ndef consume_anything_as_value(data):\n    print(\"consume_anything_as_value: \" + data)\n\n\n@create_component_from_func\ndef consume_something_as_value(data: \"Something\"):\n    print(\"consume_something_as_value: \" + data)\n\n\n@create_component_from_func\ndef consume_string_as_value(data: str):\n    print(\"consume_string_as_value: \" + data)\n\n\n# Consume as file\n@create_component_from_func\ndef consume_anything_as_file(data_path: InputPath()):\n    with open(data_path) as f:\n        print(\"consume_anything_as_file: \" + f.read())\n\n\n@create_component_from_func\ndef consume_something_as_file(data_path: InputPath('Something')):\n    with open(data_path) as f:\n        print(\"consume_something_as_file: \" + f.read())\n\n\n@create_component_from_func\ndef consume_string_as_file(data_path: InputPath(str)):\n    with open(data_path) as f:\n        print(\"consume_string_as_file: \" + f.read())\n\n\n# Pipeline\n@kfp.dsl.pipeline(name='data_passing_pipeline')\ndef data_passing_pipeline(\n    anything_param=\"anything_param\",\n    something_param: \"Something\" = \"something_param\",\n    string_param: str = \"string_param\",\n):\n    produced_anything = produce_anything().output\n    produced_something = produce_something().output\n    produced_string = produce_string().output\n\n    # Pass constant value; consume as value\n    consume_anything_as_value(\"constant\")\n    consume_something_as_value(\"constant\")\n    consume_string_as_value(\"constant\")\n\n    # Pass constant value; consume as file\n    consume_anything_as_file(\"constant\")\n    consume_something_as_file(\"constant\")\n    consume_string_as_file(\"constant\")\n\n    # Pass pipeline parameter; consume as value\n    consume_anything_as_value(anything_param)\n    consume_anything_as_value(something_param)\n    consume_anything_as_value(string_param)\n    consume_something_as_value(anything_param)\n    consume_something_as_value(something_param)\n    consume_string_as_value(anything_param)\n    consume_string_as_value(string_param)\n\n    # Pass pipeline parameter; consume as file\n    consume_anything_as_file(anything_param)\n    consume_anything_as_file(something_param)\n    consume_anything_as_file(string_param)\n    consume_something_as_file(anything_param)\n    consume_something_as_file(something_param)\n    consume_string_as_file(anything_param)\n    consume_string_as_file(string_param)\n\n    # Pass task output; consume as value\n    consume_anything_as_value(produced_anything)\n    consume_anything_as_value(produced_something)\n    consume_anything_as_value(produced_string)\n    consume_something_as_value(produced_anything)\n    consume_something_as_value(produced_something)\n    consume_string_as_value(produced_anything)\n    consume_string_as_value(produced_string)\n\n    # Pass task output; consume as file\n    consume_anything_as_file(produced_anything)\n    consume_anything_as_file(produced_something)\n    consume_anything_as_file(produced_string)\n    consume_something_as_file(produced_anything)\n    consume_something_as_file(produced_something)\n    consume_string_as_file(produced_anything)\n    consume_string_as_file(produced_string)\n\n\nif __name__ == \"__main__\":\n    kfp_endpoint = None\n    kfp.Client(host=kfp_endpoint).create_run_from_pipeline_func(\n        data_passing_pipeline, arguments={})\n"
  },
  {
    "repo": "xxxtrillionarie/KubeFlow_MLOps_Pipelines",
    "file_path": "samples/test/legacy_exit_handler.py",
    "raw_url": "https://raw.githubusercontent.com/xxxtrillionarie/KubeFlow_MLOps_Pipelines/master/samples/test/legacy_exit_handler.py",
    "content": "#!/usr/bin/env python3\n# Copyright 2019 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp.deprecated as kfp\nfrom kfp.deprecated import dsl, compiler\n\n\ndef gcs_download_op(url):\n    return dsl.ContainerOp(\n        name='GCS - Download',\n        image='google/cloud-sdk:279.0.0',\n        command=['sh', '-ce'],\n        arguments=['gsutil cp $0 $1 && cat $1', url, '/tmp/results.txt'],\n        file_outputs={\n            'data': '/tmp/results.txt',\n        }\n    )\n\n\ndef echo_op(text):\n    return dsl.ContainerOp(\n        name='echo',\n        image='library/bash:4.4.23',\n        command=['sh', '-cex'],\n        arguments=['echo \"$0\"', text],\n    )\n\n\n@dsl.pipeline(\n    name='Exit Handler',\n    description=\n    'Downloads a message and prints it. The exit handler will run after the pipeline finishes (successfully or not).'\n)\ndef download_and_print(url='gs://ml-pipeline/shakespeare1.txt'):\n    \"\"\"A sample pipeline showing exit handler.\"\"\"\n\n    exit_task = echo_op('exit!')\n\n    with dsl.ExitHandler(exit_task):\n        download_task = gcs_download_op(url)\n        echo_task = echo_op(download_task.output)\n\n\nif __name__ == '__main__':\n    compiler.Compiler().compile(download_and_print, __file__ + '.yaml')\n"
  },
  {
    "repo": "xxxtrillionarie/KubeFlow_MLOps_Pipelines",
    "file_path": "samples/test/legacy_exit_handler_test.py",
    "raw_url": "https://raw.githubusercontent.com/xxxtrillionarie/KubeFlow_MLOps_Pipelines/master/samples/test/legacy_exit_handler_test.py",
    "content": "#!/usr/bin/env python3\n# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp.deprecated as kfp\nfrom .legacy_exit_handler import download_and_print\nfrom kfp.samples.test.utils import run_pipeline_func, TestCase\n\nrun_pipeline_func([\n    # This sample is expected to not work on v2 compatible mode.\n    TestCase(\n        pipeline_func=download_and_print,\n        mode=kfp.dsl.PipelineExecutionMode.V1_LEGACY)\n])\n"
  },
  {
    "repo": "xxxtrillionarie/KubeFlow_MLOps_Pipelines",
    "file_path": "samples/test/lightweight_python_functions_v2_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/xxxtrillionarie/KubeFlow_MLOps_Pipelines/master/samples/test/lightweight_python_functions_v2_pipeline.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Sample pipeline for passing data in KFP v2.\"\"\"\nfrom typing import Dict, List\nimport os\n\nfrom kfp import dsl\nfrom kfp.dsl import Input, InputPath, Output, OutputPath, Dataset, Model, component\nimport kfp.compiler as compiler\n\n# In tests, we install a KFP package from the PR under test. Users should not\n# normally need to specify `kfp_package_path` in their component definitions.\n_KFP_PACKAGE_PATH = os.getenv('KFP_PACKAGE_PATH')\n\n\n@component(kfp_package_path=_KFP_PACKAGE_PATH)\ndef preprocess(\n    # An input parameter of type string.\n    message: str,\n    # Use Output[T] to get a metadata-rich handle to the output artifact\n    # of type `Dataset`.\n    output_dataset_one: Output[Dataset],\n    # A locally accessible filepath for another output artifact of type\n    # `Dataset`.\n    output_dataset_two_path: OutputPath('Dataset'),\n    # A locally accessible filepath for an output parameter of type string.\n    output_parameter_path: OutputPath(str),\n    # A locally accessible filepath for an output parameter of type bool.\n    output_bool_parameter_path: OutputPath(bool),\n    # A locally accessible filepath for an output parameter of type dict.\n    output_dict_parameter_path: OutputPath(Dict[str, int]),\n    # A locally accessible filepath for an output parameter of type list.\n    output_list_parameter_path: OutputPath(List[str]),\n    # An input message that defaults to empty.\n    empty_message: str = \"\",\n):\n    \"\"\"Dummy preprocessing step\"\"\"\n\n    # Use Dataset.path to access a local file path for writing.\n    # One can also use Dataset.uri to access the actual URI file path.\n    with open(output_dataset_one.path, 'w') as f:\n        f.write(message)\n\n    # OutputPath is used to just pass the local file path of the output artifact\n    # to the function.\n    with open(output_dataset_two_path, 'w') as f:\n        f.write(message)\n\n    with open(output_parameter_path, 'w') as f:\n        f.write(message)\n\n    with open(output_bool_parameter_path, 'w') as f:\n        f.write(\n            str(True)\n        )  # use either `str()` or `json.dumps()` for bool values.\n\n    import json\n    with open(output_dict_parameter_path, 'w') as f:\n        f.write(json.dumps({'A': 1, 'B': 2}))\n\n    with open(output_list_parameter_path, 'w') as f:\n        f.write(json.dumps(['a', 'b', 'c']))\n\n\n@component(kfp_package_path=_KFP_PACKAGE_PATH)\ndef train(\n    # Use InputPath to get a locally accessible path for the input artifact\n    # of type `Dataset`.\n    dataset_one_path: InputPath('Dataset'),\n    # Use Input[T] to get a metadata-rich handle to the input artifact\n    # of type `Dataset`.\n    dataset_two: Input[Dataset],\n    # An input parameter of type string.\n    message: str,\n    # Use Output[T] to get a metadata-rich handle to the output artifact\n    # of type `Dataset`.\n    model: Output[Model],\n    # An input parameter of type bool.\n    input_bool: bool,\n    # An input parameter of type dict.\n    input_dict: Dict[str, int],\n    # An input parameter of type List[str].\n    input_list: List[str],\n    # An input parameter of type int with a default value.\n    num_steps: int = 100,\n):\n    \"\"\"Dummy Training step\"\"\"\n    with open(dataset_one_path, 'r') as input_file:\n        dataset_one_contents = input_file.read()\n\n    with open(dataset_two.path, 'r') as input_file:\n        dataset_two_contents = input_file.read()\n\n    line = (\n        f'dataset_one_contents: {dataset_one_contents} || '\n        f'dataset_two_contents: {dataset_two_contents} || '\n        f'message: {message} || '\n        f'input_bool: {input_bool}, type {type(input_bool)} || '\n        f'input_dict: {input_dict}, type {type(input_dict)} || '\n        f'input_list: {input_list}, type {type(input_list)} \\n'\n    )\n\n    with open(model.path, 'w') as output_file:\n        for i in range(num_steps):\n            output_file.write('Step {}\\n{}\\n=====\\n'.format(i, line))\n\n    # model is an instance of Model artifact, which has a .metadata dictionary\n    # to store arbitrary metadata for the output artifact.\n    model.metadata['accuracy'] = 0.9\n\n\n@dsl.pipeline(name='my-test-pipeline-beta')\ndef pipeline(message: str = 'message'):\n    preprocess_task = preprocess(message=message)\n    train_task = train(\n        dataset_one_path=preprocess_task.outputs['output_dataset_one'],\n        dataset_two=preprocess_task.outputs['output_dataset_two_path'],\n        message=preprocess_task.outputs['output_parameter_path'],\n        input_bool=preprocess_task.outputs['output_bool_parameter_path'],\n        input_dict=preprocess_task.outputs['output_dict_parameter_path'],\n        input_list=preprocess_task.outputs['output_list_parameter_path'],\n    )\n\n\nif __name__ == '__main__':\n    compiler.Compiler().compile(\n        pipeline_func=pipeline, package_path=__file__.replace('.py', '.yaml')\n    )\n"
  },
  {
    "repo": "xxxtrillionarie/KubeFlow_MLOps_Pipelines",
    "file_path": "samples/test/lightweight_python_functions_v2_with_outputs.py",
    "raw_url": "https://raw.githubusercontent.com/xxxtrillionarie/KubeFlow_MLOps_Pipelines/master/samples/test/lightweight_python_functions_v2_with_outputs.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Lightweight functions v2 with outputs.\"\"\"\nfrom typing import NamedTuple\nimport os\n\nfrom kfp import compiler, dsl\nfrom kfp.dsl import Input, Dataset, Model, Metrics, component\n\n# In tests, we install a KFP package from the PR under test. Users should not\n# normally need to specify `kfp_package_path` in their component definitions.\n_KFP_PACKAGE_PATH = os.getenv('KFP_PACKAGE_PATH')\n\n\n@component(kfp_package_path=_KFP_PACKAGE_PATH)\ndef concat_message(first: str, second: str) -> str:\n    return first + second\n\n\n@component(kfp_package_path=_KFP_PACKAGE_PATH)\ndef add_numbers(first: int, second: int) -> int:\n    return first + second\n\n\n@component(kfp_package_path=_KFP_PACKAGE_PATH)\ndef output_artifact(number: int, message: str) -> Dataset:\n    result = [message for _ in range(number)]\n    return '\\n'.join(result)\n\n\n@component(kfp_package_path=_KFP_PACKAGE_PATH)\ndef output_named_tuple(\n    artifact: Input[Dataset]\n) -> NamedTuple('Outputs', [\n    ('scalar', str),\n    ('metrics', Metrics),\n    ('model', Model),\n]):\n    scalar = \"123\"\n\n    import json\n    metrics = json.dumps({\n        'metrics': [{\n            'name': 'accuracy',\n            'numberValue': 0.9,\n            'format': \"PERCENTAGE\",\n        }]\n    })\n\n    with open(artifact.path, 'r') as f:\n        artifact_contents = f.read()\n    model = \"Model contents: \" + artifact_contents\n\n    from collections import namedtuple\n    output = namedtuple('Outputs', ['scalar', 'metrics', 'model'])\n    return output(scalar, metrics, model)\n\n\n@dsl.pipeline(name='functions-with-outputs')\ndef pipeline(\n    first_message: str = 'first',\n    second_message: str = 'second',\n    first_number: int = 1,\n    second_number: int = 2,\n):\n    concat_task = concat_message(first=first_message, second=second_message)\n    add_numbers_task = add_numbers(first=first_number, second=second_number)\n    output_artifact_task = output_artifact(\n        number=add_numbers_task.output, message=concat_task.output)\n    output_name_tuple = output_named_tuple(artifact=output_artifact_task.output)\n\n\nif __name__ == '__main__':\n    compiler.Compiler().compile(\n        pipeline_func=pipeline, package_path=__file__ + '.yaml')\n"
  },
  {
    "repo": "xxxtrillionarie/KubeFlow_MLOps_Pipelines",
    "file_path": "samples/test/lightweight_python_functions_v2_with_outputs_test.py",
    "raw_url": "https://raw.githubusercontent.com/xxxtrillionarie/KubeFlow_MLOps_Pipelines/master/samples/test/lightweight_python_functions_v2_with_outputs_test.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom pprint import pprint\nimport unittest\nimport kfp.deprecated as kfp\nimport kfp_server_api\nimport os\nfrom minio import Minio\n\nfrom .lightweight_python_functions_v2_with_outputs import pipeline\nfrom kfp.samples.test.utils import KfpMlmdClient, run_pipeline_func, TestCase\n\n\ndef verify(run: kfp_server_api.ApiRun, mlmd_connection_config, **kwargs):\n    t = unittest.TestCase()\n    t.maxDiff = None  # we always want to see full diff\n    t.assertEqual(run.status, 'Succeeded')\n    client = KfpMlmdClient(mlmd_connection_config=mlmd_connection_config)\n    tasks = client.get_tasks(run_id=run.id)\n    pprint(tasks)\n\n    output_artifact = tasks['output-artifact']\n    output = [\n        a for a in output_artifact.outputs.artifacts if a.name == 'Output'\n    ][0]\n    pprint(output)\n\n    host = os.environ['MINIO_SERVICE_SERVICE_HOST']\n    port = os.environ['MINIO_SERVICE_SERVICE_PORT']\n    minio = Minio(\n        f'{host}:{port}',\n        access_key='minio',\n        secret_key='minio123',\n        secure=False)\n    bucket, key = output.uri[len('minio://'):].split('/', 1)\n    print(f'bucket={bucket} key={key}')\n    response = minio.get_object(bucket, key)\n    data = response.read().decode('UTF-8')\n    t.assertEqual(data, 'firstsecond\\nfirstsecond\\nfirstsecond')\n\n\nrun_pipeline_func([\n    TestCase(\n        pipeline_func=pipeline,\n        mode=kfp.dsl.PipelineExecutionMode.V2_ENGINE,\n    ),\n])\n"
  },
  {
    "repo": "xxxtrillionarie/KubeFlow_MLOps_Pipelines",
    "file_path": "samples/test/metrics_visualization_v1.py",
    "raw_url": "https://raw.githubusercontent.com/xxxtrillionarie/KubeFlow_MLOps_Pipelines/master/samples/test/metrics_visualization_v1.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp.deprecated.dsl as dsl\nfrom ..core.visualization.confusion_matrix import confusion_visualization\nfrom ..core.visualization.html import html_visualization\nfrom ..core.visualization.markdown import markdown_visualization\nfrom ..core.visualization.roc import roc_visualization\nfrom ..core.visualization.table import table_visualization\n\n# Note: This test is to verify that visualization metrics on V1 is runnable by KFP.\n# However, this pipeline is only runnable on V1 mode, but not V2 compatible mode.\n\n@dsl.pipeline(\n    name='metrics-visualization-v1-pipeline')\ndef metrics_visualization_v1_pipeline():\n    confusion_visualization_task = confusion_visualization()\n    html_visualization_task = html_visualization(\"\")\n    markdown_visualization_task = markdown_visualization()\n    roc_visualization_task = roc_visualization()\n    table_visualization_task = table_visualization()\n"
  },
  {
    "repo": "xxxtrillionarie/KubeFlow_MLOps_Pipelines",
    "file_path": "samples/test/metrics_visualization_v1_test.py",
    "raw_url": "https://raw.githubusercontent.com/xxxtrillionarie/KubeFlow_MLOps_Pipelines/master/samples/test/metrics_visualization_v1_test.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom pprint import pprint\nfrom .metrics_visualization_v1 import metrics_visualization_v1_pipeline\nfrom kfp.samples.test.utils import run_pipeline_func, TestCase\n\nimport kfp.deprecated as kfp\n\nrun_pipeline_func([\n    TestCase(\n        pipeline_func=metrics_visualization_v1_pipeline,\n        mode=kfp.dsl.PipelineExecutionMode.V1_LEGACY,\n    )\n])\n"
  },
  {
    "repo": "xxxtrillionarie/KubeFlow_MLOps_Pipelines",
    "file_path": "samples/test/metrics_visualization_v2.py",
    "raw_url": "https://raw.githubusercontent.com/xxxtrillionarie/KubeFlow_MLOps_Pipelines/master/samples/test/metrics_visualization_v2.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport os\n\nfrom kfp import dsl\nfrom kfp.dsl import (\n    component,\n    Output,\n    ClassificationMetrics,\n    Metrics,\n    HTML,\n    Markdown\n)\n\n# In tests, we install a KFP package from the PR under test. Users should not\n# normally need to specify `kfp_package_path` in their component definitions.\n_KFP_PACKAGE_PATH = os.getenv('KFP_PACKAGE_PATH')\n\n\n@component(\n    packages_to_install=['sklearn'],\n    base_image='python:3.9',\n    kfp_package_path=_KFP_PACKAGE_PATH,\n)\ndef digit_classification(metrics: Output[Metrics]):\n    from sklearn import model_selection\n    from sklearn.linear_model import LogisticRegression\n    from sklearn import datasets\n    from sklearn.metrics import accuracy_score\n\n    # Load digits dataset\n    iris = datasets.load_iris()\n\n    # # Create feature matrix\n    X = iris.data\n\n    # Create target vector\n    y = iris.target\n\n    #test size\n    test_size = 0.33\n\n    seed = 7\n    #cross-validation settings\n    kfold = model_selection.KFold(n_splits=10, random_state=seed, shuffle=True)\n\n    #Model instance\n    model = LogisticRegression()\n    scoring = 'accuracy'\n    results = model_selection.cross_val_score(model, X, y, cv=kfold, scoring=scoring)\n\n    #split data\n    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=test_size, random_state=seed)\n    #fit model\n    model.fit(X_train, y_train)\n\n    #accuracy on test set\n    result = model.score(X_test, y_test)\n    metrics.log_metric('accuracy', (result*100.0))\n\n@component(\n    packages_to_install=['sklearn'],\n    base_image='python:3.9',\n    kfp_package_path=_KFP_PACKAGE_PATH,\n)\ndef wine_classification(metrics: Output[ClassificationMetrics]):\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.metrics import roc_curve\n    from sklearn.datasets import load_wine\n    from sklearn.model_selection import train_test_split, cross_val_predict\n\n    X, y = load_wine(return_X_y=True)\n    # Binary classification problem for label 1.\n    y = y == 1\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n    rfc = RandomForestClassifier(n_estimators=10, random_state=42)\n    rfc.fit(X_train, y_train)\n    y_scores = cross_val_predict(rfc, X_train, y_train, cv=3, method='predict_proba')\n    y_predict = cross_val_predict(rfc, X_train, y_train, cv=3, method='predict')\n    fpr, tpr, thresholds = roc_curve(y_true=y_train, y_score=y_scores[:,1], pos_label=True)\n    metrics.log_roc_curve(fpr, tpr, thresholds)\n\n@component(\n    packages_to_install=['sklearn'],\n    base_image='python:3.9',\n    kfp_package_path=_KFP_PACKAGE_PATH,\n)\ndef iris_sgdclassifier(test_samples_fraction: float, metrics: Output[ClassificationMetrics]):\n    from sklearn import datasets, model_selection\n    from sklearn.linear_model import SGDClassifier\n    from sklearn.metrics import confusion_matrix\n\n    iris_dataset = datasets.load_iris()\n    train_x, test_x, train_y, test_y = model_selection.train_test_split(\n        iris_dataset['data'], iris_dataset['target'], test_size=test_samples_fraction)\n\n\n    classifier = SGDClassifier()\n    classifier.fit(train_x, train_y)\n    predictions = model_selection.cross_val_predict(classifier, train_x, train_y, cv=3)\n    metrics.log_confusion_matrix(\n        ['Setosa', 'Versicolour', 'Virginica'],\n        confusion_matrix(train_y, predictions).tolist() # .tolist() to convert np array to list.\n    )\n\n@component(\n    kfp_package_path=_KFP_PACKAGE_PATH,\n)\ndef html_visualization(html_artifact: Output[HTML]):\n    html_content = '<!DOCTYPE html><html><body><h1>Hello world</h1></body></html>'\n    with open(html_artifact.path, 'w') as f:\n        f.write(html_content)\n\n@component(\n    kfp_package_path=_KFP_PACKAGE_PATH,\n)\ndef markdown_visualization(markdown_artifact: Output[Markdown]):\n    markdown_content = '## Hello world \\n\\n Markdown content'\n    with open(markdown_artifact.path, 'w') as f:\n        f.write(markdown_content)\n\n@dsl.pipeline(\n    name='metrics-visualization-pipeline')\ndef metrics_visualization_pipeline():\n    wine_classification_op = wine_classification()\n    iris_sgdclassifier_op = iris_sgdclassifier(test_samples_fraction=0.3)\n    digit_classification_op = digit_classification()\n    html_visualization_op = html_visualization()\n    markdown_visualization_op = markdown_visualization()\n"
  },
  {
    "repo": "xxxtrillionarie/KubeFlow_MLOps_Pipelines",
    "file_path": "samples/test/metrics_visualization_v2_test.py",
    "raw_url": "https://raw.githubusercontent.com/xxxtrillionarie/KubeFlow_MLOps_Pipelines/master/samples/test/metrics_visualization_v2_test.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import annotations\nimport unittest\nimport unittest.mock as mock\nimport kfp.deprecated as kfp\nimport kfp_server_api\n\nfrom .metrics_visualization_v2 import metrics_visualization_pipeline\nfrom kfp.samples.test.utils import KfpTask, run_pipeline_func, TestCase\nfrom ml_metadata.proto import Execution\n\n\ndef verify(t: unittest.TestCase, run: kfp_server_api.ApiRun,\n           tasks: dict[str, KfpTask], **kwargs):\n    t.assertEqual(run.status, 'Succeeded')\n    task_names = [*tasks.keys()]\n    t.assertCountEqual(task_names, [\n        'wine-classification', 'iris-sgdclassifier', 'digit-classification',\n        'html-visualization', 'markdown-visualization'\n    ], 'task names')\n\n    wine_classification = tasks['wine-classification']\n    iris_sgdclassifier = tasks['iris-sgdclassifier']\n    digit_classification = tasks['digit-classification']\n    html_visualization = tasks['html-visualization']\n    markdown_visualization = tasks['markdown-visualization']\n\n    t.assertEqual(\n        {\n            'name': 'wine-classification',\n            'inputs': {},\n            'outputs': {\n                'artifacts': [{\n                    'metadata': {\n                        'display_name': 'metrics',\n                        'confidenceMetrics': {\n                            'list': [{\n                                'confidenceThreshold': 2.0,\n                                'falsePositiveRate': 0.0,\n                                'recall': 0.0\n                            }, {\n                                'confidenceThreshold': 1.0,\n                                'falsePositiveRate': 0.0,\n                                'recall': 0.33962264150943394\n                            }, {\n                                'confidenceThreshold': 0.9,\n                                'falsePositiveRate': 0.0,\n                                'recall': 0.6037735849056604\n                            }, {\n                                'confidenceThreshold': 0.8,\n                                'falsePositiveRate': 0.0,\n                                'recall': 0.8490566037735849\n                            }, {\n                                'confidenceThreshold': 0.6,\n                                'falsePositiveRate': 0.0,\n                                'recall': 0.8867924528301887\n                            }, {\n                                'confidenceThreshold': 0.5,\n                                'falsePositiveRate': 0.0125,\n                                'recall': 0.9245283018867925\n                            }, {\n                                'confidenceThreshold': 0.4,\n                                'falsePositiveRate': 0.075,\n                                'recall': 0.9622641509433962\n                            }, {\n                                'confidenceThreshold': 0.3,\n                                'falsePositiveRate': 0.0875,\n                                'recall': 1.0\n                            }, {\n                                'confidenceThreshold': 0.2,\n                                'falsePositiveRate': 0.2375,\n                                'recall': 1.0\n                            }, {\n                                'confidenceThreshold': 0.1,\n                                'falsePositiveRate': 0.475,\n                                'recall': 1.0\n                            }, {\n                                'confidenceThreshold': 0.0,\n                                'falsePositiveRate': 1.0,\n                                'recall': 1.0\n                            }]\n                        }\n                    },\n                    'name': 'metrics',\n                    'type': 'system.ClassificationMetrics'\n                }],\n            },\n            'type': 'system.ContainerExecution',\n            'state': Execution.State.COMPLETE,\n        }, wine_classification.get_dict())\n    t.assertEqual(\n        {\n            'inputs': {\n                'parameters': {\n                    'test_samples_fraction': 0.3\n                }\n            },\n            'name': 'iris-sgdclassifier',\n            'outputs': {\n                'artifacts': [{\n                    'metadata': {\n                        'display_name': 'metrics',\n                        'confusionMatrix': {\n                            'struct': {\n                                'annotationSpecs': [{\n                                    'displayName': 'Setosa'\n                                }, {\n                                    'displayName': 'Versicolour'\n                                }, {\n                                    'displayName': 'Virginica'\n                                }],\n                                'rows': [\n                                    {  # these numbers can be random during execution\n                                        'row': [mock.ANY, mock.ANY, mock.ANY]\n                                    },\n                                    {\n                                        'row': [mock.ANY, mock.ANY, mock.ANY]\n                                    },\n                                    {\n                                        'row': [mock.ANY, mock.ANY, mock.ANY]\n                                    }\n                                ]\n                            }\n                        }\n                    },\n                    'name': 'metrics',\n                    'type': 'system.ClassificationMetrics'\n                }],\n            },\n            'type': 'system.ContainerExecution',\n            'state': Execution.State.COMPLETE,\n        },\n        iris_sgdclassifier.get_dict())\n    rows = iris_sgdclassifier.get_dict()['outputs']['artifacts'][0]['metadata'][\n        'confusionMatrix']['struct']['rows']\n    for i, row in enumerate(rows):\n        for j, item in enumerate(row['row']):\n            t.assertIsInstance(\n                item, float,\n                f'value of confusion matrix row {i}, col {j} is not a number')\n\n    t.assertEqual(\n        {\n            'name': 'digit-classification',\n            'inputs': {},\n            'outputs': {\n                'artifacts': [{\n                    'metadata': {\n                        'display_name': 'metrics',\n                        'accuracy': 92.0,\n                    },\n                    'name': 'metrics',\n                    'type': 'system.Metrics'\n                }],\n            },\n            'type': 'system.ContainerExecution',\n            'state': Execution.State.COMPLETE,\n        }, digit_classification.get_dict())\n\n    t.assertEqual(\n        {\n            'name': 'html-visualization',\n            'inputs': {},\n            'outputs': {\n                'artifacts': [{\n                    'metadata': {\n                        'display_name': 'html_artifact'\n                    },\n                    'name': 'html_artifact',\n                    'type': 'system.HTML'\n                }],\n            },\n            'state': Execution.State.COMPLETE,\n            'type': 'system.ContainerExecution'\n        }, html_visualization.get_dict())\n\n    t.assertEqual(\n        {\n            'name': 'markdown-visualization',\n            'inputs': {},\n            'outputs': {\n                'artifacts': [{\n                    'metadata': {\n                        'display_name': 'markdown_artifact'\n                    },\n                    'name': 'markdown_artifact',\n                    'type': 'system.Markdown'\n                }],\n            },\n            'state': Execution.State.COMPLETE,\n            'type': 'system.ContainerExecution'\n        }, markdown_visualization.get_dict())\n\n\nrun_pipeline_func([\n    TestCase(\n        pipeline_func=metrics_visualization_pipeline,\n        verify_func=verify,\n        mode=kfp.dsl.PipelineExecutionMode.V2_ENGINE),\n])\n"
  },
  {
    "repo": "xxxtrillionarie/KubeFlow_MLOps_Pipelines",
    "file_path": "samples/test/parameter_with_format.py",
    "raw_url": "https://raw.githubusercontent.com/xxxtrillionarie/KubeFlow_MLOps_Pipelines/master/samples/test/parameter_with_format.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom kfp.deprecated import components\nfrom kfp.deprecated import dsl\n\n\n@components.create_component_from_func\ndef print_op(name: str) -> str:\n    print(name)\n    return name\n\n\n@dsl.pipeline(name='pipeline-with-pipelineparam-containing-format')\ndef my_pipeline(name: str = 'KFP'):\n    print_task = print_op('Hello {}'.format(name))\n    print_op('{}, again.'.format(print_task.output))\n"
  },
  {
    "repo": "xxxtrillionarie/KubeFlow_MLOps_Pipelines",
    "file_path": "samples/test/parameter_with_format_test.py",
    "raw_url": "https://raw.githubusercontent.com/xxxtrillionarie/KubeFlow_MLOps_Pipelines/master/samples/test/parameter_with_format_test.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp.deprecated as kfp\nfrom .parameter_with_format import my_pipeline\nfrom kfp.samples.test.utils import run_pipeline_func, TestCase\n\n\ndef verify(run, run_id: str, **kwargs):\n    assert run.status == 'Succeeded'\n    # TODO(Bobgy): verify output\n\n\nrun_pipeline_func([\n    # Cannot test V2_ENGINE and V1_LEGACY using the same code.\n    # V2_ENGINE requires importing everything from v2 namespace.\n    # TestCase(\n    #     pipeline_func=my_pipeline,\n    #     verify_func=verify,\n    #     mode=kfp.dsl.PipelineExecutionMode.V2_ENGINE,\n    # ),\n    TestCase(\n        pipeline_func=my_pipeline,\n        mode=kfp.dsl.PipelineExecutionMode.V1_LEGACY,\n    ),\n])\n"
  },
  {
    "repo": "mengdong/kubeflow-pipeline-nvidia-example",
    "file_path": "kubeflow-tf/JoC_end2end_serve.py",
    "raw_url": "https://raw.githubusercontent.com/mengdong/kubeflow-pipeline-nvidia-example/master/kubeflow-tf/JoC_end2end_serve.py",
    "content": "# Copyright 2018 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport kfp.dsl as dsl\nimport kfp.gcp as gcp\nfrom kubernetes import client as k8s_client\n\n\n@dsl.pipeline(\n  name='End2end Resnet50 Classification',\n  description='END to END kubeflow demo with TensorRT Inference Server, TF-AMP and TensorRT'\n)\ndef end2end_demo(  # pylint: disable=unused-argument\n    num_iter: dsl.PipelineParam = dsl.PipelineParam(name='num_iter', value=20),\n    batch_size: dsl.PipelineParam = dsl.PipelineParam(name='batch_size', value=1024),\n    use_tf_amp: dsl.PipelineParam = dsl.PipelineParam(name='use_tf_amp', value=1),\n    use_auto_loss_scaling: dsl.PipelineParam = dsl.PipelineParam(name='use_auto_loss_scaling', value=1),\n    trtserver_name: dsl.PipelineParam = dsl.PipelineParam(name='trtserver_name', value='trtserver'),\n    model_name: dsl.PipelineParam = dsl.PipelineParam(name='model_name', value='resnet_graphdef'),\n    model_version: dsl.PipelineParam = dsl.PipelineParam(name='model_version', value='1'),\n    webapp_prefix: dsl.PipelineParam = dsl.PipelineParam(name='webapp_prefix', value='webapp'),\n    webapp_port: dsl.PipelineParam = dsl.PipelineParam(name='webapp_port', value='80'),\n    storage_bucket: dsl.PipelineParam = dsl.PipelineParam(name='storage_bucket', value='gs://dongm-kubeflow'),\n    ckpt_dir: dsl.PipelineParam = dsl.PipelineParam(name='ckpt_dir', value='ckpt'),\n    mount_dir: dsl.PipelineParam = dsl.PipelineParam(name='mount_dir', value='/mnt/vol'),\n    model_dir: dsl.PipelineParam = dsl.PipelineParam(name='model_dir', value='model_repository'),\n    raw_data_dir: dsl.PipelineParam = dsl.PipelineParam(name='raw_data_dir', value='raw_data'),\n    processed_data_dir: dsl.PipelineParam = dsl.PipelineParam(name='processed_data_dir', value='processed_data')\n):\n  project_name = 'nvidia-sa-org'\n\n  preprocessing = dsl.ContainerOp(\n    name='preprocessing',\n    image='gcr.io/' + project_name + '/gcp-joc-end2end-demo-preprocessing',\n    command=['python'],\n    arguments=[\n      'download.py',\n      '--data_dir', '%s/%s' % (mount_dir, raw_data_dir)\n    ],\n    file_outputs={}\n  ).set_gpu_limit(1).add_volume(k8s_client.V1Volume(name='my-rw-pv',\n                                                    persistent_volume_claim=k8s_client.V1PersistentVolumeClaimVolumeSource(\n                                                      claim_name='my-rw-pvc'))\n                                ).add_volume_mount(k8s_client.V1VolumeMount(mount_path='/mnt/vol', name='my-rw-pv'))\n\n  training = dsl.ContainerOp(\n    name='training',\n    image='gcr.io/' + project_name + '/gcp-joc-end2end-demo-training',\n    command=['python'],\n    arguments=[\n      'main.py',\n      '--num_iter', '%s' % (num_iter),\n      '--iter_unit', 'epoch',\n      '--data_dir', '%s/%s/cifar-10-batches-bin' % (mount_dir, raw_data_dir),\n      '--batch_size', '%s' % (batch_size),\n      '--results_dir', '%s/%s' % (mount_dir, ckpt_dir),\n      '--use_tf_amp', '%s' % (use_tf_amp),\n      '--use_auto_loss_scaling', '%s' % (use_auto_loss_scaling),\n      '--storage_bucket_dir', '%s' % (storage_bucket),\n      '--model_dir', '%s/%s' % (mount_dir, model_name)\n    ],\n    file_outputs={}\n  ).set_gpu_limit(1).add_volume(k8s_client.V1Volume(name='my-rw-pv',\n                                                    persistent_volume_claim=k8s_client.V1PersistentVolumeClaimVolumeSource(\n                                                      claim_name='my-rwm-pvc'))\n                                ).add_volume_mount(k8s_client.V1VolumeMount(mount_path='/mnt/vol', name='my-rw-pv'))\n\n  serve = dsl.ContainerOp(\n    name='serve',\n    image='gcr.io/' + project_name + '/ml-pipeline-kubeflow-trtisserve',\n    arguments=[\"--trtserver_name\", trtserver_name,\n               '--model_version', model_version,\n               '--orig_model_path', '%s/%s' % (mount_dir, model_name),\n               \"--model_path\", '%s/%s' % (storage_bucket, model_dir)\n               ]\n  ).add_volume(k8s_client.V1Volume(name='my-rw-pv',\n                                   persistent_volume_claim=k8s_client.V1PersistentVolumeClaimVolumeSource(\n                                     claim_name='my-rwm-pvc'))\n               ).add_volume_mount(k8s_client.V1VolumeMount(mount_path='/mnt/vol', name='my-rw-pv')).apply(gcp.use_gcp_secret('user-nvidia-sa'))\n\n  webapp = dsl.ContainerOp(\n    name='webapp',\n    image='gcr.io/' + project_name + '/ml-pipeline-trtis-webapp-launcher',\n    arguments=[\"--workflow_name\", '%s' % ('{{workflow.name}}',),\n               \"--trtserver_name\", trtserver_name,\n               \"--model_name\", model_name,\n               \"--model_version\", model_version,\n               \"--webapp_prefix\", webapp_prefix,\n               \"--webapp_port\", webapp_port\n               ]\n  )\n\n  training.after(preprocessing)\n  serve.after(training)\n  webapp.after(serve)\n\n\nif __name__ == '__main__':\n  import kfp.compiler as compiler\n\n  compiler.Compiler().compile(end2end_demo, __file__ + '.tar.gz')\n"
  },
  {
    "repo": "mengdong/kubeflow-pipeline-nvidia-example",
    "file_path": "kubeflow-tf/end2end_serve.py",
    "raw_url": "https://raw.githubusercontent.com/mengdong/kubeflow-pipeline-nvidia-example/master/kubeflow-tf/end2end_serve.py",
    "content": "# Copyright 2018 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport kfp.dsl as dsl\nimport kfp.gcp as gcp\nfrom kubernetes import client as k8s_client\n\n@dsl.pipeline(\n  name='End2end Resnet50 Classification',\n  description='END to END kubeflow demo with TensorRT Inference Server, TF-AMP and TensorRT'\n)\ndef end2end_demo(  # pylint: disable=unused-argument\n    trtserver_name: dsl.PipelineParam = dsl.PipelineParam(name='trtserver_name', value='trtserver'),\n    model_name: dsl.PipelineParam = dsl.PipelineParam(name='model_name', value='resnet_graphdef'),\n    model_version: dsl.PipelineParam = dsl.PipelineParam(name='model_version', value='1'),\n    num_iter: dsl.PipelineParam = dsl.PipelineParam(name='num_iter', value=10),\n    batch_size: dsl.PipelineParam = dsl.PipelineParam(name='batch_size', value=128),\n    webapp_prefix: dsl.PipelineParam = dsl.PipelineParam(name='webapp_prefix', value='webapp'),\n    webapp_port: dsl.PipelineParam = dsl.PipelineParam(name='webapp_port', value='80'),\n    storage_bucket: dsl.PipelineParam = dsl.PipelineParam(name='storage_bucket', value='gs://dongm-kubeflow'),\n    ckpt_dir: dsl.PipelineParam = dsl.PipelineParam(name='ckpt_dir', value='ckpt'),\n    mount_dir: dsl.PipelineParam = dsl.PipelineParam(name='mount_dir', value='/mnt/vol'),\n    model_dir: dsl.PipelineParam = dsl.PipelineParam(name='model_dir', value='test_model_repository'),\n    raw_data_dir: dsl.PipelineParam = dsl.PipelineParam(name='raw_data_dir', value='raw_data'),\n    processed_data_dir: dsl.PipelineParam = dsl.PipelineParam(name='processed_data_dir', value='processed_data')\n):\n  project_name = 'nvidia-sa-org'\n\n  preprocessing = dsl.ContainerOp(\n    name='preprocessing',\n    image='gcr.io/' + project_name + '/gcp-end2end-demo-preprocessing',\n    command=['python'],\n    arguments=[\n      '/scripts/preprocess.py',\n      '--input_dir', '%s/%s' % (mount_dir, raw_data_dir),\n      '--output_dir', '%s/%s' % (mount_dir, processed_data_dir)\n    ],\n    file_outputs={}\n  ).add_volume(k8s_client.V1Volume(name='my-rw-pv',\n                                   persistent_volume_claim=k8s_client.V1PersistentVolumeClaimVolumeSource(\n                                     claim_name='my-rw-pvc'))\n               ).add_volume_mount(k8s_client.V1VolumeMount(mount_path='/mnt/vol', name='my-rw-pv')).set_gpu_limit(1)\n\n  training = dsl.ContainerOp(\n    name='training',\n    image='gcr.io/' + project_name + '/gcp-end2end-demo-training',\n    command=['python'],\n    arguments=[\n      '/scripts/train.py',\n      '--model_version', str(model_version),\n      '--input_dir', '%s/%s' % (mount_dir, processed_data_dir),\n      '--ckpt_dir', '%s/%s' % (mount_dir, ckpt_dir),\n      '--output_dir', '%s/%s/resnet_graphdef' % (storage_bucket, model_dir),\n      '--num_iter', '%s' % (num_iter),\n      '--batch_size', '%s' % (batch_size)\n    ],\n    file_outputs={}\n  ).set_gpu_limit(1).add_volume(k8s_client.V1Volume(name='my-rw-pv',\n                                                    persistent_volume_claim=k8s_client.V1PersistentVolumeClaimVolumeSource(\n                                                      claim_name='my-rwm-pvc'))\n                                ).add_volume_mount(k8s_client.V1VolumeMount(mount_path='/mnt/vol', name='my-rw-pv')).apply(gcp.use_gcp_secret('user-nvidia-sa'))\n\n  serve = dsl.ContainerOp(\n    name='serve',\n    image='gcr.io/' + project_name + '/ml-pipeline-kubeflow-trtisserve',\n    arguments=[\"--trtserver_name\", trtserver_name,\n               '--model_version', model_version,\n               '--orig_model_path', '%s/%s' % (mount_dir, model_name),\n               \"--model_path\", '%s/%s' % (storage_bucket, model_dir)\n               ]\n  )\n\n  webapp = dsl.ContainerOp(\n    name='webapp',\n    image='gcr.io/' + project_name + '/ml-pipeline-trtis-webapp-launcher',\n    arguments=[\"--workflow_name\", '%s' % ('{{workflow.name}}',),\n               \"--trtserver_name\", trtserver_name,\n               \"--model_name\", model_name,\n               \"--model_version\", str(model_version),\n               \"--webapp_prefix\", webapp_prefix,\n               \"--webapp_port\", str(webapp_port)\n               ]\n  )\n\n  training.after(preprocessing)\n  serve.after(training)\n  webapp.after(serve)\n\n\nif __name__ == '__main__':\n  import kfp.compiler as compiler\n\n  compiler.Compiler().compile(end2end_demo, __file__ + '.tar.gz')\n"
  },
  {
    "repo": "sanghunmoon/pytorch_classifier_pipeline",
    "file_path": "pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/sanghunmoon/pytorch_classifier_pipeline/master/pipeline.py",
    "content": "# pipeline.py\n\nfrom kfp import dsl\n@dsl.pipeline(\n    name='pytorch_classifier_test_2',\n    description='basic python classifier',\n)\n\n# n_num\uc744 \ud568\uc218\uc758 \ub9e4\uac1c\ubcc0\uc218\ub85c \uc9c0\uc815\ndef mlflow_pipeline():\n  ml = dsl.ContainerOp(\n      name=\"training pipeline\",\n      image=\"lego0142/pytorch_classifier:1.1\",\n  )\n    \nif __name__ == \"__main__\":\n\n    import kfp.compiler as compiler\n\n\t\t# Pipeline \ud30c\uc77c \uc0dd\uc131\n    compiler.Compiler().compile(mlflow_pipeline, \"sh_pytorch_classifier.tar.gz\")"
  },
  {
    "repo": "tuhinaprasad28/Machine-Learning-Pipeline-using-Kubeflow",
    "file_path": "code_rep_localHost/Kubeflow_Pipelines-master/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/tuhinaprasad28/Machine-Learning-Pipeline-using-Kubeflow/main/code_rep_localHost/Kubeflow_Pipelines-master/pipeline.py",
    "content": "import kfp\nfrom kfp import dsl\nfrom kfp.components import func_to_container_op\n\n@func_to_container_op\ndef show_results(decision_tree : float, logistic_regression : float) -> None:\n    # Given the outputs from decision_tree and logistic regression components\n    # the results are shown.\n\n    print(f\"Decision tree (accuracy): {decision_tree}\")\n    print(f\"Logistic regression (accuracy): {logistic_regression}\")\n\n\n@dsl.pipeline(name='First Pipeline', description='Applies Decision Tree and Logistic Regression for classification problem.')\ndef first_pipeline():\n\n    # Loads the yaml manifest for each component\n    download = kfp.components.load_component_from_file('download_data/download_data.yaml')\n    decision_tree = kfp.components.load_component_from_file('decision_tree/decision_tree.yaml')\n    logistic_regression = kfp.components.load_component_from_file('logistic_regression/logistic_regression.yaml')\n\n    # Run download_data task\n    download_task = download()\n\n    # Run tasks \"decison_tree\" and \"logistic_regression\" given\n    # the output generated by \"download_task\".\n    decision_tree_task = decision_tree(download_task.output)\n    logistic_regression_task = logistic_regression(download_task.output)\n\n    # Given the outputs from \"decision_tree\" and \"logistic_regression\"\n    # the component \"show_results\" is called to print the results.\n    show_results(decision_tree_task.output, logistic_regression_task.output)\n\n\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(first_pipeline, 'FirstPipeline.yaml')\n    # kfp.Client().create_run_from_pipeline_func(basic_pipeline, arguments={})"
  },
  {
    "repo": "tuhinaprasad28/Machine-Learning-Pipeline-using-Kubeflow",
    "file_path": "code_rep_gcloud/Pipeline/Kubeflow_Pipelines-master/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/tuhinaprasad28/Machine-Learning-Pipeline-using-Kubeflow/main/code_rep_gcloud/Pipeline/Kubeflow_Pipelines-master/pipeline.py",
    "content": "import kfp\nfrom kfp import dsl\nfrom kfp.components import func_to_container_op\n\n@func_to_container_op\ndef show_results(decision_tree : float, logistic_regression : float) -> None:\n    # Given the outputs from decision_tree and logistic regression components\n    # the results are shown.\n\n    print(f\"Decision tree (accuracy): {decision_tree}\")\n    print(f\"Logistic regression (accuracy): {logistic_regression}\")\n\n\n@dsl.pipeline(name='First Pipeline', description='Applies Decision Tree and Logistic Regression for classification problem.')\ndef first_pipeline():\n\n    # Loads the yaml manifest for each component\n    download = kfp.components.load_component_from_file('download_data/download_data.yaml')\n    decision_tree = kfp.components.load_component_from_file('decision_tree/decision_tree.yaml')\n    logistic_regression = kfp.components.load_component_from_file('logistic_regression/logistic_regression.yaml')\n\n    # Run download_data task\n    download_task = download()\n\n    # Run tasks \"decison_tree\" and \"logistic_regression\" given\n    # the output generated by \"download_task\".\n    decision_tree_task = decision_tree(download_task.output)\n    logistic_regression_task = logistic_regression(download_task.output)\n\n    # Given the outputs from \"decision_tree\" and \"logistic_regression\"\n    # the component \"show_results\" is called to print the results.\n    show_results(decision_tree_task.output, logistic_regression_task.output)\n\n\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(first_pipeline, 'FirstPipeline.yaml')\n    # kfp.Client().create_run_from_pipeline_func(basic_pipeline, arguments={})"
  },
  {
    "repo": "nnkkmto/sample-vertex-pipelines",
    "file_path": "src/pipeline/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/nnkkmto/sample-vertex-pipelines/main/src/pipeline/pipeline.py",
    "content": "import os\nimport glob\nimport yaml\nimport kfp\nfrom kfp.v2.compiler import Compiler\nfrom kfp.v2.google.client import AIPlatformClient\n\n\nPROJECT_ID = os.environ[\"PROJECT_ID\"]\nREGION = \"asia-east1\"\nSERVICE_ACCOUNT = \"your-service-account@{project_id}.iam.gserviceaccount.com\"\nPIPELINE_ROOT = \"gs://your-bucket/pipeline_root\"\n\n# define pipeline\n@kfp.dsl.pipeline(name=\"sample-pipeline\", description=\"sample pipeline\")\ndef pipeline(sentence: str):\n    components_store = kfp.components.ComponentStore(local_search_paths=[\"component\"])\n\n    split_sentence_op = components_store.load_component('split_sentence')\n    split_sentence = split_sentence_op(sentence=sentence)\n\n    with kfp.dsl.ParallelFor(split_sentence.outputs[\"words\"]) as word:\n        print_word_op = components_store.load_component('print_word')\n        print_word = print_word_op(word=word)\n\n\ndef override_components(project_id):\n    component_files = glob.glob('component/*/component.yaml')\n    for component_file in component_files:\n        with open(component_file, 'r') as f:\n            data = yaml.safe_load(f)\n        data[\"implementation\"][\"container\"][\"image\"] = data[\"implementation\"][\"container\"][\"image\"].format(\n            project_id=project_id)\n        with open(component_file, \"w\") as f:\n            yaml.dump(data, f)\n\n\noverride_components(PROJECT_ID)\n\n# compile pipeline\nCompiler().compile(\n    pipeline_func=pipeline,\n    package_path=\"pipeline.json\"\n)\n\napi_client = AIPlatformClient(project_id=PROJECT_ID, region=REGION)\n_ = api_client.create_run_from_job_spec(\n    \"pipeline.json\",\n    pipeline_root=PIPELINE_ROOT,\n    service_account=SERVICE_ACCOUNT.format(project_id=PROJECT_ID),\n    enable_caching=True,\n    parameter_values={\n        \"sentence\": \"Hello Vertex Pipelines\"\n    }\n)"
  },
  {
    "repo": "butuzov/kubeflow-pipline-pytorch-tacatron",
    "file_path": "pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/butuzov/kubeflow-pipline-pytorch-tacatron/master/pipeline.py",
    "content": "\nfrom kfp import dsl\nimport os\n\nfrom random import choice\nfrom string import ascii_letters as letters\n\n\n__PIPELINE_NAME__ = \"example\"\n__PIPELINE_DESCRIPTION__ = \"demo of kubeflow pipeline\"\n\n\n@dsl.pipeline(name=__PIPELINE_NAME__, description=__PIPELINE_DESCRIPTION__)\ndef pipeline(\n        dataset=\"https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\",\n        directory=\"/mnt/kf/\",\n        batch_size=16,\n        learning_rate=0.001,\n        log_step=1,\n        save_step=1,\n        epochs=1,\n    ):\n\n    OWNER   = os.environ['OWNER']\n    VERSION = os.environ['KF_PIPELINE_VERSION']\n\n\n    volume = dsl.VolumeOp(\n        name=\"volume_creation\",\n        resource_name=\"share\",\n        size=\"20Gi\"\n    )\n\n    Dataset_Download = dsl.ContainerOp(\n        name=\"dataset download\",\n        image=f\"{OWNER}/kf-dataset:{VERSION}\",\n        arguments=[\n            f\"--url={dataset}\",\n            f\"--directory={directory}\"\n        ],\n        pvolumes={\n            f\"{directory}\" : volume.volume\n        },\n    )\n\n\n    Training = dsl.ContainerOp(\n        name=\"training model\",\n        image=f\"{OWNER}/kf-training:{VERSION}\",\n        arguments=[\n            f\"--dir_data={directory}/dataset\",\n            f\"--dir_checkpoints={directory}/models\",\n            f\"--batch_size={batch_size}\",\n            f\"--learning_rate={learning_rate}\",\n            f\"--log_step={log_step}\",\n            f\"--save_step={save_step}\",\n            f\"--epochs={epochs}\",\n        ],\n        pvolumes={\n            f\"{directory}\" : volume.volume\n        },\n    )\n\n    Training.after(Dataset_Download)\n\n    Seving = dsl.ContainerOp(\n        name=\"serving\",\n        image=f\"{OWNER}/kf-webapp:{VERSION}\",\n        arguments=[\n            f\"--result={directory}/results\",\n            f\"--directory={directory}/models\",\n            f\"--model=model.pth.tar\",\n        ],\n        pvolumes={\n            f\"{directory}\" : volume.volume\n        },\n    )\n\n    Seving.after(Training)\n\n\n\nif __name__ == \"__main__\":\n    import kfp.compiler as compiler\n\n    compiler.Compiler().compile(pipeline,\n        f\"pipeline-{os.environ['KF_PIPELINE_VERSION']}.tar.gz\")\n"
  },
  {
    "repo": "hermesribeiro/kubeflow_pytorch",
    "file_path": "pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/hermesribeiro/kubeflow_pytorch/main/pipeline.py",
    "content": "from kfp import dsl\nfrom kfp.aws import use_aws_secret\n\nfrom helpers.kfp_auth import KfpAuth\n\n\n@dsl.pipeline(name=\"CIFAR Pytorch\", description=\"hello world\")\ndef cifar_pipeline(\n    num_epochs: int,\n    batch_size: int,\n    learning_rate: float,\n    momentum: float,\n    bucket: str,\n    path: str,\n):\n    train_op = (\n        dsl.ContainerOp(\n            name=\"Model Train\",\n            image=\"hermesribeiro/cifar:latest\",\n            command=[\"python\", \"train.py\"],\n            arguments=[\n                \"-n\",\n                num_epochs,\n                \"-b\",\n                batch_size,\n                \"-l\",\n                learning_rate,\n                \"-m\",\n                momentum,\n                \"-u\",\n                bucket,\n                \"-p\",\n                path,\n            ],\n        )\n        .apply(use_aws_secret())\n        .set_image_pull_policy(\"Always\")\n        .set_memory_request('2G')\n        .set_cpu_request('4')\n        # .set_gpu_limit('1', 'nvidia')\n        # .add_volume_mount(...)\n        # .add_env_variable(V1EnvVar(name='HOST', value='foo.bar'))\n        # .set_retry(10)\n    )\n\n    eval_op = (\n        dsl.ContainerOp(\n            name=\"Model Eval\",\n            image=\"hermesribeiro/cifar:latest\",\n            command=[\"python\", \"eval.py\"],\n            arguments=[\"-u\", bucket, \"-p\", path],\n        )\n        .apply(use_aws_secret())\n        .set_image_pull_policy(\"Always\")\n        .after(train_op)\n    )\n\n\nif __name__ == \"__main__\":\n    client = KfpAuth().client()\n    client.create_run_from_pipeline_func(\n        cifar_pipeline,\n        arguments={\n            \"num_epochs\": 2,\n            \"batch_size\": 4,\n            \"learning_rate\": 0.001,\n            \"momentum\": 0.0,\n            \"bucket\": \"hermes-freestyle\",\n            \"path\": \"cifar/cifar_net.pth\",\n        }\n    )\n"
  },
  {
    "repo": "litovn/kubeflow-autopipe",
    "file_path": "src/pipeline_manager.py",
    "raw_url": "https://raw.githubusercontent.com/litovn/kubeflow-autopipe/main/src/pipeline_manager.py",
    "content": "import os\nimport time\nimport yaml\nimport subprocess\nimport logging\nimport argparse\nfrom dotenv import load_dotenv\n\nfrom kfp import dsl\nfrom kfp.kubernetes import mount_pvc\n\nfrom kube.pvc_manager import *\nfrom kube.pipeline_run import *\n\n\n# Configure logging to display information based on your needs\nlogging.basicConfig(level=logging.INFO, format='[%(asctime)s] - %(message)s', datefmt='%H:%M:%S')\n\n\ndef load_dag_configuration(dag_path):\n    \"\"\"\n    Load the yaml dag configuration file, to extract the required information\n\n    :param dag_path: The file path to the YAML configuration file\n    :return: A tuple containing lists of components, dependencies, and the initial input media file path\n    \"\"\"\n    with open(dag_path, 'r') as file:\n        data = yaml.safe_load(file)\n        return data['System']['components'], data['System']['dependencies'], data['System']['input_media']\n\n\n# With download_from_pvc method defined in pvc_manager.py, it might be possible to search for the output file path\n# saved by the previous component (independently of its name) in the PVC and download it to the local machine, then\n# use it as the input for the next component.\n# -----\n# This method is not used, because it would imply to create an updated pod after each component by overwriting the\n# previous one, which is not an optimal approach.\n# -----\n# instead we consider that the component will save their output file as: output_name + \".tar.gz\"\n# where output_name = os.path.basename(output_path_dir)\n\"\"\"\ndef get_output_filepath(output_dir: str):\n    files = os.listdir(output_dir)\n    if not files:\n        logging.error(f\"No files found in directory: {output_dir}\")\n    file_path = os.path.join(output_dir, files[0])\n    return file_path\n\"\"\"\n\n\ndef create_component(username: str, component_name: str):\n    \"\"\"\n    Dynamically create a reusable container component for the Kubeflow Pipeline steps\n\n    :param username: Docker username to prefix to the Docker image name\n    :param component_name: Name of the component, used to generate the function name and specify the Docker image\n    \"\"\"\n    comp_name = component_name.replace('-', '_')\n\n    # Default kfp.container_component configuration\n    component_code = f\"\"\"\n@dsl.container_component\ndef {comp_name}(input_path: str, output_path: str):\n    return dsl.ContainerSpec(\n        image=f'{username}/{component_name}:latest',\n        command=['python', 'main.py'],\n        args=[\n            '-i', input_path, '-o', output_path\n        ]\n    )\n    \"\"\"\n    # Execute the generated code to define the component function dynamically\n    exec(component_code, globals())\n\n\ndef setup_component(component_name: str, input_path: str, output_dir: str, pvc_name: str):\n    \"\"\"\n    Set up a component for the pipeline with various configurations (including input and output paths, and PVC mounting),\n    by retrieving the dynamically created component function.\n    If needed, the method can be extended to include more configurations based on the Kubeflow pipeline requirements.\n\n    :param name: Name of the component to be included in the pipeline\n    :param input_path: Path to the input file for the component\n    :param output_dir: Output directory for the component's results\n    :param pvc_name: Name of the Persistent Volume Claim (PVC) to be mounted\n    :return: Configured component operation for the pipeline\n    \"\"\"\n    comp_func = globals().get(component_name.replace('-', '_'))\n    component_op = comp_func(input_path=input_path, output_path=output_dir)\n    component_op = mount_pvc(component_op, pvc_name=pvc_name, mount_path='/mnt/data')\n    # Caching can be enabled or disabled here for a specific component, if needed.\n    # component_op.set_caching_options(False)\n    return component_op\n\n\ndef generate_pipeline(username: str, dag_components: list, dag_dependencies: list, init_input: str):\n    \"\"\"\n    Dynamically generate a Kubeflow Pipeline based on the DAG configuration. This involves creating container\n    components for each step in the pipeline and setting up their execution order based on dependencies.\n\n    :param username: Docker username for Docker image naming\n    :param dag_components: List of components defined in the DAG configuration file\n    :param dag_dependencies: List of dependencies defined in the DAG configuration file\n    :param init_input: Name of the initial input media file path\n    :return: The Kubeflow Pipeline function\n    \"\"\"\n    for component in dag_components:\n        create_component(username, component)\n    create_component(username, 'save-media')\n\n    @dsl.pipeline(\n        name=\"Kubeflow Autopipe\",\n        description=\"Automatically generated pipeline based on the provided configuration file\"\n    )\n    def dynamic_pipeline(pvc_name: str):\n        base_mount = \"/mnt/data\"\n        component_op = {}\n        input_path = init_input\n\n        # Set up the save_media component as first component\n        output_dir = f\"{base_mount}/\"\n        component_op['save-media'] = setup_component('save-media', input_path, output_dir, pvc_name)\n\n        # Set up the other components based on dependencies\n        for dependency in dag_dependencies:\n            this_component, next_component, _ = dependency\n\n            if this_component not in component_op:\n                output_dir_this = f\"{base_mount}/{this_component}\"\n                input_path = f\"{base_mount}/{init_input}\"\n                component_op[this_component] = setup_component(this_component, input_path, output_dir_this, pvc_name)\n                component_op[this_component].after(component_op['save-media'])\n\n            if next_component not in component_op:\n                output_dir_next = f\"{base_mount}/{next_component}\"\n                input_path = f\"{base_mount}/{this_component}.tar.gz\"\n                component_op[next_component] = setup_component(next_component, input_path, output_dir_next, pvc_name)\n                component_op[next_component].after(component_op[this_component])\n\n    return dynamic_pipeline\n\n\ndef main(input_file: str):\n    # Define the local path to store the outputs saved into the pvc\n    local_path = 'output'\n    # Save need data from the configuration file\n    dag_components, dag_dependencies, media = load_dag_configuration(input_file)\n\n    # Save register_username defined in the .env file\n    load_dotenv()\n    register_username = os.getenv('register_USERNAME')\n\n    # Create the PVC for the pipeline\n    pvc_name = create_pvc()\n\n    # Save the name of the file to be used as input for the pipeline\n    input_filename = os.path.basename(media)\n\n    # Generate the pipeline function\n    pipeline_func = generate_pipeline(username=register_username, dag_components=dag_components, dag_dependencies=dag_dependencies, init_input=input_filename)\n    pipeline_filename = 'pipeline.yaml'\n    # Execute the pipeline\n    pipeline_run(pvc_name, pipeline_func, pipeline_filename)\n    time.sleep(5)\n\n    # Download the output file from the PVC to the local machine\n    download_from_pvc(pvc_name, local_path)\n    # Delete the PVC after the pipeline execution\n    delete_pvc(pvc_name)\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"-i\", \"--input\", required=True, help=\"path to application_dag.yaml configuration file\")\n    args = vars(parser.parse_args())\n\n    main(args['input'])\n"
  },
  {
    "repo": "speg03/kfp-toolbox",
    "file_path": "tests/test_cli.py",
    "raw_url": "https://raw.githubusercontent.com/speg03/kfp-toolbox/main/tests/test_cli.py",
    "content": "import os\nfrom pathlib import PosixPath\nfrom unittest.mock import patch\n\nfrom kfp.v2 import compiler, dsl\nfrom typer.testing import CliRunner\n\nfrom kfp_toolbox import __version__\nfrom kfp_toolbox.cli import app\n\nrunner = CliRunner()\n\n\nclass TestApp:\n    def test_no_arguments(self):\n        result = runner.invoke(app)\n\n        assert result.exit_code != 0\n        assert result.output.startswith(\"Usage: \")\n\n    def test_help(self):\n        result = runner.invoke(app, [\"--help\"])\n\n        assert result.exit_code == 0\n        assert result.output.startswith(\"Usage: \")\n\n    def test_version(self):\n        result = runner.invoke(app, [\"--version\"])\n\n        assert result.exit_code == 0\n        assert result.output == f\"{__version__}\\n\"\n\n\nclass TestSubmit:\n    @patch(\"kfp_toolbox.pipeline_jobs.submit_pipeline_job\")\n    def test(self, mock_submit_pipeline_job, tmp_path):\n        @dsl.component()\n        def echo() -> str:\n            return \"hello, world\"\n\n        @dsl.pipeline(name=\"echo-pipeline\")\n        def echo_pipeline(param: int = 1, weird_param__: str = \"default_string\"):\n            echo()\n\n        pipeline_path = os.fspath(tmp_path / \"pipeline.json\")\n        compiler.Compiler().compile(\n            pipeline_func=echo_pipeline, package_path=pipeline_path\n        )\n\n        result = runner.invoke(\n            app,\n            [\n                \"submit\",\n                f\"--pipeline-file={pipeline_path}\",\n                \"--endpoint=http://localhost:8080\",\n                \"--label=project:hello\",\n                \"--label=test:true\",\n                \"--\",\n                \"--param=5\",\n                \"--weird-param=passed_value\",\n            ],\n        )\n\n        assert result.exit_code == 0\n        mock_submit_pipeline_job.assert_called_once_with(\n            pipeline_file=PosixPath(pipeline_path),\n            endpoint=\"http://localhost:8080\",\n            iap_client_id=None,\n            api_namespace=\"kubeflow\",\n            other_client_id=None,\n            other_client_secret=None,\n            arguments={\"param\": 5, \"weird_param__\": \"passed_value\"},\n            run_name=None,\n            experiment_name=None,\n            namespace=None,\n            pipeline_root=None,\n            enable_caching=None,\n            service_account=None,\n            encryption_spec_key_name=None,\n            labels={\"project\": \"hello\", \"test\": \"true\"},\n            project=None,\n            location=None,\n            network=None,\n        )\n\n    @patch(\"kfp_toolbox.pipeline_jobs.submit_pipeline_job\")\n    def test_caching(self, mock_submit_pipeline_job, tmp_path):\n        @dsl.component()\n        def echo() -> str:\n            return \"hello, world\"\n\n        @dsl.pipeline(name=\"echo-pipeline\")\n        def echo_pipeline():\n            echo()\n\n        pipeline_path = os.fspath(tmp_path / \"pipeline.json\")\n        compiler.Compiler().compile(\n            pipeline_func=echo_pipeline, package_path=pipeline_path\n        )\n\n        result = runner.invoke(\n            app, [\"submit\", f\"--pipeline-file={pipeline_path}\", \"--caching\"]\n        )\n\n        assert result.exit_code == 0\n        mock_submit_pipeline_job.assert_called_once_with(\n            pipeline_file=PosixPath(pipeline_path),\n            endpoint=None,\n            iap_client_id=None,\n            api_namespace=\"kubeflow\",\n            other_client_id=None,\n            other_client_secret=None,\n            arguments={},\n            run_name=None,\n            experiment_name=None,\n            namespace=None,\n            pipeline_root=None,\n            enable_caching=True,\n            service_account=None,\n            encryption_spec_key_name=None,\n            labels={},\n            project=None,\n            location=None,\n            network=None,\n        )\n\n    @patch(\"kfp_toolbox.pipeline_jobs.submit_pipeline_job\")\n    def test_no_caching(self, mock_submit_pipeline_job, tmp_path):\n        @dsl.component()\n        def echo() -> str:\n            return \"hello, world\"\n\n        @dsl.pipeline(name=\"echo-pipeline\")\n        def echo_pipeline():\n            echo()\n\n        pipeline_path = os.fspath(tmp_path / \"pipeline.json\")\n        compiler.Compiler().compile(\n            pipeline_func=echo_pipeline, package_path=pipeline_path\n        )\n\n        result = runner.invoke(\n            app, [\"submit\", f\"--pipeline-file={pipeline_path}\", \"--no-caching\"]\n        )\n\n        assert result.exit_code == 0\n        mock_submit_pipeline_job.assert_called_once_with(\n            pipeline_file=PosixPath(pipeline_path),\n            endpoint=None,\n            iap_client_id=None,\n            api_namespace=\"kubeflow\",\n            other_client_id=None,\n            other_client_secret=None,\n            arguments={},\n            run_name=None,\n            experiment_name=None,\n            namespace=None,\n            pipeline_root=None,\n            enable_caching=False,\n            service_account=None,\n            encryption_spec_key_name=None,\n            labels={},\n            project=None,\n            location=None,\n            network=None,\n        )\n\n    def test_no_pipelie_files(self):\n        result = runner.invoke(app, [\"submit\"])\n\n        assert result.exit_code != 0\n        assert \"Error: The --pipeline-file option must be specified.\" in result.output\n\n    def test_invalid_label(self, tmp_path):\n        @dsl.component()\n        def echo() -> str:\n            return \"hello, world\"\n\n        @dsl.pipeline(name=\"echo-pipeline\")\n        def echo_pipeline():\n            echo()\n\n        pipeline_path = os.fspath(tmp_path / \"pipeline.json\")\n        compiler.Compiler().compile(\n            pipeline_func=echo_pipeline, package_path=pipeline_path\n        )\n\n        result = runner.invoke(\n            app, [\"submit\", f\"--pipeline-file={pipeline_path}\", \"--label=invalid_label\"]\n        )\n\n        assert result.exit_code != 0\n        assert (\n            \"Error: The --label value must be contained a colon.: invalid_label\"\n            in result.output\n        )\n\n    def test_help(self):\n        result = runner.invoke(app, [\"submit\", \"--help\"])\n\n        assert result.exit_code == 0\n        assert result.output.startswith(\"Usage: \")\n"
  },
  {
    "repo": "speg03/kfp-toolbox",
    "file_path": "tests/test_decorators.py",
    "raw_url": "https://raw.githubusercontent.com/speg03/kfp-toolbox/main/tests/test_decorators.py",
    "content": "import os\n\nimport yaml\nfrom kfp import compiler as compiler_v1\nfrom kfp import dsl as dsl_v1\nfrom kfp.v2 import compiler, dsl\n\nfrom kfp_toolbox.decorators import (\n    caching,\n    container_spec,\n    display_name,\n    override_docstring,\n    spec,\n)\n\n\nclass TestOverrideDocstring:\n    def test_no_decorators(self):\n        @dsl.component()\n        def echo() -> str:\n            \"\"\"Say hello\n\n            This component just says hello.\n\n            Returns:\n                str: hello\n            \"\"\"\n            return \"hello, world\"\n\n        assert echo.__doc__ == \"Echo\\nSay hello\"\n\n    def test_no_arguments(self):\n        @override_docstring()\n        @dsl.component()\n        def echo() -> str:\n            \"\"\"Say hello\n\n            This component just says hello.\n\n            Returns:\n                str: hello\n            \"\"\"\n            return \"hello, world\"\n\n        assert echo.__doc__ == (\n            \"Say hello\\n\\n            This component just says hello.\\n\\n\"\n            \"            Returns:\\n                str: hello\\n            \"\n        )\n\n    def test_specific_docs(self):\n        @override_docstring(\"Just say hello component\")\n        @dsl.component()\n        def echo() -> str:\n            \"\"\"Say hello\n\n            This component just says hello.\n\n            Returns:\n                str: hello\n            \"\"\"\n            return \"hello, world\"\n\n        assert echo.__doc__ == \"Just say hello component\"\n\n\nclass TestSpec:\n    def test_as_decorator_v1(self, tmp_path):\n        @spec(\n            name=\"Echo Component\",\n            cpu=\"2\",\n            memory=\"16G\",\n            gpu=\"1\",\n            accelerator=\"NVIDIA_TESLA_T4\",\n            caching=True,\n        )\n        @dsl.component()\n        def echo() -> str:\n            return \"hello, world\"\n\n        @dsl.pipeline(name=\"echo-pipeline\")\n        def echo_pipeline():\n            echo()\n\n        pipeline_path = os.fspath(tmp_path / \"pipeline.yaml\")\n        compiler_v1.Compiler(mode=dsl_v1.PipelineExecutionMode.V2_COMPATIBLE).compile(\n            pipeline_func=echo_pipeline, package_path=pipeline_path\n        )\n\n        with open(pipeline_path, \"r\") as f:\n            pipeline_spec = yaml.safe_load(f)\n\n        task = next(\n            t for t in pipeline_spec[\"spec\"][\"templates\"] if t[\"name\"] == \"echo\"\n        )\n        resource_limit = task[\"container\"][\"resources\"][\"limits\"]\n        node_selector = task[\"nodeSelector\"]\n        annotations = task[\"metadata\"][\"annotations\"]\n        labels = task[\"metadata\"][\"labels\"]\n\n        assert resource_limit[\"cpu\"] == \"2\"\n        assert resource_limit[\"memory\"] == \"16G\"\n        assert resource_limit[\"nvidia.com/gpu\"] == \"1\"\n        assert node_selector[\"cloud.google.com/gke-accelerator\"] == \"NVIDIA_TESLA_T4\"\n        assert (\n            annotations[\"pipelines.kubeflow.org/task_display_name\"] == \"Echo Component\"\n        )\n        assert labels[\"pipelines.kubeflow.org/enable_caching\"] == \"true\"\n\n    def test_as_decorator(self, tmp_path):\n        @spec(\n            name=\"Echo Component\",\n            cpu=\"2\",\n            memory=\"16G\",\n            gpu=\"1\",\n            accelerator=\"NVIDIA_TESLA_T4\",\n            caching=True,\n        )\n        @dsl.component()\n        def echo() -> str:\n            return \"hello, world\"\n\n        @dsl.pipeline(name=\"echo-pipeline\")\n        def echo_pipeline():\n            echo()\n\n        pipeline_path = os.fspath(tmp_path / \"pipeline.json\")\n        compiler.Compiler().compile(\n            pipeline_func=echo_pipeline, package_path=pipeline_path\n        )\n\n        with open(pipeline_path, \"r\") as f:\n            pipeline_spec = yaml.safe_load(f)\n\n        container = pipeline_spec[\"pipelineSpec\"][\"deploymentSpec\"][\"executors\"][\n            \"exec-echo\"\n        ][\"container\"]\n        task = pipeline_spec[\"pipelineSpec\"][\"root\"][\"dag\"][\"tasks\"][\"echo\"]\n\n        assert container[\"resources\"][\"cpuLimit\"] == 2.0\n        assert container[\"resources\"][\"memoryLimit\"] == 16.0\n        assert container[\"resources\"][\"accelerator\"][\"count\"] == \"1\"\n        assert container[\"resources\"][\"accelerator\"][\"type\"] == \"NVIDIA_TESLA_T4\"\n        assert task[\"taskInfo\"][\"name\"] == \"Echo Component\"\n        assert task[\"cachingOptions\"][\"enableCache\"] is True\n\n    def test_as_function_v1(self, tmp_path):\n        @dsl.component()\n        def echo() -> str:\n            return \"hello, world\"\n\n        @dsl.pipeline(name=\"echo-pipeline\")\n        def echo_pipeline():\n            spec(\n                name=\"Echo Component\",\n                cpu=\"2\",\n                memory=\"16G\",\n                gpu=\"1\",\n                accelerator=\"NVIDIA_TESLA_T4\",\n                caching=True,\n            )(echo)()\n\n        pipeline_path = os.fspath(tmp_path / \"pipeline.yaml\")\n        compiler_v1.Compiler(mode=dsl_v1.PipelineExecutionMode.V2_COMPATIBLE).compile(\n            pipeline_func=echo_pipeline, package_path=pipeline_path\n        )\n\n        with open(pipeline_path, \"r\") as f:\n            pipeline_spec = yaml.safe_load(f)\n\n        task = next(\n            t for t in pipeline_spec[\"spec\"][\"templates\"] if t[\"name\"] == \"echo\"\n        )\n        resource_limit = task[\"container\"][\"resources\"][\"limits\"]\n        node_selector = task[\"nodeSelector\"]\n        annotations = task[\"metadata\"][\"annotations\"]\n        labels = task[\"metadata\"][\"labels\"]\n\n        assert resource_limit[\"cpu\"] == \"2\"\n        assert resource_limit[\"memory\"] == \"16G\"\n        assert resource_limit[\"nvidia.com/gpu\"] == \"1\"\n        assert node_selector[\"cloud.google.com/gke-accelerator\"] == \"NVIDIA_TESLA_T4\"\n        assert (\n            annotations[\"pipelines.kubeflow.org/task_display_name\"] == \"Echo Component\"\n        )\n        assert labels[\"pipelines.kubeflow.org/enable_caching\"] == \"true\"\n\n    def test_as_function(self, tmp_path):\n        @dsl.component()\n        def echo() -> str:\n            return \"hello, world\"\n\n        @dsl.pipeline(name=\"echo-pipeline\")\n        def echo_pipeline():\n            spec(\n                name=\"Echo Component\",\n                cpu=\"2\",\n                memory=\"16G\",\n                gpu=\"1\",\n                accelerator=\"NVIDIA_TESLA_T4\",\n                caching=True,\n            )(echo)()\n\n        pipeline_path = os.fspath(tmp_path / \"pipeline.json\")\n        compiler.Compiler().compile(\n            pipeline_func=echo_pipeline, package_path=pipeline_path\n        )\n\n        with open(pipeline_path, \"r\") as f:\n            pipeline_spec = yaml.safe_load(f)\n\n        container = pipeline_spec[\"pipelineSpec\"][\"deploymentSpec\"][\"executors\"][\n            \"exec-echo\"\n        ][\"container\"]\n        task = pipeline_spec[\"pipelineSpec\"][\"root\"][\"dag\"][\"tasks\"][\"echo\"]\n\n        assert container[\"resources\"][\"cpuLimit\"] == 2.0\n        assert container[\"resources\"][\"memoryLimit\"] == 16.0\n        assert container[\"resources\"][\"accelerator\"][\"count\"] == \"1\"\n        assert container[\"resources\"][\"accelerator\"][\"type\"] == \"NVIDIA_TESLA_T4\"\n        assert task[\"taskInfo\"][\"name\"] == \"Echo Component\"\n        assert task[\"cachingOptions\"][\"enableCache\"] is True\n\n    def test_disable_caching_v1(self, tmp_path):\n        @spec(caching=False)\n        @dsl.component()\n        def echo() -> str:\n            return \"hello, world\"\n\n        @dsl.pipeline(name=\"echo-pipeline\")\n        def echo_pipeline():\n            echo()\n\n        pipeline_path = os.fspath(tmp_path / \"pipeline.yaml\")\n        compiler_v1.Compiler(mode=dsl_v1.PipelineExecutionMode.V2_COMPATIBLE).compile(\n            pipeline_func=echo_pipeline, package_path=pipeline_path\n        )\n\n        with open(pipeline_path, \"r\") as f:\n            pipeline_spec = yaml.safe_load(f)\n\n        task = next(\n            t for t in pipeline_spec[\"spec\"][\"templates\"] if t[\"name\"] == \"echo\"\n        )\n        labels = task[\"metadata\"][\"labels\"]\n\n        assert labels[\"pipelines.kubeflow.org/enable_caching\"] == \"false\"\n\n    def test_disable_caching(self, tmp_path):\n        @spec(caching=False)\n        @dsl.component()\n        def echo() -> str:\n            return \"hello, world\"\n\n        @dsl.pipeline(name=\"echo-pipeline\")\n        def echo_pipeline():\n            echo()\n\n        pipeline_path = os.fspath(tmp_path / \"pipeline.json\")\n        compiler.Compiler().compile(\n            pipeline_func=echo_pipeline, package_path=pipeline_path\n        )\n\n        with open(pipeline_path, \"r\") as f:\n            pipeline_spec = yaml.safe_load(f)\n\n        task = pipeline_spec[\"pipelineSpec\"][\"root\"][\"dag\"][\"tasks\"][\"echo\"]\n\n        assert \"enableCache\" not in task[\"cachingOptions\"]\n\n    def test_multiple_decorators_v1(self, tmp_path):\n        default_spec = spec(cpu=\"2\", memory=\"16G\")\n\n        @spec(cpu=\"1\")\n        @default_spec\n        @dsl.component()\n        def echo() -> str:\n            return \"hello, world\"\n\n        @dsl.pipeline(name=\"echo-pipeline\")\n        def echo_pipeline():\n            echo()\n\n        pipeline_path = os.fspath(tmp_path / \"pipeline.yaml\")\n        compiler_v1.Compiler(mode=dsl_v1.PipelineExecutionMode.V2_COMPATIBLE).compile(\n            pipeline_func=echo_pipeline, package_path=pipeline_path\n        )\n\n        with open(pipeline_path, \"r\") as f:\n            pipeline_spec = yaml.safe_load(f)\n\n        task = next(\n            t for t in pipeline_spec[\"spec\"][\"templates\"] if t[\"name\"] == \"echo\"\n        )\n        resource_limit = task[\"container\"][\"resources\"][\"limits\"]\n\n        assert resource_limit[\"cpu\"] == \"1\"\n        assert resource_limit[\"memory\"] == \"16G\"\n\n    def test_multiple_decorators(self, tmp_path):\n        default_spec = spec(cpu=\"2\", memory=\"16G\")\n\n        @spec(cpu=\"1\")\n        @default_spec\n        @dsl.component()\n        def echo() -> str:\n            return \"hello, world\"\n\n        @dsl.pipeline(name=\"echo-pipeline\")\n        def echo_pipeline():\n            echo()\n\n        pipeline_path = os.fspath(tmp_path / \"pipeline.json\")\n        compiler.Compiler().compile(\n            pipeline_func=echo_pipeline, package_path=pipeline_path\n        )\n\n        with open(pipeline_path, \"r\") as f:\n            pipeline_spec = yaml.safe_load(f)\n\n        container = pipeline_spec[\"pipelineSpec\"][\"deploymentSpec\"][\"executors\"][\n            \"exec-echo\"\n        ][\"container\"]\n\n        assert container[\"resources\"][\"cpuLimit\"] == 1.0\n        assert container[\"resources\"][\"memoryLimit\"] == 16.0\n\n\nclass TestContainerSpec:\n    def test_as_decorator_v1(self, tmp_path):\n        @container_spec(\n            cpu=\"2\",\n            memory=\"16G\",\n            gpu=\"1\",\n            accelerator=\"NVIDIA_TESLA_T4\",\n        )\n        @dsl.component()\n        def echo() -> str:\n            return \"hello, world\"\n\n        @dsl.pipeline(name=\"echo-pipeline\")\n        def echo_pipeline():\n            echo()\n\n        pipeline_path = os.fspath(tmp_path / \"pipeline.yaml\")\n        compiler_v1.Compiler(mode=dsl_v1.PipelineExecutionMode.V2_COMPATIBLE).compile(\n            pipeline_func=echo_pipeline, package_path=pipeline_path\n        )\n\n        with open(pipeline_path, \"r\") as f:\n            pipeline_spec = yaml.safe_load(f)\n\n        task = next(\n            t for t in pipeline_spec[\"spec\"][\"templates\"] if t[\"name\"] == \"echo\"\n        )\n        resource_limit = task[\"container\"][\"resources\"][\"limits\"]\n        node_selector = task[\"nodeSelector\"]\n\n        assert resource_limit[\"cpu\"] == \"2\"\n        assert resource_limit[\"memory\"] == \"16G\"\n        assert resource_limit[\"nvidia.com/gpu\"] == \"1\"\n        assert node_selector[\"cloud.google.com/gke-accelerator\"] == \"NVIDIA_TESLA_T4\"\n\n    def test_as_decorator(self, tmp_path):\n        @container_spec(\n            cpu=\"2\",\n            memory=\"16G\",\n            gpu=\"1\",\n            accelerator=\"NVIDIA_TESLA_T4\",\n        )\n        @dsl.component()\n        def echo() -> str:\n            return \"hello, world\"\n\n        @dsl.pipeline(name=\"echo-pipeline\")\n        def echo_pipeline():\n            echo()\n\n        pipeline_path = os.fspath(tmp_path / \"pipeline.json\")\n        compiler.Compiler().compile(\n            pipeline_func=echo_pipeline, package_path=pipeline_path\n        )\n\n        with open(pipeline_path, \"r\") as f:\n            pipeline_spec = yaml.safe_load(f)\n\n        container = pipeline_spec[\"pipelineSpec\"][\"deploymentSpec\"][\"executors\"][\n            \"exec-echo\"\n        ][\"container\"]\n\n        assert container[\"resources\"][\"cpuLimit\"] == 2.0\n        assert container[\"resources\"][\"memoryLimit\"] == 16.0\n        assert container[\"resources\"][\"accelerator\"][\"count\"] == \"1\"\n        assert container[\"resources\"][\"accelerator\"][\"type\"] == \"NVIDIA_TESLA_T4\"\n\n    def test_as_function_v1(self, tmp_path):\n        @dsl.component()\n        def echo() -> str:\n            return \"hello, world\"\n\n        @dsl.pipeline(name=\"echo-pipeline\")\n        def echo_pipeline():\n            container_spec(\n                cpu=\"2\",\n                memory=\"16G\",\n                gpu=\"1\",\n                accelerator=\"NVIDIA_TESLA_T4\",\n            )(echo)()\n\n        pipeline_path = os.fspath(tmp_path / \"pipeline.yaml\")\n        compiler_v1.Compiler(mode=dsl_v1.PipelineExecutionMode.V2_COMPATIBLE).compile(\n            pipeline_func=echo_pipeline, package_path=pipeline_path\n        )\n\n        with open(pipeline_path, \"r\") as f:\n            pipeline_spec = yaml.safe_load(f)\n\n        task = next(\n            t for t in pipeline_spec[\"spec\"][\"templates\"] if t[\"name\"] == \"echo\"\n        )\n        resource_limit = task[\"container\"][\"resources\"][\"limits\"]\n        node_selector = task[\"nodeSelector\"]\n\n        assert resource_limit[\"cpu\"] == \"2\"\n        assert resource_limit[\"memory\"] == \"16G\"\n        assert resource_limit[\"nvidia.com/gpu\"] == \"1\"\n        assert node_selector[\"cloud.google.com/gke-accelerator\"] == \"NVIDIA_TESLA_T4\"\n\n    def test_as_function(self, tmp_path):\n        @dsl.component()\n        def echo() -> str:\n            return \"hello, world\"\n\n        @dsl.pipeline(name=\"echo-pipeline\")\n        def echo_pipeline():\n            container_spec(\n                cpu=\"2\",\n                memory=\"16G\",\n                gpu=\"1\",\n                accelerator=\"NVIDIA_TESLA_T4\",\n            )(echo)()\n\n        pipeline_path = os.fspath(tmp_path / \"pipeline.json\")\n        compiler.Compiler().compile(\n            pipeline_func=echo_pipeline, package_path=pipeline_path\n        )\n\n        with open(pipeline_path, \"r\") as f:\n            pipeline_spec = yaml.safe_load(f)\n\n        container = pipeline_spec[\"pipelineSpec\"][\"deploymentSpec\"][\"executors\"][\n            \"exec-echo\"\n        ][\"container\"]\n\n        assert container[\"resources\"][\"cpuLimit\"] == 2.0\n        assert container[\"resources\"][\"memoryLimit\"] == 16.0\n        assert container[\"resources\"][\"accelerator\"][\"count\"] == \"1\"\n        assert container[\"resources\"][\"accelerator\"][\"type\"] == \"NVIDIA_TESLA_T4\"\n\n    def test_multiple_decorators_v1(self, tmp_path):\n        default_spec = container_spec(cpu=\"2\", memory=\"16G\")\n\n        @container_spec(cpu=\"1\")\n        @default_spec\n        @dsl.component()\n        def echo() -> str:\n            return \"hello, world\"\n\n        @dsl.pipeline(name=\"echo-pipeline\")\n        def echo_pipeline():\n            echo()\n\n        pipeline_path = os.fspath(tmp_path / \"pipeline.yaml\")\n        compiler_v1.Compiler(mode=dsl_v1.PipelineExecutionMode.V2_COMPATIBLE).compile(\n            pipeline_func=echo_pipeline, package_path=pipeline_path\n        )\n\n        with open(pipeline_path, \"r\") as f:\n            pipeline_spec = yaml.safe_load(f)\n\n        task = next(\n            t for t in pipeline_spec[\"spec\"][\"templates\"] if t[\"name\"] == \"echo\"\n        )\n        resource_limit = task[\"container\"][\"resources\"][\"limits\"]\n\n        assert resource_limit[\"cpu\"] == \"1\"\n        assert resource_limit[\"memory\"] == \"16G\"\n\n    def test_multiple_decorators(self, tmp_path):\n        default_spec = container_spec(cpu=\"2\", memory=\"16G\")\n\n        @container_spec(cpu=\"1\")\n        @default_spec\n        @dsl.component()\n        def echo() -> str:\n            return \"hello, world\"\n\n        @dsl.pipeline(name=\"echo-pipeline\")\n        def echo_pipeline():\n            echo()\n\n        pipeline_path = os.fspath(tmp_path / \"pipeline.json\")\n        compiler.Compiler().compile(\n            pipeline_func=echo_pipeline, package_path=pipeline_path\n        )\n\n        with open(pipeline_path, \"r\") as f:\n            pipeline_spec = yaml.safe_load(f)\n\n        container = pipeline_spec[\"pipelineSpec\"][\"deploymentSpec\"][\"executors\"][\n            \"exec-echo\"\n        ][\"container\"]\n\n        assert container[\"resources\"][\"cpuLimit\"] == 1.0\n        assert container[\"resources\"][\"memoryLimit\"] == 16.0\n\n\nclass TestDisplayName:\n    def test_v1(self, tmp_path):\n        @display_name(\"Echo Component\")\n        @dsl.component()\n        def echo() -> str:\n            return \"hello, world\"\n\n        @dsl.pipeline(name=\"echo-pipeline\")\n        def echo_pipeline():\n            echo()\n\n        pipeline_path = os.fspath(tmp_path / \"pipeline.yaml\")\n        compiler_v1.Compiler(mode=dsl_v1.PipelineExecutionMode.V2_COMPATIBLE).compile(\n            pipeline_func=echo_pipeline, package_path=pipeline_path\n        )\n\n        with open(pipeline_path, \"r\") as f:\n            pipeline_spec = yaml.safe_load(f)\n\n        task = next(\n            t for t in pipeline_spec[\"spec\"][\"templates\"] if t[\"name\"] == \"echo\"\n        )\n        annotations = task[\"metadata\"][\"annotations\"]\n        assert (\n            annotations[\"pipelines.kubeflow.org/task_display_name\"] == \"Echo Component\"\n        )\n\n    def test(self, tmp_path):\n        @display_name(\"Echo Component\")\n        @dsl.component()\n        def echo() -> str:\n            return \"hello, world\"\n\n        @dsl.pipeline(name=\"echo-pipeline\")\n        def echo_pipeline():\n            echo()\n\n        pipeline_path = os.fspath(tmp_path / \"pipeline.json\")\n        compiler.Compiler().compile(\n            pipeline_func=echo_pipeline, package_path=pipeline_path\n        )\n\n        with open(pipeline_path, \"r\") as f:\n            pipeline_spec = yaml.safe_load(f)\n\n        task = pipeline_spec[\"pipelineSpec\"][\"root\"][\"dag\"][\"tasks\"][\"echo\"]\n        assert task[\"taskInfo\"][\"name\"] == \"Echo Component\"\n\n\nclass TestCaching:\n    def test_no_caching_v1(self, tmp_path):\n        @dsl.component()\n        def echo() -> str:\n            return \"hello, world\"\n\n        @dsl.pipeline(name=\"echo-pipeline\")\n        def echo_pipeline():\n            echo()\n\n        pipeline_path = os.fspath(tmp_path / \"pipeline.yaml\")\n        compiler_v1.Compiler(mode=dsl_v1.PipelineExecutionMode.V2_COMPATIBLE).compile(\n            pipeline_func=echo_pipeline, package_path=pipeline_path\n        )\n\n        with open(pipeline_path, \"r\") as f:\n            pipeline_spec = yaml.safe_load(f)\n\n        task = next(\n            t for t in pipeline_spec[\"spec\"][\"templates\"] if t[\"name\"] == \"echo\"\n        )\n        labels = task[\"metadata\"][\"labels\"]\n\n        assert labels[\"pipelines.kubeflow.org/enable_caching\"] == \"true\"\n\n    def test_no_caching(self, tmp_path):\n        @dsl.component()\n        def echo() -> str:\n            return \"hello, world\"\n\n        @dsl.pipeline(name=\"echo-pipeline\")\n        def echo_pipeline():\n            echo()\n\n        pipeline_path = os.fspath(tmp_path / \"pipeline.json\")\n        compiler.Compiler().compile(\n            pipeline_func=echo_pipeline, package_path=pipeline_path\n        )\n\n        with open(pipeline_path, \"r\") as f:\n            pipeline_spec = yaml.safe_load(f)\n\n        task = pipeline_spec[\"pipelineSpec\"][\"root\"][\"dag\"][\"tasks\"][\"echo\"]\n\n        assert task[\"cachingOptions\"][\"enableCache\"] is True\n\n    def test_enable_caching_v1(self, tmp_path):\n        @caching(True)\n        @dsl.component()\n        def echo() -> str:\n            return \"hello, world\"\n\n        @dsl.pipeline(name=\"echo-pipeline\")\n        def echo_pipeline():\n            echo()\n\n        pipeline_path = os.fspath(tmp_path / \"pipeline.yaml\")\n        compiler_v1.Compiler(mode=dsl_v1.PipelineExecutionMode.V2_COMPATIBLE).compile(\n            pipeline_func=echo_pipeline, package_path=pipeline_path\n        )\n\n        with open(pipeline_path, \"r\") as f:\n            pipeline_spec = yaml.safe_load(f)\n\n        task = next(\n            t for t in pipeline_spec[\"spec\"][\"templates\"] if t[\"name\"] == \"echo\"\n        )\n        labels = task[\"metadata\"][\"labels\"]\n\n        assert labels[\"pipelines.kubeflow.org/enable_caching\"] == \"true\"\n\n    def test_enable_caching(self, tmp_path):\n        @caching(True)\n        @dsl.component()\n        def echo() -> str:\n            return \"hello, world\"\n\n        @dsl.pipeline(name=\"echo-pipeline\")\n        def echo_pipeline():\n            echo()\n\n        pipeline_path = os.fspath(tmp_path / \"pipeline.json\")\n        compiler.Compiler().compile(\n            pipeline_func=echo_pipeline, package_path=pipeline_path\n        )\n\n        with open(pipeline_path, \"r\") as f:\n            pipeline_spec = yaml.safe_load(f)\n\n        task = pipeline_spec[\"pipelineSpec\"][\"root\"][\"dag\"][\"tasks\"][\"echo\"]\n\n        assert task[\"cachingOptions\"][\"enableCache\"] is True\n\n    def test_disable_caching_v1(self, tmp_path):\n        @caching(False)\n        @dsl.component()\n        def echo() -> str:\n            return \"hello, world\"\n\n        @dsl.pipeline(name=\"echo-pipeline\")\n        def echo_pipeline():\n            echo()\n\n        pipeline_path = os.fspath(tmp_path / \"pipeline.yaml\")\n        compiler_v1.Compiler(mode=dsl_v1.PipelineExecutionMode.V2_COMPATIBLE).compile(\n            pipeline_func=echo_pipeline, package_path=pipeline_path\n        )\n\n        with open(pipeline_path, \"r\") as f:\n            pipeline_spec = yaml.safe_load(f)\n\n        task = next(\n            t for t in pipeline_spec[\"spec\"][\"templates\"] if t[\"name\"] == \"echo\"\n        )\n        labels = task[\"metadata\"][\"labels\"]\n\n        assert labels[\"pipelines.kubeflow.org/enable_caching\"] == \"false\"\n\n    def test_disable_caching(self, tmp_path):\n        @caching(False)\n        @dsl.component()\n        def echo() -> str:\n            return \"hello, world\"\n\n        @dsl.pipeline(name=\"echo-pipeline\")\n        def echo_pipeline():\n            echo()\n\n        pipeline_path = os.fspath(tmp_path / \"pipeline.json\")\n        compiler.Compiler().compile(\n            pipeline_func=echo_pipeline, package_path=pipeline_path\n        )\n\n        with open(pipeline_path, \"r\") as f:\n            pipeline_spec = yaml.safe_load(f)\n\n        task = pipeline_spec[\"pipelineSpec\"][\"root\"][\"dag\"][\"tasks\"][\"echo\"]\n\n        assert \"enableCache\" not in task[\"cachingOptions\"]\n"
  },
  {
    "repo": "speg03/kfp-toolbox",
    "file_path": "tests/test_pipeline_parser.py",
    "raw_url": "https://raw.githubusercontent.com/speg03/kfp-toolbox/main/tests/test_pipeline_parser.py",
    "content": "import os\nfrom typing import Dict, List\n\nimport pytest\nfrom kfp import compiler as compiler_v1\nfrom kfp import dsl as dsl_v1\nfrom kfp.v2 import compiler, dsl\n\nfrom kfp_toolbox.pipeline_parser import Parameter, parse_pipeline_package\n\n\nclass TestParsePipelinePackage:\n    def test_v1(self, tmp_path):\n        @dsl.component()\n        def echo() -> str:\n            return \"hello, world\"\n\n        @dsl.pipeline(name=\"echo-pipeline\")\n        def echo_pipeline(\n            no_default_param: int,\n            int_param: int = 1,\n            float_param: float = 1.5,\n            str_param: str = \"string_value\",\n            bool_param: bool = True,\n            list_param: List[int] = [1, 2, 3],\n            dict_param: Dict[str, int] = {\"key\": 4},\n        ):\n            echo()\n\n        pipeline_path = os.fspath(tmp_path / \"pipeline.yaml\")\n        compiler_v1.Compiler(mode=dsl_v1.PipelineExecutionMode.V2_COMPATIBLE).compile(\n            pipeline_func=echo_pipeline, package_path=pipeline_path\n        )\n        pipeline = parse_pipeline_package(pipeline_path)\n\n        no_default_param = Parameter(name=\"no_default_param\", type=int)\n        int_param = Parameter(name=\"int_param\", type=int, default=1)\n        float_param = Parameter(name=\"float_param\", type=float, default=1.5)\n        str_param = Parameter(name=\"str_param\", type=str, default=\"string_value\")\n        bool_param = Parameter(name=\"bool_param\", type=str, default=\"True\")\n        list_param = Parameter(name=\"list_param\", type=str, default=\"[1, 2, 3]\")\n        dict_param = Parameter(name=\"dict_param\", type=str, default='{\"key\": 4}')\n\n        assert pipeline.name == \"echo-pipeline\"\n        assert len(pipeline.parameters) == 7\n        assert no_default_param in pipeline.parameters\n        assert int_param in pipeline.parameters\n        assert float_param in pipeline.parameters\n        assert str_param in pipeline.parameters\n        assert bool_param in pipeline.parameters\n        assert list_param in pipeline.parameters\n        assert dict_param in pipeline.parameters\n\n    def test(self, tmp_path):\n        @dsl.component()\n        def echo() -> str:\n            return \"hello, world\"\n\n        @dsl.pipeline(name=\"echo-pipeline\")\n        def echo_pipeline(\n            no_default_param: int,\n            int_param: int = 1,\n            float_param: float = 1.5,\n            str_param: str = \"string_value\",\n            bool_param: bool = True,\n            list_param: List[int] = [1, 2, 3],\n            dict_param: Dict[str, int] = {\"key\": 4},\n        ):\n            echo()\n\n        pipeline_path = os.fspath(tmp_path / \"pipeline.json\")\n        compiler.Compiler().compile(\n            pipeline_func=echo_pipeline, package_path=pipeline_path\n        )\n        pipeline = parse_pipeline_package(pipeline_path)\n\n        no_default_param = Parameter(name=\"no_default_param\", type=int)\n        int_param = Parameter(name=\"int_param\", type=int, default=1)\n        float_param = Parameter(name=\"float_param\", type=float, default=1.5)\n        str_param = Parameter(name=\"str_param\", type=str, default=\"string_value\")\n        bool_param = Parameter(name=\"bool_param\", type=str, default=\"True\")\n        list_param = Parameter(name=\"list_param\", type=str, default=\"[1, 2, 3]\")\n        dict_param = Parameter(name=\"dict_param\", type=str, default='{\"key\": 4}')\n\n        assert pipeline.name == \"echo-pipeline\"\n        assert len(pipeline.parameters) == 7\n        assert no_default_param in pipeline.parameters\n        assert int_param in pipeline.parameters\n        assert float_param in pipeline.parameters\n        assert str_param in pipeline.parameters\n        assert bool_param in pipeline.parameters\n        assert list_param in pipeline.parameters\n        assert dict_param in pipeline.parameters\n\n    def test_falsy_default_values_v1(self, tmp_path):\n        @dsl.component()\n        def echo() -> str:\n            return \"hello, world\"\n\n        @dsl.pipeline(name=\"echo-pipeline\")\n        def echo_pipeline(\n            int_param: int = 0,\n            float_param: float = 0.0,\n            str_param: str = \"\",\n        ):\n            echo()\n\n        pipeline_path = os.fspath(tmp_path / \"pipeline.yaml\")\n        compiler_v1.Compiler(mode=dsl_v1.PipelineExecutionMode.V2_COMPATIBLE).compile(\n            pipeline_func=echo_pipeline, package_path=pipeline_path\n        )\n        pipeline = parse_pipeline_package(pipeline_path)\n\n        int_param = Parameter(name=\"int_param\", type=int, default=0)\n        float_param = Parameter(name=\"float_param\", type=float, default=0.0)\n        str_param = Parameter(name=\"str_param\", type=str, default=\"\")\n\n        assert pipeline.name == \"echo-pipeline\"\n        assert len(pipeline.parameters) == 3\n        assert int_param in pipeline.parameters\n        assert float_param in pipeline.parameters\n        assert str_param in pipeline.parameters\n\n    def test_falsy_default_values(self, tmp_path):\n        @dsl.component()\n        def echo() -> str:\n            return \"hello, world\"\n\n        @dsl.pipeline(name=\"echo-pipeline\")\n        def echo_pipeline(\n            int_param: int = 0,\n            float_param: float = 0.0,\n            str_param: str = \"\",\n        ):\n            echo()\n\n        pipeline_path = os.fspath(tmp_path / \"pipeline.json\")\n        compiler.Compiler().compile(\n            pipeline_func=echo_pipeline, package_path=pipeline_path\n        )\n        pipeline = parse_pipeline_package(pipeline_path)\n\n        int_param = Parameter(name=\"int_param\", type=int, default=0)\n        float_param = Parameter(name=\"float_param\", type=float, default=0.0)\n        str_param = Parameter(name=\"str_param\", type=str, default=\"\")\n\n        assert pipeline.name == \"echo-pipeline\"\n        assert len(pipeline.parameters) == 3\n        assert int_param in pipeline.parameters\n        assert float_param in pipeline.parameters\n        assert str_param in pipeline.parameters\n\n    def test_no_parameters_v1(self, tmp_path):\n        @dsl.component()\n        def echo() -> str:\n            return \"hello, world\"\n\n        @dsl.pipeline(name=\"echo-pipeline\")\n        def echo_pipeline():\n            echo()\n\n        pipeline_path = os.fspath(tmp_path / \"pipeline.yaml\")\n        compiler_v1.Compiler(mode=dsl_v1.PipelineExecutionMode.V2_COMPATIBLE).compile(\n            pipeline_func=echo_pipeline, package_path=pipeline_path\n        )\n\n        pipeline = parse_pipeline_package(pipeline_path)\n        assert pipeline.name == \"echo-pipeline\"\n        assert len(pipeline.parameters) == 0\n\n    def test_no_parameters(self, tmp_path):\n        @dsl.component()\n        def echo() -> str:\n            return \"hello, world\"\n\n        @dsl.pipeline(name=\"echo-pipeline\")\n        def echo_pipeline():\n            echo()\n\n        pipeline_path = os.fspath(tmp_path / \"pipeline.json\")\n        compiler.Compiler().compile(\n            pipeline_func=echo_pipeline, package_path=pipeline_path\n        )\n\n        pipeline = parse_pipeline_package(pipeline_path)\n        assert pipeline.name == \"echo-pipeline\"\n        assert len(pipeline.parameters) == 0\n\n    def test_empty_file(self, tmp_path):\n        pipeline = \"\"\n        pipeline_path = os.fspath(tmp_path / \"pipeline.json\")\n        with open(pipeline_path, \"w\") as f:\n            f.write(pipeline)\n\n        with pytest.raises(ValueError) as exc_info:\n            parse_pipeline_package(pipeline_path)\n\n        assert str(exc_info.value) == f\"invalid schema: {pipeline_path}\"\n\n    def test_invalid_schema(self, tmp_path):\n        pipeline = \"\"\"\n            {\n                \"invalid_schema\": {}\n            }\n        \"\"\"\n        pipeline_path = os.fspath(tmp_path / \"pipeline.json\")\n        with open(pipeline_path, \"w\") as f:\n            f.write(pipeline)\n\n        with pytest.raises(ValueError) as exc_info:\n            parse_pipeline_package(pipeline_path)\n\n        assert str(exc_info.value) == f\"invalid schema: {pipeline_path}\"\n"
  },
  {
    "repo": "speg03/kfp-toolbox",
    "file_path": "tests/test_pipelines.py",
    "raw_url": "https://raw.githubusercontent.com/speg03/kfp-toolbox/main/tests/test_pipelines.py",
    "content": "import os\nfrom typing import Dict, List\nfrom unittest.mock import patch\n\nimport pytest\nfrom kfp import compiler as compiler_v1\nfrom kfp import dsl as dsl_v1\nfrom kfp.v2 import compiler, dsl\n\nfrom kfp_toolbox.pipelines import (\n    Parameter,\n    load_pipeline_from_file,\n    submit_pipeline_job,\n    timestamp_pipeline,\n)\n\n\nclass TestTimestampPipeline:\n    def test_v1(self, tmp_path):\n        pipeline_path = os.fspath(tmp_path / \"pipeline.yaml\")\n        compiler_v1.Compiler(mode=dsl_v1.PipelineExecutionMode.V2_COMPATIBLE).compile(\n            pipeline_func=timestamp_pipeline, package_path=pipeline_path\n        )\n\n    def test(self, tmp_path):\n        pipeline_path = os.fspath(tmp_path / \"pipeline.json\")\n        compiler.Compiler().compile(\n            pipeline_func=timestamp_pipeline, package_path=pipeline_path\n        )\n\n\nclass TestLoadPipelineFromFile:\n    def test_v1(self, tmp_path):\n        @dsl.component()\n        def echo() -> str:\n            return \"hello, world\"\n\n        @dsl.pipeline(name=\"echo-pipeline\")\n        def echo_pipeline(\n            no_default_param: int,\n            int_param: int = 1,\n            float_param: float = 1.5,\n            str_param: str = \"string_value\",\n            bool_param: bool = True,\n            list_param: List[int] = [1, 2, 3],\n            dict_param: Dict[str, int] = {\"key\": 4},\n        ):\n            echo()\n\n        pipeline_path = os.fspath(tmp_path / \"pipeline.yaml\")\n        compiler_v1.Compiler(mode=dsl_v1.PipelineExecutionMode.V2_COMPATIBLE).compile(\n            pipeline_func=echo_pipeline, package_path=pipeline_path\n        )\n        pipeline = load_pipeline_from_file(pipeline_path)\n\n        no_default_param = Parameter(name=\"no_default_param\", type=int)\n        int_param = Parameter(name=\"int_param\", type=int, default=1)\n        float_param = Parameter(name=\"float_param\", type=float, default=1.5)\n        str_param = Parameter(name=\"str_param\", type=str, default=\"string_value\")\n        bool_param = Parameter(name=\"bool_param\", type=str, default=\"True\")\n        list_param = Parameter(name=\"list_param\", type=str, default=\"[1, 2, 3]\")\n        dict_param = Parameter(name=\"dict_param\", type=str, default='{\"key\": 4}')\n\n        assert pipeline.name == \"echo-pipeline\"\n        assert len(pipeline.parameters) == 7\n        assert no_default_param in pipeline.parameters\n        assert int_param in pipeline.parameters\n        assert float_param in pipeline.parameters\n        assert str_param in pipeline.parameters\n        assert bool_param in pipeline.parameters\n        assert list_param in pipeline.parameters\n        assert dict_param in pipeline.parameters\n\n    def test(self, tmp_path):\n        @dsl.component()\n        def echo() -> str:\n            return \"hello, world\"\n\n        @dsl.pipeline(name=\"echo-pipeline\")\n        def echo_pipeline(\n            no_default_param: int,\n            int_param: int = 1,\n            float_param: float = 1.5,\n            str_param: str = \"string_value\",\n            bool_param: bool = True,\n            list_param: List[int] = [1, 2, 3],\n            dict_param: Dict[str, int] = {\"key\": 4},\n        ):\n            echo()\n\n        pipeline_path = os.fspath(tmp_path / \"pipeline.json\")\n        compiler.Compiler().compile(\n            pipeline_func=echo_pipeline, package_path=pipeline_path\n        )\n        pipeline = load_pipeline_from_file(pipeline_path)\n\n        no_default_param = Parameter(name=\"no_default_param\", type=int)\n        int_param = Parameter(name=\"int_param\", type=int, default=1)\n        float_param = Parameter(name=\"float_param\", type=float, default=1.5)\n        str_param = Parameter(name=\"str_param\", type=str, default=\"string_value\")\n        bool_param = Parameter(name=\"bool_param\", type=str, default=\"True\")\n        list_param = Parameter(name=\"list_param\", type=str, default=\"[1, 2, 3]\")\n        dict_param = Parameter(name=\"dict_param\", type=str, default='{\"key\": 4}')\n\n        assert pipeline.name == \"echo-pipeline\"\n        assert len(pipeline.parameters) == 7\n        assert no_default_param in pipeline.parameters\n        assert int_param in pipeline.parameters\n        assert float_param in pipeline.parameters\n        assert str_param in pipeline.parameters\n        assert bool_param in pipeline.parameters\n        assert list_param in pipeline.parameters\n        assert dict_param in pipeline.parameters\n\n    def test_falsy_default_values_v1(self, tmp_path):\n        @dsl.component()\n        def echo() -> str:\n            return \"hello, world\"\n\n        @dsl.pipeline(name=\"echo-pipeline\")\n        def echo_pipeline(\n            int_param: int = 0,\n            float_param: float = 0.0,\n            str_param: str = \"\",\n        ):\n            echo()\n\n        pipeline_path = os.fspath(tmp_path / \"pipeline.yaml\")\n        compiler_v1.Compiler(mode=dsl_v1.PipelineExecutionMode.V2_COMPATIBLE).compile(\n            pipeline_func=echo_pipeline, package_path=pipeline_path\n        )\n        pipeline = load_pipeline_from_file(pipeline_path)\n\n        int_param = Parameter(name=\"int_param\", type=int, default=0)\n        float_param = Parameter(name=\"float_param\", type=float, default=0.0)\n        str_param = Parameter(name=\"str_param\", type=str, default=\"\")\n\n        assert pipeline.name == \"echo-pipeline\"\n        assert len(pipeline.parameters) == 3\n        assert int_param in pipeline.parameters\n        assert float_param in pipeline.parameters\n        assert str_param in pipeline.parameters\n\n    def test_falsy_default_values(self, tmp_path):\n        @dsl.component()\n        def echo() -> str:\n            return \"hello, world\"\n\n        @dsl.pipeline(name=\"echo-pipeline\")\n        def echo_pipeline(\n            int_param: int = 0,\n            float_param: float = 0.0,\n            str_param: str = \"\",\n        ):\n            echo()\n\n        pipeline_path = os.fspath(tmp_path / \"pipeline.json\")\n        compiler.Compiler().compile(\n            pipeline_func=echo_pipeline, package_path=pipeline_path\n        )\n        pipeline = load_pipeline_from_file(pipeline_path)\n\n        int_param = Parameter(name=\"int_param\", type=int, default=0)\n        float_param = Parameter(name=\"float_param\", type=float, default=0.0)\n        str_param = Parameter(name=\"str_param\", type=str, default=\"\")\n\n        assert pipeline.name == \"echo-pipeline\"\n        assert len(pipeline.parameters) == 3\n        assert int_param in pipeline.parameters\n        assert float_param in pipeline.parameters\n        assert str_param in pipeline.parameters\n\n    def test_no_parameters_v1(self, tmp_path):\n        @dsl.component()\n        def echo() -> str:\n            return \"hello, world\"\n\n        @dsl.pipeline(name=\"echo-pipeline\")\n        def echo_pipeline():\n            echo()\n\n        pipeline_path = os.fspath(tmp_path / \"pipeline.yaml\")\n        compiler_v1.Compiler(mode=dsl_v1.PipelineExecutionMode.V2_COMPATIBLE).compile(\n            pipeline_func=echo_pipeline, package_path=pipeline_path\n        )\n\n        pipeline = load_pipeline_from_file(pipeline_path)\n        assert pipeline.name == \"echo-pipeline\"\n        assert len(pipeline.parameters) == 0\n\n    def test_no_parameters(self, tmp_path):\n        @dsl.component()\n        def echo() -> str:\n            return \"hello, world\"\n\n        @dsl.pipeline(name=\"echo-pipeline\")\n        def echo_pipeline():\n            echo()\n\n        pipeline_path = os.fspath(tmp_path / \"pipeline.json\")\n        compiler.Compiler().compile(\n            pipeline_func=echo_pipeline, package_path=pipeline_path\n        )\n\n        pipeline = load_pipeline_from_file(pipeline_path)\n        assert pipeline.name == \"echo-pipeline\"\n        assert len(pipeline.parameters) == 0\n\n    def test_empty_file(self, tmp_path):\n        pipeline = \"\"\n        pipeline_path = os.fspath(tmp_path / \"pipeline.json\")\n        with open(pipeline_path, \"w\") as f:\n            f.write(pipeline)\n\n        with pytest.raises(ValueError) as exc_info:\n            load_pipeline_from_file(pipeline_path)\n\n        assert str(exc_info.value) == f\"invalid schema: {pipeline_path}\"\n\n    def test_invalid_schema(self, tmp_path):\n        pipeline = \"\"\"\n            {\n                \"invalid_schema\": {}\n            }\n        \"\"\"\n        pipeline_path = os.fspath(tmp_path / \"pipeline.json\")\n        with open(pipeline_path, \"w\") as f:\n            f.write(pipeline)\n\n        with pytest.raises(ValueError) as exc_info:\n            load_pipeline_from_file(pipeline_path)\n\n        assert str(exc_info.value) == f\"invalid schema: {pipeline_path}\"\n\n\nclass TestSubmitPipelineJob:\n    @patch(\"google.cloud.aiplatform.PipelineJob\")\n    @patch(\"kfp.Client\")\n    def test_no_endpoints(self, mock_kfp, mock_aip):\n        submit_pipeline_job(\n            pipeline_file=\"/path/to/file\",\n            arguments={\"param\": 1},\n            run_name=\"test-run\",\n            experiment_name=\"test-experiment\",\n        )\n\n        mock_kfp.assert_not_called()\n        mock_aip.assert_called_once_with(\n            display_name=None,\n            template_path=\"/path/to/file\",\n            job_id=\"test-run\",\n            pipeline_root=None,\n            parameter_values={\"param\": 1},\n            enable_caching=None,\n            encryption_spec_key_name=None,\n            labels=None,\n            project=None,\n            location=None,\n        )\n        mock_aip.return_value.submit.assert_called_once_with(\n            service_account=None, network=None, experiment=\"test-experiment\"\n        )\n\n    @patch(\"google.cloud.aiplatform.PipelineJob\")\n    @patch(\"kfp.Client\")\n    def test_endpoint(self, mock_kfp, mock_aip):\n        submit_pipeline_job(\n            pipeline_file=\"/path/to/file\",\n            arguments={\"param\": 1},\n            run_name=\"test-run\",\n            experiment_name=\"test-experiment\",\n            endpoint=\"http://localhost:8080\",\n        )\n\n        mock_aip.assert_not_called()\n        mock_kfp.assert_called_once_with(\n            host=\"http://localhost:8080\",\n            client_id=None,\n            namespace=\"kubeflow\",\n            other_client_id=None,\n            other_client_secret=None,\n        )\n        mock_kfp.return_value.create_run_from_pipeline_package.assert_called_once_with(\n            pipeline_file=\"/path/to/file\",\n            arguments={\"param\": 1},\n            run_name=\"test-run\",\n            experiment_name=\"test-experiment\",\n            namespace=None,\n            pipeline_root=None,\n            enable_caching=None,\n            service_account=None,\n        )\n"
  },
  {
    "repo": "speg03/kfp-toolbox",
    "file_path": "src/kfp_toolbox/pipelines.py",
    "raw_url": "https://raw.githubusercontent.com/speg03/kfp-toolbox/main/src/kfp_toolbox/pipelines.py",
    "content": "import os\nimport warnings\nfrom typing import Any, Mapping, Optional, Union\n\nfrom kfp.v2 import dsl\n\nfrom . import pipeline_jobs, pipeline_parser\nfrom .components import timestamp\nfrom .pipeline_parser import Parameter, ParameterValue, Pipeline  # noqa: F401\n\n\n@dsl.pipeline(name=\"timestamp-pipeline\")\ndef timestamp_pipeline(\n    format: str = \"%Y%m%d%H%M%S\",\n    prefix: str = \"\",\n    suffix: str = \"\",\n    separator: str = \"-\",\n    tz_offset: int = 0,\n):\n    time_string = timestamp(\n        format=format,\n        prefix=prefix,\n        suffix=suffix,\n        separator=separator,\n        tz_offset=tz_offset,\n    )\n    return time_string\n\n\ndef load_pipeline_from_file(filepath: Union[str, os.PathLike]) -> Pipeline:\n    \"\"\"Load a pipeline object from the pipeline file.\n\n    Load a :class:`Pipeline` object from a pre-compiled file that represents\n    the pipeline.\n\n    Args:\n        filepath (Union[str, os.PathLike]): The path of the pre-compiled file that\n            represents the pipeline.\n\n    Raises:\n        ValueError: If the :attr:`filepath` file has an invalid schema.\n\n    Returns:\n        Pipeline: An object that represents the pipeline.\n\n    .. deprecated:: 0.6.0\n        Use :func:`.pipeline_parser.parse_pipeline_package` instead.\n\n    \"\"\"\n\n    warnings.warn(\n        \"`load_pipeline_from_file is deprecated.\"\n        \" Use `pipeline_parser.parse_pipeline_package` instead.\",\n        DeprecationWarning,\n    )\n\n    return pipeline_parser.parse_pipeline_package(filepath=filepath)\n\n\ndef submit_pipeline_job(\n    pipeline_file: Union[str, os.PathLike],\n    endpoint: Optional[str] = None,\n    iap_client_id: Optional[str] = None,\n    api_namespace: str = \"kubeflow\",\n    other_client_id: Optional[str] = None,\n    other_client_secret: Optional[str] = None,\n    arguments: Optional[Mapping[str, Any]] = None,\n    run_name: Optional[str] = None,\n    experiment_name: Optional[str] = None,\n    namespace: Optional[str] = None,\n    pipeline_root: Optional[str] = None,\n    enable_caching: Optional[bool] = None,\n    service_account: Optional[str] = None,\n    encryption_spec_key_name: Optional[str] = None,\n    labels: Optional[Mapping[str, str]] = None,\n    project: Optional[str] = None,\n    location: Optional[str] = None,\n    network: Optional[str] = None,\n):\n    \"\"\"Submit a pipeline job.\n\n    Submit a pipeline job to the appropriate environment. If an :attr:`endpoint` is\n    specified, it is considered an instance of Kubeflow Pipelines. Otherwise, attempt\n    to submit to Vertex AI Pipelines.\n\n    Args:\n        pipeline_file (Union[str, os.PathLike]): Path of the pipeline package file.\n        endpoint (Optional[str], optional): Endpoint of the KFP API service to connect.\n            Used only for Kubeflow Pipelines. Defaults to None.\n        iap_client_id (Optional[str], optional): The client ID used by Identity-Aware\n            Proxy. Used only for Kubeflow Pipelines. Defaults to None.\n        api_namespace (str, optional): Kubernetes namespace to connect to the KFP API.\n            Used only for Kubeflow Pipelines. Defaults to \"kubeflow\".\n        other_client_id (Optional[str], optional): The client ID used to obtain\n            the auth codes and refresh tokens. Used only for Kubeflow Pipelines.\n            Defaults to None.\n        other_client_secret (Optional[str], optional): The client secret used to obtain\n            the auth codes and refresh tokens. Used only for Kubeflow Pipelines.\n            Defaults to None.\n        arguments (Optional[Mapping[str, Any]], optional): Arguments to the pipeline\n            function provided as a dict. Defaults to None.\n        run_name (Optional[str], optional): Name of the run to be shown in the UI.\n            Defaults to None.\n        experiment_name (Optional[str], optional): Name of the experiment to add the\n            run to. Defaults to None.\n        namespace (Optional[str], optional): Kubernetes namespace where the pipeline\n            runs are created. Used only for Kubeflow Pipelines. Defaults to None.\n        pipeline_root (Optional[str], optional): The root path of the pipeline outputs.\n            Defaults to None.\n        enable_caching (Optional[bool], optional): Whether or not to enable caching for\n            the run. Defaults to None.\n        service_account (Optional[str], optional): Specifies which Kubernetes service\n            account this run uses. Defaults to None.\n        encryption_spec_key_name (Optional[str], optional): The Cloud KMS resource\n            identifier of the customer managed encryption key used to protect the job.\n            Used only for Vertex AI Pipelines. Defaults to None.\n        labels (Optional[Mapping[str, str]], optional): The user defined metadata to\n            organize PipelineJob. Used only for Vertex AI Pipelines. Defaults to None.\n        project (Optional[str], optional): The project that you want to run this\n            PipelineJob in. Used only for Vertex AI Pipelines. Defaults to None.\n        location (Optional[str], optional): Location to create PipelineJob. Used only\n            for Vertex AI Pipelines. Defaults to None.\n        network (Optional[str], optional): The full name of the Compute Engine network\n            to which the job should be peered. Used only for Vertex AI Pipelines.\n            Defaults to None.\n\n    .. deprecated:: 0.6.0\n        Use :func:`.pipeline_jobs.submit_pipeline_job` instead.\n\n    \"\"\"\n\n    warnings.warn(\n        \"`pipelines.submit_pipeline_job` is deprecated.\"\n        \" Use `pipeline_jobs.submit_pipeline_job` instead.\",\n        DeprecationWarning,\n    )\n\n    return pipeline_jobs.submit_pipeline_job(\n        pipeline_file=pipeline_file,\n        endpoint=endpoint,\n        iap_client_id=iap_client_id,\n        api_namespace=api_namespace,\n        other_client_id=other_client_id,\n        other_client_secret=other_client_secret,\n        arguments=arguments,\n        run_name=run_name,\n        experiment_name=experiment_name,\n        namespace=namespace,\n        pipeline_root=pipeline_root,\n        enable_caching=enable_caching,\n        service_account=service_account,\n        encryption_spec_key_name=encryption_spec_key_name,\n        labels=labels,\n        project=project,\n        location=location,\n        network=network,\n    )\n"
  },
  {
    "repo": "ZoieD/kfp-resnet",
    "file_path": "pipeline/src/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/ZoieD/kfp-resnet/main/pipeline/src/pipeline.py",
    "content": "# Copyright (c) 2019, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport kfp.dsl as dsl\nimport datetime\nimport os\nfrom kubernetes import client as k8s_client\n\n\n# Modify image='<image>' in each op to match IMAGE in the build.sh of its corresponding component\n\ndef PreprocessOp(name, input_dir, output_dir):\n    return dsl.ContainerOp(\n        name=name,\n        image='zdou001/only_tests:preprocess-image',\n        arguments=[\n            '--input_dir', input_dir,\n            '--output_dir', output_dir,\n        ],\n        file_outputs={'output': '/output.txt'}\n    )\n\n\ndef TrainOp(name, input_dir, output_dir, model_name, model_version, epochs):\n    return dsl.ContainerOp(\n        name=name,\n        image='zdou001/only_tests:train-image',\n        arguments=[\n            '--input_dir', input_dir,\n            '--output_dir', output_dir,\n            '--model_name', model_name,\n            '--model_version', model_version,\n            '--epochs', epochs\n        ],\n        file_outputs={'output': '/output.txt'}\n    )\n\n\ndef InferenceServerLauncherOp(name, input_dir, trtserver_name):\n    return dsl.ContainerOp(\n        name=name,\n        image='zdou001/only_tests:inference-server-launcher-image',\n        arguments=[\n            '--trtserver_name', trtserver_name,\n            '--model_path', input_dir,\n        ],\n        file_outputs={'output': '/output.txt'}\n    )\n\n\ndef WebappLauncherOp(name, trtserver_name, model_name, model_version, webapp_prefix, webapp_port):\n    return dsl.ContainerOp(\n        name=name,\n        image='zdou001/only_tests:webapp-launcher-image',\n        arguments=[\n            '--workflow_name', '{{workflow.name}}',\n            '--trtserver_name', trtserver_name,\n            '--model_name', model_name,\n            '--model_version', str(model_version),\n            '--webapp_prefix', webapp_prefix,\n            '--webapp_port', str(webapp_port)\n        ],\n        file_outputs={}\n    )\n\n\n@dsl.pipeline(\n    name='resnet_cifar10_pipeline',\n    description='Demonstrate an end-to-end training & serving pipeline using ResNet and CIFAR-10'\n)\ndef resnet_pipeline(\n    raw_data_dir='/mnt/workspace/raw_data',\n    processed_data_dir='/mnt/workspace/processed_data',\n    model_dir='/mnt/workspace/saved_model',\n    epochs=50,\n    trtserver_name='trtis',\n    model_name='resnet_graphdef',\n    model_version=1,\n    webapp_prefix='webapp',\n    webapp_port=80\n):\n\n    persistent_volume_name = 'nvidia-workspace'\n    persistent_volume_path = '/mnt/workspace'\n\n    op_dict = {}\n\n    op_dict['preprocess'] = PreprocessOp(\n        'preprocess', raw_data_dir, processed_data_dir)\n\n    op_dict['train'] = TrainOp(\n        'train', op_dict['preprocess'].output, model_dir, model_name, model_version, epochs)\n\n    op_dict['deploy_inference_server'] = InferenceServerLauncherOp(\n        'deploy_inference_server', op_dict['train'].output, trtserver_name)\n\n    op_dict['deploy_webapp'] = WebappLauncherOp(\n        'deploy_webapp', op_dict['deploy_inference_server'].output, model_name, model_version, webapp_prefix, webapp_port)\n\n    for _, container_op in op_dict.items():\n        container_op.add_volume(k8s_client.V1Volume(\n            host_path=k8s_client.V1HostPathVolumeSource(\n                path=persistent_volume_path),\n            name=persistent_volume_name))\n        container_op.add_volume_mount(k8s_client.V1VolumeMount(\n            mount_path=persistent_volume_path,\n            name=persistent_volume_name))\n\n\nif __name__ == '__main__':\n    import kfp.compiler as compiler\n    compiler.Compiler().compile(resnet_pipeline, __file__ + '.tar.gz')\n"
  },
  {
    "repo": "shawar8/sfcrime-prediction-kubeflow",
    "file_path": "data_ingest_fns.py",
    "raw_url": "https://raw.githubusercontent.com/shawar8/sfcrime-prediction-kubeflow/main/data_ingest_fns.py",
    "content": "import argparse\nfrom variables import *\nfrom common_fns import *\nfrom datetime import datetime, timedelta\nimport pytz\nimport kfp\nfrom kfp.dsl.structures import yaml\nfrom kfp.client import Client\nimport os\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import component\nfrom kfp.v2.dsl import (\n    Input,\n    Output,\n    Artifact,\n    Dataset,\n    Model,\n    Metrics\n)\n\npwd = os.getcwd()\nif not os.path.exists(os.path.join(pwd, 'new_yamls')):\n    os.mkdir(os.path.join(pwd, 'new_yamls'))\n\n\n@component(\n    base_image='python:3.9',\n    packages_to_install=['pandas', 'numpy'],\n    output_component_file='new_yamls/preprocess_data.yaml'\n)\ndef preprocess_data(df_path: Input[Dataset], to_keep: list, data:str,\n                    df_output_path: Output[Dataset]):\n    import pandas as pd\n    from datetime import datetime, timedelta\n    import pytz\n    import numpy as np\n\n    def get_year_month(input):\n        return input.year, input.month, input.hour\n\n    def get_date_time(input):\n        return input.date(), input.time()\n    df = pd.read_csv(df_path.path)\n    df.drop_duplicates('incident_id', inplace = True)\n    df = df[df['incident_category'] != 'Case Closure']\n    if 'filed_online' not in df.columns:\n        df['filed_online'] = False\n    df['filed_online'].fillna(False, inplace = True)\n    df = df[to_keep]\n    df = df.dropna().reset_index(drop=True)\n    df = df[df['police_district'] != 'Out of SF']\n    if data == 'serving':\n        time_format = None\n    else:\n        time_format = '%Y/%m/%d %I:%M:%S %p'\n    for col in ['incident_datetime', 'report_datetime']:\n        df[col] = pd.to_datetime(df[col], format = time_format)\n    today = datetime.now(tz= pytz.timezone('US/Pacific'))\n    max_date_training = (today - timedelta(days=3)).date()\n    df['diff'] = (df['report_datetime'] - df['incident_datetime']).apply(lambda x: x.total_seconds()/3600)\n    df = df[df['diff'] >= 0]\n    df['diff'] = df['diff'].apply(lambda x: np.log(min(x, 365*24) + 1))\n    df['incident_date'], df['incident_time'] = zip(*df['incident_datetime'].apply(lambda x: get_date_time(x)))\n    if data == 'training':\n        df = df[df['incident_date'] <= max_date_training]\n    df['year'], df['month'], df['hour'] = zip(*df['incident_datetime'].apply(lambda x: get_year_month(x)))\n    print (df['year'].unique())\n    df['is_weekend'] = df['incident_day_of_week'].apply(lambda x: int(x in ['Saturday', 'Sunday']))\n    # df = df[df['year'].isin([2018,2023])]\n    df.to_csv(df_output_path.path, index=False)\n\n\n@component(\n    base_image='python:3.9',\n    packages_to_install=['pandas'],\n    output_component_file='new_yamls/mapping_labels.yaml'\n)\ndef map_label(df_path: Input[Dataset], mapped_df_path: Output[Dataset]):\n    import pandas as pd\n\n    df = pd.read_csv(df_path.path)\n    label_mapping = {'Human Trafficking (B), Involuntary Servitude': 'Human Trafficking',\n                     'Human Trafficking, Commercial Sex Acts':'Human Trafficking',\n                     'Human Trafficking (A), Commercial Sex Acts':'Human Trafficking',\n                     'Weapons Offense':'Weapons Offence',\n                     'Motor Vehicle Theft?':'Motor Vehicle Theft',\n                     'Suspicious': 'Suspicious Occ',\n                     'Other Offenses': 'Other',\n                     'Other Miscellaneous': 'Other',\n                     'Drug Offense': 'Drug Violation'}\n    df['incident_category'] = df['incident_category'].replace(label_mapping)\n    all_crimes = list(df['incident_category'].unique())\n    crimes_dict = {'Other': 'other'}\n    violent_crimes = ['Assault', 'Rape', 'Arson', 'Suicide', 'Weapons Offence', 'Homicide',\n                        'Sex Offense', 'Human Trafficking']\n    non_violent_crimes = [\"Lost Property\",\"Non-Criminal\",\"Warrant\",\"Suspicious Occ\",\"Missing Person\",\n                          \"Disorderly Conduct\",\"Larceny Theft\",\"Fire Report\",\"Fraud\",\"Malicious Mischief\",\"Robbery\",\n                          \"Burglary\",\"Vandalism\",\"Traffic Collision\",\"Drug Violation\",\"Motor Vehicle Theft\",\n                          \"Traffic Violation Arrest\",\"Recovered Vehicle\",\"Miscellaneous Investigation\",\"Weapons Carrying Etc\",\n                          \"Forgery And Counterfeiting\",\"Embezzlement\",\"Stolen Property\",\"Vehicle Misplaced\",\n                          \"Offences Against The Family And Children\",\"Prostitution\",\"Vehicle Impounded\",\n                          \"Courtesy Report\",\"Liquor Laws\",\"Gambling\",\"Civil Sidewalks\"]\n\n    for crime in all_crimes:\n        if crime in violent_crimes:\n            crimes_dict[crime] = 'violent'\n        elif crime in non_violent_crimes:\n            crimes_dict[crime] = 'non violent'\n        else:\n            crimes_dict[crime] = 'other'\n\n    df['incident_category'] = df['incident_category'].map(crimes_dict)\n    df.to_csv(mapped_df_path.path, index=False)\n\n@component(\n    base_image='python:3.9',\n    packages_to_install=['nfl-data-py', 'pandas', 'numpy', 'astral'],\n    output_component_file='new_yamls/merging_crime_nfl_data.yaml')\ndef merge_with_nfl(sf_dataset: Input[Dataset], merged_csv: Output[Dataset]):\n    import nfl_data_py as nfl\n    import pandas as pd\n    import numpy as np\n    from astral import LocationInfo\n    from astral.sun import sun\n    from astral.location import Location\n    from datetime import date\n\n    def get_diff_in_time(input):\n        game_time = input['datetime']\n        incident_time = input['incident_datetime']\n        if input['is_game_day'] == 0:\n            return 0\n        else:\n            if game_time > incident_time:\n                return (game_time-incident_time).total_seconds()//60\n            return (-1 * ((incident_time-game_time).total_seconds()//60))\n\n    def game_date_time(datetime):\n        date = pd.to_datetime(datetime['gameday'], format= '%Y-%m-%d').date()\n        time = pd.to_datetime(datetime['gametime'], format= '%H:%M').time()\n        return time, date\n\n    def get_sunrise_sunset(input, city, sf):\n        incident_dt = input['incident_datetime']\n        incident_time = input['incident_time']\n        s = sun(city.observer, date=incident_dt, tzinfo=sf.timezone)\n        sr = s['sunrise'].replace(tzinfo=None).time()\n        ss = s['sunset'].replace(tzinfo=None).time()\n        if incident_time <= sr or incident_time >= ss:\n            return 1\n        return 0\n\n    lat, lon = [37.7, -122.452]\n    city = LocationInfo(\"San Francisco\", \"United States\", \"US/Pacific\", lat, lon)\n    sf = Location(city)\n    df = pd.read_csv(sf_dataset.path)\n    all_years = df['year'].unique().tolist()\n\n    nfl_data = nfl.import_schedules(all_years).query('home_team == \"SF\"')[['gameday', 'gametime']]\n    nfl_data['datetime'] = pd.to_datetime(nfl_data['gameday'].astype(str) + ' ' + nfl_data['gametime'].astype(str), format='%Y-%m-%d %H:%M')\n    nfl_data['gameday'] = pd.to_datetime(nfl_data['gameday']).apply(lambda x: x.date())\n    print ('data type of date in nfl data: ', type(nfl_data['gameday'].tolist()[0]))\n    print ('nfl shape: ', nfl_data.shape)\n    print ('data type of date in orig data: ', type(df['incident_date'].tolist()[0]))\n    df['incident_datetime'] = pd.to_datetime(df['incident_datetime'])\n    df['incident_date'] = pd.to_datetime(df['incident_date']).apply(lambda x: x.date())\n    df['incident_time'] = pd.to_datetime(df['incident_time']).apply(lambda x: x.time())\n    df = df.merge(nfl_data, how = 'left', right_on='gameday', left_on='incident_date')\n    print ('number of game dates that are not null: ', df.query('gameday==gameday').shape)\n    df.loc[pd.notnull(df['gameday']), 'is_game_day'] = 1\n    df.loc[pd.isnull(df['gameday']), 'is_game_day'] = 0\n    print (df['is_game_day'].value_counts())\n    df['till_game_time'] = df.apply(lambda x: get_diff_in_time(x), axis = 1)\n    df['is_between_sr_ss'] = df.apply(lambda x: get_sunrise_sunset(x, city, sf), axis = 1)\n    df.to_csv(merged_csv.path, index=False)\n\n@component(\n    base_image='python:3.9',\n    packages_to_install=['pandas'],\n    output_component_file='new_yamls/add_dummy_cols.yaml'\n)\ndef add_dummy_cols(df_path: Input[Dataset], output_df: Output[Dataset]):\n    import pandas as pd\n\n    df = pd.read_csv(df_path.path)\n    dummy_cols = ['latitude_buckets', 'longitude_buckets', 'Prediction']\n    for col in dummy_cols:\n        df[col] = ''\n\n    df.to_csv(output_df.path, index=False)\n    \n@dsl.pipeline(\n    name='SF Data Pipeline',\n    description= 'The entire pipeline from reading sf and NFL data to pre-processing it and uploading to BQ')\ndef sf_ingest_pipeline(url: str, projectid:str = projectid, datasetid:str =datasetid, tablename:str =tablename):\n\n    get_data_task = read_training_data(url = url, method='ingesting', projectid=projectid, datasetid=datasetid,\n                                       tablename=tablename)\n    preprocess_data_task = preprocess_data(df_path= get_data_task.outputs['output_csv'], data = 'training',\n                                           to_keep = ['incident_id', 'police_district', 'incident_datetime',\n                                      'latitude', 'longitude', 'filed_online', 'incident_day_of_week',\n                                      'report_datetime', 'analysis_neighborhood', 'supervisor_district',\n                                      'incident_category', 'data_type'])\n    map_label_task = map_label(df_path= preprocess_data_task.outputs['df_output_path'])\n    merge_nfl_task = merge_with_nfl(sf_dataset = map_label_task.outputs['mapped_df_path'])\n    insert_dummy_cols_task = add_dummy_cols(df_path= merge_nfl_task.outputs['merged_csv'])\n    upload_task = upload_to_bq(df_path = insert_dummy_cols_task.outputs['output_df'], projectid= projectid,\n                                datasetid= datasetid, tableid= tablename, location='US',\n                                schema = schema)\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='Kubeflow pipeline to run ingestion script for SF Crime data.')\n    parser.add_argument('type', help='Enter \"yaml\" if file needs to be saved or \"run\" if pipline needs to be run directly',\n                       type= str)\n    args = parser.parse_args()\n    \n    if args.type == 'yaml':\n        kfp.compiler.Compiler().compile(\n            pipeline_func=sf_ingest_pipeline,\n            package_path='new_yamls/ingestion_pipeline.yaml')\n    elif args.type == 'run':\n        client = Client(host= host_url)\n        today = datetime.now(tz= pytz.timezone('US/Pacific'))\n        max_date_training = (today - timedelta(days=3)).date().strftime('%Y%m%d')\n        url = f'https://data.sfgov.org/api/views/wg3w-h783/rows.csv?date={max_date_training}&accessType=DOWNLOAD'\n        experiments = client.list_experiments().experiments\n        experiment_names = list(map(lambda x: x.display_name, experiments))\n        if ingest_experiment_name not in experiment_names:\n            client.create_experiment(name=ingest_experiment_name)\n        client.create_run_from_pipeline_func(\n        pipeline_func=sf_ingest_pipeline,\n        experiment_name=ingest_experiment_name,\n        arguments={\n            'url': url,\n        })\n"
  },
  {
    "repo": "shawar8/sfcrime-prediction-kubeflow",
    "file_path": "monitoring_fns.py",
    "raw_url": "https://raw.githubusercontent.com/shawar8/sfcrime-prediction-kubeflow/main/monitoring_fns.py",
    "content": "### Components for monitoring metrics, drift and retraining\nimport kfp\nimport argparse\nfrom training_functions import *\nfrom variables import *\nfrom typing import NamedTuple\nimport pickle\nfrom kfp.dsl.structures import yaml\nimport os\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import component\nfrom kfp.v2.dsl import (\n    Input,\n    Output,\n    Artifact,\n    Dataset,\n    Model,\n    Metrics\n)\n\n\n# class FloatOutput(NamedTuple):\n#     float_value: float\n\n@component(\n    base_image='python:3.9',\n    packages_to_install=['google-cloud-bigquery', 'pandas', 'pandas-gbq', 'google-cloud-aiplatform', 'pyarrow'],\n    output_component_file='new_yamls/read_data_from_bq.yaml'\n)\ndef read_bq_data(projectid: str, db:str, tablename: str, model_name:str,\n                 train_data_filename: str,\n                 output_df_path: Output[Dataset],\n                 train_df_path: Output[Dataset]):\n    from google.cloud import bigquery, aiplatform, storage\n    from datetime import datetime, timedelta\n    from pytz import timezone\n    import pandas as pd\n    import os\n\n    tz = timezone('US/Pacific')\n    end_date = str((datetime.now(tz) - timedelta(days=7)).date())\n    client = storage.Client(projectid)\n    models = aiplatform.Model.list(\n    filter=f'display_name={model_name}',\n    order_by=\"update_time\",\n    location='us-central1')\n    latest_model = models[-1]\n    model_path = latest_model.to_dict()['artifactUri']\n    path_split = model_path.split('/')\n    bucket_name = path_split[2]\n    bucket = client.get_bucket(bucket_name)\n    uri_folder = '/'.join(path_split[3:])\n    train_df_blob = bucket.blob(f'{uri_folder}/{train_data_filename}')\n    train_df_blob.download_to_filename(train_data_filename)\n\n    train_df = pd.read_parquet(train_data_filename)\n\n    # train_query = f'SELECT * FROM {projectid}.{db}.{tablename} where data_type = \"Train\"'\n    query = f'SELECT * FROM {projectid}.{db}.{tablename} where report_datetime >= \"{end_date}\"'\n    # train_df = pd.read_gbq(train_query, project_id=projectid, dialect= 'standard')\n    train_df.to_csv(train_df_path.path, index=False)\n    df = pd.read_gbq(query, project_id=projectid, dialect= 'standard')\n    df.to_csv(output_df_path.path, index=False)\n\n# @component(\n#     base_image='python:3.9',\n#     packages_to_install=['pandas', 'scikit-learn'],\n#     output_component_file='new_yamls/get_accuracy.yaml'\n# )\n# def get_accuracy(df_path: Input[Dataset]) -> FloatOutput:\n    \n#     import pandas as pd\n#     from sklearn.metrics import accuracy_score\n#     import pickle\n\n#     df = pd.read_csv(df_path.path)\n#     accuracy = accuracy_score(df['Prediction'], df['incident_category'])\n#     return FloatOutput(accuracy)\n    \n@component(\n    base_image='python:3.9',\n    packages_to_install=['pandas', 'scikit-learn'],\n    output_component_file='new_yamls/get_accuracy.yaml'\n)\ndef get_accuracy(df_path: Input[Dataset]) -> float:\n    \n    import pandas as pd\n    from sklearn.metrics import accuracy_score\n    import pickle\n\n    df = pd.read_csv(df_path.path)\n    accuracy = accuracy_score(df['Prediction'], df['incident_category'])\n    return accuracy\n    \n@component(\n    base_image='python:3.9',\n    packages_to_install= ['scipy', 'pandas', 'google-cloud-storage', 'google-cloud-aiplatform'],\n    output_component_file='new_yamls/compare_distributions.yaml'\n)\ndef get_and_compare_distributions(train_df_path: Input[Dataset], df_path: Input[Dataset], projectid: str, \n                                  accuracy:float, training_dist_dict_name: str, model_name: str, \n                                  eval_output: Output[Metrics]):\n\n    import pandas as pd\n    from scipy import stats\n    from google.cloud import storage, aiplatform\n    import pickle\n    import os\n\n\n    df = pd.read_csv(df_path.path)\n    train_df = pd.read_csv(train_df_path.path)\n    print ('df shape: ', df.shape)\n    print ('train df shape ', train_df.shape)\n    client = storage.Client(project=projectid)\n    models = aiplatform.Model.list(filter=f'display_name={model_name}',\n                                    order_by=\"update_time\",\n                                    location='us-central1')\n    if len(models) > 0:\n        latest_model = models[-1]\n        model_path = latest_model.to_dict()['artifactUri']\n        path_split = model_path.split('/')\n        bucket_name = path_split[2]\n        blob_name = os.path.join('/'.join(path_split[3:]), training_dist_dict_name)\n        bucket = client.get_bucket(bucket_name)\n        blob = bucket.blob(blob_name)\n        blob.download_to_filename(training_dist_dict_name)\n        with open(training_dist_dict_name, 'rb') as file:\n            train_dist_dict = pickle.load(file)\n        cols_req = list(train_dist_dict.keys())\n        for col in cols_req:\n            df[col] = df.groupby('analysis_neighborhood', group_keys = False)[col].apply(lambda x: x.fillna(x.mean()))\n            pvalue = stats.ks_2samp(train_df[col], df[col]).pvalue\n            col_avg = df[col].mean()\n            col_std = df[col].std()\n            if pvalue < 0.05:\n                print (f'Column {col} is experiencing Drift')\n            eval_output.log_metric(f'{col}_pvalue', pvalue)\n            eval_output.log_metric(f'{col}_average', col_avg)\n            eval_output.log_metric(f'{col}_std_dev', col_std)\n        eval_output.log_metric('Accuracy', accuracy)\n\n@dsl.pipeline(\n    name='SF Crime Monitor pipeline',\n    description= 'Monitoring')\ndef sf_pipeline_monitor(projectid:str =projectid, dbname:str =datasetid, tablename:str =tablename,\n                       train_df_name:str =train_df_name, model_name:str =model_name):\n    read_data_monitor_task = read_bq_data(projectid= projectid, db= datasetid,\n                                     tablename= tablename, train_data_filename= train_df_name,\n                                     model_name=model_name)\n    get_accuracy_task = get_accuracy(df_path= read_data_monitor_task.outputs['output_df_path'])\n    compare_distributions_task = get_and_compare_distributions(train_df_path= read_data_monitor_task.outputs['train_df_path'],\n                                    accuracy=get_accuracy_task.output,\n                                    df_path= read_data_monitor_task.outputs['output_df_path'],\n                                    projectid= projectid, training_dist_dict_name='data_drift_dict.pkl',\n                                    model_name=model_name)\n    \n    acc = get_accuracy_task.output\n    with dsl.If(acc < 1):\n        sf_training_pipeline(projectid= projectid, datasetid= datasetid, tablename= tablename,\n                        label_name= label_name, test_size=test_size, \n                        best_split_tries=best_split_tries, float_cols= float_cols, \n                        cat_cols= cat_cols, binary_cols= binary_cols, bucket_name= bucket_name,\n                        accuracy_threshold= accuracy_threshold, model_name= model_name)\n\nkfp.compiler.Compiler().compile(\n    pipeline_func=sf_pipeline_monitor,\n    package_path='new_yamls/monitoring.yaml')\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='Kubeflow pipeline to run monitoring script and retrain if required.')\n    parser.add_argument('type', help='Enter \"yaml\" if file needs to be saved or \"run\" if pipline needs to be run directly',\n                       type=str)\n    args = parser.parse_args()\n    if args.type == 'yaml':\n        if not os.path.exists('new_yamls'):\n            os.mkdir('new_yamls')\n        kfp.compiler.Compiler().compile(\n            pipeline_func=sf_training_pipeline,\n            package_path='new_yamls/monitoring_pipeline.yaml')\n    elif args.type == 'run':\n        client = Client(host= host_url)\n        experiments = client.list_experiments().experiments\n        experiment_names = list(map(lambda x: x.display_name, experiments))\n        if training_experiment_name not in experiment_names:\n            client.create_experiment(name=monitoring_experiment_name)\n        client.create_run_from_pipeline_func(\n        pipeline_func=sf_pipeline_monitor,\n        experiment_name=monitoring_experiment_name)"
  },
  {
    "repo": "shawar8/sfcrime-prediction-kubeflow",
    "file_path": "serving_fns.py",
    "raw_url": "https://raw.githubusercontent.com/shawar8/sfcrime-prediction-kubeflow/main/serving_fns.py",
    "content": "import os\nimport argparse\nfrom variables import *\nfrom common_fns import *\nfrom data_ingest_fns import preprocess_data, map_label, merge_with_nfl, add_dummy_cols\nfrom kfp.dsl.structures import yaml\nfrom kfp.client import Client\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import component\nfrom kfp.v2.dsl import (\n    Input,\n    Output,\n    Artifact,\n    Dataset,\n    Model,\n    Metrics\n)\n\n@component(\n    base_image='python:3.9',\n    packages_to_install=['sodapy', 'pandas'],\n    output_component_file='new_yamls/collect_batch_data.yaml'\n)\ndef get_data(serving_data_path: Output[Dataset]):\n    from datetime import datetime, timedelta\n    from sodapy import Socrata\n    import pandas as pd\n    import pytz\n\n    client = Socrata('data.sfgov.org', None)\n    police_dataset_id = 'wg3w-h783'\n    date_req = (datetime.now(pytz.timezone('US/Pacific')) - timedelta(days=2)).strftime(\"%Y-%m-%dT00:00:00\")\n    results = client.get(police_dataset_id, query= f'select * where incident_date = \"{date_req}\"')\n    if len(results) > 0:\n        df = pd.DataFrame(results).drop('point', axis = 1).drop_duplicates('incident_id')\n        # df['filed_online'] = 1\n        df.to_csv(serving_data_path.path, index=False)\n\n    else:\n        print ('No Serving Data')\n\n\n@component(\n    base_image='python:3.9',\n    packages_to_install=['pandas', 'google-cloud-storage', 'scikit-learn'],\n    output_component_file = 'new_yamls/collect_batch_data.yaml'\n)\ndef transform_cat_label_data(df_path: Input[Dataset], cat_encoder_path: str,\n                             label_encoder_path: str, projectid: str,\n                             bucket_name: str, blob_path_cat_enc: str,\n                             blob_path_lab_enc: str, df_output_path: Output[Dataset],\n                             label_output_path: Output[Artifact]):\n    import pandas as pd\n    import pickle\n    from google.cloud import storage\n\n    client = storage.Client(project=projectid)\n    bucket = client.get_bucket(bucket_name)\n    blob_cat = bucket.blob(blob_path_cat_enc)\n    blob_lab_enc = bucket.blob(blob_path_lab_enc)\n    blob_cat.download_to_filename(cat_encoder_path)\n    blob_lab_enc.download_to_filename(label_encoder_path)\n\n    with open(label_encoder_path, 'rb') as file:\n        lab_enc = pickle.load(file)\n\n    with open(cat_encoder_path, 'rb') as file:\n        cat_enc = pickle.load(file)\n\n    df = pd.read_csv(df_path.path)\n    labels = df.pop('incident_category')\n    for col in cat_enc.keys():\n        df[col] = df[col].astype(str)\n        new_values_index = df.loc[~df[col].isin(cat_enc[col].classes_)].index\n        print (new_values_index)\n        if len(new_values_index) > 0:\n            print (f'New values found in {col} column: {df.loc[new_values_index, col].values.tolist()}')\n        df.loc[new_values_index, col] = 'UNK'\n        df[col] = cat_enc[col].transform(df[col])\n    labels = lab_enc.transform(labels)\n    df.to_csv(df_output_path.path)\n    with open(label_output_path.path, 'wb') as file:\n        pickle.dump(labels, file)\n\n@component(\n    base_image='python:3.9',\n    packages_to_install=['scikit-learn', 'pandas', 'google-cloud-storage',\n                         'google-cloud-aiplatform', 'xgboost', 'pyarrow'],\n    output_component_file='new_yamls/predict_serving.yaml'\n)\ndef serving_predict(serving_data_path: Input[Dataset], labels_path: Input[Artifact],\n                    model_name: str, req_columns: list, output_df_path: Output[Dataset],\n                    projectid: str, cat_cols: list, float_cols: list,\n                    binary_cols: list):\n    from xgboost import XGBClassifier\n    import pandas as pd\n    from google.cloud import storage, aiplatform\n    import pickle\n    import os\n\n    models = aiplatform.Model.list(\n    filter=f'display_name={model_name}',\n    order_by=\"update_time\",\n    location='us-central1')\n    model = XGBClassifier()\n    if len(models) > 0:\n        serving_data = pd.read_csv(serving_data_path.path)[binary_cols+float_cols+cat_cols+req_columns]\n        with open(labels_path.path, 'rb') as file:\n            y_labels = pickle.load(file)\n        latest_model = models[-1]\n        model_path = latest_model.to_dict()['artifactUri']\n        path_split = model_path.split('/')\n        bucket_name = path_split[2]\n        blob_name = os.path.join('/'.join(path_split[3:]), 'model.pkl')\n        client = storage.Client(project=projectid)\n        bucket = client.get_bucket(bucket_name)\n        blob = bucket.blob(blob_name)\n        blob.download_to_filename('model.pkl')\n        model.load_model('model.pkl')\n        y_pred = model.predict(serving_data.drop(req_columns, axis = 1).values)\n        serving_data['Prediction'] = y_pred\n        serving_data['incident_category'] = y_labels\n        serving_data['data_type'] = 'non_ingest'\n        serving_data.to_csv(output_df_path.path, index=False)\n\n@dsl.pipeline(\n    name='SF Crime predict pipeline',\n    description= 'Reading data, using the same transformations and predict')\ndef sf_pipeline_predict(projectid:str =projectid, bucket_name:str =bucket_name, model_name:str =model_name,\n                       float_cols:list =float_cols, cat_cols:list =cat_cols, binary_cols:list =binary_cols,\n                       label_name:str =label_name, datasetid:str = datasetid, tableid:str = tablename, schema:dict =schema):\n    get_serving_data_task = get_data()\n    serving_preprocess_task = preprocess_data(df_path= get_serving_data_task.outputs['serving_data_path'], data='serving',\n                                           to_keep = ['incident_id', 'police_district', 'incident_datetime',\n                                      'latitude', 'longitude', 'filed_online', 'incident_day_of_week',\n                                      'report_datetime', 'analysis_neighborhood', 'supervisor_district',\n                                      'incident_category'])\n    map_serving_label_task = map_label(df_path= serving_preprocess_task.outputs['df_output_path'])\n    serving_merge_nfl_task = merge_with_nfl(sf_dataset = map_serving_label_task.outputs['mapped_df_path'])\n    insert_dummy_cols_task = add_dummy_cols(df_path= serving_merge_nfl_task.outputs['merged_csv'])\n    post_proc_serve_task = post_processing_after_split(input_df_path= insert_dummy_cols_task.outputs['output_df'])\n    transform_cat_label_task = transform_cat_label_data(df_path=post_proc_serve_task.outputs['output_df_path'],\n                                                        cat_encoder_path='cat_lab_encoder.pkl',\n                                                        label_encoder_path='target_lab_encoder.pkl',\n                                                        projectid=projectid,\n                                                        bucket_name=bucket_name,\n                                                        blob_path_cat_enc='cat_lab_encoder.pkl',\n                                                        blob_path_lab_enc='target_lab_encoder.pkl')\n    predict_serving_task = serving_predict(serving_data_path=transform_cat_label_task.outputs['df_output_path'],\n            labels_path = transform_cat_label_task.outputs['label_output_path'], model_name=model_name, \n            req_columns = ['incident_id', 'report_datetime','incident_datetime', 'latitude', 'year', 'longitude', 'datetime', \n                           'incident_date', 'incident_time', 'gameday', 'gametime'],\n            projectid=projectid, float_cols= float_cols, cat_cols= cat_cols, binary_cols= binary_cols)\n\n    \n    decode_serve_columns_task = decode_cols(df_path= predict_serving_task.outputs['output_df_path'], \n                                            projectid= projectid, bucket_name= bucket_name,\n                target_col= label_name, cat_enc_path= 'cat_lab_encoder.pkl', label_enc_path= 'target_lab_encoder.pkl', \n                                            cols_to_decode= ['police_district', 'incident_day_of_week',\n                                            'analysis_neighborhood', 'supervisor_district', 'month', 'hour',\n                                            'latitude_buckets', 'longitude_buckets'])\n\n    upload_serving_task = upload_to_bq(df_path = decode_serve_columns_task.outputs['output_df'], projectid= projectid,\n                                datasetid= datasetid, tableid= tableid, location='US',\n                                schema = schema)\n    \nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='Kubeflow pipeline to run inference on new data.')\n    parser.add_argument('type', help='Enter \"yaml\" if file needs to be saved or \"run\" if pipline needs to be run directly',\n                       type=str)\n    args = parser.parse_args()\n    if args.type == 'yaml':\n        if not os.path.exists('new_yamls'):\n            os.mkdir('new_yamls')\n        kfp.compiler.Compiler().compile(\n            pipeline_func=sf_training_pipeline,\n            package_path='new_yamls/serving_pipeline.yaml')\n    elif args.type == 'run':\n        client = Client(host= host_url)\n        experiments = client.list_experiments().experiments\n        experiment_names = list(map(lambda x: x.display_name, experiments))\n        if serving_experiment_name not in experiment_names:\n            client.create_experiment(name=serving_experiment_name)\n        print ('HERE!!')\n        client.create_run_from_pipeline_func(\n        pipeline_func=sf_pipeline_predict,\n        experiment_name=serving_experiment_name)"
  },
  {
    "repo": "shawar8/sfcrime-prediction-kubeflow",
    "file_path": "training_functions.py",
    "raw_url": "https://raw.githubusercontent.com/shawar8/sfcrime-prediction-kubeflow/main/training_functions.py",
    "content": "import argparse\nfrom variables import *\nfrom common_fns import *\nfrom datetime import datetime, timedelta\nimport pytz\nimport kfp\nfrom kfp.dsl.structures import yaml\nfrom kfp.client import Client\nimport os\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import component\nfrom kfp.v2.dsl import (\n    Input,\n    Output,\n    Artifact,\n    Dataset,\n    Model,\n    Metrics\n)\n\n\n@component(\n    base_image='python:3.9',\n    packages_to_install=['numpy', 'scikit-learn', 'pandas', 'scipy'],\n    output_component_file='new_yamls/split_train_test.yaml')\ndef get_train_test_split(df_path: Input[Dataset], label_column: str, test_size: float, n_tries: int,\n                        output_x_train: Output[Dataset], output_x_test: Output[Dataset],\n                        output_y_train: Output[Artifact], output_y_test: Output[Artifact],\n                        divergence_output_dict: Output[Artifact]):\n    import numpy as np\n    from sklearn.model_selection import train_test_split\n    import pickle\n    import pandas as pd\n    import scipy\n\n    def get_kolmogorov_smiron(train_df, test_df, col_name):\n        train_df[col_name] = train_df.groupby('analysis_neighborhood', group_keys = False)[col_name].apply(lambda x: x.fillna(x.mean()))\n        test_df[col_name] = test_df.groupby('analysis_neighborhood', group_keys = False)[col_name].apply(lambda x: x.fillna(x.mean()))\n        statistic = scipy.stats.ks_2samp(train_df[col_name], test_df[col_name]).statistic\n        return statistic\n\n    df = pd.read_csv(df_path.path)\n    Y = df.pop(label_column)\n    cont_cols = ['diff', 'till_game_time', 'latitude', 'longitude']\n    results = []\n    for random_state in range(n_tries):\n        x_train, x_test, y_train, y_test = train_test_split(df, Y, test_size = test_size, random_state = random_state)\n        distances = dict(map(lambda a: (a, get_kolmogorov_smiron(x_train, x_test, a)), cont_cols))\n        results.append((random_state, distances))\n\n    ks_best = min(results, key=lambda x: np.mean(list(x[1].values())))\n    best_seed = ks_best[0]\n    ks_dict = ks_best[1]\n    x_train, x_test, y_train, y_test = train_test_split(df, Y, test_size = test_size, random_state = best_seed)\n\n    y_train = np.array(y_train)\n    y_test = np.array(y_test)\n    x_train.to_csv(output_x_train.path, index=False)\n    x_test.to_csv(output_x_test.path, index=False)\n    with open(output_y_train.path, 'wb') as file:\n        pickle.dump(y_train, file)\n    with open(output_y_test.path, 'wb') as file:\n        pickle.dump(y_test, file)\n    with open(divergence_output_dict.path, 'wb') as file:\n        pickle.dump(ks_dict, file)\n\n\n@component(\n    base_image='python:3.9',\n    packages_to_install=['scikit-learn', 'pandas', 'numpy'],\n    output_component_file='new_yamls/prep_data_for_training.yaml')\ndef prepare_data_for_training(x_train_input_path: Input[Dataset], x_test_input_path: Input[Dataset],\n                              float_cols: list, cat_cols: list, binary_cols: list,\n                              x_train_output_path: Output[Dataset], x_test_output_path: Output[Dataset],\n                              cat_label_en_dict_out: Output[Artifact]):\n\n    import pandas as pd\n    import pickle\n    import numpy as np\n    from sklearn.preprocessing import LabelEncoder\n\n    req_columns = ['incident_id', 'report_datetime', 'incident_datetime', 'latitude',\n                   'longitude', 'datetime', 'incident_date', 'year',\n                   'incident_time', 'gameday', 'gametime', 'data_type']\n    cat_lab_encoder_dict = {}\n    x_train = pd.read_csv(x_train_input_path.path)\n    x_test = pd.read_csv(x_test_input_path.path)\n\n    X_TRAIN = pd.DataFrame(x_train[binary_cols + float_cols + req_columns])\n    X_TEST = pd.DataFrame(x_test[binary_cols + float_cols + req_columns])\n\n\n    for col in cat_cols:\n        x_train[col] = x_train[col].astype(str)\n        x_test[col] = x_test[col].astype(str)\n        label_enc = LabelEncoder()\n        unique_vals = np.append(x_train[col].unique(), 'UNK')\n        x_test.loc[~x_test[col].isin(unique_vals), col] = 'UNK'\n        label_enc.fit(unique_vals)\n        X_TRAIN[col] = label_enc.transform(x_train[col])\n        X_TEST[col] = label_enc.transform(x_test[col])\n        cat_lab_encoder_dict[col] = label_enc\n\n    with open(cat_label_en_dict_out.path, 'wb') as file:\n        pickle.dump(cat_lab_encoder_dict, file)\n\n    X_TRAIN.to_csv(x_train_output_path.path, index=False)\n    X_TEST.to_csv(x_test_output_path.path, index=False)\n\n@component(\n    base_image='python:3.9',\n    packages_to_install=['xgboost', 'pandas', 'scikit-learn', 'numpy', 'google-cloud-storage'],\n    output_component_file='new_yamls/training.yaml')\ndef train_model(x_train_input_path: Input[Dataset], y_train_input_path: Input[Artifact],\n                y_test_input_path: Input[Artifact], y_test_output_path: Output[Artifact],\n                y_train_output_path: Output[Artifact], model_output: Output[Artifact],\n                label_encoder_path: Output[Artifact], model_dict_path: Output[Artifact],\n                output_bucket: str, output_blob_name: str, projectid: str):\n    from xgboost import XGBClassifier\n    import pandas as pd\n    import pickle\n    from sklearn.preprocessing import LabelEncoder\n    import numpy as np\n    from datetime import datetime\n    from google.cloud import storage\n\n    model_dict = {}\n    req_columns = ['incident_id', 'report_datetime', 'incident_datetime', 'latitude',\n                   'longitude', 'datetime', 'incident_date', 'year',\n                   'incident_time', 'gameday', 'gametime', 'data_type']\n    now = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    client = storage.Client(project=projectid)\n    bucket = client.get_bucket(output_bucket)\n    output_blob_name = f'models/{output_blob_name}_{now}'\n    blob = bucket.blob(f'{output_blob_name}/model.pkl')\n    gcs_folder_path = f'gs://{output_bucket}/{output_blob_name}'\n\n    x_train = pd.read_csv(x_train_input_path.path)\n    with open(y_train_input_path.path, 'rb') as file:\n        y_train = pickle.load(file)\n\n    with open(y_test_input_path.path, 'rb') as file:\n        y_test = pickle.load(file)\n\n    label_enc_target = LabelEncoder()\n    y_train = np.asarray(label_enc_target.fit_transform(y_train))\n    y_test = np.asarray(label_enc_target.transform(y_test))\n\n    with open(y_train_output_path.path, 'wb') as file:\n        pickle.dump(y_train, file)\n\n    with open(y_test_output_path.path, 'wb') as file:\n        pickle.dump(y_test, file)\n\n    with open(label_encoder_path.path, 'wb') as file:\n        pickle.dump(label_enc_target, file)\n\n    model = XGBClassifier(learning_rate = 0.03, tree_method='hist', n_estimators = 3,\n                          objective='multi:softmax')\n    clf = model.fit(x_train.drop(req_columns, axis = 1), y_train)\n\n    model_dict['create_time'] = now\n    model_dict['uri'] = gcs_folder_path\n\n    clf.save_model(model_output.path)\n\n    with open(model_dict_path.path, 'wb') as file:\n        pickle.dump(model_dict, file)\n\n    blob.upload_from_filename(model_output.path)\n\n@component(\n    base_image='python:3.9',\n    packages_to_install=['xgboost', 'pandas', 'scikit-learn', 'numpy'],\n    output_component_file='new_yamls/predict.yaml')\n    # base_image='python:3.10.2')\ndef predict(x_train_input_path: Input[Dataset], x_test_input_path: Input[Dataset],\n            y_train_input_path: Input[Dataset], y_test_input_path: Input[Artifact],\n            model_path: Input[Artifact], float_cols: list, bq_data_path: Output[Dataset],\n            eval_metrics: Output[Artifact], stats_dict_output: Output[Artifact]):\n    import pandas as pd\n    import pickle\n    import numpy as np\n    from xgboost import XGBClassifier\n\n    model = XGBClassifier()\n    model.load_model(model_path.path)\n    req_columns = ['incident_id', 'report_datetime', 'incident_datetime', 'latitude',\n                   'longitude', 'datetime', 'incident_date', 'year',\n                   'incident_time', 'gameday', 'gametime', 'data_type']\n\n    metrics = {}\n    x_train = pd.read_csv(x_train_input_path.path)\n    x_test = pd.read_csv(x_test_input_path.path)\n    with open(y_train_input_path.path, 'rb') as file:\n        y_train = pickle.load(file)\n    with open(y_test_input_path.path, 'rb') as file:\n        y_test = pickle.load(file)\n\n    y_pred_train = model.predict(x_train.drop(req_columns, axis = 1))\n    y_pred_test = model.predict(x_test.drop(req_columns, axis = 1))\n\n    stats_dict = x_train[float_cols].describe().to_dict()\n\n    x_train['incident_category'] = y_train\n    x_train['Prediction'] = y_pred_train\n    x_test['incident_category'] = y_test\n    x_test['Prediction'] = y_pred_test\n\n    train_accuracy = np.mean(y_pred_train == y_train)\n    test_accuracy = np.mean(y_pred_test == y_test)\n    metrics['train_accuracy'] = train_accuracy\n    metrics['test_accuracy'] = test_accuracy\n\n    bq_data = pd.concat([x_train, x_test], axis = 0)\n    bq_data = bq_data.query('data_type == \"ingest\"')\n    if bq_data.shape[0] > 0:\n        bq_data['data_type'] = 'non_ingest'\n    bq_data.to_csv(bq_data_path.path, index=False)\n    with open(eval_metrics.path, 'wb') as file:\n        pickle.dump(metrics, file)\n    with open(stats_dict_output.path, 'wb') as file:\n        pickle.dump(stats_dict, file)\n\n@component(\n    base_image='python:3.9',\n    packages_to_install= ['google-cloud-aiplatform', 'xgboost', 'scikit-learn',\n                          'google-cloud-storage', 'pandas', 'pyarrow'],\n    output_component_file= 'new_yamls/upload_model_registry.yaml')\n    # base_image='python:3.10.2')\ndef upload_model_registry(train_df_path: Input[Artifact], model_path: Input[Artifact], eval_metrics: Input[Artifact],\n                          ks_dict_path: Input[Artifact], model_dict_path: Input[Artifact], projectid: str,\n                          region: str, accuracy_threshold:float, model_name: str, bucket_name: str,\n                          stats_dict_input:Input[Artifact], eval_output: Output[Metrics],\n                          output_train_name: str):\n    from google.cloud import aiplatform, storage\n    import pickle\n    import os\n    from xgboost import XGBClassifier\n    import pandas as pd\n    aiplatform.init(project=projectid, location=region)\n\n    with open(eval_metrics.path, 'rb') as file:\n        accuracy = pickle.load(file)\n        test_accuracy = accuracy['test_accuracy']\n        train_accuracy = accuracy['train_accuracy']\n\n    eval_output.log_metric('train accuracy', train_accuracy)\n    eval_output.log_metric('test accuracy', test_accuracy)\n    with open(model_dict_path.path, 'rb') as file:\n        model_dict = pickle.load(file)\n\n    train_df = pd.read_csv(train_df_path.path)\n    train_df.to_parquet(output_train_name, compression='gzip')\n    client = storage.Client(project=projectid)\n    uri = model_dict['uri']\n    print (uri)\n    uri_folder = '/'.join(uri.split('/')[-2:])\n    print ('uri folder: ', uri_folder)\n    bucket = client.get_bucket(bucket_name)\n    train_df_blob = bucket.blob(f'{uri_folder}/{output_train_name}')\n    drift_blob = bucket.blob(f'{uri_folder}/data_drift_dict.pkl')\n    metrics_blob = bucket.blob(f'{uri_folder}/metrics_dict.pkl')\n    stats_dict_blob = bucket.blob(f'{uri_folder}/stats_dict.pkl')\n    model_param_blob = bucket.blob(f'{uri_folder}/model_params.pkl')\n    container_image = 'us-docker.pkg.dev/vertex-ai/prediction/xgboost-cpu.1-7:latest'\n\n    if test_accuracy > accuracy_threshold:\n        print ('Model Performance satisfactory: Saving')\n        model = XGBClassifier()\n        model.load_model(model_path.path)\n        params = model.get_params()\n        present_models = aiplatform.Model.list(order_by= 'update_time', location= region,)\n        print (present_models)\n        if len(present_models) > 0:\n            previous_model = present_models[-1].resource_name\n            uploaded_model = aiplatform.Model.upload(\n                                    display_name = model_name,\n                                    artifact_uri = uri,\n                                    serving_container_image_uri= container_image,\n                                    parent_model = previous_model,\n                                    is_default_version = True)\n        else:\n            uploaded_model = aiplatform.Model.upload(\n                                    display_name = model_name,\n                                    artifact_uri = uri,\n                                    serving_container_image_uri= container_image)\n        with open('model_params.pkl', 'wb') as file:\n            pickle.dump(params, file)\n        drift_blob.upload_from_filename(ks_dict_path.path)\n        metrics_blob.upload_from_filename(eval_metrics.path)\n        stats_dict_blob.upload_from_filename(stats_dict_input.path)\n        model_param_blob.upload_from_filename('model_params.pkl')\n        train_df_blob.upload_from_filename(output_train_name)\n\n\n    else:\n        print ('Model Performance not satisfactory: Not saving')\n\n@component(\n    base_image='python:3.9',\n    packages_to_install= ['google-cloud-bigquery'],\n    output_component_file= 'new_yamls/delete_bq_data.yaml'\n)\ndef delete_bq_data(df: Input[Dataset], projectid: str, datasetid: str, tableid: str):\n    from google.cloud import bigquery as bq\n\n    client = bq.Client(project=projectid)\n    q = f'DELETE from {datasetid}.{tableid} where data_type = \"ingest\"'\n    job = client.query(q)\n    job.result()\n\n@dsl.pipeline(\n    name='SF Training Pipeline',\n    description= 'Postprocessing data, Training the model and uploading to registry')\n\ndef sf_training_pipeline(projectid:str = projectid, datasetid:str = datasetid, tablename:str = tablename,\n                        label_name:str = label_name, test_size:float =test_size, best_split_tries:int =best_split_tries,\n                        float_cols:list = float_cols, cat_cols:list = cat_cols, binary_cols:list = binary_cols,\n                        bucket_name:str = bucket_name, accuracy_threshold:float = accuracy_threshold,\n                        model_name:str = model_name):\n    get_data_task = read_training_data(url = '', method='training', projectid=projectid, datasetid=datasetid,\n                                       tablename=tablename)\n    split_data_task = get_train_test_split(df_path = get_data_task.outputs['output_csv'], label_column = label_name,\n                                           test_size = test_size, n_tries=best_split_tries)\n    post_proc_train_task = post_processing_after_split(input_df_path= split_data_task.outputs['output_x_train'])\n\n    post_proc_test_task = post_processing_after_split(input_df_path= split_data_task.outputs['output_x_test'])\n\n    data_prep_for_train_task = prepare_data_for_training(x_train_input_path= post_proc_train_task.outputs['output_df_path'],\n                              x_test_input_path= post_proc_test_task.outputs['output_df_path'],\n                              float_cols= float_cols,\n                              cat_cols= cat_cols,\n                              binary_cols= binary_cols)\n\n    upload_cat_lab_enc_task = upload_artifact(artifact_path= data_prep_for_train_task.outputs['cat_label_en_dict_out'],\n                                              projectid= projectid, bucket_name= bucket_name,\n                                              output_path= 'cat_lab_encoder.pkl')\n\n    train_task = train_model(x_train_input_path= data_prep_for_train_task.outputs['x_train_output_path'],\n                             y_train_input_path= split_data_task.outputs['output_y_train'],\n                             y_test_input_path = split_data_task.outputs['output_y_test'],\n                             output_bucket= bucket_name, output_blob_name= 'models',\n                             projectid= projectid)\n\n    upload_target_label_enc = upload_artifact(artifact_path= train_task.outputs['label_encoder_path'],\n                                              projectid= projectid, bucket_name= bucket_name,\n                                              output_path= 'target_lab_encoder.pkl')\n\n    predict_task = predict(x_train_input_path= data_prep_for_train_task.outputs['x_train_output_path'],\n                           y_train_input_path= train_task.outputs['y_train_output_path'],\n                           model_path= train_task.outputs['model_output'],\n                           x_test_input_path= data_prep_for_train_task.outputs['x_test_output_path'],\n                           y_test_input_path= train_task.outputs['y_test_output_path'],\n                           float_cols = float_cols)\n\n    categ_decode = decode_cols(df_path= predict_task.outputs['bq_data_path'], projectid='ml-deployments',\n                               bucket_name= bucket_name, cat_enc_path= 'cat_lab_encoder.pkl',\n                               label_enc_path= 'target_lab_encoder.pkl', target_col = label_name,\n                               cols_to_decode= cat_cols)\n\n    upload_to_registry_task = upload_model_registry(train_df_path= data_prep_for_train_task.outputs['x_train_output_path'],\n                          model_path= train_task.outputs['model_output'], \n                          stats_dict_input = predict_task.outputs['stats_dict_output'],\n                          eval_metrics= predict_task.outputs['eval_metrics'],\n                          model_dict_path= train_task.outputs['model_dict_path'], projectid= projectid,\n                          region= 'us-central1', accuracy_threshold= accuracy_threshold, model_name= model_name,\n                          ks_dict_path= split_data_task.outputs['divergence_output_dict'],\n                          bucket_name=bucket_name, output_train_name= 'train_df.parquet.gzip')\n\n    delete_duplicate_task= delete_bq_data(df = categ_decode.outputs['output_df'], projectid= projectid, \n                                          datasetid= datasetid, tableid= tablename)\n\n    upload_task = upload_to_bq(df_path = categ_decode.outputs['output_df'], projectid= projectid,\n                                datasetid= datasetid, tableid= tablename, location='US',\n                                schema = schema)\n\n    \nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='Kubeflow pipeline to run ingestion script for SF Crime data.')\n    parser.add_argument('type', help='Enter \"yaml\" if file needs to be saved or \"run\" if pipline needs to be run directly',\n                       type=str)\n    parser.add_argument('--test_size', type=float, help='Value for size of the test set, default=0.3')\n    parser.add_argument('--best_split_tries', type=int, help='Number of attempts at splitting the data to get the most similar distributions between train and test, default=5')\n    parser.add_argument('--accuracy_threshold', type=float, help='Minimum test set accuracy required to push the new model to registry, default=0.3')\n    args = parser.parse_args()\n    if args.type == 'yaml':\n        if not os.path.exists('new_yamls'):\n            os.mkdir('new_yamls')\n        kfp.compiler.Compiler().compile(\n            pipeline_func=sf_training_pipeline,\n            package_path='new_yamls/training_pipeline.yaml')\n    elif args.type == 'run':\n        print ('Here')\n        args = {key: value for (key,value) in vars(args).items() if (value != None and key != 'type')}\n        print (args)\n        client = Client(host= host_url)\n        experiments = client.list_experiments().experiments\n        experiment_names = list(map(lambda x: x.display_name, experiments))\n        if training_experiment_name not in experiment_names:\n            client.create_experiment(name=training_experiment_name)\n        client.create_run_from_pipeline_func(\n        pipeline_func=sf_training_pipeline,\n        experiment_name=training_experiment_name,\n        arguments=args)"
  },
  {
    "repo": "Davidnet/breast-cancer-detection-nnx-pipeline",
    "file_path": "pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/Davidnet/breast-cancer-detection-nnx-pipeline/master/pipeline.py",
    "content": "from google_cloud_pipeline_components.v1.custom_job import (\n    create_custom_training_job_from_component,\n)\nfrom kfp import dsl\nfrom kfp.dsl import (\n    Artifact,\n    ContainerSpec,\n    Input,\n    Output,\n    component,\n    container_component,\n)\n\n\n@container_component\ndef download_dataset(\n    dataset: Output[Artifact],\n):\n    return ContainerSpec(\n        image=\"davidnet/cbis_ddsm_dataloader:1.0.2\",\n        command=[\"python\", \"setup.py\"],\n        args=[\n            \"--output\",\n            dataset.path,\n        ],\n    )\n\n\ndownload_dataset_vertex = create_custom_training_job_from_component(\n    download_dataset, display_name=\"Download Dataset\", boot_disk_size_gb=600\n)\n\n\n@component(\n    base_image=\"python:3.12\",\n    packages_to_install=[\n        \"tensorflow-datasets==4.9.6\",\n        \"opencv-python-headless==4.10.0.84\",\n        \"tensorflow==2.17.0\",\n    ],\n)\ndef create_tf_records(\n    dataset: Input[Artifact],\n    tf_records: Output[Artifact],\n):\n    import os\n    import tarfile\n\n    import tensorflow_datasets as tfds\n\n    with tarfile.open(dataset.path) as tar:\n        # Extract all contents to the specified directory\n        tar.extractall(path=\"./extracted_dataset\")\n    curated_breast_imaging_ddsm = tfds.builder(\"curated_breast_imaging_ddsm\")\n    curated_breast_imaging_ddsm.download_and_prepare(\n        download_config=tfds.download.DownloadConfig(manual_dir=\"./extracted_dataset\")\n    )\n    datasets = curated_breast_imaging_ddsm.as_dataset()\n    # ~/tensorflow_datasets/curated_breast_imaging_ddsm/patches/3.0.0\n    folder_path = os.path.join(\n        os.path.expanduser(\"~\"),\n        \"tensorflow_datasets\",\n        \"curated_breast_imaging_ddsm\",\n        \"patches\",\n        \"3.0.0\",\n    )\n    with tarfile.open(tf_records.path, \"w\") as tar:\n        tar.add(folder_path, arcname=os.path.basename(folder_path))\n\n\ncreate_tf_records_vertex = create_custom_training_job_from_component(\n    create_tf_records, display_name=\"Create TF Records\", boot_disk_size_gb=600\n)\n\n\n@container_component\ndef train(\n    tf_records: Input[Artifact],\n    train_steps: int,\n    eval_every: int,\n    batch_size: int,\n    learning_rate: float,\n    momentum: float,\n    checkpoints: Output[Artifact],\n):\n    return ContainerSpec(\n        image=\"davidnet/flax-cnn-model:1.0.0\",\n        command=[\"python\", \"train.py\"],\n        args=[\n            \"--train-steps\",\n            str(train_steps),\n            \"--eval-every\",\n            str(eval_every),\n            \"--batch-size\",\n            str(batch_size),\n            \"--learning-rate\",\n            str(learning_rate),\n            \"--momentum\",\n            str(momentum),\n            \"--output\",\n            checkpoints.path,\n            \"--tfrecords\",\n            tf_records.path,\n        ],\n    )\n\n\ntrain_vertex = create_custom_training_job_from_component(\n    train,\n    display_name=\"Train Model\",\n    boot_disk_size_gb=600,\n    machine_type=\"n1-standard-4\",\n    accelerator_type=\"NVIDIA_TESLA_T4\",\n    accelerator_count=1,\n)\n\n\n@dsl.pipeline(\n    name=\"Breast Cancer Classification\",\n    description=\"A pipeline that process and learns from images of the curated_breast_imaging_ddsm dataset\",\n    display_name=\"Breast Cancer Classification\",\n)\ndef breast_cancer_classification(\n    train_steps: int = 1200,\n    eval_every: int = 200,\n    batch_size: int = 4,\n    learning_rate: float = 0.005,\n    momentum: float = 0.9,\n):\n    download_task = download_dataset_vertex()\n    create_tf_records_task = create_tf_records_vertex(\n        dataset=download_task.outputs[\"dataset\"]\n    )\n    train_task = train_vertex(\n        train_steps=train_steps,\n        eval_every=eval_every,\n        batch_size=batch_size,\n        learning_rate=learning_rate,\n        momentum=momentum,\n        tf_records=create_tf_records_task.outputs[\"tf_records\"],\n    )\n\n\nif __name__ == \"__main__\":\n    import kfp.compiler as compiler\n\n    compiler.Compiler().compile(breast_cancer_classification, \"pipeline.yaml\")\n"
  },
  {
    "repo": "pavan-kumar-99/ml-ops",
    "file_path": "components/TestTrainSplit.py",
    "raw_url": "https://raw.githubusercontent.com/pavan-kumar-99/ml-ops/master/components/TestTrainSplit.py",
    "content": "import kfp\nimport kfp.dsl as dsl\nimport kfp.components as components\nfrom kfp.components import create_component_from_func\n\ndataset_op = components.load_component_from_url(\n    'https://raw.githubusercontent.com/pavan-kumar-99/ml-ops/master/components/getRawData.yaml'\n)\n\n@components.create_component_from_func\ndef SplitTrainTest():\n    from sklearn.model_selection import train_test_split\n    dataset_csv_op = dataset_op().output\n    dataset = pd.read_csv(dataset_csv_op)\n    X = dataset.iloc[:, :-1].values\n    y = dataset.iloc[:, 1].values\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 1/3, random_state = 0)\n    return X_train, X_test, y_train, y_test\n   \nif __name__ == '__main__':\n    create_component_from_func(func=SplitTrainTest,base_image='python:slim',output_component_file='./components/test-train.yaml')\n"
  },
  {
    "repo": "pavan-kumar-99/ml-ops",
    "file_path": "components/getDataFromRawUri.py",
    "raw_url": "https://raw.githubusercontent.com/pavan-kumar-99/ml-ops/master/components/getDataFromRawUri.py",
    "content": "import kfp\nimport kfp.dsl as dsl\nimport kfp.components as components\nfrom kfp.components import create_component_from_func\n\n\n@components.create_component_from_func\ndef GetRawData(val):\n    import requests\n    data = requests.get(val)\n    print(\"Extracted Raw Data\")\n    return data\n   \nif __name__ == '__main__':\n    create_component_from_func(func=GetRawData,base_image='python:slim',output_component_file='./components/getRawData.yaml')\n"
  },
  {
    "repo": "pavan-kumar-99/ml-ops",
    "file_path": "components/train-plot.py",
    "raw_url": "https://raw.githubusercontent.com/pavan-kumar-99/ml-ops/master/components/train-plot.py",
    "content": "import kfp\nimport kfp.dsl as dsl\nimport kfp.components as components\nfrom kfp.components import create_component_from_func\n\ntraintest_op = components.load_component_from_url(\n    'https://raw.githubusercontent.com/pavan-kumar-99/ml-ops/master/components/test-train.yaml'\n)\n\n@components.create_component_from_func\ndef TrainPlot():\n    from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, mean_squared_error, mean_absolute_error, explained_variance_score\n    from sklearn.linear_model import LinearRegression\n    regressor = LinearRegression()\n    regressor.fit(X_train, y_train)\n\n    y_pred = regressor.predict(X_test)\n\n    plt.scatter(X_train, y_train, color = 'red')\n    plt.plot(X_train, regressor.predict(X_train), color = 'blue')\n    plt.title('Salary vs Experience (Training set)')\n    plt.xlabel('Years of Experience')\n    plt.ylabel('Salary')\n    plt.show()\n\n    plt.plot(y_pred, y_test)\n\n    plt.scatter(X_test, y_test, color = 'red')\n    plt.plot(X_train, regressor.predict(X_train), color = 'blue')\n    plt.title('Salary vs Experience (Test set)')\n    plt.xlabel('Years of Experience')\n    plt.ylabel('Salary')\n    plt.show()\n\n    mse = mean_squared_error(y_test, y_pred)\n    mae = mean_absolute_error(y_test, y_pred)\n    evs = explained_variance_score(y_test, y_pred) \n\n\n    metrics = {\"mse\":mse,\n            \"mae\":mae,\n            \"evs\":evs\n    }\n\n    print(X_train)\n    print(params)\n    print(metrics)\n   \nif __name__ == '__main__':\n    create_component_from_func(func=TrainPlot,base_image='python:slim',output_component_file='./components/train-plot.yaml')\n"
  },
  {
    "repo": "pavan-kumar-99/ml-ops",
    "file_path": "pipelines/echo-pipelines/dependency-pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/pavan-kumar-99/ml-ops/master/pipelines/echo-pipelines/dependency-pipeline.py",
    "content": "import kfp\nimport kfp.dsl as dsl\n\nimport kfp.components as components\n\n@components.create_component_from_func\ndef printString(val):\n    print(val)\n\n@dsl.pipeline(\n  name='my-pipeline',\n  description='My ML Pipeline.'\n)\ndef file_passing_pipelines():\n    step1 = dsl.ContainerOp(\n        name=\"echo\",\n        image=\"alpine\",\n        command=[\"sh\", \"-c\"],\n        arguments=[\"echo Hi Kubeflow\"],\n    )\n    step2 = printString(\"Hello, From component\")\n    step2.after(step1)\n\n   \nif __name__ == '__main__':\n    # Compiling the pipeline\n    kfp.compiler.Compiler().compile(file_passing_pipelines, __file__ + '.yaml')\n"
  },
  {
    "repo": "pavan-kumar-99/ml-ops",
    "file_path": "pipelines/echo-pipelines/echo-sh.py",
    "raw_url": "https://raw.githubusercontent.com/pavan-kumar-99/ml-ops/master/pipelines/echo-pipelines/echo-sh.py",
    "content": "import kfp\nimport kfp.dsl as dsl\n\n\n@dsl.pipeline(\n  name='my-pipeline',\n  description='My ML Pipeline.'\n)\ndef file_passing_pipelines():\n    step1 = dsl.ContainerOp(\n        name=\"echo\",\n        image=\"alpine\",\n        command=[\"sh\", \"-c\"],\n        arguments=[\"echo Hi Kubeflow\"],\n    )\n\n   \nif __name__ == '__main__':\n    # Compiling the pipeline\n    kfp.compiler.Compiler().compile(file_passing_pipelines, __file__ + '.yaml')\n"
  },
  {
    "repo": "03sarath/Kubeflow-pipelines-mlops",
    "file_path": "pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/03sarath/Kubeflow-pipelines-mlops/main/pipeline.py",
    "content": "import kfp\nfrom kfp import dsl\nfrom kfp.components import func_to_container_op\n\n@func_to_container_op\ndef show_results(decision_tree : float, logistic_regression : float) -> None:\n    # Given the outputs from decision_tree and logistic regression components\n    # the results are shown.\n\n    print(f\"Decision tree (accuracy): {decision_tree}\")\n    print(f\"Logistic regression (accuracy): {logistic_regression}\")\n\n\n@dsl.pipeline(name='First Pipeline', description='Applies Decision Tree and Logistic Regression for classification problem.')\ndef first_pipeline():\n\n    # Loads the yaml manifest for each component\n    download = kfp.components.load_component_from_file('download_data/download_data.yaml')\n    decision_tree = kfp.components.load_component_from_file('decision_tree/decision_tree.yaml')\n    logistic_regression = kfp.components.load_component_from_file('logistic_regression/logistic_regression.yaml')\n\n    # Run download_data task\n    download_task = download()\n\n    # Run tasks \"decison_tree\" and \"logistic_regression\" given\n    # the output generated by \"download_task\".\n    decision_tree_task = decision_tree(download_task.output)\n    logistic_regression_task = logistic_regression(download_task.output)\n\n    # Given the outputs from \"decision_tree\" and \"logistic_regression\"\n    # the component \"show_results\" is called to print the results.\n    show_results(decision_tree_task.output, logistic_regression_task.output)\n\n\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(first_pipeline, 'FirstPipeline.yaml')\n    # kfp.Client().create_run_from_pipeline_func(basic_pipeline, arguments={})"
  },
  {
    "repo": "rakesh283343/kubeflow-pipelines-clv",
    "file_path": "pipelines/batch_predict_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/rakesh283343/kubeflow-pipelines-clv/master/pipelines/batch_predict_pipeline.py",
    "content": "# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#            http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Batch Predict Pipeline.\"\"\"\n\nfrom helper_components import load_sales_transactions\nfrom helper_components import prepare_feature_engineering_query\n\nimport kfp\nfrom kfp import gcp\n\nimport pathlib\nimport yaml\n\n# Load pipeline settings\ncompiler_settings = yaml.safe_load(\n    pathlib.Path('settings.yaml').read_text())['compiler_settings']\nargument_defaults = yaml.safe_load(\n    pathlib.Path('settings.yaml').read_text())['argument_defaults']\n\n# Initialize component store\ncomponent_store = kfp.components.ComponentStore(\n    compiler_settings['local_search_paths'],\n    compiler_settings['url_search_prefixes'])\n\n# Create component factories\nload_sales_transactions_op = kfp.components.func_to_container_op(\n    load_sales_transactions,\n    base_image=compiler_settings['lightweight_components_base_image'])\nprepare_feature_engineering_query_op = kfp.components.func_to_container_op(\n    prepare_feature_engineering_query,\n    base_image=compiler_settings['lightweight_components_base_image'])\nengineer_features_op = component_store.load_component('bigquery/query')\nbatch_predict_op = component_store.load_component('aml-batch-predict')\n\n\n# Define the batch predict pipeline\n@kfp.dsl.pipeline(\n    name='CLV Batch Predict',\n    description='CLV Batch Predict Pipeline using BigQuery for feature engineering and Automl Tables for batch inference'\n)\ndef clv_batch_predict(\n    project_id,\n    source_gcs_path,\n    staging_gcs_path,\n    source_bq_table,\n    bq_dataset_name,\n    threshold_date,\n    predict_end,\n    max_monetary,\n    aml_model_id,\n    destination_prefix,\n    features_table_name=argument_defaults['features_table_name'],\n    transactions_table_name=argument_defaults['transactions_table_name'],\n    dataset_location=argument_defaults['dataset_location'],\n    aml_compute_region=argument_defaults['aml_compute_region'],\n    query_template_uri=argument_defaults['query_template_uri']):\n  \"\"\"Prepares and scores sales transactions dataset.\"\"\"\n\n  # Load sales transactions\n  load_transactions = load_sales_transactions_op(\n      project_id=project_id,\n      source_gcs_path=source_gcs_path,\n      source_bq_table=source_bq_table,\n      dataset_location=dataset_location,\n      dataset_name=bq_dataset_name,\n      table_id=transactions_table_name)\n\n  # Generate the feature engineering query\n  prepare_query = prepare_feature_engineering_query_op(\n      project_id=project_id,\n      source_table_id=load_transactions.output,\n      destination_dataset=bq_dataset_name,\n      features_table_name=features_table_name,\n      threshold_date=threshold_date,\n      predict_end=predict_end,\n      max_monetary=max_monetary,\n      query_template_uri=query_template_uri)\n\n  # Run the feature engineering query on BigQuery.\n  engineer_features = engineer_features_op(\n      query=prepare_query.outputs['query'],\n      project_id=project_id,\n      dataset_id=prepare_query.outputs['dataset_name'],\n      table_id=prepare_query.outputs['table_name'],\n      output_gcs_path=staging_gcs_path,\n      dataset_location=dataset_location,\n      job_config='')\n\n  # Run the batch predict task on features in Big Query\n  source_data_uri = 'bq://{}.{}.{}'.format(\n      project_id, prepare_query.outputs['dataset_name'],\n      prepare_query.outputs['table_name'])\n\n  predict_batch = batch_predict_op(\n      project_id=project_id,\n      region=aml_compute_region,\n      model_id=aml_model_id,\n      datasource=source_data_uri,\n      destination_prefix=destination_prefix)\n\n  predict_batch.after(engineer_features)\n\n  # Configure the pipeline to use a service account secret\n  if compiler_settings['use_sa_secret']:\n    steps = [load_transactions, prepare_query, engineer_features, predict_batch]\n    for step in steps:\n      step.apply(gcp.use_gcp_secret('user-gcp-sa'))\n"
  },
  {
    "repo": "rakesh283343/kubeflow-pipelines-clv",
    "file_path": "pipelines/train_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/rakesh283343/kubeflow-pipelines-clv/master/pipelines/train_pipeline.py",
    "content": "# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#            http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"CLV training and deployment pipeline.\"\"\"\n\nfrom helper_components import load_sales_transactions\nfrom helper_components import prepare_feature_engineering_query\n\nimport kfp\nfrom kfp import gcp\n\nimport pathlib\nimport yaml\n\n# Load pipeline settings\ncompiler_settings = yaml.safe_load(\n    pathlib.Path('settings.yaml').read_text())['compiler_settings']\nargument_defaults = yaml.safe_load(\n    pathlib.Path('settings.yaml').read_text())['argument_defaults']\n\n# Initialize component store\ncomponent_store = kfp.components.ComponentStore(\n    compiler_settings['local_search_paths'],\n    compiler_settings['url_search_prefixes'])\n\n# Create component factories\nload_sales_transactions_op = kfp.components.func_to_container_op(\n    load_sales_transactions,\n    base_image=compiler_settings['lightweight_components_base_image'])\nprepare_feature_engineering_query_op = kfp.components.func_to_container_op(\n    prepare_feature_engineering_query,\n    base_image=compiler_settings['lightweight_components_base_image'])\nengineer_features_op = component_store.load_component('bigquery/query')\nimport_dataset_op = component_store.load_component('aml-import-dataset')\ntrain_model_op = component_store.load_component('aml-train-model')\ndeploy_model_op = component_store.load_component('aml-deploy-model')\nlog_metrics_op = component_store.load_component('aml-log-metrics')\n\n\n# Pipeline definition\n@kfp.dsl.pipeline(\n    name='CLV Training',\n    description='CLV Training Pipeline using BigQuery for feature engineering and Automl Tables for model training'\n)\ndef clv_train(\n    project_id,\n    source_gcs_path,\n    staging_gcs_path,\n    source_bq_table,\n    bq_dataset_name,\n    threshold_date,\n    predict_end,\n    max_monetary,\n    features_table_name=argument_defaults['features_table_name'],\n    transactions_table_name=argument_defaults['transactions_table_name'],\n    dataset_location=argument_defaults['dataset_location'],\n    aml_dataset_name=argument_defaults['aml_dataset_name'],\n    aml_model_name=argument_defaults['aml_model_name'],\n    aml_compute_region=argument_defaults['aml_compute_region'],\n    train_budget=argument_defaults['train_budget'],\n    target_column_name=argument_defaults['target_column_name'],\n    features_to_exclude=argument_defaults['features_to_exclude'],\n    optimization_objective=argument_defaults['optimization_objective'],\n    primary_metric=argument_defaults['primary_metric'],\n    deployment_threshold=argument_defaults['deployment_threshold'],\n    skip_deployment=argument_defaults['skip_deployment'],\n    query_template_uri=argument_defaults['query_template_uri']):\n  \"\"\"Trains and optionally deploys a CLV Model.\"\"\"\n\n  # Load sales transactions\n  load_transactions = load_sales_transactions_op(\n      project_id=project_id,\n      source_gcs_path=source_gcs_path,\n      source_bq_table=source_bq_table,\n      dataset_location=dataset_location,\n      dataset_name=bq_dataset_name,\n      table_id=transactions_table_name)\n\n  # Generate the feature engineering query\n  prepare_query = prepare_feature_engineering_query_op(\n      project_id=project_id,\n      source_table_id=load_transactions.output,\n      destination_dataset=bq_dataset_name,\n      features_table_name=features_table_name,\n      threshold_date=threshold_date,\n      predict_end=predict_end,\n      max_monetary=max_monetary,\n      query_template_uri=query_template_uri)\n\n  # Run the feature engineering query on BigQuery.\n  engineer_features = engineer_features_op(\n      query=prepare_query.outputs['query'],\n      project_id=project_id,\n      dataset_id=prepare_query.outputs['dataset_name'],\n      table_id=prepare_query.outputs['table_name'],\n      output_gcs_path=staging_gcs_path,\n      dataset_location=dataset_location,\n      job_config='')\n\n  source_data_uri = 'bq://{}.{}.{}'.format(\n      project_id, prepare_query.outputs['dataset_name'],\n      prepare_query.outputs['table_name'])\n\n  # Import BQ table with features into AML dataset\n  import_dataset = import_dataset_op(\n      project_id=project_id,\n      region=aml_compute_region,\n      dataset_name=aml_dataset_name,\n      description='',\n      source_data_uri=source_data_uri,\n      target_column_name=target_column_name,\n      weight_column_name='',\n      ml_use_column_name='')\n  import_dataset.after(engineer_features)\n\n  # Train the model\n  train_model = train_model_op(\n      project_id=project_id,\n      region=aml_compute_region,\n      dataset_id=import_dataset.outputs['output_dataset_id'],\n      model_name=aml_model_name,\n      train_budget=train_budget,\n      optimization_objective=optimization_objective,\n      target_name=target_column_name,\n      features_to_exclude=features_to_exclude)\n\n  # Log evaluation metrics\n  log_metrics = log_metrics_op(\n      model_full_id=train_model.outputs['output_model_full_id'],\n      primary_metric=primary_metric)\n\n  # Deploy the model if configured and the primary metric below the threshold\n  with kfp.dsl.Condition(skip_deployment != True):\n    with kfp.dsl.Condition(log_metrics.outputs['output_primary_metric_value'] <\n                           deployment_threshold):\n      deploy_model = deploy_model_op(\n          train_model.outputs['output_model_full_id'])\n\n  # Configure the pipeline to use a service account secret\n  if compiler_settings['use_sa_secret']:\n    steps = [\n        load_transactions, prepare_query, engineer_features, import_dataset,\n        train_model, log_metrics, deploy_model\n    ]\n    for step in steps:\n      step.apply(gcp.use_gcp_secret('user-gcp-sa'))\n\n  \n"
  },
  {
    "repo": "rakesh283343/kubeflow-pipelines-clv",
    "file_path": "pipelines/helper_components/helper_components.py",
    "raw_url": "https://raw.githubusercontent.com/rakesh283343/kubeflow-pipelines-clv/master/pipelines/helper_components/helper_components.py",
    "content": "# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#            http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Helper Lightweight Python components.\"\"\"\n\nimport kfp\nfrom kfp import dsl\nfrom typing import NamedTuple\n\n\n@kfp.dsl.python_component(name='Load transactions')\ndef load_sales_transactions(project_id: str, source_gcs_path: str,\n                            source_bq_table: str, dataset_location: str,\n                            dataset_name: str, table_id: str) -> str:\n  \"\"\"Loads historical sales transactions to BigQuery.\n\n    If source_gcs_path is passed loads data from a CSV file in GCS.\n    If source_bq_table is passed no load is executed and source table id is\n    passed as\n    an output. One and only one type of source must be specified.\n  \n  Args:\n    project_id: A project ID for BigQuery.\n    source_gcs_path: A URL to Cloud Storage location.\n    source_bq_table: A URL to BigQuery location.\n    dataset_location: The location of the destination BigQuery dataset.\n    dataset_name: The name of the destination BiqQuery dataset.\n    table_id: The ID of the destination table.\n  \"\"\"\n\n  import uuid\n  import logging\n  from google.cloud import bigquery\n\n  logging.basicConfig(level=logging.INFO)\n\n  client = bigquery.Client(project=project_id)\n\n  if source_gcs_path:\n    # Create or get a dataset reference\n    if not dataset_name:\n      dataset_name = 'clv_dataset_{}'.format(uuid.uuid4().hex)\n    dataset = bigquery.Dataset('{}.{}'.format(project_id, dataset_name))\n    dataset.location = dataset_location\n    dataset_ref = client.create_dataset(dataset, exists_ok=True)\n\n    # Configure Load job settings\n    job_config = bigquery.LoadJobConfig()\n    job_config.schema = [\n        bigquery.SchemaField('customer_id', 'STRING'),\n        bigquery.SchemaField('order_date', 'DATE'),\n        bigquery.SchemaField('quantity', 'INTEGER'),\n        bigquery.SchemaField('unit_price', 'FLOAT')\n    ]\n    job_config.source_format = bigquery.SourceFormat.CSV\n    job_config.create_disposition = bigquery.job.CreateDisposition.CREATE_IF_NEEDED\n    job_config.write_disposition = bigquery.job.WriteDisposition.WRITE_TRUNCATE\n    job_config.skip_leading_rows = 1\n\n    if not table_id:\n      table_id = 'transactions_{}'.format(uuid.uuid4().hex)\n\n    # Start the load job\n    logging.info('Importing data from {} to {}.{}'.format(\n        source_gcs_path, dataset_name, table_id))\n    load_job = client.load_table_from_uri(\n        source_gcs_path, dataset_ref.table(table_id), job_config=job_config)\n\n    # Wait for table load to complete\n    load_job.result()\n\n    output = '{}.{}.{}'.format(project_id, dataset_name, table_id)\n  else:\n    logging.info('Source data already in BigQuery: {}'.format(source_bq_table))\n    output = source_bq_table\n\n  return output\n\n\n@kfp.dsl.python_component(name='Prepare query')\ndef prepare_feature_engineering_query(\n    project_id: str, source_table_id: str, destination_dataset: str,\n    features_table_name: str, threshold_date: str, predict_end: str,\n    max_monetary: str,\n    query_template_uri: str) -> NamedTuple('ComponentOutput', [(\n        'query', str), ('dataset_name', str), ('table_name', str)]):\n  \"\"\"Generates a feature engineering query.\n\n    This a lightweight Python KFP component that generates a query\n    that processes an input BQ table with sales transactions into features\n    that will be used for CLV model training. The component replaces\n    placeholders in a query template with values passed as parameters.\n  \"\"\"\n\n  import uuid\n  import logging\n  import re\n\n  from jinja2 import Template\n  from google.cloud import storage\n  from google.cloud import bigquery\n\n  logging.basicConfig(level=logging.INFO)\n  logging.info(\n      'Retrieving the query template from: {}'.format(query_template_uri))\n\n  # Read a query template from GCS\n  _, bucket, blob_name = re.split('gs://|/', query_template_uri, 2)\n  blob = storage.Client(project_id).get_bucket(bucket).blob(blob_name)\n  template = Template(blob.download_as_string().decode('utf-8'))\n\n  # Substitute placeholders in the query template\n  query = template.render(\n      data_source_id=source_table_id,\n      threshold_date=threshold_date,\n      predict_end=predict_end,\n      max_monetary=max_monetary)\n\n  # Create unique destination dataset and table names if not set\n  if not destination_dataset:\n    destination_dataset = 'clv_dataset_{}'.format(uuid.uuid4().hex)\n\n  if not features_table_name:\n    features_table_name = 'clv_features_{}'.format(uuid.uuid4().hex)\n\n  from collections import namedtuple\n  output = namedtuple('ComponentOutput',\n                      ['query', 'dataset_name', 'table_name'])\n\n  return output(query, destination_dataset, features_table_name)\n"
  },
  {
    "repo": "anupr567/kubeflow_pipeline",
    "file_path": "kfp_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/anupr567/kubeflow_pipeline/main/kfp_pipeline.py",
    "content": "# -*- coding: utf-8 -*-\r\n\"\"\"\r\nCreated on Fri Oct  8 17:11:47 2021\r\n\r\n@author: Nikhil\r\n\"\"\"\r\n\r\nimport kfp\r\nfrom kfp import dsl\r\n\r\n\r\n@dsl.pipeline(name='First Pipeline',\r\n              description='Applies extraction of data and pre processing it.')\r\ndef first_pipeline():\r\n    # Loads the yaml manifest for components\r\n    extract_data = kfp.components.load_component_from_file('components/extract_data/extract_data.yaml')\r\n    preprocess_data = kfp.components.load_component_from_file('components/preprocess/process_data_classifier.yaml')\r\n    logistic_regression = kfp.components.load_component_from_file('components/logistic_regression/logistic_regression.yaml')\r\n    random_forest_classifier = kfp.components.load_component_from_file('components/random_forest/random_forest_classifier.yaml')\r\n    # Run task\r\n    extract_task = extract_data()\r\n    preprocess_task = preprocess_data(extract_task.output)\r\n    preprocess_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"    # disable caching\r\n    logistic_regression_task = linear_regression(preprocess_task.output)\r\n    random_forest_classifier_task = random_forest_classifier(preprocess_task.output)\r\n\r\n\r\n# submit the pipeline for execution.\r\n# keep in mind to change the host url according to your host.\r\nif __name__ == '__main__':\r\n    pipeline = kfp.Client(\r\n        host='https://4ccec0259a834495-dot-us-central1.pipelines.googleusercontent.com').create_run_from_pipeline_func(\r\n        first_pipeline, arguments={})       \r\n"
  },
  {
    "repo": "9rince/kfp",
    "file_path": "samples/test/after.py",
    "raw_url": "https://raw.githubusercontent.com/9rince/kfp/develop/samples/test/after.py",
    "content": "# Copyright 2020 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom kfp import components\nfrom kfp.v2 import dsl\nimport kfp.v2.compiler as compiler\n\ncomponent_op = components.load_component_from_text(\n    \"\"\"\nname: Print Text\ninputs:\n- {name: text, type: String}\nimplementation:\n  container:\n    image: alpine\n    command:\n    - sh\n    - -c\n    - |\n      set -e -x\n      echo \"$0\"\n    - {inputValue: text}\n\"\"\"\n)\n\n\n@dsl.pipeline(name='pipeline-with-after')\ndef my_pipeline():\n    task1 = component_op(text='1st task')\n    task2 = component_op(text='2nd task').after(task1)\n    task3 = component_op(text='3rd task').after(task1, task2)\n\n\nif __name__ == '__main__':\n    compiler.Compiler().compile(\n        pipeline_func=my_pipeline,\n        pipeline_root='dummy_root',\n        output_path=__file__ + '.json'\n    )\n"
  },
  {
    "repo": "9rince/kfp",
    "file_path": "samples/test/after_test.py",
    "raw_url": "https://raw.githubusercontent.com/9rince/kfp/develop/samples/test/after_test.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp\nfrom .after import my_pipeline\nfrom .util import run_pipeline_func, TestCase\n\nrun_pipeline_func([\n    TestCase(pipeline_func=my_pipeline),\n    TestCase(\n        pipeline_func=my_pipeline, mode=kfp.dsl.PipelineExecutionMode.V2_ENGINE\n    ),\n])\n"
  },
  {
    "repo": "9rince/kfp",
    "file_path": "samples/test/cache_v2_test.py",
    "raw_url": "https://raw.githubusercontent.com/9rince/kfp/develop/samples/test/cache_v2_test.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Two step v2-compatible pipeline.\"\"\"\n\n# %%\n\nfrom __future__ import annotations\n\nimport random\nimport string\nimport unittest\nimport functools\n\nimport kfp\nimport kfp_server_api\n\nfrom .two_step import two_step_pipeline\nfrom .util import run_pipeline_func, TestCase, KfpMlmdClient, KfpTask\nfrom ml_metadata.proto import Execution\n\n\ndef verify_tasks(\n    t: unittest.TestCase, tasks: dict[str, KfpTask], task_state, uri: str,\n    some_int: int\n):\n    task_names = [*tasks.keys()]\n    t.assertEqual(task_names, ['train-op', 'preprocess'], 'task names')\n\n    preprocess = tasks['preprocess']\n    train = tasks['train-op']\n\n    t.assertEqual({\n        'name': 'preprocess',\n        'inputs': {\n            'artifacts': [],\n            'parameters': {\n                'some_int': some_int,\n                'uri': uri\n            }\n        },\n        'outputs': {\n            'artifacts': [{\n                'metadata': {\n                    'display_name': 'output_dataset_one',\n                },\n                'name': 'output_dataset_one',\n                'type': 'system.Dataset'\n            }],\n            'parameters': {\n                'output_parameter_one': some_int\n            }\n        },\n        'type': 'system.ContainerExecution',\n        'state': task_state,\n    }, preprocess.get_dict())\n    t.assertEqual({\n        'name': 'train-op',\n        'inputs': {\n            'artifacts': [{\n                'metadata': {\n                    'display_name': 'output_dataset_one',\n                },\n                'name': 'dataset',\n                'type': 'system.Dataset',\n            }],\n            'parameters': {\n                'num_steps': some_int\n            }\n        },\n        'outputs': {\n            'artifacts': [{\n                'metadata': {\n                    'display_name': 'model',\n                },\n                'name': 'model',\n                'type': 'system.Model',\n            }],\n            'parameters': {}\n        },\n        'type': 'system.ContainerExecution',\n        'state': task_state,\n    }, train.get_dict())\n\n\ndef verify(\n    run: kfp_server_api.ApiRun, mlmd_connection_config, uri: str, some_int,\n    state: int, **kwargs\n):\n    t = unittest.TestCase()\n    t.maxDiff = None  # we always want to see full diff\n    t.assertEqual(run.status, 'Succeeded')\n    client = KfpMlmdClient(mlmd_connection_config=mlmd_connection_config)\n    tasks = client.get_tasks(run_id=run.id)\n    verify_tasks(t, tasks, state, uri, some_int)\n\n\nif __name__ == '__main__':\n    letters = string.ascii_lowercase\n    random_uri = 'http://' + ''.join(random.choice(letters) for i in range(5))\n    random_int = random.randint(0, 10000)\n    run_pipeline_func([\n        TestCase(\n            pipeline_func=two_step_pipeline,\n            arguments={\n                'uri': f'{random_uri}',\n                'some_int': f'{random_int}'\n            },\n            verify_func=functools.partial(\n                verify,\n                uri=random_uri,\n                some_int=random_int,\n                state=Execution.State.COMPLETE,\n            ),\n            mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE,\n            enable_caching=True\n        ),\n    ]),\n    run_pipeline_func([\n        TestCase(\n            pipeline_func=two_step_pipeline,\n            arguments={\n                'uri': f'{random_uri}',\n                'some_int': f'{random_int}'\n            },\n            verify_func=functools.partial(\n                verify,\n                uri=random_uri,\n                some_int=random_int,\n                state=Execution.State.CACHED\n            ),\n            mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE,\n            enable_caching=True\n        ),\n    ])\n\n# %%\n"
  },
  {
    "repo": "9rince/kfp",
    "file_path": "samples/test/fail.py",
    "raw_url": "https://raw.githubusercontent.com/9rince/kfp/develop/samples/test/fail.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Fail pipeline.\"\"\"\n\nfrom kfp import components, dsl\n\n\ndef fail():\n    '''Fails'''\n    import sys\n    sys.exit(1)\n\n\nfail_op = components.create_component_from_func(\n    fail, base_image='alpine:latest'\n)\n\n\n@dsl.pipeline(name='fail_pipeline')\ndef fail_pipeline():\n    fail_task = fail_op()\n"
  },
  {
    "repo": "9rince/kfp",
    "file_path": "samples/test/fail_parameter_value_missing.py",
    "raw_url": "https://raw.githubusercontent.com/9rince/kfp/develop/samples/test/fail_parameter_value_missing.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom kfp import dsl, components\n\necho = components.load_component_from_text(\n    \"\"\"\nname: Echo\ninputs:\n- {name: text, type: String}\nimplementation:\n  container:\n    image: alpine\n    command:\n    - echo\n    - {inputValue: text}\n\"\"\"\n)\n\n\n@dsl.pipeline(name='parameter_value_missing')\ndef pipeline(\n    parameter:\n    str  # parameter should be specified when submitting, but we are missing it in the test\n):\n    echo_op = echo(text=parameter)\n"
  },
  {
    "repo": "9rince/kfp",
    "file_path": "samples/test/legacy_data_passing.py",
    "raw_url": "https://raw.githubusercontent.com/9rince/kfp/develop/samples/test/legacy_data_passing.py",
    "content": "# This pipeline demonstrates and verifies all ways of data passing supported by KFP.\n\n# %% [markdown]\n# KFP has simple data passing model. There are three different kinds of arguments and tw different ways to consume an input argument. All combinations are supported.\n\n# Any input can be consumed:\n# * As value (`inputValue` placeholder)\n# * As file (`inputPath` placeholder).\n\n# Input argument can come from:\n# * Constant value\n# * Pipeline parameter\n# * Upstream component output\n\n# Combining these options there are 6 end-to-end data passing cases, each of which works regardless of type:\n\n# 1. Pass constant value which gets consumed as value. (Common)\n# 2. Pass constant value which gets consumed as file. (Often used in test pipelines)\n# 3. Pass pipeline parameter that gets consumed as value. (Common)\n# 4. Pass pipeline parameter that gets consumed as file. (Rare. Sometimes used with JSON parameters)\n# 5. Pass task output that gets consumed as value. (Common)\n# 6. Pass task output that gets consumed as file. (Common)\n\n# The only restriction on types is that when both upstream output and downstream input have types, the types must match.\n\n# %%\nimport kfp\nfrom kfp.components import create_component_from_func, InputPath, OutputPath\n\n\n# Components\n# Produce\n@create_component_from_func\ndef produce_anything(data_path: OutputPath()):\n    with open(data_path, \"w\") as f:\n        f.write(\"produce_anything\")\n\n\n@create_component_from_func\ndef produce_something(data_path: OutputPath(\"Something\")):\n    with open(data_path, \"w\") as f:\n        f.write(\"produce_something\")\n\n\n@create_component_from_func\ndef produce_something2() -> 'Something':\n    return \"produce_something2\"\n\n\n@create_component_from_func\ndef produce_string() -> str:\n    return \"produce_string\"\n\n\n# Consume as value\n@create_component_from_func\ndef consume_anything_as_value(data):\n    print(\"consume_anything_as_value: \" + data)\n\n\n@create_component_from_func\ndef consume_something_as_value(data: \"Something\"):\n    print(\"consume_something_as_value: \" + data)\n\n\n@create_component_from_func\ndef consume_string_as_value(data: str):\n    print(\"consume_string_as_value: \" + data)\n\n\n# Consume as file\n@create_component_from_func\ndef consume_anything_as_file(data_path: InputPath()):\n    with open(data_path) as f:\n        print(\"consume_anything_as_file: \" + f.read())\n\n\n@create_component_from_func\ndef consume_something_as_file(data_path: InputPath('Something')):\n    with open(data_path) as f:\n        print(\"consume_something_as_file: \" + f.read())\n\n\n@create_component_from_func\ndef consume_string_as_file(data_path: InputPath(str)):\n    with open(data_path) as f:\n        print(\"consume_string_as_file: \" + f.read())\n\n\n# Pipeline\n@kfp.dsl.pipeline(name='data_passing_pipeline')\ndef data_passing_pipeline(\n    anything_param=\"anything_param\",\n    something_param: \"Something\" = \"something_param\",\n    string_param: str = \"string_param\",\n):\n    produced_anything = produce_anything().output\n    produced_something = produce_something().output\n    produced_string = produce_string().output\n\n    # Pass constant value; consume as value\n    consume_anything_as_value(\"constant\")\n    consume_something_as_value(\"constant\")\n    consume_string_as_value(\"constant\")\n\n    # Pass constant value; consume as file\n    consume_anything_as_file(\"constant\")\n    consume_something_as_file(\"constant\")\n    consume_string_as_file(\"constant\")\n\n    # Pass pipeline parameter; consume as value\n    consume_anything_as_value(anything_param)\n    consume_anything_as_value(something_param)\n    consume_anything_as_value(string_param)\n    consume_something_as_value(anything_param)\n    consume_something_as_value(something_param)\n    consume_string_as_value(anything_param)\n    consume_string_as_value(string_param)\n\n    # Pass pipeline parameter; consume as file\n    consume_anything_as_file(anything_param)\n    consume_anything_as_file(something_param)\n    consume_anything_as_file(string_param)\n    consume_something_as_file(anything_param)\n    consume_something_as_file(something_param)\n    consume_string_as_file(anything_param)\n    consume_string_as_file(string_param)\n\n    # Pass task output; consume as value\n    consume_anything_as_value(produced_anything)\n    consume_anything_as_value(produced_something)\n    consume_anything_as_value(produced_string)\n    consume_something_as_value(produced_anything)\n    consume_something_as_value(produced_something)\n    consume_string_as_value(produced_anything)\n    consume_string_as_value(produced_string)\n\n    # Pass task output; consume as file\n    consume_anything_as_file(produced_anything)\n    consume_anything_as_file(produced_something)\n    consume_anything_as_file(produced_string)\n    consume_something_as_file(produced_anything)\n    consume_something_as_file(produced_something)\n    consume_string_as_file(produced_anything)\n    consume_string_as_file(produced_string)\n\n\nif __name__ == \"__main__\":\n    kfp_endpoint = None\n    kfp.Client(host=kfp_endpoint).create_run_from_pipeline_func(\n        data_passing_pipeline, arguments={})\n"
  },
  {
    "repo": "9rince/kfp",
    "file_path": "samples/test/legacy_data_passing_test.py",
    "raw_url": "https://raw.githubusercontent.com/9rince/kfp/develop/samples/test/legacy_data_passing_test.py",
    "content": "from .data_passing import data_passing_pipeline\nfrom .util import run_pipeline_func, TestCase,\nfrom kfp.dsl import PipelineExecutionMode\n\nrun_pipeline_func([\n    TestCase(\n        pipeline_func=data_passing_pipeline,\n        mode=PipelineExecutionMode.V1_LEGACY,\n    )\n])\n"
  },
  {
    "repo": "9rince/kfp",
    "file_path": "samples/test/legacy_exit_handler.py",
    "raw_url": "https://raw.githubusercontent.com/9rince/kfp/develop/samples/test/legacy_exit_handler.py",
    "content": "#!/usr/bin/env python3\n# Copyright 2019 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp\nfrom kfp import dsl\n\n\ndef gcs_download_op(url):\n    return dsl.ContainerOp(\n        name='GCS - Download',\n        image='google/cloud-sdk:279.0.0',\n        command=['sh', '-ce'],\n        arguments=['gsutil cp $0 $1 && cat $1', url, '/tmp/results.txt'],\n        file_outputs={\n            'data': '/tmp/results.txt',\n        }\n    )\n\n\ndef echo_op(text):\n    return dsl.ContainerOp(\n        name='echo',\n        image='library/bash:4.4.23',\n        command=['sh', '-cex'],\n        arguments=['echo \"$0\"', text],\n    )\n\n\n@dsl.pipeline(\n    name='Exit Handler',\n    description=\n    'Downloads a message and prints it. The exit handler will run after the pipeline finishes (successfully or not).'\n)\ndef download_and_print(url='gs://ml-pipeline/shakespeare1.txt'):\n    \"\"\"A sample pipeline showing exit handler.\"\"\"\n\n    exit_task = echo_op('exit!')\n\n    with dsl.ExitHandler(exit_task):\n        download_task = gcs_download_op(url)\n        echo_task = echo_op(download_task.output)\n\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(download_and_print, __file__ + '.yaml')\n"
  },
  {
    "repo": "9rince/kfp",
    "file_path": "samples/test/legacy_exit_handler_test.py",
    "raw_url": "https://raw.githubusercontent.com/9rince/kfp/develop/samples/test/legacy_exit_handler_test.py",
    "content": "#!/usr/bin/env python3\n# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp\nfrom .legacy_exit_handler import download_and_print\nfrom .util import run_pipeline_func, TestCase\n\n\nrun_pipeline_func([\n    # This sample is expected to not work on v2 compatible mode.\n    TestCase(\n        pipeline_func=download_and_print,\n        mode=kfp.dsl.PipelineExecutionMode.V1_LEGACY\n    )\n])\n"
  },
  {
    "repo": "9rince/kfp",
    "file_path": "samples/test/lightweight_python_functions_v2_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/9rince/kfp/develop/samples/test/lightweight_python_functions_v2_pipeline.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Sample pipeline for passing data in KFP v2.\"\"\"\nfrom typing import Dict, List\nimport os\n\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import Input, InputPath, Output, OutputPath, Dataset, Model, component\nimport kfp.v2.compiler as compiler\n\n# In tests, we install a KFP package from the PR under test. Users should not\n# normally need to specify `kfp_package_path` in their component definitions.\n_KFP_PACKAGE_PATH = os.getenv('KFP_PACKAGE_PATH')\n\n\n@component(kfp_package_path=_KFP_PACKAGE_PATH)\ndef preprocess(\n    # An input parameter of type string.\n    message: str,\n    # Use Output[T] to get a metadata-rich handle to the output artifact\n    # of type `Dataset`.\n    output_dataset_one: Output[Dataset],\n    # A locally accessible filepath for another output artifact of type\n    # `Dataset`.\n    output_dataset_two_path: OutputPath('Dataset'),\n    # A locally accessible filepath for an output parameter of type string.\n    output_parameter_path: OutputPath(str),\n    # A locally accessible filepath for an output parameter of type bool.\n    output_bool_parameter_path: OutputPath(bool),\n    # A locally accessible filepath for an output parameter of type dict.\n    output_dict_parameter_path: OutputPath(Dict[str, int]),\n    # A locally accessible filepath for an output parameter of type list.\n    output_list_parameter_path: OutputPath(List[str]),\n    # An input message that defaults to empty.\n    empty_message: str = \"\",\n):\n    \"\"\"Dummy preprocessing step\"\"\"\n\n    # Use Dataset.path to access a local file path for writing.\n    # One can also use Dataset.uri to access the actual URI file path.\n    with open(output_dataset_one.path, 'w') as f:\n        f.write(message)\n\n    # OutputPath is used to just pass the local file path of the output artifact\n    # to the function.\n    with open(output_dataset_two_path, 'w') as f:\n        f.write(message)\n\n    with open(output_parameter_path, 'w') as f:\n        f.write(message)\n\n    with open(output_bool_parameter_path, 'w') as f:\n        f.write(\n            str(True)\n        )  # use either `str()` or `json.dumps()` for bool values.\n\n    import json\n    with open(output_dict_parameter_path, 'w') as f:\n        f.write(json.dumps({'A': 1, 'B': 2}))\n\n    with open(output_list_parameter_path, 'w') as f:\n        f.write(json.dumps(['a', 'b', 'c']))\n\n\n@component(kfp_package_path=_KFP_PACKAGE_PATH)\ndef train(\n    # Use InputPath to get a locally accessible path for the input artifact\n    # of type `Dataset`.\n    dataset_one_path: InputPath('Dataset'),\n    # Use Input[T] to get a metadata-rich handle to the input artifact\n    # of type `Dataset`.\n    dataset_two: Input[Dataset],\n    # An input parameter of type string.\n    message: str,\n    # Use Output[T] to get a metadata-rich handle to the output artifact\n    # of type `Dataset`.\n    model: Output[Model],\n    # An input parameter of type bool.\n    input_bool: bool,\n    # An input parameter of type dict.\n    input_dict: Dict[str, int],\n    # An input parameter of type List[str].\n    input_list: List[str],\n    # An input parameter of type int with a default value.\n    num_steps: int = 100,\n):\n    \"\"\"Dummy Training step\"\"\"\n    with open(dataset_one_path, 'r') as input_file:\n        dataset_one_contents = input_file.read()\n\n    with open(dataset_two.path, 'r') as input_file:\n        dataset_two_contents = input_file.read()\n\n    line = (\n        f'dataset_one_contents: {dataset_one_contents} || '\n        f'dataset_two_contents: {dataset_two_contents} || '\n        f'message: {message} || '\n        f'input_bool: {input_bool}, type {type(input_bool)} || '\n        f'input_dict: {input_dict}, type {type(input_dict)} || '\n        f'input_list: {input_list}, type {type(input_list)} \\n'\n    )\n\n    with open(model.path, 'w') as output_file:\n        for i in range(num_steps):\n            output_file.write('Step {}\\n{}\\n=====\\n'.format(i, line))\n\n    # Use model.get() to get a Model artifact, which has a .metadata dictionary\n    # to store arbitrary metadata for the output artifact.\n    model.metadata['accuracy'] = 0.9\n\n\n@dsl.pipeline(name='my-test-pipeline-beta')\ndef pipeline(message: str = 'message'):\n    preprocess_task = preprocess(message=message)\n    train_task = train(\n        dataset_one_path=preprocess_task.outputs['output_dataset_one'],\n        dataset_two=preprocess_task.outputs['output_dataset_two_path'],\n        message=preprocess_task.outputs['output_parameter_path'],\n        input_bool=preprocess_task.outputs['output_bool_parameter_path'],\n        input_dict=preprocess_task.outputs['output_dict_parameter_path'],\n        input_list=preprocess_task.outputs['output_list_parameter_path'],\n    )\n\n\nif __name__ == '__main__':\n    compiler.Compiler().compile(\n        pipeline_func=pipeline, package_path=__file__.replace('.py', '.json')\n    )\n"
  },
  {
    "repo": "9rince/kfp",
    "file_path": "samples/test/lightweight_python_functions_v2_pipeline_test.py",
    "raw_url": "https://raw.githubusercontent.com/9rince/kfp/develop/samples/test/lightweight_python_functions_v2_pipeline_test.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nfrom pprint import pprint\nimport kfp_server_api\nimport kfp.dsl as dsl\n\nfrom .lightweight_python_functions_v2_pipeline import pipeline\nfrom .util import run_pipeline_func, TestCase, KfpMlmdClient\nfrom ml_metadata.proto import Execution\n\n\ndef verify(run: kfp_server_api.ApiRun, mlmd_connection_config, **kwargs):\n    t = unittest.TestCase()\n    t.maxDiff = None  # we always want to see full diff\n    t.assertEqual(run.status, 'Succeeded')\n    client = KfpMlmdClient(mlmd_connection_config=mlmd_connection_config)\n    tasks = client.get_tasks(run_id=run.id)\n\n    task_names = [*tasks.keys()]\n    t.assertCountEqual(task_names, ['preprocess', 'train'], 'task names')\n    pprint(tasks)\n\n    preprocess = tasks['preprocess']\n    train = tasks['train']\n    pprint(preprocess.get_dict())\n    t.assertEqual(\n        {\n            'inputs': {\n                'artifacts': [],\n                'parameters': {\n                    'message': 'message',\n                    'empty_message': '',\n                }\n            },\n            'name': 'preprocess',\n            'outputs': {\n                'artifacts': [{\n                    'metadata': {\n                        'display_name': 'output_dataset_one'\n                    },\n                    'name': 'output_dataset_one',\n                    'type': 'system.Dataset'\n                }, {\n                    'metadata': {\n                        'display_name': 'output_dataset_two_path'\n                    },\n                    'name': 'output_dataset_two_path',\n                    'type': 'system.Dataset'\n                }],\n                'parameters': {\n                    'output_bool_parameter_path': 'True',\n                    'output_dict_parameter_path': '{\"A\": 1, \"B\": 2}',\n                    'output_list_parameter_path': '[\"a\", \"b\", \"c\"]',\n                    'output_parameter_path': 'message'\n                }\n            },\n            'type': 'system.ContainerExecution',\n            'state': Execution.State.COMPLETE,\n        },\n        preprocess.get_dict(),\n    )\n    t.assertEqual(\n        {\n            'inputs': {\n                'artifacts': [{\n                    'metadata': {\n                        'display_name': 'output_dataset_one'\n                    },\n                    'name': 'dataset_one_path',\n                    'type': 'system.Dataset'\n                }, {\n                    'metadata': {\n                        'display_name': 'output_dataset_two_path'\n                    },\n                    'name': 'dataset_two',\n                    'type': 'system.Dataset'\n                }],\n                'parameters': {\n                    'input_bool': 'True',\n                    'input_dict': '{\"A\": 1, \"B\": 2}',\n                    'input_list': '[\"a\", \"b\", \"c\"]',\n                    'message': 'message',\n                    'num_steps': 100,\n                }\n            },\n            'name': 'train',\n            'outputs': {\n                'artifacts': [{\n                    'metadata': {\n                        'display_name': 'model',\n                        'accuracy': 0.9,\n                    },\n                    'name': 'model',\n                    'type': 'system.Model'\n                }],\n                'parameters': {}\n            },\n            'type': 'system.ContainerExecution',\n            'state': Execution.State.COMPLETE,\n        },\n        train.get_dict(),\n    )\n\n\nrun_pipeline_func([\n    TestCase(\n        pipeline_func=pipeline,\n        verify_func=verify,\n        mode=dsl.PipelineExecutionMode.V2_COMPATIBLE\n    ),\n    TestCase(\n        pipeline_func=pipeline,\n        verify_func=verify,\n        mode=dsl.PipelineExecutionMode.V2_ENGINE\n    ),\n])\n"
  },
  {
    "repo": "9rince/kfp",
    "file_path": "samples/test/lightweight_python_functions_v2_with_outputs.py",
    "raw_url": "https://raw.githubusercontent.com/9rince/kfp/develop/samples/test/lightweight_python_functions_v2_with_outputs.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Lightweight functions v2 with outputs.\"\"\"\nfrom typing import NamedTuple\nimport os\nfrom kfp import components, dsl\nfrom kfp.v2 import compiler\nfrom kfp.v2.dsl import Input, Dataset, Model, Metrics, component\n\n# In tests, we install a KFP package from the PR under test. Users should not\n# normally need to specify `kfp_package_path` in their component definitions.\n_KFP_PACKAGE_PATH = os.getenv('KFP_PACKAGE_PATH')\n\n\n@component(kfp_package_path=_KFP_PACKAGE_PATH)\ndef concat_message(first: str, second: str) -> str:\n    return first + second\n\n\n@component(kfp_package_path=_KFP_PACKAGE_PATH)\ndef add_numbers(first: int, second: int) -> int:\n    return first + second\n\n\n@component(kfp_package_path=_KFP_PACKAGE_PATH)\ndef output_artifact(number: int, message: str) -> Dataset:\n    result = [message for _ in range(number)]\n    return '\\n'.join(result)\n\n\n@component(kfp_package_path=_KFP_PACKAGE_PATH)\ndef output_named_tuple(\n    artifact: Input[Dataset]\n) -> NamedTuple(\n        'Outputs', [\n            ('scalar', str),\n            ('metrics', Metrics),\n            ('model', Model),\n        ]\n):\n    scalar = \"123\"\n\n    import json\n    metrics = json.dumps({\n        'metrics': [{\n            'name': 'accuracy',\n            'numberValue': 0.9,\n            'format': \"PERCENTAGE\",\n        }]\n    })\n\n    with open(artifact.path, 'r') as f:\n        artifact_contents = f.read()\n    model = \"Model contents: \" + artifact_contents\n\n    from collections import namedtuple\n    output = namedtuple('Outputs', ['scalar', 'metrics', 'model'])\n    return output(scalar, metrics, model)\n\n\n@dsl.pipeline(name='functions-with-outputs')\ndef pipeline(\n    first_message: str = 'first',\n    second_message: str = 'second',\n    first_number: int = 1,\n    second_number: int = 2,\n):\n    concat_task = concat_message(first=first_message, second=second_message)\n    add_numbers_task = add_numbers(first=first_number, second=second_number)\n    output_artifact_task = output_artifact(\n        number=add_numbers_task.output, message=concat_task.output\n    )\n    output_name_tuple = output_named_tuple(output_artifact_task.output)\n\n\nif __name__ == '__main__':\n    compiler.Compiler().compile(\n        pipeline_func=pipeline,\n        pipeline_root='dummy_root',\n        output_path=__file__ + '.json'\n    )\n"
  },
  {
    "repo": "9rince/kfp",
    "file_path": "samples/test/lightweight_python_functions_v2_with_outputs_test.py",
    "raw_url": "https://raw.githubusercontent.com/9rince/kfp/develop/samples/test/lightweight_python_functions_v2_with_outputs_test.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom pprint import pprint\nimport unittest\nimport kfp\nimport kfp_server_api\nimport os\nfrom minio import Minio\n\nfrom .lightweight_python_functions_v2_with_outputs import pipeline\nfrom .util import KfpMlmdClient, run_pipeline_func, TestCase\n\n\ndef verify(run: kfp_server_api.ApiRun, mlmd_connection_config, **kwargs):\n    t = unittest.TestCase()\n    t.maxDiff = None  # we always want to see full diff\n    t.assertEqual(run.status, 'Succeeded')\n    client = KfpMlmdClient(mlmd_connection_config=mlmd_connection_config)\n    tasks = client.get_tasks(run_id=run.id)\n    pprint(tasks)\n\n    output_artifact = tasks['output-artifact']\n    output = [\n        a for a in output_artifact.outputs.artifacts if a.name == 'Output'\n    ][0]\n    pprint(output)\n\n    host = os.environ['MINIO_SERVICE_SERVICE_HOST']\n    port = os.environ['MINIO_SERVICE_SERVICE_PORT']\n    minio = Minio(\n        f'{host}:{port}',\n        access_key='minio',\n        secret_key='minio123',\n        secure=False\n    )\n    bucket, key = output.uri[len('minio://'):].split('/', 1)\n    print(f'bucket={bucket} key={key}')\n    response = minio.get_object(bucket, key)\n    data = response.read().decode('UTF-8')\n    t.assertEqual(data, 'firstsecond\\nfirstsecond\\nfirstsecond')\n\n\nrun_pipeline_func([\n    TestCase(\n        pipeline_func=pipeline,\n        mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE\n    ),\n    TestCase(\n        pipeline_func=pipeline,\n        mode=kfp.dsl.PipelineExecutionMode.V2_ENGINE,\n    ),\n    # Verify overriding pipeline root to MinIO\n    TestCase(\n        pipeline_func=pipeline,\n        verify_func=verify,\n        mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE,\n        arguments={\n            kfp.dsl.ROOT_PARAMETER_NAME: 'minio://mlpipeline/override/artifacts'\n        },\n    ),\n])\n"
  },
  {
    "repo": "9rince/kfp",
    "file_path": "samples/test/metrics_visualization_v1.py",
    "raw_url": "https://raw.githubusercontent.com/9rince/kfp/develop/samples/test/metrics_visualization_v1.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp.dsl as dsl\nfrom ..core.visualization.confusion_matrix import confusion_visualization\nfrom ..core.visualization.html import html_visualization\nfrom ..core.visualization.markdown import markdown_visualization\nfrom ..core.visualization.roc import roc_visualization\nfrom ..core.visualization.table import table_visualization\n\n# Note: This test is to verify that visualization metrics on V1 is runnable by KFP.\n# However, this pipeline is only runnable on V1 mode, but not V2 compatible mode.\n\n@dsl.pipeline(\n    name='metrics-visualization-v1-pipeline')\ndef metrics_visualization_v1_pipeline():\n    confusion_visualization_task = confusion_visualization()\n    html_visualization_task = html_visualization(\"\")\n    markdown_visualization_task = markdown_visualization()\n    roc_visualization_task = roc_visualization()\n    table_visualization_task = table_visualization()\n"
  },
  {
    "repo": "9rince/kfp",
    "file_path": "samples/test/metrics_visualization_v1_test.py",
    "raw_url": "https://raw.githubusercontent.com/9rince/kfp/develop/samples/test/metrics_visualization_v1_test.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp_server_api\nimport unittest\nfrom pprint import pprint\nfrom .metrics_visualization_v1 import metrics_visualization_v1_pipeline\nfrom .util import KfpMlmdClient, run_pipeline_func, TestCase\n\nimport kfp\nimport kfp_server_api\n\n\ndef verify(\n    run: kfp_server_api.ApiRun, mlmd_connection_config, argo_workflow_name: str,\n    **kwargs\n):\n    t = unittest.TestCase()\n    t.maxDiff = None  # we always want to see full diff\n    t.assertEqual(run.status, 'Succeeded')\n    client = KfpMlmdClient(mlmd_connection_config=mlmd_connection_config)\n    tasks = client.get_tasks(argo_workflow_name=argo_workflow_name)\n    pprint(tasks)\n\n    table_visualization = tasks['table-visualization']\n    markdown_visualization = tasks['markdown-visualization']\n    roc_visualization = tasks['roc-visualization']\n    html_visualization = tasks['html-visualization']\n    confusion_visualization = tasks['confusion-visualization']\n\n    t.assertEqual(table_visualization.get_dict()['outputs']['artifacts'][0]['name'],\n                  'mlpipeline_ui_metadata')\n    t.assertEqual(markdown_visualization.get_dict()['outputs']['artifacts'][0]['name'],\n                  'mlpipeline_ui_metadata')\n    t.assertEqual(roc_visualization.get_dict()['outputs']['artifacts'][0]['name'],\n                  'mlpipeline_ui_metadata')\n    t.assertEqual(html_visualization.get_dict()['outputs']['artifacts'][0]['name'],\n                  'mlpipeline_ui_metadata')\n    t.assertEqual(confusion_visualization.get_dict()['outputs']['artifacts'][0]['name'],\n                  'mlpipeline_ui_metadata')\n\n\nrun_pipeline_func([TestCase(pipeline_func=metrics_visualization_v1_pipeline,\n                            mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE\n                            )])\n"
  },
  {
    "repo": "9rince/kfp",
    "file_path": "samples/test/metrics_visualization_v2.py",
    "raw_url": "https://raw.githubusercontent.com/9rince/kfp/develop/samples/test/metrics_visualization_v2.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport os\n\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import (\n    component,\n    Output,\n    ClassificationMetrics,\n    Metrics,\n    HTML,\n    Markdown\n)\n\n# In tests, we install a KFP package from the PR under test. Users should not\n# normally need to specify `kfp_package_path` in their component definitions.\n_KFP_PACKAGE_PATH = os.getenv('KFP_PACKAGE_PATH')\n\n\n@component(\n    packages_to_install=['sklearn'],\n    base_image='python:3.9',\n    kfp_package_path=_KFP_PACKAGE_PATH,\n)\ndef digit_classification(metrics: Output[Metrics]):\n    from sklearn import model_selection\n    from sklearn.linear_model import LogisticRegression\n    from sklearn import datasets\n    from sklearn.metrics import accuracy_score\n\n    # Load digits dataset\n    iris = datasets.load_iris()\n\n    # # Create feature matrix\n    X = iris.data\n\n    # Create target vector\n    y = iris.target\n\n    #test size\n    test_size = 0.33\n\n    seed = 7\n    #cross-validation settings\n    kfold = model_selection.KFold(n_splits=10, random_state=seed, shuffle=True)\n\n    #Model instance\n    model = LogisticRegression()\n    scoring = 'accuracy'\n    results = model_selection.cross_val_score(model, X, y, cv=kfold, scoring=scoring)\n\n    #split data\n    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=test_size, random_state=seed)\n    #fit model\n    model.fit(X_train, y_train)\n\n    #accuracy on test set\n    result = model.score(X_test, y_test)\n    metrics.log_metric('accuracy', (result*100.0))\n\n@component(\n    packages_to_install=['sklearn'],\n    base_image='python:3.9',\n    kfp_package_path=_KFP_PACKAGE_PATH,\n)\ndef wine_classification(metrics: Output[ClassificationMetrics]):\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.metrics import roc_curve\n    from sklearn.datasets import load_wine\n    from sklearn.model_selection import train_test_split, cross_val_predict\n\n    X, y = load_wine(return_X_y=True)\n    # Binary classification problem for label 1.\n    y = y == 1\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n    rfc = RandomForestClassifier(n_estimators=10, random_state=42)\n    rfc.fit(X_train, y_train)\n    y_scores = cross_val_predict(rfc, X_train, y_train, cv=3, method='predict_proba')\n    y_predict = cross_val_predict(rfc, X_train, y_train, cv=3, method='predict')\n    fpr, tpr, thresholds = roc_curve(y_true=y_train, y_score=y_scores[:,1], pos_label=True)\n    metrics.log_roc_curve(fpr, tpr, thresholds)\n\n@component(\n    packages_to_install=['sklearn'],\n    base_image='python:3.9',\n    kfp_package_path=_KFP_PACKAGE_PATH,\n)\ndef iris_sgdclassifier(test_samples_fraction: float, metrics: Output[ClassificationMetrics]):\n    from sklearn import datasets, model_selection\n    from sklearn.linear_model import SGDClassifier\n    from sklearn.metrics import confusion_matrix\n\n    iris_dataset = datasets.load_iris()\n    train_x, test_x, train_y, test_y = model_selection.train_test_split(\n        iris_dataset['data'], iris_dataset['target'], test_size=test_samples_fraction)\n\n\n    classifier = SGDClassifier()\n    classifier.fit(train_x, train_y)\n    predictions = model_selection.cross_val_predict(classifier, train_x, train_y, cv=3)\n    metrics.log_confusion_matrix(\n        ['Setosa', 'Versicolour', 'Virginica'],\n        confusion_matrix(train_y, predictions).tolist() # .tolist() to convert np array to list.\n    )\n\n@component(\n    kfp_package_path=_KFP_PACKAGE_PATH,\n)\ndef html_visualization(html_artifact: Output[HTML]):\n    html_content = '<!DOCTYPE html><html><body><h1>Hello world</h1></body></html>'\n    with open(html_artifact.path, 'w') as f:\n        f.write(html_content)\n\n@component(\n    kfp_package_path=_KFP_PACKAGE_PATH,\n)\ndef markdown_visualization(markdown_artifact: Output[Markdown]):\n    markdown_content = '## Hello world \\n\\n Markdown content'\n    with open(markdown_artifact.path, 'w') as f:\n        f.write(markdown_content)\n\n@dsl.pipeline(\n    name='metrics-visualization-pipeline')\ndef metrics_visualization_pipeline():\n    wine_classification_op = wine_classification()\n    iris_sgdclassifier_op = iris_sgdclassifier(test_samples_fraction=0.3)\n    digit_classification_op = digit_classification()\n    html_visualization_op = html_visualization()\n    markdown_visualization_op = markdown_visualization()\n"
  },
  {
    "repo": "9rince/kfp",
    "file_path": "samples/test/metrics_visualization_v2_test.py",
    "raw_url": "https://raw.githubusercontent.com/9rince/kfp/develop/samples/test/metrics_visualization_v2_test.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport unittest.mock as mock\nfrom pprint import pprint\nimport kfp\nimport kfp_server_api\n\nfrom .metrics_visualization_v2 import metrics_visualization_pipeline\nfrom .util import run_pipeline_func, TestCase, KfpMlmdClient\nfrom ml_metadata.proto import Execution\n\n\ndef verify(run: kfp_server_api.ApiRun, mlmd_connection_config, **kwargs):\n    t = unittest.TestCase()\n    t.maxDiff = None  # we always want to see full diff\n    t.assertEqual(run.status, 'Succeeded')\n    client = KfpMlmdClient(mlmd_connection_config=mlmd_connection_config)\n    tasks = client.get_tasks(run_id=run.id)\n\n    task_names = [*tasks.keys()]\n    t.assertCountEqual(\n        task_names,\n        ['wine-classification', 'iris-sgdclassifier', 'digit-classification', 'html-visualization', 'markdown-visualization'],\n        'task names'\n    )\n\n    wine_classification = tasks['wine-classification']\n    iris_sgdclassifier = tasks['iris-sgdclassifier']\n    digit_classification = tasks['digit-classification']\n    html_visualization = tasks['html-visualization']\n    markdown_visualization = tasks['markdown-visualization']\n\n    pprint('======= wine classification task =======')\n    pprint(wine_classification.get_dict())\n    pprint('======= iris sgdclassifier task =======')\n    pprint(iris_sgdclassifier.get_dict())\n    pprint('======= digit classification task =======')\n    pprint(digit_classification.get_dict())\n    pprint('======= HTML task =======')\n    pprint(html_visualization.get_dict())\n    pprint('======= Markdown task =======')\n    pprint(markdown_visualization.get_dict())\n\n    t.assertEqual({\n        'inputs': {\n            'artifacts': [],\n            'parameters': {}\n        },\n        'name': 'wine-classification',\n        'outputs': {\n            'artifacts': [{\n                'metadata': {\n                    'display_name': 'metrics',\n                    'confidenceMetrics': {\n                        'list': [{\n                            'confidenceThreshold': 2.0,\n                            'falsePositiveRate': 0.0,\n                            'recall': 0.0\n                        }, {\n                            'confidenceThreshold': 1.0,\n                            'falsePositiveRate': 0.0,\n                            'recall': 0.33962264150943394\n                        }, {\n                            'confidenceThreshold': 0.9,\n                            'falsePositiveRate': 0.0,\n                            'recall': 0.6037735849056604\n                        }, {\n                            'confidenceThreshold': 0.8,\n                            'falsePositiveRate': 0.0,\n                            'recall': 0.8490566037735849\n                        }, {\n                            'confidenceThreshold': 0.6,\n                            'falsePositiveRate': 0.0,\n                            'recall': 0.8867924528301887\n                        }, {\n                            'confidenceThreshold': 0.5,\n                            'falsePositiveRate': 0.0125,\n                            'recall': 0.9245283018867925\n                        }, {\n                            'confidenceThreshold': 0.4,\n                            'falsePositiveRate': 0.075,\n                            'recall': 0.9622641509433962\n                        }, {\n                            'confidenceThreshold': 0.3,\n                            'falsePositiveRate': 0.0875,\n                            'recall': 1.0\n                        }, {\n                            'confidenceThreshold': 0.2,\n                            'falsePositiveRate': 0.2375,\n                            'recall': 1.0\n                        }, {\n                            'confidenceThreshold': 0.1,\n                            'falsePositiveRate': 0.475,\n                            'recall': 1.0\n                        }, {\n                            'confidenceThreshold': 0.0,\n                            'falsePositiveRate': 1.0,\n                            'recall': 1.0\n                        }]\n                    }\n                },\n                'name': 'metrics',\n                'type': 'system.ClassificationMetrics'\n            }],\n            'parameters': {}\n        },\n        'type': 'system.ContainerExecution',\n        'state': Execution.State.COMPLETE,\n    }, wine_classification.get_dict())\n    t.assertEqual(\n        {\n            'inputs': {\n                'artifacts': [],\n                'parameters': {\n                    'test_samples_fraction': 0.3\n                }\n            },\n            'name': 'iris-sgdclassifier',\n            'outputs': {\n                'artifacts': [{\n                    'metadata': {\n                        'display_name': 'metrics',\n                        'confusionMatrix': {'struct': {\n                            'annotationSpecs': [{\n                                'displayName': 'Setosa'\n                            }, {\n                                'displayName': 'Versicolour'\n                            }, {\n                                'displayName': 'Virginica'\n                            }],\n                            'rows': [{ # these numbers can be random during execution\n                                'row': [mock.ANY, mock.ANY, mock.ANY]\n                            }, {\n                                'row': [mock.ANY, mock.ANY, mock.ANY]\n                            }, {\n                                'row': [mock.ANY, mock.ANY, mock.ANY]\n                            }]\n                        }}\n                    },\n                    'name': 'metrics',\n                    'type': 'system.ClassificationMetrics'\n                }],\n                'parameters': {}\n            },\n            'type': 'system.ContainerExecution',\n            'state': Execution.State.COMPLETE,\n        }, iris_sgdclassifier.get_dict()\n    )\n    rows = iris_sgdclassifier.get_dict()['outputs']['artifacts'][0]['metadata'][\n        'confusionMatrix']['struct']['rows']\n    for i, row in enumerate(rows):\n        for j, item in enumerate(row['row']):\n            t.assertIsInstance(\n                item, float,\n                f'value of confusion matrix row {i}, col {j} is not a number'\n            )\n\n    t.assertEqual({\n        'inputs': {\n            'artifacts': [],\n            'parameters': {}\n        },\n        'name': 'digit-classification',\n        'outputs': {\n            'artifacts': [{\n                'metadata': {\n                    'display_name': 'metrics',\n                    'accuracy': 92.0,\n                },\n                'name': 'metrics',\n                'type': 'system.Metrics'\n            }],\n            'parameters': {}\n        },\n        'type': 'system.ContainerExecution',\n        'state': Execution.State.COMPLETE,\n    }, digit_classification.get_dict())\n\n    t.assertEqual({\n        'inputs': {\n            'artifacts': [], \n            'parameters': {}\n        },\n        'name': 'html-visualization',\n        'outputs': {\n            'artifacts': [{\n                'metadata': {\n                    'display_name': 'html_artifact'\n                },\n                'name': 'html_artifact',\n                'type': 'system.HTML'\n            }],\n            'parameters': {}\n        },\n        'state': Execution.State.COMPLETE,\n        'type': 'system.ContainerExecution'\n    }, html_visualization.get_dict())\n\n    t.assertEqual({\n        'inputs': {\n            'artifacts': [], \n            'parameters': {}\n        },\n        'name': 'markdown-visualization',\n        'outputs': {\n            'artifacts': [{\n                'metadata': {\n                    'display_name': 'markdown_artifact'\n                },\n                'name': 'markdown_artifact',\n                'type': 'system.Markdown'\n            }],\n            'parameters': {}\n        },\n        'state': Execution.State.COMPLETE,\n        'type': 'system.ContainerExecution'\n    }, markdown_visualization.get_dict())\n\n\nrun_pipeline_func([\n    TestCase(\n        pipeline_func=metrics_visualization_pipeline,\n        verify_func=verify,\n        mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE\n    ),\n    TestCase(\n        pipeline_func=metrics_visualization_pipeline,\n        verify_func=verify,\n        mode=kfp.dsl.PipelineExecutionMode.V2_ENGINE\n    ),\n])\n"
  },
  {
    "repo": "9rince/kfp",
    "file_path": "samples/test/parameter_with_format.py",
    "raw_url": "https://raw.githubusercontent.com/9rince/kfp/develop/samples/test/parameter_with_format.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom kfp import components\nfrom kfp import dsl\n\n\n@components.create_component_from_func\ndef print_op(name: str) -> str:\n    print(name)\n    return name\n\n\n@dsl.pipeline(name='pipeline-with-pipelineparam-containing-format')\ndef my_pipeline(name: str = 'KFP'):\n    print_task = print_op('Hello {}'.format(name))\n    print_op('{}, again.'.format(print_task.output))\n"
  },
  {
    "repo": "9rince/kfp",
    "file_path": "samples/test/parameter_with_format_test.py",
    "raw_url": "https://raw.githubusercontent.com/9rince/kfp/develop/samples/test/parameter_with_format_test.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp\nfrom .parameter_with_format import my_pipeline\nfrom .util import run_pipeline_func, TestCase\n\n\ndef verify(run, run_id: str, **kwargs):\n    assert run.status == 'Succeeded'\n    # TODO(Bobgy): verify output\n\n\nrun_pipeline_func([\n    TestCase(\n        pipeline_func=my_pipeline,\n        verify_func=verify,\n    ),\n    TestCase(\n        pipeline_func=my_pipeline,\n        verify_func=verify,\n        mode=kfp.dsl.PipelineExecutionMode.V2_ENGINE,\n    ),\n    TestCase(\n        pipeline_func=my_pipeline,\n        mode=kfp.dsl.PipelineExecutionMode.V1_LEGACY,\n    ),\n])\n"
  },
  {
    "repo": "9rince/kfp",
    "file_path": "samples/test/placeholder_concat.py",
    "raw_url": "https://raw.githubusercontent.com/9rince/kfp/develop/samples/test/placeholder_concat.py",
    "content": "# Copyright 2020 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom kfp import components\nfrom kfp.v2 import dsl\nimport kfp.v2.compiler as compiler\n\ncomponent_op = components.load_component_from_text(\"\"\"\nname: Component with concat placeholder\ninputs:\n- {name: input_one, type: String}\n- {name: input_two, type: String}\nimplementation:\n  container:\n    image: gcr.io/google-containers/busybox\n    command:\n    - sh\n    - -ec\n    args:\n    - echo \"$0\" > /tmp/test && [[ \"$0\" == 'one+two=three' ]]\n    - concat: [{inputValue: input_one}, '+', {inputValue: input_two}, '=three']\n\"\"\")\n\n\n@dsl.pipeline(name='one-step-pipeline-with-concat-placeholder')\ndef pipeline_with_concat_placeholder():\n  component = component_op(input_one='one', input_two='two')\n"
  },
  {
    "repo": "raviranjan0309/kubeflow-fairing-pipeline",
    "file_path": "pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/raviranjan0309/kubeflow-fairing-pipeline/master/pipeline.py",
    "content": "import kfp.dsl as dsl\nimport kfp.gcp as gcp\nimport kfp\nfrom kubernetes.client.models import V1EnvVar\nimport warnings\nwarnings.filterwarnings('ignore')\n\n@dsl.pipeline(\n    name='Kubeflow Fairing Pipeline',\n    description='Embedding Kubeflow fairing inside Kubeflow Pipeline',\n)\ndef pipeline():\n    lightgbm_train = dsl.ContainerOp(\n        name='lightgbm_training',\n        image='gcr.io/<GCP PROJECT ID>/lightgbm-model:latest'          \n        ).apply(gcp.use_gcp_secret(secret_name='user-gcp-sa'))\n\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(pipeline, __file__ + '.zip')\n"
  },
  {
    "repo": "fybrik/kfp-components",
    "file_path": "samples/house_price_estimates/pipeline-argo.py",
    "raw_url": "https://raw.githubusercontent.com/fybrik/kfp-components/main/samples/house_price_estimates/pipeline-argo.py",
    "content": "import kfp.dsl as dsl\nimport kfp.components as components\n\n@dsl.pipeline(\n    name = \"Fybrik housing price estimate pipeline\",\n    description = \"Pipeline that provides data policy governed access to cataloged data, analyses data, trains model, and writes the results and catalogs them\"\n)\n\ndef get_current_namespace():\n    import kubernetes\n    import os\n\n    ns_path = \"/var/run/secrets/kubernetes.io/serviceaccount/namespace\"\n    if os.path.exists(ns_path):\n        with open(ns_path) as f:\n            return f.read().strip()\n    try:\n        _, active_context = kubernetes.config.list_kube_config_contexts()\n        return active_context[\"context\"][\"namespace\"]\n    except KeyError:\n        return \"kubeflow\"\n\ndef houseprice_pipeline(\n    test_dataset_id: str,\n    train_dataset_id: str\n):\n    # Get the namespace in which kfp is running\n    namespace = get_current_namespace()\n\n    # Get the ID of the run.  Make sure it's lower case and starts with a letter for our use\n    run_name = \"run-\" + dsl.RUN_ID_PLACEHOLDER.lower()\n\n    # Where to store parameters passed between workflow steps\n    result_name = \"submission-\" + str(run_name)\n\n    getDataEndpointsOp = components.load_component_from_file('../../get_data_endpoints/component.yaml')\n    getDataEndpointsStep = getDataEndpointsOp(train_dataset_id=train_dataset_id, test_dataset_id=test_dataset_id, namespace=namespace, run_name=run_name, result_name=result_name)\n    \n    visualizeTableOp = components.load_component_from_file('./visualize_table/component.yaml')\n    visualizeTableStep = visualizeTableOp(train_endpoint='%s'% getDataEndpointsStep.outputs['train_endpoint'], train_dataset_id=train_dataset_id, namespace=namespace)\n    visualizeTableStep.after(getDataEndpointsStep)\n\n    trainModelOp = components.load_component_from_file('./train_model/component.yaml')\n    trainModelStep = trainModelOp(train_endpoint_path='%s' % getDataEndpointsStep.outputs['train_endpoint'],\n                                test_endpoint_path='%s' % getDataEndpointsStep.outputs['test_endpoint'],\n                                result_name=result_name,\n                                result_endpoint_path='%s' % getDataEndpointsStep.outputs['result_endpoint'],\n                                train_dataset_id=train_dataset_id,\n                                test_dataset_id=test_dataset_id,\n                                namespace=namespace)\n    trainModelStep.after(visualizeTableStep)\n\n    sumbitResultOp = components.load_component_from_file('./submit_result/component.yaml')\n    submitResultStep = sumbitResultOp(getDataEndpointsStep.outputs['result_catalogid'])\n    submitResultStep.after(trainModelStep)\n \nif __name__ == '__main__':\n\n    # Set environment values to ensure persistent volumes used to pass parameters are allocated successfully \n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--test_dataset_id', type=str)\n    parser.add_argument('--train_dataset_id', type=str)\n \n    args = parser.parse_args()\n    import kfp.compiler as compiler\n    compiler.Compiler().compile(pipeline_func=houseprice_pipeline, package_path=__file__.replace('.py', '.yaml'))\n    "
  },
  {
    "repo": "fybrik/kfp-components",
    "file_path": "samples/house_price_estimates/pipeline-tekton.py",
    "raw_url": "https://raw.githubusercontent.com/fybrik/kfp-components/main/samples/house_price_estimates/pipeline-tekton.py",
    "content": "import kfp.dsl as dsl\nimport kfp.components as components\n\n@dsl.pipeline(\n    name = \"Fybrik housing price estimate pipeline\",\n    description = \"Pipeline that provides data policy governed access to cataloged data, analyses data, trains model, and writes the results and catalogs them\"\n)\n\ndef get_current_namespace():\n    import kubernetes\n    import os\n\n    ns_path = \"/var/run/secrets/kubernetes.io/serviceaccount/namespace\"\n    if os.path.exists(ns_path):\n        with open(ns_path) as f:\n            return f.read().strip()\n    try:\n        _, active_context = kubernetes.config.list_kube_config_contexts()\n        return active_context[\"context\"][\"namespace\"]\n    except KeyError:\n        return \"kubeflow\"\n\ndef houseprice_pipeline(\n    test_dataset_id: str,\n    train_dataset_id: str\n):\n    # Get the namespace in which kfp is running\n    namespace = get_current_namespace()\n\n    # Get the ID of the run.  Make sure it's lower case and starts with a letter for our use\n    run_name = \"run-\" + dsl.RUN_ID_PLACEHOLDER.lower()\n\n    # Where to store parameters passed between workflow steps\n    result_name = \"submission-\" + str(run_name)\n\n    getDataEndpointsOp = components.load_component_from_file('../../get_data_endpoints/component.yaml')\n    getDataEndpointsStep = getDataEndpointsOp(train_dataset_id=train_dataset_id, test_dataset_id=test_dataset_id, namespace=namespace, run_name=run_name, result_name=result_name)\n    \n    visualizeTableOp = components.load_component_from_file('./visualize_table/component.yaml')\n    visualizeTableStep = visualizeTableOp(train_endpoint='%s'% getDataEndpointsStep.outputs['train_endpoint'], train_dataset_id=train_dataset_id, namespace=namespace)\n    visualizeTableStep.after(getDataEndpointsStep)\n\n    trainModelOp = components.load_component_from_file('./train_model/component.yaml')\n    trainModelStep = trainModelOp(train_endpoint_path='%s' % getDataEndpointsStep.outputs['train_endpoint'],\n                                test_endpoint_path='%s' % getDataEndpointsStep.outputs['test_endpoint'],\n                                result_name=result_name,\n                                result_endpoint_path='%s' % getDataEndpointsStep.outputs['result_endpoint'],\n                                train_dataset_id=train_dataset_id,\n                                test_dataset_id=test_dataset_id,\n                                namespace=namespace)\n    trainModelStep.after(visualizeTableStep)\n\n    sumbitResultOp = components.load_component_from_file('./submit_result/component.yaml')\n    submitResultStep = sumbitResultOp(getDataEndpointsStep.outputs['result_catalogid'])\n    submitResultStep.after(trainModelStep)\n \nif __name__ == '__main__':\n\n    # Set environment values to ensure persistent volumes used to pass parameters are allocated successfully \n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--test_dataset_id', type=str)\n    parser.add_argument('--train_dataset_id', type=str)\n\n    args = parser.parse_args()\n    from kfp_tekton.compiler import TektonCompiler\n \n    TektonCompiler().compile(houseprice_pipeline, __file__.replace('.py', '.yaml'))\n    "
  },
  {
    "repo": "ajinkya933/Kubeflow",
    "file_path": "mnist_REST_API.py",
    "raw_url": "https://raw.githubusercontent.com/ajinkya933/Kubeflow/master/mnist_REST_API.py",
    "content": "import kfp\nimport kfp.dsl as dsl\nimport kfp.components as comp\n\n\ndef train(data_path, model_file):\n    '''\n        This definition contains MNIST training steps:\n            * Data Import\n            * Data Preprocessing\n            * Keras model creation\n            * Model optimizer : adam\n            * Training with specified epoch\n            * Print test accuracy\n            * Save the model\n    '''\n    import pickle\n    import tensorflow as tf\n    from tensorflow.python import keras\n\n    # Load dataset\n    fashion_mnist = keras.datasets.fashion_mnist\n    (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n\n    # Normalize dataset\n    train_images = train_images / 255.0\n    test_images = test_images / 255.0\n\n    # Define the model using keras\n    model = keras.Sequential([\n        keras.layers.Flatten(input_shape=(28, 28)),\n        keras.layers.Dense(128, activation='relu'),\n        keras.layers.Dense(10)\n\n    ])\n\n    model.compile(optimizer='adam', loss=keras.losses.SparseCategoricalCrossentropy(\n        from_logits=True), metrics=['accuracy'])\n    model.fit(train_images, train_labels, epochs=10)\n    test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n    print('Test accuracy', test_acc)\n\n    # IMP this specifies the path inside the Docker container where our model will be saved\n    model.save(f'{data_path}/{model_file}')\n\n    # Save test data pickle file\n    with open(f'{data_path}/test_data', 'wb') as f:\n        pickle.dump((test_images, test_labels), f)\n\n\n# Glue together training function to the docker container\ntrain_op = comp.func_to_container_op(train, base_image='tensorflow/tensorflow:latest-gpu-py3')\n\n\n# define pipeline metadata like name, description etc.\n@dsl.pipeline(\n    name='MNIST Pipeline',\n    description='Pipeline that trains MNIST models on GPU'\n)\n# define virtual HDD space that the pipeline will take to run\ndef mnist_container_pipeline(data_path='/mnt', model_file='mnist_model.h5'):\n    vop = dsl.VolumeOp(\n        name='create_volume',\n        resource_name='data-volume',\n        size='1Gi',\n        modes=dsl.VOLUME_MODE_RWM\n    )\n\n# We have already created a volume and a Glued component(Docker+Python script). This Glued component needs \n# to communicate with the volume so lets attach a volume to Glued component \n    mnist_training_container = train_op(data_path, model_file) \\\n        .add_pvolumes({data_path: vop.volume})\n\n\n\n# pass these paths inside the volume that we made\npipeline_func = mnist_container_pipeline\nexperiment_name = 'fashion_mnist_kubeflow_training2'\nrun_name = pipeline_func.__name__ + ' run'\n\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(pipeline_func, __file__ + '.yaml')\n\n\n\n\n"
  },
  {
    "repo": "ajinkya933/Kubeflow",
    "file_path": "mnist_REST_API_temp.py",
    "raw_url": "https://raw.githubusercontent.com/ajinkya933/Kubeflow/master/mnist_REST_API_temp.py",
    "content": "import kfp\nimport sys\nimport kfp.dsl as dsl\nimport kfp.components as comp\n\n\ndef train(data_path, model_file):\n    '''\n        This definition contains MNIST training steps:\n            * Data Import\n            * Data Preprocessing\n            * Keras model creation\n            * Model optimizer : adam\n            * Training with specified epoch\n            * Print test accuracy\n            * Save the model\n    '''\n    import pickle\n    import tensorflow as tf\n    from tensorflow.python import keras\n\n    # Load dataset\n    fashion_mnist = keras.datasets.fashion_mnist\n    (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n\n    # Normalize dataset\n    train_images = train_images / 255.0\n    test_images = test_images / 255.0\n\n    # Define the model using keras\n    model = keras.Sequential([\n        keras.layers.Flatten(input_shape=(28, 28)),\n        keras.layers.Dense(128, activation='relu'),\n        keras.layers.Dense(10)\n\n    ])\n\n    model.compile(optimizer='adam', loss=keras.losses.SparseCategoricalCrossentropy(\n        from_logits=True), metrics=['accuracy'])\n    model.fit(train_images, train_labels, epochs=10)\n    test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n    print('Test accuracy', test_acc)\n\n    # IMP this specifies the path inside the Docker container where our model will be saved\n    model.save(f'{data_path}/{model_file}')\n\n    # Save test data pickle file\n    with open(f'{data_path}/test_data', 'wb') as f:\n        pickle.dump((test_images, test_labels), f)\n\n\n# Glue together training function to the docker container\ntrain_op = comp.func_to_container_op(train, base_image='tensorflow/tensorflow:latest-gpu-py3')\n\n\n# define pipeline metadata like name, description etc.\n@dsl.pipeline(\n    name='MNIST Pipeline',\n    description='Pipeline that trains MNIST models on GPU'\n)\n# define virtual HDD space that the pipeline will take to run\ndef mnist_container_pipeline(data_path='/mnt', model_file='mnist_model.h5'):\n    vop = dsl.VolumeOp(\n        name='create_volume',\n        resource_name='data-volume',\n        size='1Gi',\n        modes=dsl.VOLUME_MODE_RWM\n    )\n\n# We have already created a volume and a Glued component(Docker+Python script). This Glued component needs \n# to communicate with the volume so lets attach a volume to Glued component \n    mnist_training_container = train_op(data_path, model_file) \\\n        .add_pvolumes({data_path: vop.volume})\n\n\n\n# pass these paths inside the volume that we made\npipeline_func = mnist_container_pipeline\n#experiment_name = 'fashion_mnist_kubeflow_training2'\nexperiment_name = sys.argv[0]\nrun_name = pipeline_func.__name__ + ' run'\n\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(pipeline_func, __file__ + '.yaml')\n\n\n\n\n"
  },
  {
    "repo": "ajinkya933/Kubeflow",
    "file_path": "mnist_complete_train.py",
    "raw_url": "https://raw.githubusercontent.com/ajinkya933/Kubeflow/master/mnist_complete_train.py",
    "content": "import kfp\nimport kfp.dsl as dsl\nimport kfp.components as comp\n\n\ndef train(data_path, model_file):\n    '''\n        This definition contains MNIST training steps:\n            * Data Import\n            * Data Preprocessing\n            * Keras model creation\n            * Model optimizer : adam\n            * Training with specified epoch\n            * Print test accuracy\n            * Save the model\n    '''\n    import pickle\n    import tensorflow as tf\n    from tensorflow.python import keras\n\n    # Load dataset\n    fashion_mnist = keras.datasets.fashion_mnist\n    (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n\n    # Normalize dataset\n    train_images = train_images / 255.0\n    test_images = test_images / 255.0\n\n    # Define the model using keras\n    model = keras.Sequential([\n        keras.layers.Flatten(input_shape=(28, 28)),\n        keras.layers.Dense(128, activation='relu'),\n        keras.layers.Dense(10)\n\n    ])\n\n    model.compile(optimizer='adam', loss=keras.losses.SparseCategoricalCrossentropy(\n        from_logits=True), metrics=['accuracy'])\n    model.fit(train_images, train_labels, epochs=10)\n    test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n    print('Test accuracy', test_acc)\n\n    # IMP this specifies the path inside the Docker container where our model will be saved\n    model.save(f'{data_path}/{model_file}')\n\n    # Save test data pickle file\n    with open(f'{data_path}/test_data', 'wb') as f:\n        pickle.dump((test_images, test_labels), f)\n\ndef predict(data_path, model_file, image_number):\n    \n    # func_to_container_op requires packages to be imported inside of the function.\n    import pickle\n\n    import tensorflow as tf\n    from tensorflow import keras\n\n    import numpy as np\n    \n    # Load the saved Keras model\n    model = keras.models.load_model(f'{data_path}/{model_file}')\n\n    # Load and unpack the test_data\n    with open(f'{data_path}/test_data','rb') as f:\n        test_data = pickle.load(f)\n    # Separate the test_images from the test_labels.\n    test_images, test_labels = test_data\n    # Define the class names.\n    class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n                   'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n\n    # Define a Softmax layer to define outputs as probabilities\n    probability_model = tf.keras.Sequential([model, \n                                            tf.keras.layers.Softmax()])\n\n    # See https://github.com/kubeflow/pipelines/issues/2320 for explanation on this line.\n    image_number = int(image_number)\n\n    # Grab an image from the test dataset.\n    img = test_images[image_number]\n\n    # Add the image to a batch where it is the only member.\n    img = (np.expand_dims(img,0))\n\n    # Predict the label of the image.\n    predictions = probability_model.predict(img)\n\n    # Take the prediction with the highest probability\n    prediction = np.argmax(predictions[0])\n\n    # Retrieve the true label of the image from the test labels.\n    true_label = test_labels[image_number]\n    \n    class_prediction = class_names[prediction]\n    confidence = 100*np.max(predictions)\n    actual = class_names[true_label]\n    \n    \n    with open(f'{data_path}/result.txt', 'w') as result:\n        result.write(\" Prediction: {} | Confidence: {:2.0f}% | Actual: {}\".format(class_prediction,\n                                                                        confidence,\n                                                                        actual))\n    \n    print('Prediction has be saved successfully!')\n\n# Glue together training and inference function to the docker container\ntrain_op = comp.func_to_container_op(train, base_image='tensorflow/tensorflow:latest-gpu-py3')\npredict_op = comp.func_to_container_op(predict, base_image='tensorflow/tensorflow:latest-gpu-py3')\n\n\n# define pipeline metadata like name, description etc.\n@dsl.pipeline(\n    name='MNIST Pipeline for train and prediction',\n    description='Pipeline that trains MNIST models on GPU'\n)\n# define virtual HDD space that the pipeline will take to run\ndef mnist_container_pipeline(data_path='/mnt', model_file='mnist_model.h5', IMAGE_NUMBER='0'):\n    vop = dsl.VolumeOp(\n        name='create_volume',\n        resource_name='data-volume',\n        size='1Gi',\n        modes=dsl.VOLUME_MODE_RWM\n    )\n\n# We have already created a volume and a Glued component(Docker+Python script). This Glued component needs \n# to communicate with the volume so lets attach a volume to Glued component \n    mnist_training_container = train_op(data_path, model_file) \\\n        .add_pvolumes({data_path: vop.volume})\n\n    # Create MNIST prediction component.\n    mnist_predict_container = predict_op(data_path, model_file, IMAGE_NUMBER) \\\n                                    .add_pvolumes({data_path: mnist_training_container.pvolume})\n\n\n    # Print the result of the prediction\n    mnist_result_container = dsl.ContainerOp(\n        name=\"print_prediction\",\n        image='library/bash:4.4.23',\n        pvolumes={data_path: mnist_predict_container.pvolume},\n        arguments=['cat', f'{data_path}/result.txt']\n    )\n\n    #vop.delete().after(mnist_result_container)\n    #vop.delete()\n\n# pass these paths inside the volume that we made\npipeline_func = mnist_container_pipeline\n# experiment_name = 'fashion_mnist_kubeflow_training_prediction'\nrun_name = pipeline_func.__name__ + ' run'\n\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(pipeline_func, __file__ + '.yaml')\n\n\n"
  },
  {
    "repo": "omkarakaya/kubeflow-recommender",
    "file_path": "pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/omkarakaya/kubeflow-recommender/main/pipeline.py",
    "content": "import kfp.dsl as dsl\nimport kfp.gcp as gcp\nimport kfp.onprem as onprem\n\nfrom string import Template\nimport json\n\n@dsl.pipeline(\n  name='Kubeflow Pipeline Test',\n  description='Kubeflow Pipeline Test'\n)\ndef xgb_train_pipeline(\n    output,\n    project,\n    region='us-central1',\n    train_data='gs://ml-pipeline-playground/sfpd/train.csv',\n    eval_data='gs://ml-pipeline-playground/sfpd/eval.csv',\n    schema='gs://ml-pipeline-playground/sfpd/schema.json',\n    target='resolution',\n    rounds=200,\n    workers=2,\n    true_label='ACTION'):\n    #vol_common = dsl.PipelineVolume()\n    vol_common = dsl.VolumeOp(\n        name=\"create_pvc\",\n        resource_name=\"my-pvc\",\n        modes=dsl.VOLUME_MODE_RWO,\n        size=\"1Gi\"\n    )\n\n    preprocess = dsl.ContainerOp(\n        name='preprocess',\n        image='gcr.io/compose-flask/hub:v6',\n        arguments=[\n            '--project', project,\n            '--mode', 'cloud',\n        ],\n        file_outputs={'output': '/tmp/output.txt'},\n        pvolumes={\"/data\": vol_common.volume}\n    )\n    preprocess.after(vol_common)\n    build = dsl.ContainerOp(\n        name='build',\n        image='gcr.io/compose-flask/build:v6',\n        arguments=[\n            '--project', project,\n            '--mode', 'cloud',\n            '--preprocessed', preprocess.outputs['output'],\n            '--preprocessed2', preprocess.output\n        ],\n        file_outputs={'output': '/tmp/output.txt'},\n        pvolumes={\"/data\": vol_common.volume}\n    )\n    build.after(vol_common)\n    build.after(preprocess)"
  },
  {
    "repo": "II-VSB-II/TaxiClassificationKubeflowPipelineMinio",
    "file_path": "Taxi-Pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/II-VSB-II/TaxiClassificationKubeflowPipelineMinio/main/Taxi-Pipeline.py",
    "content": "import kfp\r\nfrom kfp import components\r\nfrom kfp import dsl\r\nfrom kfp import gcp\r\nfrom kfp import onprem\r\nfrom kubernetes.client.models import V1EnvVar\r\n\r\n\r\nsecretKey = V1EnvVar(name='MINIO_SECRET_KEY', value='minio123')\r\naccessKey = V1EnvVar(name='MINIO_ACCESS_KEY', value='minio')\r\nminio_endpoint = V1EnvVar(name='MINIO_ENDPOINT', value='minio-service:9000')\r\n\r\nplatform = 'local'\r\n\r\n#proxy=\"http://test:8080\"\r\nproxy = \"\"\r\n\r\ndataflow_tf_data_validation_op  = components.load_component_from_url(\r\n  'https://raw.githubusercontent.com/II-VSB-II/TaxiClassificationKubeflowPipelineMinio/main/yamls/tfdv_component.yaml')\r\ndataflow_tf_transform_op        = components.load_component_from_url(\r\n  'https://raw.githubusercontent.com/II-VSB-II/TaxiClassificationKubeflowPipelineMinio/main/yamls/tft_component.yaml')\r\ntf_train_op                     = components.load_component_from_url(\r\n  'https://raw.githubusercontent.com/II-VSB-II/TaxiClassificationKubeflowPipelineMinio/main/yamls/dnntrainer_component.yaml')\r\ndataflow_tf_model_analyze_op    = components.load_component_from_url(\r\n  'https://raw.githubusercontent.com/II-VSB-II/TaxiClassificationKubeflowPipelineMinio/main/yamls/tfma_component.yaml')\r\ndataflow_tf_predict_op          = components.load_component_from_url(\r\n  'https://raw.githubusercontent.com/II-VSB-II/TaxiClassificationKubeflowPipelineMinio/main/yamls/predict_component.yaml')\r\n\r\nconfusion_matrix_op             = components.load_component_from_url(\r\n  'https://raw.githubusercontent.com/II-VSB-II/TaxiClassificationKubeflowPipelineMinio/main/yamls/confusion_matrix_component.yaml')\r\nroc_op                          = components.load_component_from_url(\r\n  'https://raw.githubusercontent.com/II-VSB-II/TaxiClassificationKubeflowPipelineMinio/main/yamls/roc_component.yaml')\r\n\r\n@dsl.pipeline(\r\n  name='TFX Taxi Cab Classification Pipeline Example',\r\n  description='Example pipeline that does classification with model analysis based on a public BigQuery dataset.'\r\n)\r\ndef taxi_cab_classification(\r\n    project,\r\n    output=\"/mnt/shared\",\r\n    column_names='/mnt/shared/pipelines/column-names.json',\r\n    key_columns='trip_start_timestamp',\r\n    train='/mnt/shared/pipelines/train.csv',\r\n    evaluation='/mnt/shared/pipelines/eval.csv',\r\n    mode='local',\r\n    preprocess_module='/mnt/shared/pipelines/preprocessing.py',\r\n    learning_rate=0.1,\r\n    hidden_layer_size='1500',\r\n    steps=3000,\r\n    analyze_slice_column='trip_start_hour'\r\n):\r\n    output_template = str(output) + '/{{workflow.uid}}/{{pod.name}}/data'\r\n    target_lambda = \"\"\"lambda x: (x['target'] > x['fare'] * 0.2)\"\"\"\r\n    target_class_lambda = \"\"\"lambda x: 1 if (x['target'] > x['fare'] * 0.2) else 0\"\"\"\r\n\r\n    tf_server_name = 'taxi-cab-classification-model-{{workflow.uid}}'\r\n\r\n    if platform != 'GCP':\r\n        vop = dsl.VolumeOp(\r\n            name=\"create_pvc\",\r\n            resource_name=\"pipeline-pvc\",\r\n            modes=dsl.VOLUME_MODE_RWM,\r\n            size=\"1Gi\"\r\n        )\r\n        if proxy != \"\":\r\n            checkout = dsl.ContainerOp(\r\n            name=\"checkout\",\r\n            image=\"alpine/git:latest\",\r\n            command=[\"git\", \"clone\", \"https://github.com/II-VSB-II/TaxiClassificationKubeflowPipelineMinio.git\", str(output) + \r\n                     \"/pipelines\", \"-c\", \"http.proxy={}\".format(proxy)],\r\n        ).apply(onprem.mount_pvc(vop.outputs[\"name\"], 'local-storage', output))\r\n        else:\r\n            checkout = dsl.ContainerOp(\r\n            name=\"checkout\",\r\n            image=\"alpine/git:latest\",\r\n            command=[\"git\", \"clone\", \"https://github.com/II-VSB-II/TaxiClassificationKubeflowPipelineMinio.git\", str(output) + \"/pipelines\"],\r\n        ).apply(onprem.mount_pvc(vop.outputs[\"name\"], 'local-storage', output))\r\n            \r\n        \r\n        checkout.after(vop)\r\n\r\n    validation = dataflow_tf_data_validation_op(\r\n        inference_data=train,\r\n        validation_data=evaluation,\r\n        column_names=column_names,\r\n        key_columns=key_columns,\r\n        gcp_project=project,\r\n        run_mode=mode,\r\n        validation_output=output_template,\r\n    )\r\n    if platform != 'GCP':\r\n        validation.after(checkout)\r\n\r\n    preprocess = dataflow_tf_transform_op(\r\n        training_data_file_pattern=train,\r\n        evaluation_data_file_pattern=evaluation,\r\n        schema=validation.outputs['schema'],\r\n        gcp_project=project,\r\n        run_mode=mode,\r\n        preprocessing_module=preprocess_module,\r\n        transformed_data_dir=output_template\r\n    )\r\n\r\n    training = tf_train_op(\r\n        transformed_data_dir=preprocess.output,\r\n        schema=validation.outputs['schema'],\r\n        learning_rate=learning_rate,\r\n        hidden_layer_size=hidden_layer_size,\r\n        steps=steps,\r\n        target='tips',\r\n        preprocessing_module=preprocess_module,\r\n        training_output_dir=output_template\r\n    )\r\n\r\n    analysis = dataflow_tf_model_analyze_op(\r\n        model=training.output,\r\n        evaluation_data=evaluation,\r\n        schema=validation.outputs['schema'],\r\n        gcp_project=project,\r\n        run_mode=mode,\r\n        slice_columns=analyze_slice_column,\r\n        analysis_results_dir=output_template\r\n    ).add_env_variable(secretKey).add_env_variable(accessKey).add_env_variable(minio_endpoint)\r\n\r\n    prediction = dataflow_tf_predict_op(\r\n        data_file_pattern=evaluation,\r\n        schema=validation.outputs['schema'],\r\n        target_column='tips',\r\n        model=training.output,\r\n        run_mode=mode,\r\n        gcp_project=project,\r\n        predictions_dir=output_template\r\n    ).add_env_variable(secretKey).add_env_variable(accessKey).add_env_variable(minio_endpoint)\r\n\r\n\r\n    cm = confusion_matrix_op(\r\n        predictions=prediction.output,\r\n        target_lambda=target_lambda,\r\n        output_dir=output_template\r\n    ).add_env_variable(secretKey).add_env_variable(accessKey).add_env_variable(minio_endpoint)\r\n\r\n    roc = roc_op(\r\n        predictions_dir=prediction.output,\r\n        target_lambda=target_class_lambda,\r\n        output_dir=output_template\r\n    ).add_env_variable(secretKey).add_env_variable(accessKey).add_env_variable(minio_endpoint)\r\n\r\n\r\n    steps = [validation, preprocess, training, analysis, prediction, cm, roc]\r\n    \r\n    for step in steps:\r\n        step.apply(onprem.mount_pvc(vop.outputs[\"name\"], 'local-storage', output))\r\n\r\n\r\nif __name__ == '__main__':\r\n    kfp.compiler.Compiler().compile(taxi_cab_classification, \"TaxiPipelineMinio\" + '.zip')\r\n"
  },
  {
    "repo": "stackdemos/kf-flatcar2",
    "file_path": "components/golang/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/kf-flatcar2/master/components/golang/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef processing_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('GOLANG_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=[script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n"
  },
  {
    "repo": "stackdemos/kf-flatcar2",
    "file_path": "components/training/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/kf-flatcar2/master/components/training/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef training_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('TRAINING_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=['/usr/local/bin/python', script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n\n"
  },
  {
    "repo": "stackdemos/jjtest-ml",
    "file_path": "components/golang/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/jjtest-ml/master/components/golang/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef processing_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('GOLANG_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=[script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n"
  },
  {
    "repo": "stackdemos/jjtest-ml",
    "file_path": "components/training/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/jjtest-ml/master/components/training/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef training_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('TRAINING_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=['/usr/local/bin/python', script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n\n"
  },
  {
    "repo": "stackdemos/jj-test2",
    "file_path": "components/golang/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/jj-test2/master/components/golang/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef processing_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('GOLANG_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=[script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n"
  },
  {
    "repo": "stackdemos/jj-test2",
    "file_path": "components/training/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/jj-test2/master/components/training/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef training_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('TRAINING_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=['/usr/local/bin/python', script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n\n"
  },
  {
    "repo": "stackdemos/kfp125",
    "file_path": "components/golang/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/kfp125/master/components/golang/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef processing_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('GOLANG_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=[script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n"
  },
  {
    "repo": "stackdemos/kfp125",
    "file_path": "components/training/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/kfp125/master/components/training/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef training_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('TRAINING_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=['/usr/local/bin/python', script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n\n"
  },
  {
    "repo": "stackdemos/kcdemo",
    "file_path": "components/golang/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/kcdemo/master/components/golang/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef processing_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('GOLANG_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=[script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n"
  },
  {
    "repo": "stackdemos/kcdemo",
    "file_path": "components/training/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/kcdemo/master/components/training/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef training_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('TRAINING_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=['/usr/local/bin/python', script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n\n"
  },
  {
    "repo": "stackdemos/ghsumm2",
    "file_path": "components/golang/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/ghsumm2/master/components/golang/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef processing_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('GOLANG_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=[script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n"
  },
  {
    "repo": "stackdemos/ghsumm2",
    "file_path": "components/training/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/ghsumm2/master/components/training/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef training_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('TRAINING_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=['/usr/local/bin/python', script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n\n"
  },
  {
    "repo": "stackdemos/ml-kbf",
    "file_path": "components/golang/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/ml-kbf/master/components/golang/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef processing_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('GOLANG_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=[script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n"
  },
  {
    "repo": "stackdemos/ml-kbf",
    "file_path": "components/training/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/ml-kbf/master/components/training/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef training_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('TRAINING_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=['/usr/local/bin/python', script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n\n"
  },
  {
    "repo": "s102401002/kubeflowPipeline2",
    "file_path": "kubeflowPipeline_xgboost.py",
    "raw_url": "https://raw.githubusercontent.com/s102401002/kubeflowPipeline2/main/kubeflowPipeline_xgboost.py",
    "content": "# test\n\nfrom typing import NewType\n\nimport kfp\nimport kfp.compiler\n\nfrom kfp import dsl\nfrom kfp.dsl import OutputPath, InputPath, Input, Output, Artifact\n\nfrom pandas import DataFrame\n# from kfp.components import func_to_container_op\n\nDF = NewType('DF', DataFrame)\n\n@dsl.component(\n    base_image='python:3.9',\n    packages_to_install=['pandas==2.2.2']\n)\ndef load_data(data_output: Output[Artifact]):\n    import pandas as pd\n    \n    urls = [\n        \"https://raw.githubusercontent.com/daniel88516/diabetes-data/main/10k.csv\",\n        \"https://raw.githubusercontent.com/s102401002/kubeflowPipeline/main/data1.csv\"\n    ]\n    \n    standard_name_mapping = {\n        'gender': ['gender', 'gen', 'Gender', 'sex', 'Sex'],\n        'age': ['age', 'Age', 'AGE'],\n        'bmi': ['bmi', 'BMI', 'Bmi'],\n        'HbA1c_level': ['HbA1c_level', 'HbA1c', 'hba1c'],\n        'blood_glucose_level': ['blood_glucose_level', 'glucose', 'BloodGlucose'],\n        'diabetes': ['diabetes', 'Diabetes']\n    }\n\n    datas = [] # download all the csv in urls as a array\n    for url in urls:\n        df = pd.read_csv(url)\n        for standard_name, variants in standard_name_mapping.items():\n            for variant in variants:\n                if variant in df.columns:\n                    df.rename(columns={variant: standard_name}, inplace=True) # inplace=True: changing directly instead of creating a new column\n                    break\n        \n        datas.append(df)\n\n    df_data = pd.concat(datas, ignore_index=True)\n    \n    df_data = df_data.drop(df_data[df_data['diabetes'] == 'No Info'].index)\n    df_data = df_data[['gender','age', 'bmi', 'HbA1c_level', 'blood_glucose_level', 'diabetes']]\n    df_data = df_data.dropna(thresh=4)\n    \n    gender_map = {'Male': 0 , 'Female': 1  , 'Other': 2}\n    df_data['gender'] = df_data['gender'].map(gender_map)\n    df_data = df_data[df_data['gender'] != 2]\n    df_data['age'] = df_data['age'].replace('No Info', df_data['age'].mean())\n    df_data['bmi'] = df_data['bmi'].replace('No Info', df_data['bmi'].mean())\n    df_data['HbA1c_level'] = df_data['HbA1c_level'].replace('No Info', df_data['HbA1c_level'].mean())\n    df_data['blood_glucose_level'] = df_data['blood_glucose_level'].replace('No Info', df_data['blood_glucose_level'].mean())\n\n    df_data.to_csv(data_output.path)\n\n@dsl.component(\n    base_image='python:3.9',\n    packages_to_install=['pandas==2.2.2', 'scikit-learn==1.5.1']\n)\ndef prepare_data(\n    data_input: Input[Artifact], \n    x_train_output: Output[Artifact], x_test_output: Output[Artifact],\n    y_train_output: Output[Artifact], y_test_output: Output[Artifact]\n):\n    import pandas as pd\n    from sklearn.model_selection import train_test_split\n\n    df_data = pd.read_csv(data_input.path)\n\n    x = df_data.drop(labels=['diabetes'], axis=1)\n    y = df_data[['diabetes']]\n    \n    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n    \n    x_train_df = pd.DataFrame(x_train)\n    x_test_df = pd.DataFrame(x_test)\n    y_train_df = pd.DataFrame(y_train)\n    y_test_df = pd.DataFrame(y_test)\n\n    x_train_df.to_csv(x_train_output.path, index=False)\n    x_test_df.to_csv(x_test_output.path, index=False)\n    y_train_df.to_csv(y_train_output.path, index=False)\n    y_test_df.to_csv(y_test_output.path, index=False)\n\n@dsl.component(\n    base_image='python:3.9',\n    packages_to_install=['pandas==2.2.2', 'scikit-learn==1.5.1', 'joblib==1.4.2', 'xgboost==2.0.3']\n)\ndef train_model(x_train: Input[Artifact], y_train: Input[Artifact], train_model_output: Output[Artifact]):\n    import pandas as pd\n    from xgboost import XGBClassifier\n    import joblib\n    \n    x_train = pd.read_csv(x_train.path)\n    y_train = pd.read_csv(y_train.path)\n    \n    model = XGBClassifier(n_estimators=1000, learning_rate= 0.01)\n    model.fit(x_train, y_train.values.ravel())\n    \n    joblib.dump(model, train_model_output.path)\n\n@dsl.component(\n    base_image='python:3.9',\n    packages_to_install=['pandas==2.2.2', 'scikit-learn==1.5.1', 'joblib==1.4.2', 'xgboost==2.0.3']\n)\ndef evaluate_model(model_path: Input[Artifact], x_test: Input[Artifact], y_test: Input[Artifact]) -> str:\n    import pandas as pd\n    from sklearn.metrics import accuracy_score\n    import joblib\n\n    model = joblib.load(filename=model_path.path)\n\n    x_test_df = pd.read_csv(x_test.path)\n    y_test_df = pd.read_csv(y_test.path)\n    \n    y_pred = model.predict(x_test_df)\n    accuracy = accuracy_score(y_test_df, y_pred)\n    \n    return f'Test accuracy: {accuracy}'\n\n@dsl.pipeline(\n    name='Diabetes Prediction Pipeline',\n    description='Using kubeflow pipeline to train and evaluate a diabetes prediction model'\n)\ndef diabetes_prediction_pipeline() -> str:\n    load_data_task = load_data()\n\n    prepare_data_task = prepare_data(data_input=load_data_task.outputs['data_output'])\n    \n    train_model_task = train_model(\n        x_train = prepare_data_task.outputs['x_train_output'], \n        y_train = prepare_data_task.outputs['y_train_output']\n    )\n    \n    evaluate_model_task = evaluate_model(\n        model_path = train_model_task.outputs['train_model_output'], \n        x_test = prepare_data_task.outputs['x_test_output'], \n        y_test = prepare_data_task.outputs['y_test_output']\n    )\n    \n    return evaluate_model_task.output\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(diabetes_prediction_pipeline, 'diabetes_prediction_pipeline_xgboost.yaml')"
  },
  {
    "repo": "stackdemos/kf4",
    "file_path": "components/golang/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/kf4/master/components/golang/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef processing_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('GOLANG_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=[script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n"
  },
  {
    "repo": "stackdemos/kf4",
    "file_path": "components/training/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/kf4/master/components/training/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef training_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('TRAINING_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=['/usr/local/bin/python', script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n\n"
  },
  {
    "repo": "stackdemos/joe-kubeflow-test",
    "file_path": "components/golang/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/joe-kubeflow-test/master/components/golang/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef processing_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('GOLANG_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=[script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n"
  },
  {
    "repo": "stackdemos/joe-kubeflow-test",
    "file_path": "components/training/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/joe-kubeflow-test/master/components/training/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef training_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('TRAINING_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=['/usr/local/bin/python', script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n\n"
  },
  {
    "repo": "stackdemos/kfp11",
    "file_path": "components/golang/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/kfp11/master/components/golang/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef processing_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('GOLANG_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=[script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n"
  },
  {
    "repo": "stackdemos/kfp11",
    "file_path": "components/training/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/kfp11/master/components/training/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef training_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('TRAINING_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=['/usr/local/bin/python', script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n\n"
  },
  {
    "repo": "stackdemos/specs-kf5",
    "file_path": "components/golang/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/specs-kf5/master/components/golang/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef processing_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('GOLANG_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=[script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n"
  },
  {
    "repo": "stackdemos/specs-kf5",
    "file_path": "components/training/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/specs-kf5/master/components/training/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef training_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('TRAINING_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=['/usr/local/bin/python', script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n\n"
  },
  {
    "repo": "akranga/anton-ml1",
    "file_path": "components/golang/component.py",
    "raw_url": "https://raw.githubusercontent.com/akranga/anton-ml1/master/components/golang/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef processing_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('GOLANG_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=[script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n"
  },
  {
    "repo": "akranga/anton-ml1",
    "file_path": "components/training/component.py",
    "raw_url": "https://raw.githubusercontent.com/akranga/anton-ml1/master/components/training/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef training_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('TRAINING_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=['/usr/local/bin/python', script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n\n"
  },
  {
    "repo": "baebae-dev/kubeflow-pipelines",
    "file_path": "boston_housing/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/baebae-dev/kubeflow-pipelines/main/boston_housing/pipeline.py",
    "content": "import kfp\nfrom kfp import dsl\n\ndef preprocess_op():\n\n    return dsl.ContainerOp(\n        name='Preprocess Data',\n        image='gnovack/boston_pipeline_preprocessing:latest',\n        arguments=[],\n        file_outputs={\n            'x_train': '/app/x_train.npy',\n            'x_test': '/app/x_test.npy',\n            'y_train': '/app/y_train.npy',\n            'y_test': '/app/y_test.npy',\n        }\n    )\n\ndef train_op(x_train, y_train):\n\n    return dsl.ContainerOp(\n        name='Train Model',\n        image='gnovack/boston_pipeline_train:latest',\n        arguments=[\n            '--x_train', x_train,\n            '--y_train', y_train\n        ],\n        file_outputs={\n            'model': '/app/model.pkl'\n        }\n    )\n\ndef test_op(x_test, y_test, model):\n\n    return dsl.ContainerOp(\n        name='Test Model',\n        image='gnovack/boston_pipeline_test:latest',\n        arguments=[\n            '--x_test', x_test,\n            '--y_test', y_test,\n            '--model', model\n        ],\n        file_outputs={\n            'mean_squared_error': '/app/output.txt'\n        }\n    )\n\ndef deploy_model_op(model):\n\n    return dsl.ContainerOp(\n        name='Deploy Model',\n        image='gnovack/boston_pipeline_deploy_model:latest',\n        arguments=[\n            '--model', model\n        ]\n    )\n\n@dsl.pipeline(\n   name='Boston Housing Pipeline',\n   description='An example pipeline that trains and logs a regression model.'\n)\ndef boston_pipeline():\n    _preprocess_op = preprocess_op()\n    \n    _train_op = train_op(\n        dsl.InputArgumentPath(_preprocess_op.outputs['x_train']),\n        dsl.InputArgumentPath(_preprocess_op.outputs['y_train'])\n    ).after(_preprocess_op)\n\n    _test_op = test_op(\n        dsl.InputArgumentPath(_preprocess_op.outputs['x_test']),\n        dsl.InputArgumentPath(_preprocess_op.outputs['y_test']),\n        dsl.InputArgumentPath(_train_op.outputs['model'])\n    ).after(_train_op)\n\n    deploy_model_op(\n        dsl.InputArgumentPath(_train_op.outputs['model'])\n    ).after(_test_op)\n\nclient = kfp.Client()\nclient.create_run_from_pipeline_func(boston_pipeline, arguments={})"
  },
  {
    "repo": "stackdemos/secrets-tetst-01",
    "file_path": "components/golang/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/secrets-tetst-01/master/components/golang/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef processing_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('GOLANG_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=[script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n"
  },
  {
    "repo": "stackdemos/secrets-tetst-01",
    "file_path": "components/training/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/secrets-tetst-01/master/components/training/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef training_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('TRAINING_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=['/usr/local/bin/python', script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n\n"
  },
  {
    "repo": "stackdemos/kfappx",
    "file_path": "components/golang/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/kfappx/master/components/golang/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef processing_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('GOLANG_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=[script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n"
  },
  {
    "repo": "stackdemos/kfappx",
    "file_path": "components/training/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/kfappx/master/components/training/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef training_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('TRAINING_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=['/usr/local/bin/python', script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n\n"
  },
  {
    "repo": "akranga/machine-learning1",
    "file_path": "components/golang/component.py",
    "raw_url": "https://raw.githubusercontent.com/akranga/machine-learning1/master/components/golang/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef processing_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('GOLANG_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=[script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n"
  },
  {
    "repo": "akranga/machine-learning1",
    "file_path": "components/training/component.py",
    "raw_url": "https://raw.githubusercontent.com/akranga/machine-learning1/master/components/training/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef training_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('TRAINING_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=['/usr/local/bin/python', script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n\n"
  },
  {
    "repo": "ArianFotouhi/kubeflowPipelineSpamDetector",
    "file_path": "script.py",
    "raw_url": "https://raw.githubusercontent.com/ArianFotouhi/kubeflowPipelineSpamDetector/main/script.py",
    "content": "import kfp.dsl as dsl\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn import metrics\nimport string\nfrom typing import List\nimport joblib\n\n\n# Component 1: Extract data\n@dsl.component(\n    base_image='python:3.8',  # Specifying the base image\n    packages_to_install=['requests', 'pandas']\n)\ndef extract_data() -> dsl.OutputPath(str):\n    import requests\n    import zipfile\n    import io\n    import pandas as pd\n\n    url = 'https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip'\n\n    # Step 1: Download the zip file\n    response = requests.get(url)\n    response.raise_for_status()  # Check if the request was successful\n\n    # Step 2: Extract the contents of the zip file\n    with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n        with z.open('SMSSpamCollection') as f:\n            # Step 3: Read the contents into a DataFrame\n            df = pd.read_csv(f, sep='\\t', names=[\"label\", \"message\"], header=None)\n\n    # Step 4: Save the DataFrame to a CSV file\n    output_path = '/mnt/data/smsspamcollection.csv'\n    df.to_csv(output_path, index=False)\n\n    return output_path\n\n\n# Component 2: Data Preprocessing\n@dsl.component(\n    base_image='python:3.8',\n    packages_to_install=['pandas']\n)\ndef preprocess_data(file_path: dsl.InputPath(str)) -> None:\n    import pandas as pd\n    import string\n    \n    df = pd.read_csv(file_path)\n    \n    # Add 'length' and 'punct' features\n    df['length'] = df['message'].apply(len)\n    df['punct'] = df['message'].apply(lambda x: sum([1 for char in x if char in string.punctuation]))\n    \n    # Save the preprocessed data\n    df.to_csv('/mnt/data/preprocessed_smsspamcollection.csv', index=False)\n\n\n# Component 3: Exploratory Data Analysis (EDA)\n@dsl.component(\n    base_image='python:3.8',\n    packages_to_install=['pandas', 'matplotlib', 'numpy']\n)\ndef eda() -> None:\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    import numpy as np\n\n    df = pd.read_csv('/mnt/data/preprocessed_smsspamcollection.csv')\n\n    print('Missing values: ')\n    print(df.isnull().sum(),'\\n')\n\n    print('Categories: ',df['label'].unique(),'\\n')\n\n    print('Rate of each category: ')\n    print(df['label'].value_counts())\n\n    plt.xscale('log')\n    bins = 1.15**(np.arange(0,50))\n    plt.hist(df[df['label']=='ham']['length'], bins=bins,alpha=0.8)\n    plt.hist(df[df['label']=='spam']['length'], bins=bins,alpha=0.8)\n    plt.legend(('ham','spam'))\n    plt.title('Inference: usually spams are longer in text compared to ham')\n    plt.xlabel('Text length')\n    plt.ylabel('Category rate')\n    plt.savefig('/mnt/data/length_histogram.png')\n    plt.clf()\n\n    plt.xscale('log')\n    bins = 1.15**(np.arange(0,50))\n    plt.hist(df[df['label']=='ham']['punct'], bins=bins,alpha=0.8)\n    plt.hist(df[df['label']=='spam']['punct'], bins=bins,alpha=0.8)\n    plt.legend(('ham','spam'))\n    plt.title('Inference: a small tendency of spams towards more punctutations (not a firm inference)')\n    plt.xlabel('Text length')\n    plt.ylabel('Category rate')\n    plt.savefig('/mnt/data/punct_histogram.png')\n\n\n# Component 4: Train Model\n@dsl.component(\n    base_image='python:3.8',\n    packages_to_install=['pandas', 'scikit-learn', 'joblib']\n)\ndef train_model() -> None:\n    import pandas as pd\n    from sklearn.model_selection import train_test_split\n    from sklearn.pipeline import Pipeline\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.feature_extraction.text import TfidfVectorizer\n    from sklearn import metrics\n    import joblib\n\n    df = pd.read_csv('/mnt/data/preprocessed_smsspamcollection.csv')\n\n    X = df['message']\n    y = df['label']\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)\n\n    text_clf = Pipeline([('tfidf', TfidfVectorizer()), ('clf', RandomForestClassifier())])\n    text_clf.fit(X_train, y_train)\n\n    predictions = text_clf.predict(X_test)\n\n    df_conf_mat = pd.DataFrame(metrics.confusion_matrix(y_test, predictions), index=['ham', 'spam'], columns=['ham', 'spam'])\n    print(df_conf_mat, '\\n')\n\n    clf_report = metrics.classification_report(y_test, predictions)\n    print(clf_report, '\\n')\n\n    acc = metrics.accuracy_score(y_test, predictions)\n    print('Model accuracy: ', acc * 100)\n\n    # Save the model to a file\n    joblib.dump(text_clf, '/mnt/data/text_clf.joblib')\n\n\n# Component 5: Test Model\n@dsl.component(\n    base_image='python:3.8',\n    packages_to_install=['scikit-learn', 'joblib']\n)\ndef test_model( sample_messages: List[str]) -> List[str]:\n    import joblib\n    model = joblib.load('/mnt/data/text_clf.joblib')\n    predictions = model.predict(sample_messages)\n    return predictions\n\n\n# Define the pipeline\n@dsl.pipeline(\n    name='SMS Spam Detection Pipeline',\n    description='A pipeline for detecting spam messages from SMS data'\n)\ndef sms_spam_detection_pipeline():\n    # Step 1: Extract data\n    extracted_data = extract_data()\n    print(extracted_data.output)\n    # Step 2: Preprocess data\n    preprocess_data(\n        file_path=extracted_data.output,\n    )\n\n    # Step 3: Perform EDA\n    eda()\n\n    # Step 4: Train model\n    train_model()\n\n    # Step 5: Test model\n    test_samples = ['Hi, how you doing?', 'Congratulations! You have won a $1000 prize! Text 1 to 1423.']\n    test_output = test_model(sample_messages=test_samples)\n    print(test_output)\n\n# Compile the pipeline\nif __name__ == '__main__':\n    import kfp.compiler as compiler\n    compiler.Compiler().compile(sms_spam_detection_pipeline, 'sms_spam_detection_pipeline.yaml')\n"
  },
  {
    "repo": "felipeacunago/kubeflow",
    "file_path": "pipelines/prophet_prediction.py",
    "raw_url": "https://raw.githubusercontent.com/felipeacunago/kubeflow/master/pipelines/prophet_prediction.py",
    "content": "\"\"\"\nKubeflow Pipelines for timeseries prediction using fbprophet\nRun this script to compile pipeline\n\"\"\"\n\n\nimport kfp.dsl as dsl\nimport kfp.components as comp\nimport kfp.gcp as gcp\nimport json\n\nbigquery_query_op = comp.load_component_from_url(\n    'https://raw.githubusercontent.com/kubeflow/pipelines/e598176c02f45371336ccaa819409e8ec83743df/components/gcp/bigquery/query/component.yaml')\n\nranker_op = comp.load_component_from_url(\n    'https://raw.githubusercontent.com/membrilloski/kubeflow/master/ranker/component.yaml'\n)\n\n@dsl.pipeline(\n  name='Prophet',\n  description='A pipeline to train and serve the MNIST example.'\n)\ndef prophet_pipeline(dataset_query='',\n                   dictionary_query='',\n                   val_dataset_query='',\n                   minimum_length=10,\n                   training_date='2019-10-05',\n                   changepoint_prior_scale=0.01,\n                   evaluation_date='',\n                   evaluation_maximum_distance=1\n                    ):\n\n    \"\"\"\n      Pipeline with three stages:\n        1. Query data from Bigquery and save to storage\n        2. Generate a timeseries prediction for every distinct element in split_column\n        3. Generate a ranking based on prediction data and compare it to a ranking made with real data from that date\n    \"\"\"\n\n    rank_path_names=[''] # name of paths used for ranking (for example clicks)\n    ranking_factors=[] # factors used for ranking (score: a*x+b*y, ranking_factors=[a,b])\n    project_id='' # project name\n    split_column='' # column used to split the time series from the dataset\n    gcs_root='' # root path where all experiments files will be saved\n    prediction_y='' # y column name used for the timeseries\n    predict_periods=1 # number of periods used for timeseries prediction\n    dataset_location='US' # dataset location\n    ds_column='' # name of the column used for time\n    predict_freq='D' # prediction frequency (D: days)\n    order_ds = 'asc' # used to order items by date after splitting\n\n    original_dataset_path='{}/input/dataset.csv'.format(gcs_root)\n    val_dataset_path='{}/input/val_dataset.csv'.format(gcs_root)\n    val_split_output_path='{}/output/validation/'.format(gcs_root)\n    preprocess_output_path='{}/output/training/'.format(gcs_root)\n    model_output_path='{}/models/'.format(gcs_root)\n    predictions_path='{}/predictions/'.format(gcs_root)\n    dictionary_file_path = '{}/dictionary/dictionary.csv'.format(gcs_root)\n    prophet_rank_output_path= '{}/rankings/prediction/'.format(gcs_root)\n    validation_rank_output='{}/rankings/validation/'.format(gcs_root)\n    results_output='{}/results/'.format(gcs_root)\n\n    \n\n    dataset_query_op = bigquery_query_op(\n        query=dataset_query, \n        project_id=project_id,\n        output_gcs_path=original_dataset_path, \n        dataset_location=dataset_location, \n        job_config='').apply(gcp.use_gcp_secret('user-gcp-sa'))\n\n    preprocess = dsl.ContainerOp(\n        name='preprocess-split',\n        image='docker.io/felipeacunago/dataset-preprocess:latest',\n        arguments=[\n            \"task.py\",\n            \"--dataset-path\", dataset_query_op.outputs['output_gcs_path'],\n            \"--output-path\", preprocess_output_path,\n            \"--split-column\", split_column,\n            \"--ds-column\", ds_column,\n            \"--y-column\", prediction_y,\n            \"--minimum-length\", minimum_length,\n            \"--order-ds\", 'asc',\n            \"--training-date\", training_date\n            ]\n    ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n\n    timeseries_prophet_train = dsl.ContainerOp(\n        name='prophet-train',\n        image='docker.io/felipeacunago/timeseries-prophet-train:latest',\n        arguments=[\n            \"train.py\",\n            \"--dataset-path\", preprocess_output_path,\n            \"--changepoint-prior-scale\", changepoint_prior_scale,\n            \"--model-output-path\", model_output_path\n            ]\n    ).apply(gcp.use_gcp_secret('user-gcp-sa')).after(preprocess)\n\n    timeseries_prophet_predict = dsl.ContainerOp(\n        name='prophet-predict',\n        image='docker.io/felipeacunago/timeseries-prophet-predict:latest',\n        arguments=[\n            \"predict.py\",\n            \"--predict-periods\", predict_periods,\n            \"--predict-freq\", predict_freq,\n            \"--model-path\", model_output_path,\n            \"--predictions-path\", predictions_path\n            ]\n    ).apply(gcp.use_gcp_secret('user-gcp-sa')).after(timeseries_prophet_train)\n\n    dataset_query_op = bigquery_query_op(\n        query=dictionary_query, \n        project_id=project_id,\n        output_gcs_path=dictionary_file_path, \n        dataset_location=dataset_location, \n        job_config='').apply(gcp.use_gcp_secret('user-gcp-sa'))\n\n    prophet_rank = ranker_op(\n        input_path=predictions_path,\n        input_path_names=rank_path_names,\n        ranking_factors=ranking_factors,\n        input_dictionary=dataset_query_op.outputs['output_gcs_path'],\n        training_date=training_date,\n        ranking_output_path=prophet_rank_output_path,\n        prediction_periods=predict_periods\n    ).apply(gcp.use_gcp_secret('user-gcp-sa')).after(timeseries_prophet_predict)\n\n    val_query_op = bigquery_query_op(\n        query=val_dataset_query, \n        project_id=project_id,\n        output_gcs_path=val_dataset_path, \n        dataset_location=dataset_location, \n        job_config='').apply(gcp.use_gcp_secret('user-gcp-sa'))\n\n    val_preprocess = dsl.ContainerOp(\n        name='validation-preprocess-split',\n        image='docker.io/felipeacunago/dataset-preprocess:latest',\n        arguments=[\n            \"task.py\",\n            \"--dataset-path\", val_query_op.outputs['output_gcs_path'],\n            \"--output-path\", val_split_output_path,\n            \"--split-column\", split_column,\n            \"--ds-column\", ds_column,\n            \"--y-column\", prediction_y,\n            \"--minimum-length\", 1,\n            \"--order-ds\", order_ds,\n            \"--training-date\", evaluation_date\n            ]\n    ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n\n    val_rank = ranker_op(\n        input_path=val_split_output_path,\n        input_path_names=rank_path_names,\n        ranking_factors=ranking_factors,\n        input_dictionary=dataset_query_op.outputs['output_gcs_path'],\n        training_date=evaluation_date,\n        ranking_output_path=validation_rank_output,\n        prediction_periods=predict_periods\n    ).apply(gcp.use_gcp_secret('user-gcp-sa')).after(val_preprocess)\n\n    rank_evaluation = dsl.ContainerOp(\n        name='rank-evaluation',\n        image='docker.io/felipeacunago/ranker-eval:latest',\n        arguments=[\n            \"task.py\",\n            \"--predicted-ranking-path\", prophet_rank.outputs['ranking_output_path_file'],\n            \"--real-ranking-path\", val_rank.outputs['ranking_output_path_file'],\n            \"--eval-date\", training_date,\n            \"--maximum-distance\", evaluation_maximum_distance,\n            \"--output\", results_output,\n            ],\n        output_artifact_paths={\n            'mlpipeline-ui-metadata': '/mlpipeline-ui-metadata.json',\n        }\n    ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n\nif __name__ == '__main__':\n  import kfp.compiler as compiler\n  compiler.Compiler().compile(prophet_pipeline, __file__ + '.tar.gz')"
  },
  {
    "repo": "rakesh283343/kubeflow-sample-pipelines",
    "file_path": "core/condition/condition.py",
    "raw_url": "https://raw.githubusercontent.com/rakesh283343/kubeflow-sample-pipelines/master/core/condition/condition.py",
    "content": "#!/usr/bin/env python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp\nfrom kfp import dsl\n\n\ndef random_num_op(low, high):\n    \"\"\"Generate a random number between low and high.\"\"\"\n    return dsl.ContainerOp(\n        name='Generate random number',\n        image='python:alpine3.6',\n        command=['sh', '-c'],\n        arguments=['python -c \"import random; print(random.randint($0, $1))\" | tee $2', str(low), str(high), '/tmp/output'],\n        file_outputs={'output': '/tmp/output'}\n    )\n\n\ndef flip_coin_op():\n    \"\"\"Flip a coin and output heads or tails randomly.\"\"\"\n    return dsl.ContainerOp(\n        name='Flip coin',\n        image='python:alpine3.6',\n        command=['sh', '-c'],\n        arguments=['python -c \"import random; result = \\'heads\\' if random.randint(0,1) == 0 '\n                  'else \\'tails\\'; print(result)\" | tee /tmp/output'],\n        file_outputs={'output': '/tmp/output'}\n    )\n\n\ndef print_op(msg):\n    \"\"\"Print a message.\"\"\"\n    return dsl.ContainerOp(\n        name='Print',\n        image='alpine:3.6',\n        command=['echo', msg],\n    )\n    \n\n@dsl.pipeline(\n    name='Conditional execution pipeline',\n    description='Shows how to use dsl.Condition().'\n)\ndef flipcoin_pipeline():\n    flip = flip_coin_op()\n    with dsl.Condition(flip.output == 'heads'):\n        random_num_head = random_num_op(0, 9)\n        with dsl.Condition(random_num_head.output > 5):\n            print_op('heads and %s > 5!' % random_num_head.output)\n        with dsl.Condition(random_num_head.output <= 5):\n            print_op('heads and %s <= 5!' % random_num_head.output)\n\n    with dsl.Condition(flip.output == 'tails'):\n        random_num_tail = random_num_op(10, 19)\n        with dsl.Condition(random_num_tail.output > 15):\n            print_op('tails and %s > 15!' % random_num_tail.output)\n        with dsl.Condition(random_num_tail.output <= 15):\n            print_op('tails and %s <= 15!' % random_num_tail.output)\n\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(flipcoin_pipeline, __file__ + '.yaml')\n"
  },
  {
    "repo": "rakesh283343/kubeflow-sample-pipelines",
    "file_path": "core/execution_order/execution_order.py",
    "raw_url": "https://raw.githubusercontent.com/rakesh283343/kubeflow-sample-pipelines/master/core/execution_order/execution_order.py",
    "content": "#!/usr/bin/env python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport kfp\nfrom kfp import dsl\n\n\ndef echo1_op(text1):\n  return dsl.ContainerOp(\n      name='echo1',\n      image='library/bash:4.4.23',\n      command=['sh', '-c'],\n      arguments=['echo \"$0\"', text1])\n\n\ndef echo2_op(text2):\n  return dsl.ContainerOp(\n      name='echo2',\n      image='library/bash:4.4.23',\n      command=['sh', '-c'],\n      arguments=['echo \"$0\"', text2])\n\n\n@dsl.pipeline(\n    name='Execution order pipeline',\n    description='A pipeline to demonstrate execution order management.'\n)\ndef execution_order_pipeline(text1='message 1', text2='message 2'):\n  \"\"\"A two step pipeline with an explicitly defined execution order.\"\"\"\n  step1_task = echo1_op(text1)\n  step2_task = echo2_op(text2)\n  step2_task.after(step1_task)\n\nif __name__ == '__main__':\n  kfp.compiler.Compiler().compile(execution_order_pipeline, __file__ + '.yaml')"
  },
  {
    "repo": "rakesh283343/kubeflow-sample-pipelines",
    "file_path": "core/exit_handler/exit_handler.py",
    "raw_url": "https://raw.githubusercontent.com/rakesh283343/kubeflow-sample-pipelines/master/core/exit_handler/exit_handler.py",
    "content": "#!/usr/bin/env python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport kfp\nfrom kfp import dsl\n\n\ndef gcs_download_op(url):\n    return dsl.ContainerOp(\n        name='GCS - Download',\n        image='google/cloud-sdk:279.0.0',\n        command=['sh', '-c'],\n        arguments=['gsutil cat $0 | tee $1', url, '/tmp/results.txt'],\n        file_outputs={\n            'data': '/tmp/results.txt',\n        }\n    )\n\n\ndef echo_op(text):\n    return dsl.ContainerOp(\n        name='echo',\n        image='library/bash:4.4.23',\n        command=['sh', '-c'],\n        arguments=['echo \"$0\"', text],\n    )\n\n\n@dsl.pipeline(\n    name='Exit Handler',\n    description='Downloads a message and prints it. The exit handler will run after the pipeline finishes (successfully or not).'\n)\ndef download_and_print(url='gs://ml-pipeline/shakespeare/shakespeare1.txt'):\n    \"\"\"A sample pipeline showing exit handler.\"\"\"\n\n    exit_task = echo_op('exit!')\n\n    with dsl.ExitHandler(exit_task):\n        download_task = gcs_download_op(url)\n        echo_task = echo_op(download_task.output)\n\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(download_and_print, __file__ + '.yaml')\n"
  },
  {
    "repo": "rakesh283343/kubeflow-sample-pipelines",
    "file_path": "core/hello_world/hello_world.py",
    "raw_url": "https://raw.githubusercontent.com/rakesh283343/kubeflow-sample-pipelines/master/core/hello_world/hello_world.py",
    "content": "# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp\nfrom kfp import dsl\n\ndef echo_op():\n    return dsl.ContainerOp(\n        name='echo',\n        image='library/bash:4.4.23',\n        command=['sh', '-c'],\n        arguments=['echo \"hello world\"']\n    )\n\n@dsl.pipeline(\n    name='My first pipeline',\n    description='A hello world pipeline.'\n)\ndef hello_world_pipeline():\n    echo_task = echo_op()\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(hello_world_pipeline, __file__ + '.yaml')"
  },
  {
    "repo": "rakesh283343/kubeflow-sample-pipelines",
    "file_path": "core/imagepullsecrets/imagepullsecrets.py",
    "raw_url": "https://raw.githubusercontent.com/rakesh283343/kubeflow-sample-pipelines/master/core/imagepullsecrets/imagepullsecrets.py",
    "content": "# Copyright 2018 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Toy example demonstrating how to specify imagepullsecrets to access protected\ncontainer registry.\n\"\"\"\n\nimport kfp\nimport kfp.dsl as dsl\nfrom kubernetes import client as k8s_client\n\n\nclass GetFrequentWordOp(dsl.ContainerOp):\n  \"\"\"A get frequent word class representing a component in ML Pipelines.\n\n  The class provides a nice interface to users by hiding details such as container,\n  command, arguments.\n  \"\"\"\n  def __init__(self, name, message):\n    \"\"\"Args:\n         name: An identifier of the step which needs to be unique within a pipeline.\n         message: a dsl.PipelineParam object representing an input message.\n    \"\"\"\n    super(GetFrequentWordOp, self).__init__(\n        name=name,\n        image='python:3.5-jessie',\n        command=['sh', '-c'],\n        arguments=['python -c \"from collections import Counter; '\n                   'words = Counter(\\'%s\\'.split()); print(max(words, key=words.get))\" '\n                   '| tee /tmp/message.txt' % message],\n        file_outputs={'word': '/tmp/message.txt'})\n\n@dsl.pipeline(\n  name='Save Most Frequent',\n  description='Get Most Frequent Word and Save to GCS'\n)\ndef save_most_frequent_word(message: str):\n  \"\"\"A pipeline function describing the orchestration of the workflow.\"\"\"\n\n  counter = GetFrequentWordOp(\n          name='get-Frequent',\n          message=message)\n  # Call set_image_pull_secrets after get_pipeline_conf().\n  dsl.get_pipeline_conf()\\\n    .set_image_pull_secrets([k8s_client.V1ObjectReference(name=\"secretA\")])\n\nif __name__ == '__main__':\n  kfp.compiler.Compiler().compile(save_most_frequent_word, __file__ + '.yaml')\n"
  },
  {
    "repo": "rakesh283343/kubeflow-sample-pipelines",
    "file_path": "core/loop_output/loop_output.py",
    "raw_url": "https://raw.githubusercontent.com/rakesh283343/kubeflow-sample-pipelines/master/core/loop_output/loop_output.py",
    "content": "# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport kfp\nfrom kfp import dsl\n\n\n@dsl.pipeline(name='my-pipeline')\ndef pipeline():\n    op0 = dsl.ContainerOp(\n        name=\"my-out-cop0\",\n        image='python:alpine3.6',\n        command=[\"sh\", \"-c\"],\n        arguments=[\n            'python -c \"import json; import sys; json.dump([i for i in range(20, 31)], open(\\'/tmp/out.json\\', \\'w\\'))\"'],\n        file_outputs={'out': '/tmp/out.json'},\n    )\n\n    with dsl.ParallelFor(op0.output) as item:\n        op1 = dsl.ContainerOp(\n            name=\"my-in-cop1\",\n            image=\"library/bash:4.4.23\",\n            command=[\"sh\", \"-c\"],\n            arguments=[\"echo do output op1 item: %s\" % item],\n        )\n\n    op_out = dsl.ContainerOp(\n        name=\"my-out-cop2\",\n        image=\"library/bash:4.4.23\",\n        command=[\"sh\", \"-c\"],\n        arguments=[\"echo do output op2, outp: %s\" % op0.output],\n    )\n\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(pipeline, __file__ + '.yaml')\n"
  },
  {
    "repo": "rakesh283343/kubeflow-sample-pipelines",
    "file_path": "core/loop_parameter/loop_parameter.py",
    "raw_url": "https://raw.githubusercontent.com/rakesh283343/kubeflow-sample-pipelines/master/core/loop_parameter/loop_parameter.py",
    "content": "# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp\nfrom kfp import dsl\n\n\n@dsl.pipeline(name='my-pipeline')\ndef pipeline(loopidy_doop=[{'a': 1, 'b': 2}, {'a': 10, 'b': 20}]):\n    op0 = dsl.ContainerOp(\n        name=\"my-out-cop0\",\n        image='python:alpine3.6',\n        command=[\"sh\", \"-c\"],\n        arguments=[\n            'python -c \"import json; import sys; json.dump([i for i in range(20, 31)], open(\\'/tmp/out.json\\', \\'w\\'))\"'],\n        file_outputs={'out': '/tmp/out.json'},\n    )\n\n    with dsl.ParallelFor(loopidy_doop) as item:\n        op1 = dsl.ContainerOp(\n            name=\"my-in-cop1\",\n            image=\"library/bash:4.4.23\",\n            command=[\"sh\", \"-c\"],\n            arguments=[\"echo no output global op1, item.a: %s\" % item.a],\n        ).after(op0)\n\n    op_out = dsl.ContainerOp(\n        name=\"my-out-cop2\",\n        image=\"library/bash:4.4.23\",\n        command=[\"sh\", \"-c\"],\n        arguments=[\"echo no output global op2, outp: %s\" % op0.output],\n    )\n\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(pipeline, __file__ + '.yaml')\n"
  },
  {
    "repo": "rakesh283343/kubeflow-sample-pipelines",
    "file_path": "core/loop_static/loop_static.py",
    "raw_url": "https://raw.githubusercontent.com/rakesh283343/kubeflow-sample-pipelines/master/core/loop_static/loop_static.py",
    "content": "# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp.dsl as dsl\nimport kfp\n\n\n@dsl.pipeline(name='my-pipeline')\ndef pipeline(my_pipe_param=10):\n    loop_args = [{'A_a': 1, 'B_b': 2}, {'A_a': 10, 'B_b': 20}]\n    with dsl.ParallelFor(loop_args) as item:\n        op1 = dsl.ContainerOp(\n            name=\"my-in-coop1\",\n            image=\"library/bash:4.4.23\",\n            command=[\"sh\", \"-c\"],\n            arguments=[\"echo op1 %s %s\" % (item.A_a, my_pipe_param)],\n        )\n\n        op2 = dsl.ContainerOp(\n            name=\"my-in-coop2\",\n            image=\"library/bash:4.4.23\",\n            command=[\"sh\", \"-c\"],\n            arguments=[\"echo op2 %s\" % item.B_b],\n        )\n\n    op_out = dsl.ContainerOp(\n        name=\"my-out-cop\",\n        image=\"library/bash:4.4.23\",\n        command=[\"sh\", \"-c\"],\n        arguments=[\"echo %s\" % my_pipe_param],\n    )\n\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(pipeline, __file__ + '.yaml')\n"
  },
  {
    "repo": "rakesh283343/kubeflow-sample-pipelines",
    "file_path": "core/parallel_join/parallel_join.py",
    "raw_url": "https://raw.githubusercontent.com/rakesh283343/kubeflow-sample-pipelines/master/core/parallel_join/parallel_join.py",
    "content": "#!/usr/bin/env python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport kfp\nfrom kfp import dsl\n\ndef gcs_download_op(url):\n    return dsl.ContainerOp(\n        name='GCS - Download',\n        image='google/cloud-sdk:279.0.0',\n        command=['sh', '-c'],\n        arguments=['gsutil cat $0 | tee $1', url, '/tmp/results.txt'],\n        file_outputs={\n            'data': '/tmp/results.txt',\n        }\n    )\n\n\ndef echo2_op(text1, text2):\n    return dsl.ContainerOp(\n        name='echo',\n        image='library/bash:4.4.23',\n        command=['sh', '-c'],\n        arguments=['echo \"Text 1: $0\"; echo \"Text 2: $1\"', text1, text2]\n    )\n\n\n@dsl.pipeline(\n  name='Parallel pipeline',\n  description='Download two messages in parallel and prints the concatenated result.'\n)\ndef download_and_join(\n    url1='gs://ml-pipeline/sample-data/shakespeare/shakespeare1.txt',\n    url2='gs://ml-pipeline/sample-data/shakespeare/shakespeare2.txt'\n):\n    \"\"\"A three-step pipeline with first two running in parallel.\"\"\"\n\n    download1_task = gcs_download_op(url1)\n    download2_task = gcs_download_op(url2)\n\n    echo_task = echo2_op(download1_task.output, download2_task.output)\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(download_and_join, __file__ + '.yaml')\n"
  },
  {
    "repo": "rakesh283343/kubeflow-sample-pipelines",
    "file_path": "core/pipeline_parallelism_limits/pipeline_parallelism_limits.py",
    "raw_url": "https://raw.githubusercontent.com/rakesh283343/kubeflow-sample-pipelines/master/core/pipeline_parallelism_limits/pipeline_parallelism_limits.py",
    "content": "#!/usr/bin/env python3\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport kfp\nfrom kfp import dsl\n\n\ndef print_op(msg):\n  \"\"\"Print a message.\"\"\"\n  return dsl.ContainerOp(\n      name='Print',\n      image='alpine:3.6',\n      command=['echo', msg],\n  )\n\n\n@dsl.pipeline(\n    name='Pipeline service account',\n    description='The pipeline shows how to set the max number of parallel pods in a pipeline.'\n)\ndef pipeline_parallelism():\n  op1 = print_op('hey, what are you up to?')\n  op2 = print_op('train my model.')\n  dsl.get_pipeline_conf().set_parallelism(1)\n\nif __name__ == '__main__':\n  kfp.compiler.Compiler().compile(pipeline_parallelism, __file__ + '.yaml')\n"
  },
  {
    "repo": "rakesh283343/kubeflow-sample-pipelines",
    "file_path": "core/pipeline_transformers/pipeline_transformers.py",
    "raw_url": "https://raw.githubusercontent.com/rakesh283343/kubeflow-sample-pipelines/master/core/pipeline_transformers/pipeline_transformers.py",
    "content": "#!/usr/bin/env python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport kfp\nfrom kfp import dsl\n\ndef print_op(msg):\n  \"\"\"Print a message.\"\"\"\n  return dsl.ContainerOp(\n      name='Print',\n      image='alpine:3.6',\n      command=['echo', msg],\n  )\n\ndef add_annotation(op):\n  op.add_pod_annotation(name='hobby', value='football')\n  return op\n\n@dsl.pipeline(\n    name='Pipeline transformer',\n    description='The pipeline shows how to apply functions to all ops in the pipeline by pipeline transformers'\n)\ndef transform_pipeline():\n  op1 = print_op('hey, what are you up to?')\n  op2 = print_op('train my model.')\n  dsl.get_pipeline_conf().add_op_transformer(add_annotation)\n\nif __name__ == '__main__':\n  kfp.compiler.Compiler().compile(transform_pipeline, __file__ + '.yaml')\n"
  },
  {
    "repo": "rakesh283343/kubeflow-sample-pipelines",
    "file_path": "core/recursion/recursion.py",
    "raw_url": "https://raw.githubusercontent.com/rakesh283343/kubeflow-sample-pipelines/master/core/recursion/recursion.py",
    "content": "#!/usr/bin/env python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Notice: caching is tricky when recursion is involved. Please be careful and \n# set proper max_cache_staleness in case of infinite loop.\n\nimport kfp\nfrom kfp import dsl\n\n\ndef flip_coin_op():\n    \"\"\"Flip a coin and output heads or tails randomly.\"\"\"\n    return dsl.ContainerOp(\n        name='Flip coin',\n        image='python:alpine3.6',\n        command=['sh', '-c'],\n        arguments=['python -c \"import random; result = \\'heads\\' if random.randint(0,1) == 0 '\n                  'else \\'tails\\'; print(result)\" | tee /tmp/output'],\n        file_outputs={'output': '/tmp/output'}\n    )\n\n\ndef print_op(msg):\n    \"\"\"Print a message.\"\"\"\n    return dsl.ContainerOp(\n        name='Print',\n        image='alpine:3.6',\n        command=['echo', msg],\n    )\n\n\n# Use the dsl.graph_component to decorate pipeline functions that can be\n# recursively called.\n@dsl.graph_component\ndef flip_component(flip_result):\n    print_flip = print_op(flip_result)\n    flipA = flip_coin_op().after(print_flip)\n    # set max_cache_staleness to 0 to prevent infinite loop due to caching\n    flipA.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n    with dsl.Condition(flipA.output == 'heads'):\n        # When the flip_component is called recursively, the flipA.output\n        # from inside the graph component will be passed to the next flip_component\n        # as the input whereas the flip_result in the current graph component\n        # comes from the flipA.output in the flipcoin function.\n        flip_component(flipA.output)\n\n\n@dsl.pipeline(\n    name='Recursive loop pipeline',\n    description='Shows how to create recursive loops.'\n)\ndef flipcoin():\n    first_flip = flip_coin_op()\n    # set max_cache_staleness to 0 to prevent infinite loop due to caching\n    first_flip.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n    flip_loop = flip_component(first_flip.output)\n    # flip_loop is a graph_component with the outputs field\n    # filled with the returned dictionary.\n    print_op('cool, it is over.').after(flip_loop)\n\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(flipcoin, __file__ + '.yaml')\n"
  },
  {
    "repo": "rakesh283343/kubeflow-sample-pipelines",
    "file_path": "core/resource_ops/resource_ops.py",
    "raw_url": "https://raw.githubusercontent.com/rakesh283343/kubeflow-sample-pipelines/master/core/resource_ops/resource_ops.py",
    "content": "# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n\"\"\"\nThis example demonstrates how to use ResourceOp to specify the value of env var.\n\"\"\"\n\nimport json\nimport kfp\nimport kfp.dsl as dsl\n\n\n_CONTAINER_MANIFEST = \"\"\"\n{\n    \"apiVersion\": \"batch/v1\",\n    \"kind\": \"Job\",\n    \"metadata\": {\n        \"generateName\": \"resourceop-basic-job-\"\n    },\n    \"spec\": {\n        \"template\": {\n            \"metadata\": {\n                \"name\": \"resource-basic\"\n            },\n            \"spec\": {\n                \"containers\": [{\n                    \"name\": \"sample-container\",\n                    \"image\": \"k8s.gcr.io/busybox\",\n                    \"command\": [\"/usr/bin/env\"]\n                }],\n                \"restartPolicy\": \"Never\"\n            }\n        },\n        \"backoffLimit\": 4      \n    }\n}\n\"\"\"\n\n\n@dsl.pipeline(\n    name=\"ResourceOp Basic\",\n    description=\"A Basic Example on ResourceOp Usage.\"\n)\ndef resourceop_basic():\n\n    # Start a container. Print out env vars.\n    op = dsl.ResourceOp(\n        name='test-step',\n        k8s_resource=json.loads(_CONTAINER_MANIFEST),\n        action='create'\n    )\n\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(resourceop_basic, __file__ + '.yaml')\n"
  },
  {
    "repo": "rakesh283343/kubeflow-sample-pipelines",
    "file_path": "core/retry/retry.py",
    "raw_url": "https://raw.githubusercontent.com/rakesh283343/kubeflow-sample-pipelines/master/core/retry/retry.py",
    "content": "#!/usr/bin/env python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport kfp\nfrom kfp import dsl\n\n\ndef random_failure_op(exit_codes):\n    \"\"\"A component that fails randomly.\"\"\"\n    return dsl.ContainerOp(\n        name='random_failure',\n        image='python:alpine3.6',\n        command=['python', '-c'],\n        arguments=['import random; import sys; exit_code = int(random.choice(sys.argv[1].split(\",\"))); print(exit_code); sys.exit(exit_code)', exit_codes]\n    )\n\n\n@dsl.pipeline(\n    name='Retry random failures',\n    description='The pipeline includes two steps which fail randomly. It shows how to use ContainerOp(...).set_retry(...).'\n)\ndef retry_sample_pipeline():\n    op1 = random_failure_op('0,1,2,3').set_retry(10)\n    op2 = random_failure_op('0,1').set_retry(5)\n\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(retry_sample_pipeline, __file__ + '.yaml')\n"
  },
  {
    "repo": "rakesh283343/kubeflow-sample-pipelines",
    "file_path": "core/rnd/rnd.py",
    "raw_url": "https://raw.githubusercontent.com/rakesh283343/kubeflow-sample-pipelines/master/core/rnd/rnd.py",
    "content": "#!/usr/bin/env python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport kfp\nfrom kfp import dsl\n\n\ndef random_failure_op(exit_codes):\n    \"\"\"A component that fails randomly.\"\"\"\n    return dsl.ContainerOp(\n        name='random_failure',\n        image='python:alpine3.6',\n        command=['python', '-c'],\n        arguments=['import random; import sys; exit_code = random.choice(sys.argv[1].split(\",\")); print(exit_code); sys.exit(exit_code)', exit_codes]\n    )\n\n\n@dsl.pipeline(\n    name='Retry random failures',\n    description='The pipeline includes two steps which fail randomly. It shows how to use ContainerOp(...).set_retry(...).'\n)\ndef retry_sample_pipeline():\n    op1 = random_failure_op('0,1,2,3').set_retry(10)\n    op2 = random_failure_op('0,1').set_retry(5)\n\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(retry_sample_pipeline, __file__ + '.yaml')\n"
  },
  {
    "repo": "rakesh283343/kubeflow-sample-pipelines",
    "file_path": "core/sequential/sequential.py",
    "raw_url": "https://raw.githubusercontent.com/rakesh283343/kubeflow-sample-pipelines/master/core/sequential/sequential.py",
    "content": "#!/usr/bin/env python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport kfp\nfrom kfp import dsl\n\n\ndef gcs_download_op(url):\n    return dsl.ContainerOp(\n        name='GCS - Download',\n        image='google/cloud-sdk:279.0.0',\n        command=['sh', '-c'],\n        arguments=['gsutil cat $0 | tee $1', url, '/tmp/results.txt'],\n        file_outputs={\n            'data': '/tmp/results.txt',\n        }\n    )\n\n\ndef echo_op(text):\n    return dsl.ContainerOp(\n        name='echo',\n        image='library/bash:4.4.23',\n        command=['sh', '-c'],\n        arguments=['echo \"$0\"', text]\n    )\n\n@dsl.pipeline(\n    name='Sequential pipeline',\n    description='A pipeline with two sequential steps.'\n)\ndef sequential_pipeline(url='gs://ml-pipeline/sample-data/shakespeare/shakespeare1.txt'):\n    \"\"\"A pipeline with two sequential steps.\"\"\"\n\n    download_task = gcs_download_op(url)\n    echo_task = echo_op(download_task.output)\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(sequential_pipeline, __file__ + '.yaml')\n"
  },
  {
    "repo": "rakesh283343/kubeflow-sample-pipelines",
    "file_path": "core/sidecar/sidecar.py",
    "raw_url": "https://raw.githubusercontent.com/rakesh283343/kubeflow-sample-pipelines/master/core/sidecar/sidecar.py",
    "content": "#!/usr/bin/env python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp\nimport kfp.dsl as dsl\n\n@dsl.pipeline(\n    name=\"pipeline_with_sidecar\", \n    description=\"A pipeline that demonstrates how to add a sidecar to an operation.\"\n)\ndef pipeline_with_sidecar(sleep_sec: int = 30):\n\n    # sidecar with sevice that reply \"hello world\" to any GET request\n    echo = dsl.Sidecar(\n        name=\"echo\",\n        image=\"hashicorp/http-echo:latest\",\n        args=['-text=\"hello world\"'],\n    )\n\n    # container op with sidecar\n    op1 = dsl.ContainerOp(\n        name=\"download\",\n        image=\"busybox:latest\",\n        command=[\"sh\", \"-c\"],\n        arguments=[\n            \"sleep %s; wget localhost:5678 -O /tmp/results.txt\" % sleep_sec\n        ],  # sleep for X sec and call the sidecar and save results to output\n        sidecars=[echo],\n        file_outputs={\"downloaded\": \"/tmp/results.txt\"},\n    )\n\n    op2 = dsl.ContainerOp(\n        name=\"echo\",\n        image=\"library/bash\",\n        command=[\"sh\", \"-c\"],\n        arguments=[\"echo %s\" % op1.output],  # print out content of op1 output\n    )\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(pipeline_with_sidecar, __file__ + '.yaml')"
  },
  {
    "repo": "rakesh283343/kubeflow-sample-pipelines",
    "file_path": "core/volume_ops_readme/volume_ops.py",
    "raw_url": "https://raw.githubusercontent.com/rakesh283343/kubeflow-sample-pipelines/master/core/volume_ops_readme/volume_ops.py",
    "content": "# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp\nimport kfp.dsl as dsl\n\n\n@dsl.pipeline(\n    name=\"VolumeOp Basic\",\n    description=\"A Basic Example on VolumeOp Usage.\"\n)\ndef volumeop_basic(size):\n    vop = dsl.VolumeOp(\n        name=\"create-pvc\",\n        resource_name=\"my-pvc\",\n        modes=dsl.VOLUME_MODE_RWO,\n        size=size\n    )\n\n    cop = dsl.ContainerOp(\n        name=\"cop\",\n        image=\"library/bash:4.4.23\",\n        command=[\"sh\", \"-c\"],\n        arguments=[\"echo foo > /mnt/file1\"],\n        pvolumes={\"/mnt\": vop.volume}\n    )\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(volumeop_basic, __file__ + '.yaml')\n"
  },
  {
    "repo": "stackdemos/kubeflow-acmecr",
    "file_path": "components/golang/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/kubeflow-acmecr/master/components/golang/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef training_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('TRAINING_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=['/usr/local/bin/python', script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n\n"
  },
  {
    "repo": "stackdemos/kubeflow-acmecr",
    "file_path": "components/training/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/kubeflow-acmecr/master/components/training/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef training_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('TRAINING_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=['/usr/local/bin/python', script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n\n"
  },
  {
    "repo": "ajperry2/kubeflow_pipeline",
    "file_path": "kubeflow_pipeline/components/nuscenes/download_nuscene.py",
    "raw_url": "https://raw.githubusercontent.com/ajperry2/kubeflow_pipeline/main/kubeflow_pipeline/components/nuscenes/download_nuscene.py",
    "content": "from typing import Dict\n\nfrom kfp import dsl\nfrom kfp.dsl import Dataset\n\n\n@dsl.component(\n    base_image=\"python:3.12\", \n    packages_to_install=[\n        'requests==2.31.0', \n        'tqdm==4.65.0'])\ndef download_nuscene(\n    nuscene_email: str,\n    nuscene_password: str,\n    region:str\n) -> Dataset:\n    import requests\n    import os\n    import hashlib\n    from tqdm import tqdm\n    import tarfile\n    import gzip\n    import json \n    from pathlib import Path\n\n    download_files = {\n        \"v1.0-test_meta.tgz\":\"b0263f5c41b780a5a10ede2da99539eb\",\n        \"v1.0-test_blobs.tgz\":\"e065445b6019ecc15c70ad9d99c47b33\",\n        \"v1.0-trainval01_blobs.tgz\":\"cbf32d2ea6996fc599b32f724e7ce8f2\",\n        \"v1.0-trainval02_blobs.tgz\":\"aeecea4878ec3831d316b382bb2f72da\",\n        \"v1.0-trainval03_blobs.tgz\":\"595c29528351060f94c935e3aaf7b995\",\n        \"v1.0-trainval04_blobs.tgz\":\"b55eae9b4aa786b478858a3fc92fb72d\",\n        \"v1.0-trainval05_blobs.tgz\":\"1c815ed607a11be7446dcd4ba0e71ed0\",\n        \"v1.0-trainval06_blobs.tgz\":\"7273eeea36e712be290472859063a678\",\n        \"v1.0-trainval07_blobs.tgz\":\"46674d2b2b852b7a857d2c9a87fc755f\",\n        \"v1.0-trainval08_blobs.tgz\":\"37524bd4edee2ab99678909334313adf\",\n        \"v1.0-trainval09_blobs.tgz\":\"a7fcd6d9c0934e4052005aa0b84615c0\",\n        \"v1.0-trainval10_blobs.tgz\":\"31e795f2c13f62533c727119b822d739\",\n        \"v1.0-trainval_meta.tgz\":\"537d3954ec34e5bcb89a35d4f6fb0d4a\",\n    }\n\n    def login(username, password):\n        headers = {\n            \"Content-Type\": \"application/x-amz-json-1.1\",\n            \"X-Amz-Target\": \"AWSCognitoIdentityProviderService.InitiateAuth\",\n        }\n\n        # Use json.dumps() for correct JSON formatting\n        data = json.dumps({\n            \"AuthFlow\": \"USER_PASSWORD_AUTH\",\n            \"ClientId\": \"7fq5jvs5ffs1c50hd3toobb3b9\",\n            \"AuthParameters\": {\n                \"USERNAME\": username,\n                \"PASSWORD\": password\n            },\n            \"ClientMetadata\": {}\n        })\n\n        response = requests.post(\n            \"https://cognito-idp.us-east-1.amazonaws.com/\",\n            headers=headers,\n            data=data,\n        )\n\n        if response.status_code == 200:\n            try:\n                token = json.loads(response.content)[\"AuthenticationResult\"][\"IdToken\"]\n                return token\n            except KeyError:\n                print(\"Authentication failed. 'AuthenticationResult' not found in the response.\")\n        else:\n            print(\"Failed to login. Status code:\", response.status_code)\n\n        return None\n\n    def download_file(url, save_file,md5):\n        response = requests.get(url, stream=True)\n        if save_file.endswith(\".tgz\"):\n            content_type = response.headers.get('Content-Type', '')\n            if content_type == 'application/x-tar':\n                save_file = save_file.replace('.tgz', '.tar')\n            elif content_type != 'application/octet-stream':\n                print(\"unknow content type\",content_type)\n                return save_file\n\n        if os.path.exists(save_file):\n            print(save_file,\"has downloaded\")\n            # check md5\n            md5obj = hashlib.md5()\n            with open(save_file, 'rb') as file:\n                for chunk in file:\n                    md5obj.update(chunk)\n            hash = md5obj.hexdigest()\n            if hash != md5:\n                print(save_file,\"check md5 failed,download again\")\n            else:\n                print(save_file,\"check md5 success\")\n                return save_file\n            \n        file_size = int(response.headers.get('Content-Length', 0))\n        progress_bar = tqdm(total=file_size, unit='B', unit_scale=True, unit_divisor=1024,desc=save_file, ascii=True)\n\n\n        # save file & check md5\n        md5obj = hashlib.md5()\n        with open(save_file, 'wb') as file:\n            for chunk in response.iter_content(chunk_size=1024):\n                if chunk:\n                    md5obj.update(chunk)\n                    file.write(chunk)\n                    progress_bar.update(len(chunk))\n        progress_bar.close()\n\n        hash = md5obj.hexdigest()\n        if hash != md5:\n            print(save_file,\"check md5 failed\")\n        else:\n            print(save_file,\"check md5 success\")\n\n        return save_file\n\n\n    output_dataset = Dataset(name=\"nuscene\", uri=dsl.get_uri(), metadata={})\n\n    dataset_path = Path(output_dataset.path)\n\n    print(\"Loginging...\")\n    bearer_token = login(nuscene_email, nuscene_password)\n    # set request header\n    headers = {\n        'Authorization': f'Bearer {bearer_token}',\n        'Content-Type': 'application/json',\n    }\n\n    print(\"Getting download urls...\")\n    download_data = {}\n    for filename, md5 in download_files.items():\n        api_url = f'https://o9k5xn5546.execute-api.us-east-1.amazonaws.com/v1/archives/v1.0/{filename}?region={region}&project=nuScenes'\n\n        response = requests.get(api_url, headers=headers)\n\n        if response.status_code == 200:\n            print(filename,'request success')\n            download_url = response.json()['url']\n            download_data[filename] = [download_url,os.path.join(dataset_path,filename),md5]\n        else:\n            print(f'request failed : {response.status_code}')\n            print(response.text)\n\n    print(\"Downloading files...\")\n    for output_name,(download_url,save_file,md5) in download_data.items():\n        save_file = download_file(download_url,save_file,md5)\n        download_data[output_name] = [download_url,save_file,md5]\n\n\n\n    def extract_tgz_to_original_folder(tgz_file_path):\n        original_folder = os.path.dirname(tgz_file_path)\n        print(f\"Extracting {tgz_file_path} to {original_folder}\")\n\n        with gzip.open(tgz_file_path, 'rb') as f_in:\n            with tarfile.open(fileobj=f_in, mode='r') as tar:\n                tar.extractall(original_folder)\n\n    def extract_tar_to_original_folder(tar_file_path):\n        original_folder = os.path.dirname(tar_file_path)\n        print(f\"Extracting {tar_file_path} to {original_folder}\")\n\n        with tarfile.open(tar_file_path, 'r') as tar:\n            tar.extractall(original_folder)\n\n\n    print(\"Extracting files...\")\n    for output_name,(download_url,save_file,md5) in download_data.items():\n        if output_name.endswith(\".tgz\"):\n            extract_tgz_to_original_folder(save_file)\n        elif output_name.endswith(\".tar\"):\n            extract_tar_to_original_folder(save_file)\n        else:\n            print(\"unknow file type\",output_name)\n\n    print(\"Done!\")\n\n\n\n    return output_dataset\n"
  },
  {
    "repo": "ajperry2/kubeflow_pipeline",
    "file_path": "kubeflow_pipeline/pipelines/hello_world/hello_world.py",
    "raw_url": "https://raw.githubusercontent.com/ajperry2/kubeflow_pipeline/main/kubeflow_pipeline/pipelines/hello_world/hello_world.py",
    "content": "import os\n\nfrom kfp import dsl\n\nfrom kubeflow_pipeline import components\n\n\n@dsl.pipeline\ndef hello_world(message: str) -> str:\n    hello_task = components.hello_world.say_hello(message=message)\n    hello_task.set_display_name(\"STEP 0: Hello World\")\n    return hello_task.output\n"
  },
  {
    "repo": "ajperry2/kubeflow_pipeline",
    "file_path": "kubeflow_pipeline/pipelines/nuscenes/download_nuscene.py",
    "raw_url": "https://raw.githubusercontent.com/ajperry2/kubeflow_pipeline/main/kubeflow_pipeline/pipelines/nuscenes/download_nuscene.py",
    "content": "import os\nfrom kfp import dsl\n\nfrom kubeflow_pipeline import components\n\n\n@dsl.pipeline\ndef download_nuscene(\n    nuscene_email: str,\n    nuscene_password: str,\n    region:str\n) -> dsl.Dataset:\n    download_task = components.nuscenes.download_nuscene(\n        nuscene_email=nuscene_email,\n        nuscene_password=nuscene_password,\n        region=region,\n    )\n    download_task.set_display_name(\"STEP 0: Download Data\")\n    return download_task.output\n"
  },
  {
    "repo": "viraj-s15/KubeflowAccuPred",
    "file_path": "training_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/viraj-s15/KubeflowAccuPred/master/training_pipeline.py",
    "content": "import kfp \nfrom kfp import dsl\nimport os\nfrom components import step_data_preprocessing,step_data_splitting,step_hyperparam_optim,step_model_testing\n\n@dsl.pipeline(\n    name='Customer Frequency Training Pipeline',\n    description='A kubernetes pipeline for training a customer freq prediction model'\n)\ndef customer_freq_training_pipeline():\n    data_preprocessing_task = step_data_preprocessing()\n    data_splitting_task = step_data_splitting().after(data_preprocessing_task)\n    hyperparam_optim_task = step_hyperparam_optim().after(data_splitting_task)\n    model_testing_task = step_model_testing().after(hyperparam_optim_task)\n    \ndirectory_path = \"compiled_pipelines/\"\n\nif not os.path.exists(directory_path):\n    try:\n        os.makedirs(directory_path) \n        print(f\"Directory '{directory_path}' created successfully.\")\n    except OSError as error:\n        print(f\"Failed to create directory '{directory_path}': {error}\")\nelse:\n    print(f\"Directory '{directory_path}' already exists.\")    \n    \n\nkfp.compiler.Compiler().compile(\n    pipeline_func=customer_freq_training_pipeline,\n    package_path='./compiled_pipelines/Customer_freq_training.yaml')"
  },
  {
    "repo": "viraj-s15/KubeflowAccuPred",
    "file_path": "training_pipeline_catboost.py",
    "raw_url": "https://raw.githubusercontent.com/viraj-s15/KubeflowAccuPred/master/training_pipeline_catboost.py",
    "content": "import kfp\nfrom kfp import dsl\nimport os\nfrom components import (\n    step_model_testing_catboost,\n    step_hyperparam_optim_catboost,\n    step_data_splitting_catboost,\n    step_data_preprocessing_catboost,\n)\n\n\n@dsl.pipeline(\n    name=\"Customer Frequency Training Pipeline\",\n    description=\"A kubernetes pipeline for training a customer freq prediction model\",\n)\ndef customer_freq_training_pipeline():\n    data_preprocessing_task = step_data_preprocessing_catboost()\n    data_splitting_task = step_data_splitting_catboost().after(data_preprocessing_task)\n    hyperparam_optim_task = step_hyperparam_optim_catboost().after(data_splitting_task)\n    model_testing_task = step_model_testing_catboost().after(hyperparam_optim_task)\n\n\ndirectory_path = \"compiled_pipelines/\"\n\nif not os.path.exists(directory_path):\n    try:\n        os.makedirs(directory_path)\n        print(f\"Directory '{directory_path}' created successfully.\")\n    except OSError as error:\n        print(f\"Failed to create directory '{directory_path}': {error}\")\nelse:\n    print(f\"Directory '{directory_path}' already exists.\")\n\n\nkfp.compiler.Compiler().compile(\n    pipeline_func=customer_freq_training_pipeline,\n    package_path=\"./compiled_pipelines/Customer_freq_training_catboost.yaml\",\n)\n"
  },
  {
    "repo": "omiearicent/kubeflow_pipeline",
    "file_path": "componentymltopipeline.py",
    "raw_url": "https://raw.githubusercontent.com/omiearicent/kubeflow_pipeline/master/componentymltopipeline.py",
    "content": "#!/usr/bin/env python\n# coding: utf-8\n\n# In[3]:\n\n\nimport kfp\n# Load the component by calling load_component_from_file or load_component_from_url\n# To load the component, the pipeline author only needs to have access to the component.yaml file.\n# The Kubernetes cluster executing the pipeline needs access to the container image specified in the component.\nmodel_generation_op = kfp.components.load_component_from_file('component1.yaml') \nprediction_op = kfp.components.load_component_from_file('component2.yaml')\n# dummy_op = kfp.components.load_component_from_url('http://....../component.yaml')\n\n# dummy_op is now a \"factory function\" that accepts the arguments for the component's inputs\n# and produces a task object (e.g. ContainerOp instance).\n# Inspect the dummy_op function in Jupyter Notebook by typing \"dummy_op(\" and pressing Shift+Tab\n# You can also get help by writing help(dummy_op) or dummy_op? or dummy_op??\n# The signature of the dummy_op function corresponds to the inputs section of the component.\n# Some tweaks are performed to make the signature valid and pythonic:\n# 1) All inputs with default values will come after the inputs without default values\n# 2) The input names are converted to pythonic names (spaces and symbols replaced\n#    with underscores and letters lowercased).\n\n# Define a pipeline and create a task from a component:\n@kfp.dsl.pipeline(name='tensorflow image classification', description='demo pipeline')\ndef model_generation_new1():\n    model_generation = model_generation_op( )\n    prdediction_part = prediction_op( ).after(model_generation)\n    # To access GCS, you must configure the container to have access to a\n    # GCS secret that grants required access to the bucket.\n    # The outputs of the dummy1_task can be referenced using the\n    # dummy1_task.outputs dictionary.\n    # ! The output names are converted to lowercased dashed names.\n\n    # Pass the outputs of the dummy1_task to some other component\n \n    # To access GCS, you must configure the container to have access to a\n    # GCS secret that grants required access to the bucket.\n\n\n# In[4]:\n\n\nimport kfp\nfrom kfp import compiler\nimport kfp.compiler as compiler\nimport kfp.components as comp\nimport kfp.dsl as dsl\nfrom kfp import gcp\npipeline_func = model_generation_new1\npipeline_filename = pipeline_func.__name__ + '.pipeline.zip'\n\ncompiler.Compiler().compile(model_generation_new1, \n                            pipeline_filename)\n\n\n\n\n\n\n\n"
  },
  {
    "repo": "slv-ai/kubeflow-pipelines",
    "file_path": "kubeflow-kserve/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/slv-ai/kubeflow-pipelines/main/kubeflow-kserve/pipeline.py",
    "content": "import kfp\nfrom kfp import dsl\nfrom kfp import compiler\nfrom typing import Dict,List\nimport json\nimport os\nfrom kfp.dsl import Input,Output,Dataset,Model,component\n\n#load dataset\n@dsl.component(base_image=\"python:3.9\")\ndef load_data(output_csv: Output[Dataset]):\n    import subprocess\n    subprocess.run([\"pip\",\"install\",\"pandas\",\"scikit-learn\"],check=True)\n    from sklearn.datasets import load_iris\n    import pandas as pd \n    iris=load_iris()\n    df=pd.DataFrame(data=iris.data,columns=iris.feature_names)\n    df['target']=iris.target\n\n    df.to_csv(output_csv.path,index=False)\n\n#preprocess data\n@dsl.component(base_image=\"python:3.9\")\ndef preprocess_data(input_csv: Input[Dataset],output_train: Output[Dataset],output_test: Output[Dataset],\n                    output_ytrain: Output[Dataset],output_ytest: Output[Dataset]):\n    import subprocess\n    subprocess.run([\"pip\",\"install\",\"pandas\",\"scikit-learn\"],check=True)\n    import pandas as pd\n    from sklearn.preprocessing import StandardScalar\n    from sklearn.model_selection import train_test_split\n    df=pd.read_csv(input_csv.path)\n\n    print(\"dataset shape :\",df.shape)\n    print(\"missing values in a dataset :\"df.isnull().sum())\n    if df.isnull().values().any():\n        print(\"missing values detected\")\n        df=df.dropna()\n    features=df.drop(columns=['target'])\n    target=df['target']\n    #standardize features\n    scalar=StandardScalar()\n    scaled_features=scalar.fit_transform(features)\n\n    #train-test split\n    X_train,X_test,y_train,y_test=train_test_split(scaled_features,target,test_size=0.2,random_state=42)\n    print(\"X_train :\",X_train.shape, \"X_test :\",X_test.shape)\n    print(\"y_train :\",y_train.shape, \"y_test :\",y_test.shape)\n\n    X_train_df=pd.DataFrame(X_train,columns=features.columns)\n    X_train_df.to_csv(output_train.path,index=False)\n\n    X_test_df=pd.DataFrame(X_test,columns=features.columns)\n    X_test_df.to_csv(output_test.path,index=False)\n\n    y_train_df=pd.DataFrame(y_train)\n    y_train_df.to_csv(output_ytrain.path,index=False)\n\n    y_test_df=pd.DataFrame(y_test)\n    y_test_df.to_csv(output_ytest.path,index=False)\n\n#train the model\n@dsl.component(\n    base_image=\"python:3.9\",\n    packages_to_install=[\n        \"pandas\",\n        \"scikit-learn\",\n        \"joblib\",\n        \"boto3\",\n        \"s3fs\"\n    ] \n)\ndef train_model(\n    train_data: Input[Dataset],\n    ytrain_data: Input[Dataset],\n    model_output: Output[Model],\n    aws_access_key_id: str,\n    aws_secret_access_key: str,\n    s3_bucket: str,\n    s3_key: str\n)-> str:\n    import pandas as pd\n    import sklearn.linear_model import LogisticRegression\n    from joblib import dump\n    import boto3\n    import os\n    from datetime import datetime\n    import json\n    #load training data\n    X_train=pd.read_csv(train_data.path)\n    y_train_df=pd.read_csv(ytrain_data.path)\n    model=LogisticRegression()\n    model.fit(X_train,y_train)\n    #save model\n    #save locally\n    local_path=model_output.path\n    dump(model,local_path)\n    print(f\"model saved local {local_path}\")\n\n    try:\n        s3_client=boto3.client(\n            's3',\n            aws_access_key_id=aws_access_key_id,\n            aws_secret_access_key=aws_secret_access_key\n        )\n        #upload to s3:\n        timestamp=datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        s3_path=f\"{s3_key}/model_{timestamp}.joblib\"\n        s3_client.upload_file(\n            local_path,\n            s3_bucket,\n            s3_path\n        )\n        print(f\"model uploaded to s3://{s3_bucket}/{s3_path}\")\n\n        # Create outputs directory if it doesn't exist\n        os.makedirs('/tmp/outputs', exist_ok=True)\n\n        # Save S3 path to metadata\n        metadata_path = '/tmp/outputs/output_metadata.json'\n        model_uri = f\"s3://{s3_bucket}/{s3_path}\"\n        with open(metadata_path, 'w') as f:\n            json.dump({\n                'model_s3_path': model_uri\n            }, f)\n        print(f\"Metadata saved to: {metadata_path}\")\n\n        print(f\"Returning model URI: {model_uri}\")\n        return model_uri  # Return the S3 URI as a string\n\n    except Exception as e:\n        print(f\"Error uploading to S3: {str(e)}\")\n        raise\n\n#evaluate model\n@dsl.component(base_image=\"python:3.9\")\ndef evaluate_model(test_data: Input[Dataset],ytest_data: Input[Dataset],model: Input[Model],metrics_output: Output[Dataset]):\n    import subprocess\n    subprocess.run([\"pip\",\"install\",\"pandas\",\"scikit-learn\",\"joblib\"],check=True)\n    import pandas as pd\n    from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n    from joblib import load\n    X_test=pd.read_csv(test_data.path)\n    y_test=pd.read_csv(ytest_data.path)\n    model=load(model.path)\n    #predict\n    y_pred=model.predict(X_test)\n    report=classification_report(y_test,y_pred,output_dict=True)\n    cm=confusion_matrix(y_test,y_pred)\n    accuracy=accuracy_score(y_test,y_pred)\n    #save metrics\n    metrics_path=metrics_output.path\n    with open(metrics_path,'w')as f_in:\n        f_in.write(str(report))\n        f_in.write(f\"Accuracy: {accuracy}\")\n\n#define pipeline\n@dsl.pipeline(name=\"ml-pipeline\")\ndef ml_pipeline(\n    aws_access_key_id: str = os.getenv('AWS_ACCESS_KEY_ID'),\n    aws_secret_access_key: str = os.getenv('AWS_SECRET_ACCESS_KEY'),\n    s3_bucket: str = \"kubeflow-projects\",\n    s3_key: str = \"models/iris\"\n):\n    load_op=load_data()\n    preprocess_op=preprocess_data(input_csv=load_op.outputs[\"output_csv\"])\n    train_op=train_model(\n        train_data=preprocess_op.outputs[\"output_train\"],\n        ytrain_data=preprocess_op.outputs[\"output_ytrain\"],\n        aws_access_key_id=aws_access_key_id,\n        aws_secret_access_key=aws_secret_access_key,\n        s3_bucket=s3_bucket,\n        s3_key=s3_key\n    )\n    evaluate_op=evaluate_model(\n        test_data=preprocess_op.outputs[\"output_test\"],\n        ytest_data=preprocess_op.outputs[\"output_ytest\"],\n        model=train_op.outputs[\"model_output\"]\n    )\n\n\nif __name__ == \"main\":\n    compiler.Compiler().compile(pipeline_func=ml_pipeline,package_path=\"kubeflow_pipeline.yaml\")\n\n\n\n\n\n\n\n\n"
  },
  {
    "repo": "slv-ai/kubeflow-pipelines",
    "file_path": "kubeflow-pipeline/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/slv-ai/kubeflow-pipelines/main/kubeflow-pipeline/pipeline.py",
    "content": "import kfp\nfrom kfp import dsl\nfrom kfp import compiler\nfrom typing import Dict,List\nfrom kfp.dsl import Input,Output,Dataset,Model,component\n\n#load dataset\n@dsl.component(base_image=\"python:3.9\")\ndef load_data(output_csv: Output[Dataset]):\n    import subprocess\n    subprocess.run([\"pip\",\"install\",\"pandas\",\"scikit-learn\"],check=True)\n    from sklearn.datasets import load_iris\n    import pandas as pd \n    iris=load_iris()\n    df=pd.DataFrame(data=iris.data,columns=iris.feature_names)\n    df['target']=iris.target\n\n    df.to_csv(output_csv.path,index=False)\n\n#preprocess data\n@dsl.component(base_image=\"python:3.9\")\ndef preprocess_data(input_csv: Input[Dataset],output_train: Output[Dataset],output_test: Output[Dataset],\n                    output_ytrain: Output[Dataset],output_ytest: Output[Dataset]):\n    import subprocess\n    subprocess.run([\"pip\",\"install\",\"pandas\",\"scikit-learn\"],check=True)\n    import pandas as pd\n    from sklearn.preprocessing import StandardScalar\n    from sklearn.model_selection import train_test_split\n    df=pd.read_csv(input_csv.path)\n\n    print(\"dataset shape :\",df.shape)\n    print(\"missing values in a dataset :\"df.isnull().sum())\n    if df.isnull().values().any():\n        print(\"missing values detected\")\n        df=df.dropna()\n    features=df.drop(columns=['target'])\n    target=df['target']\n    #standardize features\n    scalar=StandardScalar()\n    scaled_features=scalar.fit_transform(features)\n\n    #train-test split\n    X_train,X_test,y_train,y_test=train_test_split(scaled_features,target,test_size=0.2,random_state=42)\n    print(\"X_train :\",X_train.shape, \"X_test :\",X_test.shape)\n    print(\"y_train :\",y_train.shape, \"y_test :\",y_test.shape)\n\n    X_train_df=pd.DataFrame(X_train,columns=features.columns)\n    X_train_df.to_csv(output_train.path,index=False)\n\n    X_test_df=pd.DataFrame(X_test,columns=features.columns)\n    X_test_df.to_csv(output_test.path,index=False)\n\n    y_train_df=pd.DataFrame(y_train)\n    y_train_df.to_csv(output_ytrain.path,index=False)\n\n    y_test_df=pd.DataFrame(y_test)\n    y_test_df.to_csv(output_ytest.path,index=False)\n\n#train the model\n@dsl.component(base_image=\"python:3.9\")\ndef train_model(train_data: Input[Dataset],ytrain_data: Input[Dataset],model_output: Output[Model]):\n    import subprocess\n    subprocess.run([\"pip\",\"install\",\"pandas\",\"scikit-learn\",\"joblib\"],check=True)\n    import pandas as pd\n    import sklearn.linear_model import LogisticRegression\n    from joblib import dump\n    #load training data\n    X_train=pd.read_csv(train_data.path)\n    y_train_df=pd.read_csv(ytrain_data.path)\n    model=LogisticRegression()\n    model.fit(X_train,y_train)\n    #save model\n    dump(model,model_output.path)\n\n#evaluate model\n@dsl.component(base_image=\"python:3.9\")\ndef evaluate_model(test_data: Input[Dataset],ytest_data: Input[Dataset],model: Input[Model],metrics_output: Output[Dataset]):\n    import subprocess\n    subprocess.run([\"pip\",\"install\",\"pandas\",\"scikit-learn\",\"joblib\"],check=True)\n    import pandas as pd\n    from sklearn.metrics import classification_report,confusion_matrix\n    from joblib import load\n    X_test=pd.read_csv(test_data.path)\n    y_test=pd.read_csv(ytest_data.path)\n    model=load(model.path)\n    #predict\n    y_pred=model.predict(X_test)\n    report=classification_report(y_test,y_pred,output_dict=True)\n    cm=confusion_matrix(y_test,y_pred)\n    #save metrics\n    metrics_path=metrics_output.path\n    with open(metrics_path,'w')as f_in:\n        f_in.write(str(report))\n\n#define pipeline\n@dsl.pipeline(name=\"ml_pipeline\")\ndef ml_pipeline:\n    load_op=load_data()\n    preprocess_op=preprocess_data(input_csv=load_op.outputs[\"output_csv\"])\n    train_op=train_model(train_data=preprocess_op.outputs[\"output_train\"],ytrain_data=preprocess_op.outputs[\"output_ytrain\"])\n    evaluate_op=evaluate_model(test_data=preprocess_op.outputs[\"output_test\"],ytest_data=preprocess_op.outputs[\"output_ytest\"],model=train_op.outputs[\"model_output\"])\n\n\nif __name__ == \"main\":\n    compiler.Compiler().compile(pipeline_func=ml_pipeline,package_path=\"kubeflow_pipeline.yaml\")\n\n\n\n\n\n\n\n\n"
  },
  {
    "repo": "stackdemos/snehal-kubeflow-ml",
    "file_path": "components/golang/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/snehal-kubeflow-ml/master/components/golang/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef processing_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('GOLANG_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=[script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n"
  },
  {
    "repo": "stackdemos/snehal-kubeflow-ml",
    "file_path": "components/training/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/snehal-kubeflow-ml/master/components/training/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef training_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('TRAINING_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=['/usr/local/bin/python', script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n\n"
  },
  {
    "repo": "tam0201/kubeflow-pipeline",
    "file_path": "pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/tam0201/kubeflow-pipeline/main/pipeline.py",
    "content": "import kfp.dsl\nfrom kfp.components import ComponentStore\n\nfrom components.definitions import *\n\nstore = ComponentStore.default_store\npandas_transform_csv_op = store.load_component('pandas/Transform_DataFrame/in_CSV_format')\ndrop_header_op = store.load_component('tables/Remove_header')\n\n@ksp.dsl.pipeline(\n    name = 'Table Classification: $NAME',\n    description = '$DESCRIPTION'\n)\ndef pipeline(\n    s3_input_csv:str,\n    num_boost_found: int,\n    target: str,\n):\n    #load data from S3\n    input_csv=download_file_from_s3(s3_input_csv).output\n\n    #Use KFP UI for simple EDA\n    set_sweetviz(input_csv)\n\n    #Split test data\n    split_train_test=split_train_test_step(input_csv, test_size='your input here')\n    train_csv = split_train_test.output['output_csv_train']\n    test_csv = split_train_test.output['output_csv_test']\n\n    #Split val data\n    split_train_val=split_train_val_step(train_csv, val_size='your input here')\n    train_csv = split_train_val.output['output_csv_train']\n    val_csv = split_train_val.output['output_csv_test']\n\n    #preprocess all datasets\n    process_train - preprocessing_step(train_csv, target = target)\n    process_val - preprocessing_step(val_csv, target = target)\n    process_test - preprocessing_step(test_csv, target = target)\n\n    train_csv_processed = process_train.output\n    val_csv_processed = process_val.output\n    test_csv_processed = process_test.output\n\n    clf_path = train_classifier_step(train_csv_processed, val_csv_processed, num_boost_found, target_name = '').output\n\n    #Remove target from test data\n    x_test = pandas_transform_csv_op(table = test_csv_processed, transform_code = f'df = df.drop(\"{target}\", axis=1)').output\n    ).outputs\n\n    #Extract target from test data\n    y_test = pandas_transform_csv_op(table = test_csv_processed, transform_code = f'df[\"{target}\"] = df[\"{target}\"].astype(\"category\").cat.codes\\n'\n                                                                                    f'df = df[[\"target\"]]'\n    ).outputs\n    y_true_test = drop_header_op(y_test).output\n    y_pred_test = infer_classifier_step(model_path = clf_path, dataset_path = x_test).output\n\n    #Compute classification metrics\n    metrics = compute_classification_metrics(y_true_path = y_true_test, y_pred_path = y_pred_test, label_name = target).output\n\n    set_metrics(metrics)\n\n    #additional bells and whistles\n    process_train.set_display_name('Process: Train')\n    process_val.set_display_name('Process: Val')\n    process_test.set_display_name('Process: Test')\n\n    split_train_test.set_display_name('Split: Train/Test')\n    split_train_val.set_display_name('Split: Train/Val')\n\n"
  },
  {
    "repo": "bartosz-bok/kubeflow-pipelines",
    "file_path": "distributed-pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/bartosz-bok/kubeflow-pipelines/main/distributed-pipeline.py",
    "content": "import kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import component\nfrom kfp.v2.dsl import (\n    Input,\n    Output,\n    Artifact,\n    Dataset,\n    Metrics\n)\n\n@component(\n    packages_to_install=['pandas']\n)\ndef download_dataframe(url: str, output_csv: Output[Dataset]):\n  import pandas as pd\n\n  df_data = pd.read_csv(url)\n\n  df_data.to_csv(output_csv.path, index=False)\n\n@component(\n    packages_to_install=['pandas',\n                         'scikit-learn']\n)\ndef preprocessing(data_path: Input[Dataset], output_preprocessed_data_x_train: Output[Dataset],\n                                             output_preprocessed_data_x_val: Output[Dataset],\n                                             output_preprocessed_data_y_train: Output[Dataset],\n                                             output_preprocessed_data_y_val: Output[Dataset]):\n  import pandas as pd\n  from sklearn.model_selection import train_test_split\n\n  df_data = pd.read_csv(data_path.path)\n\n  df_data.drop(['Race'], axis=1, inplace=True)\n  df_data.replace(to_replace=['No','No, borderline diabetes', 'Yes', 'Yes (during pregnancy)'], value=[0, 0, 1, 1], inplace=True)\n  df_data['Sex'] = df_data['Sex'].map({'Female': 1, 'Male': 0})\n  df_data['AgeCategory'] = df_data['AgeCategory'].map({'18-24': 1,'25-29': 2,'30-34': 3, '35-39': 4,'40-44': 5,'45-49': 6,'50-54': 7,  '55-59': 8, '60-64': 9, '65-69': 10, '70-74': 11, '75-79': 12, '80 or older': 13})\n  df_data['GenHealth'] = df_data['GenHealth'].map({'Poor': 1, 'Fair': 2, 'Good': 3,'Very good': 4,'Excellent': 5})\n\n  X = df_data.drop(['HeartDisease'], axis=1)\n  y = df_data['HeartDisease']\n\n  X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n  \n  X_train.to_csv(output_preprocessed_data_x_train.path, index=False)\n  X_val.to_csv(output_preprocessed_data_x_val.path, index=False)\n  y_train.to_csv(output_preprocessed_data_y_train.path, index=False)\n  y_val.to_csv(output_preprocessed_data_y_val.path, index=False)\n\n@component(\n    packages_to_install=['pandas',\n                         'torch',\n                         'scikit-learn']\n)\ndef training_1st(num_epochs: int, preprocessed_data_path_x_train: Input[Dataset],\n                                  preprocessed_data_path_x_val: Input[Dataset],\n                                  preprocessed_data_path_y_train: Input[Dataset],\n                                  preprocessed_data_path_y_val: Input[Dataset],\n                                  output_accuracy: Output[Metrics]):\n  import torch\n  import torch.nn as nn\n  import torch.optim as optim\n  import pandas as pd\n  from sklearn.preprocessing import StandardScaler\n  from sklearn.metrics import accuracy_score\n\n  X_train = pd.read_csv(preprocessed_data_path_x_train.path)\n  X_val = pd.read_csv(preprocessed_data_path_x_val.path)\n  y_train = pd.read_csv(preprocessed_data_path_y_train.path)\n  y_val = pd.read_csv(preprocessed_data_path_y_val.path)\n\n  scaler = StandardScaler()\n  X_train = scaler.fit_transform(X_train)\n  X_val = scaler.transform(X_val)\n\n  X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n  y_train_tensor = torch.tensor(y_train.values.reshape(-1, 1), dtype=torch.float32)\n  X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n  y_val_tensor = torch.tensor(y_val.values.reshape(-1, 1), dtype=torch.float32)\n\n  # Definicja modelu\n  class RegressionModel(nn.Module):\n      def __init__(self):\n          super(RegressionModel, self).__init__()\n          self.fc1 = nn.Linear(16, 8)\n          self.fc2 = nn.Linear(8, 4)\n          self.fc3 = nn.Linear(4, 1)\n\n      def forward(self, x):\n          x = torch.relu(self.fc1(x))\n          x = torch.relu(self.fc2(x))\n          x = self.fc3(x)\n          return x\n\n  model = RegressionModel()\n\n  criterion = nn.MSELoss()\n  optimizer = optim.Adam(model.parameters(), lr=0.01)\n\n  # Trenowanie modelu\n  for epoch in range(num_epochs):\n      # Forward pass\n      outputs = model(X_train_tensor)\n      loss = criterion(outputs, y_train_tensor)\n\n      optimizer.zero_grad()\n      loss.backward()\n      optimizer.step()\n\n      with torch.no_grad():\n          val_outputs = model(X_val_tensor)\n          val_loss = criterion(val_outputs, y_val_tensor)\n          print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Validation Loss: {val_loss.item():.4f}')\n\n  with torch.no_grad():\n      val_predictions = model(X_val_tensor)\n      val_predictions = torch.round(val_predictions)\n      val_accuracy = accuracy_score(y_val_tensor.numpy(), val_predictions.numpy())\n      print(f'Validation Accuracy: {val_accuracy:.4f}')\n\n  output_accuracy.log_metric('accuracy', val_accuracy) \n\n@component(\n    packages_to_install=['pandas',\n                         'torch',\n                         'scikit-learn']\n)\ndef training_2nd(num_epochs: int, preprocessed_data_path_x_train: Input[Dataset],\n                                  preprocessed_data_path_x_val: Input[Dataset],\n                                  preprocessed_data_path_y_train: Input[Dataset],\n                                  preprocessed_data_path_y_val: Input[Dataset],\n                                  output_accuracy: Output[Metrics]):\n  import torch\n  import torch.nn as nn\n  import torch.optim as optim\n  import pandas as pd\n  from sklearn.preprocessing import StandardScaler\n  from sklearn.metrics import accuracy_score\n\n  X_train = pd.read_csv(preprocessed_data_path_x_train.path)\n  X_val = pd.read_csv(preprocessed_data_path_x_val.path)\n  y_train = pd.read_csv(preprocessed_data_path_y_train.path)\n  y_val = pd.read_csv(preprocessed_data_path_y_val.path)\n\n  scaler = StandardScaler()\n  X_train = scaler.fit_transform(X_train)\n  X_val = scaler.transform(X_val)\n\n  X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n  y_train_tensor = torch.tensor(y_train.values.reshape(-1, 1), dtype=torch.float32)\n  X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n  y_val_tensor = torch.tensor(y_val.values.reshape(-1, 1), dtype=torch.float32)\n\n  # Definicja modelu\n  class RegressionModel(nn.Module):\n      def __init__(self):\n          super(RegressionModel, self).__init__()\n          self.fc1 = nn.Linear(16, 16)\n          self.fc2 = nn.Linear(16, 8)\n          self.fc3 = nn.Linear(8, 8)\n          self.fc4 = nn.Linear(8, 1)\n\n      def forward(self, x):\n          x = torch.relu(self.fc1(x))\n          x = torch.relu(self.fc2(x))\n          x = torch.relu(self.fc3(x))\n          x = self.fc4(x)\n          return x\n\n  model = RegressionModel()\n\n  criterion = nn.MSELoss()\n  optimizer = optim.Adam(model.parameters(), lr=0.01)\n\n  # Trenowanie modelu\n  for epoch in range(num_epochs):\n      # Forward pass\n      outputs = model(X_train_tensor)\n      loss = criterion(outputs, y_train_tensor)\n\n      optimizer.zero_grad()\n      loss.backward()\n      optimizer.step()\n\n      with torch.no_grad():\n          val_outputs = model(X_val_tensor)\n          val_loss = criterion(val_outputs, y_val_tensor)\n          print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Validation Loss: {val_loss.item():.4f}')\n\n  with torch.no_grad():\n      val_predictions = model(X_val_tensor)\n      val_predictions = torch.round(val_predictions)\n      val_accuracy = accuracy_score(y_val_tensor.numpy(), val_predictions.numpy())\n      print(f'Validation Accuracy: {val_accuracy:.4f}')\n\n  output_accuracy.log_metric('accuracy', val_accuracy) \n\n@component()\ndef compare_accuracy(accuracy_1st: Input[Metrics],accuracy_2nd: Input[Metrics], better_model: Output[Metrics]):\n\n  accuracy_1st_result = accuracy_1st.metadata['accuracy']\n  accuracy_2nd_result = accuracy_2nd.metadata['accuracy']\n\n  print(f'Pierwszy model osiagnal wartosc {accuracy_1st_result}, a drugi {accuracy_2nd_result}.')\n\n  which_better = 0\n\n  if accuracy_1st_result > accuracy_2nd_result:\n    print('Pierwszy model osiagnal wieksza dokladnosc')\n    which_better = 1\n  elif accuracy_1st_result < accuracy_2nd_result:\n    print('Drugi model osiagnal wieksza dokladnosc')\n    which_better = 2\n  elif accuracy_1st_result == accuracy_2nd_result:\n    print('Obydwa modele osiagnely taka sama dokladnosc')\n  else:\n    print('Blad')\n\n  better_model.log_metric('better_model', which_better)\n\n# Define a pipeline and create a task from a component:\n@dsl.pipeline(\n    name='my-pipeline',\n    # You can optionally specify your own pipeline_root\n    # pipeline_root='gs://my-pipeline-root/example-pipeline',\n)\ndef my_pipeline(url: str, num_epochs_1st: int, num_epochs_2nd: int):\n  web_downloader_task = download_dataframe(url=url)\n  preprocessing_task = preprocessing(data_path=web_downloader_task.outputs['output_csv'])\n  training_task_1st = training_1st(num_epochs=num_epochs_1st, preprocessed_data_path_x_train=preprocessing_task.outputs['output_preprocessed_data_x_train'],\n                                                              preprocessed_data_path_x_val=preprocessing_task.outputs['output_preprocessed_data_x_val'],\n                                                              preprocessed_data_path_y_train=preprocessing_task.outputs['output_preprocessed_data_y_train'],\n                                                              preprocessed_data_path_y_val=preprocessing_task.outputs['output_preprocessed_data_y_val'])\n  training_task_2nd = training_2nd(num_epochs=num_epochs_2nd, preprocessed_data_path_x_train=preprocessing_task.outputs['output_preprocessed_data_x_train'],\n                                                              preprocessed_data_path_x_val=preprocessing_task.outputs['output_preprocessed_data_x_val'],\n                                                              preprocessed_data_path_y_train=preprocessing_task.outputs['output_preprocessed_data_y_train'],\n                                                              preprocessed_data_path_y_val=preprocessing_task.outputs['output_preprocessed_data_y_val'])\n  compare_task = compare_accuracy(accuracy_1st=training_task_1st.outputs['output_accuracy'], accuracy_2nd=training_task_2nd.outputs['output_accuracy'])\n\nkfp.compiler.Compiler(mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE).compile(\n    pipeline_func=my_pipeline,\n    package_path='pipeline_distributed_v1.yaml')"
  },
  {
    "repo": "bartosz-bok/kubeflow-pipelines",
    "file_path": "sequenced-pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/bartosz-bok/kubeflow-pipelines/main/sequenced-pipeline.py",
    "content": "import kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import component\nfrom kfp.v2.dsl import (\n    Input,\n    Output,\n    Artifact,\n    Dataset,\n    Metrics\n)\n\n@component(\n    packages_to_install=['pandas']\n)\ndef download_dataframe(url: str, output_csv: Output[Dataset]):\n  import pandas as pd\n\n  df_data = pd.read_csv(url)\n\n  df_data.to_csv(output_csv.path, index=False)\n\n@component(\n    packages_to_install=['pandas',\n                         'scikit-learn']\n)\ndef preprocessing(data_path: Input[Dataset], output_preprocessed_data_x_train: Output[Dataset],\n                                             output_preprocessed_data_x_val: Output[Dataset],\n                                             output_preprocessed_data_y_train: Output[Dataset],\n                                             output_preprocessed_data_y_val: Output[Dataset]):\n  import pandas as pd\n  from sklearn.model_selection import train_test_split\n\n  df_data = pd.read_csv(data_path.path)\n\n  df_data.drop(['Race'], axis=1, inplace=True)\n  df_data.replace(to_replace=['No','No, borderline diabetes', 'Yes', 'Yes (during pregnancy)'], value=[0, 0, 1, 1], inplace=True)\n  df_data['Sex'] = df_data['Sex'].map({'Female': 1, 'Male': 0})\n  df_data['AgeCategory'] = df_data['AgeCategory'].map({'18-24': 1,'25-29': 2,'30-34': 3, '35-39': 4,'40-44': 5,'45-49': 6,'50-54': 7,  '55-59': 8, '60-64': 9, '65-69': 10, '70-74': 11, '75-79': 12, '80 or older': 13})\n  df_data['GenHealth'] = df_data['GenHealth'].map({'Poor': 1, 'Fair': 2, 'Good': 3,'Very good': 4,'Excellent': 5})\n\n  X = df_data.drop(['HeartDisease'], axis=1)\n  y = df_data['HeartDisease']\n\n  X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n  \n  X_train.to_csv(output_preprocessed_data_x_train.path, index=False)\n  X_val.to_csv(output_preprocessed_data_x_val.path, index=False)\n  y_train.to_csv(output_preprocessed_data_y_train.path, index=False)\n  y_val.to_csv(output_preprocessed_data_y_val.path, index=False)\n\n@component(\n    packages_to_install=['pandas',\n                         'torch',\n                         'scikit-learn']\n)\ndef training_1st(num_epochs: int, preprocessed_data_path_x_train: Input[Dataset],\n                                  preprocessed_data_path_x_val: Input[Dataset],\n                                  preprocessed_data_path_y_train: Input[Dataset],\n                                  preprocessed_data_path_y_val: Input[Dataset],\n                                  output_preprocessed_data_x_train: Output[Dataset],\n                                  output_preprocessed_data_x_val: Output[Dataset],\n                                  output_preprocessed_data_y_train: Output[Dataset],\n                                  output_preprocessed_data_y_val: Output[Dataset],\n                                  output_accuracy: Output[Metrics]):\n  import torch\n  import torch.nn as nn\n  import torch.optim as optim\n  import pandas as pd\n  from sklearn.preprocessing import StandardScaler\n  from sklearn.metrics import accuracy_score\n\n  X_train = pd.read_csv(preprocessed_data_path_x_train.path)\n  X_val = pd.read_csv(preprocessed_data_path_x_val.path)\n  y_train = pd.read_csv(preprocessed_data_path_y_train.path)\n  y_val = pd.read_csv(preprocessed_data_path_y_val.path)\n\n  X_train.to_csv(output_preprocessed_data_x_train.path, index=False)\n  X_val.to_csv(output_preprocessed_data_x_val.path, index=False)\n  y_train.to_csv(output_preprocessed_data_y_train.path, index=False)\n  y_val.to_csv(output_preprocessed_data_y_val.path, index=False)\n\n  scaler = StandardScaler()\n  X_train = scaler.fit_transform(X_train)\n  X_val = scaler.transform(X_val)\n\n  X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n  y_train_tensor = torch.tensor(y_train.values.reshape(-1, 1), dtype=torch.float32)\n  X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n  y_val_tensor = torch.tensor(y_val.values.reshape(-1, 1), dtype=torch.float32)\n\n  # Definicja modelu\n  class RegressionModel(nn.Module):\n      def __init__(self):\n          super(RegressionModel, self).__init__()\n          self.fc1 = nn.Linear(16, 8)\n          self.fc2 = nn.Linear(8, 4)\n          self.fc3 = nn.Linear(4, 1)\n\n      def forward(self, x):\n          x = torch.relu(self.fc1(x))\n          x = torch.relu(self.fc2(x))\n          x = self.fc3(x)\n          return x\n\n  model = RegressionModel()\n\n  criterion = nn.MSELoss()\n  optimizer = optim.Adam(model.parameters(), lr=0.01)\n\n  # Trenowanie modelu\n  for epoch in range(num_epochs):\n      # Forward pass\n      outputs = model(X_train_tensor)\n      loss = criterion(outputs, y_train_tensor)\n\n      optimizer.zero_grad()\n      loss.backward()\n      optimizer.step()\n\n      with torch.no_grad():\n          val_outputs = model(X_val_tensor)\n          val_loss = criterion(val_outputs, y_val_tensor)\n          print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Validation Loss: {val_loss.item():.4f}')\n\n  with torch.no_grad():\n      val_predictions = model(X_val_tensor)\n      val_predictions = torch.round(val_predictions)\n      val_accuracy = accuracy_score(y_val_tensor.numpy(), val_predictions.numpy())\n      print(f'Validation Accuracy: {val_accuracy:.4f}')\n\n  output_accuracy.log_metric('accuracy', val_accuracy) \n\n@component(\n    packages_to_install=['pandas',\n                         'torch',\n                         'scikit-learn']\n)\ndef training_2nd(num_epochs: int, preprocessed_data_path_x_train: Input[Dataset],\n                                  preprocessed_data_path_x_val: Input[Dataset],\n                                  preprocessed_data_path_y_train: Input[Dataset],\n                                  preprocessed_data_path_y_val: Input[Dataset],\n                                  input_accuracy_1st: Input[Metrics],\n                                  output_accuracy_1st: Output[Metrics],\n                                  output_accuracy: Output[Metrics]):\n  import torch\n  import torch.nn as nn\n  import torch.optim as optim\n  import pandas as pd\n  from sklearn.preprocessing import StandardScaler\n  from sklearn.metrics import accuracy_score\n\n  X_train = pd.read_csv(preprocessed_data_path_x_train.path)\n  X_val = pd.read_csv(preprocessed_data_path_x_val.path)\n  y_train = pd.read_csv(preprocessed_data_path_y_train.path)\n  y_val = pd.read_csv(preprocessed_data_path_y_val.path)\n\n  scaler = StandardScaler()\n  X_train = scaler.fit_transform(X_train)\n  X_val = scaler.transform(X_val)\n\n  X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n  y_train_tensor = torch.tensor(y_train.values.reshape(-1, 1), dtype=torch.float32)\n  X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n  y_val_tensor = torch.tensor(y_val.values.reshape(-1, 1), dtype=torch.float32)\n\n  # Definicja modelu\n  class RegressionModel(nn.Module):\n      def __init__(self):\n          super(RegressionModel, self).__init__()\n          self.fc1 = nn.Linear(16, 16)\n          self.fc2 = nn.Linear(16, 8)\n          self.fc3 = nn.Linear(8, 8)\n          self.fc4 = nn.Linear(8, 1)\n\n      def forward(self, x):\n          x = torch.relu(self.fc1(x))\n          x = torch.relu(self.fc2(x))\n          x = torch.relu(self.fc3(x))\n          x = self.fc4(x)\n          return x\n\n  model = RegressionModel()\n\n  criterion = nn.MSELoss()\n  optimizer = optim.Adam(model.parameters(), lr=0.01)\n\n  # Trenowanie modelu\n  for epoch in range(num_epochs):\n      # Forward pass\n      outputs = model(X_train_tensor)\n      loss = criterion(outputs, y_train_tensor)\n\n      optimizer.zero_grad()\n      loss.backward()\n      optimizer.step()\n\n      with torch.no_grad():\n          val_outputs = model(X_val_tensor)\n          val_loss = criterion(val_outputs, y_val_tensor)\n          print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Validation Loss: {val_loss.item():.4f}')\n\n  with torch.no_grad():\n      val_predictions = model(X_val_tensor)\n      val_predictions = torch.round(val_predictions)\n      val_accuracy = accuracy_score(y_val_tensor.numpy(), val_predictions.numpy())\n      print(f'Validation Accuracy: {val_accuracy:.4f}')\n\n  output_accuracy.log_metric('accuracy', val_accuracy)\n\n  output_accuracy_1st.log_metric('accuracy', input_accuracy_1st.metadata['accuracy']) \n\n@component()\ndef compare_accuracy(accuracy_1st: Input[Metrics],accuracy_2nd: Input[Metrics], better_model: Output[Metrics]):\n\n  accuracy_1st_result = accuracy_1st.metadata['accuracy']\n  accuracy_2nd_result = accuracy_2nd.metadata['accuracy']\n\n  print(f'Pierwszy model osiagnal wartosc {accuracy_1st_result}, a drugi {accuracy_2nd_result}.')\n\n  which_better = 0\n\n  if accuracy_1st_result > accuracy_2nd_result:\n    print('Pierwszy model osiagnal wieksza dokladnosc')\n    which_better = 1\n  elif accuracy_1st_result < accuracy_2nd_result:\n    print('Drugi model osiagnal wieksza dokladnosc')\n    which_better = 2\n  elif accuracy_1st_result == accuracy_2nd_result:\n    print('Obydwa modele osiagnely taka sama dokladnosc')\n  else:\n    print('Blad')\n\n  better_model.log_metric('better_model', which_better)\n\n# Define a pipeline and create a task from a component:\n@dsl.pipeline(\n    name='my-pipeline',\n    # You can optionally specify your own pipeline_root\n    # pipeline_root='gs://my-pipeline-root/example-pipeline',\n)\ndef my_pipeline(url: str, num_epochs_1st: int, num_epochs_2nd: int):\n  web_downloader_task = download_dataframe(url=url)\n  preprocessing_task = preprocessing(data_path=web_downloader_task.outputs['output_csv'])\n  training_task_1st = training_1st(num_epochs=num_epochs_1st, preprocessed_data_path_x_train=preprocessing_task.outputs['output_preprocessed_data_x_train'],\n                                                              preprocessed_data_path_x_val=preprocessing_task.outputs['output_preprocessed_data_x_val'],\n                                                              preprocessed_data_path_y_train=preprocessing_task.outputs['output_preprocessed_data_y_train'],\n                                                              preprocessed_data_path_y_val=preprocessing_task.outputs['output_preprocessed_data_y_val'])\n  training_task_2nd = training_2nd(num_epochs=num_epochs_2nd, preprocessed_data_path_x_train=training_task_1st.outputs['output_preprocessed_data_x_train'],\n                                                              preprocessed_data_path_x_val=training_task_1st.outputs['output_preprocessed_data_x_val'],\n                                                              preprocessed_data_path_y_train=training_task_1st.outputs['output_preprocessed_data_y_train'],\n                                                              preprocessed_data_path_y_val=training_task_1st.outputs['output_preprocessed_data_y_val'],\n                                                              input_accuracy_1st = training_task_1st.outputs['output_accuracy'])\n  compare_task = compare_accuracy(accuracy_1st=training_task_2nd.outputs['output_accuracy_1st'], accuracy_2nd=training_task_2nd.outputs['output_accuracy'])\n\nkfp.compiler.Compiler(mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE).compile(\n    pipeline_func=my_pipeline,\n    package_path='pipeline_sequential_v1.yaml')"
  },
  {
    "repo": "akontaxakis/kubeflow_pipelines",
    "file_path": "samples/Full_experiment.py",
    "raw_url": "https://raw.githubusercontent.com/akontaxakis/kubeflow_pipelines/master/samples/Full_experiment.py",
    "content": "\nimport kfp\nfrom kfp import dsl\n\n\ndef A(TRAIN_SET_COUNT: int, TRAIN_SET_LIMIT: int,  TRAIN_INPUT_JSON: kfp.components.OutputPath(str)):\n    from random import randint\n    import json\n    TRAIN_INPUT = list()\n    for i in range(TRAIN_SET_COUNT):\n        a = randint(0, TRAIN_SET_LIMIT)\n        b = randint(0, TRAIN_SET_LIMIT)\n        c = randint(0, TRAIN_SET_LIMIT)\n        TRAIN_INPUT.append([a, b, c])\n    jsonString = json.dumps(TRAIN_INPUT)\n    with open(TRAIN_INPUT_JSON, 'w') as odd_writer:\n        odd_writer.write(jsonString)\n\n\nA_op = kfp.components.func_to_container_op(A)\n\ndef B(TRAIN_SET_COUNT: int, TRAIN_INPUT_JSON: kfp.components.InputPath(str),TRAIN_OUTPUT_JSON: kfp.components.OutputPath(str)):\n    \"\"\"Generate a random number between minimum and maximum (inclusive).\"\"\"\n    from random import randint\n    import json\n    TRAIN_OUTPUT = list()\n    with open(TRAIN_INPUT_JSON, 'r') as reader:\n        TRAIN_INPUT = json.loads(reader.readline())\n    for i in range(TRAIN_SET_COUNT):\n        a = TRAIN_INPUT.__getitem__(i)[0]\n        b = TRAIN_INPUT.__getitem__(i)[1]\n        c = TRAIN_INPUT.__getitem__(i)[2]\n        op = a + (2 * b) + (3 * c)\n        TRAIN_OUTPUT.append(op)\n    jsonString = json.dumps(TRAIN_OUTPUT)\n    with open(TRAIN_OUTPUT_JSON, 'w') as odd_writer:\n        odd_writer.write(jsonString)\n\nB_op = kfp.components.func_to_container_op(B)\n\n\ndef AB(TRAIN_SET_COUNT: int, TRAIN_SET_LIMIT: int,  TRAIN_INPUT_JSON: kfp.components.OutputPath(str), TRAIN_OUTPUT_JSON: kfp.components.OutputPath(str)):\n    \"\"\"Generate a random number between minimum and maximum (inclusive).\"\"\"\n    from random import randint\n    import json\n    TRAIN_INPUT = list()\n    TRAIN_OUTPUT = list()\n    for i in range(TRAIN_SET_COUNT):\n        a = randint(0, TRAIN_SET_LIMIT)\n        b = randint(0, TRAIN_SET_LIMIT)\n        c = randint(0, TRAIN_SET_LIMIT)\n        op = a + (2 * b) + (3 * c)\n        TRAIN_INPUT.append([a, b, c])\n        TRAIN_OUTPUT.append(op)\n    jsonString = json.dumps(TRAIN_INPUT)\n    with open(TRAIN_INPUT_JSON, 'w') as odd_writer:\n        odd_writer.write(jsonString)\n    jsonString = json.dumps(TRAIN_OUTPUT)\n    with open(TRAIN_OUTPUT_JSON, 'w') as odd_writer:\n        odd_writer.write(jsonString)\n\nAB_op = kfp.components.func_to_container_op(AB)\n\ndef ABC(TRAIN_SET_COUNT: int, TRAIN_SET_LIMIT: int, testD: kfp.components.OutputPath(str)):\n    \"\"\"Generate a random number between minimum and maximum (inclusive).\"\"\"\n    from random import randint\n    import json\n    TRAIN_INPUT = list()\n    TRAIN_OUTPUT = list()\n    for i in range(TRAIN_SET_COUNT):\n        a = randint(0, TRAIN_SET_LIMIT)\n        b = randint(0, TRAIN_SET_LIMIT)\n        c = randint(0, TRAIN_SET_LIMIT)\n        op = a + (2 * b) + (3 * c)\n        TRAIN_INPUT.append([a, b, c])\n        TRAIN_OUTPUT.append(op)\n\n\n    import sys\n    import subprocess\n\n    # implement pip as a subprocess:\n    subprocess.check_call([sys.executable, '-m', 'pip', 'install',\n                           'sklearn'])\n\n    import sklearn\n    from sklearn.linear_model import LinearRegression\n\n    predictor = LinearRegression(n_jobs=-1)\n    import json\n\n    predictor.fit(X=TRAIN_INPUT, y=TRAIN_OUTPUT)\n\n    X_TEST = [[10, 20, 30]]\n    outcome = predictor.predict(X=X_TEST)\n    coefficients = predictor.coef_\n    with open(testD, 'w') as odd_writer:\n        odd_writer.write('Outcome : {}\\nCoefficients : {}'.format(outcome, coefficients))\n\n\n\nABC_op = kfp.components.func_to_container_op(ABC)\n\ndef C(TRAIN_INPUT_JSON: str, TRAIN_OUTPUT_JSON: str, testD: kfp.components.OutputPath(str)):\n    \"\"\"Flip a coin and output heads or tails randomly.\"\"\"\n    import sys\n    import subprocess\n\n    # implement pip as a subprocess:\n    subprocess.check_call([sys.executable, '-m', 'pip', 'install',\n                           'sklearn'])\n\n    import sklearn\n    from sklearn.linear_model import LinearRegression\n    predictor = LinearRegression(n_jobs=-1)\n    import json\n    predictor.fit(X=json.loads(TRAIN_INPUT_JSON), y=json.loads(TRAIN_OUTPUT_JSON))\n\n    X_TEST = [[10, 20, 30]]\n    outcome = predictor.predict(X=X_TEST)\n    coefficients = predictor.coef_\n    with open(testD, 'w') as odd_writer:\n        odd_writer.write('Outcome! : {}\\nCoefficients : {}'.format(outcome, coefficients))\n\n\nC_op = kfp.components.func_to_container_op(C)\n\ndef D(testD: kfp.components.InputPath(str)):\n    \"\"\"Flip a coin and output heads or tails randomly.\"\"\"\n    print(\"!!!\"+testD)\n\n\nD_op = kfp.components.func_to_container_op(D)\n\ndef ABCD(TRAIN_SET_COUNT: int, TRAIN_SET_LIMIT: int) ->str:\n    \"\"\"Generate a random number between minimum and maximum (inclusive).\"\"\"\n    from random import randint\n    import json\n    TRAIN_INPUT = list()\n    TRAIN_OUTPUT = list()\n    for i in range(TRAIN_SET_COUNT):\n        a = randint(0, TRAIN_SET_LIMIT)\n        b = randint(0, TRAIN_SET_LIMIT)\n        c = randint(0, TRAIN_SET_LIMIT)\n        op = a + (2 * b) + (3 * c)\n        TRAIN_INPUT.append([a, b, c])\n        TRAIN_OUTPUT.append(op)\n\n\n    import sys\n    import subprocess\n\n    # implement pip as a subprocess:\n    subprocess.check_call([sys.executable, '-m', 'pip', 'install',\n                           'sklearn'])\n\n    import sklearn\n    from sklearn.linear_model import LinearRegression\n\n    predictor = LinearRegression(n_jobs=-1)\n    import json\n\n    predictor.fit(X=TRAIN_INPUT, y=TRAIN_OUTPUT)\n\n    X_TEST = [[10, 20, 30]]\n    outcome = predictor.predict(X=X_TEST)\n    coefficients = predictor.coef_\n\n    return '!!Outcome : {}\\nCoefficients : {}'.format(outcome, coefficients)\n\nABCD_op = kfp.components.func_to_container_op(ABCD)\n\n@dsl.pipeline(\n    name='No merging pipeline',\n    description='Shows the cost of running an unmerged pipeline'\n)\ndef noMerge_pipeline():\n    out1 = A_op(1000, 1000)\n    out2 = B_op(1000, out1.outputs['TRAIN_INPUT_JSON'])\n    out3 = C_op(out1.outputs['TRAIN_INPUT_JSON'], out2.outputs['TRAIN_OUTPUT_JSON'])\n    D_op(out3.outputs['testD'])\n\n\ndef halfMerge_pipeline():\n    out3 = ABC_op(1000, 1000)\n    D_op(out3.outputs['testD'])\n\ndef fullMerge_pipeline():\n    out3 = ABCD_op(1000, 1000)\n\n\n\nif __name__ == '__main__':\n\n    client = kfp.Client()\n    arguments = {}  # whatever makes sense for new version\n\n    client.create_run_from_pipeline_func(fullMerge_pipeline, arguments=arguments)\n"
  },
  {
    "repo": "akontaxakis/kubeflow_pipelines",
    "file_path": "samples/HalfMerge.py",
    "raw_url": "https://raw.githubusercontent.com/akontaxakis/kubeflow_pipelines/master/samples/HalfMerge.py",
    "content": "\n\nimport kfp\nfrom kfp import dsl\nfrom kfp.components import func_to_container_op, InputPath, OutputPath\n\n\ndef get_train_input(TRAIN_SET_COUNT: int, TRAIN_SET_LIMIT: int,  TRAIN_INPUT_JSON: kfp.components.OutputPath(str), TRAIN_OUTPUT_JSON: kfp.components.OutputPath(str)):\n    \"\"\"Generate a random number between minimum and maximum (inclusive).\"\"\"\n    from random import randint\n    import json\n    TRAIN_INPUT = list()\n    TRAIN_OUTPUT = list()\n    for i in range(TRAIN_SET_COUNT):\n        a = randint(0, TRAIN_SET_LIMIT)\n        b = randint(0, TRAIN_SET_LIMIT)\n        c = randint(0, TRAIN_SET_LIMIT)\n        op = a + (2 * b) + (3 * c)\n        TRAIN_INPUT.append([a, b, c])\n        TRAIN_OUTPUT.append(op)\n    jsonString = json.dumps(TRAIN_INPUT)\n    with open(TRAIN_INPUT_JSON, 'w') as odd_writer:\n        odd_writer.write(jsonString)\n    jsonString = json.dumps(TRAIN_OUTPUT)\n    with open(TRAIN_OUTPUT_JSON, 'w') as odd_writer:\n        odd_writer.write(jsonString)\n\nget_train_input_op = kfp.components.func_to_container_op(get_train_input)\n\n\ndef train_evaluation(k: int, TRAIN_INPUT_JSON: str, TRAIN_OUTPUT_JSON: str) -> str:\n    \"\"\"Flip a coin and output heads or tails randomly.\"\"\"\n    import sys\n    import subprocess\n    j = k\n    # implement pip as a subprocess:\n    subprocess.check_call([sys.executable, '-m', 'pip', 'install','sklearn'])\n\n    import sklearn\n    from sklearn.linear_model import LinearRegression\n    predictor = LinearRegression(n_jobs=-1)\n    import json\n    predictor.fit(X=json.loads(TRAIN_INPUT_JSON), y=json.loads(TRAIN_OUTPUT_JSON))\n\n    X_TEST = [[10, 20, 30]]\n    outcome = predictor.predict(X=X_TEST)\n    coefficients = predictor.coef_\n\n    return 'Outcome : {}\\nCoefficients : {}'.format(outcome, coefficients)\n\n\ntrain_evaluation_op = kfp.components.func_to_container_op(train_evaluation)\n\n\n@dsl.pipeline(\n    name='No merging pipeline',\n    description='Shows the cost of running an unmerged pipeline'\n)\ndef halfMerge_pipeline():\n    out1 = get_train_input_op(2000, 100)\n    #output = train_evaluation_op(-1, out1.outputs['TRAIN_INPUT_JSON'], out1.outputs['TRAIN_OUTPUT_JSON'])\n\n\n# Submit the pipeline for execution:\n# kfp.Client(host=kfp_endpoint).create_run_from_pipeline_func(flipcoin_pipeline, arguments={})\nif __name__ == '__main__':\n    # Compiling the pipeline\n\n    # pipeline_func = noMerge_pipeline\n    # pipeline_filename = pipeline_func.__name__ + '.pipeline.tar.gz'\n    # import kfp.compiler as compiler\n    # compiler.Compiler().compile(pipeline_func, pipeline_filename)\n\n    client = kfp.Client()\n    # Specify pipeline argument values\n    arguments = {}  # whatever makes sense for new version\n    # Submit a pipeline run\n    client.create_run_from_pipeline_func(halfMerge_pipeline, arguments=arguments)\n"
  },
  {
    "repo": "akontaxakis/kubeflow_pipelines",
    "file_path": "samples/Hopefully.py",
    "raw_url": "https://raw.githubusercontent.com/akontaxakis/kubeflow_pipelines/master/samples/Hopefully.py",
    "content": "\n\nimport kfp\nfrom kfp import dsl\nfrom kfp.components import func_to_container_op, InputPath, OutputPath\n\n\ndef get_train_input(TRAIN_SET_COUNT: int, TRAIN_SET_LIMIT: int,  TRAIN_INPUT_JSON: kfp.components.OutputPath(str)):\n    \"\"\"Generate a random number between minimum and maximum (inclusive).\"\"\"\n    from random import randint\n    import json\n    TRAIN_INPUT = list()\n    for i in range(TRAIN_SET_COUNT):\n        a = randint(0, TRAIN_SET_LIMIT)\n        b = randint(0, TRAIN_SET_LIMIT)\n        c = randint(0, TRAIN_SET_LIMIT)\n        TRAIN_INPUT.append([a, b, c])\n    jsonString = json.dumps(TRAIN_INPUT)\n    with open(TRAIN_INPUT_JSON, 'w') as odd_writer:\n        odd_writer.write(jsonString)\n\n\nget_train_input_op = kfp.components.func_to_container_op(get_train_input)\n\n\ndef get_train_output(TRAIN_SET_COUNT: int, TRAIN_INPUT_JSON: kfp.components.InputPath(str),TRAIN_OUTPUT_JSON: kfp.components.OutputPath(str)):\n    \"\"\"Generate a random number between minimum and maximum (inclusive).\"\"\"\n    from random import randint\n    import json\n    TRAIN_OUTPUT = list()\n    with open(TRAIN_INPUT_JSON, 'r') as reader:\n        TRAIN_INPUT = json.loads(reader.readline())\n    for i in range(TRAIN_SET_COUNT):\n        a = TRAIN_INPUT.__getitem__(i)[0]\n        b = TRAIN_INPUT.__getitem__(i)[1]\n        c = TRAIN_INPUT.__getitem__(i)[2]\n        op = a + (2 * b) + (3 * c)\n        TRAIN_OUTPUT.append(op)\n    jsonString = json.dumps(TRAIN_OUTPUT)\n    with open(TRAIN_OUTPUT_JSON, 'w') as odd_writer:\n        odd_writer.write(jsonString)\n\n\nget_train_output_op = kfp.components.func_to_container_op(get_train_output)\n\n\ndef train_evaluation(TRAIN_INPUT_JSON: str, TRAIN_OUTPUT_JSON: str, tests: kfp.components.OutputPath(str)):\n    \"\"\"Flip a coin and output heads or tails randomly.\"\"\"\n    import sys\n    import subprocess\n\n    # implement pip as a subprocess:\n    subprocess.check_call([sys.executable, '-m', 'pip', 'install',\n                           'sklearn'])\n\n    import sklearn\n    from sklearn.linear_model import LinearRegression\n    predictor = LinearRegression(n_jobs=-1)\n    import json\n    predictor.fit(X=json.loads(TRAIN_INPUT_JSON), y=json.loads(TRAIN_OUTPUT_JSON))\n\n    X_TEST = [[10, 20, 30]]\n    outcome = predictor.predict(X=X_TEST)\n    coefficients = predictor.coef_\n\n    return 'Outcome : {}\\nCoefficients : {}'.format(outcome, coefficients)\n\n\ntrain_evaluation_op = kfp.components.func_to_container_op(train_evaluation)\n\ndef print_fun(test: str):\n    \"\"\"Flip a coin and output heads or tails randomly.\"\"\"\n    print(test)\n\n\nprint_fun_op = kfp.components.func_to_container_op(print_fun)\n\n\n@dsl.pipeline(\n    name='No merging pipeline',\n    description='Shows the cost of running an unmerged pipeline'\n)\ndef noMerge_pipeline():\n    out1 = get_train_input_op(2000, 2000)\n\n# Submit the pipeline for execution:\n# kfp.Client(host=kfp_endpoint).create_run_from_pipeline_func(flipcoin_pipeline, arguments={})\nif __name__ == '__main__':\n    # Compiling the pipeline\n\n    # pipeline_func = noMerge_pipeline\n    # pipeline_filename = pipeline_func.__name__ + '.pipeline.tar.gz'\n    # import kfp.compiler as compiler\n    # compiler.Compiler().compile(pipeline_func, pipeline_filename)\n\n    client = kfp.Client()\n    # Specify pipeline argument values\n    arguments = {}  # whatever makes sense for new version\n    # Submit a pipeline run\n    client.create_run_from_pipeline_func(noMerge_pipeline, arguments=arguments)\n"
  },
  {
    "repo": "akontaxakis/kubeflow_pipelines",
    "file_path": "samples/book_example.py",
    "raw_url": "https://raw.githubusercontent.com/akontaxakis/kubeflow_pipelines/master/samples/book_example.py",
    "content": "import kfp\nfrom kfp import dsl\nfrom kfp.components import func_to_container_op\n\ndef add(a: float, b: float) -> float:\n    '''Calculates sum of two arguments'''\n    return a + b\n\n\nadd_op = kfp.components.func_to_container_op(add)\n\nfrom typing import NamedTuple\n\n\ndef my_divmod(dividend: float, divisor: float) -> NamedTuple('MyDivmodOutput', [('quotient', float), ('remainder', float)]):\n    '''Divides two numbers and calculate the quotient and remainder'''\n    # Imports inside a component function:\n    import numpy as np\n    # This function demonstrates how to use nested functions inside a\n    # component function:\n    def divmod_helper(dividend, divisor):\n        return np.divmod(dividend, divisor)\n\n    (quotient, remainder) = divmod_helper(dividend, divisor)\n    from collections import namedtuple\n    divmod_output = namedtuple('MyDivmodOutput', ['quotient', 'remainder'])\n    return divmod_output(quotient, remainder)\n\n\ndivmod_op = kfp.components.func_to_container_op(my_divmod, base_image='tensorflow/tensorflow:1.14.0-py3')\n\n\n@dsl.pipeline(\n    name='Calculation pipeline',\n    description='A toy pipeline that performs arithmetic calculations.'\n)\ndef calc_pipeline(\n        a='a',\n        b='7',\n        c='17',\n):\n    # Passing pipeline parameter and a constant value as operation arguments\n    add_task = add_op(a, 4)  # Returns a dsl.ContainerOp class instance.\n    # Passing a task output reference as operation arguments\n    # For an operation with a single return value, the output\n    # reference can be accessed using `task.output`\n    # or `task.outputs['output_name']` syntax\n    divmod_task = divmod_op(add_task.output, b)\n    # For an operation with multiple return values, the output references\n    # can be accessed using `task.outputs['output_name']` syntax\n    result_task = add_op(divmod_task.outputs['quotient'], c)\n\nif __name__ == '__main__':\n    client = kfp.Client()\n    # Specify pipeline argument values\n    arguments = {'a': '7', 'b': '8'} #whatever makes sense for new version\n    # Submit a pipeline run\n    client.create_run_from_pipeline_func(calc_pipeline, arguments=arguments)\n"
  },
  {
    "repo": "akontaxakis/kubeflow_pipelines",
    "file_path": "samples/merge.py",
    "raw_url": "https://raw.githubusercontent.com/akontaxakis/kubeflow_pipelines/master/samples/merge.py",
    "content": "\n\nimport kfp\nfrom kfp import dsl\nfrom kfp.components import func_to_container_op, InputPath, OutputPath\n\n\ndef get_train_input(TRAIN_SET_COUNT: int, TRAIN_SET_LIMIT: int):\n    \"\"\"Generate a random number between minimum and maximum (inclusive).\"\"\"\n    from random import randint\n    import json\n    TRAIN_SET_LIMIT = 1000\n    TRAIN_SET_COUNT = 100\n    import sys\n    import subprocess\n\n    # implement pip as a subprocess:\n    subprocess.check_call([sys.executable, '-m', 'pip', 'install',\n                           'sklearn'])\n    TRAIN_INPUT = list()\n    TRAIN_OUTPUT = list()\n    print(type(TRAIN_OUTPUT))\n    for i in range(TRAIN_SET_COUNT):\n        a = randint(0, TRAIN_SET_LIMIT)\n        b = randint(0, TRAIN_SET_LIMIT)\n        c = randint(0, TRAIN_SET_LIMIT)\n        op = a + (2 * b) + (3 * c)\n        TRAIN_INPUT.append([a, b, c])\n        TRAIN_OUTPUT.append(op)\n\n    jsonString = json.dumps(TRAIN_INPUT)\n    print(jsonString)\n\n    TRAIN_INPUT = json.loads(jsonString)\n    print(type(jsonString))\n    for i in range(TRAIN_SET_COUNT):\n        print(TRAIN_INPUT)\n    print(TRAIN_OUTPUT)\n    from sklearn.linear_model import LinearRegression\n\n    predictor = LinearRegression(n_jobs=-1)\n    predictor.fit(X=TRAIN_INPUT, y=TRAIN_OUTPUT)\n\n    print(type(predictor))\n    X_TEST = [[10, 20, 30]]\n    outcome = predictor.predict(X=X_TEST)\n    coefficients = predictor.coef_\n\n    print('Outcome : {}\\nCoefficients : {}'.format(outcome, coefficients))\n\n\nget_train_input_op = kfp.components.func_to_container_op(get_train_input)\n\n\ndef get_train_output(TRAIN_SET_COUNT: int, TRAIN_INPUT_JSON: kfp.components.InputPath(str),TRAIN_OUTPUT_JSON: kfp.components.OutputPath(str)):\n    \"\"\"Generate a random number between minimum and maximum (inclusive).\"\"\"\n    from random import randint\n    import json\n    TRAIN_OUTPUT = list()\n    with open(TRAIN_INPUT_JSON, 'r') as reader:\n        TRAIN_INPUT = json.loads(reader.readline())\n    for i in range(TRAIN_SET_COUNT):\n        a = TRAIN_INPUT.__getitem__(i)[0]\n        b = TRAIN_INPUT.__getitem__(i)[1]\n        c = TRAIN_INPUT.__getitem__(i)[2]\n        op = a + (2 * b) + (3 * c)\n        TRAIN_OUTPUT.append(op)\n    jsonString = json.dumps(TRAIN_OUTPUT)\n    with open(TRAIN_OUTPUT_JSON, 'w') as odd_writer:\n        odd_writer.write(jsonString)\n\n\n\nget_train_output_op = kfp.components.func_to_container_op(get_train_output)\n\n\ndef train_evaluation(TRAIN_INPUT_JSON: str, TRAIN_OUTPUT_JSON: str) -> str:\n    \"\"\"Flip a coin and output heads or tails randomly.\"\"\"\n    import sys\n    import subprocess\n\n    # implement pip as a subprocess:\n    subprocess.check_call([sys.executable, '-m', 'pip', 'install',\n                           'sklearn'])\n\n    import sklearn\n    from sklearn.linear_model import LinearRegression\n    predictor = LinearRegression(n_jobs=-1)\n    import json\n    predictor.fit(X=json.loads(TRAIN_INPUT_JSON), y=json.loads(TRAIN_OUTPUT_JSON))\n\n    X_TEST = [[10, 20, 30]]\n    outcome = predictor.predict(X=X_TEST)\n    coefficients = predictor.coef_\n\n    return 'Outcome : {}\\nCoefficients : {}'.format(outcome, coefficients)\n\n\ntrain_evaluation_op = kfp.components.func_to_container_op(train_evaluation)\n\n\n@dsl.pipeline(\n    name='No merging pipeline',\n    description='Shows the cost of running an unmerged pipeline'\n)\ndef Merge_pipeline():\n    out1 = get_train_input_op(1000, 10000)\n    #out2 = get_train_output_op(1000, out1.outputs['TRAIN_INPUT_JSON'])\n    #output = train_evaluation_op(out1.outputs['TRAIN_INPUT_JSON'], out2.outputs['TRAIN_OUTPUT_JSON'])\n\n\n# Submit the pipeline for execution:\n# kfp.Client(host=kfp_endpoint).create_run_from_pipeline_func(flipcoin_pipeline, arguments={})\nif __name__ == '__main__':\n    # Compiling the pipeline\n\n    # pipeline_func = noMerge_pipeline\n    # pipeline_filename = pipeline_func.__name__ + '.pipeline.tar.gz'\n    # import kfp.compiler as compiler\n    # compiler.Compiler().compile(pipeline_func, pipeline_filename)\n\n    client = kfp.Client()\n    # Specify pipeline argument values\n    arguments = {}  # whatever makes sense for new version\n    # Submit a pipeline run\n    client.create_run_from_pipeline_func(Merge_pipeline, arguments=arguments)\n"
  },
  {
    "repo": "akontaxakis/kubeflow_pipelines",
    "file_path": "samples/mnist_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/akontaxakis/kubeflow_pipelines/master/samples/mnist_pipeline.py",
    "content": "# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nKubeflow Pipelines MNIST example\nRun this script to compile pipeline\n\"\"\"\n\n\nimport kfp.dsl as dsl\nimport kfp.gcp as gcp\nimport kfp.onprem as onprem\n\nplatform = 'local'\n\n@dsl.pipeline(\n  name='MNIST',\n  description='A pipeline to train and serve the MNIST example.'\n)\ndef mnist_pipeline(model_export_dir='gs://your-bucket/export',\n                   train_steps='200',\n                   learning_rate='0.01',\n                   batch_size='100',\n                   pvc_name=''):\n  \"\"\"\n  Pipeline with three stages:\n    1. train an MNIST classifier\n    2. deploy a tf-serving instance to the cluster\n    3. deploy a web-ui to interact with it\n  \"\"\"\n  train = dsl.ContainerOp(\n      name='train',\n      image='gcr.io/kubeflow-examples/mnist/model:v20190304-v0.2-176-g15d997b',\n      arguments=[\n          \"/opt/model.py\",\n          \"--tf-export-dir\", model_export_dir,\n          \"--tf-train-steps\", train_steps,\n          \"--tf-batch-size\", batch_size,\n          \"--tf-learning-rate\", learning_rate\n          ]\n  )\n\n\n  serve_args = [\n      '--model-export-path', model_export_dir,\n      '--server-name', \"mnist-service\"\n  ]\n  if platform != 'GCP':\n    serve_args.extend([\n        '--cluster-name', \"mnist-pipeline\",\n        '--pvc-name', pvc_name\n    ])\n\n  serve = dsl.ContainerOp(\n      name='serve',\n      image='gcr.io/ml-pipeline/ml-pipeline-kubeflow-deployer:'\n            '7775692adf28d6f79098e76e839986c9ee55dd61',\n      arguments=serve_args\n  )\n  serve.after(train)\n\n\n  webui_args = [\n          '--image', 'gcr.io/kubeflow-examples/mnist/web-ui:'\n                     'v20190304-v0.2-176-g15d997b-pipelines',\n          '--name', 'web-ui',\n          '--container-port', '5000',\n          '--service-port', '80',\n          '--service-type', \"LoadBalancer\"\n  ]\n  if platform != 'GCP':\n    webui_args.extend([\n      '--cluster-name', \"mnist-pipeline\"\n    ])\n\n  web_ui = dsl.ContainerOp(\n      name='web-ui',\n      image='gcr.io/kubeflow-examples/mnist/deploy-service:latest',\n      arguments=webui_args\n  )\n  web_ui.after(serve)\n\n  steps = [train, serve, web_ui]\n  for step in steps:\n    if platform == 'GCP':\n      step.apply(gcp.use_gcp_secret('user-gcp-sa'))\n    else:\n      step.apply(onprem.mount_pvc(pvc_name, 'local-storage', '/mnt'))\n\nif __name__ == '__main__':\n  import kfp.compiler as compiler\n  compiler.Compiler().compile(mnist_pipeline, __file__ + '.tar.gz')"
  },
  {
    "repo": "akontaxakis/kubeflow_pipelines",
    "file_path": "samples/sendData.py",
    "raw_url": "https://raw.githubusercontent.com/akontaxakis/kubeflow_pipelines/master/samples/sendData.py",
    "content": "import kfp\nfrom kfp import dsl\n\ndef SendMsg(\n    send_msg: str = 'akash'\n):\n    return dsl.ContainerOp(\n        name = 'Print msg',\n        image = 'docker.io/akashdesarda/comp1:latest',\n        command = ['python', 'msg.py'],\n        arguments=[\n            '--msg', send_msg\n        ],\n        file_outputs={\n            'output': '/output.txt',\n        }\n    )\n\ndef GetMsg(\n    get_msg: str\n):\n    return dsl.ContainerOp(\n        name = 'Read msg from 1st component',\n        image = 'docker.io/akashdesarda/comp2:latest',\n        command = ['python', 'msg.py'],\n        arguments=[\n            '--msg', get_msg\n        ]\n    )\n\n@dsl.pipeline(\n    name = 'Pass parameter',\n    description = 'Passing para')\ndef  passing_parameter(send_msg):\n    comp1 = SendMsg(\"send_msg\")\n    comp2 = GetMsg(comp1.output)\n\n\nif __name__ == '__main__':\n  import kfp.compiler as compiler\n\n  client = kfp.Client()\n  arguments = {}\n  client.create_run_from_pipeline_func(passing_parameter, arguments=arguments)"
  },
  {
    "repo": "akontaxakis/kubeflow_pipelines",
    "file_path": "samples/testFun.py",
    "raw_url": "https://raw.githubusercontent.com/akontaxakis/kubeflow_pipelines/master/samples/testFun.py",
    "content": "import json\n\nimport kfp\nfrom kfp import dsl\nfrom kfp.components import func_to_container_op, InputPath, OutputPath\n\n\ndef get_train_input(TRAIN_SET_COUNT: int, TRAIN_SET_LIMIT: int) -> str:\n    \"\"\"Generate a random number between minimum and maximum (inclusive).\"\"\"\n    from random import randint\n    TRAIN_INPUT = list()\n    for i in range(TRAIN_SET_COUNT):\n        a = randint(0, TRAIN_SET_LIMIT)\n        b = randint(0, TRAIN_SET_LIMIT)\n        c = randint(0, TRAIN_SET_LIMIT)\n        TRAIN_INPUT.append([a, b, c])\n    jsonString = json.dumps(TRAIN_INPUT)\n    return '' + jsonString\n\n\nget_train_input_op = kfp.components.func_to_container_op(get_train_input)\n\n\ndef get_train_output(TRAIN_SET_COUNT: int, TRAIN_INPUT_JSON: str) -> str:\n    \"\"\"Generate a random number between minimum and maximum (inclusive).\"\"\"\n    from random import randint\n\n    TRAIN_OUTPUT = list()\n    TRAIN_INPUT = json.loads(TRAIN_INPUT_JSON)\n    for i in range(TRAIN_SET_COUNT):\n        a = TRAIN_INPUT.__getitem__(i)[0]\n        b = TRAIN_INPUT.__getitem__(i)[1]\n        c = TRAIN_INPUT.__getitem__(i)[2]\n        op = a + (2 * b) + (3 * c)\n        TRAIN_OUTPUT.append(op)\n\n    jsonString = json.dumps(TRAIN_OUTPUT)\n    return '' + jsonString\n\n\nget_train_output_op = kfp.components.func_to_container_op(get_train_output)\n\n\ndef train_evaluation(TRAIN_INPUT_JSON: str, TRAIN_OUTPUT_JSON: str) -> str:\n    \"\"\"Flip a coin and output heads or tails randomly.\"\"\"\n    from sklearn.linear_model import LinearRegression\n    predictor = LinearRegression(n_jobs=-1)\n    predictor.fit(X=json.loads(TRAIN_INPUT_JSON), y=json.loads(TRAIN_OUTPUT_JSON))\n\n    X_TEST = [[10, 20, 30]]\n    outcome = predictor.predict(X=X_TEST)\n    coefficients = predictor.coef_\n\n    return 'Outcome : {}\\nCoefficients : {}'.format(outcome, coefficients)\n\n\ntrain_evaluation_op = kfp.components.func_to_container_op(train_evaluation)\n\n\ndef print_fun(TRAIN_OUTPUT_JSON: str):\n    \"\"\"Flip a coin and output heads or tails randomly.\"\"\"\n    print(str)\n\n\nprint_fun_op = kfp.components.func_to_container_op(print_fun)\n\n\n@dsl.pipeline(\n    name='No merging pipeline',\n    description='Shows the cost of running an unmerged pipeline'\n)\ndef noMerge_pipeline():\n    out1 = get_train_input_op(100, 1000)\n    out2 = get_train_output_op(100, out1.outputs['TRAIN_INPUT_JSON'])\n    output = train_evaluation_op(out1.outputs['TRAIN_INPUT_JSON'], out2.outputs['TRAIN_OUTPUT_JSON'])\n    print_fun_op(output)\n\n# Submit the pipeline for execution:\n# kfp.Client(host=kfp_endpoint).create_run_from_pipeline_func(flipcoin_pipeline, arguments={})\nif __name__ == '__main__':\n    # Compiling the pipeline\n\n    # pipeline_func = noMerge_pipeline\n    # pipeline_filename = pipeline_func.__name__ + '.pipeline.tar.gz'\n    # import kfp.compiler as compiler\n    # compiler.Compiler().compile(pipeline_func, pipeline_filename)\n\n    client = kfp.Client()\n    # Specify pipeline argument values\n    arguments = {}  # whatever makes sense for new version\n    # Submit a pipeline run\n    client.create_run_from_pipeline_func(noMerge_pipeline, arguments=arguments)\n"
  },
  {
    "repo": "akontaxakis/kubeflow_pipelines",
    "file_path": "samples/test/after.py",
    "raw_url": "https://raw.githubusercontent.com/akontaxakis/kubeflow_pipelines/master/samples/test/after.py",
    "content": "# Copyright 2020 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom kfp import components\nfrom kfp import dsl\nfrom kfp import compiler\n\ncomponent_op = components.load_component_from_text(\"\"\"\nname: Print Text\ninputs:\n- {name: text, type: String}\nimplementation:\n  container:\n    image: alpine\n    command:\n    - sh\n    - -c\n    - |\n      set -e -x\n      echo \"$0\"\n    - {inputValue: text}\n\"\"\")\n\n\n@dsl.pipeline(name='pipeline-with-after')\ndef my_pipeline():\n    task1 = component_op(text='1st task')\n    task2 = component_op(text='2nd task').after(task1)\n    task3 = component_op(text='3rd task').after(task1, task2)\n\n\nif __name__ == '__main__':\n    compiler.Compiler().compile(\n        pipeline_func=my_pipeline,\n        pipeline_root='dummy_root',\n        output_path=__file__ + '.json')\n"
  },
  {
    "repo": "akontaxakis/kubeflow_pipelines",
    "file_path": "samples/test/after_test.py",
    "raw_url": "https://raw.githubusercontent.com/akontaxakis/kubeflow_pipelines/master/samples/test/after_test.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp.deprecated as kfp\nfrom .after import my_pipeline\nfrom kfp.samples.test.utils import run_pipeline_func, TestCase\n\nrun_pipeline_func([\n    TestCase(\n        pipeline_func=my_pipeline,\n        mode=kfp.dsl.PipelineExecutionMode.V2_ENGINE),\n])\n"
  },
  {
    "repo": "akontaxakis/kubeflow_pipelines",
    "file_path": "samples/test/cache_v2_compatible_test.py",
    "raw_url": "https://raw.githubusercontent.com/akontaxakis/kubeflow_pipelines/master/samples/test/cache_v2_compatible_test.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Two step v2-compatible pipeline.\"\"\"\n\n# %%\n\nfrom __future__ import annotations\n\nimport random\nimport string\nimport unittest\nimport functools\n\nimport kfp_server_api\n\nfrom .two_step import two_step_pipeline\nfrom kfp.samples.test.utils import run_pipeline_func, TestCase, KfpMlmdClient, KfpTask\nfrom ml_metadata.proto import Execution\n\n\ndef verify_tasks(t: unittest.TestCase, tasks: dict[str, KfpTask], task_state,\n                 uri: str, some_int: int):\n    task_names = [*tasks.keys()]\n    t.assertCountEqual(task_names, ['train-op', 'preprocess'], 'task names')\n\n    preprocess = tasks['preprocess']\n    train = tasks['train-op']\n\n    t.assertEqual(\n        {\n            'name': 'preprocess',\n            'inputs': {\n                'artifacts': [],\n                'parameters': {\n                    'some_int': some_int,\n                    'uri': uri\n                }\n            },\n            'outputs': {\n                'artifacts': [{\n                    'metadata': {\n                        'display_name': 'output_dataset_one',\n                    },\n                    'name': 'output_dataset_one',\n                    'type': 'system.Dataset'\n                }],\n                'parameters': {\n                    'output_parameter_one': some_int\n                }\n            },\n            'type': 'system.ContainerExecution',\n            'state': task_state,\n        }, preprocess.get_dict())\n    t.assertEqual(\n        {\n            'name': 'train-op',\n            'inputs': {\n                'artifacts': [{\n                    'metadata': {\n                        'display_name': 'output_dataset_one',\n                    },\n                    'name': 'dataset',\n                    'type': 'system.Dataset',\n                }],\n                'parameters': {\n                    'num_steps': some_int\n                }\n            },\n            'outputs': {\n                'artifacts': [{\n                    'metadata': {\n                        'display_name': 'model',\n                    },\n                    'name': 'model',\n                    'type': 'system.Model',\n                }],\n                'parameters': {}\n            },\n            'type': 'system.ContainerExecution',\n            'state': task_state,\n        }, train.get_dict())\n\n\ndef verify(run: kfp_server_api.ApiRun, mlmd_connection_config, uri: str,\n           some_int, state: int, **kwargs):\n    t = unittest.TestCase()\n    t.maxDiff = None  # we always want to see full diff\n    t.assertEqual(run.status, 'Succeeded')\n    client = KfpMlmdClient(mlmd_connection_config=mlmd_connection_config)\n    tasks = client.get_tasks(run_id=run.id)\n    verify_tasks(t, tasks, state, uri, some_int)\n\n\nif __name__ == '__main__':\n    letters = string.ascii_lowercase\n    random_uri = 'http://' + ''.join(random.choice(letters) for i in range(5))\n    random_int = random.randint(0, 10000)\n    # TODO: update to v2 engine test\n    # run_pipeline_func([\n    #     TestCase(\n    #         pipeline_func=two_step_pipeline,\n    #         arguments={\n    #             'uri': f'{random_uri}',\n    #             'some_int': f'{random_int}'\n    #         },\n    #         verify_func=functools.partial(\n    #             verify,\n    #             uri=random_uri,\n    #             some_int=random_int,\n    #             state=Execution.State.COMPLETE,\n    #         ),\n    #         mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE,\n    #         enable_caching=True\n    #     ),\n    # ]),\n    # run_pipeline_func([\n    #     TestCase(\n    #         pipeline_func=two_step_pipeline,\n    #         arguments={\n    #             'uri': f'{random_uri}',\n    #             'some_int': f'{random_int}'\n    #         },\n    #         verify_func=functools.partial(\n    #             verify,\n    #             uri=random_uri,\n    #             some_int=random_int,\n    #             state=Execution.State.CACHED\n    #         ),\n    #         mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE,\n    #         enable_caching=True\n    #     ),\n    # ])\n\n# %%\n"
  },
  {
    "repo": "akontaxakis/kubeflow_pipelines",
    "file_path": "samples/test/fail.py",
    "raw_url": "https://raw.githubusercontent.com/akontaxakis/kubeflow_pipelines/master/samples/test/fail.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Fail pipeline.\"\"\"\n\nfrom kfp.deprecated import components, dsl\n\n\ndef fail():\n    '''Fails'''\n    import sys\n    sys.exit(1)\n\n\nfail_op = components.create_component_from_func(\n    fail, base_image='alpine:latest')\n\n\n@dsl.pipeline(name='fail-pipeline')\ndef fail_pipeline():\n    fail_task = fail_op()\n"
  },
  {
    "repo": "akontaxakis/kubeflow_pipelines",
    "file_path": "samples/test/fail_parameter_value_missing.py",
    "raw_url": "https://raw.githubusercontent.com/akontaxakis/kubeflow_pipelines/master/samples/test/fail_parameter_value_missing.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom kfp.deprecated import dsl, components\n\necho = components.load_component_from_text(\n    \"\"\"\nname: Echo\ninputs:\n- {name: text, type: String}\nimplementation:\n  container:\n    image: alpine\n    command:\n    - echo\n    - {inputValue: text}\n\"\"\"\n)\n\n\n@dsl.pipeline(name='parameter_value_missing')\ndef pipeline(\n    parameter:\n    str  # parameter should be specified when submitting, but we are missing it in the test\n):\n    echo_op = echo(text=parameter)\n"
  },
  {
    "repo": "akontaxakis/kubeflow_pipelines",
    "file_path": "samples/test/fail_parameter_value_missing_test.py",
    "raw_url": "https://raw.githubusercontent.com/akontaxakis/kubeflow_pipelines/master/samples/test/fail_parameter_value_missing_test.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp.deprecated as kfp\nfrom .fail_parameter_value_missing import pipeline\nfrom kfp.samples.test.utils import run_pipeline_func, TestCase\n\n\ndef verify(run, run_id: str, **kwargs):\n    assert run.status == 'Succeeded'\n    # TODO(Bobgy): should a pipeline fail when it is missing a required input?\n    # assert run.status == 'Failed'\n\n\nrun_pipeline_func([\n    TestCase(\n        pipeline_func=pipeline,\n        verify_func=verify,\n        mode=kfp.dsl.PipelineExecutionMode.V1_LEGACY)\n])\n"
  },
  {
    "repo": "akontaxakis/kubeflow_pipelines",
    "file_path": "samples/test/fail_test.py",
    "raw_url": "https://raw.githubusercontent.com/akontaxakis/kubeflow_pipelines/master/samples/test/fail_test.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Fail pipeline.\"\"\"\n\nfrom __future__ import annotations\nimport unittest\nimport kfp.deprecated as kfp\nimport kfp_server_api\nfrom ml_metadata.proto import Execution\nfrom .fail import fail_pipeline\nfrom .fail_v2 import fail_pipeline as fail_v2_pipeline\nfrom kfp.samples.test.utils import TaskInputs, TaskOutputs, run_pipeline_func, TestCase, KfpTask\n\n\ndef verify(run, **kwargs):\n    assert run.status == 'Failed'\n\n\ndef verify_v2(t: unittest.TestCase, run: kfp_server_api.ApiRun,\n              tasks: dict[str, KfpTask], **kwargs):\n    t.assertEqual(run.status, 'Failed')\n    t.assertEqual(\n        {\n            'fail':\n                KfpTask(\n                    name='fail',\n                    type='system.ContainerExecution',\n                    # TODO(Bobgy): fix v2 engine to properly publish FAILED state.\n                    state=Execution.State.RUNNING,\n                    inputs=TaskInputs(parameters={}, artifacts=[]),\n                    outputs=TaskOutputs(parameters={}, artifacts=[]),\n                )\n        },\n        tasks,\n    )\n\n\nrun_pipeline_func([\n    TestCase(\n        pipeline_func=fail_v2_pipeline,\n        verify_func=verify_v2,\n        mode=kfp.dsl.PipelineExecutionMode.V2_ENGINE),\n    TestCase(\n        pipeline_func=fail_pipeline,\n        verify_func=verify,\n        mode=kfp.dsl.PipelineExecutionMode.V1_LEGACY),\n])\n"
  },
  {
    "repo": "akontaxakis/kubeflow_pipelines",
    "file_path": "samples/test/fail_v2.py",
    "raw_url": "https://raw.githubusercontent.com/akontaxakis/kubeflow_pipelines/master/samples/test/fail_v2.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Fail pipeline.\"\"\"\n\nfrom kfp import dsl\n\n\n@dsl.component\ndef fail():\n    '''Fails'''\n    import sys\n    sys.exit(1)\n\n\n@dsl.pipeline(name='fail-pipeline')\ndef fail_pipeline():\n    fail_task = fail()\n"
  },
  {
    "repo": "akontaxakis/kubeflow_pipelines",
    "file_path": "samples/test/legacy_data_passing.py",
    "raw_url": "https://raw.githubusercontent.com/akontaxakis/kubeflow_pipelines/master/samples/test/legacy_data_passing.py",
    "content": "# This pipeline demonstrates and verifies all ways of data passing supported by KFP.\n\n# %% [markdown]\n# KFP has simple data passing model. There are three different kinds of arguments and tw different ways to consume an input argument. All combinations are supported.\n\n# Any input can be consumed:\n# * As value (`inputValue` placeholder)\n# * As file (`inputPath` placeholder).\n\n# Input argument can come from:\n# * Constant value\n# * Pipeline parameter\n# * Upstream component output\n\n# Combining these options there are 6 end-to-end data passing cases, each of which works regardless of type:\n\n# 1. Pass constant value which gets consumed as value. (Common)\n# 2. Pass constant value which gets consumed as file. (Often used in test pipelines)\n# 3. Pass pipeline parameter that gets consumed as value. (Common)\n# 4. Pass pipeline parameter that gets consumed as file. (Rare. Sometimes used with JSON parameters)\n# 5. Pass task output that gets consumed as value. (Common)\n# 6. Pass task output that gets consumed as file. (Common)\n\n# The only restriction on types is that when both upstream output and downstream input have types, the types must match.\n\n# %%\nimport kfp.deprecated as kfp\nfrom kfp.deprecated.components import create_component_from_func, InputPath, OutputPath\n\n\n# Components\n# Produce\n@create_component_from_func\ndef produce_anything(data_path: OutputPath()):\n    with open(data_path, \"w\") as f:\n        f.write(\"produce_anything\")\n\n\n@create_component_from_func\ndef produce_something(data_path: OutputPath(\"Something\")):\n    with open(data_path, \"w\") as f:\n        f.write(\"produce_something\")\n\n\n@create_component_from_func\ndef produce_something2() -> 'Something':\n    return \"produce_something2\"\n\n\n@create_component_from_func\ndef produce_string() -> str:\n    return \"produce_string\"\n\n\n# Consume as value\n@create_component_from_func\ndef consume_anything_as_value(data):\n    print(\"consume_anything_as_value: \" + data)\n\n\n@create_component_from_func\ndef consume_something_as_value(data: \"Something\"):\n    print(\"consume_something_as_value: \" + data)\n\n\n@create_component_from_func\ndef consume_string_as_value(data: str):\n    print(\"consume_string_as_value: \" + data)\n\n\n# Consume as file\n@create_component_from_func\ndef consume_anything_as_file(data_path: InputPath()):\n    with open(data_path) as f:\n        print(\"consume_anything_as_file: \" + f.read())\n\n\n@create_component_from_func\ndef consume_something_as_file(data_path: InputPath('Something')):\n    with open(data_path) as f:\n        print(\"consume_something_as_file: \" + f.read())\n\n\n@create_component_from_func\ndef consume_string_as_file(data_path: InputPath(str)):\n    with open(data_path) as f:\n        print(\"consume_string_as_file: \" + f.read())\n\n\n# Pipeline\n@kfp.dsl.pipeline(name='data_passing_pipeline')\ndef data_passing_pipeline(\n    anything_param=\"anything_param\",\n    something_param: \"Something\" = \"something_param\",\n    string_param: str = \"string_param\",\n):\n    produced_anything = produce_anything().output\n    produced_something = produce_something().output\n    produced_string = produce_string().output\n\n    # Pass constant value; consume as value\n    consume_anything_as_value(\"constant\")\n    consume_something_as_value(\"constant\")\n    consume_string_as_value(\"constant\")\n\n    # Pass constant value; consume as file\n    consume_anything_as_file(\"constant\")\n    consume_something_as_file(\"constant\")\n    consume_string_as_file(\"constant\")\n\n    # Pass pipeline parameter; consume as value\n    consume_anything_as_value(anything_param)\n    consume_anything_as_value(something_param)\n    consume_anything_as_value(string_param)\n    consume_something_as_value(anything_param)\n    consume_something_as_value(something_param)\n    consume_string_as_value(anything_param)\n    consume_string_as_value(string_param)\n\n    # Pass pipeline parameter; consume as file\n    consume_anything_as_file(anything_param)\n    consume_anything_as_file(something_param)\n    consume_anything_as_file(string_param)\n    consume_something_as_file(anything_param)\n    consume_something_as_file(something_param)\n    consume_string_as_file(anything_param)\n    consume_string_as_file(string_param)\n\n    # Pass task output; consume as value\n    consume_anything_as_value(produced_anything)\n    consume_anything_as_value(produced_something)\n    consume_anything_as_value(produced_string)\n    consume_something_as_value(produced_anything)\n    consume_something_as_value(produced_something)\n    consume_string_as_value(produced_anything)\n    consume_string_as_value(produced_string)\n\n    # Pass task output; consume as file\n    consume_anything_as_file(produced_anything)\n    consume_anything_as_file(produced_something)\n    consume_anything_as_file(produced_string)\n    consume_something_as_file(produced_anything)\n    consume_something_as_file(produced_something)\n    consume_string_as_file(produced_anything)\n    consume_string_as_file(produced_string)\n\n\nif __name__ == \"__main__\":\n    kfp_endpoint = None\n    kfp.Client(host=kfp_endpoint).create_run_from_pipeline_func(\n        data_passing_pipeline, arguments={})\n"
  },
  {
    "repo": "akontaxakis/kubeflow_pipelines",
    "file_path": "samples/test/legacy_exit_handler.py",
    "raw_url": "https://raw.githubusercontent.com/akontaxakis/kubeflow_pipelines/master/samples/test/legacy_exit_handler.py",
    "content": "#!/usr/bin/env python3\n# Copyright 2019 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp.deprecated as kfp\nfrom kfp.deprecated import dsl, compiler\n\n\ndef gcs_download_op(url):\n    return dsl.ContainerOp(\n        name='GCS - Download',\n        image='google/cloud-sdk:279.0.0',\n        command=['sh', '-ce'],\n        arguments=['gsutil cp $0 $1 && cat $1', url, '/tmp/results.txt'],\n        file_outputs={\n            'data': '/tmp/results.txt',\n        }\n    )\n\n\ndef echo_op(text):\n    return dsl.ContainerOp(\n        name='echo',\n        image='library/bash:4.4.23',\n        command=['sh', '-cex'],\n        arguments=['echo \"$0\"', text],\n    )\n\n\n@dsl.pipeline(\n    name='Exit Handler',\n    description=\n    'Downloads a message and prints it. The exit handler will run after the pipeline finishes (successfully or not).'\n)\ndef download_and_print(url='gs://ml-pipeline/shakespeare1.txt'):\n    \"\"\"A sample pipeline showing exit handler.\"\"\"\n\n    exit_task = echo_op('exit!')\n\n    with dsl.ExitHandler(exit_task):\n        download_task = gcs_download_op(url)\n        echo_task = echo_op(download_task.output)\n\n\nif __name__ == '__main__':\n    compiler.Compiler().compile(download_and_print, __file__ + '.yaml')\n"
  },
  {
    "repo": "akontaxakis/kubeflow_pipelines",
    "file_path": "samples/test/legacy_exit_handler_test.py",
    "raw_url": "https://raw.githubusercontent.com/akontaxakis/kubeflow_pipelines/master/samples/test/legacy_exit_handler_test.py",
    "content": "#!/usr/bin/env python3\n# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp.deprecated as kfp\nfrom .legacy_exit_handler import download_and_print\nfrom kfp.samples.test.utils import run_pipeline_func, TestCase\n\nrun_pipeline_func([\n    # This sample is expected to not work on v2 compatible mode.\n    TestCase(\n        pipeline_func=download_and_print,\n        mode=kfp.dsl.PipelineExecutionMode.V1_LEGACY)\n])\n"
  },
  {
    "repo": "akontaxakis/kubeflow_pipelines",
    "file_path": "samples/test/lightweight_python_functions_v2_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/akontaxakis/kubeflow_pipelines/master/samples/test/lightweight_python_functions_v2_pipeline.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Sample pipeline for passing data in KFP v2.\"\"\"\nfrom typing import Dict, List\nimport os\n\nfrom kfp import dsl\nfrom kfp.dsl import Input, InputPath, Output, OutputPath, Dataset, Model, component\nimport kfp.compiler as compiler\n\n# In tests, we install a KFP package from the PR under test. Users should not\n# normally need to specify `kfp_package_path` in their component definitions.\n_KFP_PACKAGE_PATH = os.getenv('KFP_PACKAGE_PATH')\n\n\n@component(kfp_package_path=_KFP_PACKAGE_PATH)\ndef preprocess(\n    # An input parameter of type string.\n    message: str,\n    # Use Output[T] to get a metadata-rich handle to the output artifact\n    # of type `Dataset`.\n    output_dataset_one: Output[Dataset],\n    # A locally accessible filepath for another output artifact of type\n    # `Dataset`.\n    output_dataset_two_path: OutputPath('Dataset'),\n    # A locally accessible filepath for an output parameter of type string.\n    output_parameter_path: OutputPath(str),\n    # A locally accessible filepath for an output parameter of type bool.\n    output_bool_parameter_path: OutputPath(bool),\n    # A locally accessible filepath for an output parameter of type dict.\n    output_dict_parameter_path: OutputPath(Dict[str, int]),\n    # A locally accessible filepath for an output parameter of type list.\n    output_list_parameter_path: OutputPath(List[str]),\n    # An input message that defaults to empty.\n    empty_message: str = \"\",\n):\n    \"\"\"Dummy preprocessing step\"\"\"\n\n    # Use Dataset.path to access a local file path for writing.\n    # One can also use Dataset.uri to access the actual URI file path.\n    with open(output_dataset_one.path, 'w') as f:\n        f.write(message)\n\n    # OutputPath is used to just pass the local file path of the output artifact\n    # to the function.\n    with open(output_dataset_two_path, 'w') as f:\n        f.write(message)\n\n    with open(output_parameter_path, 'w') as f:\n        f.write(message)\n\n    with open(output_bool_parameter_path, 'w') as f:\n        f.write(\n            str(True)\n        )  # use either `str()` or `json.dumps()` for bool values.\n\n    import json\n    with open(output_dict_parameter_path, 'w') as f:\n        f.write(json.dumps({'A': 1, 'B': 2}))\n\n    with open(output_list_parameter_path, 'w') as f:\n        f.write(json.dumps(['a', 'b', 'c']))\n\n\n@component(kfp_package_path=_KFP_PACKAGE_PATH)\ndef train(\n    # Use InputPath to get a locally accessible path for the input artifact\n    # of type `Dataset`.\n    dataset_one_path: InputPath('Dataset'),\n    # Use Input[T] to get a metadata-rich handle to the input artifact\n    # of type `Dataset`.\n    dataset_two: Input[Dataset],\n    # An input parameter of type string.\n    message: str,\n    # Use Output[T] to get a metadata-rich handle to the output artifact\n    # of type `Dataset`.\n    model: Output[Model],\n    # An input parameter of type bool.\n    input_bool: bool,\n    # An input parameter of type dict.\n    input_dict: Dict[str, int],\n    # An input parameter of type List[str].\n    input_list: List[str],\n    # An input parameter of type int with a default value.\n    num_steps: int = 100,\n):\n    \"\"\"Dummy Training step\"\"\"\n    with open(dataset_one_path, 'r') as input_file:\n        dataset_one_contents = input_file.read()\n\n    with open(dataset_two.path, 'r') as input_file:\n        dataset_two_contents = input_file.read()\n\n    line = (\n        f'dataset_one_contents: {dataset_one_contents} || '\n        f'dataset_two_contents: {dataset_two_contents} || '\n        f'message: {message} || '\n        f'input_bool: {input_bool}, type {type(input_bool)} || '\n        f'input_dict: {input_dict}, type {type(input_dict)} || '\n        f'input_list: {input_list}, type {type(input_list)} \\n'\n    )\n\n    with open(model.path, 'w') as output_file:\n        for i in range(num_steps):\n            output_file.write('Step {}\\n{}\\n=====\\n'.format(i, line))\n\n    # model is an instance of Model artifact, which has a .metadata dictionary\n    # to store arbitrary metadata for the output artifact.\n    model.metadata['accuracy'] = 0.9\n\n\n@dsl.pipeline(name='my-test-pipeline-beta')\ndef pipeline(message: str = 'message'):\n    preprocess_task = preprocess(message=message)\n    train_task = train(\n        dataset_one_path=preprocess_task.outputs['output_dataset_one'],\n        dataset_two=preprocess_task.outputs['output_dataset_two_path'],\n        message=preprocess_task.outputs['output_parameter_path'],\n        input_bool=preprocess_task.outputs['output_bool_parameter_path'],\n        input_dict=preprocess_task.outputs['output_dict_parameter_path'],\n        input_list=preprocess_task.outputs['output_list_parameter_path'],\n    )\n\n\nif __name__ == '__main__':\n    compiler.Compiler().compile(\n        pipeline_func=pipeline, package_path=__file__.replace('.py', '.yaml')\n    )\n"
  },
  {
    "repo": "gerardfarm/Kubeflow-Pipelines",
    "file_path": "mnist.py",
    "raw_url": "https://raw.githubusercontent.com/gerardfarm/Kubeflow-Pipelines/main/mnist.py",
    "content": "\n\n\"\"\" To run this pipeline, put into your terminal:\n        dsl-compile --py utils.py --output pipeline.yaml\n\"\"\"\n\ndef unzip_func(bucket_name, zip_data_path_in_s3='mnist.zip',\n                            unzip_data_path_in_s3='dest/',\n                            downloaded_data_path_out='data/mnist.zip',\n                            unzip_downloaded_data_path='./unzipped_data',\n                            AWS_REGION='us-east-1'):\n    \"\"\" \n    Download zip data from s3, extract and then re-upload it to S3\n    Parameters:\n        - bucket_name : str, name of the bucket\n        - zip_data_path_in_s3: str, complete path of zip data on S3\n        - unzip_data_path_in_s3: str, path where you want to extract your data on S3\n        - downloaded_data_path_out: str, path where data is downloaded on PC\n        - unzip_downloaded_data_path: str, path to extract data\n    \"\"\"\n\n    # It is mandotory to put necessary libraries here\n    import os\n    import boto3\n    import zipfile\n\n    os.makedirs(\"data\", exist_ok=True)\n    os.makedirs(\"unzipped_data\", exist_ok=True)\n\n    # Access S3\n    conn_s3 = boto3.client('s3', region_name=AWS_REGION)\n    s3 = boto3.resource('s3', region_name=AWS_REGION)\n    my_bucket = s3.Bucket(bucket_name)\n\n    # Download data on your PC\n    obj = my_bucket.Object(zip_data_path_in_s3)\n    obj.download_file(Filename=downloaded_data_path_out)\n    \n    # Unzip downloaded data\n    with zipfile.ZipFile(downloaded_data_path_out, 'r') as file:\n        file.extractall(unzip_downloaded_data_path)\n            \n    # Upload unzipped data to your S3 Storage Bucket\n    for file in os.listdir(unzip_downloaded_data_path):\n        output_path = unzip_data_path_in_s3 + file\n        conn_s3.upload_file(os.path.join(unzip_downloaded_data_path, file), bucket_name, output_path)\n\n#unzip_func(bucket_name='ali-bucket-gerard')\n\nimport kfp\nimport boto3\n\nAWS_REGION='us-east-1'\nAWS_ACCOUNT_ID = boto3.client('sts').get_caller_identity().get('Account')\n\nREPO_NAME = 'hello-repository'\nBUCKET_NAME = 'ali-bucket-gerard'\nDOCKER_REGISTRY = '{}.dkr.ecr.{}.amazonaws.com/{}:latest'.format(AWS_ACCOUNT_ID, \n                                                            AWS_REGION, \n                                                            REPO_NAME)\n\nunzip_files_op = kfp.components.create_component_from_func(unzip_func, base_image=DOCKER_REGISTRY) \n\n@kfp.dsl.pipeline(\n    name='testing-s3-in-pipeline',\n    description='dowload zip mnist data and re-upload it on s3.'\n)\ndef unzip_and_read_pipeline(BUCKET_NAME='ali-bucket-gerard'):  \n    # Call the first OP\n    first_task = unzip_files_op(BUCKET_NAME)\n\nif __name__ == \"__main__\":\n    # execute only if run as a script\n    kfp.compiler.Compiler().compile(\n        pipeline_func=unzip_and_read_pipeline,\n        package_path='test-s3-pipeline.yaml')"
  },
  {
    "repo": "gerardfarm/Kubeflow-Pipelines",
    "file_path": "examples/hello_world.py",
    "raw_url": "https://raw.githubusercontent.com/gerardfarm/Kubeflow-Pipelines/main/examples/hello_world.py",
    "content": "import kfp\nimport boto3\n\n\nAWS_REGION='region-name'\nAWS_ACCOUNT_ID = boto3.client('sts').get_caller_identity().get('Account')\n\nREPO_NAME = 'repository-name'\nBUCKET_NAME = 'bucket-name'\nDOCKER_REGISTRY = '{}.dkr.ecr.{}.amazonaws.com/{}:latest'.format(AWS_ACCOUNT_ID, \n                                                            AWS_REGION, \n                                                            REPO_NAME)\n\ndef hello_world(text: str) -> str:\n    print(text)\n    return text\n\nhello_task = kfp.components.create_component_from_func(hello_world, \n                                                        base_image=DOCKER_REGISTRY) \n\n\n@kfp.dsl.pipeline(name='hello-world', description='A simple intro pipeline')\ndef pipeline_hello_world(text: str = 'hi there'):\n    \"\"\"Pipeline that passes small pipeline parameter string to consumer op.\"\"\"\n\n    consume_task = hello_task(text)\n\nif __name__ == \"__main__\":\n    # execute only if run as a script\n    kfp.compiler.Compiler().compile(\n        pipeline_func=pipeline_hello_world,\n        package_path='pipeline.yaml')\n"
  },
  {
    "repo": "gerardfarm/Kubeflow-Pipelines",
    "file_path": "examples/mnist.py",
    "raw_url": "https://raw.githubusercontent.com/gerardfarm/Kubeflow-Pipelines/main/examples/mnist.py",
    "content": "\n\n\"\"\" To run this pipeline, put into your terminal:\n        dsl-compile --py utils.py --output pipeline.yaml\n\"\"\"\n\ndef unzip_func(bucket_name, zip_data_path_in_s3='mnist.zip',\n                            unzip_data_path_in_s3='dest/',\n                            downloaded_data_path_out='data/mnist.zip',\n                            unzip_downloaded_data_path='./unzipped_data',\n                            AWS_REGION='region-name'):\n    \"\"\" \n    Download zip data from s3, extract and then re-upload it to S3\n    Parameters:\n        - bucket_name : str, name of the bucket\n        - zip_data_path_in_s3: str, complete path of zip data on S3\n        - unzip_data_path_in_s3: str, path where you want to extract your data on S3\n        - downloaded_data_path_out: str, path where data is downloaded on PC\n        - unzip_downloaded_data_path: str, path to extract data\n    \"\"\"\n\n    # It is mandotory to put necessary libraries here\n    import os\n    import boto3\n    import zipfile\n\n    os.makedirs(\"data\", exist_ok=True)\n    os.makedirs(\"unzipped_data\", exist_ok=True)\n\n    # Access S3\n    conn_s3 = boto3.client('s3', region_name=AWS_REGION)\n    s3 = boto3.resource('s3', region_name=AWS_REGION)\n    my_bucket = s3.Bucket(bucket_name)\n\n    # Download data on your PC\n    obj = my_bucket.Object(zip_data_path_in_s3)\n    obj.download_file(Filename=downloaded_data_path_out)\n    \n    # Unzip downloaded data\n    with zipfile.ZipFile(downloaded_data_path_out, 'r') as file:\n        file.extractall(unzip_downloaded_data_path)\n            \n    # Upload unzipped data to your S3 Storage Bucket\n    for file in os.listdir(unzip_downloaded_data_path):\n        output_path = unzip_data_path_in_s3 + file\n        conn_s3.upload_file(os.path.join(unzip_downloaded_data_path, file), bucket_name, output_path)\n\n\nimport kfp\nimport boto3\n\nAWS_REGION='region-name'\nAWS_ACCOUNT_ID = boto3.client('sts').get_caller_identity().get('Account')\n\nREPO_NAME = 'repository-name'\nBUCKET_NAME = 'bucket-name'\nDOCKER_REGISTRY = '{}.dkr.ecr.{}.amazonaws.com/{}:latest'.format(AWS_ACCOUNT_ID, \n                                                            AWS_REGION, \n                                                            REPO_NAME)\n\nunzip_files_op = kfp.components.create_component_from_func(unzip_func, base_image=DOCKER_REGISTRY) \n\n@kfp.dsl.pipeline(\n    name='testing-s3-in-pipeline',\n    description='dowload zip mnist data and re-upload it on s3.'\n)\ndef unzip_and_read_pipeline(BUCKET_NAME):  \n    # Call the first OP\n    first_task = unzip_files_op(BUCKET_NAME)\n\nif __name__ == \"__main__\":\n    # execute only if run as a script\n    kfp.compiler.Compiler().compile(\n        pipeline_func=unzip_and_read_pipeline,\n        package_path='test-s3-pipeline.yaml')"
  },
  {
    "repo": "gerardfarm/Kubeflow-Pipelines",
    "file_path": "examples/math_operations/main.py",
    "raw_url": "https://raw.githubusercontent.com/gerardfarm/Kubeflow-Pipelines/main/examples/math_operations/main.py",
    "content": "import kfp\nimport kfp.dsl as dsl\n\nfrom adding import add_op, divmod_op\n\n@dsl.pipeline(\n  name='Addition pipeline',\n  description='An example pipeline that performs addition calculations.'\n)\ndef add_pipeline(\n  a='1',\n  b='7',\n  c='3'\n):\n  # Passes a pipeline parameter and a constant value to the `add_op` factory\n  # function.\n  first_task = add_op(a, 4)\n  # Passes an output reference from `first_add_task` and a pipeline parameter\n  # to the `add_op` factory function. For operations with a single return\n  # value, the output reference can be accessed as `task.output` or\n  # `task.outputs['output_name']`.\n  second_task = add_op(first_task.output, b)\n\n  third_task = divmod_op(first_task.output, \n                              second_task.output)\n\n  result_task = add_op(third_task.outputs['quotient'], c)\n\nif __name__ == \"__main__\":\n    # execute only if run as a script\n    kfp.compiler.Compiler().compile(\n        pipeline_func=add_pipeline,\n        package_path='add_div_pipeline.yaml')"
  },
  {
    "repo": "gerardfarm/Kubeflow-Pipelines",
    "file_path": "examples/object-detection/main.py",
    "raw_url": "https://raw.githubusercontent.com/gerardfarm/Kubeflow-Pipelines/main/examples/object-detection/main.py",
    "content": "import kfp\nimport boto3\n\nfrom typing import NamedTuple\n\n\n# ================================================================\n#                         Upload dataset to S3\n# ================================================================\ndef Preprocessing() -> NamedTuple('My_Output',[('feedback', str)]):\n    \n    # You should upload your dataset to s3\n    \n    # import os\n    # import boto3\n\n    # conn_s3 = boto3.client('s3', region_name=AWS_REGION)\n    \n    # # Images names list\n    # filenames = os.listdir(data_path)\n\n    # # Upload all images to s3\n    # for filename in filenames:\n    #     conn_s3.upload_file(os.path.join(data_path, filename), \n    #                         bucket_name, \n    #                         os.path.join(output_path, filename))\n\n    from collections import namedtuple\n    feedback_msg = 'Done! Data are on S3.'\n    func_output = namedtuple('MyOutput', ['feedback'])\n    return func_output(feedback_msg)\n\n# ================================================================\n#             Object Detection Evaluation on CoCo dataset\n# ================================================================\ndef test_object_detection(msg, bucket_name, AWS_REGION):\n\n    import os\n    import boto3\n    import subprocess\n\n    subprocess.call(\"python3 test.py --data coco128.yaml --weights yolov5s.pt --img 640 --batch-size 2\", shell=True)\n\n    print(msg)\n    #print(os.listdir('runs/test/exp'))\n    \n    # Upload results to s3\n    conn_s3 = boto3.client('s3', region_name=AWS_REGION)\n    output_path = 'runs/test/exp/'\n\n    for filename in os.listdir(output_path):\n        path = os.path.join(output_path, filename)\n        conn_s3.upload_file(path, bucket_name, os.path.join(\"object-detection/results/\", filename))\n\n\n\n# Define registry\nAWS_REGION='region-name'\nAWS_ACCOUNT_ID = boto3.client('sts').get_caller_identity().get('Account')\n\nREPO_NAME = 'object-detection'\nBUCKET_NAME = 'bucket-name'\nTAG_NAME = 'latest'\nDOCKER_REGISTRY = '{}.dkr.ecr.{}.amazonaws.com/{}:{}'.format(\n                                                    AWS_ACCOUNT_ID, \n                                                    AWS_REGION, \n                                                    REPO_NAME,\n                                                    TAG_NAME\n                                                )\n\n# Create components\npreprocess_task = kfp.components.create_component_from_func(Preprocessing, \n                                                    base_image=DOCKER_REGISTRY,\n                                                  #  output_component_file = 'preprocessing.yaml'\n                                                )\n\nmain_task = kfp.components.create_component_from_func(test_object_detection, \n                                                    base_image=DOCKER_REGISTRY,\n                                                  #  output_component_file = 'preprocessing.yaml'\n                                                )\n\n# Create pipeline\n@kfp.dsl.pipeline(\n    name='Object Detection Algorithm', \n    description='Testing YOLOv5 on few images.'\n)\ndef object_detection_pipeline(bucket_name: str = BUCKET_NAME,\n                                AWS_REGION: str = 'us-east-1'\n                                ):\n\n    # Upload your dataset to s3\n    first_task = preprocess_task()\n    second_task = main_task(first_task.outputs['feedback'], bucket_name, AWS_REGION)\n\nif __name__ == \"__main__\":\n    # execute only if run as a script\n    kfp.compiler.Compiler().compile(\n        pipeline_func=object_detection_pipeline,\n        package_path='full-pipeline-object-detection.yaml')\n"
  },
  {
    "repo": "kikuriyou/Kubeflow-pipelines",
    "file_path": "lda/kfp_topic_test.py",
    "raw_url": "https://raw.githubusercontent.com/kikuriyou/Kubeflow-pipelines/master/lda/kfp_topic_test.py",
    "content": "#!/usr/bin/env python3\n\nfrom datetime import datetime, date, timedelta\nimport kfp.dsl as dsl\nimport kfp.compiler as compiler\n\nPROJECT_ID = 'project_id'\nBUCKET = 'bucket'\n\ndef preprocess_op(project: 'GcpProject', bucket, date, dict_file, dataset_file, tmp_dir,\n                  preprocess_output: 'GcsUri[Directory]', step_name='preprocess'):\n    return dsl.ContainerOp(\n        name = step_name,\n        image = 'gcr.io/{}/kfp/pre:latest'.format(PROJECT_ID),\n        arguments = [\n            '--project',      project,\n            '--bucket',       bucket,\n            '--date',         date,\n            '--dict_file',    dict_file,\n            '--dataset_file', dataset_file,\n            '--tmp_dir',      tmp_dir,\n            '--output',       preprocess_output\n        ],\n        file_outputs = {'preprocess': '/output.txt'}\n    )\n\n\ndef training_op(preprocess_output: 'GcsUri[Directory]', project: 'GcpProject', bucket, table, \n                prev_date, date, dict_file, dataset_file, learning_type, pipeline_version, tmp_dir, \n                training_output: 'GcsUri[Directory]', step_name='train'):\n    return dsl.ContainerOp(\n        name = step_name,\n        image = 'gcr.io/{}/kfp/train:latest'.format(PROJECT_ID,\n        arguments = [\n            '--preprocess_output', preprocess_output,\n            '--project',           project,\n            '--bucket',            bucket,\n            '--table',             table,\n            '--prev_date',         prev_date,\n            '--date',              date,\n            '--dict_file',         dict_file,\n            '--dataset_file',      dataset_file,\n            '--learning_type',     learning_type,\n            '--pipeline_version',  pipeline_version,\n            '--tmp_dir',           tmp_dir,\n            '--output',            training_output\n        ],\n        file_outputs = {'train': '/output.txt'}\n    )\n\n\ndef postprocess_op(training_output: 'GcsUri[Directory]', project: 'GcpProject', bucket, table, date, \n                   postprocess_output: 'GcsUri[Directory]', step_name='postprocess'):\n    return dsl.ContainerOp(\n        name = step_name,\n        image = 'gcr.io/{}/kfp/post:latest'.format(PROJECT_ID,\n        arguments = [\n            '--training_output', training_output,\n            '--project',         project,\n            '--bucket',          bucket,\n            '--table',           table,\n            '--date',            date,\n            '--output',          postprocess_output\n        ],\n        file_outputs = {'postprocess': '/output.txt'}\n    )\n\n\n@dsl.pipeline(\n    name='LDA pipeline',\n    description='LDA pipeline running on every Wednesday'\n)\ndef kubeflow_training(\n    output:        dsl.PipelineParam, \n    project:       dsl.PipelineParam,\n    bucket:        dsl.PipelineParam=dsl.PipelineParam(name='bucket',          value=bucket),\n    table:         dsl.PipelineParam=dsl.PipelineParam(name='table',           value='TOPIC_TRY'),\n    prev_date:     dsl.PipelineParam=dsl.PipelineParam(name='prev-date',       value=''),\n    date:          dsl.PipelineParam=dsl.PipelineParam(name='date',            value=''),\n    dict_file:     dsl.PipelineParam=dsl.PipelineParam(name='dictionary-file', value='dict'),\n    dataset_file:  dsl.PipelineParam=dsl.PipelineParam(name='dataset-file',    value='dataset'),\n    learning_type: dsl.PipelineParam=dsl.PipelineParam(name='learning-type',   value='update')):\n\n\n    # TODO: use the argo job name as the workflow\n    #workflow = '{{workflow.name}}'\n    pipeline_version = __file__\n\n    # Make pipeline\n    preprocess = preprocess_op(project, bucket, date, dict_file, dataset_file, '/tmp', output)\n    training = training_op(preprocess.output, project, bucket, table, prev_date, date, \n                           dict_file, dataset_file, learning_type, pipeline_version, '/tmp', output)\n    postprocess = postprocess_op(training.output, project, bucket, table, date, output)\n\n\nif __name__ == '__main__':\n    compiler.Compiler().compile(kubeflow_training, __file__ + '.tar.gz')\n\n"
  },
  {
    "repo": "terus-lim-df/kubeflow-pipeline",
    "file_path": "sample1__embulk_dag.py",
    "raw_url": "https://raw.githubusercontent.com/terus-lim-df/kubeflow-pipeline/main/sample1__embulk_dag.py",
    "content": "#!/usr/bin/env python3\n\nimport kfp\nfrom kfp.components import func_to_container_op, InputPath, OutputPath\n\n\nread_csv = kfp.components.load_component_from_url(\"https://raw.githubusercontent.com/terus-lim-df/kubeflow-pipeline/main/sample1__embulk_component.yaml\")\n\n\n@kfp.dsl.pipeline(\n  name='My pipeline',\n  description='My machine learning pipeline'\n)\ndef file_passing_pipelines(file_path):\n    \"\"\"Combining all pipelines together in a single pipeline\"\"\"\n    read_csv(file_path)\n\n\nif __name__ == '__main__':\n    # Compiling the pipeline\n    kfp.compiler.Compiler().compile(file_passing_pipelines, \"dags/\" + __file__.split(\"/\")[-1] + '.yaml')\n"
  },
  {
    "repo": "SWE-Gym-Raw/kubeflow__pipelines",
    "file_path": "samples/test/after.py",
    "raw_url": "https://raw.githubusercontent.com/SWE-Gym-Raw/kubeflow__pipelines/0.1.8/samples/test/after.py",
    "content": "# Copyright 2020 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom kfp import components\nfrom kfp.v2 import dsl\nimport kfp.v2.compiler as compiler\n\ncomponent_op = components.load_component_from_text(\n    \"\"\"\nname: Print Text\ninputs:\n- {name: text, type: String}\nimplementation:\n  container:\n    image: alpine\n    command:\n    - sh\n    - -c\n    - |\n      set -e -x\n      echo \"$0\"\n    - {inputValue: text}\n\"\"\"\n)\n\n\n@dsl.pipeline(name='pipeline-with-after')\ndef my_pipeline():\n    task1 = component_op(text='1st task')\n    task2 = component_op(text='2nd task').after(task1)\n    task3 = component_op(text='3rd task').after(task1, task2)\n\n\nif __name__ == '__main__':\n    compiler.Compiler().compile(\n        pipeline_func=my_pipeline,\n        pipeline_root='dummy_root',\n        output_path=__file__ + '.json'\n    )\n"
  },
  {
    "repo": "SWE-Gym-Raw/kubeflow__pipelines",
    "file_path": "samples/test/after_test.py",
    "raw_url": "https://raw.githubusercontent.com/SWE-Gym-Raw/kubeflow__pipelines/0.1.8/samples/test/after_test.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp\nfrom .after import my_pipeline\nfrom .util import run_pipeline_func, TestCase\n\nrun_pipeline_func([\n    TestCase(pipeline_func=my_pipeline),\n    TestCase(\n        pipeline_func=my_pipeline, mode=kfp.dsl.PipelineExecutionMode.V2_ENGINE\n    ),\n])\n"
  },
  {
    "repo": "SWE-Gym-Raw/kubeflow__pipelines",
    "file_path": "samples/test/cache_v2_test.py",
    "raw_url": "https://raw.githubusercontent.com/SWE-Gym-Raw/kubeflow__pipelines/0.1.8/samples/test/cache_v2_test.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Two step v2-compatible pipeline.\"\"\"\n\n# %%\n\nfrom __future__ import annotations\n\nimport random\nimport string\nimport unittest\nimport functools\n\nimport kfp\nimport kfp_server_api\n\nfrom .two_step import two_step_pipeline\nfrom .util import run_pipeline_func, TestCase, KfpMlmdClient, KfpTask\nfrom ml_metadata.proto import Execution\n\n\ndef verify_tasks(\n    t: unittest.TestCase, tasks: dict[str, KfpTask], task_state, uri: str,\n    some_int: int\n):\n    task_names = [*tasks.keys()]\n    t.assertCountEqual(task_names, ['train-op', 'preprocess'], 'task names')\n\n    preprocess = tasks['preprocess']\n    train = tasks['train-op']\n\n    t.assertEqual({\n        'name': 'preprocess',\n        'inputs': {\n            'artifacts': [],\n            'parameters': {\n                'some_int': some_int,\n                'uri': uri\n            }\n        },\n        'outputs': {\n            'artifacts': [{\n                'metadata': {\n                    'display_name': 'output_dataset_one',\n                },\n                'name': 'output_dataset_one',\n                'type': 'system.Dataset'\n            }],\n            'parameters': {\n                'output_parameter_one': some_int\n            }\n        },\n        'type': 'system.ContainerExecution',\n        'state': task_state,\n    }, preprocess.get_dict())\n    t.assertEqual({\n        'name': 'train-op',\n        'inputs': {\n            'artifacts': [{\n                'metadata': {\n                    'display_name': 'output_dataset_one',\n                },\n                'name': 'dataset',\n                'type': 'system.Dataset',\n            }],\n            'parameters': {\n                'num_steps': some_int\n            }\n        },\n        'outputs': {\n            'artifacts': [{\n                'metadata': {\n                    'display_name': 'model',\n                },\n                'name': 'model',\n                'type': 'system.Model',\n            }],\n            'parameters': {}\n        },\n        'type': 'system.ContainerExecution',\n        'state': task_state,\n    }, train.get_dict())\n\n\ndef verify(\n    run: kfp_server_api.ApiRun, mlmd_connection_config, uri: str, some_int,\n    state: int, **kwargs\n):\n    t = unittest.TestCase()\n    t.maxDiff = None  # we always want to see full diff\n    t.assertEqual(run.status, 'Succeeded')\n    client = KfpMlmdClient(mlmd_connection_config=mlmd_connection_config)\n    tasks = client.get_tasks(run_id=run.id)\n    verify_tasks(t, tasks, state, uri, some_int)\n\n\nif __name__ == '__main__':\n    letters = string.ascii_lowercase\n    random_uri = 'http://' + ''.join(random.choice(letters) for i in range(5))\n    random_int = random.randint(0, 10000)\n    run_pipeline_func([\n        TestCase(\n            pipeline_func=two_step_pipeline,\n            arguments={\n                'uri': f'{random_uri}',\n                'some_int': f'{random_int}'\n            },\n            verify_func=functools.partial(\n                verify,\n                uri=random_uri,\n                some_int=random_int,\n                state=Execution.State.COMPLETE,\n            ),\n            mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE,\n            enable_caching=True\n        ),\n    ]),\n    run_pipeline_func([\n        TestCase(\n            pipeline_func=two_step_pipeline,\n            arguments={\n                'uri': f'{random_uri}',\n                'some_int': f'{random_int}'\n            },\n            verify_func=functools.partial(\n                verify,\n                uri=random_uri,\n                some_int=random_int,\n                state=Execution.State.CACHED\n            ),\n            mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE,\n            enable_caching=True\n        ),\n    ])\n\n# %%\n"
  },
  {
    "repo": "SWE-Gym-Raw/kubeflow__pipelines",
    "file_path": "samples/test/fail.py",
    "raw_url": "https://raw.githubusercontent.com/SWE-Gym-Raw/kubeflow__pipelines/0.1.8/samples/test/fail.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Fail pipeline.\"\"\"\n\nfrom kfp import components, dsl\n\n\ndef fail():\n    '''Fails'''\n    import sys\n    sys.exit(1)\n\n\nfail_op = components.create_component_from_func(\n    fail, base_image='alpine:latest'\n)\n\n\n@dsl.pipeline(name='fail_pipeline')\ndef fail_pipeline():\n    fail_task = fail_op()\n"
  },
  {
    "repo": "SWE-Gym-Raw/kubeflow__pipelines",
    "file_path": "samples/test/fail_parameter_value_missing.py",
    "raw_url": "https://raw.githubusercontent.com/SWE-Gym-Raw/kubeflow__pipelines/0.1.8/samples/test/fail_parameter_value_missing.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom kfp import dsl, components\n\necho = components.load_component_from_text(\n    \"\"\"\nname: Echo\ninputs:\n- {name: text, type: String}\nimplementation:\n  container:\n    image: alpine\n    command:\n    - echo\n    - {inputValue: text}\n\"\"\"\n)\n\n\n@dsl.pipeline(name='parameter_value_missing')\ndef pipeline(\n    parameter:\n    str  # parameter should be specified when submitting, but we are missing it in the test\n):\n    echo_op = echo(text=parameter)\n"
  },
  {
    "repo": "SWE-Gym-Raw/kubeflow__pipelines",
    "file_path": "samples/test/legacy_data_passing.py",
    "raw_url": "https://raw.githubusercontent.com/SWE-Gym-Raw/kubeflow__pipelines/0.1.8/samples/test/legacy_data_passing.py",
    "content": "# This pipeline demonstrates and verifies all ways of data passing supported by KFP.\n\n# %% [markdown]\n# KFP has simple data passing model. There are three different kinds of arguments and tw different ways to consume an input argument. All combinations are supported.\n\n# Any input can be consumed:\n# * As value (`inputValue` placeholder)\n# * As file (`inputPath` placeholder).\n\n# Input argument can come from:\n# * Constant value\n# * Pipeline parameter\n# * Upstream component output\n\n# Combining these options there are 6 end-to-end data passing cases, each of which works regardless of type:\n\n# 1. Pass constant value which gets consumed as value. (Common)\n# 2. Pass constant value which gets consumed as file. (Often used in test pipelines)\n# 3. Pass pipeline parameter that gets consumed as value. (Common)\n# 4. Pass pipeline parameter that gets consumed as file. (Rare. Sometimes used with JSON parameters)\n# 5. Pass task output that gets consumed as value. (Common)\n# 6. Pass task output that gets consumed as file. (Common)\n\n# The only restriction on types is that when both upstream output and downstream input have types, the types must match.\n\n# %%\nimport kfp\nfrom kfp.components import create_component_from_func, InputPath, OutputPath\n\n\n# Components\n# Produce\n@create_component_from_func\ndef produce_anything(data_path: OutputPath()):\n    with open(data_path, \"w\") as f:\n        f.write(\"produce_anything\")\n\n\n@create_component_from_func\ndef produce_something(data_path: OutputPath(\"Something\")):\n    with open(data_path, \"w\") as f:\n        f.write(\"produce_something\")\n\n\n@create_component_from_func\ndef produce_something2() -> 'Something':\n    return \"produce_something2\"\n\n\n@create_component_from_func\ndef produce_string() -> str:\n    return \"produce_string\"\n\n\n# Consume as value\n@create_component_from_func\ndef consume_anything_as_value(data):\n    print(\"consume_anything_as_value: \" + data)\n\n\n@create_component_from_func\ndef consume_something_as_value(data: \"Something\"):\n    print(\"consume_something_as_value: \" + data)\n\n\n@create_component_from_func\ndef consume_string_as_value(data: str):\n    print(\"consume_string_as_value: \" + data)\n\n\n# Consume as file\n@create_component_from_func\ndef consume_anything_as_file(data_path: InputPath()):\n    with open(data_path) as f:\n        print(\"consume_anything_as_file: \" + f.read())\n\n\n@create_component_from_func\ndef consume_something_as_file(data_path: InputPath('Something')):\n    with open(data_path) as f:\n        print(\"consume_something_as_file: \" + f.read())\n\n\n@create_component_from_func\ndef consume_string_as_file(data_path: InputPath(str)):\n    with open(data_path) as f:\n        print(\"consume_string_as_file: \" + f.read())\n\n\n# Pipeline\n@kfp.dsl.pipeline(name='data_passing_pipeline')\ndef data_passing_pipeline(\n    anything_param=\"anything_param\",\n    something_param: \"Something\" = \"something_param\",\n    string_param: str = \"string_param\",\n):\n    produced_anything = produce_anything().output\n    produced_something = produce_something().output\n    produced_string = produce_string().output\n\n    # Pass constant value; consume as value\n    consume_anything_as_value(\"constant\")\n    consume_something_as_value(\"constant\")\n    consume_string_as_value(\"constant\")\n\n    # Pass constant value; consume as file\n    consume_anything_as_file(\"constant\")\n    consume_something_as_file(\"constant\")\n    consume_string_as_file(\"constant\")\n\n    # Pass pipeline parameter; consume as value\n    consume_anything_as_value(anything_param)\n    consume_anything_as_value(something_param)\n    consume_anything_as_value(string_param)\n    consume_something_as_value(anything_param)\n    consume_something_as_value(something_param)\n    consume_string_as_value(anything_param)\n    consume_string_as_value(string_param)\n\n    # Pass pipeline parameter; consume as file\n    consume_anything_as_file(anything_param)\n    consume_anything_as_file(something_param)\n    consume_anything_as_file(string_param)\n    consume_something_as_file(anything_param)\n    consume_something_as_file(something_param)\n    consume_string_as_file(anything_param)\n    consume_string_as_file(string_param)\n\n    # Pass task output; consume as value\n    consume_anything_as_value(produced_anything)\n    consume_anything_as_value(produced_something)\n    consume_anything_as_value(produced_string)\n    consume_something_as_value(produced_anything)\n    consume_something_as_value(produced_something)\n    consume_string_as_value(produced_anything)\n    consume_string_as_value(produced_string)\n\n    # Pass task output; consume as file\n    consume_anything_as_file(produced_anything)\n    consume_anything_as_file(produced_something)\n    consume_anything_as_file(produced_string)\n    consume_something_as_file(produced_anything)\n    consume_something_as_file(produced_something)\n    consume_string_as_file(produced_anything)\n    consume_string_as_file(produced_string)\n\n\nif __name__ == \"__main__\":\n    kfp_endpoint = None\n    kfp.Client(host=kfp_endpoint).create_run_from_pipeline_func(\n        data_passing_pipeline, arguments={})\n"
  },
  {
    "repo": "SWE-Gym-Raw/kubeflow__pipelines",
    "file_path": "samples/test/legacy_data_passing_test.py",
    "raw_url": "https://raw.githubusercontent.com/SWE-Gym-Raw/kubeflow__pipelines/0.1.8/samples/test/legacy_data_passing_test.py",
    "content": "from .data_passing import data_passing_pipeline\nfrom .util import run_pipeline_func, TestCase,\nfrom kfp.dsl import PipelineExecutionMode\n\nrun_pipeline_func([\n    TestCase(\n        pipeline_func=data_passing_pipeline,\n        mode=PipelineExecutionMode.V1_LEGACY,\n    )\n])\n"
  },
  {
    "repo": "SWE-Gym-Raw/kubeflow__pipelines",
    "file_path": "samples/test/legacy_exit_handler.py",
    "raw_url": "https://raw.githubusercontent.com/SWE-Gym-Raw/kubeflow__pipelines/0.1.8/samples/test/legacy_exit_handler.py",
    "content": "#!/usr/bin/env python3\n# Copyright 2019 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp\nfrom kfp import dsl\n\n\ndef gcs_download_op(url):\n    return dsl.ContainerOp(\n        name='GCS - Download',\n        image='google/cloud-sdk:279.0.0',\n        command=['sh', '-ce'],\n        arguments=['gsutil cp $0 $1 && cat $1', url, '/tmp/results.txt'],\n        file_outputs={\n            'data': '/tmp/results.txt',\n        }\n    )\n\n\ndef echo_op(text):\n    return dsl.ContainerOp(\n        name='echo',\n        image='library/bash:4.4.23',\n        command=['sh', '-cex'],\n        arguments=['echo \"$0\"', text],\n    )\n\n\n@dsl.pipeline(\n    name='Exit Handler',\n    description=\n    'Downloads a message and prints it. The exit handler will run after the pipeline finishes (successfully or not).'\n)\ndef download_and_print(url='gs://ml-pipeline/shakespeare1.txt'):\n    \"\"\"A sample pipeline showing exit handler.\"\"\"\n\n    exit_task = echo_op('exit!')\n\n    with dsl.ExitHandler(exit_task):\n        download_task = gcs_download_op(url)\n        echo_task = echo_op(download_task.output)\n\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(download_and_print, __file__ + '.yaml')\n"
  },
  {
    "repo": "SWE-Gym-Raw/kubeflow__pipelines",
    "file_path": "samples/test/legacy_exit_handler_test.py",
    "raw_url": "https://raw.githubusercontent.com/SWE-Gym-Raw/kubeflow__pipelines/0.1.8/samples/test/legacy_exit_handler_test.py",
    "content": "#!/usr/bin/env python3\n# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp\nfrom .legacy_exit_handler import download_and_print\nfrom .util import run_pipeline_func, TestCase\n\n\nrun_pipeline_func([\n    # This sample is expected to not work on v2 compatible mode.\n    TestCase(\n        pipeline_func=download_and_print,\n        mode=kfp.dsl.PipelineExecutionMode.V1_LEGACY\n    )\n])\n"
  },
  {
    "repo": "SWE-Gym-Raw/kubeflow__pipelines",
    "file_path": "samples/test/lightweight_python_functions_v2_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/SWE-Gym-Raw/kubeflow__pipelines/0.1.8/samples/test/lightweight_python_functions_v2_pipeline.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Sample pipeline for passing data in KFP v2.\"\"\"\nfrom typing import Dict, List\nimport os\n\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import Input, InputPath, Output, OutputPath, Dataset, Model, component\nimport kfp.v2.compiler as compiler\n\n# In tests, we install a KFP package from the PR under test. Users should not\n# normally need to specify `kfp_package_path` in their component definitions.\n_KFP_PACKAGE_PATH = os.getenv('KFP_PACKAGE_PATH')\n\n\n@component(kfp_package_path=_KFP_PACKAGE_PATH)\ndef preprocess(\n    # An input parameter of type string.\n    message: str,\n    # Use Output[T] to get a metadata-rich handle to the output artifact\n    # of type `Dataset`.\n    output_dataset_one: Output[Dataset],\n    # A locally accessible filepath for another output artifact of type\n    # `Dataset`.\n    output_dataset_two_path: OutputPath('Dataset'),\n    # A locally accessible filepath for an output parameter of type string.\n    output_parameter_path: OutputPath(str),\n    # A locally accessible filepath for an output parameter of type bool.\n    output_bool_parameter_path: OutputPath(bool),\n    # A locally accessible filepath for an output parameter of type dict.\n    output_dict_parameter_path: OutputPath(Dict[str, int]),\n    # A locally accessible filepath for an output parameter of type list.\n    output_list_parameter_path: OutputPath(List[str]),\n    # An input message that defaults to empty.\n    empty_message: str = \"\",\n):\n    \"\"\"Dummy preprocessing step\"\"\"\n\n    # Use Dataset.path to access a local file path for writing.\n    # One can also use Dataset.uri to access the actual URI file path.\n    with open(output_dataset_one.path, 'w') as f:\n        f.write(message)\n\n    # OutputPath is used to just pass the local file path of the output artifact\n    # to the function.\n    with open(output_dataset_two_path, 'w') as f:\n        f.write(message)\n\n    with open(output_parameter_path, 'w') as f:\n        f.write(message)\n\n    with open(output_bool_parameter_path, 'w') as f:\n        f.write(\n            str(True)\n        )  # use either `str()` or `json.dumps()` for bool values.\n\n    import json\n    with open(output_dict_parameter_path, 'w') as f:\n        f.write(json.dumps({'A': 1, 'B': 2}))\n\n    with open(output_list_parameter_path, 'w') as f:\n        f.write(json.dumps(['a', 'b', 'c']))\n\n\n@component(kfp_package_path=_KFP_PACKAGE_PATH)\ndef train(\n    # Use InputPath to get a locally accessible path for the input artifact\n    # of type `Dataset`.\n    dataset_one_path: InputPath('Dataset'),\n    # Use Input[T] to get a metadata-rich handle to the input artifact\n    # of type `Dataset`.\n    dataset_two: Input[Dataset],\n    # An input parameter of type string.\n    message: str,\n    # Use Output[T] to get a metadata-rich handle to the output artifact\n    # of type `Dataset`.\n    model: Output[Model],\n    # An input parameter of type bool.\n    input_bool: bool,\n    # An input parameter of type dict.\n    input_dict: Dict[str, int],\n    # An input parameter of type List[str].\n    input_list: List[str],\n    # An input parameter of type int with a default value.\n    num_steps: int = 100,\n):\n    \"\"\"Dummy Training step\"\"\"\n    with open(dataset_one_path, 'r') as input_file:\n        dataset_one_contents = input_file.read()\n\n    with open(dataset_two.path, 'r') as input_file:\n        dataset_two_contents = input_file.read()\n\n    line = (\n        f'dataset_one_contents: {dataset_one_contents} || '\n        f'dataset_two_contents: {dataset_two_contents} || '\n        f'message: {message} || '\n        f'input_bool: {input_bool}, type {type(input_bool)} || '\n        f'input_dict: {input_dict}, type {type(input_dict)} || '\n        f'input_list: {input_list}, type {type(input_list)} \\n'\n    )\n\n    with open(model.path, 'w') as output_file:\n        for i in range(num_steps):\n            output_file.write('Step {}\\n{}\\n=====\\n'.format(i, line))\n\n    # Use model.get() to get a Model artifact, which has a .metadata dictionary\n    # to store arbitrary metadata for the output artifact.\n    model.metadata['accuracy'] = 0.9\n\n\n@dsl.pipeline(name='my-test-pipeline-beta')\ndef pipeline(message: str = 'message'):\n    preprocess_task = preprocess(message=message)\n    train_task = train(\n        dataset_one_path=preprocess_task.outputs['output_dataset_one'],\n        dataset_two=preprocess_task.outputs['output_dataset_two_path'],\n        message=preprocess_task.outputs['output_parameter_path'],\n        input_bool=preprocess_task.outputs['output_bool_parameter_path'],\n        input_dict=preprocess_task.outputs['output_dict_parameter_path'],\n        input_list=preprocess_task.outputs['output_list_parameter_path'],\n    )\n\n\nif __name__ == '__main__':\n    compiler.Compiler().compile(\n        pipeline_func=pipeline, package_path=__file__.replace('.py', '.json')\n    )\n"
  },
  {
    "repo": "SWE-Gym-Raw/kubeflow__pipelines",
    "file_path": "samples/test/lightweight_python_functions_v2_pipeline_test.py",
    "raw_url": "https://raw.githubusercontent.com/SWE-Gym-Raw/kubeflow__pipelines/0.1.8/samples/test/lightweight_python_functions_v2_pipeline_test.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nfrom pprint import pprint\nimport kfp_server_api\nimport kfp.dsl as dsl\n\nfrom .lightweight_python_functions_v2_pipeline import pipeline\nfrom .util import run_pipeline_func, TestCase, KfpMlmdClient\nfrom ml_metadata.proto import Execution\n\n\ndef verify(run: kfp_server_api.ApiRun, mlmd_connection_config, **kwargs):\n    t = unittest.TestCase()\n    t.maxDiff = None  # we always want to see full diff\n    t.assertEqual(run.status, 'Succeeded')\n    client = KfpMlmdClient(mlmd_connection_config=mlmd_connection_config)\n    tasks = client.get_tasks(run_id=run.id)\n\n    task_names = [*tasks.keys()]\n    t.assertCountEqual(task_names, ['preprocess', 'train'], 'task names')\n    pprint(tasks)\n\n    preprocess = tasks['preprocess']\n    train = tasks['train']\n    pprint(preprocess.get_dict())\n    t.assertEqual(\n        {\n            'inputs': {\n                'artifacts': [],\n                'parameters': {\n                    'message': 'message',\n                    'empty_message': '',\n                }\n            },\n            'name': 'preprocess',\n            'outputs': {\n                'artifacts': [{\n                    'metadata': {\n                        'display_name': 'output_dataset_one'\n                    },\n                    'name': 'output_dataset_one',\n                    'type': 'system.Dataset'\n                }, {\n                    'metadata': {\n                        'display_name': 'output_dataset_two_path'\n                    },\n                    'name': 'output_dataset_two_path',\n                    'type': 'system.Dataset'\n                }],\n                'parameters': {\n                    'output_bool_parameter_path': 'True',\n                    'output_dict_parameter_path': '{\"A\": 1, \"B\": 2}',\n                    'output_list_parameter_path': '[\"a\", \"b\", \"c\"]',\n                    'output_parameter_path': 'message'\n                }\n            },\n            'type': 'system.ContainerExecution',\n            'state': Execution.State.COMPLETE,\n        },\n        preprocess.get_dict(),\n    )\n    t.assertEqual(\n        {\n            'inputs': {\n                'artifacts': [{\n                    'metadata': {\n                        'display_name': 'output_dataset_one'\n                    },\n                    'name': 'dataset_one_path',\n                    'type': 'system.Dataset'\n                }, {\n                    'metadata': {\n                        'display_name': 'output_dataset_two_path'\n                    },\n                    'name': 'dataset_two',\n                    'type': 'system.Dataset'\n                }],\n                'parameters': {\n                    'input_bool': 'True',\n                    'input_dict': '{\"A\": 1, \"B\": 2}',\n                    'input_list': '[\"a\", \"b\", \"c\"]',\n                    'message': 'message',\n                    'num_steps': 100,\n                }\n            },\n            'name': 'train',\n            'outputs': {\n                'artifacts': [{\n                    'metadata': {\n                        'display_name': 'model',\n                        'accuracy': 0.9,\n                    },\n                    'name': 'model',\n                    'type': 'system.Model'\n                }],\n                'parameters': {}\n            },\n            'type': 'system.ContainerExecution',\n            'state': Execution.State.COMPLETE,\n        },\n        train.get_dict(),\n    )\n\n\nrun_pipeline_func([\n    TestCase(\n        pipeline_func=pipeline,\n        verify_func=verify,\n        mode=dsl.PipelineExecutionMode.V2_COMPATIBLE\n    ),\n    TestCase(\n        pipeline_func=pipeline,\n        verify_func=verify,\n        mode=dsl.PipelineExecutionMode.V2_ENGINE\n    ),\n])\n"
  },
  {
    "repo": "SWE-Gym-Raw/kubeflow__pipelines",
    "file_path": "samples/test/lightweight_python_functions_v2_with_outputs.py",
    "raw_url": "https://raw.githubusercontent.com/SWE-Gym-Raw/kubeflow__pipelines/0.1.8/samples/test/lightweight_python_functions_v2_with_outputs.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Lightweight functions v2 with outputs.\"\"\"\nfrom typing import NamedTuple\nimport os\nfrom kfp import components, dsl\nfrom kfp.v2 import compiler\nfrom kfp.v2.dsl import Input, Dataset, Model, Metrics, component\n\n# In tests, we install a KFP package from the PR under test. Users should not\n# normally need to specify `kfp_package_path` in their component definitions.\n_KFP_PACKAGE_PATH = os.getenv('KFP_PACKAGE_PATH')\n\n\n@component(kfp_package_path=_KFP_PACKAGE_PATH)\ndef concat_message(first: str, second: str) -> str:\n    return first + second\n\n\n@component(kfp_package_path=_KFP_PACKAGE_PATH)\ndef add_numbers(first: int, second: int) -> int:\n    return first + second\n\n\n@component(kfp_package_path=_KFP_PACKAGE_PATH)\ndef output_artifact(number: int, message: str) -> Dataset:\n    result = [message for _ in range(number)]\n    return '\\n'.join(result)\n\n\n@component(kfp_package_path=_KFP_PACKAGE_PATH)\ndef output_named_tuple(\n    artifact: Input[Dataset]\n) -> NamedTuple(\n        'Outputs', [\n            ('scalar', str),\n            ('metrics', Metrics),\n            ('model', Model),\n        ]\n):\n    scalar = \"123\"\n\n    import json\n    metrics = json.dumps({\n        'metrics': [{\n            'name': 'accuracy',\n            'numberValue': 0.9,\n            'format': \"PERCENTAGE\",\n        }]\n    })\n\n    with open(artifact.path, 'r') as f:\n        artifact_contents = f.read()\n    model = \"Model contents: \" + artifact_contents\n\n    from collections import namedtuple\n    output = namedtuple('Outputs', ['scalar', 'metrics', 'model'])\n    return output(scalar, metrics, model)\n\n\n@dsl.pipeline(name='functions-with-outputs')\ndef pipeline(\n    first_message: str = 'first',\n    second_message: str = 'second',\n    first_number: int = 1,\n    second_number: int = 2,\n):\n    concat_task = concat_message(first=first_message, second=second_message)\n    add_numbers_task = add_numbers(first=first_number, second=second_number)\n    output_artifact_task = output_artifact(\n        number=add_numbers_task.output, message=concat_task.output\n    )\n    output_name_tuple = output_named_tuple(output_artifact_task.output)\n\n\nif __name__ == '__main__':\n    compiler.Compiler().compile(\n        pipeline_func=pipeline,\n        pipeline_root='dummy_root',\n        output_path=__file__ + '.json'\n    )\n"
  },
  {
    "repo": "SWE-Gym-Raw/kubeflow__pipelines",
    "file_path": "samples/test/lightweight_python_functions_v2_with_outputs_test.py",
    "raw_url": "https://raw.githubusercontent.com/SWE-Gym-Raw/kubeflow__pipelines/0.1.8/samples/test/lightweight_python_functions_v2_with_outputs_test.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom pprint import pprint\nimport unittest\nimport kfp\nimport kfp_server_api\nimport os\nfrom minio import Minio\n\nfrom .lightweight_python_functions_v2_with_outputs import pipeline\nfrom .util import KfpMlmdClient, run_pipeline_func, TestCase\n\n\ndef verify(run: kfp_server_api.ApiRun, mlmd_connection_config, **kwargs):\n    t = unittest.TestCase()\n    t.maxDiff = None  # we always want to see full diff\n    t.assertEqual(run.status, 'Succeeded')\n    client = KfpMlmdClient(mlmd_connection_config=mlmd_connection_config)\n    tasks = client.get_tasks(run_id=run.id)\n    pprint(tasks)\n\n    output_artifact = tasks['output-artifact']\n    output = [\n        a for a in output_artifact.outputs.artifacts if a.name == 'Output'\n    ][0]\n    pprint(output)\n\n    host = os.environ['MINIO_SERVICE_SERVICE_HOST']\n    port = os.environ['MINIO_SERVICE_SERVICE_PORT']\n    minio = Minio(\n        f'{host}:{port}',\n        access_key='minio',\n        secret_key='minio123',\n        secure=False\n    )\n    bucket, key = output.uri[len('minio://'):].split('/', 1)\n    print(f'bucket={bucket} key={key}')\n    response = minio.get_object(bucket, key)\n    data = response.read().decode('UTF-8')\n    t.assertEqual(data, 'firstsecond\\nfirstsecond\\nfirstsecond')\n\n\nrun_pipeline_func([\n    TestCase(\n        pipeline_func=pipeline,\n        mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE\n    ),\n    TestCase(\n        pipeline_func=pipeline,\n        mode=kfp.dsl.PipelineExecutionMode.V2_ENGINE,\n    ),\n    # Verify overriding pipeline root to MinIO\n    TestCase(\n        pipeline_func=pipeline,\n        verify_func=verify,\n        mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE,\n        arguments={\n            kfp.dsl.ROOT_PARAMETER_NAME: 'minio://mlpipeline/override/artifacts'\n        },\n    ),\n])\n"
  },
  {
    "repo": "SWE-Gym-Raw/kubeflow__pipelines",
    "file_path": "samples/test/metrics_visualization_v1.py",
    "raw_url": "https://raw.githubusercontent.com/SWE-Gym-Raw/kubeflow__pipelines/0.1.8/samples/test/metrics_visualization_v1.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp.dsl as dsl\nfrom ..core.visualization.confusion_matrix import confusion_visualization\nfrom ..core.visualization.html import html_visualization\nfrom ..core.visualization.markdown import markdown_visualization\nfrom ..core.visualization.roc import roc_visualization\nfrom ..core.visualization.table import table_visualization\n\n# Note: This test is to verify that visualization metrics on V1 is runnable by KFP.\n# However, this pipeline is only runnable on V1 mode, but not V2 compatible mode.\n\n@dsl.pipeline(\n    name='metrics-visualization-v1-pipeline')\ndef metrics_visualization_v1_pipeline():\n    confusion_visualization_task = confusion_visualization()\n    html_visualization_task = html_visualization(\"\")\n    markdown_visualization_task = markdown_visualization()\n    roc_visualization_task = roc_visualization()\n    table_visualization_task = table_visualization()\n"
  },
  {
    "repo": "SWE-Gym-Raw/kubeflow__pipelines",
    "file_path": "samples/test/metrics_visualization_v1_test.py",
    "raw_url": "https://raw.githubusercontent.com/SWE-Gym-Raw/kubeflow__pipelines/0.1.8/samples/test/metrics_visualization_v1_test.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp_server_api\nimport unittest\nfrom pprint import pprint\nfrom .metrics_visualization_v1 import metrics_visualization_v1_pipeline\nfrom .util import KfpMlmdClient, run_pipeline_func, TestCase\n\nimport kfp\nimport kfp_server_api\n\n\ndef verify(\n    run: kfp_server_api.ApiRun, mlmd_connection_config, argo_workflow_name: str,\n    **kwargs\n):\n    t = unittest.TestCase()\n    t.maxDiff = None  # we always want to see full diff\n    t.assertEqual(run.status, 'Succeeded')\n    client = KfpMlmdClient(mlmd_connection_config=mlmd_connection_config)\n    tasks = client.get_tasks(argo_workflow_name=argo_workflow_name)\n    pprint(tasks)\n\n    table_visualization = tasks['table-visualization']\n    markdown_visualization = tasks['markdown-visualization']\n    roc_visualization = tasks['roc-visualization']\n    html_visualization = tasks['html-visualization']\n    confusion_visualization = tasks['confusion-visualization']\n\n    t.assertEqual(table_visualization.get_dict()['outputs']['artifacts'][0]['name'],\n                  'mlpipeline_ui_metadata')\n    t.assertEqual(markdown_visualization.get_dict()['outputs']['artifacts'][0]['name'],\n                  'mlpipeline_ui_metadata')\n    t.assertEqual(roc_visualization.get_dict()['outputs']['artifacts'][0]['name'],\n                  'mlpipeline_ui_metadata')\n    t.assertEqual(html_visualization.get_dict()['outputs']['artifacts'][0]['name'],\n                  'mlpipeline_ui_metadata')\n    t.assertEqual(confusion_visualization.get_dict()['outputs']['artifacts'][0]['name'],\n                  'mlpipeline_ui_metadata')\n\n\nrun_pipeline_func([TestCase(pipeline_func=metrics_visualization_v1_pipeline,\n                            mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE\n                            )])\n"
  },
  {
    "repo": "SWE-Gym-Raw/kubeflow__pipelines",
    "file_path": "samples/test/metrics_visualization_v2.py",
    "raw_url": "https://raw.githubusercontent.com/SWE-Gym-Raw/kubeflow__pipelines/0.1.8/samples/test/metrics_visualization_v2.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport os\n\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import (\n    component,\n    Output,\n    ClassificationMetrics,\n    Metrics,\n    HTML,\n    Markdown\n)\n\n# In tests, we install a KFP package from the PR under test. Users should not\n# normally need to specify `kfp_package_path` in their component definitions.\n_KFP_PACKAGE_PATH = os.getenv('KFP_PACKAGE_PATH')\n\n\n@component(\n    packages_to_install=['sklearn'],\n    base_image='python:3.9',\n    kfp_package_path=_KFP_PACKAGE_PATH,\n)\ndef digit_classification(metrics: Output[Metrics]):\n    from sklearn import model_selection\n    from sklearn.linear_model import LogisticRegression\n    from sklearn import datasets\n    from sklearn.metrics import accuracy_score\n\n    # Load digits dataset\n    iris = datasets.load_iris()\n\n    # # Create feature matrix\n    X = iris.data\n\n    # Create target vector\n    y = iris.target\n\n    #test size\n    test_size = 0.33\n\n    seed = 7\n    #cross-validation settings\n    kfold = model_selection.KFold(n_splits=10, random_state=seed, shuffle=True)\n\n    #Model instance\n    model = LogisticRegression()\n    scoring = 'accuracy'\n    results = model_selection.cross_val_score(model, X, y, cv=kfold, scoring=scoring)\n\n    #split data\n    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=test_size, random_state=seed)\n    #fit model\n    model.fit(X_train, y_train)\n\n    #accuracy on test set\n    result = model.score(X_test, y_test)\n    metrics.log_metric('accuracy', (result*100.0))\n\n@component(\n    packages_to_install=['sklearn'],\n    base_image='python:3.9',\n    kfp_package_path=_KFP_PACKAGE_PATH,\n)\ndef wine_classification(metrics: Output[ClassificationMetrics]):\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.metrics import roc_curve\n    from sklearn.datasets import load_wine\n    from sklearn.model_selection import train_test_split, cross_val_predict\n\n    X, y = load_wine(return_X_y=True)\n    # Binary classification problem for label 1.\n    y = y == 1\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n    rfc = RandomForestClassifier(n_estimators=10, random_state=42)\n    rfc.fit(X_train, y_train)\n    y_scores = cross_val_predict(rfc, X_train, y_train, cv=3, method='predict_proba')\n    y_predict = cross_val_predict(rfc, X_train, y_train, cv=3, method='predict')\n    fpr, tpr, thresholds = roc_curve(y_true=y_train, y_score=y_scores[:,1], pos_label=True)\n    metrics.log_roc_curve(fpr, tpr, thresholds)\n\n@component(\n    packages_to_install=['sklearn'],\n    base_image='python:3.9',\n    kfp_package_path=_KFP_PACKAGE_PATH,\n)\ndef iris_sgdclassifier(test_samples_fraction: float, metrics: Output[ClassificationMetrics]):\n    from sklearn import datasets, model_selection\n    from sklearn.linear_model import SGDClassifier\n    from sklearn.metrics import confusion_matrix\n\n    iris_dataset = datasets.load_iris()\n    train_x, test_x, train_y, test_y = model_selection.train_test_split(\n        iris_dataset['data'], iris_dataset['target'], test_size=test_samples_fraction)\n\n\n    classifier = SGDClassifier()\n    classifier.fit(train_x, train_y)\n    predictions = model_selection.cross_val_predict(classifier, train_x, train_y, cv=3)\n    metrics.log_confusion_matrix(\n        ['Setosa', 'Versicolour', 'Virginica'],\n        confusion_matrix(train_y, predictions).tolist() # .tolist() to convert np array to list.\n    )\n\n@component(\n    kfp_package_path=_KFP_PACKAGE_PATH,\n)\ndef html_visualization(html_artifact: Output[HTML]):\n    html_content = '<!DOCTYPE html><html><body><h1>Hello world</h1></body></html>'\n    with open(html_artifact.path, 'w') as f:\n        f.write(html_content)\n\n@component(\n    kfp_package_path=_KFP_PACKAGE_PATH,\n)\ndef markdown_visualization(markdown_artifact: Output[Markdown]):\n    markdown_content = '## Hello world \\n\\n Markdown content'\n    with open(markdown_artifact.path, 'w') as f:\n        f.write(markdown_content)\n\n@dsl.pipeline(\n    name='metrics-visualization-pipeline')\ndef metrics_visualization_pipeline():\n    wine_classification_op = wine_classification()\n    iris_sgdclassifier_op = iris_sgdclassifier(test_samples_fraction=0.3)\n    digit_classification_op = digit_classification()\n    html_visualization_op = html_visualization()\n    markdown_visualization_op = markdown_visualization()\n"
  },
  {
    "repo": "SWE-Gym-Raw/kubeflow__pipelines",
    "file_path": "samples/test/metrics_visualization_v2_test.py",
    "raw_url": "https://raw.githubusercontent.com/SWE-Gym-Raw/kubeflow__pipelines/0.1.8/samples/test/metrics_visualization_v2_test.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport unittest.mock as mock\nfrom pprint import pprint\nimport kfp\nimport kfp_server_api\n\nfrom .metrics_visualization_v2 import metrics_visualization_pipeline\nfrom .util import run_pipeline_func, TestCase, KfpMlmdClient\nfrom ml_metadata.proto import Execution\n\n\ndef verify(run: kfp_server_api.ApiRun, mlmd_connection_config, **kwargs):\n    t = unittest.TestCase()\n    t.maxDiff = None  # we always want to see full diff\n    t.assertEqual(run.status, 'Succeeded')\n    client = KfpMlmdClient(mlmd_connection_config=mlmd_connection_config)\n    tasks = client.get_tasks(run_id=run.id)\n\n    task_names = [*tasks.keys()]\n    t.assertCountEqual(\n        task_names,\n        ['wine-classification', 'iris-sgdclassifier', 'digit-classification', 'html-visualization', 'markdown-visualization'],\n        'task names'\n    )\n\n    wine_classification = tasks['wine-classification']\n    iris_sgdclassifier = tasks['iris-sgdclassifier']\n    digit_classification = tasks['digit-classification']\n    html_visualization = tasks['html-visualization']\n    markdown_visualization = tasks['markdown-visualization']\n\n    pprint('======= wine classification task =======')\n    pprint(wine_classification.get_dict())\n    pprint('======= iris sgdclassifier task =======')\n    pprint(iris_sgdclassifier.get_dict())\n    pprint('======= digit classification task =======')\n    pprint(digit_classification.get_dict())\n    pprint('======= HTML task =======')\n    pprint(html_visualization.get_dict())\n    pprint('======= Markdown task =======')\n    pprint(markdown_visualization.get_dict())\n\n    t.assertEqual({\n        'inputs': {\n            'artifacts': [],\n            'parameters': {}\n        },\n        'name': 'wine-classification',\n        'outputs': {\n            'artifacts': [{\n                'metadata': {\n                    'display_name': 'metrics',\n                    'confidenceMetrics': {\n                        'list': [{\n                            'confidenceThreshold': 2.0,\n                            'falsePositiveRate': 0.0,\n                            'recall': 0.0\n                        }, {\n                            'confidenceThreshold': 1.0,\n                            'falsePositiveRate': 0.0,\n                            'recall': 0.33962264150943394\n                        }, {\n                            'confidenceThreshold': 0.9,\n                            'falsePositiveRate': 0.0,\n                            'recall': 0.6037735849056604\n                        }, {\n                            'confidenceThreshold': 0.8,\n                            'falsePositiveRate': 0.0,\n                            'recall': 0.8490566037735849\n                        }, {\n                            'confidenceThreshold': 0.6,\n                            'falsePositiveRate': 0.0,\n                            'recall': 0.8867924528301887\n                        }, {\n                            'confidenceThreshold': 0.5,\n                            'falsePositiveRate': 0.0125,\n                            'recall': 0.9245283018867925\n                        }, {\n                            'confidenceThreshold': 0.4,\n                            'falsePositiveRate': 0.075,\n                            'recall': 0.9622641509433962\n                        }, {\n                            'confidenceThreshold': 0.3,\n                            'falsePositiveRate': 0.0875,\n                            'recall': 1.0\n                        }, {\n                            'confidenceThreshold': 0.2,\n                            'falsePositiveRate': 0.2375,\n                            'recall': 1.0\n                        }, {\n                            'confidenceThreshold': 0.1,\n                            'falsePositiveRate': 0.475,\n                            'recall': 1.0\n                        }, {\n                            'confidenceThreshold': 0.0,\n                            'falsePositiveRate': 1.0,\n                            'recall': 1.0\n                        }]\n                    }\n                },\n                'name': 'metrics',\n                'type': 'system.ClassificationMetrics'\n            }],\n            'parameters': {}\n        },\n        'type': 'system.ContainerExecution',\n        'state': Execution.State.COMPLETE,\n    }, wine_classification.get_dict())\n    t.assertEqual(\n        {\n            'inputs': {\n                'artifacts': [],\n                'parameters': {\n                    'test_samples_fraction': 0.3\n                }\n            },\n            'name': 'iris-sgdclassifier',\n            'outputs': {\n                'artifacts': [{\n                    'metadata': {\n                        'display_name': 'metrics',\n                        'confusionMatrix': {'struct': {\n                            'annotationSpecs': [{\n                                'displayName': 'Setosa'\n                            }, {\n                                'displayName': 'Versicolour'\n                            }, {\n                                'displayName': 'Virginica'\n                            }],\n                            'rows': [{ # these numbers can be random during execution\n                                'row': [mock.ANY, mock.ANY, mock.ANY]\n                            }, {\n                                'row': [mock.ANY, mock.ANY, mock.ANY]\n                            }, {\n                                'row': [mock.ANY, mock.ANY, mock.ANY]\n                            }]\n                        }}\n                    },\n                    'name': 'metrics',\n                    'type': 'system.ClassificationMetrics'\n                }],\n                'parameters': {}\n            },\n            'type': 'system.ContainerExecution',\n            'state': Execution.State.COMPLETE,\n        }, iris_sgdclassifier.get_dict()\n    )\n    rows = iris_sgdclassifier.get_dict()['outputs']['artifacts'][0]['metadata'][\n        'confusionMatrix']['struct']['rows']\n    for i, row in enumerate(rows):\n        for j, item in enumerate(row['row']):\n            t.assertIsInstance(\n                item, float,\n                f'value of confusion matrix row {i}, col {j} is not a number'\n            )\n\n    t.assertEqual({\n        'inputs': {\n            'artifacts': [],\n            'parameters': {}\n        },\n        'name': 'digit-classification',\n        'outputs': {\n            'artifacts': [{\n                'metadata': {\n                    'display_name': 'metrics',\n                    'accuracy': 92.0,\n                },\n                'name': 'metrics',\n                'type': 'system.Metrics'\n            }],\n            'parameters': {}\n        },\n        'type': 'system.ContainerExecution',\n        'state': Execution.State.COMPLETE,\n    }, digit_classification.get_dict())\n\n    t.assertEqual({\n        'inputs': {\n            'artifacts': [], \n            'parameters': {}\n        },\n        'name': 'html-visualization',\n        'outputs': {\n            'artifacts': [{\n                'metadata': {\n                    'display_name': 'html_artifact'\n                },\n                'name': 'html_artifact',\n                'type': 'system.HTML'\n            }],\n            'parameters': {}\n        },\n        'state': Execution.State.COMPLETE,\n        'type': 'system.ContainerExecution'\n    }, html_visualization.get_dict())\n\n    t.assertEqual({\n        'inputs': {\n            'artifacts': [], \n            'parameters': {}\n        },\n        'name': 'markdown-visualization',\n        'outputs': {\n            'artifacts': [{\n                'metadata': {\n                    'display_name': 'markdown_artifact'\n                },\n                'name': 'markdown_artifact',\n                'type': 'system.Markdown'\n            }],\n            'parameters': {}\n        },\n        'state': Execution.State.COMPLETE,\n        'type': 'system.ContainerExecution'\n    }, markdown_visualization.get_dict())\n\n\nrun_pipeline_func([\n    TestCase(\n        pipeline_func=metrics_visualization_pipeline,\n        verify_func=verify,\n        mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE\n    ),\n    TestCase(\n        pipeline_func=metrics_visualization_pipeline,\n        verify_func=verify,\n        mode=kfp.dsl.PipelineExecutionMode.V2_ENGINE\n    ),\n])\n"
  },
  {
    "repo": "SWE-Gym-Raw/kubeflow__pipelines",
    "file_path": "samples/test/parameter_with_format.py",
    "raw_url": "https://raw.githubusercontent.com/SWE-Gym-Raw/kubeflow__pipelines/0.1.8/samples/test/parameter_with_format.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom kfp import components\nfrom kfp import dsl\n\n\n@components.create_component_from_func\ndef print_op(name: str) -> str:\n    print(name)\n    return name\n\n\n@dsl.pipeline(name='pipeline-with-pipelineparam-containing-format')\ndef my_pipeline(name: str = 'KFP'):\n    print_task = print_op('Hello {}'.format(name))\n    print_op('{}, again.'.format(print_task.output))\n"
  },
  {
    "repo": "SWE-Gym-Raw/kubeflow__pipelines",
    "file_path": "samples/test/parameter_with_format_test.py",
    "raw_url": "https://raw.githubusercontent.com/SWE-Gym-Raw/kubeflow__pipelines/0.1.8/samples/test/parameter_with_format_test.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp\nfrom .parameter_with_format import my_pipeline\nfrom .util import run_pipeline_func, TestCase\n\n\ndef verify(run, run_id: str, **kwargs):\n    assert run.status == 'Succeeded'\n    # TODO(Bobgy): verify output\n\n\nrun_pipeline_func([\n    TestCase(\n        pipeline_func=my_pipeline,\n        verify_func=verify,\n    ),\n    TestCase(\n        pipeline_func=my_pipeline,\n        verify_func=verify,\n        mode=kfp.dsl.PipelineExecutionMode.V2_ENGINE,\n    ),\n    TestCase(\n        pipeline_func=my_pipeline,\n        mode=kfp.dsl.PipelineExecutionMode.V1_LEGACY,\n    ),\n])\n"
  },
  {
    "repo": "SWE-Gym-Raw/kubeflow__pipelines",
    "file_path": "samples/test/placeholder_concat.py",
    "raw_url": "https://raw.githubusercontent.com/SWE-Gym-Raw/kubeflow__pipelines/0.1.8/samples/test/placeholder_concat.py",
    "content": "# Copyright 2020 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom kfp import components\nfrom kfp.v2 import dsl\nimport kfp.v2.compiler as compiler\n\ncomponent_op = components.load_component_from_text(\"\"\"\nname: Component with concat placeholder\ninputs:\n- {name: input_one, type: String}\n- {name: input_two, type: String}\nimplementation:\n  container:\n    image: gcr.io/google-containers/busybox\n    command:\n    - sh\n    - -ec\n    args:\n    - echo \"$0\" > /tmp/test && [[ \"$0\" == 'one+two=three' ]]\n    - concat: [{inputValue: input_one}, '+', {inputValue: input_two}, '=three']\n\"\"\")\n\n\n@dsl.pipeline(name='one-step-pipeline-with-concat-placeholder')\ndef pipeline_with_concat_placeholder():\n  component = component_op(input_one='one', input_two='two')\n"
  },
  {
    "repo": "gbhasin0828/Kubeflow_Pipeline",
    "file_path": "kubeflow_model_for_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/gbhasin0828/Kubeflow_Pipeline/main/kubeflow_model_for_pipeline.py",
    "content": "# -*- coding: utf-8 -*-\n\"\"\"KubeFlow_Model_For_Pipeline.ipynb\n\nAutomatically generated by Colab.\n\nOriginal file is located at\n    https://colab.research.google.com/drive/19_0dWxpGnnSyPMuxnk1bn8u74vZiKQBN\n\"\"\"\n\n!pip install kfp --upgrade\n\nimport kfp.components as compile\nimport kfp.dsl as dsl\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport os\nfrom sklearn.linear_model import ElasticNet, LogisticRegression\n\ndef download_dataset(url: str, output_path: str) -> pd.DataFrame:\n    import pandas as pd\n    import os\n\n    # Create the directory if it doesn't exist\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n\n    # Download the dataset\n    dataset = pd.read_csv(url)\n\n    # Save the dataset locally\n    dataset.to_csv(output_path, index=False)\n    print(f\"Data saved at {output_path}\")\n\n    # Return the DataFrame\n    return dataset\n\ndef preprocess_dataset(df: pd.DataFrame):\n  import pandas as pd\n  df.columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']\n  return df\n\ndef train_test_split(df: pd.DataFrame, output_dir : str) -> dict:\n  import pandas as pd\n  import numpy as np\n  from sklearn.model_selection import train_test_split\n  import os\n\n  os.makedirs(output_dir, exist_ok = True)\n\n  target_column = 'class'\n\n  X = df.loc[:, df.columns != target_column]\n  y = df.loc[:, df.columns == target_column]\n\n  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=47)\n\n  X_train_path = f'{output_dir}/X_train.npy'\n  X_test_path = f'{output_dir}/X_test.npy'\n  y_train_path = f'{output_dir}/y_train.npy'\n  y_test_path = f'{output_dir}/y_test.npy'\n\n  np.save(X_train_path, X_train)\n  np.save(X_test_path, X_test)\n  np.save(y_train_path, y_train)\n  np.save(y_test_path, y_test)\n\n  return {\n      \"X_train\" : X_train_path,\n      \"X_test\" : X_test_path,\n      \"y_train\" : y_train_path,\n      \"y_test\" : y_test_path\n  }\n\ndef train_model(path : dict, model_output_path):\n  import numpy as np\n  import pickle\n  from sklearn.linear_model import LogisticRegression\n  import os\n\n  X_train = np.load(path['X_train'], allow_pickle = True)\n  y_train = np.load(path['y_train'], allow_pickle = True)\n\n  classifier = LogisticRegression(max_iter=500)\n  classifier.fit(X_train, y_train)\n\n  os.makedirs(os.path.dirname(model_output_path), exist_ok = True)\n  with open(model_output_path, 'wb') as f:\n    pickle.dump(classifier, f)\n  print(\"Model Training Completed\")\n\nif __name__ == '__main__':\n\n  dataset_url = 'https://raw.githubusercontent.com/gbhasin0828/Kubeflow_Files/main/iris.csv'\n\n  output_path = 'data/final_df.csv'\n  output_dir = 'data/splits'\n  model_output_path = 'data/model.pkl'\n\n  df = download_dataset(dataset_url, output_path)\n\n  processed_df  = preprocess_dataset(df)\n\n  dict_data = train_test_split(df_processed, output_dir)\n\n  train_model(data_dict, model_output_path)\n\n\"\"\"# **Kubeflow Pipeline Work Starts - Look at the file Pipeline_For_Model.py**\"\"\"\n\n\n\n\n\n\n\n"
  },
  {
    "repo": "gbhasin0828/Kubeflow_Pipeline",
    "file_path": "kubeflow_pipeline_intermediate.py",
    "raw_url": "https://raw.githubusercontent.com/gbhasin0828/Kubeflow_Pipeline/main/kubeflow_pipeline_intermediate.py",
    "content": "# -*- coding: utf-8 -*-\n\"\"\"KubeFlow_Pipeline.ipynb\n\nAutomatically generated by Colab.\n\nOriginal file is located at\n    https://colab.research.google.com/drive/19_0dWxpGnnSyPMuxnk1bn8u74vZiKQBN\n\"\"\"\n\npip install kfp --upgrade\n\nimport kfp\n\nimport kfp.components as compile\nimport kfp.dsl as dsl\n\nfrom kfp.v2.dsl import component\n\n@component(\n    base_image='python:3.12',\n    packages_to_install=['pandas', 'numpy', 'scikit-learn']\n)\n\ndef prepare_data(output_path: str):\n    # Save to the mounted volume\n    import pandas as pd\n    import os\n\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n    df = pd.read_csv(\"https://raw.githubusercontent.com/TripathiAshutosh/dataset/main/iris.csv\")\n    df.to_csv(output_path, index=False)\n    print(f\"Data saved at {output_path}\")\n\n@component(\n    base_image='python:3.12',\n    packages_to_install=['pandas', 'numpy', 'scikit-learn']\n)\n\ndef train_test_split(input_path: str, output_dir: str) -> dict:\n    import pandas as pd\n    import numpy as np\n    from sklearn.model_selection import train_test_split\n    import os\n\n    os.makedirs(output_dir, exist_ok=True)\n    final_data = pd.read_csv(input_path)\n\n    target_column = 'class'\n\n    X = final_data.loc[:, final_data.columns != target_column]\n    y = final_data.loc[:, final_data.columns == target_column]\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=47)\n\n    X_train_path = f'{output_dir}/X_train.npy'\n    X_test_path = f'{output_dir}/X_test.npy'\n    y_train_path = f'{output_dir}/y_train.npy'\n    y_test_path = f'{output_dir}/y_test.npy'\n\n    np.save(X_train_path, X_train)\n    np.save(X_test_path, X_test)\n    np.save(y_train_path, y_train)\n    np.save(y_test_path, y_test)\n\n    return {\n        \"X_train\": X_train_path,\n        \"X_test\": X_test_path,\n        \"y_train\": y_train_path,\n        \"y_test\": y_test_path\n    }\n\n@component(\n    base_image='python:3.12',\n    packages_to_install=['pandas', 'numpy', 'scikit-learn']\n)\n\n\ndef training_basic_classifier(X_train_path: str, y_train_path: str, model_output_path: str):\n    import numpy as np\n    import pickle\n    from sklearn.linear_model import LogisticRegression\n    import os\n\n    X_train = np.load(X_train_path, allow_pickle=True)\n    y_train = np.load(y_train_path, allow_pickle=True)\n\n    classifier = LogisticRegression(max_iter=500)\n    classifier.fit(X_train, y_train)\n\n    os.makedirs(os.path.dirname(model_output_path), exist_ok=True)\n    with open(model_output_path, 'wb') as f:\n        pickle.dump(classifier, f)\n    print(\"Model Training Completed\")\n\n@component(\n    base_image='python:3.12',\n    packages_to_install=['pandas', 'numpy', 'scikit-learn']\n)\n\ndef predict_on_test_data(model_path: str, X_test_path: str, y_pred_path: str):\n    import numpy as np\n    import pickle\n\n    with open(model_path, 'rb') as f:\n        model = pickle.load(f)\n\n    X_test = np.load(X_test_path, allow_pickle=True)\n    y_pred = model.predict(X_test)\n    np.save(y_pred_path, y_pred)\n    print(\"Predictions saved!\")\n\n\"\"\"# **Kubeflow Pipeline Work Starts**\"\"\"\n\nfrom kfp.v2.dsl import pipeline\nfrom kfp.v2 import compiler\n\n@pipeline(\n    name='prepare_data_pipeline',\n    description='A pipeline to prepare data and save it to a CSV file'\n)\n\ndef create_step_prepare_data(output_path: str = 'data/final_df.csv'):\n    # Step 1: Call the prepare_data component\n    prepare_data_op = prepare_data(output_path=output_path)\n\n# Compile the pipeline\ncompiler.Compiler().compile(\n    pipeline_func=create_step_prepare_data,\n    package_path='prepare_data_pipeline.yaml'\n)\n\n@pipeline(\n    name='train_test_split',\n    description='A pipeline to prepare data for training & testing'\n)\n\ndef create_step_train_test_split(input_path: str = 'data/final_df.csv', output_dir: str = 'data'):\n    \"\"\"\n    A pipeline step that splits the data into training and testing sets.\n\n    Args:\n        input_path (str): Path to the input dataset.\n        output_dir (str): Directory where the split data will be saved.\n    \"\"\"\n    # Call the train_test_split component\n    train_test_split_op = train_test_split(\n        input_path=input_path,\n        output_dir=output_dir\n    )\n\n# Compile the pipeline\ncompiler.Compiler().compile(\n    pipeline_func=create_step_train_test_split,\n    package_path='train_test_split_pipeline.yaml'\n)\n\nfrom kfp.v2.dsl import pipeline\n\n@pipeline(\n    name='training_basic_classifier',\n    description='A pipeline to prepare data classifier for training'\n)\ndef create_step_training_basic_classifier(X_train_path: str, y_train_path: str, model_output_path: str = 'data/model.pkl'):\n    \"\"\"\n    Pipeline step to train a basic classifier.\n\n    Args:\n        X_train_path (str): Path to the training features file.\n        y_train_path (str): Path to the training labels file.\n        model_output_path (str): Path to save the trained model.\n    \"\"\"\n    # Call the training_basic_classifier component\n    training_basic_classifier_op = training_basic_classifier(\n        X_train_path=X_train_path,\n        y_train_path=y_train_path,\n        model_output_path=model_output_path\n    )\n\n# Compile the pipeline\ncompiler.Compiler().compile(\n    pipeline_func=create_step_training_basic_classifier,\n    package_path='training_basic_classifier_pipeline.yaml'\n)\n\nfrom kfp.v2.dsl import pipeline\n\n@pipeline(\n    name='predict_on_test_data',\n    description='A pipeline to perform predictions'\n)\n\ndef create_step_predict_on_test_data(model_path: str, X_test_path: str, y_pred_path: str = 'data/y_pred.npy'):\n    \"\"\"\n    Pipeline step to perform predictions on test data.\n\n    Args:\n        model_path (str): Path to the trained model file.\n        X_test_path (str): Path to the test features file.\n        y_pred_path (str): Path to save the predictions.\n    \"\"\"\n    # Call the predict_on_test_data component\n    predict_on_test_data_op = predict_on_test_data(\n        model_path=model_path,\n        X_test_path=X_test_path,\n        y_pred_path=y_pred_path\n    )\n\n# Compile the pipeline\ncompiler.Compiler().compile(\n    pipeline_func=create_step_predict_on_test_data,\n    package_path='predict_on_test_data_pipeline.yaml'\n)\n\nfrom kfp.v2.dsl import pipeline, component\nimport os\n\n@pipeline(\n    name='iris-classifier-pipeline',\n    description='Pipeline for IRIS classification using a shared volume'\n)\ndef iris_classifier_pipeline(data_path: str = '/mnt/data/final_df.csv'):\n    \"\"\"\n    Defines the IRIS classification pipeline using a shared persistent volume.\n\n    Args:\n        data_path (str): Path to save the prepared dataset in the shared volume.\n    \"\"\"\n    # Step 1: Prepare Data\n    prepare_data_task = prepare_data(\n        output_path=data_path\n    ).set_volume_mounts([\n        kfp.dsl.VolumeMount(\n            name='iris-data-pvc',  # Name of the PVC created earlier\n            mount_path='/mnt/data'\n        )\n    ])\n\n    # Step 2: Train-Test Split\n    split_data_task = train_test_split(\n        input_path=data_path,\n        output_dir='/mnt/data/splits'\n    ).set_volume_mounts([\n        kfp.dsl.VolumeMount(\n            name='iris-data-pvc',\n            mount_path='/mnt/data'\n        )\n    ])\n\n    # Step 3: Train Classifier\n    train_model_task = training_basic_classifier(\n        X_train_path='/mnt/data/splits/X_train.npy',\n        y_train_path='/mnt/data/splits/y_train.npy',\n        model_output_path='/mnt/data/model.pkl'\n    ).set_volume_mounts([\n        kfp.dsl.VolumeMount(\n            name='iris-data-pvc',\n            mount_path='/mnt/data'\n        )\n    ])\n\n    # Step 4: Predict on Test Data\n    predict_task = predict_on_test_data(\n        model_path='/mnt/data/model.pkl',\n        X_test_path='/mnt/data/splits/X_test.npy',\n        y_pred_path='/mnt/data/y_pred.npy'\n    ).set_volume_mounts([\n        kfp.dsl.VolumeMount(\n            name='iris-data-pvc',\n            mount_path='/mnt/data'\n        )\n    ])"
  },
  {
    "repo": "gbhasin0828/Kubeflow_Pipeline",
    "file_path": "pipeline_for_model.py",
    "raw_url": "https://raw.githubusercontent.com/gbhasin0828/Kubeflow_Pipeline/main/pipeline_for_model.py",
    "content": "# -*- coding: utf-8 -*-\n\"\"\"Pipeline_For_Model.ipynb\n\nAutomatically generated by Colab.\n\nOriginal file is located at\n    https://colab.research.google.com/drive/1QEyX82Jm4xwSVGeY5G0Tcyb-qvxVUSU8\n\"\"\"\n\n!pip install kfp --upgrade\n!pip install Docker\n\nfrom kfp import local, dsl\nimport kfp.components as compile\nimport kfp.dsl as dsl\n\nimport pandas as pd\nimport os\nimport requests\n\nlocal.init(local.DockerRunner())\n\n@dsl.component(\n    base_image = 'python:3.12',\n    output_component_file='download_dataset_component.yaml')\ndef download_dataset(url: str, dataset: dsl.Output[dsl.Dataset]) :\n    import pandas as pd\n    import os\n\n    # Create the directory if it doesn't exist\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n\n    # Download the dataset\n    df = pd.read_csv(url)\n\n    # Save the dataset locally\n    df.to_csv(dataset.path, index=False)\n    print(f\"Data saved at {dataset.path}\")\n\n    # Return the DataFrame\n    return dataset.path\n\n@dsl.component(base_image = 'python:3.12', output_component_file = 'preprocess_dataset_component.yaml')\ndef preprocess_dataset(dataset: dsl.Input[dsl.Dataset], preprocessed_dataset: dsl.Output[dsl.Dataset]):\n  import pandas as pd\n  dataset.columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']\n  dataset.to_csv(preprocessed_dataset.path, index = False)\n  return preprocessed_dataset.path\n\n@dsl.component(\n    base_image = 'python:3.12',\n    output_component_file = 'train_test_split.yaml')\ndef train_test_split(\n    dataframe: dsl.Input[dsl.Dataset],\n    X_train: dsl.Output[dsl.Dataset],\n    X_test : dsl.Output[dsl.Dataset],\n    y_train: dsl.Output[dsl.Dataset],\n    y_test: dsl.Output[dsl.Dataset]\n    ):\n  import pandas as pd\n  import numpy as np\n  from sklearn.model_selection import train_test_split\n  import os\n\n  df = pd.read_csv(dataframe.path)\n\n\n  target_column = 'class'\n\n  X = df.loc[:, df.columns != target_column]\n  y = df.loc[:, df.columns == target_column]\n\n  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=47)\n\n  np.save(X_train.path, X_train)\n  np.save(X_test.path, X_test)\n  np.save(y_train.path, y_train)\n  np.save(y_test.path, y_test)\n\n@dsl.component(\n    base_image = 'python: 3.12',\n    output_component_file= 'train_model.yaml'\n)\ndef train_model(\n    X_train: dsl.Input[dsl.Dataset],\n    y_train: dsl.Input[dsl.Dataset],\n    model: dsl.Output[dsl.Model]\n):\n  import numpy as np\n  import pickle\n  from sklearn.linear_model import LogisticRegression\n  import os\n\n  X_train = np.load(path['X_train'], allow_pickle = True)\n  y_train = np.load(path['y_train'], allow_pickle = True)\n\n  classifier = LogisticRegression(max_iter=500)\n  classifier.fit(X_train, y_train)\n\n  os.makedirs(os.path.dirname(model_output_path), exist_ok = True)\n  with open(model.path, 'wb') as f:\n    pickle.dump(classifier, f)\n  print(\"Model Training Completed\")\n\n@dsl.pipeline\ndef mlops_pipeline(url : str, output_path: str):\n  download_dataset_task = download_dataset(url = url)\n  preprocess_dataset_task = preprocess_dataset(dataset = download_dataset_task.outputs[\"dataset\"])\n  train_test_split_task = train_test_split(dataframe = preprocess_dataset_task.outputs['preprocessed_dataset'] )\n  train_model_task = train_model(X_train = train_test_split_task.outputs['X_train'],\n                                 y_train = train_test_split_task.outputs['y_train'])\n\nif __name__ == '__main__':\n  dataset_url = 'https://raw.githubusercontent.com/gbhasin0828/Kubeflow_Files/main/iris.csv'\n  output_path = 'data/final_df.csv'\n  model_output_path = 'data/model.pkl'\n\n# Compile the pipeline\nfrom kfp.v2 import compiler\n\npipeline_file = 'mlops_complete_pipeline_file.yaml'\n\ncompiler.Compiler().compile(\n    pipeline_func = mlops_pipeline,\n    package_path= pipeline_file\n)"
  },
  {
    "repo": "Jabor047/Kubeflow-Pipelines",
    "file_path": "lightweight_pipeline/telco_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/Jabor047/Kubeflow-Pipelines/main/lightweight_pipeline/telco_pipeline.py",
    "content": "import kfp\nfrom kfp import dsl\nfrom typing import NamedTuple\n\ndef download_data() -> str:\n    import pandas as pd\n    from requests import get\n    import io\n\n    url = \"https://storage.googleapis.com/<GCS Bucket>/data/telco.csv\"\n    s = get(url).content\n    df = pd.read_csv(io.StringIO(s.decode(\"utf-8\")))\n\n    data_path = \"/data/telco.csv\"\n    df.to_csv(data_path)\n\n    return data_path\n\n\ndef clean_data(input_path: str) -> str:\n    import pandas as pd\n\n    df = pd.read_csv(input_path)\n    df_fill_nan = df.drop([\"Bearer Id\", \"IMSI\", \"MSISDN/Number\", \"IMEI\", \"Start\", \"End\", \"Last Location Name\",\n                           \"Handset Manufacturer\", \"Handset Type\"], axis=1)\n\n    # fill all the null values with the mean of the column they occur in\n    df_fill_nan = df_fill_nan.apply(lambda x: x.fillna(x.median()), axis=1)\n\n    # replace the columns where values have been filled in\n    for col in df_fill_nan.columns.to_list():\n        df[col] = df_fill_nan[col]\n\n    clean_data_path = \"/data/clean_telco.csv\"\n    df.to_csv(clean_data_path)\n\n    return clean_data_path\n\ndef feature_prep(input_path: str) -> NamedTuple(\"feature_paths\", [(\"exp_path\", str), (\"eng_path\", str)]):\n    import pandas as pd\n\n    # this sums up all the values in a dataframe col\n    def sum_agg(dataframe: pd.DataFrame, col: str) -> pd.DataFrame:\n        s = dataframe.groupby(\"MSISDN/Number\")[col].agg(\"sum\").sort_values(ascending=False)\n        df = pd.DataFrame({\"MSISDN/Number\": s.index, col: s.values})\n\n        return df\n\n    df = pd.read_csv(input_path)\n\n    # combine both the download and upload data cols in bytes\n    df[\"Total traffic\"] = df[\"Total DL (Bytes)\"] + df[\"Total UL (Bytes)\"]\n\n    # get total duration and traffic for each use and merhe them into one dataframe\n    df_dur_ms = sum_agg(df, \"Dur. (ms)\")\n    df_total_bytes = sum_agg(df, \"Total traffic\")\n    df_engagement = pd.merge(df_dur_ms, df_total_bytes, on=\"MSISDN/Number\")\n\n    # get the number of sessions each user has\n    session_series = df.groupby(\"MSISDN/Number\")[\"Dur. (ms)\"].count().sort_values(ascending=False)\n    df_sess_freq = pd.DataFrame({\"MSISDN/Number\": session_series.index, \"sessions freq\": session_series.values})\n\n    # merge the engagement dataframe and sessions frequency dataframe\n    df_engagement = pd.merge(df_engagement, df_sess_freq, on=\"MSISDN/Number\")\n\n    # Summing the Uploads and Downloads columns to get the Total data columns\n    df[\"Avg RTT\"] = df[\"Avg RTT DL (ms)\"] + df[\"Avg RTT UL (ms)\"]\n    df[\"Avg Bearer TP (kbps)\"] = df[\"Avg Bearer TP DL (kbps)\"] + df[\"Avg Bearer TP UL (kbps)\"]\n    df[\"TCP Retrans. Vol (Bytes)\"] = df[\"TCP DL Retrans. Vol (Bytes)\"] + df[\"TCP UL Retrans. Vol (Bytes)\"]\n\n    # select the required columns for experience analysis\n    df_experience = df[[\"MSISDN/Number\", \"Avg RTT\", \"Avg Bearer TP (kbps)\", \"TCP Retrans. Vol (Bytes)\"]]\n\n    df_exp_path = \"/data/experience.csv\"\n    df_eng_path = \"/data/engagement.csv\"\n\n    df_engagement.to_csv(df_eng_path)\n    df_experience.to_csv(df_exp_path)\n\n    # convert the feature paths to a named tuple\n    from collections import namedtuple\n    feature_paths = namedtuple(\"feature_paths\", [\"exp_path\", \"eng_path\"])\n    return feature_paths(df_eng_path, df_eng_path)\n\ndef find_eng_and_exp_score(exp_path: str, eng_path: str) -> str:\n    import joblib\n    import json\n    import pandas as pd\n    import numpy as np\n    from sklearn.preprocessing import normalize\n    from sklearn.cluster import KMeans\n    from google.cloud import storage\n    from requests import get\n\n    url = \"https://storage.googleapis.com/<GCS Bucket>/kubeflow-tutorials-340813-d338387fc0f4.json\"\n    s = get(url).content\n\n    serv_acc_json = json.loads(s)\n    with open(\"/data/service_account.json\", 'w') as f:\n        json.dump(serv_acc_json, f)\n\n    storage_client = storage.Client.from_service_account_json(\"/data/service_account.json\")\n    bucket = storage_client.bucket(\"<GCS Bucket>\")\n    eng_blob = bucket.blob(\"models/sklearn/engagement/001/model.pkl\")\n    exp_blob = bucket.blob(\"models/sklearn/experience/001/model.pkl\")\n\n    df_experience = pd.read_csv(exp_path)\n    df_engagement = pd.read_csv(eng_path)\n\n    # normalize the columns to be entered in to the clustering algo\n    def normalize_df(df: pd.DataFrame) -> pd.DataFrame:\n        df.drop(\"MSISDN/Number\", axis=1, inplace=True)\n        for col in df.columns.to_list():\n            df[col] = np.transpose(normalize([np.array(df[col])]))\n\n        return df\n\n    df_experience_norm = normalize_df(df_experience.copy())\n    df_engagement_norm = normalize_df(df_engagement.copy())\n\n    # cluster the engagement and experience into 3 clusters each\n    engagement_kmeans = KMeans(n_clusters=3, random_state=42).fit(df_engagement_norm)\n    experience_kmeans = KMeans(n_clusters=3, random_state=42).fit(df_experience_norm)\n\n    joblib.dump(engagement_kmeans, \"/data/engagement_kmeans.pkl\")\n    joblib.dump(experience_kmeans, \"/data/experience_kmeans.pkl\")\n    eng_blob.upload_from_filename(\"/data/engagement_kmeans.pkl\")\n    exp_blob.upload_from_filename(\"/data/experience_kmeans.pkl\")\n\n    least_eng_cluster = engagement_kmeans.cluster_centers_[0]\n    least_exp_cluster = experience_kmeans.cluster_centers_[0]\n\n    # finding the eculidean distance between each data point and the least engaged cluster\n    engagement_score = []\n    for row in df_engagement_norm.to_numpy():\n        eng_score = np.linalg.norm(row - least_eng_cluster)\n        engagement_score.append(eng_score)\n\n    # finding the eculidean distance between each data point and the least experience cluster\n    experience_score = []\n    for row in df_experience_norm.to_numpy():\n        exp_score = np.linalg.norm(row - least_exp_cluster)\n        experience_score.append(exp_score)\n\n    # add the engagement and experience scores to normalized dataframes\n    df_engagement_norm[\"Engagement Score\"] = np.transpose(np.array(engagement_score))\n    df_experience_norm[\"Experience Score\"] = np.transpose(np.array(experience_score))\n\n    # add the unique identifier for merging\n    df_experience_norm[\"MSISDN/Number\"] = df_experience[\"MSISDN/Number\"]\n    df_engagement_norm[\"MSISDN/Number\"] = df_engagement[\"MSISDN/Number\"]\n\n    # merging the two dataframes\n    df = pd.merge(df_engagement_norm, df_experience_norm, on=\"MSISDN/Number\")\n\n    sat_data_path = \"/data/satisfication.csv\"\n    df.to_csv(sat_data_path)\n\n    return sat_data_path\n\ndef find_satisfaction(input_path: str):\n    import joblib\n    import json\n    import pandas as pd\n    import numpy as np\n    from sklearn.cluster import KMeans\n    from google.cloud import storage\n    from requests import get\n\n    url = \"https://storage.googleapis.com/<GCS Bucket>/kubeflow-tutorials-340813-d338387fc0f4.json\"\n    s = get(url).content\n\n    serv_acc_json = json.loads(s)\n    with open(\"/data/service_account.json\", 'w') as f:\n        json.dump(serv_acc_json, f)\n\n    storage_client = storage.Client.from_service_account_json(\"/data/service_account.json\")\n    bucket = storage_client.bucket(\"<GCS Bucket>\")\n    blob = bucket.blob(\"models/sklearn/satisfaction/001/model.pkl\")\n\n    df = pd.read_csv(input_path)\n    satisfaction_kmeans = KMeans(n_clusters=2, random_state=42).fit(df[[\"Engagement Score\", \"Experience Score\"]])\n    df[\"Satisfaction\"] = np.transpose(satisfaction_kmeans.labels_)\n    joblib.dump(satisfaction_kmeans, \"/data/satisfaction_kmeans.pkl\")\n    blob.upload_from_filename(\"/data/satisfaction_kmeans.pkl\")\n\n    print(df[[\"MSISDN/Number\", \"Satisfaction\"]])\n\n@dsl.pipeline(name=\"telco_pipeline\",\n              description=\"lightweight component Telco pipeline for the presentation\")\ndef telco_pipeline():\n\n    data_op = dsl.VolumeOp(name=\"create-pvc\",\n                           resource_name=\"data-volume\",\n                           size=\"2Gi\",\n                           modes=dsl.VOLUME_MODE_RWO)\n\n    download_data_op = kfp.components.func_to_container_op(download_data,\n                                                           packages_to_install=[\"pandas\", \"requests\"])\n    clean_data_op = kfp.components.func_to_container_op(clean_data, packages_to_install=[\"pandas\"])\n    feature_prep_op = kfp.components.func_to_container_op(feature_prep, packages_to_install=[\"pandas\"])\n    find_eng_and_exp_score_op = kfp.components.func_to_container_op(find_eng_and_exp_score,\n                                                                    packages_to_install=[\"pandas\", \"numpy\",\n                                                                                         \"scikit-learn\",\n                                                                                         \"google-cloud-storage\",\n                                                                                         \"requests\"])\n    find_satisfaction_op = kfp.components.func_to_container_op(find_satisfaction,\n                                                               packages_to_install=[\"pandas\", \"numpy\",\n                                                                                    \"scikit-learn\",\n                                                                                    \"google-cloud-storage\",\n                                                                                    \"requests\"])\n\n    step1 = download_data_op().add_pvolumes({\"/data\": data_op.volume})\n    step2 = clean_data_op(step1.output).add_pvolumes({\"/data\": data_op.volume})\n    step3 = feature_prep_op(step2.output).add_pvolumes({\"/data\": data_op.volume})\n    step4 = find_eng_and_exp_score_op(step3.outputs[\"exp_path\"], step3.outputs[\"eng_path\"])\\\n        .add_pvolumes({\"/data\": data_op.volume})\n    step5 = find_satisfaction_op(step4.output).add_pvolumes({\"/data\": data_op.volume})\n\n    kf_serve = kfp.components.load_component_from_url(\"https://raw.githubusercontent.com/kubeflow/pipelines/master/comp\"\n                                                      \"onents/kubeflow/kfserving/component.yaml\")\n    kf_serve_op = kf_serve(\n        action=\"apply\",\n        model_uri=\"gs://<GCS Bucket>/models/sklearn/satisfaction\",\n        model_name=\"satisfactionkmeans\",\n        namespace=\"gkkarobia\",\n        framework=\"sklearn\",\n        watch_timeout=\"300\"\n    )\n    kf_serve_op.after(step5)\n\nkfp.compiler.Compiler().compile(telco_pipeline, \"telco_pipeline.zip\")\n\nkubeflow_gateway_endpoint = \"localhost:7777\"\nauthservice_session_cookie = \"MTY1Mjg2MTY4MnxOd3dBTkZnM1dVdFVRMFEwTkVWR1dVWlZWbEpYVmxBMVNrUlpTRXhTUkZoV05ETkVTalpaUWtaU\"\n\"E5GaExXbFZKVlU0MlNWZE9XbEU9fCF_YdjoAAYppXzd0de2fRiN9xrLret1r5AhyO25J0XO\"\nnamespace = \"gkkarobia\"\n\nclient = kfp.Client(f\"http://{kubeflow_gateway_endpoint}/pipeline\",\n                    cookies=f\"authservice_session={authservice_session_cookie}\")\n\nexperiment = client.create_experiment(\"telco\", namespace=namespace)\nprint(client.list_experiments(namespace=namespace))\n\nrun = client.run_pipeline(experiment_id=experiment.id, job_name=\"telco_pipeline\",\n                          pipeline_package_path=\"telco_pipeline.zip\")\n"
  },
  {
    "repo": "qiuosier/kubeflow_pipelines",
    "file_path": "kf_sklearn_iris.py",
    "raw_url": "https://raw.githubusercontent.com/qiuosier/kubeflow_pipelines/main/kf_sklearn_iris.py",
    "content": "from pickle import load\nimport kfp\nfrom kfp import dsl\nfrom kfp.components import func_to_container_op, InputPath, OutputPath\n\n\ndef load_and_split(\n    train_x_path: OutputPath(str), \n    test_x_path: OutputPath(str),\n    train_y_path: OutputPath(str), \n    test_y_path: OutputPath(str)\n):\n    import numpy\n    from sklearn import datasets\n    from sklearn.model_selection import train_test_split\n\n    iris_x, iris_y = datasets.load_iris(return_X_y=True)\n    train_x, test_x, train_y , test_y = train_test_split(iris_x, iris_y, test_size=0.2)\n    print(f\"Training data size - X: {train_x.size}, y: {train_y.size}\")\n    print(f\"Testing data size - X: {test_x.size}, y: {test_y.size}\")\n\n    with open(train_x_path, 'wb') as f:\n        numpy.save(f, train_x)\n    with open(test_x_path, 'wb') as f:\n        numpy.save(f, test_x)\n    with open(train_y_path, 'wb') as f:\n        numpy.save(f, train_y)\n    with open(test_y_path, 'wb') as f:\n        numpy.save(f, test_y)\n\n\ndef train_knn(train_x_path: InputPath(), train_y_path: InputPath(), model_path: OutputPath()):\n    import numpy\n    import pickle\n    from sklearn.neighbors import KNeighborsClassifier\n    train_x = numpy.load(train_x_path)\n    train_y = numpy.load(train_y_path)\n    print(f\"Training data size - X: {train_x.size}, y: {train_y.size}\")\n    knn = KNeighborsClassifier()\n    knn.fit(train_x, train_y)\n    with open(model_path, 'wb') as f:\n        pickle.dump(knn, f)\n\n\ndef test_knn(test_x_path: InputPath(), test_y_path: InputPath(), model_path: InputPath()):\n    import numpy\n    import pickle\n    from sklearn.metrics import classification_report\n    test_x = numpy.load(test_x_path)\n    test_y = numpy.load(test_y_path)\n    print(f\"Testing data size - X: {test_x.size}, y: {test_y.size}\")\n    with open(model_path, 'rb') as f:\n        knn = pickle.load(f)\n    pred_y = knn.predict(test_x)\n    print(classification_report(test_y, pred_y))\n\n\ndef train_test_knn():\n    load_op = func_to_container_op(load_and_split, base_image='qiuosier/sklearn')\n    train_op = func_to_container_op(train_knn, base_image='qiuosier/sklearn')\n    test_op = func_to_container_op(test_knn, base_image='qiuosier/sklearn')\n    \n    load_task = load_op()\n    train_task = train_op(load_task.outputs['train_x'], load_task.outputs['train_y'])\n    test_task = test_op(load_task.outputs['test_x'], load_task.outputs['test_y'], train_task.output)\n\n\n@dsl.pipeline(name='sklearn-iris', description='Classifying Iris data with KNN.')\ndef kf_pipeline():\n    train_test_knn()\n\n\n# if __name__ == '__main__':\n#     # Compile the pipeline\n#     kfp.compiler.Compiler().compile(kf_pipeline, __file__ + '.yaml')\n"
  },
  {
    "repo": "qiuosier/kubeflow_pipelines",
    "file_path": "kf_sklearn_iris_with_condition.py",
    "raw_url": "https://raw.githubusercontent.com/qiuosier/kubeflow_pipelines/main/kf_sklearn_iris_with_condition.py",
    "content": "import kfp\nfrom kfp import dsl\nfrom kfp.components import func_to_container_op, InputPath, OutputPath\n\n\ndef load_data(x_path: OutputPath(str), y_path: OutputPath(str)):\n    import numpy\n    from sklearn import datasets\n    iris_x, iris_y = datasets.load_iris(return_X_y=True)\n    with open(x_path, 'wb') as f:\n        numpy.save(f, iris_x)\n    with open(y_path, 'wb') as f:\n        numpy.save(f, iris_y)\n\ndef split_data(\n    x_path: InputPath(), \n    y_path: InputPath(),\n    train_x_path: OutputPath(str), \n    test_x_path: OutputPath(str),\n    train_y_path: OutputPath(str), \n    test_y_path: OutputPath(str),\n):\n    import numpy\n    from sklearn.model_selection import train_test_split\n    iris_x = numpy.load(x_path)\n    iris_y = numpy.load(y_path)\n    train_x, test_x, train_y , test_y = train_test_split(iris_x, iris_y, test_size=0.2)\n    print(f\"Training data size - X: {train_x.size}, y: {train_y.size}\")\n    print(f\"Testing data size - X: {test_x.size}, y: {test_y.size}\")\n\n    with open(train_x_path, 'wb') as f:\n        numpy.save(f, train_x)\n    with open(test_x_path, 'wb') as f:\n        numpy.save(f, test_x)\n    with open(train_y_path, 'wb') as f:\n        numpy.save(f, train_y)\n    with open(test_y_path, 'wb') as f:\n        numpy.save(f, test_y)\n\n\ndef train_knn(train_x_path: InputPath(), train_y_path: InputPath(), model_path: OutputPath()):\n    import numpy\n    import pickle\n    from sklearn.neighbors import KNeighborsClassifier\n    train_x = numpy.load(train_x_path)\n    train_y = numpy.load(train_y_path)\n    print(f\"Training data size - X: {train_x.size}, y: {train_y.size}\")\n    knn = KNeighborsClassifier()\n    knn.fit(train_x, train_y)\n    with open(model_path, 'wb') as f:\n        pickle.dump(knn, f)\n\ndef train_logistics(train_x_path: InputPath(), train_y_path: InputPath(), model_path: OutputPath()):\n    import numpy\n    import pickle\n    import random\n    from sklearn.linear_model import LogisticRegression\n    p = random.random()\n    print(p)\n    if p > 0.5:\n        raise Exception()\n    train_x = numpy.load(train_x_path)\n    train_y = numpy.load(train_y_path)\n    print(f\"Training data size - X: {train_x.size}, y: {train_y.size}\")\n    knn = LogisticRegression()\n    knn.fit(train_x, train_y)\n    with open(model_path, 'wb') as f:\n        pickle.dump(knn, f)\n\n\ndef test_model(test_x_path: InputPath(), test_y_path: InputPath(), model_path: InputPath()):\n    import numpy\n    import pickle\n    import random\n    from sklearn.metrics import classification_report\n    p = random.random()\n    print(p)\n    if p > 0.5:\n        raise Exception()\n    test_x = numpy.load(test_x_path)\n    test_y = numpy.load(test_y_path)\n    print(f\"Testing data size - X: {test_x.size}, y: {test_y.size}\")\n    with open(model_path, 'rb') as f:\n        model = pickle.load(f)\n    pred_y = model.predict(test_x)\n    print(classification_report(test_y, pred_y))\n\n@func_to_container_op\ndef select_classifier(minimum: int, maximum: int) -> int:\n    \"\"\"Generate a random number between minimum and maximum (inclusive).\"\"\"\n    import random\n    result = random.randint(minimum, maximum)\n    print(result)\n    return result\n\n@func_to_container_op\ndef fail_op(message):\n    \"\"\"Fails.\"\"\"\n    import sys\n    print(message)    \n    sys.exit(1)\n\ndef train_test_knn():\n\n    load_op = func_to_container_op(load_data, base_image='qiuosier/sklearn')\n    split_op = func_to_container_op(split_data, base_image='qiuosier/sklearn')\n    test_op = func_to_container_op(test_model, base_image='qiuosier/sklearn')\n    train_knn_op = func_to_container_op(train_knn, base_image='qiuosier/sklearn')\n    train_logistics_op = func_to_container_op(train_logistics, base_image='qiuosier/sklearn')\n    \n    load_task = load_op()\n\n    split_task = split_op(load_task.outputs['x'], load_task.outputs['y'])\n    split_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n\n    p = select_classifier(0, 9)\n    p.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n\n    with dsl.Condition(p.output >= 5):\n        train_task = train_knn_op(split_task.outputs['train_x'], split_task.outputs['train_y'])\n        test_op(split_task.outputs['test_x'], split_task.outputs['test_y'], train_task.output)\n    with dsl.Condition(p.output < 5):\n        train_task = train_logistics_op(split_task.outputs['train_x'], split_task.outputs['train_y'])\n        test_op(split_task.outputs['test_x'], split_task.outputs['test_y'], train_task.output)\n\n\n@dsl.pipeline(name='sklearn-iris', description='Classifying Iris data with KNN.')\ndef kf_pipeline():\n    train_test_knn()\n\n\n# if __name__ == '__main__':\n#     # Compile the pipeline\n#     kfp.compiler.Compiler().compile(kf_pipeline, __file__ + '.yaml')\n"
  },
  {
    "repo": "tiktakdad/kubeflow_pipeline",
    "file_path": "iris/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/tiktakdad/kubeflow_pipeline/master/iris/pipeline.py",
    "content": "import kfp\nimport kfp.components as comp\nfrom kubernetes.client.models import V1SecurityContext\nfrom kfp import dsl\n@dsl.pipeline(\n    name='tiktakdad-iris',\n    description='tiktakdad iris test'\n)\n\ndef soojin_pipeline():\n    user = 1000\n    add_p = dsl.ContainerOp(\n        name=\"load iris data pipeline\",\n        image=\"tiktakdad/iris-preprocessing@sha256:9afcd31ddd4f9068f09201669915911ca9e4504f6f993f514d26418b6627959a\",\n        arguments=[\n            '--data_path', './Iris.csv'\n        ],\n        file_outputs={'iris' : '/iris.csv'}\n    )\n    add_p.set_security_context(V1SecurityContext(run_as_user=user, run_as_group=0))\n\n    ml = dsl.ContainerOp(\n        name=\"training pipeline\",\n        image=\"tiktakdad/iris-training@sha256:5a316f66ad586d9494bc9b2b70225948337cbb1d1341e92fb8e83d70ff3f92d9\",\n        arguments=[\n            '--data', add_p.outputs['iris']\n        ]\n    )\n    ml.set_security_context(V1SecurityContext(run_as_user=user, run_as_group=0))\n\n    ml.after(add_p)\n    \nif __name__ == \"__main__\":\n    import kfp.compiler as compiler\n    compiler.Compiler().compile(test_pipeline, __file__ + \".tar.gz\")\n"
  },
  {
    "repo": "tiktakdad/kubeflow_pipeline",
    "file_path": "iris/pipeline_test.py",
    "raw_url": "https://raw.githubusercontent.com/tiktakdad/kubeflow_pipeline/master/iris/pipeline_test.py",
    "content": "from kfp import dsl\nfrom kubernetes.client.models import V1EnvVar, V1SecretKeySelector\n@dsl.pipeline(\n    name='foo',\n    description='hello world')\ndef foo_pipeline(tag: str, pull_image_policy: str):\n  # any attributes can be parameterized (both serialized string or actual PipelineParam)\n  op = dsl.ContainerOp(name='foo',\n                      image='busybox:%s' % tag,\n                      # pass in init_container list\n                      init_containers=[dsl.UserContainer('print', 'busybox:latest', command='echo \"hello\"')],\n                      # pass in sidecars list\n                      sidecars=[dsl.Sidecar('print', 'busybox:latest', command='echo \"hello\"')],\n                      # pass in k8s container kwargs\n                      container_kwargs={'env': [V1EnvVar('foo', 'bar')]},\n  )\n  # set `imagePullPolicy` property for `container` with `PipelineParam`\n  op.container.set_image_pull_policy(pull_image_policy)\n  # add sidecar with parameterized image tag\n  # sidecar follows the argo sidecar swagger spec\n  op.add_sidecar(dsl.Sidecar('redis', 'redis:%s' % tag).set_image_pull_policy('Always'))\n"
  },
  {
    "repo": "tiktakdad/kubeflow_pipeline",
    "file_path": "metrics_evaluation_and_check_condition/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/tiktakdad/kubeflow_pipeline/master/metrics_evaluation_and_check_condition/pipeline.py",
    "content": "import kfp\nimport kfp.components as comp\nfrom kfp import dsl\n\ndef print_op(msg):\n    \"\"\"Print a message.\"\"\"\n    return dsl.ContainerOp(\n        name='Print',\n        image='alpine:3.6',\n        command=['echo', msg],\n)\n\n@dsl.pipeline(\n    name='soojin-iris',\n    description='soojin iris test'\n)\n\ndef soojin_pipeline():\n    add_p = dsl.ContainerOp(\n        name=\"load iris data pipeline\",\n        image=\"lsjsj92/soojin-iris-load:0.7\",\n        arguments=[\n            '--data_path', './Iris.csv'\n        ],\n        file_outputs={'iris' : '/iris.csv'}\n    )\n\n    train_and_eval = dsl.ContainerOp(\n        name=\"training pipeline\",\n        image=\"lsjsj92/soojin-iris-train_and_eval:0.4\",\n        arguments=[\n            '--data', add_p.outputs['iris']\n        ],\n        file_outputs={\n            'accuracy' : '/accuracy.json',\n            'mlpipeline-metrics' : '/mlpipeline-metrics.json'\n        }\n    )\n\n    train_and_eval.after(add_p)\n    baseline = 0.7\n    with dsl.Condition(train_and_eval.outputs['accuracy'] > baseline ) as check_condition:\n        print_op(f\"accuracy\ub294 {train_and_eval.outputs['accuracy']}\ub85c accuracy baseline\uc778 {baseline}\ubcf4\ub2e4 \ud07d\ub2c8\ub2e4!\")\n    \n    with dsl.Condition(train_and_eval.outputs['accuracy'] < baseline) as check_condition:\n        print_op(f\"accuracy\ub294 {train_and_eval.outputs['accuracy']}\ub85c accuracy baseline\uc778 {baseline}\ubcf4\ub2e4 \uc791\uc2b5\ub2c8\ub2e4.\")\n    \nif __name__ == \"__main__\":\n    import kfp.compiler as compiler\n    compiler.Compiler().compile(soojin_pipeline, __file__ + \".tar.gz\")\n"
  },
  {
    "repo": "tiktakdad/kubeflow_pipeline",
    "file_path": "titanic/pipelines.py",
    "raw_url": "https://raw.githubusercontent.com/tiktakdad/kubeflow_pipeline/master/titanic/pipelines.py",
    "content": "import kfp\nimport kfp.components as comp\nfrom kfp import dsl\nfrom kfp import compiler\n\n@dsl.pipeline(\n    name='soojin-kubeflow-titanic',\n    description = \"This is a pipeline of titanic made by soojin(lsjsj92.tistory.com)\"\n)\ndef titanic_pipeline(\n    BUCKETNAME,\n    ACCESSKEY,\n    SECRETKEY,\n    REGIONNAME,\n    ORIKEY,\n    SAVEKEY,\n    MODELKEY):\n    \n    step_1_preprocessing = dsl.ContainerOp(\n        name = 'cleaning titanic data',\n        image = 'lsjsj92/titanic_preprocessing:latest',\n        arguments=[\n            '--bucket_name' , BUCKETNAME,\n            '--ACCESSKEY' , ACCESSKEY,\n            '--SECRETKEY' , SECRETKEY,\n            '--region_name' , REGIONNAME,\n            '--data_key' , ORIKEY,\n            '--save_key' , SAVEKEY\n        ]\n    )\n\n    step_2_training = dsl.ContainerOp(\n        name = 'training model',\n        image = 'lsjsj92/titanic_modeling:latest',\n        arguments=[\n            '--bucket_name' , BUCKETNAME,\n            '--ACCESSKEY', ACCESSKEY,\n            '--SECRETKEY', SECRETKEY,\n            '--region_name' , REGIONNAME,\n            '--model_key' , MODELKEY,\n            '--data' , SAVEKEY\n        ]\n    )\n\n    step_2_training.after(step_1_preprocessing)\n\ncompiler.Compiler().compile(titanic_pipeline, 'soojin_titanic.tar.gz')"
  },
  {
    "repo": "swe-train/kubeflow__pipelines",
    "file_path": "samples/test/after.py",
    "raw_url": "https://raw.githubusercontent.com/swe-train/kubeflow__pipelines/0.1.8/samples/test/after.py",
    "content": "# Copyright 2020 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom kfp import components\nfrom kfp.v2 import dsl\nimport kfp.v2.compiler as compiler\n\ncomponent_op = components.load_component_from_text(\n    \"\"\"\nname: Print Text\ninputs:\n- {name: text, type: String}\nimplementation:\n  container:\n    image: alpine\n    command:\n    - sh\n    - -c\n    - |\n      set -e -x\n      echo \"$0\"\n    - {inputValue: text}\n\"\"\"\n)\n\n\n@dsl.pipeline(name='pipeline-with-after')\ndef my_pipeline():\n    task1 = component_op(text='1st task')\n    task2 = component_op(text='2nd task').after(task1)\n    task3 = component_op(text='3rd task').after(task1, task2)\n\n\nif __name__ == '__main__':\n    compiler.Compiler().compile(\n        pipeline_func=my_pipeline,\n        pipeline_root='dummy_root',\n        output_path=__file__ + '.json'\n    )\n"
  },
  {
    "repo": "swe-train/kubeflow__pipelines",
    "file_path": "samples/test/after_test.py",
    "raw_url": "https://raw.githubusercontent.com/swe-train/kubeflow__pipelines/0.1.8/samples/test/after_test.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp\nfrom .after import my_pipeline\nfrom .util import run_pipeline_func, TestCase\n\nrun_pipeline_func([\n    TestCase(pipeline_func=my_pipeline),\n    TestCase(\n        pipeline_func=my_pipeline, mode=kfp.dsl.PipelineExecutionMode.V2_ENGINE\n    ),\n])\n"
  },
  {
    "repo": "swe-train/kubeflow__pipelines",
    "file_path": "samples/test/cache_v2_test.py",
    "raw_url": "https://raw.githubusercontent.com/swe-train/kubeflow__pipelines/0.1.8/samples/test/cache_v2_test.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Two step v2-compatible pipeline.\"\"\"\n\n# %%\n\nfrom __future__ import annotations\n\nimport random\nimport string\nimport unittest\nimport functools\n\nimport kfp\nimport kfp_server_api\n\nfrom .two_step import two_step_pipeline\nfrom .util import run_pipeline_func, TestCase, KfpMlmdClient, KfpTask\nfrom ml_metadata.proto import Execution\n\n\ndef verify_tasks(\n    t: unittest.TestCase, tasks: dict[str, KfpTask], task_state, uri: str,\n    some_int: int\n):\n    task_names = [*tasks.keys()]\n    t.assertCountEqual(task_names, ['train-op', 'preprocess'], 'task names')\n\n    preprocess = tasks['preprocess']\n    train = tasks['train-op']\n\n    t.assertEqual({\n        'name': 'preprocess',\n        'inputs': {\n            'artifacts': [],\n            'parameters': {\n                'some_int': some_int,\n                'uri': uri\n            }\n        },\n        'outputs': {\n            'artifacts': [{\n                'metadata': {\n                    'display_name': 'output_dataset_one',\n                },\n                'name': 'output_dataset_one',\n                'type': 'system.Dataset'\n            }],\n            'parameters': {\n                'output_parameter_one': some_int\n            }\n        },\n        'type': 'system.ContainerExecution',\n        'state': task_state,\n    }, preprocess.get_dict())\n    t.assertEqual({\n        'name': 'train-op',\n        'inputs': {\n            'artifacts': [{\n                'metadata': {\n                    'display_name': 'output_dataset_one',\n                },\n                'name': 'dataset',\n                'type': 'system.Dataset',\n            }],\n            'parameters': {\n                'num_steps': some_int\n            }\n        },\n        'outputs': {\n            'artifacts': [{\n                'metadata': {\n                    'display_name': 'model',\n                },\n                'name': 'model',\n                'type': 'system.Model',\n            }],\n            'parameters': {}\n        },\n        'type': 'system.ContainerExecution',\n        'state': task_state,\n    }, train.get_dict())\n\n\ndef verify(\n    run: kfp_server_api.ApiRun, mlmd_connection_config, uri: str, some_int,\n    state: int, **kwargs\n):\n    t = unittest.TestCase()\n    t.maxDiff = None  # we always want to see full diff\n    t.assertEqual(run.status, 'Succeeded')\n    client = KfpMlmdClient(mlmd_connection_config=mlmd_connection_config)\n    tasks = client.get_tasks(run_id=run.id)\n    verify_tasks(t, tasks, state, uri, some_int)\n\n\nif __name__ == '__main__':\n    letters = string.ascii_lowercase\n    random_uri = 'http://' + ''.join(random.choice(letters) for i in range(5))\n    random_int = random.randint(0, 10000)\n    run_pipeline_func([\n        TestCase(\n            pipeline_func=two_step_pipeline,\n            arguments={\n                'uri': f'{random_uri}',\n                'some_int': f'{random_int}'\n            },\n            verify_func=functools.partial(\n                verify,\n                uri=random_uri,\n                some_int=random_int,\n                state=Execution.State.COMPLETE,\n            ),\n            mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE,\n            enable_caching=True\n        ),\n    ]),\n    run_pipeline_func([\n        TestCase(\n            pipeline_func=two_step_pipeline,\n            arguments={\n                'uri': f'{random_uri}',\n                'some_int': f'{random_int}'\n            },\n            verify_func=functools.partial(\n                verify,\n                uri=random_uri,\n                some_int=random_int,\n                state=Execution.State.CACHED\n            ),\n            mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE,\n            enable_caching=True\n        ),\n    ])\n\n# %%\n"
  },
  {
    "repo": "swe-train/kubeflow__pipelines",
    "file_path": "samples/test/fail.py",
    "raw_url": "https://raw.githubusercontent.com/swe-train/kubeflow__pipelines/0.1.8/samples/test/fail.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Fail pipeline.\"\"\"\n\nfrom kfp import components, dsl\n\n\ndef fail():\n    '''Fails'''\n    import sys\n    sys.exit(1)\n\n\nfail_op = components.create_component_from_func(\n    fail, base_image='alpine:latest'\n)\n\n\n@dsl.pipeline(name='fail_pipeline')\ndef fail_pipeline():\n    fail_task = fail_op()\n"
  },
  {
    "repo": "swe-train/kubeflow__pipelines",
    "file_path": "samples/test/fail_parameter_value_missing.py",
    "raw_url": "https://raw.githubusercontent.com/swe-train/kubeflow__pipelines/0.1.8/samples/test/fail_parameter_value_missing.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom kfp import dsl, components\n\necho = components.load_component_from_text(\n    \"\"\"\nname: Echo\ninputs:\n- {name: text, type: String}\nimplementation:\n  container:\n    image: alpine\n    command:\n    - echo\n    - {inputValue: text}\n\"\"\"\n)\n\n\n@dsl.pipeline(name='parameter_value_missing')\ndef pipeline(\n    parameter:\n    str  # parameter should be specified when submitting, but we are missing it in the test\n):\n    echo_op = echo(text=parameter)\n"
  },
  {
    "repo": "swe-train/kubeflow__pipelines",
    "file_path": "samples/test/legacy_data_passing.py",
    "raw_url": "https://raw.githubusercontent.com/swe-train/kubeflow__pipelines/0.1.8/samples/test/legacy_data_passing.py",
    "content": "# This pipeline demonstrates and verifies all ways of data passing supported by KFP.\n\n# %% [markdown]\n# KFP has simple data passing model. There are three different kinds of arguments and tw different ways to consume an input argument. All combinations are supported.\n\n# Any input can be consumed:\n# * As value (`inputValue` placeholder)\n# * As file (`inputPath` placeholder).\n\n# Input argument can come from:\n# * Constant value\n# * Pipeline parameter\n# * Upstream component output\n\n# Combining these options there are 6 end-to-end data passing cases, each of which works regardless of type:\n\n# 1. Pass constant value which gets consumed as value. (Common)\n# 2. Pass constant value which gets consumed as file. (Often used in test pipelines)\n# 3. Pass pipeline parameter that gets consumed as value. (Common)\n# 4. Pass pipeline parameter that gets consumed as file. (Rare. Sometimes used with JSON parameters)\n# 5. Pass task output that gets consumed as value. (Common)\n# 6. Pass task output that gets consumed as file. (Common)\n\n# The only restriction on types is that when both upstream output and downstream input have types, the types must match.\n\n# %%\nimport kfp\nfrom kfp.components import create_component_from_func, InputPath, OutputPath\n\n\n# Components\n# Produce\n@create_component_from_func\ndef produce_anything(data_path: OutputPath()):\n    with open(data_path, \"w\") as f:\n        f.write(\"produce_anything\")\n\n\n@create_component_from_func\ndef produce_something(data_path: OutputPath(\"Something\")):\n    with open(data_path, \"w\") as f:\n        f.write(\"produce_something\")\n\n\n@create_component_from_func\ndef produce_something2() -> 'Something':\n    return \"produce_something2\"\n\n\n@create_component_from_func\ndef produce_string() -> str:\n    return \"produce_string\"\n\n\n# Consume as value\n@create_component_from_func\ndef consume_anything_as_value(data):\n    print(\"consume_anything_as_value: \" + data)\n\n\n@create_component_from_func\ndef consume_something_as_value(data: \"Something\"):\n    print(\"consume_something_as_value: \" + data)\n\n\n@create_component_from_func\ndef consume_string_as_value(data: str):\n    print(\"consume_string_as_value: \" + data)\n\n\n# Consume as file\n@create_component_from_func\ndef consume_anything_as_file(data_path: InputPath()):\n    with open(data_path) as f:\n        print(\"consume_anything_as_file: \" + f.read())\n\n\n@create_component_from_func\ndef consume_something_as_file(data_path: InputPath('Something')):\n    with open(data_path) as f:\n        print(\"consume_something_as_file: \" + f.read())\n\n\n@create_component_from_func\ndef consume_string_as_file(data_path: InputPath(str)):\n    with open(data_path) as f:\n        print(\"consume_string_as_file: \" + f.read())\n\n\n# Pipeline\n@kfp.dsl.pipeline(name='data_passing_pipeline')\ndef data_passing_pipeline(\n    anything_param=\"anything_param\",\n    something_param: \"Something\" = \"something_param\",\n    string_param: str = \"string_param\",\n):\n    produced_anything = produce_anything().output\n    produced_something = produce_something().output\n    produced_string = produce_string().output\n\n    # Pass constant value; consume as value\n    consume_anything_as_value(\"constant\")\n    consume_something_as_value(\"constant\")\n    consume_string_as_value(\"constant\")\n\n    # Pass constant value; consume as file\n    consume_anything_as_file(\"constant\")\n    consume_something_as_file(\"constant\")\n    consume_string_as_file(\"constant\")\n\n    # Pass pipeline parameter; consume as value\n    consume_anything_as_value(anything_param)\n    consume_anything_as_value(something_param)\n    consume_anything_as_value(string_param)\n    consume_something_as_value(anything_param)\n    consume_something_as_value(something_param)\n    consume_string_as_value(anything_param)\n    consume_string_as_value(string_param)\n\n    # Pass pipeline parameter; consume as file\n    consume_anything_as_file(anything_param)\n    consume_anything_as_file(something_param)\n    consume_anything_as_file(string_param)\n    consume_something_as_file(anything_param)\n    consume_something_as_file(something_param)\n    consume_string_as_file(anything_param)\n    consume_string_as_file(string_param)\n\n    # Pass task output; consume as value\n    consume_anything_as_value(produced_anything)\n    consume_anything_as_value(produced_something)\n    consume_anything_as_value(produced_string)\n    consume_something_as_value(produced_anything)\n    consume_something_as_value(produced_something)\n    consume_string_as_value(produced_anything)\n    consume_string_as_value(produced_string)\n\n    # Pass task output; consume as file\n    consume_anything_as_file(produced_anything)\n    consume_anything_as_file(produced_something)\n    consume_anything_as_file(produced_string)\n    consume_something_as_file(produced_anything)\n    consume_something_as_file(produced_something)\n    consume_string_as_file(produced_anything)\n    consume_string_as_file(produced_string)\n\n\nif __name__ == \"__main__\":\n    kfp_endpoint = None\n    kfp.Client(host=kfp_endpoint).create_run_from_pipeline_func(\n        data_passing_pipeline, arguments={})\n"
  },
  {
    "repo": "swe-train/kubeflow__pipelines",
    "file_path": "samples/test/legacy_data_passing_test.py",
    "raw_url": "https://raw.githubusercontent.com/swe-train/kubeflow__pipelines/0.1.8/samples/test/legacy_data_passing_test.py",
    "content": "from .data_passing import data_passing_pipeline\nfrom .util import run_pipeline_func, TestCase,\nfrom kfp.dsl import PipelineExecutionMode\n\nrun_pipeline_func([\n    TestCase(\n        pipeline_func=data_passing_pipeline,\n        mode=PipelineExecutionMode.V1_LEGACY,\n    )\n])\n"
  },
  {
    "repo": "swe-train/kubeflow__pipelines",
    "file_path": "samples/test/legacy_exit_handler.py",
    "raw_url": "https://raw.githubusercontent.com/swe-train/kubeflow__pipelines/0.1.8/samples/test/legacy_exit_handler.py",
    "content": "#!/usr/bin/env python3\n# Copyright 2019 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp\nfrom kfp import dsl\n\n\ndef gcs_download_op(url):\n    return dsl.ContainerOp(\n        name='GCS - Download',\n        image='google/cloud-sdk:279.0.0',\n        command=['sh', '-ce'],\n        arguments=['gsutil cp $0 $1 && cat $1', url, '/tmp/results.txt'],\n        file_outputs={\n            'data': '/tmp/results.txt',\n        }\n    )\n\n\ndef echo_op(text):\n    return dsl.ContainerOp(\n        name='echo',\n        image='library/bash:4.4.23',\n        command=['sh', '-cex'],\n        arguments=['echo \"$0\"', text],\n    )\n\n\n@dsl.pipeline(\n    name='Exit Handler',\n    description=\n    'Downloads a message and prints it. The exit handler will run after the pipeline finishes (successfully or not).'\n)\ndef download_and_print(url='gs://ml-pipeline/shakespeare1.txt'):\n    \"\"\"A sample pipeline showing exit handler.\"\"\"\n\n    exit_task = echo_op('exit!')\n\n    with dsl.ExitHandler(exit_task):\n        download_task = gcs_download_op(url)\n        echo_task = echo_op(download_task.output)\n\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(download_and_print, __file__ + '.yaml')\n"
  },
  {
    "repo": "swe-train/kubeflow__pipelines",
    "file_path": "samples/test/legacy_exit_handler_test.py",
    "raw_url": "https://raw.githubusercontent.com/swe-train/kubeflow__pipelines/0.1.8/samples/test/legacy_exit_handler_test.py",
    "content": "#!/usr/bin/env python3\n# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp\nfrom .legacy_exit_handler import download_and_print\nfrom .util import run_pipeline_func, TestCase\n\n\nrun_pipeline_func([\n    # This sample is expected to not work on v2 compatible mode.\n    TestCase(\n        pipeline_func=download_and_print,\n        mode=kfp.dsl.PipelineExecutionMode.V1_LEGACY\n    )\n])\n"
  },
  {
    "repo": "swe-train/kubeflow__pipelines",
    "file_path": "samples/test/lightweight_python_functions_v2_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/swe-train/kubeflow__pipelines/0.1.8/samples/test/lightweight_python_functions_v2_pipeline.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Sample pipeline for passing data in KFP v2.\"\"\"\nfrom typing import Dict, List\nimport os\n\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import Input, InputPath, Output, OutputPath, Dataset, Model, component\nimport kfp.v2.compiler as compiler\n\n# In tests, we install a KFP package from the PR under test. Users should not\n# normally need to specify `kfp_package_path` in their component definitions.\n_KFP_PACKAGE_PATH = os.getenv('KFP_PACKAGE_PATH')\n\n\n@component(kfp_package_path=_KFP_PACKAGE_PATH)\ndef preprocess(\n    # An input parameter of type string.\n    message: str,\n    # Use Output[T] to get a metadata-rich handle to the output artifact\n    # of type `Dataset`.\n    output_dataset_one: Output[Dataset],\n    # A locally accessible filepath for another output artifact of type\n    # `Dataset`.\n    output_dataset_two_path: OutputPath('Dataset'),\n    # A locally accessible filepath for an output parameter of type string.\n    output_parameter_path: OutputPath(str),\n    # A locally accessible filepath for an output parameter of type bool.\n    output_bool_parameter_path: OutputPath(bool),\n    # A locally accessible filepath for an output parameter of type dict.\n    output_dict_parameter_path: OutputPath(Dict[str, int]),\n    # A locally accessible filepath for an output parameter of type list.\n    output_list_parameter_path: OutputPath(List[str]),\n    # An input message that defaults to empty.\n    empty_message: str = \"\",\n):\n    \"\"\"Dummy preprocessing step\"\"\"\n\n    # Use Dataset.path to access a local file path for writing.\n    # One can also use Dataset.uri to access the actual URI file path.\n    with open(output_dataset_one.path, 'w') as f:\n        f.write(message)\n\n    # OutputPath is used to just pass the local file path of the output artifact\n    # to the function.\n    with open(output_dataset_two_path, 'w') as f:\n        f.write(message)\n\n    with open(output_parameter_path, 'w') as f:\n        f.write(message)\n\n    with open(output_bool_parameter_path, 'w') as f:\n        f.write(\n            str(True)\n        )  # use either `str()` or `json.dumps()` for bool values.\n\n    import json\n    with open(output_dict_parameter_path, 'w') as f:\n        f.write(json.dumps({'A': 1, 'B': 2}))\n\n    with open(output_list_parameter_path, 'w') as f:\n        f.write(json.dumps(['a', 'b', 'c']))\n\n\n@component(kfp_package_path=_KFP_PACKAGE_PATH)\ndef train(\n    # Use InputPath to get a locally accessible path for the input artifact\n    # of type `Dataset`.\n    dataset_one_path: InputPath('Dataset'),\n    # Use Input[T] to get a metadata-rich handle to the input artifact\n    # of type `Dataset`.\n    dataset_two: Input[Dataset],\n    # An input parameter of type string.\n    message: str,\n    # Use Output[T] to get a metadata-rich handle to the output artifact\n    # of type `Dataset`.\n    model: Output[Model],\n    # An input parameter of type bool.\n    input_bool: bool,\n    # An input parameter of type dict.\n    input_dict: Dict[str, int],\n    # An input parameter of type List[str].\n    input_list: List[str],\n    # An input parameter of type int with a default value.\n    num_steps: int = 100,\n):\n    \"\"\"Dummy Training step\"\"\"\n    with open(dataset_one_path, 'r') as input_file:\n        dataset_one_contents = input_file.read()\n\n    with open(dataset_two.path, 'r') as input_file:\n        dataset_two_contents = input_file.read()\n\n    line = (\n        f'dataset_one_contents: {dataset_one_contents} || '\n        f'dataset_two_contents: {dataset_two_contents} || '\n        f'message: {message} || '\n        f'input_bool: {input_bool}, type {type(input_bool)} || '\n        f'input_dict: {input_dict}, type {type(input_dict)} || '\n        f'input_list: {input_list}, type {type(input_list)} \\n'\n    )\n\n    with open(model.path, 'w') as output_file:\n        for i in range(num_steps):\n            output_file.write('Step {}\\n{}\\n=====\\n'.format(i, line))\n\n    # Use model.get() to get a Model artifact, which has a .metadata dictionary\n    # to store arbitrary metadata for the output artifact.\n    model.metadata['accuracy'] = 0.9\n\n\n@dsl.pipeline(name='my-test-pipeline-beta')\ndef pipeline(message: str = 'message'):\n    preprocess_task = preprocess(message=message)\n    train_task = train(\n        dataset_one_path=preprocess_task.outputs['output_dataset_one'],\n        dataset_two=preprocess_task.outputs['output_dataset_two_path'],\n        message=preprocess_task.outputs['output_parameter_path'],\n        input_bool=preprocess_task.outputs['output_bool_parameter_path'],\n        input_dict=preprocess_task.outputs['output_dict_parameter_path'],\n        input_list=preprocess_task.outputs['output_list_parameter_path'],\n    )\n\n\nif __name__ == '__main__':\n    compiler.Compiler().compile(\n        pipeline_func=pipeline, package_path=__file__.replace('.py', '.json')\n    )\n"
  },
  {
    "repo": "swe-train/kubeflow__pipelines",
    "file_path": "samples/test/lightweight_python_functions_v2_pipeline_test.py",
    "raw_url": "https://raw.githubusercontent.com/swe-train/kubeflow__pipelines/0.1.8/samples/test/lightweight_python_functions_v2_pipeline_test.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nfrom pprint import pprint\nimport kfp_server_api\nimport kfp.dsl as dsl\n\nfrom .lightweight_python_functions_v2_pipeline import pipeline\nfrom .util import run_pipeline_func, TestCase, KfpMlmdClient\nfrom ml_metadata.proto import Execution\n\n\ndef verify(run: kfp_server_api.ApiRun, mlmd_connection_config, **kwargs):\n    t = unittest.TestCase()\n    t.maxDiff = None  # we always want to see full diff\n    t.assertEqual(run.status, 'Succeeded')\n    client = KfpMlmdClient(mlmd_connection_config=mlmd_connection_config)\n    tasks = client.get_tasks(run_id=run.id)\n\n    task_names = [*tasks.keys()]\n    t.assertCountEqual(task_names, ['preprocess', 'train'], 'task names')\n    pprint(tasks)\n\n    preprocess = tasks['preprocess']\n    train = tasks['train']\n    pprint(preprocess.get_dict())\n    t.assertEqual(\n        {\n            'inputs': {\n                'artifacts': [],\n                'parameters': {\n                    'message': 'message',\n                    'empty_message': '',\n                }\n            },\n            'name': 'preprocess',\n            'outputs': {\n                'artifacts': [{\n                    'metadata': {\n                        'display_name': 'output_dataset_one'\n                    },\n                    'name': 'output_dataset_one',\n                    'type': 'system.Dataset'\n                }, {\n                    'metadata': {\n                        'display_name': 'output_dataset_two_path'\n                    },\n                    'name': 'output_dataset_two_path',\n                    'type': 'system.Dataset'\n                }],\n                'parameters': {\n                    'output_bool_parameter_path': 'True',\n                    'output_dict_parameter_path': '{\"A\": 1, \"B\": 2}',\n                    'output_list_parameter_path': '[\"a\", \"b\", \"c\"]',\n                    'output_parameter_path': 'message'\n                }\n            },\n            'type': 'system.ContainerExecution',\n            'state': Execution.State.COMPLETE,\n        },\n        preprocess.get_dict(),\n    )\n    t.assertEqual(\n        {\n            'inputs': {\n                'artifacts': [{\n                    'metadata': {\n                        'display_name': 'output_dataset_one'\n                    },\n                    'name': 'dataset_one_path',\n                    'type': 'system.Dataset'\n                }, {\n                    'metadata': {\n                        'display_name': 'output_dataset_two_path'\n                    },\n                    'name': 'dataset_two',\n                    'type': 'system.Dataset'\n                }],\n                'parameters': {\n                    'input_bool': 'True',\n                    'input_dict': '{\"A\": 1, \"B\": 2}',\n                    'input_list': '[\"a\", \"b\", \"c\"]',\n                    'message': 'message',\n                    'num_steps': 100,\n                }\n            },\n            'name': 'train',\n            'outputs': {\n                'artifacts': [{\n                    'metadata': {\n                        'display_name': 'model',\n                        'accuracy': 0.9,\n                    },\n                    'name': 'model',\n                    'type': 'system.Model'\n                }],\n                'parameters': {}\n            },\n            'type': 'system.ContainerExecution',\n            'state': Execution.State.COMPLETE,\n        },\n        train.get_dict(),\n    )\n\n\nrun_pipeline_func([\n    TestCase(\n        pipeline_func=pipeline,\n        verify_func=verify,\n        mode=dsl.PipelineExecutionMode.V2_COMPATIBLE\n    ),\n    TestCase(\n        pipeline_func=pipeline,\n        verify_func=verify,\n        mode=dsl.PipelineExecutionMode.V2_ENGINE\n    ),\n])\n"
  },
  {
    "repo": "swe-train/kubeflow__pipelines",
    "file_path": "samples/test/lightweight_python_functions_v2_with_outputs.py",
    "raw_url": "https://raw.githubusercontent.com/swe-train/kubeflow__pipelines/0.1.8/samples/test/lightweight_python_functions_v2_with_outputs.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Lightweight functions v2 with outputs.\"\"\"\nfrom typing import NamedTuple\nimport os\nfrom kfp import components, dsl\nfrom kfp.v2 import compiler\nfrom kfp.v2.dsl import Input, Dataset, Model, Metrics, component\n\n# In tests, we install a KFP package from the PR under test. Users should not\n# normally need to specify `kfp_package_path` in their component definitions.\n_KFP_PACKAGE_PATH = os.getenv('KFP_PACKAGE_PATH')\n\n\n@component(kfp_package_path=_KFP_PACKAGE_PATH)\ndef concat_message(first: str, second: str) -> str:\n    return first + second\n\n\n@component(kfp_package_path=_KFP_PACKAGE_PATH)\ndef add_numbers(first: int, second: int) -> int:\n    return first + second\n\n\n@component(kfp_package_path=_KFP_PACKAGE_PATH)\ndef output_artifact(number: int, message: str) -> Dataset:\n    result = [message for _ in range(number)]\n    return '\\n'.join(result)\n\n\n@component(kfp_package_path=_KFP_PACKAGE_PATH)\ndef output_named_tuple(\n    artifact: Input[Dataset]\n) -> NamedTuple(\n        'Outputs', [\n            ('scalar', str),\n            ('metrics', Metrics),\n            ('model', Model),\n        ]\n):\n    scalar = \"123\"\n\n    import json\n    metrics = json.dumps({\n        'metrics': [{\n            'name': 'accuracy',\n            'numberValue': 0.9,\n            'format': \"PERCENTAGE\",\n        }]\n    })\n\n    with open(artifact.path, 'r') as f:\n        artifact_contents = f.read()\n    model = \"Model contents: \" + artifact_contents\n\n    from collections import namedtuple\n    output = namedtuple('Outputs', ['scalar', 'metrics', 'model'])\n    return output(scalar, metrics, model)\n\n\n@dsl.pipeline(name='functions-with-outputs')\ndef pipeline(\n    first_message: str = 'first',\n    second_message: str = 'second',\n    first_number: int = 1,\n    second_number: int = 2,\n):\n    concat_task = concat_message(first=first_message, second=second_message)\n    add_numbers_task = add_numbers(first=first_number, second=second_number)\n    output_artifact_task = output_artifact(\n        number=add_numbers_task.output, message=concat_task.output\n    )\n    output_name_tuple = output_named_tuple(output_artifact_task.output)\n\n\nif __name__ == '__main__':\n    compiler.Compiler().compile(\n        pipeline_func=pipeline,\n        pipeline_root='dummy_root',\n        output_path=__file__ + '.json'\n    )\n"
  },
  {
    "repo": "swe-train/kubeflow__pipelines",
    "file_path": "samples/test/lightweight_python_functions_v2_with_outputs_test.py",
    "raw_url": "https://raw.githubusercontent.com/swe-train/kubeflow__pipelines/0.1.8/samples/test/lightweight_python_functions_v2_with_outputs_test.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom pprint import pprint\nimport unittest\nimport kfp\nimport kfp_server_api\nimport os\nfrom minio import Minio\n\nfrom .lightweight_python_functions_v2_with_outputs import pipeline\nfrom .util import KfpMlmdClient, run_pipeline_func, TestCase\n\n\ndef verify(run: kfp_server_api.ApiRun, mlmd_connection_config, **kwargs):\n    t = unittest.TestCase()\n    t.maxDiff = None  # we always want to see full diff\n    t.assertEqual(run.status, 'Succeeded')\n    client = KfpMlmdClient(mlmd_connection_config=mlmd_connection_config)\n    tasks = client.get_tasks(run_id=run.id)\n    pprint(tasks)\n\n    output_artifact = tasks['output-artifact']\n    output = [\n        a for a in output_artifact.outputs.artifacts if a.name == 'Output'\n    ][0]\n    pprint(output)\n\n    host = os.environ['MINIO_SERVICE_SERVICE_HOST']\n    port = os.environ['MINIO_SERVICE_SERVICE_PORT']\n    minio = Minio(\n        f'{host}:{port}',\n        access_key='minio',\n        secret_key='minio123',\n        secure=False\n    )\n    bucket, key = output.uri[len('minio://'):].split('/', 1)\n    print(f'bucket={bucket} key={key}')\n    response = minio.get_object(bucket, key)\n    data = response.read().decode('UTF-8')\n    t.assertEqual(data, 'firstsecond\\nfirstsecond\\nfirstsecond')\n\n\nrun_pipeline_func([\n    TestCase(\n        pipeline_func=pipeline,\n        mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE\n    ),\n    TestCase(\n        pipeline_func=pipeline,\n        mode=kfp.dsl.PipelineExecutionMode.V2_ENGINE,\n    ),\n    # Verify overriding pipeline root to MinIO\n    TestCase(\n        pipeline_func=pipeline,\n        verify_func=verify,\n        mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE,\n        arguments={\n            kfp.dsl.ROOT_PARAMETER_NAME: 'minio://mlpipeline/override/artifacts'\n        },\n    ),\n])\n"
  },
  {
    "repo": "swe-train/kubeflow__pipelines",
    "file_path": "samples/test/metrics_visualization_v1.py",
    "raw_url": "https://raw.githubusercontent.com/swe-train/kubeflow__pipelines/0.1.8/samples/test/metrics_visualization_v1.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp.dsl as dsl\nfrom ..core.visualization.confusion_matrix import confusion_visualization\nfrom ..core.visualization.html import html_visualization\nfrom ..core.visualization.markdown import markdown_visualization\nfrom ..core.visualization.roc import roc_visualization\nfrom ..core.visualization.table import table_visualization\n\n# Note: This test is to verify that visualization metrics on V1 is runnable by KFP.\n# However, this pipeline is only runnable on V1 mode, but not V2 compatible mode.\n\n@dsl.pipeline(\n    name='metrics-visualization-v1-pipeline')\ndef metrics_visualization_v1_pipeline():\n    confusion_visualization_task = confusion_visualization()\n    html_visualization_task = html_visualization(\"\")\n    markdown_visualization_task = markdown_visualization()\n    roc_visualization_task = roc_visualization()\n    table_visualization_task = table_visualization()\n"
  },
  {
    "repo": "swe-train/kubeflow__pipelines",
    "file_path": "samples/test/metrics_visualization_v1_test.py",
    "raw_url": "https://raw.githubusercontent.com/swe-train/kubeflow__pipelines/0.1.8/samples/test/metrics_visualization_v1_test.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp_server_api\nimport unittest\nfrom pprint import pprint\nfrom .metrics_visualization_v1 import metrics_visualization_v1_pipeline\nfrom .util import KfpMlmdClient, run_pipeline_func, TestCase\n\nimport kfp\nimport kfp_server_api\n\n\ndef verify(\n    run: kfp_server_api.ApiRun, mlmd_connection_config, argo_workflow_name: str,\n    **kwargs\n):\n    t = unittest.TestCase()\n    t.maxDiff = None  # we always want to see full diff\n    t.assertEqual(run.status, 'Succeeded')\n    client = KfpMlmdClient(mlmd_connection_config=mlmd_connection_config)\n    tasks = client.get_tasks(argo_workflow_name=argo_workflow_name)\n    pprint(tasks)\n\n    table_visualization = tasks['table-visualization']\n    markdown_visualization = tasks['markdown-visualization']\n    roc_visualization = tasks['roc-visualization']\n    html_visualization = tasks['html-visualization']\n    confusion_visualization = tasks['confusion-visualization']\n\n    t.assertEqual(table_visualization.get_dict()['outputs']['artifacts'][0]['name'],\n                  'mlpipeline_ui_metadata')\n    t.assertEqual(markdown_visualization.get_dict()['outputs']['artifacts'][0]['name'],\n                  'mlpipeline_ui_metadata')\n    t.assertEqual(roc_visualization.get_dict()['outputs']['artifacts'][0]['name'],\n                  'mlpipeline_ui_metadata')\n    t.assertEqual(html_visualization.get_dict()['outputs']['artifacts'][0]['name'],\n                  'mlpipeline_ui_metadata')\n    t.assertEqual(confusion_visualization.get_dict()['outputs']['artifacts'][0]['name'],\n                  'mlpipeline_ui_metadata')\n\n\nrun_pipeline_func([TestCase(pipeline_func=metrics_visualization_v1_pipeline,\n                            mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE\n                            )])\n"
  },
  {
    "repo": "swe-train/kubeflow__pipelines",
    "file_path": "samples/test/metrics_visualization_v2.py",
    "raw_url": "https://raw.githubusercontent.com/swe-train/kubeflow__pipelines/0.1.8/samples/test/metrics_visualization_v2.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport os\n\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import (\n    component,\n    Output,\n    ClassificationMetrics,\n    Metrics,\n    HTML,\n    Markdown\n)\n\n# In tests, we install a KFP package from the PR under test. Users should not\n# normally need to specify `kfp_package_path` in their component definitions.\n_KFP_PACKAGE_PATH = os.getenv('KFP_PACKAGE_PATH')\n\n\n@component(\n    packages_to_install=['sklearn'],\n    base_image='python:3.9',\n    kfp_package_path=_KFP_PACKAGE_PATH,\n)\ndef digit_classification(metrics: Output[Metrics]):\n    from sklearn import model_selection\n    from sklearn.linear_model import LogisticRegression\n    from sklearn import datasets\n    from sklearn.metrics import accuracy_score\n\n    # Load digits dataset\n    iris = datasets.load_iris()\n\n    # # Create feature matrix\n    X = iris.data\n\n    # Create target vector\n    y = iris.target\n\n    #test size\n    test_size = 0.33\n\n    seed = 7\n    #cross-validation settings\n    kfold = model_selection.KFold(n_splits=10, random_state=seed, shuffle=True)\n\n    #Model instance\n    model = LogisticRegression()\n    scoring = 'accuracy'\n    results = model_selection.cross_val_score(model, X, y, cv=kfold, scoring=scoring)\n\n    #split data\n    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=test_size, random_state=seed)\n    #fit model\n    model.fit(X_train, y_train)\n\n    #accuracy on test set\n    result = model.score(X_test, y_test)\n    metrics.log_metric('accuracy', (result*100.0))\n\n@component(\n    packages_to_install=['sklearn'],\n    base_image='python:3.9',\n    kfp_package_path=_KFP_PACKAGE_PATH,\n)\ndef wine_classification(metrics: Output[ClassificationMetrics]):\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.metrics import roc_curve\n    from sklearn.datasets import load_wine\n    from sklearn.model_selection import train_test_split, cross_val_predict\n\n    X, y = load_wine(return_X_y=True)\n    # Binary classification problem for label 1.\n    y = y == 1\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n    rfc = RandomForestClassifier(n_estimators=10, random_state=42)\n    rfc.fit(X_train, y_train)\n    y_scores = cross_val_predict(rfc, X_train, y_train, cv=3, method='predict_proba')\n    y_predict = cross_val_predict(rfc, X_train, y_train, cv=3, method='predict')\n    fpr, tpr, thresholds = roc_curve(y_true=y_train, y_score=y_scores[:,1], pos_label=True)\n    metrics.log_roc_curve(fpr, tpr, thresholds)\n\n@component(\n    packages_to_install=['sklearn'],\n    base_image='python:3.9',\n    kfp_package_path=_KFP_PACKAGE_PATH,\n)\ndef iris_sgdclassifier(test_samples_fraction: float, metrics: Output[ClassificationMetrics]):\n    from sklearn import datasets, model_selection\n    from sklearn.linear_model import SGDClassifier\n    from sklearn.metrics import confusion_matrix\n\n    iris_dataset = datasets.load_iris()\n    train_x, test_x, train_y, test_y = model_selection.train_test_split(\n        iris_dataset['data'], iris_dataset['target'], test_size=test_samples_fraction)\n\n\n    classifier = SGDClassifier()\n    classifier.fit(train_x, train_y)\n    predictions = model_selection.cross_val_predict(classifier, train_x, train_y, cv=3)\n    metrics.log_confusion_matrix(\n        ['Setosa', 'Versicolour', 'Virginica'],\n        confusion_matrix(train_y, predictions).tolist() # .tolist() to convert np array to list.\n    )\n\n@component(\n    kfp_package_path=_KFP_PACKAGE_PATH,\n)\ndef html_visualization(html_artifact: Output[HTML]):\n    html_content = '<!DOCTYPE html><html><body><h1>Hello world</h1></body></html>'\n    with open(html_artifact.path, 'w') as f:\n        f.write(html_content)\n\n@component(\n    kfp_package_path=_KFP_PACKAGE_PATH,\n)\ndef markdown_visualization(markdown_artifact: Output[Markdown]):\n    markdown_content = '## Hello world \\n\\n Markdown content'\n    with open(markdown_artifact.path, 'w') as f:\n        f.write(markdown_content)\n\n@dsl.pipeline(\n    name='metrics-visualization-pipeline')\ndef metrics_visualization_pipeline():\n    wine_classification_op = wine_classification()\n    iris_sgdclassifier_op = iris_sgdclassifier(test_samples_fraction=0.3)\n    digit_classification_op = digit_classification()\n    html_visualization_op = html_visualization()\n    markdown_visualization_op = markdown_visualization()\n"
  },
  {
    "repo": "swe-train/kubeflow__pipelines",
    "file_path": "samples/test/metrics_visualization_v2_test.py",
    "raw_url": "https://raw.githubusercontent.com/swe-train/kubeflow__pipelines/0.1.8/samples/test/metrics_visualization_v2_test.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport unittest.mock as mock\nfrom pprint import pprint\nimport kfp\nimport kfp_server_api\n\nfrom .metrics_visualization_v2 import metrics_visualization_pipeline\nfrom .util import run_pipeline_func, TestCase, KfpMlmdClient\nfrom ml_metadata.proto import Execution\n\n\ndef verify(run: kfp_server_api.ApiRun, mlmd_connection_config, **kwargs):\n    t = unittest.TestCase()\n    t.maxDiff = None  # we always want to see full diff\n    t.assertEqual(run.status, 'Succeeded')\n    client = KfpMlmdClient(mlmd_connection_config=mlmd_connection_config)\n    tasks = client.get_tasks(run_id=run.id)\n\n    task_names = [*tasks.keys()]\n    t.assertCountEqual(\n        task_names,\n        ['wine-classification', 'iris-sgdclassifier', 'digit-classification', 'html-visualization', 'markdown-visualization'],\n        'task names'\n    )\n\n    wine_classification = tasks['wine-classification']\n    iris_sgdclassifier = tasks['iris-sgdclassifier']\n    digit_classification = tasks['digit-classification']\n    html_visualization = tasks['html-visualization']\n    markdown_visualization = tasks['markdown-visualization']\n\n    pprint('======= wine classification task =======')\n    pprint(wine_classification.get_dict())\n    pprint('======= iris sgdclassifier task =======')\n    pprint(iris_sgdclassifier.get_dict())\n    pprint('======= digit classification task =======')\n    pprint(digit_classification.get_dict())\n    pprint('======= HTML task =======')\n    pprint(html_visualization.get_dict())\n    pprint('======= Markdown task =======')\n    pprint(markdown_visualization.get_dict())\n\n    t.assertEqual({\n        'inputs': {\n            'artifacts': [],\n            'parameters': {}\n        },\n        'name': 'wine-classification',\n        'outputs': {\n            'artifacts': [{\n                'metadata': {\n                    'display_name': 'metrics',\n                    'confidenceMetrics': {\n                        'list': [{\n                            'confidenceThreshold': 2.0,\n                            'falsePositiveRate': 0.0,\n                            'recall': 0.0\n                        }, {\n                            'confidenceThreshold': 1.0,\n                            'falsePositiveRate': 0.0,\n                            'recall': 0.33962264150943394\n                        }, {\n                            'confidenceThreshold': 0.9,\n                            'falsePositiveRate': 0.0,\n                            'recall': 0.6037735849056604\n                        }, {\n                            'confidenceThreshold': 0.8,\n                            'falsePositiveRate': 0.0,\n                            'recall': 0.8490566037735849\n                        }, {\n                            'confidenceThreshold': 0.6,\n                            'falsePositiveRate': 0.0,\n                            'recall': 0.8867924528301887\n                        }, {\n                            'confidenceThreshold': 0.5,\n                            'falsePositiveRate': 0.0125,\n                            'recall': 0.9245283018867925\n                        }, {\n                            'confidenceThreshold': 0.4,\n                            'falsePositiveRate': 0.075,\n                            'recall': 0.9622641509433962\n                        }, {\n                            'confidenceThreshold': 0.3,\n                            'falsePositiveRate': 0.0875,\n                            'recall': 1.0\n                        }, {\n                            'confidenceThreshold': 0.2,\n                            'falsePositiveRate': 0.2375,\n                            'recall': 1.0\n                        }, {\n                            'confidenceThreshold': 0.1,\n                            'falsePositiveRate': 0.475,\n                            'recall': 1.0\n                        }, {\n                            'confidenceThreshold': 0.0,\n                            'falsePositiveRate': 1.0,\n                            'recall': 1.0\n                        }]\n                    }\n                },\n                'name': 'metrics',\n                'type': 'system.ClassificationMetrics'\n            }],\n            'parameters': {}\n        },\n        'type': 'system.ContainerExecution',\n        'state': Execution.State.COMPLETE,\n    }, wine_classification.get_dict())\n    t.assertEqual(\n        {\n            'inputs': {\n                'artifacts': [],\n                'parameters': {\n                    'test_samples_fraction': 0.3\n                }\n            },\n            'name': 'iris-sgdclassifier',\n            'outputs': {\n                'artifacts': [{\n                    'metadata': {\n                        'display_name': 'metrics',\n                        'confusionMatrix': {'struct': {\n                            'annotationSpecs': [{\n                                'displayName': 'Setosa'\n                            }, {\n                                'displayName': 'Versicolour'\n                            }, {\n                                'displayName': 'Virginica'\n                            }],\n                            'rows': [{ # these numbers can be random during execution\n                                'row': [mock.ANY, mock.ANY, mock.ANY]\n                            }, {\n                                'row': [mock.ANY, mock.ANY, mock.ANY]\n                            }, {\n                                'row': [mock.ANY, mock.ANY, mock.ANY]\n                            }]\n                        }}\n                    },\n                    'name': 'metrics',\n                    'type': 'system.ClassificationMetrics'\n                }],\n                'parameters': {}\n            },\n            'type': 'system.ContainerExecution',\n            'state': Execution.State.COMPLETE,\n        }, iris_sgdclassifier.get_dict()\n    )\n    rows = iris_sgdclassifier.get_dict()['outputs']['artifacts'][0]['metadata'][\n        'confusionMatrix']['struct']['rows']\n    for i, row in enumerate(rows):\n        for j, item in enumerate(row['row']):\n            t.assertIsInstance(\n                item, float,\n                f'value of confusion matrix row {i}, col {j} is not a number'\n            )\n\n    t.assertEqual({\n        'inputs': {\n            'artifacts': [],\n            'parameters': {}\n        },\n        'name': 'digit-classification',\n        'outputs': {\n            'artifacts': [{\n                'metadata': {\n                    'display_name': 'metrics',\n                    'accuracy': 92.0,\n                },\n                'name': 'metrics',\n                'type': 'system.Metrics'\n            }],\n            'parameters': {}\n        },\n        'type': 'system.ContainerExecution',\n        'state': Execution.State.COMPLETE,\n    }, digit_classification.get_dict())\n\n    t.assertEqual({\n        'inputs': {\n            'artifacts': [], \n            'parameters': {}\n        },\n        'name': 'html-visualization',\n        'outputs': {\n            'artifacts': [{\n                'metadata': {\n                    'display_name': 'html_artifact'\n                },\n                'name': 'html_artifact',\n                'type': 'system.HTML'\n            }],\n            'parameters': {}\n        },\n        'state': Execution.State.COMPLETE,\n        'type': 'system.ContainerExecution'\n    }, html_visualization.get_dict())\n\n    t.assertEqual({\n        'inputs': {\n            'artifacts': [], \n            'parameters': {}\n        },\n        'name': 'markdown-visualization',\n        'outputs': {\n            'artifacts': [{\n                'metadata': {\n                    'display_name': 'markdown_artifact'\n                },\n                'name': 'markdown_artifact',\n                'type': 'system.Markdown'\n            }],\n            'parameters': {}\n        },\n        'state': Execution.State.COMPLETE,\n        'type': 'system.ContainerExecution'\n    }, markdown_visualization.get_dict())\n\n\nrun_pipeline_func([\n    TestCase(\n        pipeline_func=metrics_visualization_pipeline,\n        verify_func=verify,\n        mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE\n    ),\n    TestCase(\n        pipeline_func=metrics_visualization_pipeline,\n        verify_func=verify,\n        mode=kfp.dsl.PipelineExecutionMode.V2_ENGINE\n    ),\n])\n"
  },
  {
    "repo": "swe-train/kubeflow__pipelines",
    "file_path": "samples/test/parameter_with_format.py",
    "raw_url": "https://raw.githubusercontent.com/swe-train/kubeflow__pipelines/0.1.8/samples/test/parameter_with_format.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom kfp import components\nfrom kfp import dsl\n\n\n@components.create_component_from_func\ndef print_op(name: str) -> str:\n    print(name)\n    return name\n\n\n@dsl.pipeline(name='pipeline-with-pipelineparam-containing-format')\ndef my_pipeline(name: str = 'KFP'):\n    print_task = print_op('Hello {}'.format(name))\n    print_op('{}, again.'.format(print_task.output))\n"
  },
  {
    "repo": "swe-train/kubeflow__pipelines",
    "file_path": "samples/test/parameter_with_format_test.py",
    "raw_url": "https://raw.githubusercontent.com/swe-train/kubeflow__pipelines/0.1.8/samples/test/parameter_with_format_test.py",
    "content": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp\nfrom .parameter_with_format import my_pipeline\nfrom .util import run_pipeline_func, TestCase\n\n\ndef verify(run, run_id: str, **kwargs):\n    assert run.status == 'Succeeded'\n    # TODO(Bobgy): verify output\n\n\nrun_pipeline_func([\n    TestCase(\n        pipeline_func=my_pipeline,\n        verify_func=verify,\n    ),\n    TestCase(\n        pipeline_func=my_pipeline,\n        verify_func=verify,\n        mode=kfp.dsl.PipelineExecutionMode.V2_ENGINE,\n    ),\n    TestCase(\n        pipeline_func=my_pipeline,\n        mode=kfp.dsl.PipelineExecutionMode.V1_LEGACY,\n    ),\n])\n"
  },
  {
    "repo": "swe-train/kubeflow__pipelines",
    "file_path": "samples/test/placeholder_concat.py",
    "raw_url": "https://raw.githubusercontent.com/swe-train/kubeflow__pipelines/0.1.8/samples/test/placeholder_concat.py",
    "content": "# Copyright 2020 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom kfp import components\nfrom kfp.v2 import dsl\nimport kfp.v2.compiler as compiler\n\ncomponent_op = components.load_component_from_text(\"\"\"\nname: Component with concat placeholder\ninputs:\n- {name: input_one, type: String}\n- {name: input_two, type: String}\nimplementation:\n  container:\n    image: gcr.io/google-containers/busybox\n    command:\n    - sh\n    - -ec\n    args:\n    - echo \"$0\" > /tmp/test && [[ \"$0\" == 'one+two=three' ]]\n    - concat: [{inputValue: input_one}, '+', {inputValue: input_two}, '=three']\n\"\"\")\n\n\n@dsl.pipeline(name='one-step-pipeline-with-concat-placeholder')\ndef pipeline_with_concat_placeholder():\n  component = component_op(input_one='one', input_two='two')\n"
  },
  {
    "repo": "smk508/kung-fu-pipelines",
    "file_path": "kungfupipelines/workflow.py",
    "raw_url": "https://raw.githubusercontent.com/smk508/kung-fu-pipelines/master/kungfupipelines/workflow.py",
    "content": "import abc\nimport kfp\nfrom kfp import components, dsl, gcp\nimport wrapt\nfrom typing import Callable, List\nfrom kungfupipelines.step import Step\nfrom kungfupipelines.cli import StepSwitch\n\ndef make_sequence(ops: List[dsl.ContainerOp]) -> None:\n    \"\"\" \n    Links a sequence of pipeline operations so that they are configured\n    to take place one after another.\n    Args:\n        ops - list[dsl.ContainerOp]\n    \"\"\"\n    l = len(ops)\n    if l <= 1:\n        return\n    i = 0\n    before = ops[i]\n    for op in ops[1:]:\n        op.after(before)\n        before = op\n    \nclass Workflow(abc.ABC):\n    \"\"\"\n    A Workflow abstracts the connectivity structure between pipeline steps. Individual steps\n    can be provided to a Workflow, and they will then be compiled together so that they happen\n    in a predefined sequence specified by that Workflow. This makes is easy to have 'standardized'\n    pipeline structures in which you can simply swap out individual steps as needed.\n    For example, a machine learning workflow might have slots for dataset preprocessing, training,\n    hyperparameter optimization, etc. You can provide specific pipeline steps to fill in those slots\n    and generate the pipeline spec without having to rewrite the connectivity structure each time.\n    \"\"\"\n    @abc.abstractmethod\n    def compile(self, image:str, script_path:str) -> Callable:\n        \"\"\" \n        Generates a kfp Pipeline spec which can be used to generate an \n        Argo workflow.yaml\n        Args:\n            image: The uri for the image to use.\n            script_path: The path that your script is located in in the image.\n        \"\"\"\n        pass\n\n    def generate_yaml(self, filename, *compile_args):\n        \"\"\"\n        Generates an argo workflow.yaml spec which can be used to submit this\n        workflow to Argo / Kubeflow.\n        \"\"\"\n        pipeline = self.compile(*compile_args)\n        kfp.compiler.Compiler().compile(pipeline, filename)\n\nclass BasicMLWorkflow(Workflow):\n    \"\"\"\n    This specifies a simple pipeline for machiine learning. It consists of the following steps \n    taking place one after another:\n    1) Create/download/acquire the master dataset\n    2) Perform a train/test split\n    3) Apply any preprocessing logic\n    4) Train your model\n    5) Apply any post processing logic using the test set, including computing accuracy, ROC, etc.\n\n    Args:\n        name: Name to use for the compiled pipeline\n        image: Docker container URI containing all of the scripts\n        script_path: Path to script that runs the steps\n        make_dataset: The Step to use for making the dataset\n        train_test_split: The Step to use for train_test_split\n        preprocess: The Step to use for preprocessing\n        train: The Step to use for model training\n        postprocess_ops: A list of Steps to perform post-training operations with\n        description: An optional string describing your pipeline\n    \"\"\"\n    def __init__(\n        self,\n        name: str,\n        image: str,\n        script_path: str,\n        make_dataset: Step,\n        train_test_split: Step,\n        preprocess: Step,\n        train: Step,\n        postprocess_ops: List[Step] = [],\n        description: str = None,\n    ):\n        self.make_dataset = make_dataset\n        self.train_test_split = train_test_split\n        self.preprocess = preprocess\n        self.train = train\n        self.postprocess_ops = postprocess_ops\n        self.image = image\n        self.script_path = script_path\n\n    def compile(self):\n        \n        def pipeline(*args, **kwargs):\n\n            make_dataset_op = self.make_dataset.dslContainerOp(self.image, self.script_path, **kwargs)\n            train_test_split_op = self.train_test_split.dslContainerOp(self.image, self.script_path, **kwargs)\n            train_op = self.train.dslContainerOp(\n                self.image, self.script_path, **kwargs\n                ).set_gpu_limit(1).add_toleration({\n                    'key': 'nvidia.com/gpu',\n                    'operator': 'Equal',\n                    'value': 'present',\n                    'effect': 'NoSchedule'\n                    })\n            postprocess_ops = [\n                pp.dslContainerOp(self.image, self.script_path, **kwargs)\n                for pp in self.postprocess_ops\n            ]\n\n            make_sequence([make_dataset_op, train_test_split_op, train_op])\n            for pp in postprocess_ops:\n                pp.after(train_op)\n\n        return pipeline\n\nclass SequentialWorkflow(Workflow):\n    \n    def __init__(self, name: str, steps: List[Step]):\n        self.steps = steps\n        self.step_switch = StepSwitch(name, steps)\n\n    def compile(self, image: str):\n\n        def pipeline(*args, **kwargs):\n            ops = []\n            for step in self.steps:\n                op = step.dslContainerOp(image)\n            \n            make_sequence(ops)\n\n        return pipeline\n\n\ndef Pipeline(pipeline_func: Callable, name:str, description:str=''): # NOTE: This does not work\n\n    @dsl.pipeline(name=name, description=description)\n    @wrapt.decorator\n    def wrapper(wrapped, instance, args, kwargs):\n        return pipeline_func(*args, **kwargs)\n    \n    return wrapper \n"
  },
  {
    "repo": "duttab49/kubeflow-pipelines",
    "file_path": "add_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/duttab49/kubeflow-pipelines/main/add_pipeline.py",
    "content": "from asyncio.windows_events import NULL\r\nimport kfp.dsl as dsl\r\nimport kfp.compiler as compiler\r\nimport kfp.components as comp\r\n\r\n\r\ndef add(a: float, b: float) -> float:\r\n  '''Calculates sum of two arguments'''\r\n  return a + b\r\n\r\nadd_op = comp.create_component_from_func(\r\n    add, output_component_file='add_component.yaml')\r\n\r\ndef echo_out(result: float) -> NULL:\r\n    print(\"The result is :\", result)\r\n\r\necho_op = comp.create_component_from_func(echo_out)\r\n\r\n\r\n@dsl.pipeline(\r\n    name='add_pipeline',\r\n    description='sample pipeline to add two numbers and then print the result.'\r\n)\r\n\r\ndef add_pipeline(\r\n  a='1',\r\n  b='7',\r\n):\r\n  # Passes a pipeline parameter and a constant value to the `add_op` factory\r\n  # function.\r\n  first_add_task = add_op(a, 4)\r\n  # Passes an output reference from `first_add_task` and a pipeline parameter\r\n  # to the `add_op` factory function. For operations with a single return\r\n  # value, the output reference can be accessed as `task.output` or\r\n  # `task.outputs['output_name']`.\r\n  second_add_task = add_op(first_add_task.output, b)\r\n\r\n  # Print the results\r\n  third_echo_task = echo_op(second_add_task.output)\r\n\r\nif __name__ == '__main__':\r\n    compiler.Compiler().compile(add_pipeline, __file__ + '.yaml')"
  },
  {
    "repo": "dhiebtarak/kubeflow_pipeline",
    "file_path": "kubeflow_pipeline_test.py",
    "raw_url": "https://raw.githubusercontent.com/dhiebtarak/kubeflow_pipeline/master/kubeflow_pipeline_test.py",
    "content": "from typing import Dict, List\nfrom kfp import dsl\nfrom kfp import compiler\nimport kfp\nfrom client_manager import KFPClientManager\nfrom kfp.dsl import Input, Output, Dataset, Model, component\n# Preprocessing Function for Prediction \ndef preprocess_input(raw_input: str) -> str:\n    \"\"\"\n    Preprocess raw input (e.g., TrdCaptRpt string) to match input_message_clean format.\n    Args:\n        raw_input (str): Raw input string, e.g., concatenated TrdCaptRpt messages.\n    Returns:\n        str: Cleaned input string ready for model prediction.\n    \"\"\"\n    import re\n    # Remove XML-like tags and special characters\n    pattern = r'[<\"/]'\n    cleaned_text = re.sub(pattern, '', raw_input)\n    cleaned_text = cleaned_text.replace('>', ' ')\n    # Remove extra whitespace\n    cleaned_text = ' '.join(cleaned_text.split())\n    return cleaned_text\n\n# Step 1: Generate Dataset\n@component(base_image=\"tarakdhieb7/kubeflow-testing:latest\")\ndef generate_data(output_csv: Output[Dataset], execution_id: str = \"\"):\n    import subprocess\n    subprocess.run([\"pip\", \"install\", \"minio\"], check=True)\n    import random\n    import pandas as pd\n    import numpy as np\n    from minio import Minio\n    from minio.error import S3Error\n    import os\n    import uuid\n\n    # Initialize minio client\n    minio_client = Minio(\n        \"10.106.67.253:9000\",\n        access_key=\"minio\",  # Replace with your minio access key\n        secret_key=\"minio123\",  # Replace with your minio secret key\n        secure=False\n    )\n\n    # Ensure bucket exists\n    bucket_name = \"mlpipeline\"\n    if not minio_client.bucket_exists(bucket_name):\n        minio_client.make_bucket(bucket_name)\n\n    def generate_full_service_wf():\n        rptid = random.randint(1000, 400000)\n        trdid = random.randint(20281004800, 40281004800)\n        trdtyp = random.choice([0, 1, 2, 3, 11, 15, 20, 29, 31, 45])\n        mtchid = random.randint(20281001777, 40281001777)\n        pxtyp = random.choice([2, 10])\n        lastqty = random.randint(10, 100)\n        lastpx = random.choice([100, 370, 876, 250.45, 300.9, 540.698, 1050, 3590.59])\n        trdt = random.choice([\"2022-12-17\", \"2022-12-21\", \"2023-01-02\", \"2023-01-05\"])\n        bizdt = random.choice([\"2023-01-06\", \"2023-01-11\", \"2023-01-20\", \"2023-02-05\", \"2023-02-10\"])\n        t = random.choice([\"2023-02-15\", \"2023-02-21\", \"2023-03-20\", \"2023-03-30\"])\n        time1 = random.choice(['T05:17:30', 'T10:18:15', 'T12:25:13', 'T16:40:55'])\n        r1 = random.choice(['.100', '.230', '.326'])\n        r2 = random.choice(['.350', '.470', '.768'])\n        tz1 = random.choice(['-5:00', '-6:00', '-7:00'])\n        txntm = t + time1 + r1 + tz1\n        time2 = random.choice(['T17:30:55.600', 'T19:33:05.333', 'T22:03:12.436'])\n        snt1 = t + time1 + r2 + tz1\n        snt2 = t + time2 + '+01:00'\n        snt3 = t + 'T00:12:55.138' + tz1\n        snt4 = t + 'T00:12:55.160' + tz1\n        sym = random.choice([\"BUI\", \"ABP\", \"TMP\"])\n        id5 = random.choice(['B', 'W', 'AS', 'LM', 'BUI', 'IP0'])\n        my = random.choice([\"2023-05\", \"2023-07\", \"2023-10\"])\n        mmy = my.replace('-', '')\n        matdt = my + '-' + str(random.randint(1, 28))\n        inptsrc = random.choice(['MA', 'EL', 'SY'])\n        clordid = random.choice(['123', '134', '145', '146ZBT', '245BTN', '987NL', 'OrdAT'])\n        tid = random.choice(['ABCD', 'D210', 'B509', 'HBPL', 'TL98'])\n        id2 = random.choice(['Emira-115', 'Ahlem-110', 'Karim-200', 'Mohamed-300', 'acc-AT', 'bl-NL', 'Mariem', 'ahmed'])\n        typ = random.randint(20, 30)\n        id3 = random.choice([\"FIRMACT1\", \"AKLPM9\", \"MPLAMP6\", \"00120007\", \"Hassen\", \"Ahlem\", 'arij'])\n        custcpcty1 = random.randint(1, 4)\n        custcpcty2 = random.randint(1, 4)\n        ex = random.choice(['BTNL', 'SNTL', 'HDPL', 'XMGE'])\n        mult = random.choice([0.1, 0.2, 0.3, 0.4, 0.9, 1000, 2000, 5000, 300, 200, 50, 70])\n        pos1 = random.choice(['Y', 'N'])\n        pos3 = random.choice(['Y', 'N'])\n        cfi = random.choice(['FCCPXX', 'APYBKL', 'MKLPON', 'FCAPSX'])\n        pos2 = random.choice(['O', 'C'])\n        side = random.randint(1, 2)\n        id26 = random.randint(1, 2)\n        mlrty = random.randint(1, 3)\n        alin = random.randint(0, 1)\n        inpdev = random.choice(['PORTAL', 'EXCHANGE', 'API', 'CLEARING'])\n        id4 = random.randint(100, 500)\n        id12 = random.choice(['100', '200', '400', 'A123', 'B115'])\n        cfi2 = random.choice(['FCCPXX', 'APYBKL', 'MKLPON', 'FCAPSX'])\n        return (\n            f\"\"\"<TrdCaptRpt RptID=\"{rptid}\" TrdID=\"{trdid}\" TransTyp=\"0\" RptTyp=\"2\" TrdTyp=\"{trdtyp}\" MtchID=\"{mtchid}\" PxTyp=\"{pxtyp}\" LastQty=\"{lastqty}\" LastPx=\"{lastpx}\" TrdDt=\"{trdt}\" BizDt=\"{bizdt}\" TxnTm=\"{txntm}\"><Hdr SID=\"MGEX\" TID=\"{tid}\" PosDup=\"{pos1}\" PosRsnd=\"{pos3}\" Snt=\"{snt1}\"></Hdr>\n        <Instrmt Sym=\"{sym}\" ID=\"{id5}\" Src=\"H\" CFI=\"{cfi}\" MMY=\"{mmy}\" MatDt=\"{matdt}\" Mult=\"{mult}\" Exch=\"{ex}\"></Instrmt><RptSide Side=\"{side}\" Ccy=\"USD\" InptSrc=\"{inptsrc}\" InptDev=\"{inpdev}\" CustCpcty=\"{custcpcty1}\" PosEfct=\"{pos2}\" ClOrdID=\"{clordid}\" MLegRptTyp=\"{mlrty}\" AllocInd=\"{alin}\"><Pty ID=\"MGEX\" R=\"21\"></Pty><Pty ID=\"210\" R=\"1\"></Pty><Pty ID=\"{id4}\" R=\"4\"></Pty>\n        <Pty ID=\"{id2}\" R=\"24\"><Sub ID=\"{id26}\" Typ=\"26\"></Sub></Pty><Pty ID=\"{id12}\" R=\"12\"></Pty><TrdRegTS TS=\"{txntm}\" Typ=\"1\">\n        </TrdRegTS></RptSide></TrdCaptRpt>\"\"\".replace('\\n', ''),\n            f\"\"\"<TrdCaptRpt RptID=\"{trdid}\" TrdDt=\"{trdt}\" BizDt=\"{bizdt}\" TxnTm=\"{txntm}\" TrdID=\"{trdid}\" TransTyp=\"2\" RptTyp=\"0\" LastQty=\"{float(lastqty)}\" LastPx=\"{float(lastpx)}\"><Hdr Snt=\"{snt2}\" TID=\"MGEX\" SID=\"{tid}\"/><Instrmt Exch=\"{ex}\" ID=\"{id5}\" MMY=\"{mmy}\" CFI=\"{cfi2}\"/><RptSide Side=\"{side}\" CustCpcty=\"{custcpcty2}\">\n        <Pty ID=\"210\" R=\"1\"/><Pty ID=\"{id3}\" R=\"24\"><Sub ID=\"{id26}\" Typ=\"26\"/></Pty></RptSide></TrdCaptRpt>\"\"\".replace('\\n', ''),\n            f\"\"\"<TrdCaptRpt RptID=\"580198\" TrdID=\"{trdid}\" TransTyp=\"2\" RptTyp=\"2\" TrdTyp=\"{trdtyp}\" MtchID=\"{mtchid}\" PxTyp=\"{pxtyp}\" LastQty=\"{lastqty}\" LastPx=\"{lastpx}\" TrdDt=\"{trdt}\" BizDt=\"{bizdt}\" TxnTm=\"{snt3}\"><Hdr SID=\"MGEX\" TID=\"{tid}\" PosDup=\"{pos1}\" PosRsnd=\"{pos3}\" Snt=\"{snt4}\"></Hdr>\n        <Instrmt Sym=\"{sym}\" ID=\"{id5}\" Src=\"H\" CFI=\"{cfi}\" MMY=\"{mmy}\" MatDt=\"{matdt}\" Mult=\"{mult}\" Exch=\"{ex}\"></Instrmt><RptSide Side=\"{side}\" Ccy=\"USD\" InptSrc=\"{inptsrc}\" InptDev=\"{inpdev}\" CustCpcty=\"{custcpcty2}\" PosEfct=\"{pos2}\" ClOrdID=\"{clordid}\" MLegRptTyp=\"{mlrty}\" AllocInd=\"{alin}\"><Pty ID=\"MGEX\" R=\"21\"></Pty><Pty ID=\"210\" R=\"1\"></Pty><Pty ID=\"{id4}\" R=\"4\"></Pty>\n        <Pty ID=\"{id3}\" R=\"24\"><Sub ID=\"{id26}\" Typ=\"26\"></Sub></Pty><Pty ID=\"{id12}\" R=\"12\"></Pty><TrdRegTS TS=\"{txntm}\" Typ=\"1\"></TrdRegTS></RptSide>\n        </TrdCaptRpt>\"\"\".replace('\\n', '')\n        )\n\n    dataset = [generate_full_service_wf() for _ in range(500)]\n    ccp_new_trade_messages = [message[0] for message in dataset]\n    sgw_fs_operation_messages = [message[1] for message in dataset]\n    ccp_response_messages = [message[2] for message in dataset]\n    df = pd.DataFrame({\n        'Executed Trade': ccp_new_trade_messages,\n        'Full Service Operation': sgw_fs_operation_messages,\n        'CCP Response': ccp_response_messages\n    })\n    df['input_message'] = df['Executed Trade'] + ' ' + df['Full Service Operation']\n    df.drop(['Executed Trade', 'Full Service Operation'], axis=1, inplace=True)\n    df.to_csv(output_csv.path, index=False)\n    print(output_csv.path)\n    # Upload to minio\n    execution_id = execution_id if execution_id else str(uuid.uuid4())\n    object_name = f\"{execution_id}/generate_data/output.csv\"\n    try:\n        minio_client.fput_object(bucket_name, object_name, output_csv.path)\n        print(f\"Uploaded {object_name} to minio bucket {bucket_name}\")\n    except S3Error as e:\n        print(f\"Failed to upload to minio: {e}\")\n\n# Step 2: Preprocess Data\n@component(base_image=\"tarakdhieb7/kubeflow-testing:latest\")\ndef preprocess_data(input_csv: Input[Dataset], output_train: Output[Dataset], output_val: Output[Dataset], execution_id: str = \"\"):\n    import subprocess\n    subprocess.run([\"pip\", \"install\", \"minio\"], check=True)\n    import re\n    import pandas as pd\n    from minio import Minio\n    from minio.error import S3Error\n    import uuid\n\n    # Initialize minio client\n    minio_client = Minio(\n        \"10.106.67.253:9000\",\n        access_key=\"minio\",  # Replace with your minio access key\n        secret_key=\"minio123\",  # Replace with your minio secret key\n        secure=False\n    )\n\n    # Ensure bucket exists\n    bucket_name = \"mlpipeline\"\n    if not minio_client.bucket_exists(bucket_name):\n        minio_client.make_bucket(bucket_name)\n\n    df = pd.read_csv(input_csv.path)\n    def clean_text(input_message):\n        pattern = r'[<\"/]'\n        cleaned_text = re.sub(pattern, '', input_message)\n        cleaned_text = cleaned_text.replace('>', ' ')\n        cleaned_text = ' '.join(cleaned_text.split())\n        return cleaned_text\n\n    df['input_message_clean'] = df['input_message'].apply(clean_text)\n    df.drop(['input_message'], axis=1, inplace=True)\n    train_data = df.iloc[:int(len(df) * 0.9)]\n    val_data = df.iloc[int(len(df) * 0.9):]\n    print(\"Shapes after train-val split:\")\n    print(\"train_df:\", train_data.shape)\n    print(\"val_df:\", val_data.shape)\n    train_data.to_csv(output_train.path, index=False)\n    val_data.to_csv(output_val.path, index=False)\n    print(\"Preprocessed data saved to:\", output_train.path, output_val.path)\n\n    # Upload to minio\n    execution_id = execution_id if execution_id else str(uuid.uuid4())\n    for output, name in [(output_train, \"output_train.csv\"), (output_val, \"output_val.csv\")]:\n        object_name = f\"{execution_id}/preprocess_data/{name}\"\n        try:\n            minio_client.fput_object(bucket_name, object_name, output.path)\n            print(f\"Uploaded {object_name} to minio bucket {bucket_name}\")\n        except S3Error as e:\n            print(f\"Failed to upload to minio: {e}\")\n\n# Step 3: Train Model\n@component(base_image=\"tarakdhieb7/kubeflow-testing:latest\")\ndef train_model(train_data: Input[Dataset], val_data: Input[Dataset], model_output: Output[Model], execution_id: str = \"\"):\n    import subprocess\n    subprocess.run([\"pip\", \"install\", \"minio\"], check=True)\n\n    import torch\n    import pandas as pd\n    from transformers import GPT2Tokenizer, AutoModelForCausalLM\n    from torch.utils.data import Dataset, DataLoader\n    import time\n    import os\n    from minio import Minio\n    from minio.error import S3Error\n    import uuid\n\n    # Initialize minio client\n    minio_client = Minio(\n        \"10.106.67.253:9000\",\n        access_key=\"minio\",  # Replace with your minio access key\n        secret_key=\"minio123\",  # Replace with your minio secret key\n        secure=False\n    )\n\n    # Ensure bucket exists\n    bucket_name = \"mlpipeline\"\n    if not minio_client.bucket_exists(bucket_name):\n        minio_client.make_bucket(bucket_name)\n\n    os.environ['CURL_CA_BUNDLE'] = ''\n    os.environ['REQUESTS_CA_BUNDLE'] = ''\n\n    class CustomDataset(Dataset):\n        def __init__(self, data, tokenizer, max_length):\n            self.data = data\n            self.tokenizer = tokenizer\n            self.max_length = max_length\n\n        def __len__(self):\n            return len(self.data)\n\n        def __getitem__(self, index):\n            input_seq = self.data.iloc[index]['input_message_clean']\n            target_seq = self.data.iloc[index]['CCP Response']\n            concat_seq = f\"{input_seq} {target_seq}\"\n            tokenized_seq = self.tokenizer.encode(\n                concat_seq,\n                return_tensors='pt',\n                max_length=self.max_length,\n                truncation=True,\n                padding='max_length'\n            )\n            return tokenized_seq.squeeze(0)\n\n    # Load data\n    train_df = pd.read_csv(train_data.path)\n    val_df = pd.read_csv(val_data.path)\n    print(\"Shape of train_df:\", train_df.shape)\n    print(\"Shape of val_df:\", val_df.shape)\n\n    # Initialize tokenizer and model\n    tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2', low_cpu_mem_usage=True)\n    tokenizer.pad_token = tokenizer.eos_token\n    model = AutoModelForCausalLM.from_pretrained('distilgpt2')\n\n    # Create datasets\n    max_length = 512  # Reduced for CPU\n    batch_size = 1\n    train_dataset = CustomDataset(train_df, tokenizer, max_length)\n    val_dataset = CustomDataset(val_df, tokenizer, max_length)\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n\n    # Move to device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n    model.to(device)\n\n    # Train\n    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n    epochs = 1\n    start_time = time.time()\n    model.train()\n    total_loss = 0\n    for i, batch in enumerate(train_loader):\n        input_ids = batch.to(device)\n        outputs = model(input_ids=input_ids, labels=input_ids)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        total_loss += loss.item()\n        print(f\"Batch {i+1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n    avg_loss = total_loss / len(train_loader)\n    print(f\"Average Training Loss: {avg_loss:.4f}\")\n    print(f\"Training Time: {(time.time() - start_time)/60:.2f} minutes\")\n\n    # Save model and tokenizer\n    model.save_pretrained(model_output.path)\n    tokenizer.save_pretrained(model_output.path)\n\n    # Upload model files to minio\n    execution_id = execution_id if execution_id else str(uuid.uuid4())\n    model_files = [\n        \"config.json\",\n        \"pytorch_model.bin\",\n        \"vocab.json\",\n        \"merges.txt\",\n        \"tokenizer_config.json\",\n        \"special_tokens_map.json\"\n    ]\n    for file in model_files:\n        file_path = os.path.join(model_output.path, file)\n        if os.path.exists(file_path):\n            object_name = f\"{execution_id}/train_model/{file}\"\n            try:\n                minio_client.fput_object(bucket_name, object_name, file_path)\n                print(f\"Uploaded {object_name} to minio bucket {bucket_name}\")\n            except S3Error as e:\n                print(f\"Failed to upload to minio: {e}\")\n\n@component(base_image=\"tarakdhieb7/kubeflow-testing:latest\")\ndef train_model_distributed(train_data: Input[Dataset], val_data: Input[Dataset], model_output: Output[Model], execution_id: str = \"\"):\n    import subprocess\n    subprocess.run([\"pip\", \"install\", \"minio\"], check=True)\n    import os\n    import torch\n    import torch.distributed as dist\n    import pandas as pd\n    from torch.utils.data import Dataset, DataLoader\n    from transformers import GPT2Tokenizer, AutoModelForCausalLM\n    from torch.nn.parallel import DistributedDataParallel as DDP\n    import logging\n    import time\n    import sys\n    from minio import Minio\n    from minio.error import S3Error\n    import uuid\n\n    # Initialize minio client\n    minio_client = Minio(\n        \"10.106.67.253:9000\",\n        access_key=\"minio\",  # Replace with your minio access key\n        secret_key=\"minio123\",  # Replace with your minio secret key\n        secure=False\n    )\n\n    # Ensure bucket exists\n    bucket_name = \"mlpipeline\"\n    if not minio_client.bucket_exists(bucket_name):\n        minio_client.make_bucket(bucket_name)\n\n# Set up logging\n    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n    logger = logging.getLogger(__name__)\n    logger.info(f\"Starting train_model_distributed, execution_id: {execution_id}\")\n    logger.info(f\"Model output path: {model_output.path}\")\n\n    os.environ['CURL_CA_BUNDLE'] = ''\n    os.environ['REQUESTS_CA_BUNDLE'] = ''\n\n    class CustomDataset(Dataset):\n        def __init__(self, data, tokenizer, max_length):\n            self.data = data\n            self.tokenizer = tokenizer\n            self.max_length = max_length\n\n        def __len__(self):\n            return len(self.data)\n\n        def __getitem__(self, index):\n            input_seq = self.data.iloc[index]['input_message_clean']\n            target_seq = self.data.iloc[index]['CCP Response']\n            concat_seq = f\"{input_seq} {target_seq}\"\n            tokenized_seq = self.tokenizer.encode(\n                concat_seq,\n                return_tensors='pt',\n                max_length=self.max_length,\n                truncation=True,\n                padding='max_length'\n            )\n            return tokenized_seq.squeeze(0)\n\n    def setup_ddp(rank, world_size):\n        try:\n            os.environ['MASTER_ADDR'] = 'localhost'\n            os.environ['MASTER_PORT'] = '12345'\n            dist.init_process_group(backend='gloo', rank=rank, world_size=world_size)\n            logger.info(f\"DDP initialized for rank {rank}/{world_size}\")\n        except Exception as e:\n            logger.error(f\"DDP setup failed: {e}\")\n            raise\n\n    def train(rank, world_size, train_loader, model, optimizer, device, epochs):\n        try:\n            setup_ddp(rank, world_size)\n            model = model.to(device)\n            model = DDP(model, device_ids=None)\n            model.train()\n            for epoch in range(epochs):\n                total_loss = 0\n                for i, batch in enumerate(train_loader):\n                    input_ids = batch.to(device)\n                    outputs = model(input_ids=input_ids, labels=input_ids)\n                    loss = outputs.loss\n                    loss.backward()\n                    optimizer.step()\n                    optimizer.zero_grad()\n                    total_loss += loss.item()\n                    if rank == 0:\n                        logger.info(f\"Epoch {epoch+1}/{epochs}, Batch {i+1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n                avg_loss = total_loss / len(train_loader)\n                if rank == 0:\n                    logger.info(f\"Epoch {epoch+1}/{epochs}, Average Loss: {avg_loss:.4f}\")\n            dist.destroy_process_group()\n        except Exception as e:\n            logger.error(f\"Training failed on rank {rank}: {e}\")\n            dist.destroy_process_group()\n            raise\n\n    # Function to initialize the dataset and model (called in each process)\n    def initialize_training_components():\n        # Load data\n        train_df = pd.read_csv(train_data.path)\n        logger.info(f\"Shape of train_df: {train_df.shape}\")\n\n        # Initialize tokenizer and model\n        tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2', low_cpu_mem_usage=True)\n        tokenizer.pad_token = tokenizer.eos_token\n        model = AutoModelForCausalLM.from_pretrained('distilgpt2')\n\n        # Create dataset\n        max_length = 512\n        batch_size = 2\n        train_dataset = CustomDataset(train_df, tokenizer, max_length)\n        train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\n        train_loader = DataLoader(\n            train_dataset,\n            batch_size=batch_size,\n            shuffle=False,\n            sampler=train_sampler\n        )\n\n        # Training setup\n        device = torch.device('cpu')\n        optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n        epochs = 1\n\n        return train_loader, model, optimizer, device, epochs, tokenizer\n\n    def distributed_training():\n        try:\n            # Get rank and world_size from environment (set by torchrun)\n            rank = int(os.environ[\"RANK\"])\n            world_size = int(os.environ[\"WORLD_SIZE\"])\n            \n            start_time = time.time()\n            logger.info(f\"Starting distributed training with rank={rank}, world_size={world_size}\")\n\n            # Initialize training components in each process\n            train_loader, model, optimizer, device, epochs, tokenizer = initialize_training_components()\n\n            # Run the training\n            train(rank, world_size, train_loader, model, optimizer, device, epochs)\n\n            # Save model and tokenizer (only rank 0)\n            if rank == 0:\n                model.module.save_pretrained(model_output.path)\n                model.metadata['framework'] = 'transformers'\n                tokenizer.save_pretrained(model_output.path)\n                logger.info(f\"Model saved to {model_output.path}\")\n\n                # Upload model files to minio\n                execution_id = execution_id if execution_id else str(uuid.uuid4())\n                model_files = [\n                    \"config.json\",\n                    \"pytorch_model.bin\",\n                    \"vocab.json\",\n                    \"merges.txt\",\n                    \"tokenizer_config.json\",\n                    \"special_tokens_map.json\"\n                ]\n                for file in model_files:\n                    file_path = os.path.join(model_output.path, file)\n                    if os.path.exists(file_path):\n                        object_name = f\"{execution_id}/train_model_distributed/{file}\"\n                        try:\n                            minio_client.fput_object(bucket_name, object_name, file_path)\n                            logger.info(f\"Uploaded {object_name} to minio bucket {bucket_name}\")\n                        except S3Error as e:\n                            logger.error(f\"Failed to upload to minio: {e}\")\n            logger.info(f\"Training Time: {(time.time() - start_time)/60:.2f} minutes\")\n        except Exception as e:\n            logger.error(f\"Distributed training failed: {e}\")\n            raise\n\n    if __name__ == \"__main__\":\n        # Since this is a Kubeflow component, we'll assume it's invoked with torchrun\n        # Example command (not run here, but for reference):\n        # torchrun --nproc_per_node=3 --master_addr=localhost --master_port=12345 this_script.py\n        distributed_training()\n\n# Step 4: Evaluate Model\n@component(base_image=\"tarakdhieb7/kubeflow-testing:latest\")\ndef evaluate_model(val_data: Input[Dataset], model: Input[Model], metrics_output: Output[Dataset], execution_id: str = \"\"):\n    import subprocess\n    subprocess.run([\"pip\", \"install\", \"minio\"], check=True)\n    import torch\n    import pandas as pd\n    from transformers import GPT2Tokenizer, AutoModelForCausalLM\n    from torch.utils.data import Dataset, DataLoader\n    import time\n    from minio import Minio\n    from minio.error import S3Error\n    import uuid\n\n    # Initialize minio client\n    minio_client = Minio(\n        \"10.106.67.253:9000\",\n        access_key=\"minio\",  # Replace with your minio access key\n        secret_key=\"minio123\",  # Replace with your minio secret key\n        secure=False\n    )\n\n    # Ensure bucket exists\n    bucket_name = \"mlpipeline\"\n    if not minio_client.bucket_exists(bucket_name):\n        minio_client.make_bucket(bucket_name)\n\n    class CustomDataset(Dataset):\n        def __init__(self, data, tokenizer, max_length):\n            self.data = data\n            self.tokenizer = tokenizer\n            self.max_length = max_length\n\n        def __len__(self):\n            return len(self.data)\n\n        def __getitem__(self, index):\n            input_seq = self.data.iloc[index]['input_message_clean']\n            target_seq = self.data.iloc[index]['CCP Response']\n            concat_seq = f\"{input_seq} {target_seq}\"\n            tokenized_seq = self.tokenizer.encode(\n                concat_seq,\n                return_tensors='pt',\n                max_length=self.max_length,\n                truncation=True,\n                padding='max_length'\n            )\n            return tokenized_seq.squeeze(0)\n\n    # Load data\n    val_df = pd.read_csv(val_data.path)\n    print(\"Shape of val_df:\", val_df.shape)\n\n    # Load tokenizer and model\n    tokenizer = GPT2Tokenizer.from_pretrained(model.path)\n    model = AutoModelForCausalLM.from_pretrained(model.path)\n    max_length = 512\n    val_dataset = CustomDataset(val_df, tokenizer, max_length)\n    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=True)\n\n    # Evaluate\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n    model.eval()\n    total_loss = 0\n    start_time = time.time()\n    with torch.no_grad():\n        for batch in val_loader:\n            input_ids = batch.to(device)\n            outputs = model(input_ids=input_ids, labels=input_ids)\n            total_loss += outputs.loss.item()\n    avg_loss = total_loss / len(val_loader)\n    print(f\"Average Validation Loss: {avg_loss:.4f}\")\n    print(f\"Validation Time: {(time.time() - start_time)/60:.2f} minutes\")\n\n    # Save metrics\n    with open(metrics_output.path, 'w') as f:\n        f.write(f\"Average Validation Loss: {avg_loss:.4f}\")\n\n    # Upload to minio\n    execution_id = execution_id if execution_id else str(uuid.uuid4())\n    object_name = f\"{execution_id}/evaluate_model/metrics.txt\"\n    try:\n        minio_client.fput_object(bucket_name, object_name, metrics_output.path)\n        print(f\"Uploaded {object_name} to minio bucket {bucket_name}\")\n    except S3Error as e:\n        print(f\"Failed to upload to minio: {e}\")\n\n# Step 5: Predict\n@component(base_image=\"tarakdhieb7/kubeflow-testing:latest\")\ndef predict(val_data: Input[Dataset], model: Input[Model], prediction_output: Output[Dataset], execution_id: str = \"\"):\n    import subprocess\n    subprocess.run([\"pip\", \"install\", \"minio\"], check=True)\n    import torch\n    import pandas as pd\n    from transformers import GPT2Tokenizer, AutoModelForCausalLM\n    import time\n    from minio import Minio\n    from minio.error import S3Error\n    import uuid\n\n    # Initialize minio client\n    minio_client = Minio(\n        \"10.106.67.253:9000\",\n        access_key=\"minio\",  # Replace with your minio access key\n        secret_key=\"minio123\",  # Replace with your minio secret key\n        secure=False\n    )\n\n    # Ensure bucket exists\n    bucket_name = \"mlpipeline\"\n    if not minio_client.bucket_exists(bucket_name):\n        minio_client.make_bucket(bucket_name)\n\n    def preprocess_input(raw_input: str) -> str:\n        import re\n        pattern = r'[<\"/]'\n        cleaned_text = re.sub(pattern, '', raw_input)\n        cleaned_text = cleaned_text.replace('>', ' ')\n        cleaned_text = ' '.join(cleaned_text.split())\n        return cleaned_text\n\n    def predict_single(input_message: str, model, tokenizer, device, max_length=512):\n        model.eval()\n        cleaned_input = preprocess_input(input_message)\n        tokenized_input = tokenizer.encode(\n            cleaned_input,\n            return_tensors='pt',\n            max_length=max_length,\n            truncation=True\n        )\n        tokenized_input = tokenized_input.to(device)\n        with torch.no_grad():\n            outputs = model.generate(\n                input_ids=tokenized_input,\n                max_length=max_length,\n                num_return_sequences=1,\n                pad_token_id=tokenizer.eos_token_id\n            )\n        decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        predicted_output = decoded_output[len(cleaned_input):].strip()\n        return predicted_output\n\n    # Load data\n    val_df = pd.read_csv(val_data.path)\n    print(\"Shape of val_df:\", val_df.shape)\n\n    # Load tokenizer and model\n    tokenizer = GPT2Tokenizer.from_pretrained(model.path)\n    model = AutoModelForCausalLM.from_pretrained(model.path)\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n  \n    # Predict on sample input\n    sample_input = val_df.iloc[0]['input_message_clean']  # Use first validation input\n    start_time = time.time()\n    predicted_output = predict_single(sample_input, model, tokenizer, device)\n    print(f\"Sample Input: {sample_input[:100]}...\")\n    print(f\"Predicted Output: {predicted_output}\")\n    print(f\"Prediction Time: {(time.time() - start_time):.2f} seconds\")\n\n    # Save prediction\n    with open(prediction_output.path, 'w') as f:\n        f.write(f\"Input: {sample_input}\\nPrediction: {predicted_output}\")\n    print(f\"model saved to: {model.path}\")\n    # Upload to minio\n    execution_id = execution_id if execution_id else str(uuid.uuid4())\n    object_name = f\"{execution_id}/predict/prediction.txt\"\n    try:\n        minio_client.fput_object(bucket_name, object_name, prediction_output.path)\n        print(f\"Uploaded {object_name} to minio bucket {bucket_name}\")\n    except S3Error as e:\n        print(f\"Failed to upload to minio: {e}\")\n\n@component(base_image=\"tarakdhieb7/kubeflow-testing:latest\")\ndef upload_model(model: Output[Model]):\n    import subprocess\n    subprocess.run([\"pip\", \"install\", \"minio\"], check=True)\n    import torch\n    import pandas as pd\n    from transformers import GPT2Tokenizer, AutoModelForCausalLM\n    import time\n    from minio import Minio\n    from minio.error import S3Error\n\n    # Initialize minio client\n    minio_client = Minio(\n        \"10.106.67.253:9000\",\n        access_key=\"minio\",  # Replace with your minio access key\n        secret_key=\"minio123\",  # Replace with your minio secret key\n        secure=False\n    )\n    # Ensure bucket exists\n    bucket_name = \"mlpipeline\"\n    if not minio_client.bucket_exists(bucket_name):\n        minio_client.make_bucket(bucket_name)\n    # Upload model files to minio\n    \n# Define the Pipeline\n@dsl.pipeline(name=\"ml-pipeline\")\ndef ml_pipeline():\n    import uuid\n    unique_suffix = str(uuid.uuid4())\n    execution_id = str(uuid.uuid4())  # Generate a unique execution ID for this pipeline run\n    # Step 1: Generate Dataset\n    load_op = generate_data(execution_id=execution_id)\n    # Step 2: Preprocess Data\n    preprocess_op = preprocess_data(input_csv=load_op.outputs[\"output_csv\"], execution_id=execution_id)\n\n    # Step 3: Train Model\n#    train_op = train_model(\n#      train_data=preprocess_op.outputs[\"output_train\"],\n#       val_data=preprocess_op.outputs[\"output_val\"]\n#    )\n\n    train_op = train_model(\n        train_data=preprocess_op.outputs[\"output_train\"],\n        val_data=preprocess_op.outputs[\"output_val\"],\n        execution_id=execution_id\n    )\n    #train_op.set_caching_options(enable_caching=False)\n    # Step 4: Evaluate Model\n    evaluate_op = evaluate_model(\n        val_data=preprocess_op.outputs[\"output_val\"],\n        model=train_op.outputs[\"model_output\"],\n        execution_id=execution_id\n    )\n\n    # Step 5: Predict\n    predict_op = predict(\n        val_data=preprocess_op.outputs[\"output_val\"],\n        model=train_op.outputs[\"model_output\"],\n        execution_id=execution_id\n    )\n\nif __name__ == \"__main__\":\n    import argparse\n    from kfp import Client, compiler\n    import kfp_server_api.exceptions as kfp_exceptions\n    import uuid\n\n    # Parse command-line arguments\n    parser = argparse.ArgumentParser(description=\"Run and upload Kubeflow pipeline\")\n    parser.add_argument(\n        \"--pipeline-exists\",\n        action=\"store_true\",\n        help=\"Set if the pipeline 'demo_test_distributed' already exists\"\n    )\n    parser.add_argument(\n        \"--version\",\n        type=float,\n        default=0.1,\n        help=\"Version number for the pipeline (required if --pipeline-exists, e.g., 0.2)\"\n    )\n    args = parser.parse_args()\n\n    \n    name_space = \"kubeflow-user-example-com\"  # Specify the namespace\n    endpoint = \"http://localhost:8080/pipeline\"  # Specify the endpoint\n    # initialize a KFPClientManager\n    kfp_client_manager = KFPClientManager(\n    api_url=endpoint,\n    skip_tls_verify=True,\n\n    dex_username=\"user@example.com\",\n    dex_password=\"12341234\",\n\n    # can be 'ldap' or 'local' depending on your Dex configuration\n    dex_auth_type=\"local\",\n)\n    client = kfp_client_manager.create_kfp_client()\n\n    \n    # Run pipeline with unique run_id to force fresh execution\n    try:\n        unique_run_id = f\"run-{uuid.uuid4()}\"\n        run_result = client.create_run_from_pipeline_func(\n            ml_pipeline,\n            arguments={},\n            experiment_name=\"test\",\n            run_name=unique_run_id,\n            namespace=\"kubeflow-user-example-com\",  # Specify the namespace\n            service_account=\"pipeline-runner\"  # Specify the service account\n        )\n        print(f\"Started run: {run_result.run_id}\")\n        print(f\"Run details: {endpoint}/#/runs/details/{run_result.run_id}\")\n    except Exception as e:\n        print(f\"Failed to start run: {e}\")\n        exit(1)\n\n    # Compile pipeline\n    pipeline_name = \"demo_test\"\n    yaml_file = \"kubeflow_pipeline_demo.yaml\"\n    try:\n        compiler.Compiler().compile(pipeline_func=ml_pipeline, package_path=yaml_file)\n        print(f\"Compiled pipeline to {yaml_file}\")\n    except Exception as e:\n        print(f\"Failed to compile pipeline: {e}\")\n        exit(1)\n\n    # Upload pipeline based on --pipeline-exists flag\n    try:\n        if args.pipeline_exists:\n            if not args.version:\n                print(\"Error: --version required when --pipeline-exists is set\")\n                exit(1)\n            print(f\"Uploading version {args.version} for existing pipeline {pipeline_name}\")\n            try:\n                client.upload_pipeline_version(\n                    pipeline_package_path=yaml_file,\n                    pipeline_version_name=str(args.version),\n                    pipeline_name=pipeline_name,\n                    description=\"just for testing\"\n                )\n            except kfp_exceptions.ApiException as api_err:\n                if api_err.status == 404:  # Pipeline doesn't exist\n                    print(f\"Pipeline {pipeline_name} not found, uploading as new pipeline\")\n                    client.upload_pipeline(\n                        pipeline_package_path=yaml_file,\n                        pipeline_name=pipeline_name,\n                        description=\"just for testing\"\n                    )\n                else:\n                    raise api_err\n        else:\n            print(f\"Uploading new pipeline {pipeline_name}\")\n            client.upload_pipeline(\n                pipeline_package_path=yaml_file,\n                pipeline_name=pipeline_name,\n                description=\"just for testing\"\n            )\n    except Exception as e:\n        print(f\"Failed to upload pipeline: {e}\")\n        exit(1)"
  },
  {
    "repo": "stackdemos/kubeflow-acm-ecr",
    "file_path": "components/golang/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/kubeflow-acm-ecr/master/components/golang/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef training_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('TRAINING_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=['/usr/local/bin/python', script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n\n"
  },
  {
    "repo": "stackdemos/kubeflow-acm-ecr",
    "file_path": "components/training/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/kubeflow-acm-ecr/master/components/training/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef training_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('TRAINING_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=['/usr/local/bin/python', script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n\n"
  },
  {
    "repo": "stackdemos/s3-kubeflow-asi",
    "file_path": "components/golang/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/s3-kubeflow-asi/master/components/golang/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef processing_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('GOLANG_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=[script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n"
  },
  {
    "repo": "stackdemos/s3-kubeflow-asi",
    "file_path": "components/training/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/s3-kubeflow-asi/master/components/training/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef training_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('TRAINING_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=['/usr/local/bin/python', script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n\n"
  },
  {
    "repo": "antongollbo123/kfp_utils",
    "file_path": "local_pipeline2.py",
    "raw_url": "https://raw.githubusercontent.com/antongollbo123/kfp_utils/main/local_pipeline2.py",
    "content": "import json\n\nfrom google.cloud import storage  # type:ignore\nfrom google.oauth2 import service_account  # type:ignore\n\n# from google.cloud import storage  #type:ignore\n# from google.oauth2 import service_account  #type:ignore\nfrom kfp import compiler, dsl, local  # type:ignore\nfrom kfp.dsl import Artifact, Output  # type:ignore\n\nlocal.init(runner=local.DockerRunner())\n\n\n@dsl.component(base_image=\"custom_python:latest\", packages_to_install=[\"numpy\"])\ndef upload_to_gcs(\n    bucket_name: str,\n    source_file_name: str,\n    destination_blob_name: str,\n    out_artifact: Output[Artifact],\n) -> str:\n    import logging\n\n    import numpy as np\n    from google.cloud import storage  # type:ignore\n    from google.oauth2 import service_account  # type:ignore\n\n    logging.basicConfig(level=logging.INFO)\n    logger = logging.getLogger(__name__)\n    try:\n        credentials = service_account.Credentials.from_service_account_file(\n            \"credentials.json\"\n        )\n        storage_client = storage.Client(credentials=credentials)\n        bucket = storage_client.bucket(bucket_name)\n        logger.info(np.abs(1))\n        blob = bucket.blob(destination_blob_name)\n        blob.upload_from_filename(source_file_name)\n        print(\n            f\"File {source_file_name} uploaded to {destination_blob_name} in bucket {bucket_name}\"\n        )\n        #out_artifact.metadata[\"test\"] = \"test123\"\n        logger.info(\"Success!\")\n    except Exception as e:\n        logger.error(f\"Error has occured: {e}\")\n    return \"hello\"\n\n\n@dsl.component(base_image=\"python:3.8\")\ndef add(a: int, b: int, out_artifact: Output[Artifact]):\n    import json\n    import logging\n\n    logging.basicConfig(level=logging.INFO)\n    logger = logging.getLogger(__name__)\n    result = json.dumps(a + b)\n    logger.info(f\"Hello from inside the pipeline, with message {a}, {b}\")\n\n    with open(out_artifact.path, \"w\") as f:\n        f.write(result)\n\n    out_artifact.metadata[\"operation\"] = \"addition\"\n    out_artifact.metadata[\"test\"] = \"test123\"\n\n\"\"\"\ntask = upload_to_gcs(\n    bucket_name=\"new_bucket123123213\",\n    source_file_name=\"sample_file.txt\",\n    destination_blob_name=\"sample_file.txt\",\n)  # type:ignore\n\"\"\"\n@dsl.pipeline()\ndef gcs_pipeline() -> None:\n    upload_to_gcs(bucket_name=\"new_bucket123123213\", \n                         source_file_name=\"sample_file.txt\",\n                         destination_blob_name=\"sample_file.txt\"\n                         ) # type:ignore\n\n\n\npipeline_file = \"test_pipeline.yaml\"\ncompiler.Compiler().compile(\n    gcs_pipeline,  # type: ignore\n    pipeline_file,\n)\n\n# can read artifact contents\n#with open(task.outputs[\"out_artifact\"].path) as f:\n#    contents = f.read()\n"
  },
  {
    "repo": "antongollbo123/kfp_utils",
    "file_path": "test_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/antongollbo123/kfp_utils/main/test_pipeline.py",
    "content": "\n\nimport kfp  #type:ignore\nfrom kfp import compiler, dsl, local\n\nlocal.init(runner=local.DockerRunner())\n\n@dsl.component(base_image=\"custom_python:latest\")\ndef print_hello(message: str) -> None:\n    import logging\n    logging.basicConfig(level=logging.INFO)\n    logger = logging.getLogger(__name__)\n    logger.info(f\"Hello from inside the pipeline, with message {message}\")\n    #return message\n\n@dsl.pipeline\ndef logging_pipeline(message: str) -> None:\n    \"\"\"My test pipeline\"\"\"\n    print_hello(message=message) # type: ignore\n\npipeline = logging_pipeline(message=\"hello from the other side\")\n#pipeline_file = \"logging_pipeline.yaml\"\n#compiler.Compiler().compile(logging_pipeline,  # type: ignore\n#                            pipeline_file)\n\n#client = kfp.Client()\n#client.create_run_from_pipeline_package(pipeline_file=pipeline_file,\n#                                        arguments={},\n#                                        experiment_name=\"Logging pipeline experiment\",\n#                                        namespace=None, )"
  },
  {
    "repo": "antongollbo123/kfp_utils",
    "file_path": "local_run/custom_python_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/antongollbo123/kfp_utils/main/local_run/custom_python_pipeline.py",
    "content": "import json\nfrom typing import Any, List\n\nimport pandas as pd  # type:ignore\nimport pyarrow  # type:ignore\nfrom google.cloud import storage  # type:ignore\nfrom google.oauth2 import service_account  # type:ignore\n\n# from google.cloud import storage  #type:ignore\n# from google.oauth2 import service_account  #type:ignore\nfrom kfp import compiler, dsl, local  # type:ignore\nfrom kfp.dsl import Artifact, Dataset, Input, Output #type:ignore\nimport trace\n\nlocal.init(runner=local.DockerRunner())\n\n\n@dsl.component(\n    base_image=\"custom_python:latest\",\n    packages_to_install=[\"numpy\", \"pyarrow\", \"pandas\"],\n)\ndef read_from_gcs(\n    bucket_name: str, blob_name: str, output_dataset: Output[Dataset]\n) -> None:\n    import logging\n    import os\n\n    import pandas as pd  # type:ignore\n    import pyarrow.parquet as pq  # type:ignore\n    from google.cloud import storage  # type:ignore\n\n    from google.oauth2 import service_account  # type:ignore\n\n    logging.basicConfig(level=logging.INFO)\n    logger = logging.getLogger(__name__)\n    try:\n        os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"credentials.json\"\n        gcs_path = f\"gs://{bucket_name}/{blob_name}\"\n        table = pq.read_table(gcs_path)\n        logger.info(f\"Succesfully read {table}\")\n        local_path = \"output_file.parquet\"\n        output_dataset.name = \"output_file\"\n\n        df = table.to_pandas()\n        new_df = df.drop(columns=[\"Gender\"])\n        new_df.to_parquet(local_path)\n\n        client = storage.Client()\n        bucket = client.bucket(bucket_name+2)\n        blob = bucket.blob(f\"{output_dataset.name}.parquet\")\n        blob.upload_from_filename(local_path)\n\n        os.remove(local_path)\n        output_dataset.path = f\"gs://{bucket_name}/{output_dataset.name}.parquet\"\n        \n        logger.info(f\"Succesfully transformed to pandas df: {output_dataset.path},  \")\n    except Exception as e:\n        logger.error(f\"Error has occured: {e}\")\n        logger.exception(e)\n\n\n@dsl.component(\n    base_image=\"custom_python:latest\", packages_to_install=[\"pandas\", \"pyarrow\"]\n)\ndef do_df_operation(input_dataset: Input[Dataset]) -> str:\n    import logging\n    import os\n\n    import pandas as pd\n    import pyarrow.parquet as pq\n\n    logging.basicConfig(level=logging.INFO)\n    logger = logging.getLogger()\n\n    try:\n        logger.info(input_dataset.uri)\n        os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"credentials.json\"\n        table = pq.read_table(input_dataset.uri)\n        logger.info(f\"Succesfully read {table}\")\n        logger.info(f\"Path is equal to: {input_dataset.path}, type: {type(input_dataset.path)}\")\n        logger.info(f\"URI is equal to: {input_dataset.uri}, type: {type(input_dataset.uri)}\")\n        logger.info(f\"Metadata is equal to: {input_dataset.metadata}, type: {type(input_dataset.metadata)}\")\n        logger.info(f\"Name is equal to: {input_dataset.name}, type: {type(input_dataset.name)}\")\n        df = table.to_pandas()\n        return_val = df.iloc[0].Name\n    except Exception as e:\n        logger.error(f\"Error {e} caught...\")\n    return return_val\n\n\n@dsl.pipeline\ndef df_pass_pipeline(bucket_name: str, blob_name: str) -> None:\n    task1 = read_from_gcs(bucket_name=bucket_name, blob_name=blob_name)  # type:ignore\n    task2 = do_df_operation(input_dataset=task1.outputs[\"output_dataset\"])\n\n\npipeline = df_pass_pipeline(\n    bucket_name=\"new_bucket123123213\", blob_name=\"example_file.parquet\"\n)"
  },
  {
    "repo": "antongollbo123/kfp_utils",
    "file_path": "local_run/custom_python_pipeline_multi.py",
    "raw_url": "https://raw.githubusercontent.com/antongollbo123/kfp_utils/main/local_run/custom_python_pipeline_multi.py",
    "content": "import json\nfrom typing import Any, List\nimport logging\n\nimport pandas as pd  # type:ignore\nimport pyarrow  # type:ignore\nfrom google.cloud import storage  # type:ignore\nfrom google.oauth2 import service_account  # type:ignore\n\n# from google.cloud import storage  #type:ignore\n# from google.oauth2 import service_account  #type:ignore\nfrom kfp import compiler, dsl, local  # type:ignore\nfrom kfp.dsl import Artifact, Dataset, Input, Output, OutputPath  # type:ignore\n\nlocal.init(runner=local.DockerRunner())\n\n\n@dsl.component(\n    base_image=\"python:3.11\",\n    packages_to_install=[\"numpy\", \"pyarrow\", \"pandas\"],\n)\ndef save_df(output_dataset: Output[Dataset]\n) -> Dataset:\n    \n    import logging\n    import os\n\n    import pandas as pd  # type:ignore\n    import pyarrow.parquet as pq  # type:ignore\n\n    logging.basicConfig(level=logging.INFO)\n    logger = logging.getLogger(__name__)\n\n    data1 = {'Name': ['Alice', 'Bob', 'Charlie'],\n         'Age': [25, 30, 35],\n         'City': ['New York', 'Los Angeles', 'Chicago']}\n    df1 = pd.DataFrame(data1)\n\n    # Creating the second example DataFrame\n    data2 = {'Product': ['Laptop', 'Phone', 'Tablet'],\n            'Price': [1200, 800, 500],\n            'Stock': [10, 20, 15]}\n    df2 = pd.DataFrame(data2)\n    \n    df1.to_csv(output_dataset.path, header=True, index=False)\n    df2.to_csv(output_dataset.path, header=True, index=False)\n    \n    logger.info(f\"Succesfully saved df to disk at location {output_dataset.path}!\")\n\n@dsl.component(\n    base_image=\"python:3.11\",\n    packages_to_install=[\"numpy\", \"pyarrow\", \"pandas\"],\n)\ndef read_df(input_dataset: Input[Dataset]) -> None:\n    import pandas as pd\n    import logging\n    df1  = pd.read_csv(input_dataset.path)\n    return_val = f\"DF_SHAPE: {df1.shape[0]}\"\n    logger = logging.getLogger(__name__)\n    logging.basicConfig(level=logging.INFO)\n    logger.info(input_dataset.path)\n    logger.info(input_dataset.uri)\n\n\n@dsl.pipeline\ndef df_pass_pipeline() -> None:\n    task1 = save_df()  # type:ignore\n    task2 = read_df(input_dataset=task1.outputs[\"output_dataset\"])\n\npipe_df = df_pass_pipeline()\n"
  },
  {
    "repo": "antongollbo123/kfp_utils",
    "file_path": "local_run/gcp_client_dependency_injection.py",
    "raw_url": "https://raw.githubusercontent.com/antongollbo123/kfp_utils/main/local_run/gcp_client_dependency_injection.py",
    "content": "import json\nfrom typing import Any, List\nimport logging\n\nimport pandas as pd  # type:ignore\nimport pyarrow  # type:ignore\nfrom google.cloud import storage, client  # type:ignore\nfrom google.oauth2 import service_account  # type:ignore\n\n# from google.cloud import storage  #type:ignore\n# from google.oauth2 import service_account  #type:ignore\nfrom kfp import compiler, dsl, local  # type:ignore\nfrom kfp.dsl import Artifact, Dataset, Input, Output, OutputPath  # type:ignore\nfrom google.cloud.storage import Client\n\n@dsl.component(\n    base_image=\"python:3.11\",\n    packages_to_install=[\"numpy\", \"google\"],\n)\ndef component_inject_client(client: Client) -> list:\n    from google.cloud.storage import Client\n    bucket = client.bucket(bucket_name=\"anton-test-bucket123\")\n    blob_list = bucket.list_blobs()\n    blob_name_list = []\n    for blob in blob_list:\n        blob_name_list.append(blob.name)\n    return blob_list\n\n\n\n\n@dsl.pipeline\ndef dep_injection_pipeline() -> None:\n    client = storage.Client.from_service_account_json(\"credentials.json\")\n    output_list = component_inject_client(client=client)\n    print(output_list)\n\npipe = dep_injection_pipeline()"
  },
  {
    "repo": "antongollbo123/kfp_utils",
    "file_path": "local_run/get_gcs_object_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/antongollbo123/kfp_utils/main/local_run/get_gcs_object_pipeline.py",
    "content": "import json\n\nfrom google.cloud import storage  # type:ignore\nfrom google.oauth2 import service_account  # type:ignore\n\n# from google.cloud import storage  #type:ignore\n# from google.oauth2 import service_account  #type:ignore\nfrom kfp import compiler, dsl, local  # type:ignore\nfrom kfp.dsl import Artifact, Output  # type:ignore\nimport jsonargparse\n\nlocal.init(runner=local.DockerRunner())\n\n\n@dsl.component(\n    base_image=\"custom_python:latest\",\n    packages_to_install=[\"numpy\", \"pandas\", \"pyarrow\"],\n)\ndef get_gcs_object(\n    bucket_name: str\n) -> list:\n    \"\"\"\n    Custom Python image has 'google' package installed, but not numpy.\n    Further, GCP credentials are mounted into this image.\n    \"\"\"\n    import logging\n    from google.cloud import storage  # type:ignore\n    logging.basicConfig(level=logging.INFO)\n    logger = logging.getLogger(__name__)\n    try:\n        bucket_name = \"anton-test-bucket123\"\n        client = storage.Client.from_service_account_json(\"credentials.json\")\n        bucket = client.bucket(bucket_name=bucket_name)\n        blob_list = bucket.list_blobs()\n        blob_name_list = []\n        for blob in blob_list:\n            blob_name_list.append(f\"{blob.name}\")\n\n    except Exception as e:\n        logger.error(f\"Error has occured: {e}\")\n    return blob_name_list\n\n@dsl.component(\n    base_image=\"custom_python:latest\",\n    packages_to_install=[\"numpy\", \"pandas\", \"pyarrow\"],\n)\n\ndef print_gcs_objects(blob_list: list, bucket_name: str) -> None:\n    import pandas as pd\n    from google.cloud import storage\n    from io import BytesIO\n    client = storage.Client.from_service_account_json(\"credentials.json\")\n    bucket = client.bucket(bucket_name=bucket_name)\n    for blob in blob_list: \n        blob = bucket.get_blob(blob)\n        content = blob.download_as_string()\n        print(content)\n\n#@dsl.pipeline\ndef pipeline_orchestrator(bucket_name: str, \n                          pipeline_name: str,\n                          upload_path: str\n                          ) -> None:\n    print(pipeline_name)\n    if pipeline_name == \"mega_pipeline\":\n        print(\"true\")\n        @dsl.pipeline\n        def mega_pipeline() -> None:\n            task_1 = get_gcs_object(bucket_name=bucket_name)\n            print(task_1.output)\n            task_2 = print_gcs_objects(blob_list=task_1.output, bucket_name=bucket_name)\n        def mega_pipeline2() -> None:\n            task_1 = get_gcs_object(bucket_name=bucket_name)\n        pipeline = mega_pipeline()\n\nif __name__ == \"__main__\":\n    jsonargparse.CLI(pipeline_orchestrator, as_positional=False)\n    #pipe = gcs_obj_pipeline(bucket_name=\"anton-test-bucket123\")\n"
  },
  {
    "repo": "antongollbo123/kfp_utils",
    "file_path": "local_run/import_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/antongollbo123/kfp_utils/main/local_run/import_pipeline.py",
    "content": "from number_sum_pipeline import number_sum_pipeline\nfrom kfp.dsl import pipeline # type:ignore\nfrom kfp import dsl, local  # type:ignore\nfrom google.cloud import bigquery \nlocal.init(runner=local.SubprocessRunner())\n\n\n@pipeline\ndef double_pipeline() -> None:\n    pipeline1 = number_sum_pipeline(x=1, y=2, z=3)\n    pipeline2 = number_sum_pipeline(x=2, y=3, z=4).after(pipeline1)\n\n\npipe = double_pipeline()"
  },
  {
    "repo": "antongollbo123/kfp_utils",
    "file_path": "local_run/number_sum_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/antongollbo123/kfp_utils/main/local_run/number_sum_pipeline.py",
    "content": "from kfp import dsl, local  # type:ignore\n\nlocal.init(runner=local.SubprocessRunner())\n\n\n@dsl.component\ndef add(a: int, b: int) -> int:\n    import logging\n\n    logging.basicConfig(level=logging.INFO)\n    logger = logging.getLogger(__name__)\n    logger.info(f\"Hello from inside the pipeline, with message {a}, {b}\")\n    return a + b\n\n@dsl.component\ndef add_2(a: int, b: int) -> list:\n    return [a,b]\n\n@dsl.component\ndef add_3(a: list) -> int:\n    import logging\n    logging.basicConfig(level=logging.INFO)\n    logger = logging.getLogger(__name__)\n    sum_ = 0\n    for i in a:\n        logger.info(f\"Iterating through list, with element {i}\")\n        sum_ += i\n    sum_ = int(sum_)\n    return sum_\n\n@dsl.component\ndef add_4(a: list) -> list:\n    str_list = []\n    for i in a:\n        str_list.append(str(i))\n    return str_list\n\n\n# or run it in a pipeline\n@dsl.pipeline\ndef number_sum_pipeline(x: int, y: int, z: int) -> list:\n    t1 = add(a=x, b=y)  # type:ignore\n    t2 = add(a=t1.output, b=z)  # type:ignore\n    t3 = add_2(a=t2.output, b=t2.output)\n    #t4 = add_3(a=t3.output)\n    t4 = add_4(a=t3.output)\n    return t4.output"
  },
  {
    "repo": "antongollbo123/kfp_utils",
    "file_path": "local_run/test_import.py",
    "raw_url": "https://raw.githubusercontent.com/antongollbo123/kfp_utils/main/local_run/test_import.py",
    "content": "from get_gcs_object_pipeline import get_gcs_object, print_gcs_objects\nimport kfp #type:ignore\nfrom kfp import dsl\n\n@dsl.pipeline\ndef mega_pipeline() -> None:\n    task_1 = get_gcs_object(bucket_name=\"anton-test-bucket123\")\n    task_2 = print_gcs_objects(blob_list=task_1.output)\n\npipe = mega_pipeline()"
  },
  {
    "repo": "antongollbo123/kfp_utils",
    "file_path": "local_testing/testing_mock_example.py",
    "raw_url": "https://raw.githubusercontent.com/antongollbo123/kfp_utils/main/local_testing/testing_mock_example.py",
    "content": "from kfp.dsl import component, pipeline  # type:ignore\nimport typing\nfrom kfp.dsl import Dataset, Output, Input, component, Model  # type:ignore\nimport pytest\nimport pandas as pd  # type:ignore\n\n\ndef make_test_artifact(artifact_type):\n    class TestArtifact(artifact_type):  # type: ignore\n        def _get_path(self):\n            return super()._get_path() or self.uri\n\n    return TestArtifact\n\n\n@pytest.fixture(scope=\"session\", name=\"input_data\")\ndef input_dataset_artifact(tmp_path_factory):\n    temp_dir = tmp_path_factory.mktemp(\"artifact_store\")\n    uri = str(temp_dir / \"input.csv\")\n    df = pd.DataFrame({\"text\": [\"Hello world\", \"Goodbye world\"]})\n    df.to_csv(uri)\n    output_dataset = make_test_artifact(Dataset)(uri=uri)\n    return output_dataset\n\n\n@pytest.fixture(scope=\"session\", name=\"output_data\")\ndef output_dataset_artifact(tmp_path_factory):\n    temp_dir = tmp_path_factory.mktemp(\"artifact_store\")\n    uri = str(temp_dir / \"output.csv\")\n    output_dataset = make_test_artifact(Dataset)(uri=uri)\n    return output_dataset\n\n\n@component(\n    packages_to_install=[\n        \"pandas==1.3.4\",\n        \"numpy==1.23.2\",\n    ],\n    base_image=\"python:3.10.5-slim-bullseye\",\n)\ndef clean_text_data(input_art: Input[Dataset], output_art: Output[Dataset]) -> None:\n    import pandas as pd\n    import numpy as np\n\n    df = pd.read_csv(input_art.path)\n\n    df[\"text\"] = df[\"text\"].str.lower()\n    df[\"text\"] = df[\"text\"].str.replace(r\"[^\\w\\s]\", \"\")\n    df[\"text\"] = df[\"text\"].str.replace(r\"\\d+\", \"\")\n    df[\"text\"] = df[\"text\"].str.replace(r\"\\s+\", \" \")\n    df[\"text\"] = df[\"text\"].str.strip()\n\n    df.to_csv(output_art.path, index=False)\n\n\ndef test_clean_text_data_cleans_text_data(input_data, output_data):\n    clean_text_data.python_func(\n        input_art=input_data, output_art=output_data)\n    df = pd.read_csv(output_data.path)\n    breakpoint()\n    assert df['text'].str.contains('hello world').any()\n    assert df['text'].str.contains('goodbye world').any()"
  },
  {
    "repo": "talaman/kubeflow-ml-pipeline",
    "file_path": "mlp-example/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/talaman/kubeflow-ml-pipeline/main/mlp-example/pipeline.py",
    "content": "\n\nfrom typing import Dict, List\n\nfrom kfp import dsl\nfrom kfp.dsl import Input, InputPath, Output, OutputPath, Dataset, Model, component, HTML\nimport kfp\n\n\n@component(packages_to_install=['scikit-learn','pandas'])\ndef exctract_data(\n    dataset: Output[Dataset],\n):\n    from sklearn import datasets\n    data = datasets.load_iris(as_frame=True).frame\n    with open(dataset.path, 'w') as f:\n        f.write(data.to_csv(index=False))\n\n@component(packages_to_install=['evidently'],base_image='python:3.10')\ndef data_validation(\n    dataset: Input[Dataset],\n    tests: Output[HTML],\n    reports: Output[HTML],\n):\n    from evidently.test_suite import TestSuite\n    from evidently.test_preset import DataQualityTestPreset\n    from evidently.test_preset import DataStabilityTestPreset\n    from evidently.report import Report\n    from evidently.metric_preset import DataQualityPreset\n    from evidently.metric_preset import DataDriftPreset\n    import pandas as pd\n    \n    with open(dataset.path, 'r') as input_file:\n        data = pd.read_csv(input_file)\n\n    tests_suite= TestSuite(tests=[\n        DataStabilityTestPreset(),\n        DataQualityTestPreset()\n    ])\n    tests_suite.run(current_data=data.iloc[:60], reference_data=data.iloc[60:], column_mapping=None)\n    tests_suite.save_html(tests.path)\n\n\n    reports_suite = Report(metrics=[\n        DataQualityPreset(),\n        DataDriftPreset()\n    ])\n\n    reports_suite.run(current_data=data.iloc[:60], reference_data=data.iloc[60:], column_mapping=None)\n    reports_suite.save_html(reports.path)\n\n\n@component(packages_to_install=['scikit-learn', 'pandas'], base_image='python:3.9')\ndef data_preparation(\n    dataset: Input[Dataset],\n    dataset_transformed: Output[Dataset],\n):\n    import pandas as pd\n    from sklearn.preprocessing import MinMaxScaler, OneHotEncoder    \n\n    NUMERIC_FEATURE_KEYS = ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n    LABEL_KEY = 'target'\n\n    def preprocessing_fn(inputs):\n        \"\"\"Preprocess input columns into transformed columns.\"\"\"\n        outputs = inputs.copy()\n\n        # Scale numeric features to range [0, 1]\n        scaler = MinMaxScaler()\n        outputs[NUMERIC_FEATURE_KEYS] = scaler.fit_transform(outputs[NUMERIC_FEATURE_KEYS])\n\n        # One-hot encode the label column\n        encoder = OneHotEncoder()\n        outputs[LABEL_KEY] = encoder.fit_transform(outputs[LABEL_KEY].values.reshape(-1, 1)).toarray()\n\n        return outputs\n\n    with open(dataset.path, 'r') as input_file:\n        data = pd.read_csv(input_file)\n\n    data_transformed = preprocessing_fn(data)\n\n    with open(dataset_transformed.path, 'w') as f:\n        f.write(data_transformed.to_csv(index=False))\n\n@component(packages_to_install=['pandas', 'scikit-learn'],base_image='python:3.9')\ndef split_data(dataset: Input[Dataset], dataset_train: Output[Dataset], dataset_validation: Output[Dataset], dataset_test: Output[Dataset]):\n    import pandas as pd\n    from sklearn.model_selection import train_test_split\n    with open(dataset.path, 'r') as input_file:\n        data = pd.read_csv(input_file)\n\n    train, test = train_test_split(data, test_size=0.2)\n    train, validation = train_test_split(train, test_size=0.2)\n\n    with open(dataset_train.path, 'w') as f:\n        f.write(train.to_csv(index=False))\n    with open(dataset_validation.path, 'w') as f:\n        f.write(validation.to_csv(index=False))\n    with open(dataset_test.path, 'w') as f:\n        f.write(test.to_csv(index=False))\n\n@component(packages_to_install=['tensorflow', 'pandas', 'joblib'\n                                ],base_image='python:3.9')\ndef train_model(\n    dataset_train: Input[Dataset],\n    dataset_validation: Input[Dataset],\n    model_artifact: Output[Model],\n    epochs: int = 10,\n):\n    import pandas as pd\n    import tensorflow as tf\n    from tensorflow import keras\n    from tensorflow.keras import layers\n    def create_model():\n        tf_model = keras.Sequential([\n            layers.Dense(64, activation='relu', input_shape=(4,)),\n            layers.Dense(64, activation='relu'),\n            layers.Dense(3, activation='softmax')\n        ])\n        return tf_model\n\n\n    with open(dataset_train.path, 'r') as train_file:\n        train_data = pd.read_csv(train_file)\n    with open(dataset_validation.path, 'r') as validation_file:\n        validation_data = pd.read_csv(validation_file)\n\n    # Preprocess the data\n    train_features = train_data.drop('target', axis=1)\n    train_labels = train_data['target']\n    validation_features = validation_data.drop('target', axis=1)\n    validation_labels = validation_data['target']\n\n    # Create the model\n    tf_model = create_model()\n\n    # Compile the model\n    tf_model.compile(optimizer='adam',\n                    loss='sparse_categorical_crossentropy',\n                    metrics=['accuracy'])\n\n    # Train the model\n    tf_model.fit(train_features, train_labels, epochs=epochs, validation_data=(validation_features, validation_labels))\n\n    # Evaluate the model\n    r = tf_model.evaluate(validation_features, validation_labels)\n    print(\"Result:\",r)\n\n    # Save the model\n    tf_model.save( \"model_artifact.keras\")\n\n    # Copy the model to the output path\n    import shutil\n    shutil.move(\"model_artifact.keras\", model_artifact.path)\n\n\n\n        \n\n\n@component(packages_to_install=['tensorflow', 'pandas'],base_image='python:3.9')\ndef validate_model(model_artifact: Input[Model], dataset_test: Input[Dataset]):\n    import pandas as pd\n    import tensorflow as tf\n\n    # Copy the model to the current directory\n    import shutil\n    shutil.copy(model_artifact.path, \"model_artifact.keras\")\n\n    # Load the model\n    tf_model = tf.keras.models.load_model(\"model_artifact.keras\")\n\n    with open(dataset_test.path, 'r') as test_file:\n        test_data = pd.read_csv(test_file)\n\n    # Preprocess the data\n    test_features = test_data.drop('target', axis=1)\n    test_labels = test_data['target']\n\n    # Evaluate the model\n    r = tf_model.evaluate(test_features, test_labels)\n    print(r)\n\n\n\n@dsl.pipeline(pipeline_root='', name='mlp-example-pipeline')\ndef ml_pipeline(message: str = 'message'):\n    exctract_data_task = exctract_data()\n    data_validation_task = data_validation(dataset=exctract_data_task.outputs['dataset'])\n    data_preparation_task = data_preparation(dataset=exctract_data_task.outputs['dataset']).after(data_validation_task)\n    split_data_task = split_data(dataset=data_preparation_task.outputs['dataset_transformed'])\n    train_model_task = train_model(epochs=30, dataset_train=split_data_task.outputs['dataset_train'], dataset_validation=split_data_task.outputs['dataset_validation'])\n    validate_model_task = validate_model(model_artifact=train_model_task.outputs['model_artifact'], dataset_test=split_data_task.outputs['dataset_test'])\n\n\n\ndef run_pipeline():\n    host = \"http://localhost:9000/pipeline\"\n    \n    # Compile and run the pipeline\n    kfp.compiler.Compiler().compile(ml_pipeline, 'mlp-example.yaml')\n    kfp.Client(host=host).create_run_from_pipeline_func(ml_pipeline, arguments={})\n\nif __name__ == '__main__':\n    run_pipeline()"
  },
  {
    "repo": "zinzinhust96/kubeflow_pipeline_examples",
    "file_path": "build_pipeline/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/zinzinhust96/kubeflow_pipeline_examples/master/build_pipeline/pipeline.py",
    "content": "import kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import component\nfrom kfp.v2.dsl import (\n    Input,\n    Output,\n    Artifact,\n    Dataset,\n)\n\n@component(\n    packages_to_install=['pandas==1.1.4'],\n    output_component_file='component.yaml'\n)\ndef merge_csv(tar_data: Input[Artifact], output_csv: Output[Dataset]):\n    import glob\n    import pandas as pd\n    import tarfile\n\n    tarfile.open(name=tar_data.path, mode=\"r|gz\").extractall('data')\n    df = pd.concat(\n        [pd.read_csv(csv_file, header=None)\n        for csv_file in glob.glob('data/*.csv')])\n    df.to_csv(output_csv.path, index=False, header=False)\n\nweb_downloader_op = kfp.components.load_component_from_url(\n    'https://raw.githubusercontent.com/kubeflow/pipelines/master/components/web/Download/component-sdk-v2.yaml')\n\n# Define a pipeline and create a task from a component:\n@dsl.pipeline(\n    name='build_pipeline_example',\n    # You can optionally specify your own pipeline_root\n    # pipeline_root='gs://my-pipeline-root/example-pipeline',\n)\ndef build_pipeline(url: str):\n    web_downloader_task = web_downloader_op(url=url)\n    print('web_downloader_task: ', web_downloader_task)\n    print('web_downloader_task output: ', web_downloader_task.outputs['data'])\n    merge_csv_task = merge_csv(tar_data=web_downloader_task.outputs['data'])\n    print('merge_csv_task: ', merge_csv_task)\n    print('merge_csv_task.outputs: ', merge_csv_task.outputs)\n    # The outputs of the merge_csv_task can be referenced using the\n    # merge_csv_task.outputs dictionary: merge_csv_task.outputs['output_csv']\n\nclient = kfp.Client(\"http://localhost:8080/pipeline\")\nclient.create_run_from_pipeline_func(\n    build_pipeline,\n    mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE,\n    # You can optionally override your pipeline_root when submitting the run too:\n    # pipeline_root='gs://my-pipeline-root/example-pipeline',\n    arguments={\n        'url': 'https://storage.googleapis.com/ml-pipeline-playground/iris-csv-files.tar.gz'\n    })"
  },
  {
    "repo": "zinzinhust96/kubeflow_pipeline_examples",
    "file_path": "kubeflow_pipeline_sample/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/zinzinhust96/kubeflow_pipeline_examples/master/kubeflow_pipeline_sample/pipeline.py",
    "content": "import kfp\nimport kfp.components as comp\nfrom kfp.v2.dsl import component\nfrom kubernetes import client as k8s_client\nfrom kfp.v2.dsl import (\n    Input,\n    Output,\n    Artifact,\n    Dataset,\n)\n\ncreate_step_get_lines = comp.load_component_from_file('./component.yaml')\n\n# create_step_get_lines is a \"factory function\" that accepts the arguments\n# for the component's inputs and output paths and returns a pipeline step\n# (ContainerOp instance).\n#\n# To inspect the get_lines_op function in Jupyter Notebook, enter\n# \"get_lines_op(\" in a cell and press Shift+Tab.\n# You can also get help by entering `help(get_lines_op)`, `get_lines_op?`,\n# or `get_lines_op??`.\n\n@component\ndef print_op(input_txt: Input[Artifact]):\n    with open(input_txt.path, 'r') as fr:\n        for line in fr.readlines():\n            print(line.strip())\n\n# Define your pipeline\n@kfp.dsl.pipeline(\n    name=\"example-pipeline\",\n)\ndef my_pipeline():\n    get_lines_step = create_step_get_lines(\n        # Input name \"Input 1\" is converted to pythonic parameter name \"input_1\"\n        input_1='/data_processing/input.txt',\n        parameter_1='5',\n        ).add_volume(k8s_client.V1Volume(name='data-processing',\n                                         host_path=k8s_client.V1HostPathVolumeSource(path='/home/docker/data_processing'))) \\\n        .add_volume_mount(k8s_client.V1VolumeMount(mount_path='/data_processing',\n                                                   name='data-processing'))\n\n    print('write output at: ', get_lines_step.outputs, type(get_lines_step.outputs['output_1']))\n    check_output_step = print_op(get_lines_step.outputs['output_1'])\n\n\n# def my_pipeline():\n#     get_lines_step = create_step_get_lines(\n#         # Input name \"Input 1\" is converted to pythonic parameter name \"input_1\"\n#         input_1='one\\ntwo\\nthree\\nfour\\nfive\\nsix\\nseven\\neight\\nnine\\nten',\n#         parameter_1='5',\n#     )\n\n# If you run this command on a Jupyter notebook running on Kubeflow,\n# you can exclude the host parameter.\n# client = kfp.Client()\nclient = kfp.Client(host='http://localhost:8080/pipeline')\n\n# Compile, upload, and submit this pipeline for execution.\nclient.create_run_from_pipeline_func(my_pipeline, arguments={},\n    mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE)\n"
  },
  {
    "repo": "zinzinhust96/kubeflow_pipeline_examples",
    "file_path": "train_classifier/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/zinzinhust96/kubeflow_pipeline_examples/master/train_classifier/pipeline.py",
    "content": "import os\nfrom pathlib import Path\nimport requests\nfrom kubernetes import client as k8s_client\n\nimport kfp\n\ncomponent_url_prefix = '/data_processing/'\ntest_data_url_prefix = component_url_prefix + 'testdata'\n\n#Prepare input/output paths and data\n# input_data_gcs_dir = 'gs://<my bucket>/<path>/'\n# output_data_gcs_dir = 'gs://<my bucket>/<path>/'\n\n#Downloading the training set (to upload to GCS later)\n# training_set_features_local_path = os.path.join('.', 'training_set_features.tsv')\n# training_set_labels_local_path = os.path.join('.', 'training_set_labels.tsv')\n\n# training_set_features_url = test_data_url_prefix + '/training_set_features.tsv'\n# training_set_labels_url = test_data_url_prefix + '/training_set_labels.tsv'\n\ntraining_set_features_local_path = os.path.join(test_data_url_prefix, 'training_set_features.tsv')\ntraining_set_labels_local_path = os.path.join(test_data_url_prefix, 'training_set_labels.tsv')\n\n# Path(training_set_features_local_path).write_bytes(requests.get(training_set_features_url).content)\n# Path(training_set_labels_local_path).write_bytes(requests.get(training_set_labels_url).content)\n\n#Uploading the data to GCS where it can be read by the trainer\n# training_set_features_gcs_path = os.path.join(input_data_gcs_dir, 'training_set_features.tsv')\n# training_set_labels_gcs_path = os.path.join(input_data_gcs_dir, 'training_set_labels.tsv')\n\n# gfile.Copy(training_set_features_local_path, training_set_features_gcs_path)\n# gfile.Copy(training_set_labels_local_path, training_set_labels_gcs_path)\n\n# output_model_uri_template = os.path.join(output_data_gcs_dir, kfp.dsl.EXECUTION_ID_PLACEHOLDER, 'output_model_uri', 'data')\n# output_model_local_template = component_url_prefix + 'tests/output/trained'\n\n# xor_model_config = requests.get('https://raw.githubusercontent.com/kubeflow/pipelines/master/components/sample/keras/train_classifier/tests/testdata/' + 'model_config.json').content\n\nxor_model_config = Path('./tests/testdata').joinpath('model_config.json').read_text()\n# print('xor_model_config: ', type(xor_model_config), xor_model_config)\n\n\n#Load the component\ntrain_op = kfp.components.load_component_from_file('./component.yaml')\nkeras_convert_hdf5_model_to_tf_saved_model_op = kfp.components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/51e49282d9511e4b72736c12dc66e37486849c6e/components/_converters/KerasModelHdf5/to_TensorflowSavedModel/component.yaml')\n\n#Use the component as part of the pipeline\n@kfp.dsl.pipeline(name='Test keras/train_classifier', description='Pipeline to test keras/train_classifier component')\ndef pipeline_to_test_keras_train_classifier():\n    train_task = train_op(\n        training_set_features_path=training_set_features_local_path,\n        training_set_labels_path=training_set_labels_local_path,\n        model_config=xor_model_config,\n        number_of_classes=2,\n        number_of_epochs=10,\n        batch_size=32,\n    ).add_volume(k8s_client.V1Volume(name='data-processing',\n                                     host_path=k8s_client.V1HostPathVolumeSource(path='/home/docker/data_processing'))) \\\n    .add_volume_mount(k8s_client.V1VolumeMount(mount_path='/data_processing',\n                                               name='data-processing'))\n\n    keras_model_in_tf_format = keras_convert_hdf5_model_to_tf_saved_model_op(\n        model=train_task.outputs['output_model_uri'],\n    ).output\n    #Use train_task.outputs['output_model_uri'] to obtain the reference to the trained model URI that can be a passed to other pipeline tasks (e.g. for prediction or analysis)\n\n\n# If you run this command on a Jupyter notebook running on Kubeflow,\n# you can exclude the host parameter.\n# client = kfp.Client()\nclient = kfp.Client(host='http://localhost:8080/pipeline')\n\n# Compile, upload, and submit this pipeline for execution.\nclient.create_run_from_pipeline_func(pipeline_to_test_keras_train_classifier, arguments={},\n    mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE)"
  },
  {
    "repo": "ima-tester/sample-kubeflow-app-001",
    "file_path": "components/golang/component.py",
    "raw_url": "https://raw.githubusercontent.com/ima-tester/sample-kubeflow-app-001/master/components/golang/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef training_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('TRAINING_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=['/usr/local/bin/python', script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n\n"
  },
  {
    "repo": "ima-tester/sample-kubeflow-app-001",
    "file_path": "components/training/component.py",
    "raw_url": "https://raw.githubusercontent.com/ima-tester/sample-kubeflow-app-001/master/components/training/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef training_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('TRAINING_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=['/usr/local/bin/python', script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n\n"
  },
  {
    "repo": "davide-belfiori/kubeflow_test_pipelines",
    "file_path": "simple_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/davide-belfiori/kubeflow_test_pipelines/master/simple_pipeline.py",
    "content": "\"\"\"\n    End-to-End Kubeflow Pipeline, from data loading to model serving.\n\"\"\"\n\n# TODOs: \n#   - add minio_access_key and minio_secret_key as parameter for all components\n\n# --------------\n# --- IMPORT ---\n# --------------\n\nimport sys\nfrom kfp import dsl, compiler, components\nfrom typing import NamedTuple\n\n# -----------------\n# --- FUNCTIONS ---\n# -----------------\n\ndef load_dataset(m: float = 0.5,\n                 q: float = 1.0,\n                 noise_mean: float = 0,\n                 noise_scale: float = 0.1,\n                 test_size: float = 0.3,\n                 random_state: int = 123,\n                 sep: str = ',',\n                 minio_endpoint: str = \"minio-service.kubeflow\",\n                 bucket_name: str = \"datasets\",\n                 update_dataset: bool = False):\n    \"\"\"\n    Function for data loading.\n    \"\"\"\n    # >>> IMPORT\n    import pandas as pd\n    import numpy as np\n    from sklearn.model_selection import train_test_split\n    from minio import Minio\n\n    print(\"--- Loading dataset ---\")\n\n    # >>> CONNECT TO MINIO\n    minio_client = Minio(endpoint=minio_endpoint+\":9000\",\n                         access_key=\"minio\",\n                         secret_key=\"minio123\",\n                         secure=False)\n    \n    if not minio_client.bucket_exists(bucket_name=bucket_name):\n        print(\"--- Creating bucket {} ---\".format(bucket_name))\n        minio_client.make_bucket(bucket_name=bucket_name)\n\n    obj_names = list(map(lambda obj: obj.object_name, minio_client.list_objects(bucket_name=bucket_name)))\n    if not update_dataset and \"simple_pipeline_dataset/\" in obj_names:   \n        # >>> DOWNLOAD DATASET FROM MINIO\n        minio_client.fget_object(bucket_name=bucket_name,\n                                    object_name=\"simple_pipeline_dataset/train_data.csv\",\n                                    file_path=\"data/train_data.csv\")\n        minio_client.fget_object(bucket_name=bucket_name,\n                                    object_name=\"simple_pipeline_dataset/test_data.csv\",\n                                    file_path=\"data/test_data.csv\")\n        print(\"--- Dataset found ---\")\n        return\n    else:\n        print(\"--- Creating Dataset ---\")\n        # >>> CREATE DATA\n        X = np.linspace(-10, 10, 100)\n        Y = m * X + q\n        rs = np.random.RandomState(np.random.MT19937(seed = random_state))\n        Y += rs.normal(loc = noise_mean, scale = noise_scale, size = Y.shape)\n\n        # >>> TRAIN-TEST SPLIT\n        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, shuffle=True, random_state=random_state)\n        train_df = pd.DataFrame({\"X\": X_train, \"Y\": Y_train})\n        test_df = pd.DataFrame({\"X\": X_test, \"Y\": Y_test})\n\n        # >>> SAVE DATA LOCALLY\n        train_df.to_csv(f\"data/train_data.csv\", sep=sep, index=False)\n        test_df.to_csv(f\"data/test_data.csv\", sep=sep, index=False)\n\n        # >>> UPLOAD TO MINIO\n        minio_client.fput_object(bucket_name = bucket_name, \n                                 object_name = \"simple_pipeline_dataset/train_data.csv\",\n                                 file_path = \"data/train_data.csv\")\n        minio_client.fput_object(bucket_name = bucket_name, \n                                 object_name = \"simple_pipeline_dataset/test_data.csv\",\n                                 file_path = \"data/test_data.csv\")\n\n        print(\"--- Data csv saved to MinIO location simple_pipeline_dataset/ ---\")\n\ndef print_data_desc(sep: str = ',', \n                    minio_endpoint: str = \"minio-service.kubeflow\", \n                    bucket_name: str = \"datasets\") -> NamedTuple('PrintDataOutput', [('mlpipeline_ui_metadata', 'UI_metadata')]):\n    \"\"\"\n    Print Dataset description.\n    \"\"\"\n    # >>> IMPORT\n    import pandas as pd\n    from minio import Minio\n    from collections import namedtuple\n    import json\n\n    # >>> CONNECT TO MINIO\n    minio_client = Minio(endpoint=minio_endpoint+\":9000\",\n                         access_key=\"minio\",\n                         secret_key=\"minio123\",\n                         secure=False)\n    if not minio_client.bucket_exists(bucket_name=bucket_name):\n        raise ValueError(\"{} bucket does not exists.\".format(bucket_name))\n\n    # >>> DOWNLOAD DATASET FROM MINIO\n    try:\n        minio_client.fget_object(bucket_name=bucket_name,\n                                    object_name=\"simple_pipeline_dataset/train_data.csv\",\n                                    file_path=\"data/train_data.csv\")\n        minio_client.fget_object(bucket_name=bucket_name,\n                                    object_name=\"simple_pipeline_dataset/test_data.csv\",\n                                    file_path=\"data/test_data.csv\")\n    except:\n        raise RuntimeError(\"Error while loading dataset.\")\n    \n    # >>> READ DATASET\n    train_df = pd.read_csv(f\"data/train_data.csv\", sep=sep)\n    test_df = pd.read_csv(f\"data/test_data.csv\", sep=sep)\n    \n    # >>> PRINT DESCRIPTION\n    print(\"--- TRAIN SET ---\")\n    print(\"> Info :\")\n    print(train_df.info())\n    print(\"> Description :\")\n    print(train_df.describe())\n\n    print(\"--- TEST SET ---\")\n    print(\"> Info :\")\n    print(test_df.info())\n    print(\"> Description :\")\n    print(test_df.describe())\n\n    train_head = train_df.head().to_csv(index = False, header = False)\n    test_head = test_df.head().to_csv(index = False, header = False)\n\n    metadata = {\n        'outputs' : [{\n            'type': 'table',\n            'format': 'csv',\n            'storage': 'inline',\n            'header': train_df.columns.to_list(),\n            'source': train_head\n        },\n        {\n            'type': 'table',\n            'format': 'csv',\n            'storage': 'inline',\n            'header': test_df.columns.to_list(),\n            'source': test_head\n        }]\n    }\n    output = namedtuple('PrintDataOutput', ['mlpipeline_ui_metadata'])\n    return output(json.dumps(metadata))\n\ndef train_model(sep: str = ',',\n                save_model: bool = True,\n                model_name: str = \"simple_pipeline_model\",\n                minio_endpoint: str = \"minio-service.kubeflow\",\n                data_bucket: str = \"datasets\",\n                model_bucket: str = \"models\") -> NamedTuple('TrainModelOutput', [('mlpipeline_ui_metadata', 'UI_metadata')]):\n    \"\"\"\n    Train a Linear Regression model.\n    \"\"\"\n    # >>> IMPORT\n    import pandas as pd\n    from sklearn.linear_model import LinearRegression\n    from minio import Minio\n    import joblib\n    import plotly.graph_objects as go\n    from collections import namedtuple\n    import json\n\n    # >>> CONNECT TO MINIO\n    minio_client = Minio(endpoint=minio_endpoint+\":9000\",\n                         access_key=\"minio\",\n                         secret_key=\"minio123\",\n                         secure=False)\n    if not minio_client.bucket_exists(bucket_name=data_bucket):\n        raise ValueError(\"{} bucket does not exists.\".format(data_bucket))\n\n    # >>> DOWNLOAD DATASET FROM MINIO\n    try:\n        minio_client.fget_object(bucket_name=data_bucket,\n                                    object_name=\"simple_pipeline_dataset/train_data.csv\",\n                                    file_path=\"data/train_data.csv\")\n    except:\n        raise RuntimeError(\"Error while loading dataset.\")\n\n    # >>> SPLIT DATA\n    train_df = pd.read_csv(f\"data/train_data.csv\", sep=sep)\n    X_train = train_df.loc[:,\"X\"].values.reshape(-1, 1)\n    Y_train = train_df.loc[:,\"Y\"].values.reshape(-1, 1)\n\n    # >>> TRAIN MODEL\n    model = LinearRegression()\n    model.fit(X=X_train, y=Y_train)\n\n    # >>> SAVE MODEL\n    if save_model:\n        # Dump model\n        model_filename = \"data/\" + model_name + \".joblib\"\n        joblib.dump(model, filename=model_filename)\n        # Upload to MinIO\n        if not minio_client.bucket_exists(bucket_name=model_bucket):\n            print(\"--- Creating bucket {} ---\".format(model_bucket))\n            minio_client.make_bucket(bucket_name=model_bucket)\n        try:\n            minio_client.fput_object(bucket_name = model_bucket, \n                                    object_name = \"simple_pipeline/\"+ model_name + \".joblib\",\n                                    file_path = model_filename)\n        except:\n            raise RuntimeError(\"Error while uploading model.\") \n        print(\"--- Model saved to MinIO location simple_pipeline/ ---\")\n\n    # >>> VISUALIZE MODEL\n    score = model.score(X_train, Y_train)\n    Y_pred = model.predict(X_train)\n    fig = go.Figure()\n    fig.add_trace(go.Scatter(x=X_train[:,0], y=Y_train[:,0], mode=\"markers\", name=\"train data\"))\n    fig.add_trace(go.Scatter(x=X_train[:,0], y=Y_pred[:,0], mode=\"lines\", name=\"model\"))\n    fig.update_layout({\n        'title': \"Model on train set (R2: {:.2f})\".format(score),\n        'xaxis_title': \"X\",\n        'yaxis_title': \"Y\"\n    })\n    fig.write_html(\"data/train_plot.html\")\n    minio_client.fput_object(bucket_name = model_bucket, \n                             object_name = \"simple_pipeline/train_plot.html\",\n                             file_path = \"data/train_plot.html\")\n    metadata = {\n        'outputs' : [{\n            'source': \"minio://{}/simple_pipeline/train_plot.html\".format(model_bucket),\n            'type': 'web-app',\n    }]}\n    output = namedtuple('TrainModelOutput', ['mlpipeline_ui_metadata'])\n    return output(json.dumps(metadata))\n\ndef eval_model(minio_endpoint: str = \"minio-service.kubeflow\",\n               data_bucket: str = \"datasets\",\n               sep: str = ',',\n               model_bucket: str = \"models\",\n               model_name: str = \"simple_pipeline_model\") -> NamedTuple('EvalModelOutput', [('mlpipeline_ui_metadata', 'UI_metadata'), ('mlpipeline_metrics', 'Metrics')]):\n    \"\"\"\n    Evaluate model on test set.\n    \"\"\"\n    # >>> IMPORT\n    from minio import Minio\n    import pandas as pd\n    import joblib\n    import plotly.graph_objects as go\n    from collections import namedtuple\n    import json\n\n    # >>> CONNECT TO MINIO\n    minio_client = Minio(endpoint=minio_endpoint+\":9000\",\n                         access_key=\"minio\",\n                         secret_key=\"minio123\",\n                         secure=False)\n    if not minio_client.bucket_exists(bucket_name=data_bucket):\n        raise ValueError(\"{} bucket does not exists.\".format(data_bucket))\n    if not minio_client.bucket_exists(bucket_name=model_bucket):\n        raise ValueError(\"{} bucket does not exists.\".format(model_bucket))\n\n    # >>> DOWNLOAD DATASET AND MODEL FROM MINIO\n    try:\n        minio_client.fget_object(bucket_name=data_bucket,\n                                 object_name=\"simple_pipeline_dataset/test_data.csv\",\n                                 file_path=\"data/test_data.csv\")\n        minio_client.fget_object(bucket_name=model_bucket,\n                                 object_name=\"simple_pipeline/\"+ model_name + \".joblib\",\n                                 file_path=\"data/\" + model_name + \".joblib\")\n    except:\n        raise RuntimeError(\"Error while loading dataset and model.\")\n\n    # >>> SPLIT DATA\n    test_df = pd.read_csv(f\"data/test_data.csv\", sep=sep)\n    X_test = test_df.loc[:,\"X\"].values.reshape(-1, 1)\n    Y_test = test_df.loc[:,\"Y\"].values.reshape(-1, 1)\n\n    # >>> LOAD MODEL\n    model = joblib.load(filename=\"data/\" + model_name + \".joblib\")\n\n    # >>> EVAL MODEL\n    score = model.score(X=X_test, y=Y_test)\n    Y_pred = model.predict(X_test)\n    fig = go.Figure()\n    fig.add_trace(go.Scatter(x=X_test[:,0], y=Y_test[:,0], mode=\"markers\", name=\"test data\"))\n    fig.add_trace(go.Scatter(x=X_test[:,0], y=Y_pred[:,0], mode=\"lines\", name=\"model\"))\n    fig.update_layout({\n        'title': \"Model on test set (R2: {:.2f})\".format(score),\n        'xaxis_title': \"X\",\n        'yaxis_title': \"Y\"\n    })\n    fig.write_html(\"data/test_plot.html\")\n    minio_client.fput_object(bucket_name = model_bucket, \n                             object_name = \"simple_pipeline/test_plot.html\",\n                             file_path = \"data/test_plot.html\")\n    metadata = {\n        'outputs' : [{\n            'source': \"minio://{}/simple_pipeline/test_plot.html\".format(model_bucket),\n            'type': 'web-app',\n    }]}\n\n    # >>> LOG METRICS\n    print(\"Score on test set: {:.2f} %\".format(score * 100))\n    metrics = {\n      'metrics': [{\n          'name': 'test-r2-score',\n          'numberValue':  float(score),\n          'format' : \"PERCENTAGE\"\n        }]}\n    \n    output = namedtuple('EvalModelOutput', ['mlpipeline_ui_metadat', 'mlpipeline_metrics'])\n    return output(json.dumps(metadata), json.dumps(metrics))\n\ndef serve_model(service_name: str = \"simple-serving\",\n                add_service_version: bool = True,\n                namespace: str = None,\n                kserve_version: str = \"v1beta1\",\n                model_bucket: str = \"models\",\n                model_name: str = \"simple_pipeline_model\",\n                service_account_name: str = \"kserve-service-credentials\",\n                update_credentials: bool = False,\n                minio_endpoint: str = \"minio-service.kubeflow\",\n                minio_access_key: str = \"minio\",\n                minio_secret_key: str = \"minio123\",\n                access_key_name: str = None,\n                secret_key_name: str = None,\n                profile: str = \"default\",\n                use_https: bool = False,\n                verify_ssl: bool = False):\n    \"\"\"\n    Create an InferenceService for the trained model.\n    \"\"\"\n    # >>> IMPORT\n    from kserve import (\n        KServeClient, \n        utils, \n        constants, \n        V1beta1SKLearnSpec, \n        V1beta1PredictorSpec,\n        V1beta1InferenceServiceSpec,\n        V1beta1InferenceService\n    )\n    from kubernetes import client\n    import base64\n    import datetime\n\n    KServe = KServeClient()\n    if namespace == None:\n        namespace = utils.get_default_target_namespace()\n\n    # >>> MINIO CREDENTIALS\n    if update_credentials:\n        # Encode access and secret key in base64\n        b64_access_key = base64.b64encode(minio_access_key.encode(\"ascii\"))\n        b64_access_key = b64_access_key.decode(\"ascii\")\n        b64_secret = base64.b64encode(minio_secret_key.encode(\"ascii\"))\n        b64_secret = b64_secret.decode(\"ascii\")\n        # Write credentials\n        if access_key_name == None or access_key_name == \"\":\n            access_key_name = constants.S3_ACCESS_KEY_ID_DEFAULT_NAME\n        if secret_key_name == None or secret_key_name == \"\":\n            secret_key_name = constants.S3_SECRET_ACCESS_KEY_DEFAULT_NAME\n        credentials = \"[{profile}]\\n{access_key_name}={access_key}\\n{secret_key_name}={secret_key}\".format(\n            profile = profile,\n            access_key_name = access_key_name,\n            access_key = b64_access_key, \n            secret_key_name = secret_key_name,\n            secret_key = b64_secret)\n        with open(\"data/credentials\", \"wt\") as cred_file:\n            cred_file.write(credentials)\n        KServe.set_credentials(storage_type='S3',\n                                namespace=namespace,\n                                credentials_file='data/credentials',\n                                service_account=service_account_name,\n                                s3_profile=profile,\n                                s3_endpoint=minio_endpoint+\":9000\",\n                                s3_use_https='1' if use_https else '0',\n                                s3_verify_ssl='1' if verify_ssl else '0')\n        \n    # >>> INFERENCE SERVICE OPTIONS\n    api_version = constants.KSERVE_GROUP + '/' + kserve_version\n    storage_uri = \"s3://{}/simple_pipeline/{}.joblib\".format(model_bucket, model_name)\n    if add_service_version:\n        service_name = service_name + \"-\" + datetime.datetime.now().strftime(\"%Y-%m-%d--%H-%M-%S\")\n\n    # >>> INFERENCE SERVICE DEFINITION\n    metadata = client.V1ObjectMeta(name = service_name, namespace = namespace)\n    sklearn_spec = V1beta1SKLearnSpec(storage_uri = storage_uri)\n    predictor = V1beta1PredictorSpec(sklearn = sklearn_spec, service_account_name = service_account_name)\n    spec = V1beta1InferenceServiceSpec(predictor = predictor)\n    isvc = V1beta1InferenceService(api_version = api_version,\n                                   kind = constants.KSERVE_KIND,\n                                   metadata = metadata,\n                                   spec = spec)\n    \n    # >>> INFERENCE SERVICE CREATION\n    info = KServe.create(isvc)\n    print(info)\n\n# ----------------\n# --- PIPELINE ---\n# ----------------\n\n# > COMPONENTS\nload_dataset_component = components.create_component_from_func(\n    func = load_dataset,\n    base_image = \"python:3.8\",\n    packages_to_install = [\"pandas==2.0.0\", \n                            \"numpy==1.24.2\", \n                            \"scikit-learn==1.2.2\",\n                            \"minio==7.1.14\"]\n)\nprint_data_desc_component = components.create_component_from_func(\n    func = print_data_desc,\n    base_image = \"python:3.8\",\n    packages_to_install = [\"pandas==2.0.0\", \n                           \"minio==7.1.14\"]\n)\ntrain_model_component = components.create_component_from_func(\n    func = train_model,\n    base_image = \"python:3.8\",\n    packages_to_install = [\"pandas==2.0.0\", \n                           \"numpy==1.24.2\", \n                           \"scikit-learn==1.2.2\", \n                           \"plotly==5.14.1\",\n                           \"minio==7.1.14\",\n                           \"joblib==1.2.0\"] \n)\neval_model_component = components.create_component_from_func(\n    func = eval_model,\n    base_image = \"python:3.8\",\n    packages_to_install = [\"pandas==2.0.0\",\n                           \"plotly==5.14.1\",\n                           \"minio==7.1.14\",\n                           \"scikit-learn==1.2.2\",\n                           \"joblib==1.2.0\"]\n)\nserve_model_component = components.create_component_from_func(\n    func = serve_model,\n    base_image = \"python:3.8\",\n    packages_to_install = [\"kserve==0.10.1\",\n                           \"kubernetes==25.3.0\"]\n)\n\n# > PIPELINE\n@dsl.pipeline(\n        name=\"simple-pipeline\",\n        description=\"Simple pipeline example.\"\n)\ndef simple_pipeline(m: float = 0.5,\n                    q: float = 1.0,\n                    noise_mean: float = 0,\n                    noise_scale: float = 0.1,\n                    test_size: float = 0.3,\n                    random_state: int = 123,\n                    update_dataset: bool = False,\n                    sep: str = ',',\n                    save_model: bool = True,\n                    model_name: str = \"simple_pipeline_model\",\n                    minio_endpoint: str = \"minio-service.kubeflow\",\n                    data_bucket: str = \"datasets\",\n                    model_bucket: str = \"models\",\n                    inference_service_name: str = \"simple-serving\",\n                    add_inference_service_version: bool = True,\n                    namespace: str = None,\n                    kserve_version: str = \"v1beta1\",\n                    service_account_name: str = \"kserve-service-credentials\",\n                    update_credentials: bool = False,\n                    access_key_name: str = None,\n                    secret_key_name: str = None,\n                    profile: str = \"default\",\n                    use_https: bool = False,\n                    verify_ssl: bool = False):\n    \"\"\"\n    Pipeline function\n    \"\"\"\n    # >>> Create a volume for this pipeline\n    vop = dsl.VolumeOp(name = \"simple_pipeline_volume\", \n                        resource_name = \"simple_pipeline_volume\",\n                        size = \"1Gi\",\n                        modes = dsl.VOLUME_MODE_RWO)\n    # >>> Run tasks\n    load_data_task = load_dataset_component(m = m, \n                                            q = q,\n                                            noise_mean = noise_mean,\n                                            noise_scale = noise_scale,\n                                            test_size = test_size,\n                                            random_state = random_state,\n                                            sep = sep,\n                                            minio_endpoint = minio_endpoint,\n                                            bucket_name = data_bucket,\n                                            update_dataset = update_dataset).add_pvolumes({\"/data\": vop.volume})\n    print_desc_task = print_data_desc_component(sep = sep,\n                                                minio_endpoint = minio_endpoint,\n                                                bucket_name = data_bucket).add_pvolumes({\"/data\": vop.volume}).after(load_data_task)\n    train_model_task = train_model_component(sep = sep,\n                                            save_model = save_model,\n                                            model_name = model_name,\n                                            minio_endpoint = minio_endpoint,\n                                            data_bucket = data_bucket,\n                                            model_bucket = model_bucket).add_pvolumes({\"/data\": vop.volume}).after(load_data_task)\n    eval_model_task = eval_model_component(sep = sep,\n                                            model_name = model_name,\n                                            minio_endpoint = minio_endpoint,\n                                            data_bucket = data_bucket,\n                                            model_bucket = model_bucket).add_pvolumes({\"/data\": vop.volume}).after(train_model_task)\n    serve_model_task = serve_model_component(service_name = inference_service_name,\n                                             add_service_version = add_inference_service_version,\n                                             namespace = namespace,\n                                             kserve_version = kserve_version,\n                                             model_bucket = model_bucket,\n                                             model_name = model_name,\n                                             service_account_name = service_account_name,\n                                             update_credentials = update_credentials,\n                                             minio_endpoint = minio_endpoint,\n                                             minio_access_key = \"minio\",\n                                             minio_secret_key = \"minio123\",\n                                             access_key_name = access_key_name,\n                                             secret_key_name = secret_key_name,\n                                             profile = profile,\n                                             use_https = use_https,\n                                             verify_ssl = verify_ssl).add_pvolumes({\"/data\": vop.volume}).after(eval_model_task)\n    # >>> Disable caching\n    load_data_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n    print_desc_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n    train_model_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n    eval_model_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n    serve_model_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n    \n\nif __name__ == \"__main__\":\n    a_count = len(sys.argv)\n    if a_count > 1:\n        mode = sys.argv[1]\n    else:\n        mode = \"test\"\n    if mode == \"test\":\n        # >>> Run test\n        minio_endpoint = \"127.0.0.1\"\n        load_dataset(minio_endpoint = minio_endpoint)\n        print_data_desc(minio_endpoint = minio_endpoint)\n        train_model(minio_endpoint = minio_endpoint)\n        eval_model(minio_endpoint = minio_endpoint)\n        # serve_model(minio_endpoint = minio_endpoint)\n    elif mode == \"compile\":\n        # >>> Compile pipeline\n        compiler.Compiler().compile(\n            pipeline_func=simple_pipeline,\n            package_path='pipelines/SimplePipeline.yaml'\n        )\n    else:\n        print(\"USAGE: {} [OPTION]\".format(sys.argv[0]))\n        print(\"OPTIONS:\\n \\ttest: Run test.\\n \\tcompile Compile pipeline\")\n"
  },
  {
    "repo": "davide-belfiori/kubeflow_test_pipelines",
    "file_path": "test_minio_connection.py",
    "raw_url": "https://raw.githubusercontent.com/davide-belfiori/kubeflow_test_pipelines/master/test_minio_connection.py",
    "content": "\"\"\"\n    Kubeflow Pipeline for MinIO connection test.\n\"\"\"\n\n# --------------\n# --- IMPORT ---\n# --------------\n\nimport kfp\nimport sys\n\n# -----------------\n# --- CONSTANTS ---\n# -----------------\n\nPIPELINE_NAME = \"minio-connection-test\"\nPIPELINE_DESC = \"MinIO connection test pipeline\"\nPIPELINE_FILENAME = \"MinIOConnectionTest\"\n\n# ------------------\n# --- CONN. TEST ---\n# ------------------\n\ndef test_connection(endpoint_addr: str = \"minio-service.kubeflow.svc.cluster.local\",\n                    endpoint_port: str = \"9000\",\n                    access_key: str = \"minio\",\n                    secret_key: str = \"minio123\",\n                    secure: bool = False,\n                    remove_tmp_bucket: bool = True):\n    # > IMPORT\n    from minio import Minio\n    from datetime import datetime\n    # > MINIO CLIENT\n    minio_client = Minio(\n        endpoint = endpoint_addr + \":\" + endpoint_port,\n        access_key = access_key,\n        secret_key = secret_key,\n        secure = secure\n    )\n    # > CREATE AN EMPTY BUCKET\n    dt = datetime.now().strftime(\"%m%d%Y-%H%M%S\")\n    bucket_name = \"tmp-bucket-\"+dt\n    print(\"--- Creating bucket {} ---\".format(bucket_name))\n    minio_client.make_bucket(bucket_name=bucket_name)\n    # > REMOVE BUCKET\n    if remove_tmp_bucket:\n        print(\"--- Removing bucket {} ---\".format(bucket_name))\n        minio_client.remove_bucket(bucket_name=bucket_name)\n\n# ----------------\n# --- PIPELINE ---\n# ----------------\n\n# > DEFINE COMPONENTS\ntest_connection_component = kfp.components.create_component_from_func(\n    func = test_connection,\n    base_image = \"python:3.8\",\n    packages_to_install = [\"minio==7.1.14\"]\n)\n\n# > PIPELINE\n@kfp.dsl.pipeline(\n    name = PIPELINE_NAME,\n    description = PIPELINE_DESC\n)\ndef test_connection_pipeline(endpoint_addr: str = \"minio-service.kubeflow.svc.cluster.local\",\n                             endpoint_port: str = \"9000\",\n                             access_key: str = \"minio\",\n                             secret_key: str = \"minio123\",\n                             secure: bool = False,\n                             remove_tmp_bucket: bool = True):\n    # > RUN TASKS\n    test_connection_task = test_connection_component(endpoint_addr = endpoint_addr,\n                                                     endpoint_port = endpoint_port,\n                                                     access_key = access_key,\n                                                     secret_key = secret_key,\n                                                     secure = secure,\n                                                     remove_tmp_bucket = remove_tmp_bucket)\n    # > DISABLE CACHING\n    test_connection_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n\nif __name__ == \"__main__\":\n    a_count = len(sys.argv)\n    if a_count > 1:\n        mode = sys.argv[1]\n    else:\n        mode = \"test\"\n    if mode == \"test\":\n        # >>> Run test\n        minio_endpoint = \"127.0.0.1\"\n        test_connection(endpoint_addr=minio_endpoint)\n    elif mode == \"compile\":\n        # >>> Compile pipeline\n        kfp.compiler.Compiler().compile(\n            pipeline_func=test_connection_pipeline,\n            package_path='pipelines/' + PIPELINE_FILENAME + '.yaml'\n        )\n    else:\n        print(\"USAGE: {} [OPTION]\".format(sys.argv[0]))\n        print(\"OPTIONS:\\n \\ttest: Run test.\\n \\tcompile Compile pipeline\")"
  },
  {
    "repo": "ahmedyass/mnist-kubeflow-pipeline",
    "file_path": "components.py",
    "raw_url": "https://raw.githubusercontent.com/ahmedyass/mnist-kubeflow-pipeline/main/components.py",
    "content": "# data_processing.py\nfrom kfp.dsl import component, Output, Dataset\n\n@component(\n        packages_to_install=['tensorflow==2.4.0', 'numpy']\n)\ndef preprocess_data(\n    training_data_output: Output[Dataset],\n    test_data_output: Output[Dataset]\n):\n    import tensorflow as tf\n    from tensorflow.keras.datasets import mnist\n    import numpy as np\n    import os\n\n    # Load dataset\n    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n\n    # Normalize data\n    x_train, x_test = x_train / 255.0, x_test / 255.0\n\n    # Save preprocessed data to the component's output paths\n    np.savez_compressed(training_data_output.path, x_train=x_train, y_train=y_train)\n    np.savez_compressed(test_data_output.path, x_test=x_test, y_test=y_test)\n\n# model_training.py\nfrom kfp.v2.dsl import component, Input, Output, Model, Dataset\n\n@component(\n        packages_to_install=['tensorflow==2.4.0', 'numpy']\n)\ndef train_model(\n    training_data: Input[Dataset],\n    model_output: Output[Model]\n):\n    import tensorflow as tf\n    from tensorflow.keras.models import Sequential\n    from tensorflow.keras.layers import Dense, Flatten\n    from tensorflow.keras.utils import to_categorical\n    import numpy as np\n\n    # Load preprocessed training data\n    with np.load(training_data.path) as data:\n        x_train = data['x_train']\n        y_train = data['y_train']\n    y_train = to_categorical(y_train)\n\n    # Define and compile the model\n    model = Sequential([\n        Flatten(input_shape=(28, 28)),\n        Dense(128, activation='relu'),\n        Dense(10, activation='softmax')\n    ])\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n    # Train the model\n    model.fit(x_train, y_train, epochs=5)\n\n    # Save the trained model\n    model.save(model_output.path)\n\n# model_evaluation.py\nfrom kfp.dsl import component, Input, Model\n\n@component(\n        packages_to_install=['tensorflow==2.4.0']\n)\ndef evaluate_model(\n    model_input: Input[Model]\n    ):\n    import tensorflow as tf\n    from tensorflow.keras.datasets import mnist\n    from tensorflow.keras.utils import to_categorical\n\n    # Load and preprocess test dataset\n    (_, _), (x_test, y_test) = mnist.load_data()\n    x_test = x_test / 255.0\n    y_test = to_categorical(y_test)\n\n    # Load the model from the input path provided by Kubeflow\n    model = tf.keras.models.load_model(model_input.path)\n\n    # Evaluate the model\n    loss, accuracy = model.evaluate(x_test, y_test)\n    print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")\n\n# pipeline.py\nfrom kfp.dsl import pipeline\nimport kfp.compiler as compiler\nfrom data_loading import preprocess_data\nfrom model_evaluation import evaluate_model\nfrom model_training import train_model\n\n@pipeline(\n  name='mnist-classification-pipeline',\n  description='A pipeline that processes MNIST data and trains a model.'\n)\ndef mnist_pipeline():\n    preprocess_task = preprocess_data()\n    train_task = train_model(\n        training_data=preprocess_task.outputs['training_data_output']\n    )\n    evaluate_task = evaluate_model(\n        model_input=train_task.outputs['model_output']\n    )\n    \n# Compile the pipeline\ncompiler.Compiler().compile(\n    pipeline_func=mnist_pipeline,\n    package_path='../pipeline/mnist_pipeline.yaml'\n)\n"
  },
  {
    "repo": "ahmedyass/mnist-kubeflow-pipeline",
    "file_path": "pipeline_def.py",
    "raw_url": "https://raw.githubusercontent.com/ahmedyass/mnist-kubeflow-pipeline/main/pipeline_def.py",
    "content": "from kfp.dsl import pipeline\nimport kfp.compiler as compiler\nfrom data_loading import load_dataset\nfrom data_processing import preprocessing\nfrom model_building import model_building\nfrom model_training import model_training\n\n@pipeline(\n    name='mnist-classifier-dev',\n    description='Detect digits'\n)\ndef mnist_pipeline(hyperparameters: dict):\n    load_task = load_dataset()\n    preprocess_task = preprocessing(\n        x_train_artifact = load_task.outputs[\"x_train_artifact\"],\n        x_test_artifact = load_task.outputs[\"x_test_artifact\"]\n    )\n\n    model_building_task = model_building()\n\n    training_task = model_training(\n        ml_model = model_building_task.outputs[\"ml_model\"],\n        x_train_processed = preprocess_task.outputs[\"x_train_processed\"],\n        x_test_processed = preprocess_task.outputs[\"x_test_processed\"],\n        y_train_artifact = load_task.outputs[\"y_train_artifact\"],\n        y_test_artifact = load_task.outputs[\"y_test_artifact\"],\n        hyperparameters = hyperparameters\n    )\n\n    \n# Compile the pipeline\ncompiler.Compiler().compile(\n    pipeline_func=mnist_pipeline,\n    package_path='output/mnist_pipeline.yaml'\n)\n"
  },
  {
    "repo": "ahmedyass/mnist-kubeflow-pipeline",
    "file_path": "data_loading/__init__.py",
    "raw_url": "https://raw.githubusercontent.com/ahmedyass/mnist-kubeflow-pipeline/main/data_loading/__init__.py",
    "content": "from kfp.dsl import component, Output, Dataset\n\n@component(\n        base_image=\"tensorflow/tensorflow\"\n)\ndef load_dataset(x_train_artifact: Output[Dataset],\n                 x_test_artifact: Output[Dataset],\n                 y_train_artifact: Output[Dataset],\n                 y_test_artifact: Output[Dataset]\n    ):\n    '''\n    get dataset from Keras and load it separating input from output and train from test\n    '''\n    import numpy as np\n    from tensorflow import keras\n    import os\n   \n    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n    \n    np.save(\"/tmp/x_train.npy\",x_train)\n    os.rename(\"/tmp/x_train.npy\", x_train_artifact.path)\n    \n    np.save(\"/tmp/y_train.npy\",y_train)\n    os.rename(\"/tmp/y_train.npy\", y_train_artifact.path)\n    \n    np.save(\"/tmp/x_test.npy\",x_test)\n    os.rename(\"/tmp/x_test.npy\", x_test_artifact.path)\n    \n    np.save(\"/tmp/y_test.npy\",y_test)\n    os.rename(\"/tmp/y_test.npy\", y_test_artifact.path)\n"
  },
  {
    "repo": "ahmedyass/mnist-kubeflow-pipeline",
    "file_path": "data_processing/__init__.py",
    "raw_url": "https://raw.githubusercontent.com/ahmedyass/mnist-kubeflow-pipeline/main/data_processing/__init__.py",
    "content": "from kfp.dsl import component, Input, Output, Dataset, Metrics\n\n@component(\n        packages_to_install['numpy']\n)\ndef preprocessing(metrics : Output[Metrics], x_train_processed : Output[Dataset], x_test_processed: Output[Dataset],\n                  x_train_artifact: Input[Dataset], x_test_artifact: Input[Dataset]):\n    ''' \n    just reshape and normalize data\n    '''\n    import numpy as np\n    import os\n    \n    # load data artifact store\n    x_train = np.load(x_train_artifact.path) \n    x_test = np.load(x_test_artifact.path)\n    \n    # reshaping the data\n    # reshaping pixels in a 28x28px image with greyscale, canal = 1. This is needed for the Keras API\n    x_train = x_train.reshape(-1,28,28,1)\n    x_test = x_test.reshape(-1,28,28,1)\n    # normalizing the data\n    # each pixel has a value between 0-255. Here we divide by 255, to get values from 0-1\n    x_train = x_train / 255\n    x_test = x_test / 255\n    \n    #logging metrics using Kubeflow Artifacts\n    metrics.log_metric(\"Len x_train\", x_train.shape[0])\n    metrics.log_metric(\"Len y_train\", x_test.shape[0])\n   \n    \n    # save feuture in artifact store\n    np.save(\"tmp/x_train.npy\",x_train)\n    os.rename(\"tmp/x_train.npy\", x_train_processed.path)\n    \n    np.save(\"tmp/x_test.npy\",x_test)\n    os.rename(\"tmp/x_test.npy\", x_test_processed.path)"
  },
  {
    "repo": "ahmedyass/mnist-kubeflow-pipeline",
    "file_path": "model_building/__init__.py",
    "raw_url": "https://raw.githubusercontent.com/ahmedyass/mnist-kubeflow-pipeline/main/model_building/__init__.py",
    "content": "import os\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import component, Input, Output, Dataset, Model, Metrics, ClassificationMetrics\n\n@component(base_image=\"tensorflow/tensorflow\")\ndef model_building(ml_model : Output[Model]):\n    '''\n    Define the model architecture\n    This way it's more simple to change the model architecture and all the steps and indipendent\n    '''\n    from tensorflow import keras\n    import tensorflow as tf\n    import os\n    \n    #model definition\n    model = keras.models.Sequential()\n    model.add(keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=(28,28,1)))\n    model.add(keras.layers.MaxPool2D(2, 2))\n\n    model.add(keras.layers.Flatten())\n    model.add(keras.layers.Dense(64, activation='relu'))\n    model.add(keras.layers.Dense(32, activation='relu'))\n\n    model.add(keras.layers.Dense(10, activation='softmax'))\n    \n    #saving model\n    model.save(ml_model.path)"
  },
  {
    "repo": "ahmedyass/mnist-kubeflow-pipeline",
    "file_path": "model_evaluation/__init__.py",
    "raw_url": "https://raw.githubusercontent.com/ahmedyass/mnist-kubeflow-pipeline/main/model_evaluation/__init__.py",
    "content": "from kfp.dsl import component, Input, Model\n\n@component(\n        packages_to_install=['tensorflow==2.4.0']\n)\ndef evaluate_model(\n    model_input: Input[Model]\n    ):\n    import tensorflow as tf\n    from tensorflow.keras.datasets import mnist\n    from tensorflow.keras.utils import to_categorical\n\n    # Load and preprocess test dataset\n    (_, _), (x_test, y_test) = mnist.load_data()\n    x_test = x_test / 255.0\n    y_test = to_categorical(y_test)\n\n    # Load the model from the input path provided by Kubeflow\n    model = tf.keras.models.load_model(model_input.path)\n\n    # Evaluate the model\n    loss, accuracy = model.evaluate(x_test, y_test)\n    print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")\n"
  },
  {
    "repo": "ahmedyass/mnist-kubeflow-pipeline",
    "file_path": "model_training/__init__.py",
    "raw_url": "https://raw.githubusercontent.com/ahmedyass/mnist-kubeflow-pipeline/main/model_training/__init__.py",
    "content": "import os\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import component, Input, Output, Dataset, Model, Metrics, ClassificationMetrics\n\n@component(base_image=\"tensorflow/tensorflow\", packages_to_install=['scikit-learn'])\ndef model_training(\n    ml_model : Input[Model],\n    x_train_processed : Input[Dataset], x_test_processed: Input[Dataset],\n    y_train_artifact : Input[Dataset], y_test_artifact :Input[Dataset],\n    hyperparameters : dict, \n    metrics: Output[Metrics], classification_metrics: Output[ClassificationMetrics], model_trained: Output[Model]\n    ):\n    \"\"\"\n    Build the model with Keras API\n    Export model metrics\n    \"\"\"\n    from tensorflow import keras\n    import tensorflow as tf\n    import numpy as np\n    import os\n    import glob\n    from sklearn.metrics import confusion_matrix\n    \n    #load dataset\n    x_train = np.load(x_train_processed.path)\n    x_test = np.load(x_test_processed.path)\n    y_train = np.load(y_train_artifact.path)\n    y_test = np.load(y_test_artifact.path)\n    \n    #load model structure\n    model = keras.models.load_model(ml_model.path)\n    \n    #reading best hyperparameters from katib\n    lr=float(hyperparameters[\"lr\"])\n    no_epochs = int(hyperparameters[\"num_epochs\"])\n    \n    #compile the model - we want to have a binary outcome\n    model.compile(tf.keras.optimizers.SGD(learning_rate=lr),\n              loss=\"sparse_categorical_crossentropy\",\n              metrics=['accuracy'])\n\n    \n    #fit the model and return the history while training\n    history = model.fit(\n      x=x_train,\n      y=y_train,\n      epochs=no_epochs,\n      batch_size=20,\n    )\n\n     \n    # Test the model against the test dataset\n    # Returns the loss value & metrics values for the model in test mode.\n    model_loss, model_accuracy = model.evaluate(x=x_test,y=y_test)\n    \n    #build a confusione matrix\n    y_predict = model.predict(x=x_test)\n    y_predict = np.argmax(y_predict, axis=1)\n    cmatrix = confusion_matrix(y_test, y_predict)\n    cmatrix = cmatrix.tolist()\n    numbers_list = ['0','1','2','3','4','5','6','7','8','9']\n    #log confusione matrix\n    classification_metrics.log_confusion_matrix(numbers_list,cmatrix)\n  \n    #Kubeflox metrics export\n    metrics.log_metric(\"Test loss\", model_loss)\n    metrics.log_metric(\"Test accuracy\", model_accuracy)\n    \n    #adding /1/ subfolder for TFServing and saving model to artifact store\n    model_trained.uri = model_trained.uri + '/1/'\n    keras.models.save_model(model,model_trained.path)"
  },
  {
    "repo": "MouadE0/Kubeflow-Regression-Pipeline",
    "file_path": "pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/MouadE0/Kubeflow-Regression-Pipeline/main/pipeline.py",
    "content": "import kfp\nfrom kfp import dsl\nfrom kfp.components import func_to_container_op\n# import matplotlib.pyplot as plt\n\n\n@func_to_container_op\ndef show_results(score_rf_reg: float, score_hist_gradient_boosting: float, score_decision_tree: float\n, score_elastic_net: float, score_lasso: float, \n    score_ridge: float, score_polynomial : float\n) -> None:\n    # Given the outputs from decision_tree and logistic regression components\n    # the results are shown.\n    print(f\"Random Forest regression (accuracy): {score_rf_reg}\")\n    print(f\"Hist Gradient Boosting regression (accuracy): {score_hist_gradient_boosting}\")\n    print(f\"Decision Tree regression (accuracy): {score_decision_tree}\")\n    print(f\"Elastic Net regression (accuracy): {score_elastic_net}\")\n    print(f\"Lasso regression (accuracy): {score_lasso}\")\n    print(f\"Ridge regression (accuracy): {score_ridge}\")\n    print(f\"Polynomial regression (accuracy): {score_polynomial}\")\n    # Best Model\n    best_model = max(score_rf_reg, score_hist_gradient_boosting, score_decision_tree, score_elastic_net)\n    # Switch Case for best model\n    if best_model == score_rf_reg:\n        print(\"Best Model is Random Forest\" + str(best_model))\n    elif best_model == score_hist_gradient_boosting:\n        print(\"Best Model is Hist Gradient Boosting\" + str(best_model))\n    elif best_model == score_decision_tree:\n        print(\"Best Model is Decision Tree\" + str(best_model))\n    elif best_model == score_elastic_net:\n        print(\"Best Model is Elastic Net\" + str(best_model))\n    elif best_model == score_lasso:\n        print(\"Best Model is Lasso\" + str(best_model))\n    elif best_model == score_polynomial:\n        print(\"Best Model is Polynomial\" + str(best_model))\n    elif best_model == score_ridge:\n        print(\"Best Model is Ridge\" + str(best_model))\n    else:\n        print(\"No Best Model\")\n\n    # Print Comparative Graph\n    # plt.bar([\"Linear Regression\", \"Random Forest\", \n    #         \"Hist Gradient Boosting\", \n    #         \"Decision Tree\", \"Elastic Net\", \"Lasso\", \n    #         \"Polynomial\", \"Ridge\"],\n    #         [score_lin_reg, score_rf_reg, score_hist_gradient_boosting, score_decision_tree, \n    #         score_elastic_net, score_lasso, score_polynomial, score_ridge])\n    # plt.title(\"Comparative Graph\")\n    # plt.xlabel(\"Models\")\n    # plt.show()\n\n\n\n@dsl.pipeline(\n    name=\"Pipeline\",\n    description=\"Applies Preprocess, Linear and Random Forest Regression problem.\",\n)\ndef test_pipeline():\n    # # Loads the yaml manifest for each component\n    # preprocess_clean = kfp.components.load_component_from_file(\n    #     \"preprocess_clean/preprocess_clean.yaml\"\n    # )\n    # preprocess_split = kfp.components.load_component_from_file(\n    #     \"preprocess_split/preprocess_split.yaml\"\n    # )\n\n    # # Loads the yaml manifest for each component\n    preprocess = kfp.components.load_component_from_file(\n        \"preprocess/preprocess.yaml\"\n    )\n    rf_regression = kfp.components.load_component_from_file(\n        \"rf_regression/rf_regression.yaml\"\n    )\n    hist_gradient_boosting = kfp.components.load_component_from_file(\n        \"hist_gradient_boosting_regression/hist_gradient_boosting_regression.yaml\"\n    )\n    decision_tree = kfp.components.load_component_from_file(\n        \"decision_tree_regression/decision_tree_regression.yaml\"\n    )\n    elastic_net = kfp.components.load_component_from_file(\n        \"elastic_net_regression/elastic_net_regression.yaml\"\n    )\n    lasso = kfp.components.load_component_from_file(\n        \"lasso_regression/lasso_regression.yaml\"\n    )\n    ridge = kfp.components.load_component_from_file(\n        \"ridge_regression/ridge_regression.yaml\"\n    )\n    polynomial = kfp.components.load_component_from_file(\n        \"polynomial_regression/polynomial_regression.yaml\"\n    )\n\n    # Preprocess data \n    preprocess_task = preprocess()\n    rf_regression_task = rf_regression(preprocess_task.output)\n    hist_gradient_boosting_task = hist_gradient_boosting(preprocess_task.output)\n    decision_tree = decision_tree(preprocess_task.output)\n    elastic_net = elastic_net(preprocess_task.output)\n    lasso = lasso(preprocess_task.output)\n    ridge = ridge(preprocess_task.output)\n    polynomial = polynomial(preprocess_task.output)\n\n    # the component \"show_results\" is called to print the results.\n    show_results(rf_regression_task.output, \n                hist_gradient_boosting_task.output, decision_tree.output, \n                elastic_net.output, lasso.output, ridge.output, polynomial.output)\n\n\nif __name__ == \"__main__\":\n    kfp.compiler.Compiler().compile(test_pipeline, \"pipeline.yaml\")\n"
  },
  {
    "repo": "tanzumlai/sample-kubeflow-pipeline",
    "file_path": "app/main.py",
    "raw_url": "https://raw.githubusercontent.com/tanzumlai/sample-kubeflow-pipeline/main/app/main.py",
    "content": "import kfp.dsl as dsl\nfrom kfp import Client\nimport logging\nimport warnings\nfrom . import utils\nimport datetime\n\nlogging.basicConfig()\nlogging.getLogger().setLevel(logging.INFO)\nwarnings.filterwarnings('ignore')\n\n\n@dsl.pipeline(\n    name='cifar_cnn_kfp',\n    description='Pipeline which trains and serves a CNN model using Keras.'\n)\ndef cifar_pipeline():\n\n    # Upload Dataset\n    upload_dataset = dsl.ContainerOp(\n        name='upload_dataset',\n        image='oawofolu/ml-image-processor:latest',\n        command=\"python\",\n        arguments=[\n            \"/app/main.py\",\n            \"mlflow_entry=upload_dataset\",\n            f\"mlflow_stage={utils.get_env_var('MLFLOW_STAGE')}\",\n            f\"git_repo={utils.get_env_var('GIT_REPO')}\",\n            f\"experiment_name={utils.get_env_var('EXPERIMENT_NAME')}\",\n            f\"environment_name={utils.get_env_var('ENVIRONMENT_NAME')}\",\n            f\"mlflow_s3_uri={utils.get_cmd_arg('mlflow_s3_uri') or utils.get_env_var('MLFLOW_S3_ENDPOINT_URL')}\",\n            f\"mlflow_tracking_uri={utils.get_cmd_arg('mlflow_tracking_uri') or utils.get_env_var('MLFLOW_TRACKING_URI')}\",\n            f\"default_token={datetime.datetime.now()}\"\n        ]\n    )\n\n    # Train Model\n    train_model = dsl.ContainerOp(\n        name='train_model',\n        image='oawofolu/ml-image-processor:latest',\n        command=\"python\",\n        arguments=[\n            \"/app/main.py\",\n            \"mlflow_entry=train_model\",\n            f\"mlflow_stage={utils.get_env_var('MLFLOW_STAGE')}\",\n            f\"git_repo={utils.get_env_var('GIT_REPO')}\",\n            f\"experiment_name={utils.get_env_var('EXPERIMENT_NAME')}\",\n            f\"environment_name={utils.get_env_var('ENVIRONMENT_NAME')}\",\n            f\"mlflow_s3_uri={utils.get_cmd_arg('mlflow_s3_uri') or utils.get_env_var('MLFLOW_S3_ENDPOINT_URL')}\",\n            f\"mlflow_tracking_uri={utils.get_cmd_arg('mlflow_tracking_uri') or utils.get_env_var('MLFLOW_TRACKING_URI')}\",\n            f\"default_token={datetime.datetime.now()}\"\n        ]\n    )\n    train_model.after(upload_dataset)\n\n    # Evaluate Model\n    evaluate_model = dsl.ContainerOp(\n        name='evaluate_model',\n        image='oawofolu/ml-image-processor:latest',\n        command=\"python\",\n        arguments=[\n            \"/app/main.py\",\n            \"mlflow_entry=evaluate_model\",\n            f\"mlflow_stage={utils.get_env_var('MLFLOW_STAGE')}\",\n            f\"git_repo={utils.get_env_var('GIT_REPO')}\",\n            f\"experiment_name={utils.get_env_var('EXPERIMENT_NAME')}\",\n            f\"environment_name={utils.get_env_var('ENVIRONMENT_NAME')}\",\n            f\"mlflow_s3_uri={utils.get_cmd_arg('mlflow_s3_uri') or utils.get_env_var('MLFLOW_S3_ENDPOINT_URL')}\",\n            f\"mlflow_tracking_uri={utils.get_cmd_arg('mlflow_tracking_uri') or utils.get_env_var('MLFLOW_TRACKING_URI')}\",\n            f\"default_token={datetime.datetime.now()}\"\n        ]\n    )\n    evaluate_model.after(train_model)\n\n    # Promote Model to Staging\n    promote_model_to_staging = dsl.ContainerOp(\n        name='promote_model_to_staging',\n        image='oawofolu/ml-image-processor:latest',\n        command=\"python\",\n        arguments=[\n            \"/app/main.py\",\n            \"mlflow_entry=promote_model_to_staging\",\n            f\"mlflow_stage={utils.get_env_var('MLFLOW_STAGE')}\",\n            f\"git_repo={utils.get_env_var('GIT_REPO')}\",\n            f\"experiment_name={utils.get_env_var('EXPERIMENT_NAME')}\",\n            f\"environment_name={utils.get_env_var('ENVIRONMENT_NAME')}\",\n            f\"mlflow_s3_uri={utils.get_cmd_arg('mlflow_s3_uri') or utils.get_env_var('MLFLOW_S3_ENDPOINT_URL')}\",\n            f\"mlflow_tracking_uri={utils.get_cmd_arg('mlflow_tracking_uri') or utils.get_env_var('MLFLOW_TRACKING_URI')}\",\n            f\"default_token={datetime.datetime.now()}\"\n        ]\n    )\n    promote_model_to_staging.after(evaluate_model)\n\n\nif __name__ == '__main__':\n    logging.info(f\"Compiling Kubeflow pipeline...host={utils.get_env_var('KUBEFLOW_PIPELINES_HOST')}\")\n    import kfp.compiler as compiler\n    compiler.Compiler().compile(cifar_pipeline, __file__ + '.zip')\n\n    logging.info(\"Generating new experiment run...\")\n    client = Client(host=f'{utils.get_env_var(\"KUBEFLOW_PIPELINES_HOST\")}')\n    cifar_experiment = client.create_experiment(name=utils.get_env_var('EXPERIMENT_NAME'))\n    cifar_run = client.run_pipeline(cifar_experiment.id, 'cifar-pipeline', __file__ + '.zip')\n    logging.info(\"Run completed.\")\n"
  },
  {
    "repo": "Our-Glass/kubeflow_pipelines_example",
    "file_path": "pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/Our-Glass/kubeflow_pipelines_example/main/pipeline.py",
    "content": "import kfp\r\nimport kfp.dsl as dsl\r\nfrom kubernetes import client as k8s\r\nfrom kubernetes import client as k8s_client\r\n\r\nEXPERIMENT_NAME = 'Sentiment Analysis Pipeline'\r\nIMAGE_USERNAME = <your docker hub username>\r\n\r\nclient = kfp.Client()\r\n\r\n@dsl.pipeline(\r\n    name='Sentiment Analysis Pipeline',\r\n    description='A pipeline to analyze sentiment of BTS videos'\r\n)\r\ndef sentiment_analysis_pipeline(\r\n\r\n    data_extraction_image=f'{IMAGE_USERNAME}/dataextraction:latest',\r\n    preprocessing_image=f'{IMAGE_USERNAME}/preprocessing:latest',\r\n    data_file='/mnt/bts.data',\r\n    train_file='/mnt/bts_train.data',\r\n    test_file='/mnt/bts_test.data',\r\n    validation_file='/mnt/bts_validation.data',\r\n    preprocess_file='/mnt/bts_preprocessed.data',\r\n    split_size=0.2,\r\n\r\n):\r\n\r\n    vop = dsl.VolumeOp(\r\n        name=\"create_pvc\",\r\n        resource_name=\"bts-pvc\",\r\n        modes=dsl.VOLUME_MODE_RWO,\r\n        size=\"1Gi\"\r\n    )\r\n\r\n    data_extraction_op = dsl.ContainerOp(\r\n            name='data_extraction',\r\n            image=data_extraction_image,\r\n            command=['python'],\r\n            arguments=['/app/dataextraction.py',\"--data_file\", data_file,],\r\n            pvolumes={\"/mnt\": vop.volume}\r\n        )\r\n\r\n    preprocessing_op = dsl.ContainerOp(\r\n            name='preprocessing',\r\n            image=preprocessing_image,\r\n            command=['python'],\r\n            arguments=[\r\n                '/app/preprocessing.py',\r\n                '--data_file', data_file,\r\n                '--preprocess_file', preprocess_file,\r\n                '--train_file', train_file,\r\n                '--test_file', test_file,\r\n                '--validation_file', validation_file,\r\n                '--split_size', split_size,\r\n                ],\r\n            pvolumes={\"/mnt\": data_extraction_op.pvolume}\r\n        )\r\n\r\n\r\nif __name__ == '__main__':\r\n    import kfp.compiler as compiler\r\n    pipeline_func = sentiment_analysis_pipeline\r\n    pipeline_filename = pipeline_func.__name__ + '.pipeline.zip'\r\n    compiler.Compiler().compile(pipeline_func, pipeline_filename)\r\n\r\n    try:\r\n        experiment = client.create_experiment(EXPERIMENT_NAME)\r\n    except:\r\n        experiment = client.create_experiment(EXPERIMENT_NAME)\r\n\r\n\r\n    arguments = {}\r\n    run_name = pipeline_func.__name__ + ' sentiment_run'\r\n    run_result = client.run_pipeline(experiment.id, run_name, pipeline_filename, arguments)\r\n\r\n    print(run_result)\r\n    print(run_name)\r\n    print(pipeline_filename)\r\n    print(arguments)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n            \r\n"
  },
  {
    "repo": "Our-Glass/kubeflow_pipelines_example",
    "file_path": "example_custom_module/pipeline_contenarized.py",
    "raw_url": "https://raw.githubusercontent.com/Our-Glass/kubeflow_pipelines_example/main/example_custom_module/pipeline_contenarized.py",
    "content": "import kfp\nfrom kfp.v2 import dsl\n# Load components\nfrom components.load_data.load_data import load_data\nfrom components.print_meta.print_meta import print_meta\nfrom components.split_data_custom.split_data_custom import split_data_custom as split_data\n\n# Declare pipeline\n@dsl.pipeline(name='pipeline artifcats')\ndef pipeline_custom_module_artifacts() -> str:\n        task_load_data = load_data()\n        task_split_data = split_data( x=task_load_data.outputs['X'], y=task_load_data.outputs['y'])\n        task_split_data.enable_caching = False\n        # task_print = print_meta(\n        #         x_train=task_split_data.outputs['x_train_out'],\n        #         x_test=task_split_data.outputs['x_test_out'])\n        return \"task_print.output\"\n\n\nNAME_EXPERIMENT = '02_pipeline_artifacts'\nOUTPUT_FILE = '02_pipeline_artifacts.zip'\n\n# Compile\nkfp.compiler.Compiler(mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE).compile(\n        pipeline_func=pipeline_custom_module_artifacts,\n        package_path=OUTPUT_FILE)\n        \n\n# Instance client\nclient = kfp.Client()\n\n# Create experiment\nmy_exp = client.create_experiment(name=NAME_EXPERIMENT)\n\n# Run pipeline\nmy_run = client.run_pipeline(\n        experiment_id = my_exp.id,\n        job_name = 'pipeline_custom_module_artifacts',\n        pipeline_package_path = OUTPUT_FILE\n)\n\"\"\" for running\npython contenerized_examples/02_pipeline_custom_component2/pipeline.py\n\"\"\""
  },
  {
    "repo": "Our-Glass/kubeflow_pipelines_example",
    "file_path": "example_custom_module/pipeline_lightweight.py",
    "raw_url": "https://raw.githubusercontent.com/Our-Glass/kubeflow_pipelines_example/main/example_custom_module/pipeline_lightweight.py",
    "content": "import kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import Input, Output, Dataset, Model\n\n\n@dsl.component(packages_to_install=['pandas', 'scikit-learn'])\ndef load_data(\n        X: Output[Dataset],\n        y: Output[Dataset]):\n        import pandas as pd\n        from sklearn.datasets import load_iris\n        \n        iris = load_iris()\n        X_data = pd.DataFrame(iris.data, columns=iris.feature_names)\n        y_data = pd.Series(iris.target)\n        \n        # transform series to dataframe and add column 'y'\n        y_data = pd.DataFrame(y_data, columns=['y'])\n        \n        X_data.to_csv(X.path)\n        y_data.to_csv(y.path)\n\n\n# split_data_light --> lightweigth component with custom python model\n@dsl.component(base_image='mevo12318/base_im:latest', packages_to_install=['pandas', 'scikit-learn'])\ndef split_data_light(\n        x: Input[Dataset],\n        y: Input[Dataset],\n        x_train_out: Output[Dataset],\n        x_test_out: Output[Dataset]):\n\n        # Import libraries\n        import pandas as pd\n        from src.utils import parse_df\n        from sklearn.model_selection import train_test_split\n        \n        # Load data from input\n        with open(x.path, \"rb\") as f:\n                X_data = pd.read_csv(f)\n        with open(y.path, \"rb\") as f:\n                y_data = pd.read_csv(f)\n\n        print(f\"type(x) = {type(X_data)}\")\n        print(f\"type(y) = {type(y_data)}\")\n                \n        # Split data\n        x_train, x_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=42)\n        \n        # Parse data\n        x_train, x_test = parse_df(x_train, x_test, y_train, y_test)\n        \n        # save data\n        with open(x_train_out.path, \"wb\") as f:\n                x_train.to_csv(f)\n        with open(x_test_out.path, \"wb\") as f:\n                x_test.to_csv(f)\n\n\n@dsl.component(packages_to_install=['pandas'])\ndef print_meta(\n        x_train: Input[Dataset],\n        x_test: Input[Dataset],\n        ) -> str:\n        import pandas as pd\n        \n\n        # Load data from input\n        with open(x_train.path, \"rb\") as f:\n                X_train = pd.read_csv(f)\n        with open(x_test.path, \"rb\") as f:\n                X_test = pd.read_csv(f)\n        \n        len_X_train = len(X_train)\n        path_X_train = x_train.path\n        uri_X_train = x_train.uri\n        \n        len_X_test = len(X_test)\n        path_X_test = x_test.path\n        uri_X_test = x_test.uri\n        \n        return f\"\"\"\n        len_X_train: {len_X_train}\n        path_X_train: {path_X_train}\n        uri_X_train: {uri_X_train}\n        \n        len_X_test: {len_X_test}\n        path_X_test: {path_X_test}\n        uri_X_test: {uri_X_test}\n        \"\"\"\n\n\n@dsl.component(packages_to_install=['pandas', 'scikit-learn', 'pickle5'])\ndef train_model(\n        x_train: Input[Dataset],\n        model_out: Output[Model]):\n        import pickle\n        import pandas as pd\n        from sklearn.linear_model import LinearRegression\n\n        # Load data from input\n        with open(x_train.path, \"rb\") as f:\n                X_train = pd.read_csv(f)\n        \n        # Train model\n        y_train = X_train['y']\n        X_train = X_train.drop(['y'], axis=1)\n        \n        # create model for classification iris dataset\n        model = LinearRegression()\n        model.fit(X_train, y_train)\n        \n        # save model\n        with open(model_out.path, \"wb\") as f:\n                pickle.dump(model, f)\n\n\n@dsl.component(packages_to_install=['pandas', 'pickle5'])\ndef test_model(\n        x_test: Input[Dataset],\n        model_in: Input[Model]) -> str:\n        import pickle\n        import pandas as pd\n\n        # Load data from input\n        with open(x_test.path, \"rb\") as f:\n                X_test = pd.read_csv(f)\n        \n        # Load model\n        with open(model_in.path, \"rb\") as f:\n                model = pickle.load(f)\n        \n        # Test model\n        y_test = X_test['y']\n        X_test = X_test.drop(['y'], axis=1)\n        \n        # Get metrics\n        score = model.score(X_test, y_test)\n        \n        return f\"score: {score}\"\n        \n\n# Declare pipeline\n@dsl.pipeline(name='pipeline artifcats')\ndef pipeline_custom_module_artifacts_light() -> str:\n        task_load_data = load_data()\n        task_split_data = split_data_light( x=task_load_data.outputs['X'], y=task_load_data.outputs['y'])\n        task_print = print_meta(\n                x_train=task_split_data.outputs['x_train_out'],\n                x_test=task_split_data.outputs['x_test_out'])        \n        task_train_model = train_model( x_train=task_split_data.outputs['x_train_out'])\n        task_test_model = test_model( x_test=task_split_data.outputs['x_test_out'], model_in=task_train_model.outputs['model_out'])\n\n        return task_print.output\n\n\nNAME_EXPERIMENT = '02_pipeline_artifacts_light'\nOUTPUT_FILE = '02_pipeline_artifacts_light.zip'\n\n# Compile\nkfp.compiler.Compiler(mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE).compile(\n        pipeline_func=pipeline_custom_module_artifacts_light,\n        package_path=OUTPUT_FILE)\n        \n\n# Instance client\nclient = kfp.Client()\n\n# Create experiment\nmy_exp = client.create_experiment(name=NAME_EXPERIMENT)\n\n# Run pipeline\nmy_run = client.run_pipeline(\n        experiment_id = my_exp.id,\n        job_name = 'pipeline_custom_module_artifacts_light',\n        pipeline_package_path = OUTPUT_FILE\n)\n\"\"\" for running\npython contenerized_examples/02_pipeline_custom_component2/pipeline.py\n\"\"\""
  },
  {
    "repo": "Our-Glass/kubeflow_pipelines_example",
    "file_path": "kfpv2/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/Our-Glass/kubeflow_pipelines_example/main/kfpv2/pipeline.py",
    "content": "\r\nfrom kfp.v2 import dsl\r\nfrom kubernetes import client as k8s\r\nfrom kubernetes import client as k8s_client\r\nfrom components.preprocessing.preprocessing import training_data_processing\r\nfrom components.dataextraction.dataextraction import get_data\r\nfrom components.train_linear.train_linear import train_linear_model\r\nfrom components.train_logistic.train_logistic import train_logistic_model\r\n\r\n\r\n# create a breast cancer pipeline\r\n@dsl.pipeline(\r\n    name='Breast Cancer Pipeline',\r\n    description='A pipeline to predict breast cancer'\r\n)\r\ndef breast_cancer_pipeline(\r\n    split_size: float = 0.2\r\n):\r\n    # get data\r\n    get_data_task = get_data()\r\n    # split data\r\n    split_data_task = training_data_processing(dataset_input=get_data_task.outputs['dataset'],split_size=split_size)\r\n    train_linear_model_task = train_linear_model(dataset=split_data_task.outputs['dataset'])\r\n    train_logistic_model_task = train_logistic_model(dataset=split_data_task.outputs['dataset'])\r\n    # TODO: add a component to evaluate the model\r\n\r\nif __name__ == '__main__':\r\n    import kfp\r\n    EXPERIMENT_NAME = 'brest_cancer'\r\n    pipelineGzFile = 'breast_cancer.zip'\r\n\r\n    kfp.compiler.Compiler(mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE).compile(\r\n        pipeline_func=breast_cancer_pipeline,\r\n        package_path=pipelineGzFile)\r\n\r\n\r\n\r\n# run the pipeline\r\n\r\n    client = kfp.Client()\r\n    my_exp = client.create_experiment(\r\n        name=EXPERIMENT_NAME\r\n    )\r\n    my_run = client.run_pipeline(\r\n        my_exp.id, \r\n        'breast_cancer_pipeline', \r\n        pipelineGzFile,\r\n        #enable_caching=False\r\n    )"
  },
  {
    "repo": "mouachan/kubeflow-pipeline-project",
    "file_path": "pipeline/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/mouachan/kubeflow-pipeline-project/main/pipeline/pipeline.py",
    "content": "from kfp import dsl\n\n@dsl.pipeline(\n    name='My Kubeflow Pipeline',\n    description='A pipeline that orchestrates an R script and a Python script.'\n)\ndef my_pipeline():\n    # Define the R script component\n    r_script_op = dsl.ContainerOp(\n        name='run-r-script',\n        image='your-r-image:latest',  # Replace with your R Docker image\n        command=['Rscript', '/scripts/script.R'],\n        file_outputs={\n            'output': '/output/output.txt'  # Adjust as necessary\n        }\n    )\n    \n    # Define the Python script component\n    python_script_op = dsl.ContainerOp(\n        name='run-python-script',\n        image='your-python-image:latest',  # Replace with your Python Docker image\n        command=['python', '/scripts/script.py'],\n        arguments=['--input', r_script_op.outputs['output']],\n        file_outputs={\n            'output': '/output/output.txt'  # Adjust as necessary\n        }\n    )\n\n    # Set dependencies\n    python_script_op.after(r_script_op)\n\nif __name__ == '__main__':\n    import kfp.compiler as compiler\n    compiler.Compiler().compile(my_pipeline, 'pipeline.yaml')"
  },
  {
    "repo": "chuyangzh/kubeflow-spark-pipeline",
    "file_path": "kubeflow-pipeline/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/chuyangzh/kubeflow-spark-pipeline/main/kubeflow-pipeline/pipeline.py",
    "content": "import kfp\nfrom kfp import dsl\nfrom kfp.components import load_component_from_text\n\n# Define the Spark component\nspark_component = load_component_from_text(\"\"\"\nname: Spark Weather Job\ndescription: Runs the Spark job to process weather data.\nimplementation:\n    container:\n        image: spark-weather-job:latest\n        command:\n        - spark-submit\n        - /app/spark-job.py\n\"\"\")\n\n@dsl.pipeline(\n    name=\"Spark Weather Pipeline\",\n    description=\"A pipeline that runs a Spark job to process weather data.\"\n)\ndef spark_weather_pipeline():\n    # Add the Spark component to the pipeline\n    spark_task = spark_component()\n\n# Compile the pipeline for v1 compatibility\nif __name__ == \"__main__\":\n    kfp.compiler.Compiler().compile(\n        pipeline_func=spark_weather_pipeline,\n        package_path=\"kubeflow-pipeline/spark_weather_pipeline.yaml\"\n    )\n"
  },
  {
    "repo": "j-cunanan/kubeflow-pipelines-TFOD",
    "file_path": "license.py",
    "raw_url": "https://raw.githubusercontent.com/j-cunanan/kubeflow-pipelines-TFOD/master/license.py",
    "content": "import json\n\nimport kfp\nfrom kfp.components import func_to_container_op, InputPath, OutputPath\nfrom kfp.components import load_component_from_file, load_component_from_url\n\n\n@func_to_container_op\ndef dl_pipeline_config(\n    pipeline_config: OutputPath(),\n):\n    import subprocess\n    import sys\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"requests\"])\n    import requests\n        \n    with open(pipeline_config, 'wb') as file:\n        r = requests.get(\"https://www.dropbox.com/s/ftl82cdyf5twgev/licence_plate.config?dl=1\", allow_redirects=True)\n        file.write(r.content)\n\n@func_to_container_op\ndef list_dir_files_python_op(input_dir_path: InputPath()):\n    import os\n    dir_items = os.listdir(input_dir_path)\n    for dir_item in dir_items:\n        print(dir_item)\n\n@func_to_container_op\ndef read_files_python_op(input_dir_path: InputPath()):\n    with open(input_dir_path, 'r') as f:\n        print(f.read())\n\ntrain_eval_op = load_component_from_file(\"train_eval/component.yaml\")\ntfrecordgen_op = load_component_from_file(\"TFRecordsGen/component.yaml\")\nloadweights_op = load_component_from_file(\"LoadWeights/component.yaml\")\ntfserving_op = load_component_from_file(\"TFServing/component.yaml\")\n\n@kfp.dsl.pipeline(name='First Pipeline', description='describe this')\ndef my_pipeline(\n    model_name: str = 'model',\n    num_train_steps: int =100,\n                data_url='https://www.dropbox.com/s/gx9zmtlkjlfg1m5/license.zip?dl=1',\n                converter_script_url='https://www.dropbox.com/s/j18c859mqkzs52o/create_licence_plate_tf_record.py?dl=1',\n                pbtxt_url='https://www.dropbox.com/s/jy7bzzgeax9b95t/licence_plate_label_map.pbtxt?dl=1',\n                weights_url='http://download.tensorflow.org/models/object_detection/tf2/20200711/efficientdet_d0_coco17_tpu-32.tar.gz',\n                num_shards: int = 1):\n\n    dl_task = dl_pipeline_config()\n    loadweights_task = loadweights_op(weights_url=weights_url)\n    conversion_task = tfrecordgen_op(data_url=data_url,\n                                     converter_script_url=converter_script_url,\n                                     pbtxt_url=pbtxt_url,\n                                     num_shards=num_shards)\n\n    list_dir_files_python_op(conversion_task.outputs['output_dir'])\n\n    modelling_task = train_eval_op(pipeline_config=dl_task.output,\n                                   record_summaries=False,\n                                   label_map=conversion_task.outputs['label_map'],\n                                   data=conversion_task.outputs['output_dir'],\n                                   pretrained_weights=loadweights_task.output,\n                                   num_train_steps=num_train_steps,\n                                   model=model_name\n                                  )\n\n    modelling_task.container.set_gpu_limit(1)\n\n    list_dir_files_python_op(modelling_task.outputs['model_dir'])\n    list_dir_files_python_op(modelling_task.outputs['export_dir'])\n    \n    tfserving_task = tfserving_op(model_name=model_name,\n                                 export_dir=modelling_task.outputs['export_dir'])\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(my_pipeline, 'my_pipeline.tar.gz')\n    "
  },
  {
    "repo": "Minseojeonn/Kubeflow_capstone_pipeline",
    "file_path": "Pipeline/pipe/tmp.py",
    "raw_url": "https://raw.githubusercontent.com/Minseojeonn/Kubeflow_capstone_pipeline/master/Pipeline/pipe/tmp.py",
    "content": "import kfp\nimport kfp.components as comp\nfrom kfp import dsl\nfrom kfp import onprem\n@dsl.pipeline(\n    name='ms_min',\n    description='minseo'\n)\n\ndef create_inference_model():\n    kserve_op = comp.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/'\n                                               'master/components/kserve/component.yaml')\n    \n    isvc_yaml = '''\n                apiVersion: \"serving.kserve.io/v1beta1\"\n                kind: \"InferenceService\"\n                metadata:\n                    name: \"torchserve-temp\"\n                    namespace: \"kubeflow-user-example-com\"\n                spec:\n                    predictor:\n                        serviceAccountName: 'sa'\n                        pytorch:\n                            storageUri: s3://efficientcluster\n                            resources:\n                                limits:\n                                    cpu: \"5\"\n                '''\n    \n    return kserve_op(action=\"apply\",\n              inferenceservice_yaml=isvc_yaml\n              )    \n\ndef ms_min_pipeline():\n\n    start = dsl.ContainerOp(\n        name=\"Start\",\n        command=[\"/bin/sh\"],\n        image=\"python:3.9\"\n       \n    )\n    \n    end = dsl.ContainerOp(\n        name=\"END\",\n        command=[\"/bin/sh\"],\n        image=\"python:3.9\"\n    )\n\n\n    add_p = dsl.ContainerOp(\n        name=\"load_data\",\n        image=\"raichal2000/capstonepipe:4\",\n        command=[\"python\",\"1_videos_to_img_with_extract_face_and_crop.py\"],\n        arguments=[\n            '--video','/data/src', '--output', '/data/faces'\n        ]\n    ).apply(onprem.mount_pvc(\"data\", volume_name=\"data\", volume_mount_path=\"/data\"))\n\n    ml = dsl.ContainerOp(\n        name=\"training pipeline\",\n        image=\"raichal2000/capstonepipe:8\",\n        command=[\"python\",\"2_train_efficient_vit.py\"],\n        arguments=[\n            '--path', '/data/faces', '--savepath', '/train/pth_saves'\n        ]\n    ).set_gpu_limit(1).apply(onprem.mount_pvc(\"data\", volume_name=\"data\", volume_mount_path=\"/data\")).apply(onprem.mount_pvc(\"train\", volume_name=\"train\", volume_mount_path=\"/train\"))\n\n    mar = dsl.ContainerOp(\n        name=\"Creating Marfile\",\n        command=[\"/bin/sh\"],\n        image=\"python:3.9\",\n        arguments=[\n            \"-c\",\n            \"cd /mar/efficient_vit_mar; pip install torchserve torch-model-archiver torch-workflow-archiver; torch-model-archiver --model-name efficient --version 1.0 --serialized-file /train/efficient_vit.pth --extra-files ./src/efficient_vit.py,./src/architecture.yaml,./src/temp2.png,./src/utils.py,./src/tester.py,./src/albu.py  --handler ./src/handler.py --requirements-file ./src/requirements.txt\"\n        ],  # pip install => create mar file => make model_store folder => mv marfile to model_store\n    ).apply(onprem.mount_pvc(\"mar\", volume_name=\"mar\", volume_mount_path=\"/mar\")).apply(onprem.mount_pvc(\"train\", volume_name=\"train\", volume_mount_path=\"/train\"))\n    \n    add_p.after(start)\n    ml.after(add_p)\n    mar.after(ml)\n    inference_model = create_inference_model()\n    inference_model.apply(onprem.mount_pvc(pvc_name=\"mar\", volume_name=\"mar\", volume_mount_path=\"/mar\"))\n    inference_model.after(mar)\n    end.after(inference_model)\n    \n    \nif __name__ == \"__main__\":\n    import kfp.compiler as compiler\n    compiler.Compiler().compile(ms_min_pipeline, __file__ + \".tar.gz\")"
  },
  {
    "repo": "Louis5499/Kubeflow-mnist-pipeline",
    "file_path": "tfJob_kfServing_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/Louis5499/Kubeflow-mnist-pipeline/master/tfJob_kfServing_pipeline.py",
    "content": "import json\nfrom kfp import components\nimport kfp.dsl as dsl\n\n@dsl.pipeline(\n    name=\"Launch kubeflow tfjob & kfserving template\",\n    description=\"An example to launch tfjob.\"\n)\ndef mnist_pipeline(\n        name=\"mnist\",\n        namespace=\"kubeflow\",\n        workerNum=2,\n        deleteAfterDone=False):\n    tfjob_launcher_op = components.load_component_from_file(\"./tfJobComponent.yaml\")\n    kfserving_op = components.load_component_from_file(\"./kfServingComponent.yaml\")\n    duplicated_gs_deletion_op = components.load_component_from_file(\"./duplicatedGsDeleteComponent.yaml\")\n    bucket = \"kf_second_test\"\n    \n    chief = {\n      \"replicas\": 1,\n      \"template\": {\n        \"metadata\": {\n          \"annotations\": {\n            \"sidecar.istio.io/inject\": \"false\"\n          }\n        },\n        \"spec\": {\n          \"containers\": [\n            {\n              \"command\": [\n                \"/usr/bin/python\",\n                \"/opt/model.py\",\n                \"--tf-model-dir=$(modelDir)\",\n                \"--tf-export-dir=$(exportDir)\",\n                \"--tf-train-steps=$(trainSteps)\",\n                \"--tf-batch-size=$(batchSize)\",\n                \"--tf-learning-rate=$(learningRate)\"\n              ],\n              \"env\": [\n                {\n                  \"name\": \"modelDir\",\n                  \"value\": f\"gs://{bucket}/my-model\"\n                },\n                {\n                  \"name\": \"exportDir\",\n                  \"value\": f\"gs://{bucket}/my-model/export\"\n                },\n                {\n                  \"name\": \"trainSteps\",\n                  \"value\": \"200\"\n                },\n                {\n                  \"name\": \"batchSize\",\n                  \"value\": \"100\"\n                },\n                {\n                  \"name\": \"learningRate\",\n                  \"value\": \"0.01\"\n                }\n              ],\n              \"image\": \"gcr.io/kubeflow-examples/mnist/model:build-1202842504546750464\",\n              \"name\": \"tensorflow\",\n              \"workingDir\": \"/opt\"\n            }\n          ],\n          \"restartPolicy\": \"OnFailure\",\n          \"serviceAccount\": \"k8s-sa\"\n        }\n      }\n    }\n    worker = {}\n    if workerNum > 0:\n      worker = {\n        \"replicas\": workerNum,\n        \"template\": {\n          \"metadata\": {\n            \"annotations\": {\n              \"sidecar.istio.io/inject\": \"false\"\n            }\n          },\n          \"spec\": {\n            \"containers\": [\n              {\n                \"command\": [\n                  \"/usr/bin/python\",\n                  \"/opt/model.py\",\n                  \"--tf-model-dir=$(modelDir)\",\n                  \"--tf-export-dir=$(exportDir)\",\n                  \"--tf-train-steps=$(trainSteps)\",\n                  \"--tf-batch-size=$(batchSize)\",\n                  \"--tf-learning-rate=$(learningRate)\"\n                ],\n                \"env\": [\n                  {\n                    \"name\": \"modelDir\",\n                    \"value\": f\"gs://{bucket}/my-model\"\n                  },\n                  {\n                    \"name\": \"exportDir\",\n                    \"value\": f\"gs://{bucket}/my-model/export\"\n                  },\n                  {\n                    \"name\": \"trainSteps\",\n                    \"value\": \"200\"\n                  },\n                  {\n                    \"name\": \"batchSize\",\n                    \"value\": \"100\"\n                  },\n                  {\n                    \"name\": \"learningRate\",\n                    \"value\": \"0.01\"\n                  }\n                ],\n                \"image\": \"gcr.io/kubeflow-examples/mnist/model:build-1202842504546750464\",\n                \"name\": \"tensorflow\",\n                \"workingDir\": \"/opt\"\n              }\n            ],\n            \"restartPolicy\": \"OnFailure\",\n            \"serviceAccount\": \"k8s-sa\"\n          }\n        }\n      }\n\n    ps = {\n      \"replicas\": 1,\n      \"template\": {\n        \"metadata\": {\n          \"annotations\": {\n            \"sidecar.istio.io/inject\": \"false\"\n          }\n        },\n        \"spec\": {\n          \"containers\": [\n            {\n              \"command\": [\n                \"/usr/bin/python\",\n                \"/opt/model.py\",\n                \"--tf-model-dir=$(modelDir)\",\n                \"--tf-export-dir=$(exportDir)\",\n                \"--tf-train-steps=$(trainSteps)\",\n                \"--tf-batch-size=$(batchSize)\",\n                \"--tf-learning-rate=$(learningRate)\"\n              ],\n              \"env\": [\n                {\n                  \"name\": \"modelDir\",\n                  \"value\": f\"gs://{bucket}/my-model\"\n                },\n                {\n                  \"name\": \"exportDir\",\n                  \"value\": f\"gs://{bucket}/my-model/export\"\n                },\n                {\n                  \"name\": \"trainSteps\",\n                  \"value\": \"200\"\n                },\n                {\n                  \"name\": \"batchSize\",\n                  \"value\": \"100\"\n                },\n                {\n                  \"name\": \"learningRate\",\n                  \"value\": \"0.01\"\n                }\n              ],\n              \"image\": \"gcr.io/kubeflow-examples/mnist/model:build-1202842504546750464\",\n              \"name\": \"tensorflow\",\n              \"workingDir\": \"/opt\"\n            }\n          ],\n          \"restartPolicy\": \"OnFailure\",\n          \"serviceAccount\": \"k8s-sa\"\n        }\n      }\n    }\n    tfJobLauncher = tfjob_launcher_op(\n      name=name,\n      namespace=namespace,\n      worker_spec=worker,\n      chief_spec=chief,\n      ps_spec=ps,\n      delete_finished_tfjob=deleteAfterDone\n    )\n\n    duplicatedGsDirDeletion = duplicated_gs_deletion_op(\n      bucket_name=bucket\n    ).after(tfJobLauncher)\n\n    kfserving_op(\n      name=\"kfserving\",\n      default_model_uri=f\"gs://{bucket}/my-model/export\",\n      model_name=\"main\",\n      transformer_custom_image=\"jackfantasy/image-transformer:v1\"\n    ).after(duplicatedGsDirDeletion)\n\nif __name__ == \"__main__\":\n    import kfp.compiler as compiler\n    compiler.Compiler().compile(mnist_pipeline, __file__ + \".tar.gz\")\n"
  },
  {
    "repo": "iqer/kubeflow_demo_pipeline",
    "file_path": "demo_pipeline1/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/iqer/kubeflow_demo_pipeline/main/demo_pipeline1/pipeline.py",
    "content": "import kfp\nfrom kfp import dsl\n\n\ndef preprocess_op():\n    return dsl.ContainerOp(\n        name='Preprocess Data',\n        image='shanau2/tf_pipeline_preprocess:v0.2',\n        arguments=[\n        ],\n        file_outputs={\n            'x_train': '/tmp/data/x_train.pickle',\n            'x_test': '/tmp/data/x_test.pickle',\n            'y_train': '/tmp/data/y_train.pickle',\n            'y_test': '/tmp/data/y_test.pickle',\n            'model_dir': 'tmp/data'\n        }\n    )\n\n\ndef train_op(x_train, y_train, model_dir):\n    return dsl.ContainerOp(\n        name='Train Model',\n        image='shanau2/tf_pipeline_train:v0.1',\n        arguments=[\n            '--x-train', x_train,\n            '--y_train', y_train,\n            '--model_dir', model_dir\n        ],\n        file_outputs={\n            'model_dir': model_dir,\n        }\n    )\n\n\n# def test_op(x_test, y_test, model_dir, output_dir):\n#     return dsl.ContainerOp(\n#         name='Test Model',\n#         image='shanau2/boston_pipeline_test:v3',\n#         arguments=[\n#             '--x_test', x_test,\n#             '--y_test', y_test,\n#             '--model_dir', model_dir,\n#             '--output-dir', output_dir,\n#         ],\n#         file_outputs={\n#             'output_dir': output_dir,\n#         }\n#     )\n\n\n# def deploy(output_dir):\n#     return dsl.ContainerOp(\n#         name='Deploy Model',\n#         image='shanau2/tf_pipeline_deploy_model:v0.1',\n#         arguments=[\n#             '--output_dir', output_dir\n#         ]\n#     )\n\n\n@dsl.pipeline(\n    name='Fashion MNIST Training Pipeline',\n    description='Fashion MNIST Training Pipeline to be executed on KubeFlow.'\n)\ndef training_pipeline():\n    _preprocess_op = preprocess_op()\n\n    _train_op = train_op(\n        dsl.InputArgumentPath(_preprocess_op.outputs['x_train']),\n        dsl.InputArgumentPath(_preprocess_op.outputs['y_train']),\n        dsl.InputArgumentPath(_preprocess_op.outputs['model_dir']),\n    ).after(_preprocess_op)\n\n    # _test_op = test_op(\n    #     dsl.InputArgumentPath(_preprocess_op.outputs['x_test']),\n    #     dsl.InputArgumentPath(_preprocess_op.outputs['y_test']),\n    #     dsl.InputArgumentPath(_train_op.outputs['model_dir'])\n    # ).after(_train_op)\n\n    # deploy_model_op(\n    #     dsl.InputArgumentPath(_train_op.outputs['model_dir'])\n    # ).after(_test_op)\n\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(training_pipeline, 'demo.tar.gz')\n\n"
  },
  {
    "repo": "iqer/kubeflow_demo_pipeline",
    "file_path": "demo_pipeline3/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/iqer/kubeflow_demo_pipeline/main/demo_pipeline3/pipeline.py",
    "content": "import kfp\nfrom kfp import dsl\n\n\ndef demo_op():\n    return dsl.ContainerOp(name='demo', image='demo:v0.0.1')\n\n\n@dsl.pipeline(\n    name='demo',\n    description='test'\n)\ndef pipeline():\n    op = demo_op()\n\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(pipeline, __file__ + '.zip')\n"
  },
  {
    "repo": "sagravat/kubeflow-pipelines-examples",
    "file_path": "chest-xray/pipelines/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/sagravat/kubeflow-pipelines-examples/master/chest-xray/pipelines/pipeline.py",
    "content": "#!/usr/bin/env python3\n# Copyright 2018 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport kfp.dsl as dsl\n\nclass ObjectDict(dict):\n  def __getattr__(self, name):\n    if name in self:\n      return self[name]\n    else:\n      raise AttributeError(\"No such attribute: \" + name)\n\n\n@dsl.pipeline(\n  name='my-pipeline',\n  description='preprocess pipeline'\n)\ndef run(\n    project,\n    bucket,\n    input_dir,\n    output_dir,\n    labels_file,\n    mode='cloud'\n):\n  \"\"\"Pipeline to train babyweight model\"\"\"\n  start_step = 1\n\n  # Step 1: create training dataset using Apache Beam on Cloud Dataflow\n  if start_step <= 1:\n    preprocess = dsl.ContainerOp(\n      name='preprocess',\n      # image needs to be a compile-time string\n      image='gcr.io/{}/dicom-preprocess:latest'.format(project),\n      arguments=[\n        '--input_dir', input_dir,\n        '--output_dir', output_dir,\n        '--labels_file', labels_file,\n        '--project', project,\n        '--mode', mode,\n      ],\n      file_outputs={'bucket': '/output.txt'}\n    )\n  else:\n    preprocess = ObjectDict({\n      'outputs': {\n        'bucket': bucket\n      }\n    })\n\n\n\nif __name__ == '__main__':\n  import kfp.compiler as compiler\n  import sys\n  if len(sys.argv) != 2:\n    print(\"Usage: pipeline pipeline-output-name\")\n    sys.exit(-1)\n  \n  filename = sys.argv[1]\n  compiler.Compiler().compile(run, filename)\n"
  },
  {
    "repo": "sagravat/kubeflow-pipelines-examples",
    "file_path": "chest-xray/pipelines/train_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/sagravat/kubeflow-pipelines-examples/master/chest-xray/pipelines/train_pipeline.py",
    "content": "#!/usr/bin/env python3\n# Copyright 2018 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport kfp.dsl as dsl\n\nclass ObjectDict(dict):\n  def __getattr__(self, name):\n    if name in self:\n      return self[name]\n    else:\n      raise AttributeError(\"No such attribute: \" + name)\n\n\n@dsl.pipeline(\n  name='train-pipeline',\n  description='train pipeline'\n)\ndef run(\n    project,\n    bucket,\n    train_file,\n    eval_file\n):\n  \"\"\"Pipeline to train babyweight model\"\"\"\n  start_step = 1\n\n  # Step 1: create training dataset using Apache Beam on Cloud Dataflow\n  if start_step <= 1:\n    train = dsl.ContainerOp(\n      name='train',\n      # image needs to be a compile-time string\n      image='gcr.io/{}/chest-xray-xfer-learning-train:latest'.format(project),\n      arguments=[\n        '--train_file', train_file,\n        '--eval_file', eval_file,\n      ],\n      file_outputs={'bucket': '/output.txt'}\n    )\n    train.set_gpu_limit(4)\n  else:\n    train = ObjectDict({\n      'outputs': {\n        'bucket': bucket\n      }\n    })\n\n\n\nif __name__ == '__main__':\n  import kfp.compiler as compiler\n  compiler.Compiler().compile(run, __file__ + '.tar.gz')\n"
  },
  {
    "repo": "luizrennocosta/kubeflow-ml-pipeline",
    "file_path": "src/deep_model_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/luizrennocosta/kubeflow-ml-pipeline/main/src/deep_model_pipeline.py",
    "content": "import kfp.dsl as dsl\n\n\n@dsl.pipeline(\n  name='NLP',\n  description='A pipeline demonstrating reproducible steps for NLP'\n)\ndef nlp_pipeline(\n        csv_url=\"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_multilingual_UK_v1_00.tsv.gz\",\n        embed_weights_url=\"http://nlp.stanford.edu/data/glove.42B.300d.zip\",\n        features_column=\"review_body\",\n        labels_column=\"product_category\",\n        raw_text_path='/mnt/text.data',\n        labels_path='/mnt/labels.data',\n        data_folder='/mnt/data',\n        clean_text_path='/mnt/clean.data',\n        tokens_path='/mnt/tokens.data',\n        tfidf_vectors_path='/mnt/tfidf.data',\n        model_prediction_path='/mnt/predicted_train.data',\n        tfidf_model_path='/mnt/tfidf.model',\n        word_index_path='/mnt/word_index.data',\n        embedded_matrix_path='/mnt/embedded_matrix.data',\n        pre_embedded_weights='/mnt/data/glove.42B.300d.txt',\n        train_ratio=0.98,\n        validation_ratio=0.01,\n        test_ratio=0.01,\n        num_words=20000,\n        sentence_max_length=50,\n        deep_model='/mnt/deep_model.model',\n        batch_size='100'):\n    \"\"\"\n    Pipeline\n    \"\"\"\n    vop = dsl.VolumeOp(\n      name='my-pvc',\n      resource_name=\"my-pvc\",\n      modes=[\"ReadWriteMany\"],\n      size=\"30Gi\"\n    )\n\n    download_step = dsl.ContainerOp(\n        name='data_downloader',\n        image='docker.io/cyferino/component-kubeflow:0.0.14',\n        command=\"python\",\n        arguments=[\n            \"/src/pipeline_steps/data_downloader/pipeline_step.py\",\n            \"--labels-path\", labels_path,\n            \"--data-folder\", data_folder,\n            \"--features-path\", raw_text_path,\n            \"--csv-url\", csv_url,\n            \"--features-column\", features_column,\n            \"--labels-column\", labels_column\n        ],\n        pvolumes={\"/mnt\": vop.volume}\n    )\n\n    download_embed_step = dsl.ContainerOp(\n        name='embed_weights_downloader',\n        image='docker.io/cyferino/component-kubeflow:0.0.14',\n        command=\"python\",\n        arguments=[\n            \"/src/pipeline_steps/weights_downloader/pipeline_step.py\",\n            \"--url\", embed_weights_url\n        ],\n        pvolumes={\"/mnt\": vop.volume}\n    )\n\n    clean_step = dsl.ContainerOp(\n        name='clean_text',\n        image='docker.io/cyferino/component-kubeflow:0.0.14',\n        command=\"python\",\n        arguments=[\n            \"/src/pipeline_steps/clean_text/pipeline_step.py\",\n            \"--in-path\", raw_text_path,\n            \"--out-path\", clean_text_path,\n        ],\n        pvolumes={\"/mnt\": vop.volume}\n    ).after(download_step)\n\n    data_split_step = dsl.ContainerOp(\n        name='data_splitter',\n        image='docker.io/cyferino/component-kubeflow:0.0.14',\n        command=\"python\",\n        arguments=[\n            \"/src/pipeline_steps/train_val_test/pipeline_step.py\",\n            \"--in-path\", clean_text_path,\n            \"--labels-path\", labels_path,\n            \"--out-folder\", data_folder,\n            \"--train-ratio\", train_ratio,\n            \"--validation-ratio\", validation_ratio,\n            \"--test-ratio\", test_ratio\n        ],\n        pvolumes={\"/mnt\": vop.volume}\n    ).after(clean_step)\n\n    tokenize_step = dsl.ContainerOp(\n        name='tokenize',\n        image='docker.io/cyferino/component-kubeflow:0.0.14',\n        command=\"python\",\n        arguments=[\n            \"/src/pipeline_steps/tokenize/pipeline_step.py\",\n            \"--in-path\", \"/mnt/data/train.data\",\n            \"--out-path\", tokens_path,\n            \"--word-index-path\", word_index_path,\n            \"--tokenizer-path\", \"/mnt/tokenizer.model\",\n            \"--action\", \"train\",\n            \"--num-words\", num_words,\n            \"--max-length\", sentence_max_length,\n        ],\n        pvolumes={\"/mnt\": vop.volume}\n    ).after(data_split_step)\n\n    tokenize_step_val = dsl.ContainerOp(\n        name='val_tokenize',\n        image='docker.io/cyferino/component-kubeflow:0.0.14',\n        command=\"python\",\n        arguments=[\n            \"/src/pipeline_steps/tokenize/pipeline_step.py\",\n            \"--in-path\", \"/mnt/data/val.data\",\n            \"--out-path\", \"/mnt/data/tokenized_val.data\",\n            \"--word-index-path\", word_index_path,\n            \"--tokenizer-path\", \"/mnt/tokenizer.model\",\n            \"--action\", \"predict\",\n            \"--num-words\", num_words,\n            \"--max-length\", sentence_max_length,\n        ],\n        pvolumes={\"/mnt\": vop.volume}\n    ).after(tokenize_step)\n\n    tokenize_step_test = dsl.ContainerOp(\n        name='test_tokenize',\n        image='docker.io/cyferino/component-kubeflow:0.0.14',\n        command=\"python\",\n        arguments=[\n            \"/src/pipeline_steps/tokenize/pipeline_step.py\",\n            \"--in-path\", \"/mnt/data/test.data\",\n            \"--out-path\", \"/mnt/data/tokenized_test.data\",\n            \"--word-index-path\", word_index_path,\n            \"--tokenizer-path\", \"/mnt/tokenizer.model\",\n            \"--action\", \"predict\",\n            \"--num-words\", num_words,\n            \"--max-length\", sentence_max_length,\n        ],\n        pvolumes={\"/mnt\": vop.volume}\n    ).after(tokenize_step)\n\n    embedding_step = dsl.ContainerOp(\n        name='embedder',\n        image='docker.io/cyferino/component-kubeflow:0.0.14',\n        command=\"python\",\n        arguments=[\n            \"/src/pipeline_steps/embedder/pipeline_step.py\",\n            \"--in-path\", word_index_path,\n            \"--out-path\", embedded_matrix_path,\n            \"--glove-file\", pre_embedded_weights,\n        ],\n        pvolumes={\"/mnt\": vop.volume}\n    ).after(tokenize_step, download_embed_step)\n\n    train_step = dsl.ContainerOp(\n        name='predictor',\n        image='docker.io/cyferino/component-kubeflow:0.0.14',\n        command=\"python\",\n        arguments=[\n            \"/src/pipeline_steps/deep_classifier/pipeline_step.py\",\n            \"--in-path\", tokens_path,\n            \"--embed-weight\", embedded_matrix_path,\n            \"--out-path\", model_prediction_path,\n            \"--action\", \"train\",\n            \"--model-path\", deep_model,\n            \"--epochs\", 20,\n            \"--batch-size\", 1024,\n            \"--optimizer\", \"rmsprop\",\n            \"--metrics\", [\"acc\"]\n        ],\n        pvolumes={\"/mnt\": vop.volume}\n    ).after(embedding_step)\n\n    predict_val = dsl.ContainerOp(\n        name='val_predictor',\n        image='docker.io/cyferino/component-kubeflow:0.0.14',\n        command=\"python\",\n        arguments=[\n            \"/src/pipeline_steps/deep_classifier/pipeline_step.py\",\n            \"--in-path\", \"/mnt/data/tokenized_val.data\",\n            \"--embed-weight\", embedded_matrix_path,\n            \"--out-path\", '/mnt/predicted_val.data',\n            \"--action\", \"predict\",\n            \"--model-path\", deep_model,\n        ],\n        pvolumes={\"/mnt\": vop.volume}\n    ).after(train_step, tokenize_step_val)\n\n    predict_test = dsl.ContainerOp(\n        name='test_predictor',\n        image='docker.io/cyferino/component-kubeflow:0.0.14',\n        command=\"python\",\n        arguments=[\n            \"/src/pipeline_steps/deep_classifier/pipeline_step.py\",\n            \"--in-path\", \"/mnt/data/tokenized_test.data\",\n            \"--embed-weight\", embedded_matrix_path,\n            \"--out-path\", '/mnt/predicted_test.data',\n            \"--action\", \"predict\",\n            \"--model-path\", deep_model,\n        ],\n        pvolumes={\"/mnt\": vop.volume}\n    ).after(train_step, tokenize_step_test)\n\n    evaluate_model = dsl.ContainerOp(\n        name='model_evaluator',\n        image='docker.io/cyferino/component-kubeflow:0.0.14',\n        command=\"python\",\n        arguments=[\n            \"/src/pipeline_steps/deep_classifier/pipeline_step.py\",\n            \"--data-folder\", \"/mnt/data\",\n            \"--predicted-train-data\", '/mnt/predicted_train.data',\n            \"--predicted-val-data\", '/mnt/predicted_val.data',\n            \"--predicted-test-data\", '/mnt/predicted_test.data'\n        ],\n        pvolumes={\"/mnt\": vop.volume},\n        file_outputs={\n            'mlpipeline-metrics': '/mlpipeline-metrics.json',\n            'mlpipeline-ui-data': '/mlpipeline-ui-data.json'\n        },\n        output_artifact_paths={\n            'mlpipeline-metrics': '/mlpipeline-metrics.json',\n            'mlpipeline-ui-data': '/mlpipeline-ui-data.json'\n        }\n    ).after(train_step, predict_val, predict_test)\n\n\nif __name__ == '__main__':\n    import kfp.compiler as compiler\n    compiler.Compiler().compile(nlp_pipeline, __file__ + '.tar.gz')"
  },
  {
    "repo": "luizrennocosta/kubeflow-ml-pipeline",
    "file_path": "src/lr_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/luizrennocosta/kubeflow-ml-pipeline/main/src/lr_pipeline.py",
    "content": "import kfp.dsl as dsl\nimport yaml\n\n\n@dsl.pipeline(\n  name='NLP',\n  description='A pipeline demonstrating reproducible steps for NLP using logistic regression'\n)\ndef nlp_pipeline(\n        csv_url=\"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_multilingual_UK_v1_00.tsv.gz\",\n        embed_weights_url=\"http://nlp.stanford.edu/data/glove.42B.300d.zip\",\n        features_column=\"review_body\",\n        labels_column=\"product_category\",\n        raw_text_path='/mnt/text.data',\n        labels_path='/mnt/labels.data',\n        data_folder='/mnt/data',\n        clean_text_path='/mnt/clean.data',\n        tokens_path='/mnt/tokens.data',\n        tfidf_vectors_path='/mnt/tfidf.data',\n        model_prediction_path='/mnt/predicted_train.data',\n        tfidf_model_path='/mnt/tfidf.model',\n        word_index_path='/mnt/word_index.data',\n        embedded_matrix_path='/mnt/embedded_matrix.data',\n        pre_embedded_weights='/mnt/data/glove.42B.300d.txt',\n        train_ratio=0.98,\n        validation_ratio=0.01,\n        test_ratio=0.01,\n        num_words=20000,\n        sentence_max_length=50,\n        deep_model='/mnt/deep_model.model',\n        batch_size='100'):\n    \"\"\"\n    Pipeline\n    \"\"\"\n    vop = dsl.VolumeOp(\n      name='lr-pvc',\n      resource_name=\"lr-pvc\",\n      modes=[\"ReadWriteMany\"],\n      size=\"30Gi\"\n    )\n\n    download_step = dsl.ContainerOp(\n        name='data_downloader',\n        image='docker.io/cyferino/component-kubeflow:0.0.14',\n        command=\"python\",\n        arguments=[\n            \"/src/pipeline_steps/data_downloader/pipeline_step.py\",\n            \"--labels-path\", labels_path,\n            \"--data-folder\", data_folder,\n            \"--features-path\", raw_text_path,\n            \"--csv-url\", csv_url,\n            \"--features-column\", features_column,\n            \"--labels-column\", labels_column\n        ],\n        pvolumes={\"/mnt\": vop.volume}\n    )\n\n    clean_step = dsl.ContainerOp(\n        name='clean_text',\n        image='docker.io/cyferino/component-kubeflow:0.0.14',\n        command=\"python\",\n        arguments=[\n            \"/src/pipeline_steps/clean_text/pipeline_step.py\",\n            \"--in-path\", raw_text_path,\n            \"--out-path\", clean_text_path,\n        ],\n        pvolumes={\"/mnt\": vop.volume}\n    ).after(download_step)\n\n    data_split_step = dsl.ContainerOp(\n        name='data_splitter',\n        image='docker.io/cyferino/component-kubeflow:0.0.14',\n        command=\"python\",\n        arguments=[\n            \"/src/pipeline_steps/train_val_test/pipeline_step.py\",\n            \"--in-path\", clean_text_path,\n            \"--labels-path\", labels_path,\n            \"--out-folder\", data_folder,\n            \"--train-ratio\", train_ratio,\n            \"--validation-ratio\", validation_ratio,\n            \"--test-ratio\", test_ratio\n        ],\n        pvolumes={\"/mnt\": vop.volume}\n    ).after(clean_step)\n\n    tfidf_step = dsl.ContainerOp(\n        name='tfidf',\n        image='docker.io/cyferino/component-kubeflow:0.0.14',\n        command=\"python\",\n        arguments=[\n            \"/src/pipeline_steps/tfidf_vectorizer/pipeline_step.py\",\n            \"--in-path\", \"/mnt/data/train.data\",\n            \"--out-path\", tokens_path,\n            \"--model-path\", \"/mnt/tfidf.model\",\n            \"--action\", \"train\",\n            \"--ngram-range\", 2,\n            \"--max-features\", 1000,\n        ],\n        pvolumes={\"/mnt\": vop.volume}\n    ).after(data_split_step)\n\n    tfidf_step_val = dsl.ContainerOp(\n        name='tfidf_val',\n        image='docker.io/cyferino/component-kubeflow:0.0.14',\n        command=\"python\",\n        arguments=[\n            \"/src/pipeline_steps/tfidf_vectorizer/pipeline_step.py\",\n            \"--in-path\", \"/mnt/data/val.data\",\n            \"--out-path\", \"/mnt/data/tokenized_val.data\",\n            \"--model-path\", \"/mnt/tfidf.model\",\n            \"--action\", \"predict\",\n        ],\n        pvolumes={\"/mnt\": vop.volume}\n    ).after(tfidf_step)\n\n    tfidf_step_test = dsl.ContainerOp(\n        name='tfidf_test',\n        image='docker.io/cyferino/component-kubeflow:0.0.14',\n        command=\"python\",\n        arguments=[\n            \"/src/pipeline_steps/tfidf_vectorizer/pipeline_step.py\",\n            \"--in-path\", \"/mnt/data/test.data\",\n            \"--out-path\", \"/mnt/data/tokenized_test.data\",\n            \"--model-path\", \"/mnt/tfidf.model\",\n            \"--action\", \"predict\",\n        ],\n        pvolumes={\"/mnt\": vop.volume}\n    ).after(tfidf_step)\n\n    train_step = dsl.ContainerOp(\n        name='predictor',\n        image='docker.io/cyferino/component-kubeflow:0.0.14',\n        command=\"python\",\n        arguments=[\n            \"/src/pipeline_steps/lr_text_model/pipeline_step.py\",\n            \"--in-path\", tokens_path,\n            \"--out-path\", model_prediction_path,\n            \"--model-path\", '/mnt/lr_text.model',\n            \"--action\", \"train\",\n            \"--c-param\", 0.1,\n        ],\n        pvolumes={\"/mnt\": vop.volume}\n    ).after(tfidf_step)\n\n    predict_val = dsl.ContainerOp(\n        name='val_predictor',\n        image='docker.io/cyferino/component-kubeflow:0.0.14',\n        command=\"python\",\n        arguments=[\n            \"/src/pipeline_steps/lr_text_model/pipeline_step.py\",\n            \"--in-path\", \"/mnt/data/tokenized_val.data\",\n            \"--out-path\", '/mnt/predicted_val.data',\n            \"--model-path\", '/mnt/lr_text.model',\n            \"--action\", \"predict\",\n        ],\n        pvolumes={\"/mnt\": vop.volume}\n    ).after(train_step, tfidf_step_val)\n\n    predict_test = dsl.ContainerOp(\n        name='test_predictor',\n        image='docker.io/cyferino/component-kubeflow:0.0.14',\n        command=\"python\",\n        arguments=[\n            \"/src/pipeline_steps/lr_text_model/pipeline_step.py\",\n            \"--in-path\", \"/mnt/data/tokenized_test.data\",\n            \"--out-path\", '/mnt/predicted_test.data',\n            \"--model-path\", '/mnt/lr_text.model',\n            \"--action\", \"predict\",\n        ],\n        pvolumes={\"/mnt\": vop.volume}\n    ).after(train_step, tfidf_step_test)\n\n    evaluate_model = dsl.ContainerOp(\n        name='model_evaluator',\n        image='docker.io/cyferino/component-kubeflow:0.0.14',\n        command=\"python\",\n        arguments=[\n            \"/src/pipeline_steps/evaluate_model/pipeline_step.py\",\n            \"--data-folder\", \"/mnt/data\",\n            \"--predicted-train-data\", '/mnt/predicted_train.data',\n            \"--predicted-val-data\", '/mnt/predicted_val.data',\n            \"--predicted-test-data\", '/mnt/predicted_test.data'\n        ],\n        pvolumes={\"/mnt\": vop.volume},\n        file_outputs={'mlpipeline-metrics': '/mlpipeline-metrics.json',\n            'mlpipeline-ui-metadata': '/mlpipeline-ui-metadata.json'},\n        output_artifact_paths={'mlpipeline-metrics': '/mlpipeline-metrics.json',\n            'mlpipeline-ui-metadata': '/mlpipeline-ui-metadata.json'}\n    ).after(train_step, predict_val, predict_test)\n\n    seldon_config = yaml.load(open(\"seldon_production_pipeline.yaml\"))\n\n    deploy_step = dsl.ResourceOp(\n        name=\"seldondeploy\",\n        k8s_resource=seldon_config,\n        attribute_outputs={\"name\": \"{.metadata.name}\"})\n\n    deploy_step.after(train_step)\n\n\nif __name__ == '__main__':\n    import kfp.compiler as compiler\n    compiler.Compiler().compile(nlp_pipeline, __file__ + '_lr.tar.gz')"
  },
  {
    "repo": "Here2ServeU/kubeflow-ml-pipeline",
    "file_path": "pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/Here2ServeU/kubeflow-ml-pipeline/main/pipeline.py",
    "content": "import kfp\nfrom kfp import dsl\nfrom kfp.components import load_component_from_file\n\n@dsl.pipeline(name=\"Kubeflow ML Pipeline\", description=\"End-to-end ML pipeline\")\ndef pipeline():\n    data_load = load_component_from_file(\"components/data_load.py\")()\n    preprocess = load_component_from_file(\"components/preprocess.py\")(data_load.output)\n    train = load_component_from_file(\"components/train.py\")(preprocess.output)\n    evaluate = load_component_from_file(\"components/evaluate.py\")(preprocess.output, train.output)\n    deploy = load_component_from_file(\"components/deploy.py\")(evaluate.output)\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(pipeline, 'pipeline.yaml')\n\n"
  },
  {
    "repo": "javierdarksoul/Kubeflow-pipelines-tutorial",
    "file_path": "tutorial-1/sum_mul_paralell_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/javierdarksoul/Kubeflow-pipelines-tutorial/main/tutorial-1/sum_mul_paralell_pipeline.py",
    "content": "import kfp\nimport kfp.dsl as dsl\nfrom kfp.v2.dsl import component\nfrom kfp import compiler\n@component\ndef add(a: float, b: float) -> float:\n  return a + b\n\n@component\ndef mul(a: float, b: float) -> float:\n  return a * b\n\n\n@dsl.pipeline(\n  name='add-mul-pipeline',\n  description='An example pipeline that performs addition calculations.',\n  # pipeline_root='gs://my-pipeline-root/example-pipeline'\n)\ndef my_pipeline(a: float = 1, b: float = 7, c: float = 2.3):\n  add_task_1 = add(a, b)\n  add_task_4 = add(10, 10)\n  mul_task_1 = mul(add_task_1.output, c)\n  add_task_2 = add(add_task_1.output, mul_task_1.output)\n  mul_task_2 = mul(add_task_2.output, mul_task_1.output)\n\n\n\n\ncompiler.Compiler(mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE).compile(pipeline_func=my_pipeline, package_path='pipeline.yaml')\n\nclient = kfp.Client()\n# run the pipeline in v2 compatibility mode\nclient.create_run_from_pipeline_func(\n    my_pipeline,\n    arguments={'a': 7, 'b': 8, 'c': 3.2},\n    mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE,\n)"
  },
  {
    "repo": "javierdarksoul/Kubeflow-pipelines-tutorial",
    "file_path": "tutorial-1/sum_mul_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/javierdarksoul/Kubeflow-pipelines-tutorial/main/tutorial-1/sum_mul_pipeline.py",
    "content": "import kfp\nimport kfp.dsl as dsl\nfrom kfp.v2.dsl import component\nfrom kfp import compiler\n@component\ndef add(a: float, b: float) -> float:\n  return a + b\n\n@component\ndef mul(a: float, b: float) -> float:\n  return a * b\n\n\n@dsl.pipeline(\n  name='add-mul-pipeline',\n  description='An example pipeline that performs addition calculations.',\n  # pipeline_root='gs://my-pipeline-root/example-pipeline'\n)\ndef my_pipeline(a: float = 1, b: float = 7, c: float = 2.3):\n  add_task = add(a, b)\n  mul_task = mul(add_task.output, c)\n\n\n\n\ncompiler.Compiler(mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE).compile(pipeline_func=my_pipeline, package_path='pipeline.yaml')\n\nclient = kfp.Client()\n# run the pipeline in v2 compatibility mode\nclient.create_run_from_pipeline_func(\n    my_pipeline,\n    arguments={'a': 7, 'b': 8, 'c': 3.2},\n    mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE,\n)"
  },
  {
    "repo": "javierdarksoul/Kubeflow-pipelines-tutorial",
    "file_path": "tutorial-1/sum_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/javierdarksoul/Kubeflow-pipelines-tutorial/main/tutorial-1/sum_pipeline.py",
    "content": "import kfp\nimport kfp.dsl as dsl\nfrom kfp.v2.dsl import component\nfrom kfp import compiler\n@component\ndef add(a: float, b: float) -> float:\n  return a + b\n\n@dsl.pipeline(\n  name='addition-pipeline',\n  description='An example pipeline that performs addition calculations.',\n  # pipeline_root='gs://my-pipeline-root/example-pipeline'\n)\ndef add_pipeline(a: float = 1, b: float = 7):\n  add_task = add(a, b)\n\n\ncompiler.Compiler(mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE).compile(pipeline_func=add_pipeline, package_path='pipeline.yaml')\n\nclient = kfp.Client()\n# run the pipeline in v2 compatibility mode\nclient.create_run_from_pipeline_func(\n    add_pipeline,\n    arguments={'a': 7, 'b': 8},\n    mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE,\n)"
  },
  {
    "repo": "javierdarksoul/Kubeflow-pipelines-tutorial",
    "file_path": "tutorial-2/main.py",
    "raw_url": "https://raw.githubusercontent.com/javierdarksoul/Kubeflow-pipelines-tutorial/main/tutorial-2/main.py",
    "content": "from components.dataset import load_dataset\nimport kfp.dsl as dsl\nfrom kfp.v2 import compiler\nimport kfp\n@dsl.pipeline(\n  name='nn-pipeline',\n  description='un ejemplo de una pipeline completa de un modelo',\n)\ndef nnpipeline(datatype: str):\n  dataset= load_dataset(datatype)\n\n\n\ncompiler.Compiler().compile(pipeline_func=nnpipeline, package_path='pipeline.json')\n\nclient = kfp.Client()\nclient.create_run_from_pipeline_func(\n    nnpipeline,\n    arguments={'datatype': \"mnist\"},\n    mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE,\n)"
  },
  {
    "repo": "javierdarksoul/Kubeflow-pipelines-tutorial",
    "file_path": "tutorial-3/main.py",
    "raw_url": "https://raw.githubusercontent.com/javierdarksoul/Kubeflow-pipelines-tutorial/main/tutorial-3/main.py",
    "content": "from components.dataset import load_dataset\nfrom components.build_src import build_src\nfrom components.loads import loads\nimport kfp.dsl as dsl\nfrom kfp.v2 import compiler\nimport kfp\n@dsl.pipeline(\n  name='nn-pipeline',\n  description='un ejemplo de una pipeline completa de un modelo',\n)\ndef nnpipeline(datatype: str):\n  dataset= load_dataset(datatype)\n  src = build_src()\n  load_task= loads(src.outputs[\"source\"],dataset.outputs[\"dataset\"])\n \n  \n\n\n\ncompiler.Compiler().compile(pipeline_func=nnpipeline, package_path='pipeline.json')\n\nclient = kfp.Client()\nclient.create_run_from_pipeline_func(\n    nnpipeline,\n    arguments={'datatype': \"fmnist\"},\n    mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE,\n)"
  },
  {
    "repo": "javierdarksoul/Kubeflow-pipelines-tutorial",
    "file_path": "tutorial-4/main.py",
    "raw_url": "https://raw.githubusercontent.com/javierdarksoul/Kubeflow-pipelines-tutorial/main/tutorial-4/main.py",
    "content": "import kfp.dsl as dsl\nfrom kfp.v2 import compiler\nimport kfp\nfrom secret import access_secret_version\nfrom kfp.v2.dsl import component\nfrom kfp.v2.dsl import (\n    Output,\n    Input,\n    Artifact,\n)\nimport json\n\n#secret = projects/209915815446/secrets/vertex-ai-secret/versions/1\n\ncredentials=access_secret_version(\"209915815446\",\"vertex-ai-secret\", \"1\")\nget_model=kfp.components.load_component_from_file(\"components/get_model_component.yaml\")\nget_data=kfp.components.load_component_from_file(\"components/get_data_component.yaml\")\n\n@component(\n    packages_to_install=['torch','torchvision'],\n    base_image='python:3.8',\n)\ndef loads(dataset: Input[Artifact],source: Input[Artifact]):\n    import pickle\n    import torch\n    import tarfile\n    tarfile.open(name=source.path, mode=\"r\").extractall('.')\n    from src.nn import my_nn\n    with open(dataset.path,'rb') as file:\n        dataloaders = pickle.load(file)\n    train_loader=dataloaders[\"train_loader\"]\n    valid_loader=dataloaders[\"valid_loader\"]\n    print(train_loader)\n    for image,label in train_loader:\n        print(image.shape)\n        break\n\n@dsl.pipeline(\n  name='nn-pipeline',\n  description='un ejemplo de una pipeline completa de un modelo',\n)\ndef nnpipeline():\n  src = get_model(githubpath='https://github.com/javierdarksoul/src_test.git')\n  data = get_data(githubpath=' https://github.com/javierdarksoul/data_test.git',folder =\"FashionMNIST\",credentials=credentials)\n  load_task= loads(data.outputs['trainloader'],src.outputs['output1path'])\n\ncompiler.Compiler().compile(pipeline_func=nnpipeline, package_path='pipeline.json')\n\nclient = kfp.Client()\nclient.create_run_from_pipeline_func(\n    nnpipeline,\n    arguments={},\n    mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE,\n)\n"
  },
  {
    "repo": "javierdarksoul/Kubeflow-pipelines-tutorial",
    "file_path": "tutorial-5/main.py",
    "raw_url": "https://raw.githubusercontent.com/javierdarksoul/Kubeflow-pipelines-tutorial/main/tutorial-5/main.py",
    "content": "import kfp.dsl as dsl\nfrom kfp.v2 import compiler\nimport kfp\n\nfrom components.pythoncomponents.train import train\nfrom components.pythoncomponents.test import test\nget_model=kfp.components.load_component_from_file(\"components/yaml-components/get_model_component.yaml\")\nget_data=kfp.components.load_component_from_file(\"components/yaml-components/get_data_component.yaml\")\n\n\n@dsl.pipeline(\n  name='nn-pipeline',\n  description='un ejemplo de una pipeline completa de un modelo',\n)\ndef nnpipeline():\n  src = get_model(githubpath='https://github.com/javierdarksoul/src_test.git')\n  data = get_data(githubpath=' https://github.com/javierdarksoul/data_test.git',folder =\"FashionMNIST\")\n  train_task= train(src.outputs[\"output1path\"], data.outputs[\"trainloader\"]) \n  test(src.outputs[\"output1path\"],train_task.outputs[\"weights\"], data.outputs[\"testloader\"])\n  #load_task= loads(src.outputs['output1path'])\n\ncompiler.Compiler().compile(pipeline_func=nnpipeline, package_path='pipeline.json')\n\nclient = kfp.Client()\nclient.create_run_from_pipeline_func(\n    nnpipeline,\n    arguments={},\n    mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE,\n)\n"
  },
  {
    "repo": "javierdarksoul/Kubeflow-pipelines-tutorial",
    "file_path": "example/train_nn/main.py",
    "raw_url": "https://raw.githubusercontent.com/javierdarksoul/Kubeflow-pipelines-tutorial/main/example/train_nn/main.py",
    "content": "import kfp.dsl as dsl\nfrom kfp.v2 import compiler\nimport kfp\nimport google.cloud.aiplatform as aip\nimport os\nimport google.auth\nfrom google_cloud_pipeline_components import aiplatform as gcc_aip\nfrom components.pythoncomponents.train import train\nfrom components.pythoncomponents.test import test\nget_model=kfp.components.load_component_from_file(\"components/yaml-components/get_model_component.yaml\")\nget_data=kfp.components.load_component_from_file(\"components/yaml-components/get_data_component.yaml\")\nmar_comp=kfp.components.load_component_from_file(\"components/yaml-components/component_mar.yaml\")\nbuild_torchserve=kfp.components.load_component_from_file(\"components/yaml-components/build_torchserve_component.yaml\")\n\n@dsl.pipeline(\n  name='nn-pipeline',\n  description='un ejemplo de una pipeline completa de un modelo',\n)\ndef nnpipeline():\n  project_id = \"zippedi-project-01\"\n  region= \"us-central1\"\n  src_repo = 'https://github.com/javierdarksoul/src_test.git'\n  data_repo = 'https://github.com/javierdarksoul/data_test.git'\n  dataset = \"FashionMNIST\"\n  model_name= \"example-model\"\n  artifact_folder = \"us-docker.pkg.dev/zippedi-project-01/kubeflow-components/\"\n  tensorboard_route = \"gs://example-vertex-ai/tensorboard_runs/example001/\"\n  src = get_model(githubpath=src_repo)\n  data = get_data(githubpath=data_repo,folder =dataset)\n  train_task= train(src.outputs[\"output1path\"], data.outputs[\"trainloader\"],tensorboard_route,True).add_node_selector_constraint('cloud.google.com/gke-accelerator','nvidia-tesla-t4').set_gpu_limit(1)\n  test_task=test(src.outputs[\"output1path\"],train_task.outputs[\"weights\"], data.outputs[\"testloader\"])\n  test_task.set_caching_options(False)\n\n  with dsl.Condition(test_task.outputs['output']>0.8):\n\n    mar_comp_task=mar_comp(src.outputs[\"output1path\"],model_name,train_task.outputs[\"weights\"] )\n    task=build_torchserve(model_name,mar_comp_task.outputs[\"output_1\"],artifact_folder,project_id)\n    task.set_caching_options(False)\n    model_upload_op = gcc_aip.ModelUploadOp(\n        project=project_id,\n        display_name=model_name,\n        serving_container_image_uri=task.outputs['output_1'],\n        serving_container_predict_route=\"/predictions/{}\".format(model_name),\n        serving_container_health_route=\"/ping\",\n        location=region,\n        serving_container_ports=[{\"containerPort\": 7080}]      \n    )\n    model_upload_op.set_caching_options(False)\n    \n  # gcc_aip.EndpointCreateOp.component_spec.implementation.container.image = \"gcr.io/ml-pipeline/google-cloud-pipeline-components:0.1.7\"\n    endpoint_create_op = gcc_aip.EndpointCreateOp(\n          project=project_id,\n          display_name=model_name+\"-endpoint\",\n          location=region\n      ).set_caching_options(False)\n    \n    #gcc_aip.ModelDeployOp.component_spec.implementation.container.image = \"gcr.io/ml-pipeline/google-cloud-pipeline-components:0.1.7\"\n    model_deploy_op = gcc_aip.ModelDeployOp(\n          #project=project_id,\n          endpoint=endpoint_create_op.outputs[\"endpoint\"],\n          model=model_upload_op.outputs[\"model\"],\n          deployed_model_display_name=model_name,\n          dedicated_resources_machine_type=\"n1-standard-4\",\n          dedicated_resources_min_replica_count=1,\n          #accelerator_type='NVIDIA_TESLA_P100',  # CHANGE THIS as necessary\n          #accelerator_count=1        \n      )\n    \n    \ncompiler.Compiler().compile(pipeline_func=nnpipeline, package_path='pipeline.json')\n\nimport google.auth\n\ncred = google.auth.default() #google.auth.load_credentials_from_file(\"../../../credentials/credentials.json\")\njob = aip.PipelineJob(\n    display_name=\"train_nn_pipeline\",\n    template_path=\"pipeline.json\",\n    pipeline_root=\"gs://example-vertex-ai\",\n    credentials=cred[0],\n    project\t= \"zippedi-project-01\",\n    location = \"us-central1\",\n    parameter_values={\n    }\n)\n#print(job)\njob.submit()\nos.system(\"rm pipeline.json\")\n"
  },
  {
    "repo": "javierdarksoul/Kubeflow-pipelines-tutorial",
    "file_path": "tutorial-6/sum_mul/sum_mul_paralell_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/javierdarksoul/Kubeflow-pipelines-tutorial/main/tutorial-6/sum_mul/sum_mul_paralell_pipeline.py",
    "content": "import kfp\nimport kfp.dsl as dsl\nfrom kfp.v2.dsl import component\nfrom kfp.v2 import compiler\nimport google.cloud.aiplatform as aip\nimport os\n@component\ndef add(a: float, b: float) -> float:\n  return a + b\n\n@component\ndef mul(a: float, b: float) -> float:\n  return a * b\n\n\n@dsl.pipeline(\n  name='add-mul-pipeline',\n  description='An example pipeline that performs addition calculations.',\n  # pipeline_root='gs://my-pipeline-root/example-pipeline'\n)\ndef my_pipeline(a: float = 1, b: float = 7, c: float = 2.3):\n  add_task_1 = add(a, b)\n  mul_task_1 = mul(add_task_1.output, c)\n  add_task_2 = add(add_task_1.output, mul_task_1.output)\n  mul_task_2 = mul(add_task_2.output, mul_task_1.output)\n\n\n\n\ncompiler.Compiler().compile(pipeline_func=my_pipeline, package_path='sum_mul_paralell.json')\n\nimport google.auth\ncred =  google.auth.default()  #google.auth.load_credentials_from_file(\"../../../credentials/credentials.json\")\n\njob = aip.PipelineJob(\n    display_name=\"sum_mul_paralell\",\n    template_path=\"sum_mul_paralell.json\",\n    pipeline_root= \"gs://example-vertex-ai\",\n    credentials=cred[0],\n    project\t= \"zippedi-project-01\",\n    location = \"us-central1\",\n    parameter_values={\n        'a': 1,\n        'b': 2,\n        'c': 3\n    }\n)\njob.submit()\nos.system(\"rm sum_mul_paralell.json\")\n"
  },
  {
    "repo": "javierdarksoul/Kubeflow-pipelines-tutorial",
    "file_path": "tutorial-6/train_nn/main.py",
    "raw_url": "https://raw.githubusercontent.com/javierdarksoul/Kubeflow-pipelines-tutorial/main/tutorial-6/train_nn/main.py",
    "content": "import kfp.dsl as dsl\nfrom kfp.v2 import compiler\nimport kfp\nimport google.cloud.aiplatform as aip\nimport os\nfrom components.pythoncomponents.train import train\nfrom components.pythoncomponents.test import test\nget_model=kfp.components.load_component_from_file(\"components/yaml-components/get_model_component.yaml\")\nget_data=kfp.components.load_component_from_file(\"components/yaml-components/get_data_component.yaml\")\n\n\n@dsl.pipeline(\n  name='nn-pipeline',\n  description='un ejemplo de una pipeline completa de un modelo',\n)\ndef nnpipeline():\n  src = get_model(githubpath='https://github.com/javierdarksoul/src_test.git')\n  data = get_data(githubpath=' https://github.com/javierdarksoul/data_test.git',folder =\"FashionMNIST\")\n  train_task= train(src.outputs[\"output1path\"], data.outputs[\"trainloader\"])#.add_node_selector_constraint('cloud.google.com/gke-accelerator','NVIDIA_TESLA_P100').set_gpu_limit(1))\n  test(src.outputs[\"output1path\"],train_task.outputs[\"weights\"], data.outputs[\"testloader\"])\n \n\ncompiler.Compiler().compile(pipeline_func=nnpipeline, package_path='pipeline.json')\n\nimport google.auth\ncred = google.auth.default() #google.auth.load_credentials_from_file(\"../../../credentials/credentials.json\")\njob = aip.PipelineJob(\n    display_name=\"train_nn_pipeline\",\n    template_path=\"pipeline.json\",\n    pipeline_root=\"gs://example-vertex-ai\",\n    credentials=cred[0],\n    project\t= \"zippedi-project-01\",\n    location = \"us-central1\",\n    parameter_values={\n    }\n)\n#print(job)\njob.submit()\nos.system(\"rm pipeline.json\")\n"
  },
  {
    "repo": "THEANMOZHI/kubeflow-iris-pipeline",
    "file_path": "iris.py",
    "raw_url": "https://raw.githubusercontent.com/THEANMOZHI/kubeflow-iris-pipeline/main/iris.py",
    "content": "# kfp\r\nimport kfp\r\nfrom kfp import dsl\r\nfrom kfp.v2 import compiler\r\nfrom kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\r\n                        OutputPath, ClassificationMetrics, Metrics, component)\r\nfrom kfp.v2.google.client import AIPlatformClient\r\n\r\n# gcp\r\nfrom google.cloud import aiplatform\r\nfrom google_cloud_pipeline_components import aiplatform as gcc_aip\r\nfrom google.cloud import storage\r\nfrom googleapiclient import discovery\r\nfrom oauth2client.client import GoogleCredentials\r\n\r\n# i/o\r\nfrom typing import NamedTuple\r\nfrom io import StringIO\r\nimport os\r\n\r\n# pandas & sklearn\r\nimport pandas as pd\r\nimport sklearn\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.svm import SVC\r\nfrom sklearn.metrics import accuracy_score\r\n\r\n\r\n# definites\r\nPROJECT_ID=\"kedro-kubeflow-334417\"\r\nBUCKET_NAME=\"gs://diab-gsbucket\"\r\nREGION=\"us-central1\"\r\nPIPELINE_NAME = \"diabetes_pipeline\"\r\nPIPELINE_ROOT = f\"{BUCKET_NAME}/pipeline_root/\"\r\nPIPELINE_ROOT\r\n\r\n\r\n# data component\r\n@component(base_image='python:3.7',\r\n           packages_to_install=['numpy', 'pandas', 'google-cloud-storage==1.43.0', 'google-cloud-aiplatform==1.0.0',\r\n                         'kubeflow-metadata', 'scikit-learn', 'gcsfs==2021.11.1'])\r\ndef data_component(bucket: str, value: float, marker: int) -> int:\r\n    import kfp\r\n    import pandas as pd\r\n    import sklearn\r\n    from sklearn.model_selection import train_test_split\r\n    from kfp.v2.google.client import AIPlatformClient\r\n    from google.cloud import storage\r\n    \r\n    # read data from gcs bucket\r\n    data = pd.read_csv('gs://iris-kfp/iris.csv', index_col=False) \r\n    \r\n    # data preprocessing\r\n    # normalizing data\r\n    df = data\r\n    for column in df.columns:\r\n        df[column] = (df[column] - df[column].mean()) / df[column].std() \r\n    data = df\r\n    # dependent and independent data sets\r\n    train_data = data.drop('class',axis=1)\r\n    test_data = data['class']    \r\n    \r\n    # test-train  data split\r\n    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(train_data, test_data, test_size = value, random_state=42)\r\n    X_train = X_train.to_csv()\r\n    X_test = X_test.to_csv()\r\n    y_train = y_train.to_csv()\r\n    y_test = y_test.to_csv()\r\n    \r\n    # storage client\r\n    storage_client = storage.Client()\r\n    bucket = storage_client.get_bucket('iris-kfp')\r\n    # blobs\r\n    d1 = bucket.blob('X_train.csv')\r\n    d2 = bucket.blob('X_test.csv')\r\n    d3 = bucket.blob('y_train.csv')\r\n    d4 = bucket.blob('y_test.csv')\r\n    \r\n    # uploading train-test datasets into gcs bucket\r\n    d1.upload_from_string(f'{X_train}.csv', 'text/csv')\r\n    d2.upload_from_string(f'{X_test}.csv', 'text/csv')\r\n    d3.upload_from_string(f'{y_train}.csv', 'text/csv')\r\n    d4.upload_from_string(f'{y_test}.csv', 'text/csv')\r\n    \r\n    # setting flag\r\n    df1 = pd.read_csv(\"gs://iris-kfp/X_train.csv\", index_col=0)\r\n    df2 = pd.read_csv(\"gs://iris-kfp/X_test.csv\", index_col=0)\r\n    df3 = pd.read_csv(\"gs://iris-kfp/y_train.csv\", index_col=0)\r\n    df4 = pd.read_csv(\"gs://iris-kfp/y_test.csv\", index_col=0)\r\n    \r\n    if(df1.empty == True and df2.empty == True and df3.empty == True and df4.empty == True):\r\n        marker = 1\r\n    else:\r\n        marker = 0\r\n\r\n    return marker\r\n\r\n\r\n\r\n# model component\r\n@component(base_image='python:3.7',\r\n        packages_to_install=['numpy', 'pandas', 'google-cloud-storage==1.43.0', 'google-cloud-aiplatform==1.0.0',\r\n                            'kubeflow-metadata', 'scikit-learn', 'gcsfs==2021.11.1'])\r\ndef model_component(bucket:str, xtrain:str, ytrain:str, xtest:str, ytest:str) -> float:\r\n    import pandas as pd    \r\n    from sklearn.ensemble import RandomForestClassifier\r\n    from sklearn.metrics import accuracy_score\r\n    \r\n    # read test-train data sets from GCS bucket\r\n    X_train = pd.read_csv(f'{bucket}/{xtrain}.csv', sep=\",\")\r\n    y_train = pd.read_csv(f'{bucket}/{ytrain}.csv', sep=\",\")\r\n    X_test = pd.read_csv(f'{bucket}/{xtest}.csv', sep=\",\")\r\n    y_test = pd.read_csv(f'{bucket}/{ytest}.csv', sep=\",\")    \r\n        \r\n    # train model\r\n    model = RandomForestClassifier(max_depth=2, random_state=3)\r\n    model.fit(X_train, y_train)\r\n    predictions = model.predict(X_test)\r\n\r\n    # find accuracy\r\n    accuracy = accuracy_score(y_test, predictions)\r\n    \r\n    return accuracy\r\n\r\n\r\n# when accuracy >= threshold\r\n@component(base_image='python:3.7',\r\n           packages_to_install=['numpy', 'pandas', 'google-cloud-storage==1.43.0', 'google-cloud-aiplatform==1.0.0',\r\n                         'kubeflow-metadata', 'scikit-learn'])\r\ndef true_component(accuracy:float) -> None:\r\n    print(f'Yes!! {accuracy} is the Accuracy and its greater than the threshold')\r\n\r\n\r\n# when accuracy < thrershold\r\n@component(base_image='python:3.7',\r\n           packages_to_install=['numpy', 'pandas', 'google-cloud-storage==1.43.0', 'google-cloud-aiplatform==1.0.0',\r\n                         'kubeflow-metadata', 'scikit-learn'])\r\ndef false_component(accuracy:float) -> None:\r\n    print(f'No. {accuracy} is the Accuracy and its smaller than the threshold')\r\n\r\n\r\n# pipeline\r\n@kfp.dsl.pipeline(name = \"iris-pipeline\",\r\n                  pipeline_root = PIPELINE_ROOT)\r\ndef iris_pipeline(\r\n    display_name: str=\"iris-kfp\",\r\n    project: str = PROJECT_ID,\r\n    gcp_region: str = \"us-central1\",\r\n    api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",\r\n    marker: int = 0,\r\n    test_train_split_ratio: float = 0.3,\r\n    accuracy_threshold: float = 0.5,\r\n    bucket: str = \"gs://iris-kfp\"\r\n) -> None:\r\n        \r\n    # initiating data component\r\n    data_op = data_component(bucket, test_train_split_ratio, marker)\r\n\r\n    # initiating model component\r\n    with dsl.Condition(data_op.output == 1):\r\n        model_op = model_component(bucket, \"X_train\", \"y_train\", \"X_test\", \"y_test\")\r\n    \r\n        with dsl.Condition(model_op.output >= accuracy_threshold, name=\"accuracy>=50\"):\r\n            true_component(model_op.output)\r\n        with dsl.Condition(model_op.output < accuracy_threshold, name=\"accuracy<50\"):\r\n            false_component(model_op.output)\r\n\r\n\r\ncompiler.Compiler().compile(\r\n    pipeline_func=iris_pipeline, package_path=\"iris_kfp.json\"\r\n)\r\n\r\napi_client = AIPlatformClient(\r\n    project_id=PROJECT_ID,\r\n    region=REGION,\r\n)\r\n\r\nresponse = api_client.create_run_from_job_spec(\r\n    \"iris_kfp.json\", pipeline_root=PIPELINE_ROOT,\r\n)"
  },
  {
    "repo": "agapebondservant/kubeflow-pipelines-accelerator",
    "file_path": "app/main.py",
    "raw_url": "https://raw.githubusercontent.com/agapebondservant/kubeflow-pipelines-accelerator/main/app/main.py",
    "content": "import kfp.dsl as dsl\nfrom kfp import Client\nimport logging\nimport warnings\nimport utils\nimport datetime\n\nlogging.basicConfig()\nlogging.getLogger().setLevel(logging.INFO)\nwarnings.filterwarnings('ignore')\n\n\n@dsl.pipeline(\n    name='cifar_cnn_kfp',\n    description='Pipeline which trains and serves a CNN model using Keras.'\n)\ndef cifar_pipeline():\n\n    # Upload Dataset\n    upload_dataset = dsl.ContainerOp(\n        name='upload_dataset',\n        image='oawofolu/ml-image-processor:latest',\n        command=\"python\",\n        arguments=[\n            \"/app/main.py\",\n            \"mlflow_entry=upload_dataset\",\n            f\"mlflow_stage={utils.get_env_var('MLFLOW_STAGE')}\",\n            f\"git_repo={utils.get_env_var('GIT_REPO')}\",\n            f\"experiment_name={utils.get_env_var('EXPERIMENT_NAME')}\",\n            f\"environment_name={utils.get_env_var('ENVIRONMENT_NAME')}\",\n            f\"default_token={datetime.datetime.now()}\"\n        ]\n    )\n\n    # Train Model\n    train_model = dsl.ContainerOp(\n        name='train_model',\n        image='oawofolu/ml-image-processor:latest',\n        command=\"python\",\n        arguments=[\n            \"/app/main.py\",\n            \"mlflow_entry=train_model\",\n            f\"mlflow_stage={utils.get_env_var('MLFLOW_STAGE')}\",\n            f\"git_repo={utils.get_env_var('GIT_REPO')}\",\n            f\"experiment_name={utils.get_env_var('EXPERIMENT_NAME')}\",\n            f\"environment_name={utils.get_env_var('ENVIRONMENT_NAME')}\",\n            f\"default_token={datetime.datetime.now()}\"\n        ]\n    )\n    train_model.after(upload_dataset)\n\n    # Evaluate Model\n    evaluate_model = dsl.ContainerOp(\n        name='evaluate_model',\n        image='oawofolu/ml-image-processor:latest',\n        command=\"python\",\n        arguments=[\n            \"/app/main.py\",\n            \"mlflow_entry=evaluate_model\",\n            f\"mlflow_stage={utils.get_env_var('MLFLOW_STAGE')}\",\n            f\"git_repo={utils.get_env_var('GIT_REPO')}\",\n            f\"experiment_name={utils.get_env_var('EXPERIMENT_NAME')}\",\n            f\"environment_name={utils.get_env_var('ENVIRONMENT_NAME')}\",\n            f\"default_token={datetime.datetime.now()}\"\n        ]\n    )\n    evaluate_model.after(train_model)\n\n    # Promote Model to Staging\n    promote_model_to_staging = dsl.ContainerOp(\n        name='promote_model_to_staging',\n        image='oawofolu/ml-image-processor:latest',\n        command=\"python\",\n        arguments=[\n            \"/app/main.py\",\n            \"mlflow_entry=promote_model_to_staging\",\n            f\"mlflow_stage={utils.get_env_var('MLFLOW_STAGE')}\",\n            f\"git_repo={utils.get_env_var('GIT_REPO')}\",\n            f\"experiment_name={utils.get_env_var('EXPERIMENT_NAME')}\",\n            f\"environment_name={utils.get_env_var('ENVIRONMENT_NAME')}\",\n            f\"default_token={datetime.datetime.now()}\"\n        ]\n    )\n    promote_model_to_staging.after(evaluate_model)\n\n\nif __name__ == '__main__':\n    logging.info(f\"Compiling Kubeflow pipeline...host={utils.get_env_var('KUBEFLOW_PIPELINES_HOST')}\")\n    import kfp.compiler as compiler\n    compiler.Compiler().compile(cifar_pipeline, __file__ + '.zip')\n\n    logging.info(\"Generating new experiment run...\")\n    client = Client(host=f'{utils.get_env_var(\"KUBEFLOW_PIPELINES_HOST\")}')\n    cifar_experiment = client.create_experiment(name=utils.get_env_var('EXPERIMENT_NAME'))\n    cifar_run = client.run_pipeline(cifar_experiment.id, 'cifar-pipeline', __file__ + '.zip')\n    logging.info(\"Run completed.\")\n"
  },
  {
    "repo": "himanshusharma89/kubeflow-ml-pipeline",
    "file_path": "pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/himanshusharma89/kubeflow-ml-pipeline/main/pipeline.py",
    "content": "# pipeline.py\nfrom kfp import dsl, compiler\nfrom components.data_acquisition import acquire_dataset\nfrom components.feature_preparation import prepare_features\nfrom components.model_development import develop_model\nfrom components.performance_assessment import assess_performance\n\n@dsl.pipeline(name=\"iris-classification-pipeline\")\ndef classification_pipeline():\n    \"\"\"Orchestrate the end-to-end classification pipeline.\"\"\"\n    # Data acquisition\n    data_op = acquire_dataset()\n    \n    # Feature preparation\n    prep_op = prepare_features(raw_dataset=data_op.outputs[\"dataset_output\"])\n    \n    # Model development\n    model_op = develop_model(\n        training_features=prep_op.outputs[\"training_features\"],\n        training_labels=prep_op.outputs[\"training_labels\"]\n    )\n    \n    # Performance assessment - Fixed the output reference\n    assess_op = assess_performance(\n        testing_features=prep_op.outputs[\"testing_features\"],\n        testing_labels=prep_op.outputs[\"testing_labels\"],  # This was the issue\n        trained_model=model_op.outputs[\"model_artifact\"]\n    )\n\nif __name__ == \"__main__\":\n    compiler.Compiler().compile(\n        pipeline_func=classification_pipeline,\n        package_path=\"iris_pipeline.yaml\"\n    )"
  },
  {
    "repo": "himanshusharma89/kubeflow-ml-pipeline",
    "file_path": "components/data_acquisition.py",
    "raw_url": "https://raw.githubusercontent.com/himanshusharma89/kubeflow-ml-pipeline/main/components/data_acquisition.py",
    "content": "from kfp import dsl\nfrom kfp.dsl import Output, Dataset, component\n\n@dsl.component(base_image=\"python:3.9\")\ndef acquire_dataset(dataset_output: Output[Dataset]):\n    \"\"\"Acquire and prepare the initial dataset.\"\"\"\n    import subprocess\n    subprocess.run([\"pip\", \"install\", \"pandas\", \"scikit-learn\"], check=True)\n    \n    from sklearn.datasets import load_iris\n    import pandas as pd\n    \n    raw_data = load_iris()\n    dataset = pd.DataFrame(\n        raw_data.data,\n        columns=[name.replace(' ', '_').lower() for name in raw_data.feature_names]\n    )\n    dataset['species_class'] = raw_data.target\n    \n    dataset.to_csv(dataset_output.path, index=False)\n"
  },
  {
    "repo": "himanshusharma89/kubeflow-ml-pipeline",
    "file_path": "components/feature_preparation.py",
    "raw_url": "https://raw.githubusercontent.com/himanshusharma89/kubeflow-ml-pipeline/main/components/feature_preparation.py",
    "content": "from kfp import dsl\nfrom kfp.dsl import Input, Output, Dataset, component\n\n@dsl.component(base_image=\"python:3.9\")\ndef prepare_features(\n    raw_dataset: Input[Dataset],\n    training_features: Output[Dataset],\n    testing_features: Output[Dataset],\n    training_labels: Output[Dataset],\n    testing_labels: Output[Dataset]\n):\n    \"\"\"Transform and split the dataset for modeling.\"\"\"\n    import subprocess\n    subprocess.run([\"pip\", \"install\", \"pandas\", \"scikit-learn\"], check=True)\n    \n    import pandas as pd\n    import numpy as np\n    from sklearn.preprocessing import RobustScaler\n    from sklearn.model_selection import train_test_split\n    \n    dataset = pd.read_csv(raw_dataset.path)\n    assert dataset.notna().all().all(), \"Dataset contains missing values\"\n    \n    features = dataset.drop(columns=['species_class'])\n    target = dataset['species_class']\n    \n    feature_transformer = RobustScaler()\n    normalized_features = feature_transformer.fit_transform(features)\n    \n    X_train, X_test, y_train, y_test = train_test_split(\n        normalized_features, \n        target,\n        test_size=0.25,\n        random_state=42,\n        stratify=target\n    )\n    \n    train_df = pd.DataFrame(X_train, columns=features.columns)\n    test_df = pd.DataFrame(X_test, columns=features.columns)\n    train_labels_df = pd.DataFrame(y_train, columns=['species_class'])\n    test_labels_df = pd.DataFrame(y_test, columns=['species_class'])\n    \n    train_df.to_csv(training_features.path, index=False)\n    test_df.to_csv(testing_features.path, index=False)\n    train_labels_df.to_csv(training_labels.path, index=False)\n    test_labels_df.to_csv(testing_labels.path, index=False)\n"
  },
  {
    "repo": "himanshusharma89/kubeflow-ml-pipeline",
    "file_path": "components/model_development.py",
    "raw_url": "https://raw.githubusercontent.com/himanshusharma89/kubeflow-ml-pipeline/main/components/model_development.py",
    "content": "from kfp import dsl\nfrom kfp.dsl import Input, Output, Dataset, Model, component\n\n@dsl.component(base_image=\"python:3.9\")\ndef develop_model(\n    training_features: Input[Dataset],\n    training_labels: Input[Dataset],\n    model_artifact: Output[Model]\n):\n    \"\"\"Build and train the classification model.\"\"\"\n    import subprocess\n    subprocess.run([\"pip\", \"install\", \"pandas\", \"scikit-learn\", \"joblib\"], check=True)\n    \n    import pandas as pd\n    from sklearn.linear_model import LogisticRegression\n    from joblib import dump\n    \n    X = pd.read_csv(training_features.path)\n    y = pd.read_csv(training_labels.path)['species_class']\n    \n    classifier = LogisticRegression(\n        class_weight='balanced',\n        max_iter=1000,\n        random_state=42,\n        multi_class='multinomial'\n    )\n    classifier.fit(X, y)\n    \n    dump(classifier, model_artifact.path)"
  },
  {
    "repo": "himanshusharma89/kubeflow-ml-pipeline",
    "file_path": "components/performance_assessment.py",
    "raw_url": "https://raw.githubusercontent.com/himanshusharma89/kubeflow-ml-pipeline/main/components/performance_assessment.py",
    "content": "from kfp import dsl\nfrom kfp.dsl import Input, Output, Dataset, Model, component\n\n@dsl.component(base_image=\"python:3.9\")\ndef assess_performance(\n    testing_features: Input[Dataset],\n    testing_labels: Input[Dataset],\n    trained_model: Input[Model],\n    performance_metrics: Output[Dataset]\n):\n    \"\"\"Evaluate model performance and generate visualization.\"\"\"\n    import subprocess\n    subprocess.run([\"pip\", \"install\", \"pandas\", \"scikit-learn\", \"seaborn\", \"joblib\"], check=True)\n    \n    import pandas as pd\n    import seaborn as sns\n    import matplotlib.pyplot as plt\n    from sklearn.metrics import classification_report, confusion_matrix\n    from joblib import load\n    \n    X_test = pd.read_csv(testing_features.path)\n    y_true = pd.read_csv(testing_labels.path)['species_class']\n    classifier = load(trained_model.path)\n    \n    y_pred = classifier.predict(X_test)\n    \n    metrics = classification_report(y_true, y_pred, output_dict=True)\n    conf_matrix = confusion_matrix(y_true, y_pred)\n    \n    plt.figure(figsize=(10, 8))\n    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='YlOrRd')\n    plt.title('Confusion Matrix Heatmap')\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Actual Class')\n    \n    results = {\n        'metrics': metrics,\n        'confusion_matrix': conf_matrix.tolist()\n    }\n    pd.DataFrame([results]).to_json(performance_metrics.path)\n"
  },
  {
    "repo": "Louis5499/DRAGON-Kubeflow",
    "file_path": "tf_job_dragon_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/Louis5499/DRAGON-Kubeflow/master/tf_job_dragon_pipeline.py",
    "content": "import json\nfrom kfp import components\nimport kfp.dsl as dsl\n\n@dsl.pipeline(\n    name=\"Launch kubeflow tfjob & kfserving template\",\n    description=\"An example to launch tfjob.\"\n)\ndef mnist_pipeline(\n        name=\"mnist\",\n        namespace=\"kubeflow\",\n        workerNum=2,\n        deleteAfterDone=False):\n    tfjob_launcher_op = components.load_component_from_file(\"./tfJobComponent.yaml\")\n    bucket = \"kf_second_test\"\n    \n    chief = {}\n    worker = {}\n    if workerNum > 0:\n      worker = {\n        \"replicas\": 1,\n        \"restartPolicy\": \"OnFailure\",\n        \"template\": {\n          \"spec\": {\n            \"terminationGracePeriodSeconds\": 0,\n            \"containers\": [\n              {\n                \"args\": [\n                  \"curl -s http://140.114.78.229/web/mnist-new.py | python3 -\"\n                ],\n                \"env\": [\n                  {\n                    \"name\": \"global_steps\",\n                    \"value\": \"100000\"\n                  }\n                ],\n                \"command\": [\n                  \"/bin/bash\",\n                  \"-c\"\n                ],\n                \"image\": \"ncy9371/tensorflow:1.15.2-py3-noavx\",\n                \"name\": \"tensorflow\",\n                \"ports\": [\n                  {\n                    \"containerPort\": 2222,\n                    \"name\": \"tfjob-port\"\n                  }\n                ],\n                \"resources\": {\n                  \"requests\": {\n                    \"cpu\": \"1\",\n                    \"memory\": \"2Gi\"\n                  }\n                }\n              }\n            ]\n          }\n        }\n      }\n\n    ps = {\n        \"replicas\": 1,\n        \"restartPolicy\": \"OnFailure\",\n        \"template\": {\n            \"spec\": {\n            \"terminationGracePeriodSeconds\": 0,\n            \"containers\": [\n                {\n                \"args\": [\n                    \"curl -s http://140.114.78.229/web/mnist-new.py | python3 -\"\n                ],\n                \"env\": [\n                    {\n                    \"name\": \"global_steps\",\n                    \"value\": \"100000\"\n                    }\n                ],\n                \"command\": [\n                    \"/bin/bash\",\n                    \"-c\"\n                ],\n                \"image\": \"ncy9371/tensorflow:1.15.2-py3-noavx\",\n                \"name\": \"tensorflow\",\n                \"ports\": [\n                    {\n                    \"containerPort\": 2222,\n                    \"name\": \"tfjob-port\"\n                    }\n                ]\n                }\n            ]\n            }\n        }\n    }\n    tfJobLauncher = tfjob_launcher_op(\n      name=name,\n      namespace=namespace,\n      worker_spec=worker,\n      chief_spec=chief,\n      ps_spec=ps,\n      delete_finished_tfjob=deleteAfterDone\n    )\n\nif __name__ == \"__main__\":\n    import kfp.compiler as compiler\n    compiler.Compiler().compile(mnist_pipeline, __file__ + \".tar.gz\")\n"
  },
  {
    "repo": "dhodun/kubeflow-pipeline-examples",
    "file_path": "mask_rcnn/mask_rcnn.py",
    "raw_url": "https://raw.githubusercontent.com/dhodun/kubeflow-pipeline-examples/master/mask_rcnn/mask_rcnn.py",
    "content": "#!/usr/bin/env python3\n# Copyright 2018 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport kfp.dsl as dsl\nfrom kfp.gcp import use_tpu\n\ndef export_op_fn(name, arguments):\n  op = dsl.ContainerOp(\n    name=name,\n    image='gcr.io/dhodun1/export-mask-rcnn-saved:latest',\n    arguments=arguments,\n    file_outputs={'export_dir': '/output.txt'}\n  )\n  return op\n\n\n@dsl.pipeline(\n  name='mask_rcnn',\n  description='Preprocess COCO and train Mask RCNN Model'\n)\ndef train_and_deploy(\n        project='dhodun1',\n        bucket='gs://maskrcnn-kfp',\n        #TODO: non-camel-case was conflicting with the use_tpu op modifier\n):\n  usetpu = True\n  istest = True\n\n  \"\"\"Pipeline to train Mask RCNN\"\"\"\n  start_step = 1\n\n  if start_step <= 1:\n    preprocess_coco = dsl.ContainerOp(\n      name='preprocess_coco',\n      # image needs to be compile-time string\n      image='gcr.io/dhodun1/preprocess-coco:latest',\n      arguments=[bucket],\n      file_outputs={'coco_dir': '/output.txt'}\n    )\n    preprocess_coco.container.set_cpu_request('8')\n    preprocess_coco.container.set_memory_request('30G')\n\n  if start_step <=2:\n    train_mask_rcnn = dsl.ContainerOp(\n      name='train_mask_rcnn_tpu',\n      # image needs to be a compile-time string\n      image='gcr.io/dhodun1/train-mask-rcnn:latest',\n      arguments=[bucket,\n                 preprocess_coco.outputs['coco_dir'],\n                 str(usetpu),\n                 str(istest)],\n      file_outputs={'model_dir': '/model_dir.txt',\n                    'mAP_box': '/map_box.txt',\n                    'mAP_segm': '/map_segm.txt'}\n    )\n    train_mask_rcnn.after(preprocess_coco)\n    train_mask_rcnn.container.set_cpu_request('8')\n    train_mask_rcnn.container.set_memory_request('30G')\n    #train_mask_rcnn_tpu.container.set_pull_image_policy('Always')\n    if usetpu:\n      train_mask_rcnn.apply(use_tpu(tpu_cores=8, tpu_resource='v3', tf_version='1.12'))\n      # note needed now that i've consolidated TPU\n      #train_mask_rcnn.container.image='gcr.io/dhodun1/train-mask-rcnn-tpu:latest'\n\n\n  if start_step <=3:\n    export_model_jpeg = export_op_fn(name='export_model_jpeg',\n                                     arguments=['jpeg',\n                                                train_mask_rcnn.outputs['model_dir'],\n                                                train_mask_rcnn.outputs['model_dir']]\n    )\n    export_model_jpeg.after(train_mask_rcnn)\n\n    export_model_tensor = export_op_fn(name='export_model_tensor',\n                                     arguments=['tensor',\n                                                train_mask_rcnn.outputs['model_dir'],\n                                                train_mask_rcnn.outputs['model_dir']]\n                                     )\n    export_model_tensor.after(train_mask_rcnn)\n\n\n\n\n\n\n\"\"\"\n  if start_step <= 2:\n    download_and_preprocess = dsl.ContainerOp(\n      name='download_and_preprocess',\n      # image needs to be a compile-time string\n      image='gcr.io/dhodun1/tpu-test',\n      arguments=[\n        '--project', project,\n        '--mode', 'cloud',\n        '--bucket', bucket,\n        '--start_year', startYear\n      ],\n      file_outputs={'result': '/output.txt'}\n    )\n  else:\n    preprocess = ObjectDict({\n      'outputs': {\n        'bucket': bucket\n      }\n    })\n  download_and_preprocess.apply(use_tpu(tpu_cores=8, tpu_resource='v2', tf_version='1.11'))\n  download_and_preprocess.set_gpu_limit(8, vendor='nvidia')\n\n\n  # Step 1: Download COCO dataset and transform to TFRecords\n  if start_step <= 1:\n    download_and_preprocess = dsl.ContainerOp(\n      name='download_and_preprocess',\n      # image needs to be a compile-time string\n      image='gcr.io/tensorflow/tpu-models:r1.11',\n      command = '''          - /bin/bash\n          - -c\n          - >\n            cd /tensorflow_tpu_models/tools/datasets &&\n            bash download_and_preprocess_coco.sh /scratch-dir &&\n            gsutil -m cp /scratch-dir/*.tfrecord ${DATA_BUCKET}/coco &&\n            gsutil cp /scratch-dir/raw-data/annotations/*.json ${DATA_BUCKET}/coco''',\n      arguments=[\n        '--project', project,\n        '--mode', 'cloud',\n        '--bucket', bucket,\n        '--start_year', startYear\n      ],\n      file_outputs={'bucket': '/output.txt'}\n    )\n    #download_and_preprocess.set_memory_request('2G')\n    download_and_preprocess.add_env_variable([])\n  else:\n    preprocess = ObjectDict({\n      'outputs': {\n        'bucket': bucket\n      }\n    })\n  # Step 2: Do hyperparameter tuning of the model on Cloud ML Engine\n  if start_step <= 2:\n    hparam_train = dsl.ContainerOp(\n      name='hypertrain',\n      # image needs to be a compile-time string\n      image='gcr.io/cloud-training-demos/babyweight-pipeline-hypertrain:latest',\n      arguments=[\n        preprocess.outputs['bucket']\n      ],\n      file_outputs={'jobname': '/output.txt'}\n    )\n  else:\n    hparam_train = ObjectDict({\n      'outputs': {\n        'jobname': 'babyweight_181008_210829'\n      }\n    })\n\n  # Step 3: Train the model some more, but on the pipelines cluster itself\n  if start_step <= 3:\n    train_tuned = dsl.ContainerOp(\n      name='traintuned',\n      # image needs to be a compile-time string\n      image='gcr.io/cloud-training-demos/babyweight-pipeline-traintuned-trainer:latest',\n      #image='gcr.io/cloud-training-demos/babyweight-pipeline-traintuned-trainer@sha256:3d73c805430a16d0675aeafa9819d6d2cfbad0f0f34cff5fb9ed4e24493bc9a8',\n      arguments=[\n        hparam_train.outputs['jobname'],\n        bucket\n      ],\n      file_outputs={'train': '/output.txt'}\n    )\n    train_tuned.set_memory_request('2G')\n    train_tuned.set_cpu_request('1')\n  else:\n    train_tuned = ObjectDict({\n        'outputs': {\n          'train': 'gs://cloud-training-demos-ml/babyweight/hyperparam/15'\n        }\n    })\n\n  # Step 4: Deploy the trained model to Cloud ML Engine\n  if start_step <= 4:\n    deploy_cmle = dsl.ContainerOp(\n      name='deploycmle',\n      # image needs to be a compile-time string\n      image='gcr.io/cloud-training-demos/babyweight-pipeline-deploycmle:latest',\n      arguments=[\n        train_tuned.outputs['train'],  # modeldir\n        'babyweight',\n        'mlp'\n      ],\n      file_outputs={\n        'model': '/model.txt',\n        'version': '/version.txt'\n      }\n    )\n  else:\n    deploy_cmle = ObjectDict({\n      'outputs': {\n        'model': 'babyweight',\n        'version': 'mlp'\n      }\n    })\n\n  # Step 5: Deploy the trained model to AppEngine\n  if start_step <= 5:\n    deploy_cmle = dsl.ContainerOp(\n      name='deployapp',\n      # image needs to be a compile-time string\n      image='gcr.io/cloud-training-demos/babyweight-pipeline-deployapp:latest',\n      arguments=[\n        deploy_cmle.outputs['model'],\n        deploy_cmle.outputs['version']\n      ],\n      file_outputs={\n        'appurl': '/appurl.txt'\n      }\n    )\n  else:\n    deploy_cmle = ObjectDict({\n      'outputs': {\n        'appurl': 'https://cloud-training-demos.appspot.com/'\n      }\n    })\n\"\"\"\n\nif __name__ == '__main__':\n  import kfp.compiler as compiler\n\n  compiler.Compiler().compile(train_and_deploy, __file__ + '.tgz')\n"
  },
  {
    "repo": "dhodun/kubeflow-pipeline-examples",
    "file_path": "mask_rcnn/mask_trt_example.py",
    "raw_url": "https://raw.githubusercontent.com/dhodun/kubeflow-pipeline-examples/master/mask_rcnn/mask_trt_example.py",
    "content": "#!/usr/bin/env python3\n# Copyright 2018 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport kfp.dsl as dsl\n\nclass ObjectDict(dict):\n  def __getattr__(self, name):\n    if name in self:\n      return self[name]\n    else:\n      raise AttributeError(\"No such attribute: \" + name)\n\nfrom kfp.gcp import use_tpu\n\n@dsl.pipeline(\n  name='mask_rcnn_trt_full',\n  description='Preprocess COCO and train Mask RCNN Model'\n)\ndef train_and_deploy(\n    project=dsl.PipelineParam(name='project', value='dhodun1'),\n    bucket=dsl.PipelineParam(name='bucket', value='gs://dhodun1-central1'),\n    startYear=dsl.PipelineParam(name='startYear', value='2000')\n):\n  \"\"\"Pipeline to train Mask RCNN\"\"\"\n\n  reprocess_coco = dsl.ContainerOp(\n    name='preprocess_coco',\n    # image needs to be compile-time string\n    image='gcr.io/dhodun1/preprocess-coco:latest',\n    arguments=[\n      bucket,\n    ],\n    file_outputs={'bucket': '/output.txt'}\n  )\n\n  if start_step <= 1:\n    preprocess_coco = dsl.ContainerOp(\n      name='preprocess_coco',\n      # image needs to be compile-time string\n      image='gcr.io/dhodun1/preprocess-coco:latest',\n      arguments=[\n        bucket,\n      ],\n      file_outputs={'bucket': '/output.txt'}\n    )\n    preprocess_coco.set_cpu_request('8')\n    preprocess_coco.set_memory_request('30G')\n  else:\n    preprocess_coco = ObjectDict({\n      'outputs': {\n        'bucket': bucket\n      }\n    })\n\n  if start_step <=2:\n    train_mask_rcnn = dsl.ContainerOp(\n      name='train_mask_rcnn_tpu',\n      # image needs to be a compile-time string\n      image='gcr.io/dhodun1/train-mask-rcnn',\n      arguments=[\n        bucket,\n      ],\n      #file_outputs={'results': '/output.txt'}\n    )\n    train_mask_rcnn.apply(use_tpu(tpu_cores=8, tpu_resource='v3', tf_version='1.12'))\n    train_mask_rcnn.set_cpu_request('8')\n    train_mask_rcnn.set_memory_request('30G')\n\"\"\"\n  if start_step <= 2:\n    download_and_preprocess = dsl.ContainerOp(\n      name='download_and_preprocess',\n      # image needs to be a compile-time string\n      image='gcr.io/dhodun1/tpu-test',\n      arguments=[\n        '--project', project,\n        '--mode', 'cloud',\n        '--bucket', bucket,\n        '--start_year', startYear\n      ],\n      file_outputs={'result': '/output.txt'}\n    )\n  else:\n    preprocess = ObjectDict({\n      'outputs': {\n        'bucket': bucket\n      }\n    })\n  download_and_preprocess.apply(use_tpu(tpu_cores=8, tpu_resource='v2', tf_version='1.11'))\n  download_and_preprocess.set_gpu_limit(8, vendor='nvidia')\n\n\n  # Step 1: Download COCO dataset and transform to TFRecords\n  if start_step <= 1:\n    download_and_preprocess = dsl.ContainerOp(\n      name='download_and_preprocess',\n      # image needs to be a compile-time string\n      image='gcr.io/tensorflow/tpu-models:r1.11',\n      command = '''          - /bin/bash\n          - -c\n          - >\n            cd /tensorflow_tpu_models/tools/datasets &&\n            bash download_and_preprocess_coco.sh /scratch-dir &&\n            gsutil -m cp /scratch-dir/*.tfrecord ${DATA_BUCKET}/coco &&\n            gsutil cp /scratch-dir/raw-data/annotations/*.json ${DATA_BUCKET}/coco''',\n      arguments=[\n        '--project', project,\n        '--mode', 'cloud',\n        '--bucket', bucket,\n        '--start_year', startYear\n      ],\n      file_outputs={'bucket': '/output.txt'}\n    )\n    #download_and_preprocess.set_memory_request('2G')\n    download_and_preprocess.add_env_variable([])\n  else:\n    preprocess = ObjectDict({\n      'outputs': {\n        'bucket': bucket\n      }\n    })\n  # Step 2: Do hyperparameter tuning of the model on Cloud ML Engine\n  if start_step <= 2:\n    hparam_train = dsl.ContainerOp(\n      name='hypertrain',\n      # image needs to be a compile-time string\n      image='gcr.io/cloud-training-demos/babyweight-pipeline-hypertrain:latest',\n      arguments=[\n        preprocess.outputs['bucket']\n      ],\n      file_outputs={'jobname': '/output.txt'}\n    )\n  else:\n    hparam_train = ObjectDict({\n      'outputs': {\n        'jobname': 'babyweight_181008_210829'\n      }\n    })\n\n  # Step 3: Train the model some more, but on the pipelines cluster itself\n  if start_step <= 3:\n    train_tuned = dsl.ContainerOp(\n      name='traintuned',\n      # image needs to be a compile-time string\n      image='gcr.io/cloud-training-demos/babyweight-pipeline-traintuned-trainer:latest',\n      #image='gcr.io/cloud-training-demos/babyweight-pipeline-traintuned-trainer@sha256:3d73c805430a16d0675aeafa9819d6d2cfbad0f0f34cff5fb9ed4e24493bc9a8',\n      arguments=[\n        hparam_train.outputs['jobname'],\n        bucket\n      ],\n      file_outputs={'train': '/output.txt'}\n    )\n    train_tuned.set_memory_request('2G')\n    train_tuned.set_cpu_request('1')\n  else:\n    train_tuned = ObjectDict({\n        'outputs': {\n          'train': 'gs://cloud-training-demos-ml/babyweight/hyperparam/15'\n        }\n    })\n\n  # Step 4: Deploy the trained model to Cloud ML Engine\n  if start_step <= 4:\n    deploy_cmle = dsl.ContainerOp(\n      name='deploycmle',\n      # image needs to be a compile-time string\n      image='gcr.io/cloud-training-demos/babyweight-pipeline-deploycmle:latest',\n      arguments=[\n        train_tuned.outputs['train'],  # modeldir\n        'babyweight',\n        'mlp'\n      ],\n      file_outputs={\n        'model': '/model.txt',\n        'version': '/version.txt'\n      }\n    )\n  else:\n    deploy_cmle = ObjectDict({\n      'outputs': {\n        'model': 'babyweight',\n        'version': 'mlp'\n      }\n    })\n\n  # Step 5: Deploy the trained model to AppEngine\n  if start_step <= 5:\n    deploy_cmle = dsl.ContainerOp(\n      name='deployapp',\n      # image needs to be a compile-time string\n      image='gcr.io/cloud-training-demos/babyweight-pipeline-deployapp:latest',\n      arguments=[\n        deploy_cmle.outputs['model'],\n        deploy_cmle.outputs['version']\n      ],\n      file_outputs={\n        'appurl': '/appurl.txt'\n      }\n    )\n  else:\n    deploy_cmle = ObjectDict({\n      'outputs': {\n        'appurl': 'https://cloud-training-demos.appspot.com/'\n      }\n    })\n\"\"\"\n\nif __name__ == '__main__':\n  import kfp.compiler as compiler\n\n  compiler._compile(train_and_deploy, __file__ + '.tgz')\n"
  },
  {
    "repo": "dhodun/kubeflow-pipeline-examples",
    "file_path": "mask_rcnn/mlp_babyweight.bak.py",
    "raw_url": "https://raw.githubusercontent.com/dhodun/kubeflow-pipeline-examples/master/mask_rcnn/mlp_babyweight.bak.py",
    "content": "#!/usr/bin/env python3\n# Copyright 2018 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport kfp.dsl as dsl\n\nclass ObjectDict(dict):\n  def __getattr__(self, name):\n    if name in self:\n      return self[name]\n    else:\n      raise AttributeError(\"No such attribute: \" + name)\n\n\n@dsl.pipeline(\n  name='babyweight',\n  description='Train Babyweight model'\n)\ndef train_and_deploy(\n    project=dsl.PipelineParam(name='project', value='cloud-training-demos'),\n    bucket=dsl.PipelineParam(name='bucket', value='cloud-training-demos-ml'),\n    startYear=dsl.PipelineParam(name='startYear', value='2000')\n):\n  \"\"\"Pipeline to train babyweight model\"\"\"\n  start_step = 1\n\n  # Step 1: create training dataset using Apache Beam on Cloud Dataflow\n  if start_step <= 1:\n    preprocess = dsl.ContainerOp(\n      name='preprocess',\n      # image needs to be a compile-time string\n      image='gcr.io/cloud-training-demos/babyweight-pipeline-bqtocsv:latest',\n      arguments=[\n        '--project', project,\n        '--mode', 'cloud',\n        '--bucket', bucket,\n        '--start_year', startYear\n      ],\n      file_outputs={'bucket': '/output.txt'}\n    )\n  else:\n    preprocess = ObjectDict({\n      'outputs': {\n        'bucket': bucket\n      }\n    })\n\n  # Step 2: Do hyperparameter tuning of the model on Cloud ML Engine\n  if start_step <= 2:\n    hparam_train = dsl.ContainerOp(\n      name='hypertrain',\n      # image needs to be a compile-time string\n      image='gcr.io/cloud-training-demos/babyweight-pipeline-hypertrain:latest',\n      arguments=[\n        preprocess.outputs['bucket']\n      ],\n      file_outputs={'jobname': '/output.txt'}\n    )\n  else:\n    hparam_train = ObjectDict({\n      'outputs': {\n        'jobname': 'babyweight_181008_210829'\n      }\n    })\n\n  # Step 3: Train the model some more, but on the pipelines cluster itself\n  if start_step <= 3:\n    train_tuned = dsl.ContainerOp(\n      name='traintuned',\n      # image needs to be a compile-time string\n      image='gcr.io/cloud-training-demos/babyweight-pipeline-traintuned-trainer:latest',\n      #image='gcr.io/cloud-training-demos/babyweight-pipeline-traintuned-trainer@sha256:3d73c805430a16d0675aeafa9819d6d2cfbad0f0f34cff5fb9ed4e24493bc9a8',\n      arguments=[\n        hparam_train.outputs['jobname'],\n        bucket\n      ],\n      file_outputs={'train': '/output.txt'}\n    )\n    train_tuned.set_memory_request('2G')\n    train_tuned.set_cpu_request('1')\n  else:\n    train_tuned = ObjectDict({\n        'outputs': {\n          'train': 'gs://cloud-training-demos-ml/babyweight/hyperparam/15'\n        }\n    })\n\n\n  # Step 4: Deploy the trained model to Cloud ML Engine\n  if start_step <= 4:\n    deploy_cmle = dsl.ContainerOp(\n      name='deploycmle',\n      # image needs to be a compile-time string\n      image='gcr.io/cloud-training-demos/babyweight-pipeline-deploycmle:latest',\n      arguments=[\n        train_tuned.outputs['train'],  # modeldir\n        'babyweight',\n        'mlp'\n      ],\n      file_outputs={\n        'model': '/model.txt',\n        'version': '/version.txt'\n      }\n    )\n  else:\n    deploy_cmle = ObjectDict({\n      'outputs': {\n        'model': 'babyweight',\n        'version': 'mlp'\n      }\n    })\n\n  # Step 5: Deploy the trained model to AppEngine\n  if start_step <= 5:\n    deploy_cmle = dsl.ContainerOp(\n      name='deployapp',\n      # image needs to be a compile-time string\n      image='gcr.io/cloud-training-demos/babyweight-pipeline-deployapp:latest',\n      arguments=[\n        deploy_cmle.outputs['model'],\n        deploy_cmle.outputs['version']\n      ],\n      file_outputs={\n        'appurl': '/appurl.txt'\n      }\n    )\n  else:\n    deploy_cmle = ObjectDict({\n      'outputs': {\n        'appurl': 'https://cloud-training-demos.appspot.com/'\n      }\n    })\n\n\nif __name__ == '__main__':\n  import kfp.compiler as compiler\n\n  compiler._compile(train_and_deploy, __file__ + '.tgz')\n"
  },
  {
    "repo": "dhodun/kubeflow-pipeline-examples",
    "file_path": "mask_rcnn/test_client.py",
    "raw_url": "https://raw.githubusercontent.com/dhodun/kubeflow-pipeline-examples/master/mask_rcnn/test_client.py",
    "content": "import kfp.components as comp\n\nEXPERIMENT_NAME='lightweight python components'\n\n#Define a Python function\ndef add(a: float, b: float) -> float:\n   '''Calculates sum of two arguments'''\n   return a + b\n\nadd_op = comp.func_to_container_op(add)\n\n# Advanced function\n# Demonstrates imports, helper functions and multiple outputs\nfrom typing import NamedTuple\n\n\ndef my_divmod(dividend: float, divisor: float, output_dir: str = './') -> NamedTuple('MyDivmodOutput',\n                                                                                     [('quotient', float),\n                                                                                      ('remainder', float)]):\n    '''Divides two numbers and calculate  the quotient and remainder'''\n    # Imports inside a component function:\n    import numpy as np\n\n    # This function demonstrates how to use nested functions inside a component function:\n    def divmod_helper(dividend, divisor):\n        return np.divmod(dividend, divisor)\n\n    (quotient, remainder) = divmod_helper(dividend, divisor)\n\n    from tensorflow.python.lib.io import file_io\n    import json\n\n    # Exports a sample tensorboard:\n    metadata = {\n        'outputs': [{\n            'type': 'tensorboard',\n            'source': 'gs://ml-pipeline-dataset/tensorboard-train',\n        }]\n    }\n    with open(output_dir + 'mlpipeline-ui-metadata.json', 'w') as f:\n        json.dump(metadata, f)\n\n    # Exports two sample metrics:\n    metrics = {\n        'metrics': [{\n            'name': 'quotient',\n            'numberValue': float(quotient),\n        }, {\n            'name': 'remainder',\n            'numberValue': float(remainder),\n        }]}\n\n    with file_io.FileIO(output_dir + 'mlpipeline-metrics.json', 'w') as f:\n        json.dump(metrics, f)\n\n    from collections import namedtuple\n    divmod_output = namedtuple('MyDivmodOutput', ['quotient', 'remainder'])\n    return divmod_output(quotient, remainder)\n\nmy_divmod(100, 7)\n\ndivmod_op = comp.func_to_container_op(my_divmod, base_image='tensorflow/tensorflow:1.11.0-py3')\n\nimport kfp.dsl as dsl\n\n\n@dsl.pipeline(\n    name='Calculation pipeline',\n    description='A toy pipeline that performs arithmetic calculations.'\n)\ndef calc_pipeline(\n        a='a',\n        b='7',\n        c='17',\n):\n    # Passing pipeline parameter and a constant value as operation arguments\n    add_task = add_op(a, 4)  # Returns a dsl.ContainerOp class instance.\n\n    # Passing a task output reference as operation arguments\n    # For an operation with a single return value, the output reference can be accessed using `task.output` or `task.outputs['output_name']` syntax\n    divmod_task = divmod_op(add_task.output, b, '/')\n\n    # For an operation with a multiple return values, the output references can be accessed using `task.outputs['output_name']` syntax\n    result_task = add_op(divmod_task.outputs['quotient'], c)\n\n\npipeline_func = calc_pipeline\npipeline_filename = pipeline_func.__name__ + '.pipeline.zip'\nimport kfp.compiler as compiler\ncompiler.Compiler().compile(pipeline_func, pipeline_filename)\n\n\n#Specify pipeline argument values\narguments = {'a': '7', 'b': '8'}\n\n#Get or create an experiment and submit a pipeline run\nimport kfp\nclient = kfp.Client('127.0.0.1:8085/pipeline')\nexperiment = client.create_experiment(EXPERIMENT_NAME)\n\n#Submit a pipeline run\nrun_name = pipeline_func.__name__ + ' run'\nrun_result = client.run_pipeline(experiment.id, run_name, pipeline_filename, arguments)\n\n#vvvvvvvvv This link leads to the run information page. (Note: There is a bug in JupyterLab that modifies the URL and makes the link stop working)"
  },
  {
    "repo": "ventus550/kubeflow-pipelines-project",
    "file_path": "pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/ventus550/kubeflow-pipelines-project/master/pipeline.py",
    "content": "# +\nfrom google.cloud import aiplatform\nfrom kfp.registry import RegistryClient\nfrom kfp import compiler, dsl\nfrom kfp.dsl import (\n    Dataset,\n    Input,\n    Model,\n    If,\n    Else,\n)\n\nimport google_cloud_pipeline_components.v1.custom_job.utils as custom_job\nimport components\nfrom src.secrets import configs\n\n# -\n\n# ## Static Configuration\n\nTENSORBOARD_ID = str(configs.tensorboard)\nTRAIN_LOCATION = configs.location\nACCELERATOR_TYPE = \"NVIDIA_TESLA_T4\"\n\n# ## Pipeline definition\n\n# +\ntensorboard = aiplatform.Tensorboard(\n    location=TRAIN_LOCATION, project=configs.project, tensorboard_name=TENSORBOARD_ID\n)\n\ntraining_config = dict(\n    component_spec=components.train_model,\n    display_name=configs.model,\n    tensorboard=tensorboard.resource_name,\n    base_output_directory=configs.pipeline_directory,\n    service_account=configs.service_account,\n)\n\ngpu = dict(accelerator_type=ACCELERATOR_TYPE)\n\n\n@dsl.pipeline(\n    pipeline_root=configs.pipeline_directory,\n    name=configs.model,\n)\ndef pipeline(\n    dataset: str = f\"{configs.data_directory}/words.npz\",\n    epochs: int = 10,\n    upload: bool = False,\n    upload_threshold: float = 0.0,\n    accelerator: bool = False,\n    pretrained_model: Input[Model] = None,\n):\n    importer = dsl.importer(\n        artifact_uri=dataset,\n        artifact_class=Dataset,\n        reimport=False,\n    )\n\n    data = components.split_data(ratio=0.1, dataset=importer.output)\n    train = data.outputs[\"train\"]\n    test = data.outputs[\"test\"]\n\n    train_model_gpu_op = custom_job.create_custom_training_job_op_from_component(\n        **(training_config | gpu)\n    )\n    train_model_cpu_op = custom_job.create_custom_training_job_op_from_component(\n        **training_config\n    )\n\n    def branch(train_model_op):\n        # replace with OneOf once https://github.com/kubeflow/pipelines/issues/10271 is resolved\n        model = train_model_op.outputs[\"trained_model\"]\n        components.shap_explainer(rows=4, cols=5, dataset=test, model=model)\n        metric = components.metrics(dataset=test, model=model).outputs[\"Output\"]\n        with If(upload and metric > upload_threshold, name=\"metric > threshold\"):\n            components.upload_model(model=model)\n\n    with If(accelerator == True, name=\"gpu\"):\n        train_model_gpu = train_model_gpu_op(\n            epochs=epochs, dataset=train, location=TRAIN_LOCATION\n        )\n        branch(train_model_gpu)\n    with Else(name=\"cpu\"):\n        train_model_cpu = train_model_cpu_op(\n            epochs=epochs, dataset=train, location=TRAIN_LOCATION\n        )\n        branch(train_model_cpu)\n\n\n# -\n\n\n# ## Compilation and upload\n\nclient = RegistryClient(host=configs.artifactory)\ncompiler.Compiler().compile(pipeline_func=pipeline, package_path=configs.pipeline_name)\nclient.upload_pipeline(file_name=configs.pipeline_name, tags=[\"latest\"])\n"
  },
  {
    "repo": "ventus550/kubeflow-pipelines-project",
    "file_path": "components/_metrics.py",
    "raw_url": "https://raw.githubusercontent.com/ventus550/kubeflow-pipelines-project/master/components/_metrics.py",
    "content": "from kfp.dsl import component, Dataset, Input, Output, Model, Markdown\nfrom src.secrets import configs\n\n\n@component(base_image=configs.keras_image, packages_to_install=[\"seaborn\"])\ndef metrics(\n    dataset: Input[Dataset],\n    model: Input[Model],\n    edit_distance_histogram: Output[Markdown],\n    predictions: Output[Markdown],\n) -> int:\n    from src import aitoolkit\n    from src.utils import capture_image\n    import seaborn\n    import numpy\n\n    X, Y = numpy.load(dataset.path + \".npz\").values()\n    X, Y = aitoolkit.format_data(X, Y, aitoolkit.characters)\n    model = aitoolkit.load(model.path)\n    batch = aitoolkit.batch_prediction(model, X, Y)\n\n    edit_distance_values = numpy.array([sample.value for sample in batch])\n    seaborn.histplot(edit_distance_values, bins=10, kde=True, alpha=0.6)\n    open(edit_distance_histogram.path, \"w\").write(f\"![Image]({capture_image()})\")\n\n    batch.show()\n    open(predictions.path, \"w\").write(f\"![Image]({capture_image()})\")\n\n    average_edit_distance = numpy.mean(edit_distance_values)\n    print(\"Average edit distance:\", average_edit_distance)\n    return int(average_edit_distance)\n"
  },
  {
    "repo": "ventus550/kubeflow-pipelines-project",
    "file_path": "components/_shap_explainer.py",
    "raw_url": "https://raw.githubusercontent.com/ventus550/kubeflow-pipelines-project/master/components/_shap_explainer.py",
    "content": "from kfp.dsl import component, Dataset, Input, Output, Model, Markdown\nfrom src.secrets import configs\n\n\n@component(base_image=configs.keras_image, packages_to_install=[\"shap==0.44\"])\ndef shap_explainer(\n    rows: int,\n    cols: int,\n    dataset: Input[Dataset],\n    model: Input[Model],\n    explanation: Output[Markdown],\n):\n    import cv2\n    import shap\n    import numpy\n    import keras\n    import tensorflow as tf\n\n    from src import aitoolkit\n    from src.utils import capture_image\n\n    characters = aitoolkit.characters\n    X, Y = numpy.load(dataset.path + \".npz\").values()\n    X, Y = aitoolkit.format_data(X, Y, characters)\n    model = aitoolkit.load(model.path)\n\n    # repurpose model for \"classification\"\n    classifier = keras.models.Model(\n        model.input, tf.reduce_max(model.output, axis=1)[:, : len(characters)]\n    )\n\n    masker = shap.maskers.Image(\"inpaint_telea\", X[0].shape)\n    explainer = shap.Explainer(classifier, masker, output_names=list(characters))\n    indices = numpy.random.randint(len(X), size=rows)\n    shap_values = explainer(\n        X[indices],\n        max_evals=100,\n        batch_size=1000,\n        outputs=shap.Explanation.argsort.flip[:cols],\n    )\n\n    labels = shap_values.output_names\n    if len(numpy.shape(shap_values.output_names)) == 1:\n        # shap is an awful library and thus requires awful fixes\n        labels = numpy.reshape(labels * rows, (rows, cols))\n\n    shap.image_plot(shap_values, labels=labels)\n    open(explanation.path, \"w\").write(f\"![Image]({capture_image()})\")\n"
  },
  {
    "repo": "ventus550/kubeflow-pipelines-project",
    "file_path": "components/_split_data.py",
    "raw_url": "https://raw.githubusercontent.com/ventus550/kubeflow-pipelines-project/master/components/_split_data.py",
    "content": "from kfp.dsl import component, Dataset, Input, Output\nfrom src.secrets import configs\n\n\n@component(base_image=configs.keras_image)\ndef split_data(\n    ratio: float, dataset: Input[Dataset], train: Output[Dataset], test: Output[Dataset]\n):\n    import numpy\n\n    X, Y = numpy.load(dataset.path).values()\n    test_size = int(ratio * len(X))\n\n    train_set = (X[test_size:], Y[test_size:])\n    test_set = (X[:test_size], Y[:test_size])\n\n    numpy.savez(train.path, *train_set)\n    numpy.savez(test.path, *test_set)\n"
  },
  {
    "repo": "ventus550/kubeflow-pipelines-project",
    "file_path": "components/_train_model.py",
    "raw_url": "https://raw.githubusercontent.com/ventus550/kubeflow-pipelines-project/master/components/_train_model.py",
    "content": "from kfp.dsl import component, Dataset, Input, Output, Model\nfrom src.secrets import configs\n\n\n@component(\n    base_image=configs.keras_image,\n    packages_to_install=[\"google-cloud-secret-manager>=2.17.0\"],\n)\ndef train_model(epochs: int, dataset: Input[Dataset], trained_model: Output[Model]):\n    import numpy\n    import keras\n    import os\n\n    from keras.layers import (\n        Conv2D,\n        BatchNormalization,\n        MaxPooling2D,\n        Dropout,\n        Flatten,\n        Dense,\n        Activation,\n        Permute,\n        Reshape,\n        Bidirectional,\n        LSTM,\n    )\n    from keras import callbacks\n    import tensorflow as tf\n    from contextlib import suppress\n    from src import aitoolkit\n    from src.secrets import configs\n\n    image_width = 128\n    image_height = 32\n\n    X, Y = numpy.load(dataset.path + \".npz\").values()\n    X, Y = aitoolkit.format_data(X, Y, aitoolkit.characters)\n    X_test, Y_test = X[:1000], Y[:1000]\n    X_train, Y_train = X[1000:], Y[1000:]\n\n    tensorboard = callbacks.TensorBoard(\n        # vertex-provided directory for logs\n        log_dir=os.environ[\"AIP_TENSORBOARD_LOG_DIR\"],\n        histogram_freq=1,\n    )\n    checkpoints = callbacks.ModelCheckpoint(\n        monitor=\"edit_distance\",\n        filepath=\"./weights.h5\",\n        save_weights_only=True,\n        save_best_only=True,\n        mode=\"min\",\n    )\n    plateau = callbacks.ReduceLROnPlateau(\n        monitor=\"edit_distance\", patience=10, verbose=1, factor=0.90\n    )\n\n    namespace = tf.Graph()\n\n    def Convolutions(filters, kernel=5):\n        return keras.models.Sequential(\n            [\n                Conv2D(filters, kernel, padding=\"same\", kernel_initializer=\"he_normal\"),\n                BatchNormalization(),\n                Activation(\"relu\"),\n            ],\n            name=namespace.unique_name(\"convolutions\"),\n        )\n\n    def build_model():\n        input = keras.Input(shape=(image_height, image_width, 1), name=\"images\")\n\n        x = Convolutions(128, 3)(input)\n        x = MaxPooling2D((2, 2))(x)\n\n        x = Convolutions(64, 3)(x)\n        x = MaxPooling2D((2, 2))(x)\n\n        x = Permute([2, 1, 3])(x)\n        x = Reshape([image_width // 4, -1])(x)\n\n        x = Dense(64, activation=\"relu\")(x)\n        x = Dropout(0.2)(x)\n\n        x = Bidirectional(LSTM(128, return_sequences=True, dropout=0.25))(x)\n        x = Bidirectional(LSTM(64, return_sequences=True, dropout=0.25))(x)\n\n        # Reserve two extra tokens for the ctc_loss\n        x = Dense(\n            len(aitoolkit.characters) + 2, activation=\"softmax\", name=\"predictions\"\n        )(x)\n\n        model = keras.models.Model(inputs=input, outputs=x, name=configs.model)\n        model.compile(\n            optimizer=\"adam\", loss=aitoolkit.ctc_loss, metrics=[aitoolkit.edit_distance]\n        )\n        return model\n\n    model = build_model()\n    model.summary()\n\n    model.fit(\n        X_train,\n        Y_train,\n        validation_split=0.1,\n        epochs=epochs,\n        callbacks=[tensorboard, checkpoints, plateau],\n    )\n\n    # load from last checkpoint\n    with suppress(FileNotFoundError):\n        model.load_weights(\"weights.h5\")\n\n    meta = {\n        \"epochs\": epochs,\n        \"characters\": \"\".join(aitoolkit.characters),\n        \"padded\": -1,\n        \"parameters\": model.count_params(),\n    }\n\n    aitoolkit.save(model, trained_model.path, metadata=meta)\n"
  },
  {
    "repo": "ventus550/kubeflow-pipelines-project",
    "file_path": "components/_upload_model.py",
    "raw_url": "https://raw.githubusercontent.com/ventus550/kubeflow-pipelines-project/master/components/_upload_model.py",
    "content": "from kfp.dsl import component, Dataset, Input, Output, Model, Markdown\nfrom src.secrets import configs\n\n\n@component(\n    base_image=configs.keras_image,\n    packages_to_install=[\"google-cloud-secret-manager>=2.17.0\"],\n)\ndef upload_model(\n    model: Input[Model],\n    container_image: str = \"europe-docker.pkg.dev/vertex-ai-restricted/prediction/tf_opt-cpu.2-15:latest\",\n):\n    import keras\n    from google.cloud import aiplatform\n    from src.secrets import configs\n    from src import aitoolkit\n\n    path = model.path\n    model = aitoolkit.load(path)\n    model = keras.Model(model.input, model.output)\n    model.save(path, save_format=\"tf\")\n\n    print(\"uploading to model registry\")\n    model = aiplatform.Model.upload(\n        display_name=configs.model,\n        artifact_uri=path,\n        serving_container_image_uri=container_image,\n        location=configs.location,\n        project=configs.project,\n        staging_bucket=configs.bucket,\n    )\n"
  },
  {
    "repo": "Michallote/kubeflow-from-pipeline",
    "file_path": "generated_functions.py",
    "raw_url": "https://raw.githubusercontent.com/Michallote/kubeflow-from-pipeline/main/generated_functions.py",
    "content": "\nfrom typing import Any, NamedTuple\nimport kfp.dsl as dsl\nfrom collections import namedtuple\nfrom kfp.dsl import Input, Output, Dataset, component\nfrom consts import MLFLOW_IMAGE\n\n@component(base_image=MLFLOW_IMAGE)\ndef hello_project_raw_catalog_segmentation_tyrany(a_dataset: Input[Dataset], b_dataset: Input[Dataset], catalog_dataset: Output[Dataset], segmentation_dataset: Output[Dataset], tyrany_dataset: Output[Dataset]) -> None:\n    # Imports\n    from kubeflow_from_pipeline.databases import gs_read_auto\n    from kubeflow_from_pipeline.databases import gs_store_auto\n    from hello import hello_project\n    # Body\n    # Inputs Loading\n    a = gs_read_auto(a_dataset.path)\n    b = gs_read_auto(b_dataset.path)\n    # Function Call\n    catalog, segmentation, tyrany = hello_project(a, b, sql_file='data.sql', n_iqr=1.5)\n    # Outputs Packing\n    gs_store_auto(catalog, catalog_dataset.path)\n    gs_store_auto(segmentation, segmentation_dataset.path)\n    gs_store_auto(tyrany, tyrany_dataset.path)"
  },
  {
    "repo": "Michallote/kubeflow-from-pipeline",
    "file_path": "main_legacy.py",
    "raw_url": "https://raw.githubusercontent.com/Michallote/kubeflow-from-pipeline/main/main_legacy.py",
    "content": "from collections import namedtuple\nfrom typing import Any, Callable, Iterable, NamedTuple, Optional\n\nfrom jinja2 import Template\n\nfrom hello import hello_project\nfrom kubeflow_from_pipeline.databases import gs_read_auto, load_gs_data\n\nGLOBAL_IMPORTS = \"\"\"\nfrom typing import Any, NamedTuple\nimport kfp.dsl as dsl\nfrom collections import namedtuple\nfrom kfp.dsl import Input, Output, Dataset, component\nfrom consts import MLFLOW_IMAGE\n\"\"\"\n\n\nTUPLE_FUNCTION_TEMPLATE = Template(\n    \"\"\"\n{{ decorator }}\ndef {{ function_name }}({{ parameters|join(', ') }}) -> {{ output_type_hint }}:\n    # Imports\n    {{ imports|join('\\n')|indent(4) }}\n    # Body\n    {{ body|indent(4) }}\n\n\"\"\"\n)\n\nDECLARATIVE_FUNCTION_TEMPLATE = Template(\n    \"\"\"\n{{ decorator }}\ndef {{ function_name }}({{ parameters|join(', ') }}):\n    # Imports\n    {{ imports|join('\\n')|indent(4) }}\n    # Body\n    {{ body|indent(4) }}\n\"\"\"\n)\n\n\ndef make_input(params: Iterable) -> list[str]:\n    return list(map(lambda x: f\"{x}: Input[Dataset]\", params))\n\n\ndef create_function_code(\n    func_name: str,\n    params: list[str],\n    body: str,\n    imports: list[str],\n    output_type_hint: str = \"Any\",\n    decorator: Optional[str] = None,\n) -> str:\n\n    if decorator is None:\n        decorator = \"\"\n\n    code = TUPLE_FUNCTION_TEMPLATE.render(\n        function_name=func_name,\n        parameters=params,\n        output_type_hint=output_type_hint,\n        imports=imports,\n        body=body,\n        decorator=decorator,\n    )\n\n    return code\n\n\ndef create_import_statement(function: Callable) -> str:\n    return f\"from {function.__module__} import {function.__name__}\"\n\n\ndef create_function_call(\n    function_name: str, args: list[str], kwargs: dict[str, Any], outputs: list[str]\n) -> str:\n\n    function_arguments = args + [f\"{k}={repr(v)}\" for k, v in kwargs.items()]\n\n    if outputs:\n        function_outputs = \", \".join(outputs) + \" = \"\n    else:\n        function_outputs = \"\"\n\n    return f\"{function_outputs}{function_name}({', '.join(function_arguments)})\"\n\n\ndef create_return_statement(outputs: list[str]) -> tuple[str, str]:\n\n    type_statement = \", \".join(map(lambda x: f\"{x}=dsl.Dataset\", outputs))\n\n    output_asignment = \", \".join(map(lambda x: f\"{x}={x}\", outputs))\n\n    output_statement = (\n        f'outputs = NamedTuple(\"outputs\", {type_statement})\\n'\n        f\"return outputs({output_asignment})\"\n    )\n    output_type_hint = f'NamedTuple(\"outputs\", {type_statement})'\n\n    return output_statement, output_type_hint\n\n\ndef create_function_from_step(\n    name,\n    function: Callable,\n    inputs: Optional[list[str]] = None,\n    outputs: Optional[list[str]] = None,\n    kwargs: Optional[dict] = None,\n    decorator: Optional[str] = None,\n) -> str:\n\n    if inputs is None:\n        inputs = []\n\n    if outputs is None:\n        outputs = []\n\n    if kwargs is None:\n        kwargs = {}\n\n    component_name = \"_\".join(([function.__name__, name] + outputs))\n\n    params = list(map(lambda x: f\"{x}_dataset\", inputs))\n\n    imports = []\n\n    if inputs:\n        imports.append(create_import_statement(gs_read_auto))\n\n    imports.append(create_import_statement(function))\n\n    loading_lines = [\n        f\"{inpt} = gs_read_auto({param}.path)\" for param, inpt in zip(params, inputs)\n    ]\n\n    loading_statement = \"\\n\".join(loading_lines)\n\n    function_call = create_function_call(\n        function_name=function.__name__, args=inputs, kwargs=kwargs, outputs=outputs\n    )\n\n    output_statement, output_type_hint = create_return_statement(outputs=outputs)\n\n    body = \"\\n\".join(\n        [\n            \"# Inputs Loading\",\n            loading_statement,\n            \"# Function Call\",\n            function_call,\n            \"# Outputs Packing\",\n            output_statement,\n        ]\n    )\n\n    component_code = create_function_code(\n        component_name,\n        make_input(params),\n        body,\n        imports,\n        output_type_hint,\n        decorator=\"@component(base_image=MLFLOW_IMAGE)\",\n    )\n\n    return component_code\n\n\nstep = {\n    \"name\": \"raw\",\n    \"function\": hello_project,\n    \"inputs\": [\"a\", \"b\"],\n    \"outputs\": [\"catalog\"],\n    \"kwargs\": {\"sql_file\": \"data.sql\", \"n_iqr\": 1.5},\n}\n\nfunc_name = \"multiply_numbers\"\nparams = [\n    \"a\",\n    \"b\",\n]\nbody = \"\"\"\n\ncatalog_full = combine_catalog_sources(a, b)\n\noutputs = namedtuple(\"outputs\", catalog_full=dsl.Dataset)\nreturn outputs(catalog_full=catalog_full)\"\"\"\nimports = [\n    \"from src.databases.load_gs_data import gs_read_df\",\n    \"from src.databases.data_collection import combine_catalog_sources\",\n]\noutput_type_hint = 'NamedTuple(\"outputs\", catalog_full=dsl.Dataset)'\n\ndecorator = \"@component(base_image=MLFLOW_IMAGE)\"\n\ncode = GLOBAL_IMPORTS\ncode += create_function_code(func_name, params, body, imports, output_type_hint)\ncode += create_function_code(\n    func_name, make_input(params), body, imports, decorator=decorator\n)\n\ncode += create_function_from_step(**step)\n\nwith open(\"generated_functions.py\", \"w\") as f:\n    f.write(code)\n"
  },
  {
    "repo": "Michallote/kubeflow-from-pipeline",
    "file_path": "kubeflow_from_pipeline/kfp_generator_declarative.py",
    "raw_url": "https://raw.githubusercontent.com/Michallote/kubeflow-from-pipeline/main/kubeflow_from_pipeline/kfp_generator_declarative.py",
    "content": "from collections import namedtuple\nfrom typing import Any, Callable, Iterable, NamedTuple, Optional\n\nfrom jinja2 import Template\n\nfrom hello import hello_project\nfrom kubeflow_from_pipeline.databases import gs_read_auto, gs_store_auto, load_gs_data\n\nGLOBAL_IMPORTS = \"\"\"\nfrom typing import Any, NamedTuple\nimport kfp.dsl as dsl\nfrom collections import namedtuple\nfrom kfp.dsl import Input, Output, Dataset, component\nfrom consts import MLFLOW_IMAGE\n\"\"\"\n\nDECLARATIVE_FUNCTION_TEMPLATE = Template(\n    \"\"\"\n{{ decorator }}\ndef {{ function_name }}({{ parameters|join(', ') }}) -> {{ output_type_hint }}:\n    # Imports\n    {{ imports|join('\\n')|indent(4) }}\n    # Body\n    {{ body|indent(4) }}\n\"\"\"\n)\n\n\ndef make_input(params: Iterable) -> list[str]:\n    return list(map(lambda x: f\"{x}: Input[Dataset]\", params))\n\n\ndef make_output(params: Iterable) -> list[str]:\n    return list(map(lambda x: f\"{x}: Output[Dataset]\", params))\n\n\ndef create_function_code(\n    func_name: str,\n    params: list[str],\n    body: str,\n    imports: list[str],\n    output_type_hint: str = \"Any\",\n    decorator: Optional[str] = None,\n) -> str:\n\n    if decorator is None:\n        decorator = \"\"\n\n    code = DECLARATIVE_FUNCTION_TEMPLATE.render(\n        function_name=func_name,\n        parameters=params,\n        output_type_hint=output_type_hint,\n        imports=imports,\n        body=body,\n        decorator=decorator,\n    )\n\n    return code\n\n\ndef create_import_statement(function: Callable) -> str:\n    return f\"from {function.__module__} import {function.__name__}\"\n\n\ndef create_function_call(\n    function_name: str, args: list[str], kwargs: dict[str, Any], outputs: list[str]\n) -> str:\n\n    function_arguments = args + [f\"{k}={repr(v)}\" for k, v in kwargs.items()]\n\n    if outputs:\n        function_outputs = \", \".join(outputs) + \" = \"\n    else:\n        function_outputs = \"\"\n\n    return f\"{function_outputs}{function_name}({', '.join(function_arguments)})\"\n\n\ndef create_function_from_step(\n    name,\n    function: Callable,\n    inputs: Optional[list[str]] = None,\n    outputs: Optional[list[str]] = None,\n    kwargs: Optional[dict] = None,\n    decorator: Optional[str] = None,\n) -> str:\n\n    if inputs is None:\n        inputs = []\n\n    if outputs is None:\n        outputs = []\n\n    if kwargs is None:\n        kwargs = {}\n\n    component_name = \"_\".join(([function.__name__, name] + outputs))\n\n    imports = []\n\n    if inputs:\n        imports.append(create_import_statement(gs_read_auto))\n\n    if outputs:\n        imports.append(create_import_statement(gs_store_auto))\n\n    imports.append(create_import_statement(function))\n\n    input_params = list(map(lambda x: f\"{x}_dataset\", inputs))\n    loading_lines = [\n        f\"{inpt} = gs_read_auto({param}.path)\"\n        for param, inpt in zip(input_params, inputs)\n    ]\n\n    loading_statement = \"\\n\".join(loading_lines)\n\n    function_call = create_function_call(\n        function_name=function.__name__, args=inputs, kwargs=kwargs, outputs=outputs\n    )\n\n    output_params = list(map(lambda x: f\"{x}_dataset\", outputs))\n\n    output_lines = [\n        f\"gs_store_auto({outpt}, {param}.path)\"\n        for outpt, param in zip(outputs, output_params)\n    ]\n\n    output_statement = \"\\n\".join(output_lines)\n\n    body = \"\\n\".join(\n        [\n            \"# Inputs Loading\",\n            loading_statement,\n            \"# Function Call\",\n            function_call,\n            \"# Outputs Packing\",\n            output_statement,\n        ]\n    )\n\n    output_type_hint = \"None\"\n\n    component_params = make_input(input_params) + make_output(output_params)\n\n    component_code = create_function_code(\n        component_name,\n        component_params,\n        body,\n        imports,\n        output_type_hint,\n        decorator=decorator,\n    )\n\n    return component_code\n\n\nif __name__ == \"__main__\":\n\n    step = {\n        \"name\": \"raw\",\n        \"function\": hello_project,\n        \"inputs\": [\"a\", \"b\"],\n        \"outputs\": [\"catalog\"],\n        \"kwargs\": {\"sql_file\": \"data.sql\", \"n_iqr\": 1.5},\n    }\n\n    func_name = \"multiply_numbers\"\n    params = [\n        \"a\",\n        \"b\",\n    ]\n    body = \"\"\"\n\n    catalog_full = combine_catalog_sources(a, b)\n\n    outputs = namedtuple(\"outputs\", catalog_full=dsl.Dataset)\n    return outputs(catalog_full=catalog_full)\"\"\"\n    imports = [\n        \"from src.databases.load_gs_data import gs_read_df\",\n        \"from src.databases.data_collection import combine_catalog_sources\",\n    ]\n    output_type_hint = 'NamedTuple(\"outputs\", catalog_full=dsl.Dataset)'\n\n    decorator = \"@component(base_image=MLFLOW_IMAGE)\"\n\n    code = GLOBAL_IMPORTS\n    code += create_function_code(func_name, params, body, imports, output_type_hint)\n    code += create_function_code(\n        func_name, make_input(params), body, imports, decorator=decorator\n    )\n    name = step[\"name\"]\n    function: Callable = step[\"function\"]\n    inputs: list = step[\"inputs\"]\n    outputs: list = step[\"outputs\"]\n    kwargs: dict = step[\"kwargs\"]\n\n    code += create_function_from_step(\n        name,\n        function,\n        inputs,\n        outputs,\n        kwargs,\n        decorator=\"@component(base_image=MLFLOW_IMAGE)\",\n    )\n\n    with open(\"generated_functions.py\", \"w\") as f:\n        f.write(code)\n"
  },
  {
    "repo": "Michallote/kubeflow-from-pipeline",
    "file_path": "kubeflow_from_pipeline/kfp_generator_namedtuple.py",
    "raw_url": "https://raw.githubusercontent.com/Michallote/kubeflow-from-pipeline/main/kubeflow_from_pipeline/kfp_generator_namedtuple.py",
    "content": "from collections import namedtuple\nfrom typing import Any, Callable, Iterable, NamedTuple, Optional\n\nfrom jinja2 import Template\n\nfrom kubeflow_from_pipeline.databases import gs_read_auto, load_gs_data\n\nGLOBAL_IMPORTS = \"\"\"\nfrom typing import Any, NamedTuple\nimport kfp.dsl as dsl\nfrom collections import namedtuple\nfrom kfp.dsl import Input, Output, Dataset, component\nfrom consts import MLFLOW_IMAGE\n\"\"\"\n\n\nTUPLE_FUNCTION_TEMPLATE = Template(\n    \"\"\"\n{{ decorator }}\ndef {{ function_name }}({{ parameters|join(', ') }}) -> {{ output_type_hint }}:\n    # Imports\n    {{ imports|join('\\n')|indent(4) }}\n    # Body\n    {{ body|indent(4) }}\n\n\"\"\"\n)\n\nDECLARATIVE_FUNCTION_TEMPLATE = Template(\n    \"\"\"\n{{ decorator }}\ndef {{ function_name }}({{ parameters|join(', ') }}):\n    # Imports\n    {{ imports|join('\\n')|indent(4) }}\n    # Body\n    {{ body|indent(4) }}\n\"\"\"\n)\n\n\ndef make_input(params: Iterable) -> list[str]:\n    return list(map(lambda x: f\"{x}: Input[Dataset]\", params))\n\n\ndef create_function_code(\n    func_name: str,\n    params: list[str],\n    body: str,\n    imports: list[str],\n    output_type_hint: str = \"Any\",\n    decorator: Optional[str] = None,\n) -> str:\n\n    if decorator is None:\n        decorator = \"\"\n\n    code = TUPLE_FUNCTION_TEMPLATE.render(\n        function_name=func_name,\n        parameters=params,\n        output_type_hint=output_type_hint,\n        imports=imports,\n        body=body,\n        decorator=decorator,\n    )\n\n    return code\n\n\ndef create_import_statement(function: Callable) -> str:\n    return f\"from {function.__module__} import {function.__name__}\"\n\n\ndef create_function_call(\n    function_name: str, args: list[str], kwargs: dict[str, Any], outputs: list[str]\n) -> str:\n\n    function_arguments = args + [f\"{k}={repr(v)}\" for k, v in kwargs.items()]\n\n    if outputs:\n        function_outputs = \", \".join(outputs) + \" = \"\n    else:\n        function_outputs = \"\"\n\n    return f\"{function_outputs}{function_name}({', '.join(function_arguments)})\"\n\n\ndef create_return_statement(outputs: list[str]) -> tuple[str, str]:\n\n    type_statement = \", \".join(map(lambda x: f\"{x}=dsl.Dataset\", outputs))\n\n    output_asignment = \", \".join(map(lambda x: f\"{x}={x}\", outputs))\n\n    output_statement = (\n        f'outputs = NamedTuple(\"outputs\", {type_statement})\\n'\n        f\"return outputs({output_asignment})\"\n    )\n    output_type_hint = f'NamedTuple(\"outputs\", {type_statement})'\n\n    return output_statement, output_type_hint\n\n\ndef create_function_from_step(\n    name,\n    function: Callable,\n    inputs: Optional[list[str]] = None,\n    outputs: Optional[list[str]] = None,\n    kwargs: Optional[dict] = None,\n    decorator: Optional[str] = None,\n) -> str:\n\n    if inputs is None:\n        inputs = []\n\n    if outputs is None:\n        outputs = []\n\n    if kwargs is None:\n        kwargs = {}\n\n    component_name = \"_\".join(([function.__name__, name] + outputs))\n\n    params = list(map(lambda x: f\"{x}_dataset\", inputs))\n\n    imports = []\n\n    if inputs:\n        imports.append(create_import_statement(gs_read_auto))\n\n    imports.append(create_import_statement(function))\n\n    loading_lines = [\n        f\"{inpt} = gs_read_auto({param}.path)\" for param, inpt in zip(params, inputs)\n    ]\n\n    loading_statement = \"\\n\".join(loading_lines)\n\n    function_call = create_function_call(\n        function_name=function.__name__, args=inputs, kwargs=kwargs, outputs=outputs\n    )\n\n    output_statement, output_type_hint = create_return_statement(outputs=outputs)\n\n    body = \"\\n\".join(\n        [\n            \"# Inputs Loading\",\n            loading_statement,\n            \"# Function Call\",\n            function_call,\n            \"# Outputs Packing\",\n            output_statement,\n        ]\n    )\n\n    component_code = create_function_code(\n        component_name,\n        make_input(params),\n        body,\n        imports,\n        output_type_hint,\n        decorator=\"@component(base_image=MLFLOW_IMAGE)\",\n    )\n\n    return component_code\n"
  },
  {
    "repo": "kashiftriffort/kubeflow-pipelines-master",
    "file_path": "boston_housing/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/kashiftriffort/kubeflow-pipelines-master/main/boston_housing/pipeline.py",
    "content": "import kfp\nfrom kfp import dsl\n\ndef preprocess_op():\n\n    return dsl.ContainerOp(\n        name='Preprocess Data',\n        image='gnovack/boston_pipeline_preprocessing:latest',\n        arguments=[],\n        file_outputs={\n            'x_train': '/app/x_train.npy',\n            'x_test': '/app/x_test.npy',\n            'y_train': '/app/y_train.npy',\n            'y_test': '/app/y_test.npy',\n        }\n    )\n\ndef train_op(x_train, y_train):\n\n    return dsl.ContainerOp(\n        name='Train Model',\n        image='gnovack/boston_pipeline_train:latest',\n        arguments=[\n            '--x_train', x_train,\n            '--y_train', y_train\n        ],\n        file_outputs={\n            'model': '/app/model.pkl'\n        }\n    )\n\ndef test_op(x_test, y_test, model):\n\n    return dsl.ContainerOp(\n        name='Test Model',\n        image='gnovack/boston_pipeline_test:latest',\n        arguments=[\n            '--x_test', x_test,\n            '--y_test', y_test,\n            '--model', model\n        ],\n        file_outputs={\n            'mean_squared_error': '/app/output.txt'\n        }\n    )\n\ndef deploy_model_op(model):\n\n    return dsl.ContainerOp(\n        name='Deploy Model',\n        image='gnovack/boston_pipeline_deploy_model:latest',\n        arguments=[\n            '--model', model\n        ]\n    )\n\n@dsl.pipeline(\n   name='Boston Housing Pipeline',\n   description='An example pipeline that trains and logs a regression model.'\n)\n\ndef boston_pipeline():\n    _preprocess_op = preprocess_op()\n    \n    _train_op = train_op(\n        dsl.InputArgumentPath(_preprocess_op.outputs['x_train']),\n        dsl.InputArgumentPath(_preprocess_op.outputs['y_train'])\n    ).after(_preprocess_op)\n\n    _test_op = test_op(\n        dsl.InputArgumentPath(_preprocess_op.outputs['x_test']),\n        dsl.InputArgumentPath(_preprocess_op.outputs['y_test']),\n        dsl.InputArgumentPath(_train_op.outputs['model'])\n    ).after(_train_op)\n\n    deploy_model_op(\n        dsl.InputArgumentPath(_train_op.outputs['model'])\n    ).after(_test_op)\n\nclient = kfp.Client()\nclient.create_run_from_pipeline_func(boston_pipeline, arguments={})\n"
  },
  {
    "repo": "ygeat7/kubeflow_pipeline_myxgb",
    "file_path": "pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/ygeat7/kubeflow_pipeline_myxgb/main/pipeline.py",
    "content": "import kfp\nimport kfp.components as comp\nfrom kfp import dsl\n\n@dsl.pipeline(\n    name='xgb-pipeline-katib',\n    description='xgb pipeline with katib'\n)\n\ndef xgb_pipeline_katib(building_num: int=1):\n    \n    xgb_pvc = dsl.PipelineVolume('xgb-pvc')\n    exp_pvc = dsl.PipelineVolume('exp-pvc')\n    data_pvc = dsl.PipelineVolume('data-pvc')\n    \n    data_load = dsl.ContainerOp(\n        name='data load',\n        image='kubeflow-registry.default.svc.cluster.local:30000/xgb_data_load:5.0',\n        command=['python', 'data_load.py'],\n        arguments=[\n            '--num', str(building_num)\n        ],\n        file_outputs={'dataset' : '/dataset.csv'}\n    )\n    \n    data_split = dsl.ContainerOp(\n        name='data split',\n        image='kubeflow-registry.default.svc.cluster.local:30000/data_split:5.5',\n        arguments=[\n            '--dataset', dsl.InputArgumentPath(data_load.outputs['dataset']),\n            '--num', str(building_num)\n        ],\n        command=['python', 'data_split.py'],\n        file_outputs={'xtrain' : '/xtrain.csv',\n                      'xtest' : '/xtest.csv',\n                      'ytrain' : '/ytrain.csv',\n                      'ytest' : '/ytest.csv',\n                      'expyaml' : '/test.yaml'},\n        pvolumes={'/app/exp': exp_pvc,\n\t\t  '/app/data': data_pvc}\n    )\n\n    katib_create_exp = dsl.ContainerOp(\n        name='create katib experiment',\n        image='kubeflow-registry.default.svc.cluster.local:30000/create_exp:5.1',\n        arguments=[\n            '--expyaml', dsl.InputArgumentPath(data_split.outputs['expyaml'])\n        ],\n        command=['python', 'create_exp.py'],\n    )\n\n    get_best_params = dsl.ContainerOp(\n        name='get best params',\n        image='kubeflow-registry.default.svc.cluster.local:30000/get_bp:5.3',\n        arguments=[\n            '--expyaml', dsl.InputArgumentPath(data_split.outputs['expyaml'])\n        ],\n        command=['python', 'get_bp.py'],\n\tfile_outputs={'best_params' : '/best_params.csv'}\n    )\n\n    train = dsl.ContainerOp(\n        name='xgb train',\n        image='kubeflow-registry.default.svc.cluster.local:30000/xgb_train:5.1',\n        arguments=[\n            '--xtrain',  dsl.InputArgumentPath(data_split.outputs['xtrain']),\n            '--ytrain',  dsl.InputArgumentPath(data_split.outputs['ytrain']),\n            '--best_params', dsl.InputArgumentPath(get_best_params.outputs['best_params'])\n        ],\n        command=['python', 'xgb_train.py'],\n        file_outputs={'model' : '/xgb_model.model'}\n    )\n    \n    test = dsl.ContainerOp(\n        name='test and evaluation',\n        image='kubeflow-registry.default.svc.cluster.local:30000/test_eval:5.1',\n        arguments=[\n            '--xtest',  dsl.InputArgumentPath(data_split.outputs['xtest']),\n            '--ytest',  dsl.InputArgumentPath(data_split.outputs['ytest']),\n            '--model',  dsl.InputArgumentPath(train.outputs['model']),\n            '--num', str(building_num)\n        ],\n        command=['python', 'test_eval.py'],\n        pvolumes={'/app/model': xgb_pvc},\n        file_outputs={'mlpipeline-metrics' : '/mlpipeline-metrics.json'}\n    )\n    \n    data_split.after(data_load)\n    katib_create_exp.after(data_split)\n    get_best_params.after(katib_create_exp)\n    train.after(data_split)\n    test.after(train)\n\nif __name__ == \"__main__\":\n    import kfp.compiler as compiler\n    compiler.Compiler().compile(xgb_pipeline_katib, __file__ + \".tar.gz\")\n"
  },
  {
    "repo": "Nannakaroliina/kubeflow-pipeline-demo",
    "file_path": "pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/Nannakaroliina/kubeflow-pipeline-demo/main/pipeline.py",
    "content": "import kfp\nfrom kfp import dsl\n\n\n# Build ContainerOps of Docker images with needed arguments for data/model access and file_output definitions.\n# TODO Fix the FutureWarning regarding ContainerOp\n\ndef preprocess_op():\n    return dsl.ContainerOp(\n        name='Preprocess Data',\n        image='nannakaroliina/kubeflow_pipeline_demo_preprocessing:latest',\n        arguments=[],\n        file_outputs={\n            'x_train': '/app/x_train.npy',\n            'x_test': '/app/x_test.npy',\n            'y_train': '/app/y_train.npy',\n            'y_test': '/app/y_test.npy',\n        }\n    )\n\n\ndef train_op(x_train, y_train):\n    return dsl.ContainerOp(\n        name='Train Model',\n        image='nannakaroliina/kubeflow_pipeline_demo_train:latest',\n        arguments=[\n            '--x_train', x_train,\n            '--y_train', y_train\n        ],\n        file_outputs={\n            'model': '/app/model.plk'\n        }\n    )\n\n\ndef predict_op(x_test, y_test, model):\n    return dsl.ContainerOp(\n        name='Test Model',\n        image='nannakaroliina/kubeflow_pipeline_demo_predict:latest',\n        arguments=[\n            '--x_test', x_test,\n            '--y_test', y_test,\n            '--model', model\n        ],\n        file_outputs={\n            'f1_score': '/app/results.txt'\n        }\n    )\n\n\n@dsl.pipeline(\n    name='Kubeflow pipeline demo',\n    description='Kubeflow pipeline demo with simple model training'\n)\ndef kubeflow_demo_pipeline():\n    _preprocess_op = preprocess_op()\n\n    _train_op = train_op(\n        dsl.InputArgumentPath(_preprocess_op.outputs['x_train']),\n        dsl.InputArgumentPath(_preprocess_op.outputs['y_train'])\n    ).after(_preprocess_op)\n\n    _test_op = predict_op(\n        dsl.InputArgumentPath(_preprocess_op.outputs['x_test']),\n        dsl.InputArgumentPath(_preprocess_op.outputs['y_test']),\n        dsl.InputArgumentPath(_train_op.outputs['model'])\n    ).after(_train_op)\n\n\nif __name__ == '__main__':\n    # Build a pipeline yaml file to be uploaded to Kubeflow Pipeline UI\n    # TODO implement local run option without manual pipeline creation\n    kfp.compiler.Compiler().compile(kubeflow_demo_pipeline, 'pipeline.yaml')\n"
  },
  {
    "repo": "nevrets/kubeflow-pipelines-example",
    "file_path": "01-hello-world/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/nevrets/kubeflow-pipelines-example/main/01-hello-world/pipeline.py",
    "content": "from kfp import dsl\nfrom kfp import compiler\n\n@dsl.component\ndef say_hello(name: str) -> str:\n    hello_text = f'Hello, {name}!'\n    print(hello_text)\n    return hello_text\n\n@dsl.pipeline\ndef hello_pipeline(recipient: str) -> str:\n    hello_task = say_hello(name=recipient)\n    return hello_task.output\n\n\n\nif __name__ == '__main__':\n    compiler.Compiler().compile(hello_pipeline, 'pipeline.yaml')"
  },
  {
    "repo": "nevrets/kubeflow-pipelines-example",
    "file_path": "02-Iris/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/nevrets/kubeflow-pipelines-example/main/02-Iris/pipeline.py",
    "content": "import kfp\nfrom kfp import dsl\nfrom config import CFG\n\n\n@dsl.pipeline(\n    name='nevret-iris',\n    description='nevret iris test'\n)\ndef iris_pipeline():\n    add_p = dsl.ContainerOp(\n        name=\"load iris data pipeline\",\n        image=\"nevret/nevret-iris-preprocessing:0.5\",\n        command=['python', 'load_data.py'],\n        arguments=[\n            '--data_path', './Iris.csv'\n        ],\n        file_outputs={'iris' : '/iris.csv'}\n    )\n    \n    ml = dsl.ContainerOp(\n        name=\"training pipeline\",\n        image=\"nevret/nevret-iris-training:0.5\",\n        command=['python', 'train.py'],\n        arguments=[\n            '--data', add_p.outputs['iris']\n        ]\n    )\n\n    ml.after(add_p)\n    \n    \n    \nif __name__ == \"__main__\":\n    import kfp.compiler as compiler\n    compiler.Compiler().compile(iris_pipeline, \"02-Iris/pipeline.yaml\")\n    \n    import kfp\n    import requests\n\n    USERNAME = CFG.kf_username\n    PASSWORD = CFG.kf_password\n    NAMESPACE = CFG.kf_namespace\n    HOST = CFG.kf_host\n\n    session = requests.Session()\n    response = session.get(HOST)\n\n    headers = {\n        \"Content-Type\": \"application/x-www-form-urlencoded\",\n    }\n\n    data = {\"login\": USERNAME, \"password\": PASSWORD}\n    session.post(response.url, headers=headers, data=data)\n    session_cookie = session.cookies.get_dict()[\"authservice_session\"]    \n\n    \n    # Submit the pipeline for execution\n    pipeline_func = iris_pipeline\n    client = kfp.Client(\n        host=f\"{HOST}/pipeline\",\n        namespace=f\"{NAMESPACE}\",\n        cookies=f\"authservice_session={session_cookie}\",\n    )\n    print(client.list_pipelines())\n    \n    run = client.create_run_from_pipeline_func(pipeline_func, arguments={})\n"
  },
  {
    "repo": "nevrets/kubeflow-pipelines-example",
    "file_path": "03-ML-model-project/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/nevrets/kubeflow-pipelines-example/main/03-ML-model-project/pipeline.py",
    "content": "import kfp\nfrom kfp import dsl\nfrom kfp.components import func_to_container_op\nfrom config import CFG\n@func_to_container_op\ndef show_results(decision_tree:float, \n                 logistic_regression:float, \n                 svm:float, \n                 naive_bayes:float, \n                 xgb:float) -> None:\n    \n    # Given the outputs from decision_tree, logistic regression, svm, naive_bayes, xgboost components\n    print(f\"Decision tree (accuracy): {decision_tree}\")\n    print(f\"Logistic regression (accuracy): {logistic_regression}\")\n    print(f\"SVM (SVC) (accuracy): {svm}\")\n    print(f\"Naive Bayes (Gaussian) (accuracy): {naive_bayes}\")\n    print(f\"XGBoost (accuracy): {xgb}\")\n\n\n@dsl.pipeline(name='ML Models Pipeline', description='Applies Decision Tree, Logistic Regression, SVM, Naive Bayes, XGBoost for classification problem.')\ndef ml_models_pipeline():\n    # Loads the yaml manifest for each component\n    download = kfp.components.load_component_from_file('/data/nevret/kubeflow_pipelines/03-ML-model-project/01-download_data/download_data.yaml')\n    decision_tree = kfp.components.load_component_from_file('/data/nevret/kubeflow_pipelines/03-ML-model-project/02-decision_tree/decision_tree.yaml')\n    logistic_regression = kfp.components.load_component_from_file('/data/nevret/kubeflow_pipelines/03-ML-model-project/03-logistic_regression/logistic_regression.yaml')\n    svm = kfp.components.load_component_from_file('/data/nevret/kubeflow_pipelines/03-ML-model-project/04-svm/svm.yaml')\n    naive_bayes = kfp.components.load_component_from_file('/data/nevret/kubeflow_pipelines/03-ML-model-project/05-naive_bayes/naive_bayes.yaml')\n    xgb = kfp.components.load_component_from_file('/data/nevret/kubeflow_pipelines/03-ML-model-project/06-xgb/xgb.yaml')\n    \n    # Run download_data task\n    download_task = download()\n    \n    # Run ML models tasks with input data\n    decision_tree_task = decision_tree(download_task.output)\n    logistic_regression_task = logistic_regression(download_task.output)\n    svm_task = svm(download_task.output)\n    naive_bayes_task = naive_bayes(download_task.output)\n    xgb_task = xgb(download_task.output)\n    \n    # Given the outputs from ML models tasks\n    # the component \"show_results\" is called to print the results.\n    show_results(decision_tree_task.output, logistic_regression_task.output, svm_task.output, naive_bayes_task.output, xgb_task.output)\n\n\n\nif __name__ == '__main__':\n    # ml_models_pipeline()\n    kfp.compiler.Compiler().compile(ml_models_pipeline, \"03-ML-model-project/ML-model-pipeline.yaml\")\n    \n    import requests\n\n    USERNAME = CFG.kf_username\n    PASSWORD = CFG.kf_password\n    NAMESPACE = CFG.kf_namespace\n    HOST = CFG.kf_host\n    \n    session = requests.Session()\n    response = session.get(HOST)\n\n    headers = {\n        \"Content-Type\": \"application/x-www-form-urlencoded\",\n    }\n\n    data = {\"login\": USERNAME, \"password\": PASSWORD}\n    session.post(response.url, headers=headers, data=data)\n    session_cookie = session.cookies.get_dict()[\"authservice_session\"]    \n\n    \n    # Submit the pipeline for execution\n    pipeline_func = ml_models_pipeline\n    client = kfp.Client(\n        host=f\"{HOST}/pipeline\",\n        namespace=f\"{NAMESPACE}\",\n        cookies=f\"authservice_session={session_cookie}\",\n    )\n    \n    print(client.list_pipelines())\n    \n    run = client.create_run_from_pipeline_func(pipeline_func, arguments={})\n    \n    print('')"
  },
  {
    "repo": "nevrets/kubeflow-pipelines-example",
    "file_path": "06-MLOps-example/iris-train-pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/nevrets/kubeflow-pipelines-example/main/06-MLOps-example/iris-train-pipeline.py",
    "content": "import os\nimport kfp\nfrom kfp import compiler, dsl\n\nfrom utils import add_nfs_volume, add_configmap, KubeflowClient, add_sharedmemory\nfrom config import CFG\n\n\n@kfp.dsl.pipeline(\n    name=\"IRIS Model Training Pipeline\",\n    description=\"\",\n)\ndef iris_train_pipeline(\n    lakefs_date: str = \"2025-04-08-104352/\",\n    random_state: int = 42,\n    max_iter: int = 1000,\n    multi_class: str = \"multinomial\",\n):\n    # ----- Temporary volume create ----- #\n    vop = dsl.VolumeOp(\n        name=\"Temporary volume create\",\n        storage_class=\"k8s-nfs\",\n        resource_name=\"tmp-volume\",\n        modes=dsl.VOLUME_MODE_RWM,     # ReadWriteMany\n        size=\"1Gi\",\n    ).add_pod_annotation(name=\"pipelines.kubeflow.org/max_cache_staleness\", value=\"P0D\")\n\n\n    download_datasets_op = (\n        dsl.ContainerOp(\n            name=\"Download Datasets\",\n            image=CFG.hb_iris_demo_function_image,\n            command=[\"python\", \"lakefs_download.py\"],\n            arguments=[\n                \"--root\", \"/mnt/preprocessed\",\n                \"--repo\", \"demo-iris\",\n                \"--date\", lakefs_date,\n            ],\n            pvolumes={\"/mnt\": vop.volume},\n        )\n        .add_pod_annotation(name=\"pipelines.kubeflow.org/max_cache_staleness\", value=\"P0D\")\n        .after(vop)\n    )\n\n\n    train_model_op = (\n        dsl.ContainerOp(\n            name=\"Train Model\",\n            image=CFG.hb_iris_demo_model_image,\n            command=[\"python\", \"train.py\"],\n            arguments=[\n                \"--data_path\", \"/mnt/preprocessed\", \n                \"--random_state\", random_state,\n                \"--max_iter\", max_iter,\n                \"--multi_class\", multi_class,\n            ],\n            pvolumes={\"/mnt\": vop.volume},\n        )\n        .add_pod_annotation(name=\"pipelines.kubeflow.org/max_cache_staleness\", value=\"P0D\")\n        .after(download_datasets_op)\n    )\n    \n    \n\n\nif __name__ == \"__main__\":\n    # ----- build ----- #\n    pipeconf = kfp.dsl.PipelineConf()\n\n    file_name = os.path.splitext(os.path.basename(__file__))[0]\n    compiler.Compiler().compile(iris_train_pipeline, f\"{file_name}.tar.gz\", pipeline_conf=pipeconf)\n\n    try:\n        client = KubeflowClient(\n            endpoint=CFG.kf_host,\n            username=CFG.kf_username,\n            password=CFG.kf_password,\n            namespace=CFG.kf_namespace,\n        )\n        res = client.upload_pipeline(f\"{file_name}.tar.gz\", \"iris-train-pipeline\")\n        print(\"\uc5c5\ub85c\ub4dc \uacb0\uacfc:\", res)\n    except Exception as e:\n        print(\"\uc5d0\ub7ec \ubc1c\uc0dd:\", str(e))\n        \n    print(\"\ud30c\uc774\ud504\ub77c\uc778 \ubaa9\ub85d:\", client.client.list_pipelines())"
  },
  {
    "repo": "ithingv/kubeflow_pipeline_mnist",
    "file_path": "add_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/ithingv/kubeflow_pipeline_mnist/master/add_pipeline.py",
    "content": "import kfp\nimport kfp.dsl as dsl\nfrom kfp.v2.dsl import component\n\n@component\ndef add(a: float, b: float) -> float:\n    \"\"\"Calculates sum of two arguments\"\"\"\n    return a + b\n\n@dsl.pipeline(\n    name=\"addition-pipeline\",\n    description=\"An example pipeline that performs addition calculations.\",\n    # pipeline_root='gs://my-pipeline-root/example-pipeline'\n)\ndef add_pipeline(a: float = 1, b: float = 7):\n    add_task = add(a, b)\n\nclient = kfp.Client()\nclient.create_run_from_pipeline_func(\n    add_pipeline,\n    arguments={\"a\": 7, \"b\": 8},\n    mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE,\n)"
  },
  {
    "repo": "ithingv/kubeflow_pipeline_mnist",
    "file_path": "mnist_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/ithingv/kubeflow_pipeline_mnist/master/mnist_pipeline.py",
    "content": "import kfp\nimport kfp.dsl as dsl\nfrom kfp.v2.dsl import component, Input, Output, Dataset, Model, Metrics\n\n@component(\n    packages_to_install=[\"tensorflow\", \"numpy\"]\n)\ndef load_data(output_dataset: Output[Dataset]):\n    print(\"data loading...\")\n    import tensorflow as tf\n    import numpy as np\n\n    mnist = tf.keras.datasets.mnist\n    (train_x, train_y), (test_x, test_y) = mnist.load_data()\n\n    with open(output_dataset.path, \"wb\") as f:\n        np.savez(f, train_x=train_x, train_y=train_y, test_x=test_x, test_y=test_y)\n    print(f\"Saved raw data on : {output_dataset.path}\")\n\n@component(\n    packages_to_install=[\"numpy\"]\n)\ndef preprocessing(input_dataset: Input[Dataset], output_dataset: Output[Dataset]):\n    print(\"Preprocessing...\")\n    import numpy as np\n\n    with open(input_dataset.path, \"rb\") as f:\n        mnist = np.load(f)\n        train_x, train_y = mnist[\"train_x\"], mnist[\"train_y\"]\n        test_x, test_y = mnist[\"test_x\"], mnist[\"test_y\"]\n\n    train_x = train_x / 255.0\n    test_x = test_x / 255.0\n\n    with open(output_dataset.path, \"wb\") as f:\n        np.savez(f, train_x=train_x, train_y=train_y, test_x=test_x, test_y=test_y)\n    print(f\"Saved preproceesed data on : {output_dataset.path}\")\n\n@component(\n    packages_to_install=[\"tensorflow\", \"numpy\"]\n)\ndef train(\n    dataset: Input[Dataset], output_model: Output[Model], metrics: Output[Metrics]\n):\n    print(\"training...\")\n    import tensorflow as tf\n    import numpy as np\n\n    with open(dataset.path, \"rb\") as f:\n        mnist = np.load(f)\n        train_x, train_y = mnist[\"train_x\"], mnist[\"train_y\"]\n        test_x, test_y = mnist[\"test_x\"], mnist[\"test_y\"]\n    print(f\"train x shape: {train_x.shape}\")\n    print(f\"train y shape: {train_y.shape}\")\n    print(f\"test x shape: {test_x.shape}\")\n    print(f\"test y shape: {test_y.shape}\")\n    model = tf.keras.Sequential(\n        [\n            tf.keras.layers.Flatten(input_shape=(28, 28)),\n            tf.keras.layers.Dense(128, activation=\"relu\"),\n            tf.keras.layers.Dense(10, activation=\"softmax\"),\n        ]\n    )\n    model.compile(\n        loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"acc\"]\n    )\n    model.fit(train_x, train_y)\n    loss, acc = model.evaluate(test_x, test_y)\n\n    metrics.log_metric(\"accuracy\", (acc * 100.0))\n    metrics.log_metric(\"loss\", loss)\n    metrics.log_metric(\"framework\", \"Tensorflow\")\n    metrics.log_metric(\"Model\", \"LinearModel\")\n    metrics.log_metric(\"dataset_size\", len(train_x))\n\n    model.save(output_model.path)\n\n@dsl.pipeline(\n    name=\"mnist-pipeline\", description=\"An example pipeline that mnist training.\"\n)\ndef mnist_pipeline():\n    load_data_task = load_data()\n    print(\"outputs: \", load_data_task.output)\n    preprocessing_task = preprocessing(load_data_task.outputs[\"output_dataset\"])\n    train_task = train(preprocessing_task.outputs[\"output_dataset\"])\n\nclient = kfp.Client()\nclient.create_run_from_pipeline_func(\n    mnist_pipeline, arguments={}, mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE,\n)"
  },
  {
    "repo": "raldebsi/Kubeflow-Experimental-Pipelines",
    "file_path": "src/pipelines/add_randoms.py",
    "raw_url": "https://raw.githubusercontent.com/raldebsi/Kubeflow-Experimental-Pipelines/master/src/pipelines/add_randoms.py",
    "content": "from collections import namedtuple\nfrom typing import NamedTuple\nimport kfp\nimport kfp.components as comp\nimport kfp.compiler\n\n\ndef add_random(num : int)-> NamedTuple(\n\t'Outputs',\n\t[\n\t\t('num', int),\n\t]):\n\tfrom collections import namedtuple\n\n\texample_output = namedtuple(\n\t\t'Outputs',\n\t\t['num'])\n\treturn example_output(num + 3)\n\treturn num + 3\n\ndef add_number(num : int)-> NamedTuple(\n\t'Outputs',\n\t[\n\t\t('num', int),\n\t]):\n\tfrom collections import namedtuple\n\texample_output = namedtuple(\n\t\t'Outputs',\n\t\t['num'])\n\treturn example_output(num + 5)\n\ndef mult_int(a : int, b : int)-> NamedTuple(\n\t'Outputs',\n\t[\n\t\t('num', int),\n\t]):\n\tfrom collections import namedtuple\n\texample_output = namedtuple(\n\t\t'Outputs',\n\t\t['num'])\n\treturn example_output(a * b)\n\n@kfp.dsl.pipeline(\n\tname='Add Random ',\n\tdescription='Add Random'\n)\ndef main_pipeline(a : int, b : int):\n\tadd_random_comp = comp.create_component_from_func(\n\t\tadd_random\n\t)\n \n\tadd_random_task1 = add_random_comp(a)\n\tadd_random_task2 = add_random_comp(b)\n\t\n\tmult_int_comp = comp.create_component_from_func(\n\t\tmult_int\n\t)\n\t# print(add_random_task1.outputs)\n\tmult_int_task = mult_int_comp(add_random_task1.outputs['num'], add_random_task2.outputs['num'])\n\tprint(mult_int_task.outputs['num'])\n \nif __name__ == \"__main__\":\n\tkfp.compiler.Compiler().compile(main_pipeline, 'random_pipeline.yaml')\n"
  },
  {
    "repo": "raldebsi/Kubeflow-Experimental-Pipelines",
    "file_path": "src/pipelines/common_utils.py",
    "raw_url": "https://raw.githubusercontent.com/raldebsi/Kubeflow-Experimental-Pipelines/master/src/pipelines/common_utils.py",
    "content": "import uuid\r\n\r\nimport yaml\r\nimport kfp\r\nfrom kfp import dsl\r\nfrom kfp.dsl._pipeline_param import sanitize_k8s_name\r\nfrom kfp.dsl._pipeline_volume import PipelineVolume\r\n\r\ndef generate_random_hex_service(randomize_service_suffix=True):\r\n    def generate_random(randomize_service_suffix: bool) -> str:\r\n        if not randomize_service_suffix: # No need to generate random\r\n            return \"\"\r\n        \r\n        from uuid import uuid4\r\n        return uuid4().hex[:7]\r\n    \r\n    return cacheless_task(kfp.components.func_to_container_op(generate_random)(randomize_service_suffix)).output\r\n\r\ndef sanitize_service(string: str, randomize_service_suffix):\r\n    def sanitize_service_name(name: str, extra: str) -> str:\r\n        print(\"Sanitizing\", name)\r\n        if extra:\r\n            name += '-' + extra[:5]\r\n        s = name.lower().replace(\" \", \"_\").replace(\".\", \"-\").replace(\"_\", \"-\")\r\n        print(\"Sanitized\", s)\r\n        return s\r\n    random_string = generate_random_hex_service(randomize_service_suffix)\r\n    return kfp.components.func_to_container_op(sanitize_service_name)(string, random_string).output\r\n\r\ndef spec_from_file_format(yaml_file, **kwargs):\r\n    with open(yaml_file, 'r') as f:\r\n        component_spec = f.read()\r\n        for k, v in kwargs.items():\r\n            component_spec = component_spec.replace('{' + k + '}', str(v))\r\n        return yaml.safe_load(component_spec)\r\n\r\ndef get_volume_by_name(name, unique_name = \"\") -> PipelineVolume:\r\n    # Get volume    \r\n    name = str(name)\r\n    if not unique_name:\r\n        is_pipeline_name = name.startswith('{{') and name.endswith('}}')\r\n        volume_name = sanitize_k8s_name(name) if not is_pipeline_name else name\r\n        unique_volume_name = \"%s-%s\" % (volume_name, uuid.uuid4().hex[:7])\r\n    else:\r\n        unique_volume_name = unique_name\r\n    mount_vol = PipelineVolume(\r\n        name=unique_volume_name, # Parameter name, if found then it will be reused. Therefore it should be unique.\r\n        pvc=name,\r\n        volume=None, # type: ignore \r\n    )\r\n\r\n    return mount_vol\r\n\r\ndef get_or_create_pvc(name: str, size_: str, resource: str, randomize: bool = False, mode=dsl.VOLUME_MODE_RWO):\r\n    if not resource and not randomize:\r\n        raise ValueError(\"Either resource or randomize should be provided\")\r\n    return dsl.VolumeOp(\r\n        name=name,  # Operation Unique Name\r\n                    # If operation exists then it will be reused.\r\n                    # If operation does not exist then it will be created and the name will be resource_name below.\r\n        size=size_,\r\n        modes=mode,\r\n        resource_name=resource, # PVC Name, will be {pipeline_name}-{id}-resource if name does not exist and generate_unique_name is True.\r\n        # volume_name=\"ridhwan-personal-volume\", # Volume Name, do not use as it will cause the volume to not be found\r\n        # data_source=PipelineParam(name=\"data_source\"), # This way it will create a volume snapshot from param\r\n        generate_unique_name=randomize,\r\n    )\r\n\r\ndef add_pvolumes_func(pvolumes):\r\n    # kfp.dsl._container_op.ContainerOp\r\n    return lambda func: func.add_pvolumes(pvolumes)\r\n\r\ndef number_to_base_26(number):\r\n    \"\"\"\r\n        Converts a number to base 26 then to alphabet, 0 -> a, 25 -> z, 26 -> aa\r\n    \"\"\"\r\n    if number < 26:\r\n        return chr(ord('a') + number)\r\n    else:\r\n        return number_to_base_26(number // 26 - 1) + chr(ord('a') + number % 26)\r\n\r\ndef setup_volume(volume_name, *mount_points):\r\n    if len(mount_points) == 1:\r\n        mount_vol = get_volume_by_name(volume_name, \"volume-bind\")\r\n        return add_pvolumes_func({mount_points[0]: mount_vol})\r\n    else:\r\n        mount_dict = {}\r\n        for i, mount_point in enumerate(mount_points):\r\n            mount_vol = get_volume_by_name(volume_name, \"volume-bind-%s\" % number_to_base_26(i))\r\n            mount_dict[mount_point] = mount_vol\r\n\r\n    return add_pvolumes_func(mount_dict)\r\n\r\ndef cacheless_task(task):\r\n    task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\r\n    return task # To allow chaining"
  },
  {
    "repo": "raldebsi/Kubeflow-Experimental-Pipelines",
    "file_path": "src/pipelines/kube_exp_ridhwan.py",
    "raw_url": "https://raw.githubusercontent.com/raldebsi/Kubeflow-Experimental-Pipelines/master/src/pipelines/kube_exp_ridhwan.py",
    "content": "from typing import NamedTuple, List\nimport kfp\nimport kfp.compiler\nimport kfp.components\n\ndef wget_data(url: str, output_path: str) -> None:\n    \"\"\"\n    Download data from a URL.\n    \"\"\"\n    import wget\n    import os\n    \n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n    print(\"Downloading {} to {}\".format(url, output_path))\n    data = wget.download(url, output_path)\n    if not data:\n        raise ValueError(\"Failed to download data from {}\".format(url))\n    \n    return\n\ndef validate_data(data_paths: List[str], data_root: str = \"/data\"):\n    \"\"\"\n    Validate data.\n    \"\"\"\n    import os\n\n    for data_path in data_paths:\n        if not data_path.startswith(data_root):\n            data_path = os.path.join(data_root, data_path)\n        if not os.path.exists(data_path):\n            raise ValueError(\"Data path {} does not exist\".format(data_path))\n        if not os.path.isfile(data_path):\n            raise ValueError(\"Data path {} is not a file\".format(data_path))\n        print(\"Data {} is valid\".format(data_path))\n\n    return\n\n\n@kfp.dsl.pipeline(\n    name=\"Kube Lablelizer Experiment\",\n    description=\"Experiment using KubeFlow to run a ML Train & Test pipeline\",\n)\ndef kube_exp_ridhwan(train_source: str, valid_source: str, test_data: kfp.components.InputPath(\"txt\")):\n    from kfp import dsl\n\n    print(\"Starting Pipeline Generation\")\n    print(\"Source Params:\")\n    print(\"Train Source: {}\".format(train_source))\n    print(\"Valid Source: {}\".format(valid_source))\n    print(\"Test data: {}\".format(test_data))\n\n    mount_vol = dsl.VolumeOp(\n        name=\"Mount Volume\",\n        resource_name=\"kube-mount-ridhwan\",\n        size=\"4Gi\",\n        modes=dsl.VOLUME_MODE_RWO,\n    )\n\n    data_store_path = \"/data\"\n    pvolumes = {data_store_path: mount_vol.volume}\n\n    # print(\"Volume:\", mount_vol)\n    # print(\"Volume Mount:\", pvolumes)\n\n    volumetrize = lambda func: func.add_pvolumes(pvolumes)\n\n    print(\"Generating Components\")\n    retreive_data_component = kfp.components.create_component_from_func(wget_data, base_image=\"python:slim-buster\", packages_to_install=[\"wget\"])\n    get_train_task = volumetrize(retreive_data_component(train_source, \"{root}/train.json\".format(root=data_store_path)))\n    get_valid_task = volumetrize(retreive_data_component(valid_source, \"{root}/dev.json\".format(root=data_store_path)))\n\n    validate_data_component = kfp.components.create_component_from_func(validate_data, base_image=\"python:slim-buster\")\n    validation_task = validate_data_component([\"/data/train.json\", \"/data/dev.json\"], \"/data\")\n    validation_task = volumetrize(validation_task)\n    validation_task.after(get_train_task, get_valid_task)\n\n\n    print(\"Pipeline Created\")\n\nif __name__ == \"__main__\":\n    kfp.compiler.Compiler().compile(kube_exp_ridhwan, __file__.rsplit(\".\", 1)[0] + \".yaml\")"
  },
  {
    "repo": "raldebsi/Kubeflow-Experimental-Pipelines",
    "file_path": "src/pipelines/test_upload.py",
    "raw_url": "https://raw.githubusercontent.com/raldebsi/Kubeflow-Experimental-Pipelines/master/src/pipelines/test_upload.py",
    "content": "from typing import NamedTuple\nimport kfp\nimport kfp.compiler\nimport kfp.components\n\ndef receive_and_print_data(bin_input: kfp.components.InputPath(\"DataInput\")) -> NamedTuple(\n    'Outputs', [\n        (\"my_out_data\", kfp.components.OutputBinaryFile) # ?\n    ]\n):\n    print(\"Bin_Input: \", bin_input)\n\n    with open(bin_input, encoding='utf-8') as f:\n        print(\"Content:\", f.read())\n    return \"This is my output\"\n\ndef receive_and_print_any(input):\n    print(\"input\", input)\n    import os\n    if os.path.exists(input):\n        print(\"Is File\")\n        with open(input, encoding='utf-8') as f:\n            print(\"Data:\", f.read())\n\n@kfp.dsl.pipeline(\n    \"Test File Upload\",\n    \"Testing uploading files from ui\",\n)\ndef pipeline(test_data_2: kfp.components.InputPath(\"DataInput\")):\n    print(test_data_2)\n\n    comp = kfp.components.create_component_from_func(\n        receive_and_print_data,\n    )\n    comp2 = kfp.components.create_component_from_func(receive_and_print_any)\n\n    task = comp(test_data_2)\n    task2 = comp2(task.outputs[\"my_out_data\"])\n"
  },
  {
    "repo": "raldebsi/Kubeflow-Experimental-Pipelines",
    "file_path": "src/pipelines/end_to_end/just_write.py",
    "raw_url": "https://raw.githubusercontent.com/raldebsi/Kubeflow-Experimental-Pipelines/master/src/pipelines/end_to_end/just_write.py",
    "content": "import kfp\r\nfrom src.pipelines.common_utils import get_volume_by_name, add_pvolumes_func \r\n\r\ndef write_me(to_write: str):\r\n    with open(\"/store/my_file.txt\", \"w\") as f:\r\n        f.write(to_write)\r\n\r\n\r\n@kfp.dsl.pipeline(\r\n    \"Just Write\"\r\n)\r\ndef pipeline(volume_name: str, to_write: str):\r\n    mount_vol = get_volume_by_name(volume_name, \"my-bind-volume\")\r\n    mount_dict = {\"/store\": mount_vol}\r\n    volumetrize = add_pvolumes_func(mount_dict)\r\n\r\n    write_me_op = kfp.components.func_to_container_op(write_me)\r\n    write_me_task = volumetrize(write_me_op(to_write))"
  },
  {
    "repo": "raldebsi/Kubeflow-Experimental-Pipelines",
    "file_path": "src/pipelines/end_to_end/mnist_experiment.py",
    "raw_url": "https://raw.githubusercontent.com/raldebsi/Kubeflow-Experimental-Pipelines/master/src/pipelines/end_to_end/mnist_experiment.py",
    "content": "import kfp\r\nfrom kfp import components, dsl\r\nfrom kfp.components import func_to_container_op\r\nfrom kubeflow.katib import (ApiClient, V1beta1AlgorithmSpec,\r\n                            V1beta1ExperimentSpec, V1beta1FeasibleSpace,\r\n                            V1beta1ObjectiveSpec, V1beta1ParameterSpec,\r\n                            V1beta1TrialParameterSpec, V1beta1TrialTemplate)\r\nimport yaml\r\n\r\nfrom src.pipelines.common_utils import get_or_create_pvc, spec_from_file_format\r\nimport json\r\n\r\ndef katib_experiment_factory(experiment, namespace, steps):\r\n    max_trial_count = 5\r\n    max_failed_trial_count = 3\r\n    parallel_trial_count = 2\r\n\r\n    objective = V1beta1ObjectiveSpec( # Objective is to minimize the loss\r\n        type=\"minimize\",\r\n        goal=0.001,\r\n        objective_metric_name=\"loss\"\r\n    )\r\n\r\n    algorithm = V1beta1AlgorithmSpec(\r\n        algorithm_name=\"random\"\r\n    )\r\n\r\n    param_learning_rate = V1beta1ParameterSpec(\r\n            name=\"learning_rate\",\r\n            parameter_type=\"double\",\r\n            feasible_space=V1beta1FeasibleSpace(\r\n                min=0.01,\r\n                max=0.05,\r\n        )\r\n    )\r\n    param_learning_rate_spec = V1beta1TrialParameterSpec(\r\n        name=\"learningRate\",\r\n        description=\"Learning rate for the training model\",\r\n        reference=\"learning_rate\",\r\n    )\r\n\r\n    param_batch_size = V1beta1ParameterSpec(\r\n        name=\"batch_size\",\r\n        parameter_type=\"int\",\r\n        feasible_space=V1beta1FeasibleSpace(\r\n            min=\"80\",\r\n            max=\"100\",\r\n        )        \r\n    )\r\n    param_batch_size_spec = V1beta1TrialParameterSpec(\r\n        name=\"batchSize\",\r\n        description=\"Batch size for the training model\",\r\n        reference=\"batch_size\",\r\n    )\r\n\r\n    params = [\r\n        param_learning_rate,\r\n        param_batch_size        \r\n    ]\r\n\r\n    trial_spec = spec_from_file_format(\"src/pipelines/yamls/Specs/TFJob.yaml\", trainStepsParamVal=steps)\r\n    trial_template = V1beta1TrialTemplate(\r\n        primary_container_name=\"tensorflow\",\r\n        trial_parameters=[\r\n            param_learning_rate_spec,\r\n            param_batch_size_spec\r\n        ],\r\n        trial_spec=trial_spec,\r\n    )\r\n\r\n    experiment_spec = V1beta1ExperimentSpec(\r\n        max_trial_count=max_trial_count,\r\n        max_failed_trial_count=max_failed_trial_count,\r\n        parallel_trial_count=parallel_trial_count,\r\n        objective=objective,\r\n        algorithm=algorithm,\r\n        parameters=params,\r\n        trial_template=trial_template,\r\n    )\r\n\r\n    katib_op = components.load_component_from_file(\"src/pipelines/yamls/Components/katib_launcher.yaml\")\r\n    katib_task = katib_op(\r\n        experiment_name=experiment,\r\n        experiment_namespace=namespace,\r\n        experiment_spec=ApiClient().sanitize_for_serialization(experiment_spec),\r\n        experiment_timeout_minutes=60,\r\n        delete_finished_experiment=False\r\n    )\r\n\r\n    return katib_task\r\n\r\ndef convert_hyperparams(hyperparams) -> str:\r\n    import json\r\n    results = json.loads(hyperparams)\r\n    print(\"Hyperparams FineTuned: \", results)\r\n    best_params = []\r\n    for param in results[\"currentOptimalTrial\"][\"parameterAssignments\"]:\r\n        if param[\"name\"] == \"learning_rate\":\r\n            best_params.append(\"--tf-learning-rate={}\".format(param[\"value\"]))\r\n        elif param[\"name\"] == \"batch_size\":\r\n            best_params.append(\"--tf-batch-size={}\".format(param[\"value\"]))\r\n    print(\"Best Params\", best_params)\r\n    return \" \".join(best_params)\r\n\r\ndef create_tfjob_task(job_name, job_namespace, steps, hyperparams, mount_name):\r\n    tfjob_chief_spec = spec_from_file_format(\"src/pipelines/yamls/Specs/TFJobChief.yaml\",\r\n        trainingStepsParamVal=steps,\r\n        bestHPsParamVal=hyperparams,\r\n        volumeResourceName=mount_name\r\n    )\r\n    tfjob_worker_spec = spec_from_file_format(\"src/pipelines/yamls/Specs/TFJobWorker.yaml\",\r\n        trainingStepsParamVal=steps,\r\n        bestHPsParamVal=hyperparams,\r\n    )\r\n\r\n    tfjob_op = components.load_component_from_file(\"src/pipelines/yamls/Components/tfjob_launcher.yaml\")\r\n\r\n    tfjob_task = tfjob_op(\r\n        name=job_name,\r\n        namespace=job_namespace,\r\n        chief_spec=json.dumps(tfjob_chief_spec),\r\n        worker_spec=json.dumps(tfjob_worker_spec),\r\n        tfjob_timeout_minutes=60,\r\n        delete_finished_tfjob=False\r\n    )\r\n\r\n    return tfjob_task\r\n\r\ndef create_serve_task(model_name, model_namespace, mount_name):\r\n    infer_service = spec_from_file_format(\r\n        \"src/pipelines/yamls/Specs/KFServe.yaml\",\r\n        apiVersion=\"serving.kubeflow.org/v1beta1\",\r\n        modelName=model_name,\r\n        modelNamespace=model_namespace,\r\n        volumeResourceName=mount_name,\r\n    )\r\n\r\n    serve_op = components.load_component_from_file(\"src/pipelines/yamls/Components/kserve_launcher.yaml\")\r\n    serve_task = serve_op(\r\n        action=\"apply\",\r\n        inferenceservice_yaml=yaml.dump(infer_service),\r\n    )\r\n    \r\n    return serve_task\r\n\r\n\r\n@dsl.pipeline(\r\n    name=\"MNIST E2E Test\",\r\n    description=\"Testin MNIST end to end pipeline\"\r\n)\r\ndef pipeline(name=\"mnist-e2e-test-ridhwan\", training_steps=\"200\"):\r\n    name = \"{{workflow.name}}-%s\" % name\r\n    namespace=\"{{workflow.namespace}}\"\r\n    katib_task = katib_experiment_factory(name, namespace, training_steps)\r\n\r\n    volume_pvc = get_or_create_pvc(\"Create Ridhwan Volumes Simple\", \"4Gi\", \"ridhwan-pvc-mount-four\")\r\n    volume_name = volume_pvc.outputs[\"name\"]\r\n\r\n    # Convert Params\r\n    convert_params_op = func_to_container_op(convert_hyperparams)\r\n    convert_params_task = convert_params_op(katib_task.output)\r\n\r\n    # TFJob\r\n    tfjob_task = create_tfjob_task(name, namespace, training_steps, convert_params_task.output, volume_name)\r\n\r\n\r\n    # Serve\r\n    serve_task = create_serve_task(name, namespace, volume_name).after(tfjob_task)\r\n"
  },
  {
    "repo": "raldebsi/Kubeflow-Experimental-Pipelines",
    "file_path": "src/pipelines/end_to_end/mnist_simple.py",
    "raw_url": "https://raw.githubusercontent.com/raldebsi/Kubeflow-Experimental-Pipelines/master/src/pipelines/end_to_end/mnist_simple.py",
    "content": "from kserve import KServeClient\r\nimport kfp\r\nfrom kfp import components, dsl\r\nfrom kfp.components import func_to_container_op\r\nfrom kubeflow.katib import (ApiClient, V1beta1AlgorithmSpec,\r\n                            V1beta1ExperimentSpec, V1beta1FeasibleSpace,\r\n                            V1beta1ObjectiveSpec, V1beta1ParameterSpec,\r\n                            V1beta1TrialParameterSpec, V1beta1TrialTemplate)\r\nimport yaml\r\n\r\nfrom src.pipelines.common_utils import get_or_create_pvc, spec_from_file_format\r\nimport json\r\n\r\ndef katib_experiment_factory(experiment, namespace, steps):\r\n    max_trial_count = 5\r\n    max_failed_trial_count = 3\r\n    parallel_trial_count = 2\r\n\r\n    objective = V1beta1ObjectiveSpec( # Objective is to minimize the loss\r\n        type=\"minimize\",\r\n        goal=0.001,\r\n        objective_metric_name=\"loss\"\r\n    )\r\n\r\n    algorithm = V1beta1AlgorithmSpec(\r\n        algorithm_name=\"random\"\r\n    )\r\n\r\n    param_learning_rate = V1beta1ParameterSpec(\r\n            name=\"learning_rate\",\r\n            parameter_type=\"double\",\r\n            feasible_space=V1beta1FeasibleSpace(\r\n                min=0.01,\r\n                max=0.05,\r\n        )\r\n    )\r\n    param_learning_rate_spec = V1beta1TrialParameterSpec(\r\n        name=\"learningRate\",\r\n        description=\"Learning rate for the training model\",\r\n        reference=\"learning_rate\",\r\n    )\r\n\r\n    param_batch_size = V1beta1ParameterSpec(\r\n        name=\"batch_size\",\r\n        parameter_type=\"int\",\r\n        feasible_space=V1beta1FeasibleSpace(\r\n            min=\"80\",\r\n            max=\"100\",\r\n        )        \r\n    )\r\n    param_batch_size_spec = V1beta1TrialParameterSpec(\r\n        name=\"batchSize\",\r\n        description=\"Batch size for the training model\",\r\n        reference=\"batch_size\",\r\n    )\r\n\r\n    params = [\r\n        param_learning_rate,\r\n        param_batch_size        \r\n    ]\r\n\r\n    trial_spec = spec_from_file_format(\"src/pipelines/yamls/Specs/TFJob.yaml\", trainStepsParamVal=steps)\r\n    trial_template = V1beta1TrialTemplate(\r\n        primary_container_name=\"tensorflow\",\r\n        trial_parameters=[\r\n            param_learning_rate_spec,\r\n            param_batch_size_spec\r\n        ],\r\n        trial_spec=trial_spec,\r\n    )\r\n\r\n    experiment_spec = V1beta1ExperimentSpec(\r\n        max_trial_count=max_trial_count,\r\n        max_failed_trial_count=max_failed_trial_count,\r\n        parallel_trial_count=parallel_trial_count,\r\n        objective=objective,\r\n        algorithm=algorithm,\r\n        parameters=params,\r\n        trial_template=trial_template,\r\n    )\r\n\r\n    katib_op = components.load_component_from_file(\"src/pipelines/yamls/Components/katib_launcher.yaml\")\r\n    katib_task = katib_op(\r\n        experiment_name=experiment,\r\n        experiment_namespace=namespace,\r\n        experiment_spec=ApiClient().sanitize_for_serialization(experiment_spec),\r\n        experiment_timeout_minutes=60,\r\n        delete_finished_experiment=False\r\n    )\r\n\r\n    return katib_task\r\n\r\ndef convert_hyperparams(hyperparams) -> str:\r\n    import json\r\n    results = json.loads(hyperparams)\r\n    print(\"Hyperparams FineTuned: \", results)\r\n    best_params = []\r\n    for param in results[\"currentOptimalTrial\"][\"parameterAssignments\"]:\r\n        if param[\"name\"] == \"learning_rate\":\r\n            best_params.append(\"--tf-learning-rate={}\".format(param[\"value\"]))\r\n        elif param[\"name\"] == \"batch_size\":\r\n            best_params.append(\"--tf-batch-size={}\".format(param[\"value\"]))\r\n    print(\"Best Params\", best_params)\r\n    return \" \".join(best_params)\r\n\r\ndef create_tfjob_task(job_name, job_namespace, steps, mount_name, hyperparams):\r\n    tfjob_chief_spec = spec_from_file_format(\"src/pipelines/yamls/Specs/TFJobChief.yaml\",\r\n        trainingStepsParamVal=steps,\r\n        bestHPsParamVal=hyperparams,\r\n        volumeResourceName=mount_name\r\n    )\r\n    tfjob_worker_spec = spec_from_file_format(\"src/pipelines/yamls/Specs/TFJobWorker.yaml\",\r\n        trainingStepsParamVal=steps,\r\n        bestHPsParamVal=hyperparams,\r\n    )\r\n\r\n    tfjob_op = components.load_component_from_file(\"src/pipelines/yamls/Components/tfjob_launcher.yaml\")\r\n\r\n    tfjob_task = tfjob_op(\r\n        name=job_name,\r\n        namespace=job_namespace,\r\n        chief_spec=json.dumps(tfjob_chief_spec),\r\n        worker_spec=json.dumps(tfjob_worker_spec),\r\n        tfjob_timeout_minutes=60,\r\n        delete_finished_tfjob=False\r\n    )\r\n\r\n    return tfjob_task\r\n\r\ndef create_serve_task(model_name, model_namespace, mount_name):\r\n    infer_service = spec_from_file_format(\r\n        \"src/pipelines/yamls/Specs/KFServe.yaml\",\r\n        apiVersion=\"kserve3/beta4\",\r\n        modelName=model_name,\r\n        modelNamespace=model_namespace,\r\n        volumeResourceName=mount_name,\r\n    )\r\n\r\n    serve_op = components.load_component_from_file(\"src/pipelines/yamls/Components/kserve_launcher.yaml\")\r\n    serve_task = serve_op(\r\n        action=\"apply\",\r\n        inferenceservice_yaml=yaml.dump(infer_service),\r\n    )\r\n    \r\n    return serve_task\r\n\r\n\r\n@dsl.pipeline(\r\n    name=\"MNIST E2E Test\",\r\n    description=\"Testin MNIST end to end pipeline\"\r\n)\r\ndef pipeline(name=\"mnist-e2e-test-ridhwan\", training_steps=\"2\"):\r\n    name = \"{{workflow.name}}-%s\" % name\r\n    namespace=\"{{workflow.namespace}}\"\r\n\r\n    volume_pvc = get_or_create_pvc(\"Create Ridhwan Volumes Simple\", \"4Gi\", \"ridhwan-pvc-mount-four\")\r\n    volume_name = volume_pvc.outputs[\"name\"]\r\n\r\n    # Convert Params\r\n    # convert_params_op = func_to_container_op(convert_hyperparams)\r\n    # convert_params_task = convert_params_op()\r\n\r\n    # TFJob\r\n    tfjob_task = create_tfjob_task(name, namespace, training_steps, volume_name, \"--tf-learning-rate=0.03369294498160041 --tf-batch-size=93\")\r\n\r\n\r\n    # Serve\r\n    serve_task = create_serve_task(name, namespace, volume_name).after(tfjob_task)\r\n"
  },
  {
    "repo": "raldebsi/Kubeflow-Experimental-Pipelines",
    "file_path": "src/pipelines/end_to_end/serve_mnist_premade.py",
    "raw_url": "https://raw.githubusercontent.com/raldebsi/Kubeflow-Experimental-Pipelines/master/src/pipelines/end_to_end/serve_mnist_premade.py",
    "content": "import kfp\nimport yaml\nfrom kfp import components\nfrom src.pipelines.common_utils import cacheless_task, spec_from_file_format\n\n\ndef create_serve_task():\n   \n    infer_service = spec_from_file_format(\n        \"src/pipelines/yamls/Specs/KServe_MNIST.yaml\",\n        modelNamespace=\"{{workflow.namespace}}\",\n    )\n\n    serve_op = components.load_component_from_file(\"src/pipelines/yamls/Components/kserve_launcher.yaml\")\n    serve_task = serve_op(\n        action=\"apply\",\n        inferenceservice_yaml=yaml.dump(infer_service), \n    ) \n    \n    return serve_task\n\n@kfp.dsl.pipeline(\n    name=\"MNIST Example Server\",\n    description=\"Serves the public MNIST torchserve example\"\n)\ndef pipeline(): \n    serve_task = create_serve_task()\n    cacheless_task(serve_task)\n"
  },
  {
    "repo": "raldebsi/Kubeflow-Experimental-Pipelines",
    "file_path": "src/pipelines/end_to_end/serve_pt.py",
    "raw_url": "https://raw.githubusercontent.com/raldebsi/Kubeflow-Experimental-Pipelines/master/src/pipelines/end_to_end/serve_pt.py",
    "content": "from typing import NamedTuple\n\nimport kfp\nimport yaml\nfrom kfp import components\nfrom kfp.components import func_to_container_op\nfrom src.pipelines.common_utils import (add_pvolumes_func, cacheless_task, generate_random_hex_service, get_volume_by_name, sanitize_service, setup_volume,\n                                        spec_from_file_format)\n\n\ndef create_serve_task(dataset_name: str, experiment_name: str, mount_name: str, randomize_service_suffix: bool, use_seed: bool):\n    # gen_name_comp = func_to_container_op(generate_inference_service_name)\n    # gen_name_task = gen_name_comp(dataset_name, experiment_name)\n\n    sane_service_name = sanitize_service(experiment_name, randomize_service_suffix)\n    randomSeed = generate_random_hex_service(use_seed)\n    \n    infer_service = spec_from_file_format(\n        \"src/pipelines/yamls/Specs/KServe_HF.yaml\",\n        modelNamespace=\"{{workflow.namespace}}\",\n        serviceName=sane_service_name,\n        volumeResourceName=mount_name,\n        experimentName=experiment_name,\n        datasetName=dataset_name,\n        randomSeed=randomSeed, # Prevent reuse of revision\n    )\n\n    serve_op = components.load_component_from_file(\"src/pipelines/yamls/Components/kserve_launcher.yaml\")\n    serve_task = serve_op(\n        action=\"apply\", \n        inferenceservice_yaml=yaml.dump(infer_service),\n        # model_name=sane_service_name,\n        # model_uri=\"pvc://{}/{}\".format(mount_name, experiment_name),\n        # framework=\"pytorch\",\n        # namespace=\"{{workflow.namespace}}\",\n        # enable_istio_sidecar=False,\n    )\n    \n    return serve_task\n\n\ndef generate_inference_service_name(dataset_name, experiment_name) -> str:\n    return \"{}-{}\".format(dataset_name, experiment_name).lower().replace('_', '-')\n \nclass OHandler(NamedTuple):\n    temp_path: str\n    handler_path: str\n    requirements_path: str\n\ndef create_handler(handler_code: str, experiment_name: str, root_path: str, additional_requirements: list) -> OHandler:\n    import os\n\n    temp_path = os.path.join(root_path, experiment_name, \"handler\")\n    os.makedirs(temp_path, exist_ok=True)\n    handler = os.path.join(temp_path, \"handler.py\")\n    requirements = os.path.join(temp_path, \"requirements.txt\")\n\n    print(\"Temp Path\", temp_path)\n    print(\"Handler\", handler)\n    print(\"Requirements\", requirements)\n\n    with open(handler, \"w\", encoding='utf-8') as f:\n        f.write(handler_code)\n    \n    with open(requirements, \"w\", encoding='utf-8') as f:\n        for r in additional_requirements:\n            f.write(\"{}\\n\".format(r))\n\n    return temp_path, handler, requirements # type: ignore\n\ndef create_mar_config(experiment_name: str, root_path: str, mar_file: str, model_name: str = \"\", model_version: str = \"1.0\", # Pipeline Config\n        threads_count: int = 4, job_queue_size: int = 10, install_dependencies: bool = True, is_default: bool = True, # Config Config\n        workers_count: int = 1, workers_max: int = 5, batch_size: int = 1, timeout: int = 120, # Model Config\n) -> None:\n    import os\n    import json\n\n    mar_file = mar_file if mar_file.endswith(\".mar\") else mar_file + \".mar\"\n    model_name = model_name or mar_file.rsplit(\".\", 1)[0] # If Empty replace with mar file\n    model_name = model_name.replace(\"-\", \"\").lower()\n\n    experiment_path = os.path.join(root_path, experiment_name)\n    print(\"Experiment Path is\", experiment_path)\n    model_store = os.path.join(experiment_path, \"model-store\")\n    config_dir = os.path.join(experiment_path, \"config\")\n    config_path = os.path.join(config_dir, \"config.properties\")\n\n    previous_models = {}\n\n    os.makedirs(config_dir, exist_ok=True)\n\n    if os.path.isfile(config_path):\n        with open(config_path, \"r\", encoding='utf-8') as f:\n            for line in f:\n                line = line.strip()\n                if line[0] == \"#\":\n                    continue\n                key, val = line.split(\"=\")\n                if key == \"model_snapshot\":\n                    previous_models = json.loads(val.strip())\n    if previous_models:\n        print(\"Found Previous Snapshot\")\n        print(previous_models)\n    # mar_files = [ file.rsplit(\".\", 1)[0]\n    #     for file in os.listdir(model_store)\n    #     if file.lower().endswith(\".mar\")\n    # ]\n\n    if model_version in previous_models.get(\"models\", {}).get(model_name, {}):\n        replace_mode = True\n    else:\n        replace_mode = False\n\n    model_snapshot = {\n        \"name\": \"startup.cfg\",\n        \"modelCount\": previous_models.get(\"modelCount\", 0) + (1 if not replace_mode else 0),\n        \"models\": previous_models.get(\"models\", {}) \n    }\n\n    model_dict = model_snapshot[\"models\"].setdefault(model_name, {})\n    if is_default:\n        print(\"Setting Default Model to\", model_version)\n        for model_version_dict in model_snapshot[\"models\"][model_name].values():\n            model_version_dict[\"defaultVersion\"] = False # Reset all to false since I am the new default\n    else:\n        print(\"Default model unchanged\")\n    \n    model_dict[model_version] = {\n        \"defaultVersion\": is_default,\n        \"marName\": mar_file,\n        \"minWorkers\": workers_count,\n        \"maxWorkers\": workers_max,\n        \"batchSize\": batch_size,\n        \"maxBatchDelay\": 5000,\n        \"responseTimeout\": timeout,\n    }\n\n    config_spec = {\n        \"inference_address\": \"http://0.0.0.0:8085\",\n        \"management_address\": \"http://0.0.0.0:8085\",\n        \"metrics_address\": \"http://0.0.0.0:8082\",\n        \"enable_metrics_api\": True,\n        \"metrics_format\": \"prometheus\",\n        \"number_of_netty_threads\": threads_count,\n        \"job_queue_size\": job_queue_size,\n        \"model_store\": \"/mnt/models/model-store\",\n        \"model_snapshot\": json.dumps(model_snapshot),\n        \"install_py_dep_per_model\": install_dependencies,\n    }\n\n    print(\"Saving config to\", config_path)\n    with open(config_path, \"w\", encoding='utf-8') as f:\n        for key, val in config_spec.items():\n            if isinstance(val, (set, tuple)):\n                val = list(val)\n            if isinstance(val, (bool, dict, list)):\n                val = json.dumps(val)\n            s = \"{}={}\".format(key, val)\n            print(s)\n            f.write(s + \"\\n\")\n        f.write(\"# AutoGenerated by KFP.\" \"{{workflow.name}}\")\n    \n\n\n\ndef move_mar_model(model_name: str, experiment_name: str, root_path: str, temp_path: str) -> None: # Can be replaced with text component\n    import os\n    import shutil\n\n    mar_name = \"{}.mar\".format(model_name)\n    model_store_path = os.path.join(root_path, experiment_name, \"model-store\")\n    mar_model = os.path.join(temp_path, mar_name)\n    \n    if os.path.exists(mar_model):\n        os.makedirs(model_store_path, exist_ok=True)\n        move_path = os.path.join(root_path, model_store_path)\n        file_path = os.path.join(move_path, mar_name)\n        if os.path.exists(file_path):\n            os.remove(file_path)\n        shutil.move(mar_model, move_path)\n    else:\n        raise Exception(\"Model not found at\" + mar_model)        \n    \n    \nclass OMarFiles(NamedTuple):\n    model_path: str\n    extra_files: str\n\ndef get_mar_required_files(experiment_name: str, dataset_name: str, root_path: str) -> OMarFiles:\n    import os\n    model_folder = os.path.join(root_path, experiment_name, \"outputs\", dataset_name)\n    model = os.path.join(model_folder, \"pytorch_model.bin\")\n    vocab = os.path.join(model_folder, \"vocab.txt\")\n    config = os.path.join(model_folder, \"config.json\")\n\n    return model, \"{},{}\".format(vocab, config) # type: ignore\n\n@kfp.dsl.pipeline(\n    name=\"End to End Hugging Face Topic Classifier - Serving\",\n    description=\"The Serving part of the E2E HF Topic Classifier - DEBUG ONLY\"\n)\ndef pipeline(experiment_name: str, volume_name: str, dataset_name: str,\n    model_version: str = \"1.0\", randomize_service_suffix: bool = False, use_seed: bool = True, additional_requirements: list = [\"anltk\",\"torchserve\",\"transformers\",],\n    model_serve_name: str = \"\", model_serve_threads_count: int = 4, model_serve_queue_size: int = 10,\n    model_serve_install_dependencies: bool = True, model_serve_is_default: bool = True, model_serve_workers: int = 1, model_serve_workers_max: int = 5,\n    model_serve_batch_size: int = 1, model_serve_timeout: int = 120,\n): \n    mount_dir = \"/store\"\n    volumetrize = setup_volume(volume_name, mount_dir)\n\n    # Convert this to a utility function\n    handler_import_path = \"src/handlers/topic_class.py\"\n    handler_code = open(handler_import_path, encoding='utf-8').read() \n\n    handler_op = components.func_to_container_op(create_handler)\n    handler_task = handler_op(handler_code, experiment_name, mount_dir, additional_requirements)\n    handler_task = volumetrize(handler_task) # To allow saving\n\n    get_mar_op = components.func_to_container_op(get_mar_required_files)\n    get_mar_task = get_mar_op(experiment_name, dataset_name, mount_dir)\n\n    model_path = get_mar_task.outputs[\"model_path\"]\n    extra_files = get_mar_task.outputs[\"extra_files\"]\n    model_name = dataset_name\n\n    handler_path = handler_task.outputs[\"handler_path\"]\n    requirements_path = handler_task.outputs[\"requirements_path\"]\n    out_temp_path = handler_task.outputs[\"temp_path\"]\n\n    mar_convert_op = components.load_component_from_file(\"src/pipelines/yamls/Components/hf_make_mar_file.yaml\")\n    mar_convert_task = mar_convert_op(\n        model_name = model_name,\n        model_version = model_version,\n        export_path = out_temp_path,\n        model_file = model_path,\n        extra_files = extra_files,\n        handler_file = handler_path,\n        requirements_file = requirements_path, \n    )\n    mar_convert_task = volumetrize(mar_convert_task)\n    cacheless_task(mar_convert_task)\n\n    move_model_op = components.func_to_container_op(move_mar_model)\n    move_model_task = move_model_op(model_name, experiment_name, mount_dir, out_temp_path)\n    move_model_task = volumetrize(move_model_task)\n    move_model_task = move_model_task.after(mar_convert_task).after(get_mar_task)\n    cacheless_task(move_model_task)\n \n    # Create Config\n    create_config_op = components.func_to_container_op(create_mar_config)\n    create_config_task = create_config_op(\n        experiment_name, mount_dir, dataset_name,\n        model_serve_name, model_version, model_serve_threads_count, model_serve_queue_size,\n        model_serve_install_dependencies, model_serve_is_default, model_serve_workers, model_serve_workers_max,\n        model_serve_batch_size, model_serve_timeout\n    )\n    create_config_task = volumetrize(create_config_task)\n\n\n    serve_task = create_serve_task(dataset_name, experiment_name, volume_name, randomize_service_suffix, use_seed)\n    serve_task = volumetrize(serve_task)\n    serve_task = serve_task.after(move_model_task).after(create_config_task)\n    cacheless_task(serve_task)\n"
  },
  {
    "repo": "raldebsi/Kubeflow-Experimental-Pipelines",
    "file_path": "src/pipelines/end_to_end/topic_class.py",
    "raw_url": "https://raw.githubusercontent.com/raldebsi/Kubeflow-Experimental-Pipelines/master/src/pipelines/end_to_end/topic_class.py",
    "content": "import json\nfrom typing import NamedTuple, Optional\n\nimport kfp\nimport yaml\nfrom kfp import components\nfrom kfp.components import func_to_container_op\nfrom src.pipelines.common_utils import (add_pvolumes_func, get_volume_by_name,\n                                        spec_from_file_format)\n\n\ndef get_run_args(dataset_name, has_test, seq_len, batch_size_dev, learn_rate, epochs, seed, experiment_name) -> NamedTuple(\n    \"Outputs\",\n    [\n        (\"train_args\", dict),\n        (\"extra_args\", dict),\n    ]\n):\n    import json\n    import os\n    args = {\"extra\": {}}\n\n    args[\"save_steps\"] = 1000\n    args[\"extra\"][\"overwrite_output_dir\"] = \"\"\n\n    model_name_or_path = \"CAMeL-Lab/bert-base-arabic-camelbert-msa-sixteenth\"\n    args[\"model_name_or_path\"] = model_name_or_path\n    \n    train_file = '/store/datasets/{}/train.json'.format(dataset_name)\n    args[\"train_file\"] = train_file\n    args[\"extra\"][\"do_train\"] = \"\"\n    \n    \n    valid_file = '/store/datasets/{}/valid.json'.format(dataset_name)\n    args[\"extra\"][\"validation_file\"] = valid_file\n    args[\"extra\"][\"do_eval\"] = \"\"\n\n    if has_test == True or (isinstance(has_test, str) and has_test.lower() in \"true1yes\"):\n        test_file = '/store/datasets/{}/test.json'.format(dataset_name)\n        args[\"extra\"][\"test_file\"] = test_file\n        args[\"extra\"][\"do_predict\"] = \"\"\n        \n    \n    args[\"max_seq_length\"] = seq_len\n    \n    args[\"per_device_train_batch_size\"] = batch_size_dev\n    \n    args[\"learning_rate\"] = learn_rate\n    \n    args[\"num_train_epochs\"] = epochs\n    \n    output_dir = '/store/{}/outputs/{}'.format(experiment_name, dataset_name)\n    os.makedirs(output_dir, exist_ok=True)\n    args[\"output_dir\"] = output_dir\n    \n    if seed:\n        args[\"extra\"][\"seed\"] = hash(seed) # Hash of int is the same as int, hash of str is int\n\n    # write the args to a file\n    with open(os.path.join(output_dir, \"{}-{}-best-hps.json\".format(experiment_name, dataset_name)), \"w\") as f:\n        json.dump(args, f)\n\n\n    print(\"Args:\")\n    print(args)\n\n    # convert args to string\n    # return \" \".join(\"--{} {}\".format(k, v) for k, v in args.items())\n\n    return args, args.pop(\"extra\")\n\ndef create_tfjob_task(job_name, hyperparams, mount_name):\n    tfjob_chief_spec = spec_from_file_format(\"src/pipelines/yamls/Specs/HFChief.yaml\",\n        trainParamValues=hyperparams,\n        volumeResourceName=mount_name,\n    )\n    tfjob_worker_spec = spec_from_file_format(\"src/pipelines/yamls/Specs/HFWorker.yaml\",\n        trainParamValues=hyperparams,\n    )\n\n    tfjob_op = components.load_component_from_file(\"src/pipelines/yamls/Components/tfjob_launcher.yaml\")\n\n    tfjob_task = tfjob_op(\n        name=str(job_name) + \"-{{workflow.name}}\",\n        namespace=\"{{workflow.namespace}}\",\n        chief_spec=json.dumps(tfjob_chief_spec),\n        worker_spec=json.dumps(tfjob_worker_spec),\n        tfjob_timeout_minutes=60,\n        delete_finished_tfjob=False\n    )\n\n    return tfjob_task\n\n\n\ndef hf_task(args: str, is_print=False):\n    file = \"src/pipelines/yamls/Components/hf_trainer_internal.yaml\"\n    if is_print:\n        with open(file, \"r\", encoding=\"utf-8\") as f:\n            hfjob_op = yaml.safe_load(f)\n            commands: list = hfjob_op[\"implementation\"][\"container\"][\"args\"]\n            name: str = hfjob_op[\"name\"]\n            description = hfjob_op[\"description\"]\n            first_arg = \"echo \" + commands.pop(0)\n            commands.insert(0, first_arg)\n            description = \"Prints the expected arguments and commands for \" + name\n            name = \"Print \" + name\n            hfjob_op[\"implementation\"][\"container\"][\"args\"] = commands\n            hfjob_op[\"name\"] = name\n            hfjob_op[\"description\"] = description\n            hfjob_op = components.load_component_from_text(yaml.dump(hfjob_op))\n    else:\n        hfjob_op = components.load_component_from_file(file)\n\n    return hfjob_op(params=args)\n\n\ndef convert_run_args_to_str(train_args: dict, extra_args: dict) -> str:\n    print(\"Args\")\n    print(train_args)\n    print(\"Extra Args\")\n    print(extra_args)\n    e_args = \" \".join(\"--{} {}\".format(k, v) if v != \"\" else \"--\" + k for k, v in extra_args.items())\n    p_args = \" \".join(\"--{} {}\".format(k, v) if v != \"\" else \"--\" + k for k, v in train_args.items())\n    print(\"Formatted\")\n    print(p_args)\n    print(e_args)\n\n    return \"{} {}\".format(p_args, e_args)\n\n\ndef convert_run_args(args: dict, extra_args: dict) -> NamedTuple(\n    \"Outputs\",\n    [\n        (\"model\", str),\n        (\"tf\", str),\n        (\"seq\", int),\n        (\"batch\", int),\n        (\"lr\", float),\n        (\"epoch\", int),\n        (\"out\", str),\n        (\"save\", int),\n        (\"extra_args\", str)\n    ]\n    ):\n    print(\"Input Dict\")\n    print(args)\n    return \\\n        str     (args[\"model_name_or_path\"]),           \\\n        str     (args[\"train_file\"]),                   \\\n        int     (args[\"max_seq_length\"]),               \\\n        int     (args[\"per_device_train_batch_size\"]),  \\\n        float   (args[\"learning_rate\"]),                \\\n        int     (args[\"num_train_epochs\"]),             \\\n        str     (args[\"output_dir\"]),                   \\\n        int     (args[\"save_steps\"]),                   \\\n        \" \".join(\"--{} {}\".format(k, v) if v != \"\" else \"--\" + k for k, v in extra_args.items())\n\ndef create_serve_task(model_name, experiment_name, mount_name):\n    model_namespace = \"{{workflow.namespace}}\"\n    model_name = \"{}_{}\".format(model_name, experiment_name)\n\n    infer_service = spec_from_file_format(\n        \"src/pipelines/yamls/Specs/KFServe.yaml\",\n        apiVersion=\"kserve3/beta4\",\n        modelName=model_name,\n        modelNamespace=model_namespace,\n        volumeResourceName=mount_name,\n    )\n\n    serve_op = components.load_component_from_file(\"src/pipelines/yamls/Components/kserve_launcher.yaml\")\n    serve_task = serve_op(\n        action=\"apply\",\n        inferenceservice_yaml=yaml.dump(infer_service),\n    )\n    \n    return serve_task\n\n@kfp.dsl.pipeline(\n    name=\"End to End Hugging Face Topic Classifier\",\n    description=\"End to End Topic Classiciation using HuggingFace Framework and CamelBert model\"\n)\ndef pipeline(experiment_name: str, volume_name: str,\n                dataset_name: str, has_test: bool = False,\n                max_sequence_length: int = 512, device_batch_size: int = 8,\n                learning_rate: float = 3e-5, epochs: int = 5, seed: Optional[int] = None):\n \n    mount_vol = get_volume_by_name(volume_name, \"volume-bind\")\n    mount_dict = {\"/store\": mount_vol}\n    volumetrize = add_pvolumes_func(mount_dict)\n\n    \n    get_args_comp = func_to_container_op(get_run_args) # Simulate Katib\n    get_args_task = get_args_comp(dataset_name, has_test, max_sequence_length, device_batch_size, learning_rate, epochs, seed, experiment_name)\n    get_args_task = volumetrize(get_args_task)\n\n    convert_args_comp = func_to_container_op(convert_run_args_to_str)\n    convert_args_task = convert_args_comp(\n        get_args_task.outputs[\"train_args\"],\n        get_args_task.outputs[\"extra_args\"],\n    )\n\n\n    train_params = convert_args_task.output\n\n    print_train_task =  hf_task(\n        train_params,\n        True\n    )\n\n    train_task = hf_task(train_params)\n    train_task = volumetrize(train_task)\n\n\n    # serve_task = create_serve_task(dataset_name, experiment_name, volume_name).after(train_task)\n"
  },
  {
    "repo": "raldebsi/Kubeflow-Experimental-Pipelines",
    "file_path": "src/pipelines/volume_test/read.py",
    "raw_url": "https://raw.githubusercontent.com/raldebsi/Kubeflow-Experimental-Pipelines/master/src/pipelines/volume_test/read.py",
    "content": "import kfp\r\nimport kfp.compiler\r\nimport kfp.components\r\nfrom kfp.dsl._pipeline_param import sanitize_k8s_name\r\nfrom kfp.dsl._pipeline_volume import PipelineVolume\r\nfrom src.pipelines.common_utils import get_volume_by_name, add_pvolumes_func\r\n\r\n\r\ndef read_file(path: str):\r\n    import os\r\n    full_path = os.path.join(path, \"file.txt\")\r\n    print(\"Full path is {}\".format(full_path))\r\n    print(\"Current path is {}\".format(os.getcwd()))\r\n    with open(full_path, \"r\", encoding='utf-8') as f:\r\n        data = f.read()\r\n        print(data)\r\n        return data\r\n\r\n@kfp.dsl.pipeline(\r\n    name=\"Shared Volume Pipeline\",\r\n    description=\"Test reading from a consistent volume\",\r\n)\r\ndef read_volume_pipeline(data_path: str):\r\n    # mount_vol = kfp.dsl.VolumeOp(\r\n    #     name=\"Read Ridhwan Volumes\",\r\n    #     size=\"1Gi\",\r\n    #     modes=kfp.dsl.VOLUME_MODE_RWO,\r\n    #     resource_name=\"ridhwan-pvc-mount\", # PVC Name, will be {pipeline_name}-{id}-resource if name does not exist and generate_unique_name is True.\r\n    #     volume_name=\"volumesecond\",\r\n    #     generate_unique_name=False,\r\n    # )\r\n\r\n    mount_vol = get_volume_by_name(\"ridhwan-pvc-mount\")\r\n    data_store_path = data_path\r\n    pvolumes = {data_store_path: mount_vol}\r\n    volumetrize = add_pvolumes_func(pvolumes)\r\n\r\n    # Components\r\n    read_volume_component = kfp.components.create_component_from_func(read_file)\r\n    # Tasks\r\n    read_task = volumetrize(read_volume_component(data_store_path))\r\n"
  },
  {
    "repo": "raldebsi/Kubeflow-Experimental-Pipelines",
    "file_path": "src/pipelines/volume_test/write.py",
    "raw_url": "https://raw.githubusercontent.com/raldebsi/Kubeflow-Experimental-Pipelines/master/src/pipelines/volume_test/write.py",
    "content": "from typing import NamedTuple, List\r\nimport kfp\r\nimport kfp.compiler\r\nimport kfp.components\r\n\r\ndef write_file(contents: str, path: str):\r\n    import os\r\n    os.makedirs(path, exist_ok=True)\r\n    full_path = os.path.join(path, \"file.txt\")\r\n    with open(full_path, \"w\", encoding='utf-8') as f:\r\n        f.write(contents)\r\n\r\ndef read_file(path: str):\r\n    import os\r\n    full_path = os.path.join(path, \"file.txt\")\r\n    with open(full_path, \"r\", encoding='utf-8') as f:\r\n        return f.read()\r\n\r\n@kfp.dsl.pipeline(\r\n    name=\"Writing Into PVC Test\",\r\n    description=\"Test writing to a consistent volume\",\r\n)\r\ndef write_volume_pipeline():\r\n    from kfp import dsl\r\n\r\n    mount_vol = dsl.VolumeOp(\r\n        name=\"Create Ridhwan Volumes\", # Volume Unique Name, will be used as identifier to operation.\r\n                                               # If operation exists then it will be reused.\r\n                                               # If operation does not exist then it will be created and the name will be resource_name below.\r\n        size=\"1Gi\",\r\n        modes=dsl.VOLUME_MODE_RWO,\r\n        resource_name=\"ridhwan-pvc-mount\", # PVC Name, will be {pipeline_name}-{id}-resource if name does not exist and generate_unique_name is True.\r\n        # volume_name=\"ridhwan-personal-volume\", # Volume Name, do not use as it will cause the volume to not be found\r\n        # data_source=PipelineParam(name=\"data_source\"), # This way it will create a volume snapshot from param\r\n        generate_unique_name=False,\r\n    )\r\n\r\n    data_store_path = \"/data\"\r\n    pvolumes = {data_store_path: mount_vol.volume}\r\n    volumetrize = lambda func: func.add_pvolumes(pvolumes)\r\n\r\n    # Components\r\n    write_volume_component = kfp.components.create_component_from_func(write_file)\r\n    read_volume_component = kfp.components.create_component_from_func(read_file)\r\n    # Tasks\r\n    write_task = volumetrize(write_volume_component(\"This is a file created by the pipeline into {}\".format(\"/data\"), data_store_path))\r\n    read_task = volumetrize(read_volume_component(data_store_path))\r\n    read_task.after(write_task)"
  },
  {
    "repo": "markbarna/kubeflow-pipelines-poc",
    "file_path": "main.py",
    "raw_url": "https://raw.githubusercontent.com/markbarna/kubeflow-pipelines-poc/master/main.py",
    "content": "from kfp import components, dsl, compiler, Client\n\nfrom components.fetch_data import fetch_data, split_data\nfrom components.model import train\nfrom utils.git import create_version_name\n\nBASE_IMAGE = 'dabarnyarddawg/kf-pipelines-base-images:latest'\nCLIENT = 'http://127.0.0.1:8080'\nPIPELINE_NAME = 'cancer-classifier'\nEXPERIMENT_NAME = 'cancer_detection'\n\nfetch_data_op = components.create_component_from_func(fetch_data, base_image=BASE_IMAGE)\nsplit_data_op = components.create_component_from_func(split_data, base_image=BASE_IMAGE)\ntrain_op = components.create_component_from_func(train, base_image=BASE_IMAGE)\n\n\n@dsl.pipeline(name=PIPELINE_NAME, description='test classifier pipeline with breast cancer dataset')\ndef pipeline(test_size: float = 0.2):\n    fetch_data_task = fetch_data_op()\n    split_data_task = split_data_op(x=fetch_data_task.outputs['x'], y=fetch_data_task.outputs['y'], test_size=test_size)\n    # TODO: train model(s) (with tuning) in parallel?\n    train_task = train_op(x=split_data_task.outputs['x_train'], y=split_data_task.outputs['y_train'])\n    # TODO: batch predictions (move to separate pipeline)\n    # TODO: serve model\n\n\nif __name__ == '__main__':\n    client = Client(host=CLIENT)\n    client.create_run_from_pipeline_func(\n        pipeline, arguments={}, run_name=create_version_name(), experiment_name=f'{EXPERIMENT_NAME}_dev'\n    )\n    # TODO: github action to compile & deploy pipeline on release\n    # TODO: unit tests on commit\n    # TODO: connect artifacts to local storage mount\n"
  },
  {
    "repo": "hafizurcse/azure-kubeflow-pipeline",
    "file_path": "code/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/hafizurcse/azure-kubeflow-pipeline/main/code/pipeline.py",
    "content": "import kfp.dsl as dsl\r\nfrom kubernetes import client as k8s_client\r\n\r\n\r\n@dsl.pipeline(\r\n    name='Tacos vs. Burritos',\r\n    description='Simple TF CNN for binary classifier between burritos and tacos'\r\n)\r\ndef tacosandburritos_train(\r\n    tenant_id,\r\n    service_principal_id,\r\n    service_principal_password,\r\n    subscription_id,\r\n    resource_group,\r\n    workspace,\r\n    persistent_volume_name='azure',\r\n    persistent_volume_path='/mnt/azure',\r\n    data_download='https://aiadvocate.blob.core.windows.net/public/tacodata.zip',\r\n    epochs=5,\r\n    batch=32,\r\n    learning_rate=0.0001,\r\n    imagetag='latest',\r\n    model_name='tacosandburritos',\r\n    profile_name='tacoprofile'\r\n):\r\n\r\n    operations = {}\r\n    image_size = 160\r\n    training_folder = 'train'\r\n    training_dataset = 'train.txt'\r\n    model_folder = 'model'\r\n\r\n    # preprocess data\r\n    operations['preprocess'] = dsl.ContainerOp(\r\n        name='preprocess',\r\n        image='insert your image here',\r\n        command=['python'],\r\n        arguments=[\r\n            '/scripts/data.py',\r\n            '--base_path', persistent_volume_path,\r\n            '--data', training_folder,\r\n            '--target', training_dataset,\r\n            '--img_size', image_size,\r\n            '--zipfile', data_download\r\n        ]\r\n    )\r\n\r\n    #train\r\n    operations['training'] = dsl.ContainerOp(\r\n        name='training',\r\n        image='insert your image here',\r\n        command=['python'],\r\n        arguments=[\r\n            '/scripts/train.py',\r\n            '--base_path', persistent_volume_path,\r\n            '--data', training_folder, \r\n            '--epochs', epochs, \r\n            '--batch', batch, \r\n            '--image_size', image_size, \r\n            '--lr', learning_rate, \r\n            '--outputs', model_folder, \r\n            '--dataset', training_dataset\r\n        ]\r\n    )\r\n    operations['training'].after(operations['preprocess'])\r\n\r\n    # register model\r\n    operations['register'] = dsl.ContainerOp(\r\n        name='register',\r\n        image='insert your image here',\r\n        command=['python'],\r\n        arguments=[\r\n            '/scripts/register.py',\r\n            '--base_path', persistent_volume_path,\r\n            '--model', 'latest.h5',\r\n            '--model_name', model_name,\r\n            '--tenant_id', tenant_id,\r\n            '--service_principal_id', service_principal_id,\r\n            '--service_principal_password', service_principal_password,\r\n            '--subscription_id', subscription_id,\r\n            '--resource_group', resource_group,\r\n            '--workspace', workspace\r\n        ]\r\n    )\r\n    operations['register'].after(operations['training'])\r\n\r\n    operations['profile'] = dsl.ContainerOp(\r\n        name='profile',\r\n        image='insert your image here',\r\n        command=['sh'],\r\n        arguments=[\r\n            '/scripts/profile.sh',\r\n            '-n', profile_name,\r\n            '-m', model_name,\r\n            '-i', '/scripts/inferenceconfig.json',\r\n            '-d', '{\"image\":\"https://www.exploreveg.org/files/2015/05/sofritas-burrito.jpeg\"}',\r\n            '-t', tenant_id,\r\n            '-r', resource_group,\r\n            '-w', workspace,\r\n            '-s', service_principal_id,\r\n            '-p', service_principal_password,\r\n            '-u', subscription_id,\r\n            '-b', persistent_volume_path\r\n        ]\r\n    )\r\n    operations['profile'].after(operations['register'])\r\n\r\n    operations['deploy'] = dsl.ContainerOp(\r\n        name='deploy',\r\n        image='insert your image here',\r\n        command=['sh'],\r\n        arguments=[\r\n            '/scripts/deploy.sh',\r\n            '-n', model_name,\r\n            '-m', model_name,\r\n            '-i', '/scripts/inferenceconfig.json',\r\n            '-d', '/scripts/deploymentconfig.json',\r\n            '-t', tenant_id,\r\n            '-r', resource_group,\r\n            '-w', workspace,\r\n            '-s', service_principal_id,\r\n            '-p', service_principal_password,\r\n            '-u', subscription_id,\r\n            '-b', persistent_volume_path\r\n        ]\r\n    )\r\n    operations['deploy'].after(operations['profile'])\r\n\r\n    for _, op in operations.items():\r\n        op.container.set_image_pull_policy(\"Always\")\r\n        op.add_volume(\r\n            k8s_client.V1Volume(\r\n                name='azure',\r\n                persistent_volume_claim=k8s_client.V1PersistentVolumeClaimVolumeSource(\r\n                    claim_name='azure-managed-disk')\r\n                )\r\n            ).add_volume_mount(k8s_client.V1VolumeMount(\r\n                mount_path='/mnt/azure', \r\n                name='azure')\r\n            )\r\n\r\n\r\nif __name__ == '__main__':\r\n   import kfp.compiler as compiler\r\n   compiler.Compiler().compile(tacosandburritos_train, __file__ + '.tar.gz')\r\n"
  },
  {
    "repo": "fontaine-raphael/kubeflow",
    "file_path": "pipelines/nyc_taxi.py",
    "raw_url": "https://raw.githubusercontent.com/fontaine-raphael/kubeflow/master/pipelines/nyc_taxi.py",
    "content": "import kfp.compiler as compiler\nimport kfp.components as comp\nimport kfp.dsl as dsl\nimport kfp.gcp as gcp\nimport os\n\n\nextract_path = '/Users/fontaine/projects/kubeflow/components/yellow-taxis-nyc/extract'\npreprocessing_path = '/Users/fontaine/projects/kubeflow/components/yellow-taxis-nyc/pre-processing'\n\nextract_op = comp.load_component_from_file(os.path.join(extract_path, 'component.yaml'))\npreprocessing_op = comp.load_component_from_file(os.path.join(preprocessing_path, 'component.yaml'))\n\n@dsl.pipeline(name='NYC Yellow Taxi Fare Predict', description='Pipeline to predict the fare amount of NYC Yellow Cab.')\ndef nyc_taxi_pipeline(\n    project='kubeflow-xyz',\n    dataset='yellow_taxi',\n    bucket='gs://yellow-taxi-nyc',\n    start_date='2015-01-01',\n    end_date='2015-01-05'\n):\n    extract = extract_op(\n        project=project,\n        dataset=dataset,\n        bucket=bucket,\n        start_date=start_date,\n        end_date=end_date\n    ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n\n    preprocessing = preprocessing_op(\n        project=project,\n        staging_bucket=extract.outputs['staging_bucket']\n    ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n\n# Compile\npipeline_func = nyc_taxi_pipeline\npipeline_filename = pipeline_func.__name__ + \".tar.gz\"\ncompiler.Compiler().compile(pipeline_func, pipeline_filename)\nprint(pipeline_filename)\n"
  },
  {
    "repo": "rahuja23/Blogpost_kubeflow_pipeline",
    "file_path": "Pipeline/kfp_v2.py",
    "raw_url": "https://raw.githubusercontent.com/rahuja23/Blogpost_kubeflow_pipeline/master/Pipeline/kfp_v2.py",
    "content": "import kfp\nimport kfp.dsl as dsl\nfrom kfp.v2.dsl import (\n    component,\n    Input,\n    Output,\n    Dataset,\n    Artifact,\n    InputPath,\n    OutputPath,\n)\nfrom kfp import compiler\nimport subprocess\nfrom kfp.aws import use_aws_secret\nfrom typing import NamedTuple\n\n@component(base_image=\"racahu23/blog_mlflow:1\", packages_to_install=['mlflow'])\ndef mlflow_setup_experiment(tracking_uri:str)->NamedTuple('Outputs',[(\"exp_id\", str), (\"run_id\", str)]):\n    import main\n    op = main.main(tracking_uri)\n    return op\n@component(base_image=\"racahu23/preprocess:blog\",  packages_to_install=['mlflow', 'boto3'])\ndef twitter_download_preprocess(information: Output[Artifact],  experiment_id: str, run_id: str, tracking_uri: str):\n    from main import twitter_sample_download_and_preprocess\n    args={\n        \"experiment_id\": experiment_id,\n        \"run_id\": run_id,\n        \"tracking_uri\": tracking_uri,\n        \"log_folder\": information.path\n    }\n    twitter_sample_download_and_preprocess(args)\n\n@component(base_image=\"racahu23/numpy:blog_final\",  packages_to_install=['mlflow', 'boto3'])\ndef numpy_process(information: Input[Artifact], information_output: Output[Artifact],  experiment_id: str, run_id: str,\n                  tracking_uri: str):\n    from main import numpy_process\n    args = {\n        \"experiment_id\": experiment_id,\n        \"run_id\": run_id,\n        \"tracking_uri\": tracking_uri,\n        \"log_folder\": information.path,\n        \"output_dir\": information_output.path\n           }\n    op = numpy_process(args)\n    return op\n@component(base_image=\"racahu23/scikit:3\", packages_to_install=['boto3'])\ndef sklearn_logistic(information_input: Input[Artifact], experiment_id: str,\n                     tracking_uri: str, run_id: str,  sklearn_output: Output[Artifact]):\n    from main import sklearn_logistic\n    args = {\n        \"experiment_id\": experiment_id,\n        \"tracking_uri\": tracking_uri,\n        \"log_folder\":  information_input.path,\n        \"output_dir\": sklearn_output.path,\n        \"run_id\": run_id\n    }\n    op = sklearn_logistic(args)\n@component(base_image=\"racahu23/logistic:2\",   packages_to_install=['mlflow', 'boto3'])\ndef logistic_op(sklearn_input:Input[Artifact], logistic_output: Output[Artifact],  experiment_id: str, run_id: str,\n                tracking_uri: str):\n    from main import logistic\n    args = {\n        \"experiment_id\": experiment_id,\n        \"run_id\": run_id,\n        \"tracking_uri\": tracking_uri,\n        \"log_folder\": sklearn_input.path,\n        \"output_dir\": logistic_output.path\n    }\n    op = logistic(args)\n@component(base_image=\"racahu23/torch:1\",  packages_to_install=['mlflow', 'boto3'])\ndef torch_op(logistic_input:Input[Artifact], torch_output: Output[Artifact],\n             experiment_id: str, run_id: str, tracking_uri: str\n             ):\n    from main import torch_process_logistic\n    args = {\n        \"experiment_id\": experiment_id,\n        \"run_id\": run_id,\n        \"tracking_uri\": tracking_uri,\n        \"log_folder\": logistic_input.path,\n        \"output_dir\": torch_output.path\n    }\n    op = torch_process_logistic(args)\n\n@component(base_image=\"racahu23/svm:1\",  packages_to_install=['mlflow', 'boto3'])\ndef svm_op(svm_input: Input[Artifact], svm_output: Output[Artifact],\n           experiment_id: str, run_id: str, tracking_uri: str\n           ):\n    from main import svm_process\n    args = {\n        \"experiment_id\": experiment_id,\n        \"run_id\": run_id,\n        \"tracking_uri\": tracking_uri,\n        \"log_folder\": svm_input.path,\n        \"output_dir\": svm_output.path\n    }\n    op = svm_process(args)\n\n@component(base_image=\"racahu23/register:4\",  packages_to_install=['mlflow', 'python-dotenv'])\ndef register_op( run_id: str, tracking_uri: str):\n    from main import register_model\n    args = {\n        \"run_id\": run_id,\n        \"tracking_uri\": tracking_uri,\n    }\n    op = register_model(args)\n\nif __name__ ==\"__main__\":\n    @dsl.pipeline(\n        name='Twitter nltk pipeline',\n        description='Writing code by the other way.'\n    )\n    def pipeline(mlflow_uri: str):\n        pvc_name = \"twitter-5000\"\n        \"\"\"\n        vop = dsl.VolumeOp(\n            name=pvc_name,\n            resource_name=\"twitter-5000\",\n            size=\"1Gi\",\n            modes=dsl.VOLUME_MODE_RWM\n        )\n        \"\"\"\n        op_mlflow = mlflow_setup_experiment(tracking_uri=mlflow_uri)\n        download_task= twitter_download_preprocess(experiment_id=op_mlflow.outputs[\"exp_id\"], run_id=op_mlflow.outputs[\"run_id\"], tracking_uri=mlflow_uri)\n        numpy_task = numpy_process(information=download_task.outputs['information'], experiment_id=op_mlflow.outputs[\"exp_id\"], run_id=op_mlflow.outputs[\"run_id\"]\n                                   , tracking_uri=mlflow_uri).after(\n            download_task)\n\n\n        sklearn_task= sklearn_logistic(information_input=numpy_task.outputs[\"information_output\"], experiment_id=op_mlflow.outputs[\"exp_id\"], tracking_uri=mlflow_uri, run_id=op_mlflow.outputs[\"run_id\"]).after(numpy_task)\n\n\n        logistic_task= logistic_op(sklearn_input=numpy_task.outputs[\"information_output\"], experiment_id=op_mlflow.outputs[\"exp_id\"], run_id=op_mlflow.outputs[\"run_id\"]\n                                   , tracking_uri=mlflow_uri).after(sklearn_task)\n\n\n        torch_task = torch_op(logistic_input=numpy_task.outputs[\"information_output\"], experiment_id=op_mlflow.outputs[\"exp_id\"], run_id=op_mlflow.outputs[\"run_id\"]\n                                   , tracking_uri=mlflow_uri ).after(logistic_task)\n\n        svm_task = svm_op(svm_input=numpy_task.outputs[\"information_output\"], experiment_id=op_mlflow.outputs[\"exp_id\"], run_id=op_mlflow.outputs[\"run_id\"]\n                                   , tracking_uri=mlflow_uri).after(torch_task)\n        register_task = register_op(tracking_uri=mlflow_uri, run_id=op_mlflow.outputs[\"run_id\"]).after(svm_task)\n    client = kfp.Client(namespace=\"kubeflow\", host=\"http://localhost:8080\")\n    client.create_run_from_pipeline_func(pipeline,\n                                         arguments={\"mlflow_uri\": \"http://mlflow.use-case.svc.cluster.local:5000\"},\n                                         mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE,\n                                         enable_caching=False)"
  },
  {
    "repo": "imsazzad/kubeflow-piplines-ml",
    "file_path": "app/kfp_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/imsazzad/kubeflow-piplines-ml/dev/app/kfp_pipeline.py",
    "content": "import kfp\nfrom kfp import dsl\nfrom app.preprocess_data.kf_preprocess_component import preprocess_op\nfrom app.train.kf_train_component import train_op\nfrom app.test.kf_test_component import test_op\nfrom app.deploy.kf_deploy_component import deploy_model_op\n\n@dsl.pipeline(\n    name='Boston Housing Pipeline',\n    description='An example pipeline.'\n)\ndef boston_pipeline():\n    _preprocess_op = preprocess_op()\n\n    _train_op = train_op(\n        dsl.InputArgumentPath(_preprocess_op.outputs['x_train']),\n        dsl.InputArgumentPath(_preprocess_op.outputs['y_train'])\n    ).after(_preprocess_op)\n\n    _test_op = test_op(\n        dsl.InputArgumentPath(_preprocess_op.outputs['x_test']),\n        dsl.InputArgumentPath(_preprocess_op.outputs['y_test']),\n        dsl.InputArgumentPath(_train_op.outputs['model'])\n    ).after(_train_op)\n\n    deploy_model_op(\n        dsl.InputArgumentPath(_train_op.outputs['model'])\n    ).after(_test_op)\n\n\n\nhost='http://kf-centraldashboard.k8sdev.infolytx.tech/pipeline/'\nnamespace='sazzad'\n\n\nclient = kfp.Client(host=host, namespace=namespace)\nclient.create_run_from_pipeline_func(boston_pipeline, arguments={}, experiment_name= \"Boston Housing Pipeline Experiment\")\n"
  },
  {
    "repo": "sfujiwara/kfpc",
    "file_path": "examples/simple.py",
    "raw_url": "https://raw.githubusercontent.com/sfujiwara/kfpc/main/examples/simple.py",
    "content": "import argparse\nfrom kfp.v2 import compiler\nfrom google.cloud import aiplatform\nimport kfpc\nimport kfp.dsl\n\n\ndef parse():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--project\", type=str, required=True)\n    args = parser.parse_args()\n    return args\n\n\n@kfp.dsl.pipeline(name=\"simple\")\ndef pipeline_fn(project: str):\n    query_select1_task = kfpc.bigquery.Query(name=\"select-1\").task(\n        query=\"SELECT 1\",\n        job_project=project,\n        destination_project=project,\n        destination_dataset=\"sandbox\",\n        destination_table=\"select1\",\n    )\n\n    query_select2_task = kfpc.bigquery.Query(name=\"select-2\").task(\n        query=\"SELECT 2\",\n        job_project=project,\n        destination_project=project,\n        destination_dataset=\"sandbox\",\n        destination_table=\"select2\",\n    )\n\n    query_select3_task = kfpc.bigquery.Query(name=\"select-3\").task(\n        query=\"SELECT 3\",\n        job_project=project,\n        destination_project=project,\n        destination_dataset=\"sandbox\",\n        destination_table=\"select3\",\n        depend_on=[\n            query_select1_task.destination_table,\n            query_select2_task.destination_table,\n        ],\n    )\n\n    extract_task = kfpc.bigquery.ExtractArtifact(name=\"extract\").task(\n        job_project=project,\n        source_table_artifact=query_select3_task.destination_table,\n    )\n\n    load_artifact_task = kfpc.bigquery.Load(name=\"load\").task(\n        job_project=project,\n        source_artifact=extract_task.output_files,\n        destination_project=project,\n        destination_dataset=\"sandbox\",\n        destination_table=\"load\",\n        schema=[{\"name\": \"f0_\", \"type\": \"INTEGER\"}],\n        source_uri_suffix=\"data-*.jsonl\",\n    )\n\n\ndef main():\n    args = parse()\n    project = args.project\n\n    compiler.Compiler().compile(pipeline_func=pipeline_fn, package_path=\"pipeline.yaml\")\n    job = aiplatform.PipelineJob(\n        project=project,\n        display_name=\"simple\",\n        enable_caching=False,\n        template_path=\"pipeline.yaml\",\n        parameter_values={\"project\": project},\n        pipeline_root=f\"gs://{project}-vertex-ai/pipeline-root\",\n    )\n    job.submit()\n\n\nif __name__ == \"__main__\":\n    main()\n"
  },
  {
    "repo": "dhirajpatra/kubeflow-demo",
    "file_path": "pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/dhirajpatra/kubeflow-demo/main/pipeline.py",
    "content": "from kfp import dsl\nfrom kfp.dsl import container_component, Output, OutputPath, Input, InputPath, pipeline, Artifact\n\n\n@container_component\ndef preprocess_op(output_data: Output[Artifact]):\n    return dsl.ContainerSpec(\n        image='dhirajpatra/kfp-components:latest',\n        command=['python', 'preprocess.py'],\n        args=[],\n        output_artifacts={'output_data': output_data}\n    )\n\n\n@container_component\ndef train_op(input_data: Input[Artifact]):\n    return dsl.ContainerSpec(\n        image='dhirajpatra/kfp-components:latest',\n        command=['python', 'train.py'],\n        args=[],\n        input_artifacts={'input_data': input_data}\n    )\n\n\n@pipeline(name='iris-classifier-pipeline')\ndef iris_pipeline():\n    preprocess = preprocess_op()\n    train = train_op(input_data=preprocess.outputs['output_data'])\n"
  },
  {
    "repo": "secrettoad/kubeflow",
    "file_path": "multifamily_pricing/main.py",
    "raw_url": "https://raw.githubusercontent.com/secrettoad/kubeflow/main/multifamily_pricing/main.py",
    "content": "from google.cloud import aiplatform\nfrom kfp import dsl\nfrom kfp.v2 import compiler\nfrom kfp.v2.dsl import component, Input, Output, Dataset, Model, Artifact\nimport functions_framework\nimport tempfile\n\n\n@component(packages_to_install=['dask[dataframe]', 'pyarrow', 'gcsfs'], base_image='python:3.7')\ndef ingest_component(df_uri: str, df: Output[Dataset]):\n    import dask.dataframe as dd\n    _df = dd.read_csv(df_uri).set_index('Unnamed: 0')\n    _df.to_parquet(df.uri)\n\n\n@component(packages_to_install=['dask[dataframe]', 'xgboost', 'scikit-learn', 'pyarrow', 'gcsfs'], base_image='python:3.7')\ndef train_component(df_train: Input[Dataset], model: Output[Model], params: Output[Artifact]):\n    import xgboost as xgb\n    import dask.dataframe as dd\n    import gcsfs\n    import pickle\n    import json\n\n    fs = gcsfs.GCSFileSystem()\n    _df_train = dd.read_parquet(df_train.uri).compute()\n    X = _df_train[['latitude', 'longitude', 'property_type', 'sqft', 'beds', 'baths', 'days_since_2014']]\n    y = _df_train['trans_log_price']\n    ##TODO add capability for distributed cluster on compute engine\n    ##TODO add hyperparameter tuning\n    ##TODO invert control of parameters\n    _params = {\"params\": {\"objective\": \"reg:squarederror\", \"random_state\": 111, \"monotonic_constraints\": {\"sqft\": 1}}}\n    _model = xgb.XGBRegressor(**_params['params'])\n    _model.fit(X, y)\n    model.uri = model.uri + '.pkl'\n\n    with fs.open(model.uri, 'wb') as f:\n        pickle.dump(_model, f)\n\n    with fs.open(params.uri, 'w') as f:\n        json.dump(_params, f)\n\n\n@component(packages_to_install=['dask[dataframe]', 'xgboost', 'scikit-learn', 'pyarrow', 'gcsfs'], base_image='python:3.7')\ndef predict_component(df_predict: Input[Dataset], model: Input[Model], y_hat: Output[Dataset]):\n    import dask.dataframe as dd\n    import pickle\n    import gcsfs\n\n    fs = gcsfs.GCSFileSystem()\n\n    _df_predict = dd.read_parquet(df_predict.uri).compute()\n\n    X = _df_predict[['latitude','longitude','property_type','sqft','beds','baths','days_since_2014']]\n\n    with fs.open(model.uri, 'rb') as f:\n        _model = pickle.load(f)\n\n    _df_predict['y_hat'] = _model.predict(X)\n\n    dd.from_pandas(_df_predict, chunksize=1000000).to_parquet(y_hat.uri)\n\n\n@component(packages_to_install=['dask[dataframe]', 'pyarrow', 'gcsfs',], base_image='python:3.7')\ndef validate_component(y_hat: Input[Dataset], y_hat_test: Input[Dataset], metrics: Output[Artifact]):\n    import dask.dataframe as dd\n    import gcsfs\n    import json\n    import numpy as np\n\n    fs = gcsfs.GCSFileSystem()\n\n    _y_hat = dd.read_parquet(y_hat.uri).compute()\n    _y_hat_test = dd.read_parquet(y_hat_test.uri).compute()\n\n    def inv_zscore_log_price(y, mean, std):\n        return np.exp((y * std) + mean)\n\n    def median_absolute_percentage_error(actual, predicted):\n        return np.median((np.abs(actual - predicted) / actual)) * 100\n\n    def moments(s):\n        return s.apply(np.log).mean(), s.apply(np.log).std()\n\n\n    _metrics = {}\n    _metrics['training_performance'] = median_absolute_percentage_error(inv_zscore_log_price(_y_hat['y_hat'], *moments(_y_hat['price'])),\n                                           inv_zscore_log_price(_y_hat['trans_log_price'], *moments(_y_hat['price'])))\n    _metrics['test_performance'] = median_absolute_percentage_error(inv_zscore_log_price(_y_hat_test['y_hat'], *moments(_y_hat['price'])),\n                                           inv_zscore_log_price(_y_hat_test['trans_log_price'], *moments(_y_hat['price'])))\n\n    with fs.open(metrics.uri, 'w') as f:\n        json.dump(_metrics, f)\n\n\n@component(packages_to_install=['dask[dataframe]', 'pyarrow', 'gcsfs', 'google-cloud-aiplatform'], base_image='python:3.7')\ndef deployment_component(model: Input[Model], metrics: Input[Artifact], vertex_endpoint: Output[Artifact], vertex_model: Output[Model]):\n    from google.cloud import aiplatform\n    import gcsfs\n    import json\n    aiplatform.init(project='demos-362417')\n\n    fs = gcsfs.GCSFileSystem()\n    with fs.open(metrics.uri, 'rb') as f:\n        _metrics = json.load(f)\n\n    ##TODO compare against current version performance\n    ##TODO log performance metrics to metadata\n    if _metrics['test_performance'] > 0:\n\n        _model = aiplatform.Model.upload(\n            display_name=\"multifamily_demo_model\",\n            artifact_uri='/'.join(model.uri.split('/')[:-1]),\n            serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest\",\n        )\n\n        endpoint = _model.deploy(machine_type=\"n1-standard-8\")\n        vertex_endpoint.uri = endpoint.resource_name\n        vertex_model.uri = _model.resource_name\n\n\n@dsl.pipeline(\n    name=\"demo-pipeline\",\n    description=\"demo\",\n    pipeline_root='gs://coysu-demo-pipelines/multifamily-pricing',\n)\ndef pipeline():\n    train_data = ingest_component(df_uri='gs://coysu-demo-datasets/multifamily_pricing/train/*.csv')\n    test_data = ingest_component(df_uri='gs://coysu-demo-datasets/multifamily_pricing/test/*.csv')\n    model = train_component(train_data.outputs['df'])\n    insample_preds = predict_component(train_data.outputs['df'], model.outputs['model'])\n    test_preds = predict_component(test_data.outputs['df'], model.outputs['model'])\n    metrics = validate_component(insample_preds.outputs['y_hat'], test_preds.outputs['y_hat'])\n    deployed_model = deployment_component(model.outputs['model'], metrics.outputs['metrics'])\n\n    ###todo start here - make serving pipeline and add business logic - add splitting to train pipeline\n\n\n# Triggered by a change in a storage bucket\n@functions_framework.cloud_event\ndef run_pricing_pipeline(event=None):\n    template_path = tempfile.gettempdir() + '/pipeline.json'\n    compiler.Compiler().compile(pipeline_func=pipeline, package_path=template_path)\n\n    aiplatform.init(project='demos-362417', staging_bucket=\"gs://coysu-demo-pipelines/multifamily_pricing/staging\")\n\n    job = aiplatform.PipelineJob(\n        display_name='multifamily_demo',\n        template_path=template_path,\n        pipeline_root='gs://coysu-demo-pipelines/multifamily_pricing'\n    )\n\n    job.run(sync=False)\n\n"
  },
  {
    "repo": "romanzdk/-test-kubeflow",
    "file_path": "hello_kf.py",
    "raw_url": "https://raw.githubusercontent.com/romanzdk/-test-kubeflow/main/hello_kf.py",
    "content": "# Copyright 2019 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp\nfrom kfp import dsl\n\ndef echo_op():\n    return dsl.ContainerOp(\n        name='echo',\n        image='library/bash:4.4.23',\n        command=['sh', '-c'],\n        arguments=['echo \"hello world\"']\n    )\n\n@dsl.pipeline(\n    name='my-first-pipeline',\n    description='A hello world pipeline.'\n)\ndef hello_world_pipeline():\n    echo_task = echo_op()\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(hello_world_pipeline, __file__ + '.yaml')"
  },
  {
    "repo": "Natrofl/kubeflow-linear-pipline",
    "file_path": "convert.py",
    "raw_url": "https://raw.githubusercontent.com/Natrofl/kubeflow-linear-pipline/main/convert.py",
    "content": "import kfp\nimport kfp.dsl as dsl\nfrom kfp.components import func_to_container_op\n\n@func_to_container_op\ndef show_results(linear_regression : float) -> None:\n\n    print(f\"Linear regression (accuracy): {linear_regression}\")\n\n@dsl.pipeline(name='Boston Housing Pipeline', description='Test kubeflow pipline for Boston Housing dataset .')\ndef first_pipeline():\n\n    download = kfp.components.load_component_from_file('download_data/download_data.yaml')\n    linear_regression = kfp.components.load_component_from_file('linear_regression/linear_regression.yaml')\n\n    download_task = download()\n\n    linear_regression_task = linear_regression(download_task.output)\n\n    show_results(linear_regression_task.output)\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(first_pipeline, 'LinearPipline.yaml')\n"
  },
  {
    "repo": "TassaraR/vertex-kubeflow-pipelines-tutorial",
    "file_path": "model/pipelines/container-component-pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/TassaraR/vertex-kubeflow-pipelines-tutorial/main/model/pipelines/container-component-pipeline.py",
    "content": "import os\nfrom kfp.dsl import (\n    container_component,\n    ContainerSpec,\n    Input,\n    Output,\n    Dataset,\n    Artifact,\n    Model,\n    pipeline,\n)\n\nPIPELINE_NAME = \"containerized-penguins-pipeline\"\nBASE_BUCKET = \"gs://kfp-tutorial-b0e2a25a\"\nPIPELINE_ROOT = os.path.join(BASE_BUCKET, PIPELINE_NAME)\n\n\n@container_component\ndef preprocessing_component(\n    project_id: str,\n    input_path: str,\n    num_cols: str,\n    cat_cols: str,\n    label_col: str,\n    output_train_path: Output[Dataset],\n    output_test_path: Output[Dataset],\n    output_encoder_path: Output[Artifact],\n):\n    return ContainerSpec(\n        image=f\"us-central1-docker.pkg.dev/{project_id}/pipeline-tutorial/preprocessing:latest\",  # noqa: E501\n        command=[\"python\", \"runner.py\"],\n        args=[\n            \"--input-path\",\n            input_path,\n            \"--num-cols\",\n            num_cols,\n            \"--cat-cols\",\n            cat_cols,\n            \"--label-col\",\n            label_col,\n            \"--output-train-path\",\n            output_train_path.uri,\n            \"--output-test-path\",\n            output_test_path.uri,\n            \"--output-encoder-path\",\n            output_encoder_path.uri,\n        ],\n    )\n\n\n@container_component\ndef training_component(\n    project_id: str,\n    num_cols: str,\n    cat_cols: str,\n    label_col: str,\n    input_train: Input[Dataset],\n    input_test: Input[Dataset],\n    encoder_path: Input[Artifact],\n    output_model: Output[Model],\n):\n    return ContainerSpec(\n        image=f\"us-central1-docker.pkg.dev/{project_id}/pipeline-tutorial/training:latest\",\n        command=[\"python\", \"runner.py\"],\n        args=[\n            \"--input-train\",\n            input_train.uri,\n            \"--input-test\",\n            input_test.uri,\n            \"--num-cols\",\n            num_cols,\n            \"--cat-cols\",\n            cat_cols,\n            \"--label-col\",\n            label_col,\n            \"--encoder-path\",\n            encoder_path.uri,\n            \"--output-model\",\n            output_model.uri,\n        ],\n    )\n\n\n@pipeline(\n    name=PIPELINE_NAME,\n    description=\"Penguins tutorial pipeline\",\n    pipeline_root=PIPELINE_ROOT,\n)\ndef penguins_pipeline(\n    project_id: str,\n    input_path: str,\n    num_cols: str,\n    cat_cols: str,\n    label_col: str,\n):\n    preproc_step = preprocessing_component(\n        project_id=project_id,\n        input_path=input_path,\n        num_cols=num_cols,\n        cat_cols=cat_cols,\n        label_col=label_col,\n    )\n\n    train_step = training_component(  # noqa: F841\n        project_id=project_id,\n        input_train=preproc_step.outputs[\"output_train_path\"],\n        input_test=preproc_step.outputs[\"output_test_path\"],\n        encoder_path=preproc_step.outputs[\"output_encoder_path\"],\n        num_cols=num_cols,\n        cat_cols=cat_cols,\n        label_col=label_col,\n    )\n"
  },
  {
    "repo": "TassaraR/vertex-kubeflow-pipelines-tutorial",
    "file_path": "model/pipelines/lightweight-component-pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/TassaraR/vertex-kubeflow-pipelines-tutorial/main/model/pipelines/lightweight-component-pipeline.py",
    "content": "import os\nfrom kfp.dsl import (\n    component,\n    Input,\n    Output,\n    Dataset,\n    Artifact,\n    Model,\n    Metrics,\n    ClassificationMetrics,\n    pipeline,\n)\n\n\nPIPELINE_NAME = \"lightweight-penguins-pipeline\"\nBASE_BUCKET = \"gs://kfp-tutorial-b0e2a25a\"\nPIPELINE_ROOT = os.path.join(BASE_BUCKET, PIPELINE_NAME)\n\nREPO = \"us-central1-docker.pkg.dev\"\nPROJECT = \"tassarar-ml\"\nBASE_PATH = \"pipeline-tutorial\"\nIMG_BASE_PATH = os.path.join(REPO, PROJECT, BASE_PATH)\n\n\n@component(base_image=os.path.join(IMG_BASE_PATH, \"preprocessing:latest\"))\ndef preprocessing_component(\n    input_path: str,\n    num_cols: str,\n    cat_cols: str,\n    label_col: str,\n    output_train_path: Output[Dataset],\n    output_test_path: Output[Dataset],\n    output_encoder_path: Output[Artifact],\n):\n    import joblib\n    import pandas as pd\n    from prepare_data import PreprocessData\n\n    input_data = pd.read_csv(input_path)\n    parse_cat_cols = cat_cols.split(\",\")\n    parse_num_cols = num_cols.split(\",\")\n\n    preproc = PreprocessData(\n        input_data=input_data,\n        categorical_cols=parse_cat_cols,\n        numerical_cols=parse_num_cols,\n        label_col=label_col,\n    )\n    preproc.build()\n\n    ds_train, y_train = preproc.get_train_set()\n    ds_train[label_col] = y_train\n    ds_train.to_csv(output_train_path.path, index=False)\n\n    ds_test, y_test = preproc.get_test_set()\n    ds_test[label_col] = y_test\n    ds_test.to_csv(output_test_path.path, index=False)\n\n    joblib.dump(preproc.get_encoder(), output_encoder_path.path)\n\n    output_train_path.metadata[\"shape\"] = str(ds_train.shape)\n    output_train_path.metadata[\"columns\"] = ds_train.columns.tolist()\n    output_test_path.metadata[\"shape\"] = str(ds_test.shape)\n    output_test_path.metadata[\"columns\"] = ds_test.columns.tolist()\n    output_encoder_path.metadata[\"params\"] = str(preproc.get_encoder().classes_)\n\n\n@component(base_image=os.path.join(IMG_BASE_PATH, \"training:latest\"))\ndef training_component(\n    num_cols: str,\n    cat_cols: str,\n    label_col: str,\n    input_train: Input[Dataset],\n    input_test: Input[Dataset],\n    encoder_path: Input[Artifact],\n    output_model: Output[Model],\n    eval_metrics: Output[Metrics],\n    confusion_matrix: Output[ClassificationMetrics],\n):\n    import joblib\n    import pandas as pd\n    from trainer import create_pipeline, evaluate_model\n\n    encoder = joblib.load(encoder_path.path)\n\n    train = pd.read_csv(input_train.path)\n    train_set = train.drop(columns=[label_col])\n    train_label = train[label_col].to_numpy()\n\n    test = pd.read_csv(input_test.path)\n    test_set = test.drop(columns=[label_col])\n    test_label = test[label_col].to_numpy()\n\n    parse_cat_cols = cat_cols.split(\",\")\n    parse_num_cols = num_cols.split(\",\")\n\n    clf = create_pipeline(categorical_cols=parse_cat_cols, numeric_cols=parse_num_cols)\n\n    clf.fit(train_set, train_label)\n\n    metrics = evaluate_model(\n        model=clf, eval_set=test_set, eval_y=test_label, encoder=encoder\n    )\n\n    joblib.dump(clf, output_model.path)\n\n    # Kubeflow artifact's metrics & metadata\n    output_model.metadata[\"params\"] = str(clf.get_params())\n\n    eval_metrics.log_metric(\"accuracy\", metrics[\"metrics\"][\"accuracy\"])\n    for model_class, class_metrics in metrics[\"metrics\"].items():\n        if isinstance(class_metrics, dict):\n            for metric, score in class_metrics.items():\n                eval_metrics.log_metric(f\"{model_class} - {metric}\", score)\n\n    conf_matrix = metrics[\"confusion-matrix\"]\n    conf_matrix_labels = [lbl.split(\"_\")[1] for lbl in conf_matrix.index]\n\n    confusion_matrix.log_confusion_matrix(\n        conf_matrix_labels, conf_matrix.T.to_numpy().tolist()\n    )\n\n\n@pipeline(\n    name=PIPELINE_NAME,\n    description=\"Penguins tutorial pipeline\",\n    pipeline_root=PIPELINE_ROOT,\n)\ndef penguins_pipeline(\n    input_path: str,\n    num_cols: str,\n    cat_cols: str,\n    label_col: str,\n):\n\n    preproc_step = preprocessing_component(\n        input_path=input_path,\n        num_cols=num_cols,\n        cat_cols=cat_cols,\n        label_col=label_col,\n    )\n\n    train_step = training_component(  # noqa: F841\n        input_train=preproc_step.outputs[\"output_train_path\"],\n        input_test=preproc_step.outputs[\"output_test_path\"],\n        encoder_path=preproc_step.outputs[\"output_encoder_path\"],\n        num_cols=num_cols,\n        cat_cols=cat_cols,\n        label_col=label_col,\n    )\n"
  },
  {
    "repo": "stackdemos/centralstate-edu",
    "file_path": "components/golang/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/centralstate-edu/master/components/golang/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef processing_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('GOLANG_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=[script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n"
  },
  {
    "repo": "stackdemos/centralstate-edu",
    "file_path": "components/training/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/centralstate-edu/master/components/training/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef training_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('TRAINING_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=['/usr/local/bin/python', script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n\n"
  },
  {
    "repo": "HibernationNo1/project_4_kubeflow_pipeline",
    "file_path": "pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/HibernationNo1/project_4_kubeflow_pipeline/master/pipeline.py",
    "content": "\nimport kfp\nimport kfp.dsl as dsl\n \nimport argparse\n\nfrom component.record.record_op import record_op\nfrom component.train.train_op import train_op\nfrom component.evaluate.evaluate_op import evaluate_op\nfrom component.test.test_op import test_op\n\n\nfrom pipeline_config import set_config, CONFIGS\nfrom pipeline_utils import (connet_client, get_experiment, run_pipeline, upload_pipeline, set_intput_papams, \n                            kfb_print)\n\n\nfrom kubernetes.client.models import V1EnvVar, V1EnvVarSource, V1SecretKeySelector\nfrom kubernetes.client import V1Volume, V1EmptyDirVolumeSource\n\nSECRETS = dict()\n\n@dsl.pipeline(name=\"hibernation_project\")\ndef project_pipeline(run_flag: dict, \n                     input_cfg: dict):           \n\n    # persistance volume\n    pvc_cfg = CONFIGS['pipeline'].kbf.volume.pvc\n    pvc_volume = dsl.VolumeOp(name= pvc_cfg.name,\n                       resource_name= pvc_cfg.resource_name,\n                       modes= pvc_cfg.mode,\n                       storage_class = pvc_cfg.storage_class,\n                       size= pvc_cfg.size)\n    \n    \n    # for allocate shared memory\n    shm_volume_cfg = CONFIGS['pipeline'].kbf.volume.share_memory\n    shm_volume = dsl.PipelineVolume(\n        volume=V1Volume(\n            name= shm_volume_cfg.name,\n            empty_dir=V1EmptyDirVolumeSource(medium=shm_volume_cfg.medium))\n        )  \n\n    \n    # set secrets\n    client_sc_name = \"client-secrets\"\n    for secrets_cate, secrets_cfg in SECRETS.items():\n        for key in secrets_cfg:\n            SECRETS[secrets_cate][key] = V1EnvVar(name=key, value_from=V1EnvVarSource(secret_key_ref=V1SecretKeySelector(name=client_sc_name, key=key)))\n       \n    _record_op = record_op(input_cfg, run_flag) \\\n        .add_env_variable(SECRETS['gs'][\"type\"]) \\\n        .add_env_variable(SECRETS['gs'][\"project_id\"]) \\\n        .add_env_variable(SECRETS['gs'][\"private_key_id\"]) \\\n        .add_env_variable(SECRETS['gs'][\"private_key\"]) \\\n        .add_env_variable(SECRETS['gs'][\"client_email\"]) \\\n        .add_env_variable(SECRETS['gs'][\"client_id\"]) \\\n        .add_env_variable(SECRETS['gs'][\"auth_uri\"]) \\\n        .add_env_variable(SECRETS['gs'][\"token_uri\"]) \\\n        .add_env_variable(SECRETS['gs'][\"auth_provider_x509_cert_url\"]) \\\n        .add_env_variable(SECRETS['gs'][\"client_x509_cert_url\"]) \\\n        .add_env_variable(SECRETS['db'][\"password\"]) \\\n        .add_env_variable(SECRETS['db'][\"host\"]) \\\n        .add_env_variable(SECRETS['db'][\"port\"]) \\\n        .add_pvolumes({pvc_cfg.mount_path: pvc_volume.volume})\n    \n    _train_op = train_op(input_cfg, _record_op.outputs['run_flag'])\\\n        .add_env_variable(SECRETS['gs'][\"type\"]) \\\n        .add_env_variable(SECRETS['gs'][\"project_id\"]) \\\n        .add_env_variable(SECRETS['gs'][\"private_key_id\"]) \\\n        .add_env_variable(SECRETS['gs'][\"private_key\"]) \\\n        .add_env_variable(SECRETS['gs'][\"client_email\"]) \\\n        .add_env_variable(SECRETS['gs'][\"client_id\"]) \\\n        .add_env_variable(SECRETS['gs'][\"auth_uri\"]) \\\n        .add_env_variable(SECRETS['gs'][\"token_uri\"]) \\\n        .add_env_variable(SECRETS['gs'][\"auth_provider_x509_cert_url\"]) \\\n        .add_env_variable(SECRETS['gs'][\"client_x509_cert_url\"]) \\\n        .add_env_variable(SECRETS['db'][\"password\"]) \\\n        .add_env_variable(SECRETS['db'][\"host\"]) \\\n        .add_env_variable(SECRETS['db'][\"port\"]) \\\n        .add_pvolumes({shm_volume_cfg.path: shm_volume})\\\n        .add_pvolumes({pvc_cfg.mount_path: pvc_volume.volume})\n\n    _evaluate_op = evaluate_op(input_cfg, _train_op.outputs['run_flag'])\\\n        .add_env_variable(SECRETS['gs'][\"type\"]) \\\n        .add_env_variable(SECRETS['gs'][\"project_id\"]) \\\n        .add_env_variable(SECRETS['gs'][\"private_key_id\"]) \\\n        .add_env_variable(SECRETS['gs'][\"private_key\"]) \\\n        .add_env_variable(SECRETS['gs'][\"client_email\"]) \\\n        .add_env_variable(SECRETS['gs'][\"client_id\"]) \\\n        .add_env_variable(SECRETS['gs'][\"auth_uri\"]) \\\n        .add_env_variable(SECRETS['gs'][\"token_uri\"]) \\\n        .add_env_variable(SECRETS['gs'][\"auth_provider_x509_cert_url\"]) \\\n        .add_env_variable(SECRETS['gs'][\"client_x509_cert_url\"]) \\\n        .add_env_variable(SECRETS['db'][\"password\"]) \\\n        .add_env_variable(SECRETS['db'][\"host\"]) \\\n        .add_env_variable(SECRETS['db'][\"port\"]) \\\n        .add_pvolumes({shm_volume_cfg.path: shm_volume})\\\n        .add_pvolumes({pvc_cfg.mount_path: pvc_volume.volume})\n\n    _test_op = test_op(input_cfg, _evaluate_op.outputs['run_flag'])\\\n        .add_env_variable(SECRETS['gs'][\"type\"]) \\\n        .add_env_variable(SECRETS['gs'][\"project_id\"]) \\\n        .add_env_variable(SECRETS['gs'][\"private_key_id\"]) \\\n        .add_env_variable(SECRETS['gs'][\"private_key\"]) \\\n        .add_env_variable(SECRETS['gs'][\"client_email\"]) \\\n        .add_env_variable(SECRETS['gs'][\"client_id\"]) \\\n        .add_env_variable(SECRETS['gs'][\"auth_uri\"]) \\\n        .add_env_variable(SECRETS['gs'][\"token_uri\"]) \\\n        .add_env_variable(SECRETS['gs'][\"auth_provider_x509_cert_url\"]) \\\n        .add_env_variable(SECRETS['gs'][\"client_x509_cert_url\"]) \\\n        .add_env_variable(SECRETS['db'][\"password\"]) \\\n        .add_env_variable(SECRETS['db'][\"host\"]) \\\n        .add_env_variable(SECRETS['db'][\"port\"]) \\\n        .add_pvolumes({shm_volume_cfg.path: shm_volume})\\\n        .add_pvolumes({pvc_cfg.mount_path: pvc_volume.volume})\n\n\ndef _parse_args():\n    \n    parser = argparse.ArgumentParser()    \n    parser.add_argument(\"--cfg_pipeline\", help=\"name of config file which for pipeline\")       \n    parser.add_argument(\"--cfg_train\", help=\"name of config file which for training\")                           # TODO: rename\n    parser.add_argument(\"--cfg_record\", help=\"name of config file which for record\") \n    parser.add_argument(\"--cfg_test\", help=\"name of config file which for test\")  \n    parser.add_argument(\"--cfg_eval\", help=\"name of config file which for evaluate\") \n    \n    kbf_parser = parser.add_argument_group('kubeflow')\n    kbf_parser.add_argument(\"--dashboard_pw\", type = str , help=\"password of kubeflow dashboard\")        \n    kbf_parser.add_argument(\"--pipeline_v\", type = str, help=\"version of pipeline\")                    \n    kbf_parser.add_argument(\"--pipeline_n\", type = str, help=\"name of pipeline\")    \n    kbf_parser.add_argument(\"--experiment_n\", type = str, help=\"name of experiment\") \n    kbf_parser.add_argument(\"--run_n\", type = str, help=\"name of run\") \n\n    kbf_parser.add_argument(\"--katib\", action = 'store_true',  help=\"If run for experiment\") \n    # gs_parser = parser.add_argument_group('google_storage')\n\n\n    db_parser = parser.add_argument_group('database')\n    db_parser.add_argument('--name_db', type = str, help = 'Database name to connect to database')\n    db_parser.add_argument('--user_db', type = str, help = 'User name to connect to database')\n    \n    \n    train_parser = parser.add_argument_group('train')\n    train_parser.add_argument(\"--model\", type = str, choices = ['MaskRCNN'],\n                              help=\"Name of the model to be trained\") \n    train_parser.add_argument(\"--epoch\", type = int, help=\"epoch for training\") \n    train_parser.add_argument(\"--lr\", type = str, choices = ['0.0001', '0.0005', '0.001', '0.00005', '0.00001', '0.000005'],\n                              help=\"Regular learning rate\")             # why str?: To get linear value with katib.\n    train_parser.add_argument(\"--swin_drop_rate\", type = str, choices = ['0.0', '0.1', '0.2', '0.3', '0.4'],\n                               help=\"drop_rate of swin transformar\")         # why str?: To get linear value with katib.\n    train_parser.add_argument(\"--swin_window_size\", type = str, choices = ['5', '7', '9', '11', '13'],\n                              help=\"window_size of swin transformar\")     # why str?: To get linear value with katib.\n    train_parser.add_argument(\"--swin_mlp_ratio\", type = str, choices = ['3', '4', '5'],\n                              help=\"mlp_ratio of swin transformar\")         # why str?: To get linear value with katib.\n    \n    \n    \n    test_parser = parser.add_argument_group('test')\n    test_parser.add_argument(\"--model_path\", type = str, help = \"Path of trained model(.pth format)\")\n    \n    \n    swin_parser = parser.add_argument_group('SwinTransformer')\n    swin_parser.add_argument('--pm_dilation', type = int, help= \"dilation of SwinTransformer.PatchMerging\") \n    swin_parser.add_argument('--drop_rate', type = float, help= \"drop_rate of SwinTransformer\") \n    swin_parser.add_argument('--drop_path_rate', type = float, help= \"drop_path_rate of SwinTransformer\") \n    swin_parser.add_argument('--attn_drop_rate', type = float, help= \"attn_drop_rate of SwinTransformer.SwinBlockSequence.ShiftWindowMSA.WindowMSA\") \n    \n    args = parser.parse_args()\n    input_args = vars(args)     # TODO delete\n    \n    return args, input_args\n\n\n\n\nif __name__==\"__main__\":      \n    args, input_args = _parse_args()  \n    set_config(args)\n\n    \n    SECRETS['gs'] = dict(CONFIGS['pipeline'].secrets.gs)  \n    SECRETS['db'] = dict(CONFIGS['pipeline'].secrets.db)  \n    \n\n    cfg_pipeline = CONFIGS.get('pipeline', None)\n    kfb_print(\"connet to kubeflow client\")\n    client = connet_client(cfg_pipeline.kbf.dashboard)  \n        \n    kfb_print(\"compile pipeline\")             \n    kfp.compiler.Compiler().compile(\n        project_pipeline,\n        f\"./{cfg_pipeline.kbf.pipeline.pac}\"\n        )\n    \n    # get experiment id by create experiment or load experiment info\n    kfb_print(\"get experiment\")\n    experiment_id = get_experiment(client, cfg_pipeline)          \n\n    # get experiment id by create pipeline or updata pipeline version\n    pipeline_id = upload_pipeline(client, cfg_pipeline.kbf.pipeline)     \n     \n    params = set_intput_papams() \n\n    run_pipeline(client, cfg_pipeline.kbf, experiment_id, pipeline_id, params)\n    \n    \n    \n        \n\n"
  },
  {
    "repo": "HibernationNo1/project_4_kubeflow_pipeline",
    "file_path": "pipeline_config.py",
    "raw_url": "https://raw.githubusercontent.com/HibernationNo1/project_4_kubeflow_pipeline/master/pipeline_config.py",
    "content": "import os, os.path as osp\n\nfrom pipeline_base_config import Config\n\nCONFIGS = dict()     # parameters for pipeline run  \nMAP_CONFIG = \"config/map.py\"\n\ndef set_cfg_pipeline(args, cfg):\n    if args.pipeline_n is not None: cfg.kbf.pipeline.name = args.pipeline_n\n    cfg.kbf.pipeline.version =  args.pipeline_v\n    if args.experiment_n is not None: cfg.kbf.experiment.name = args.experiment_n\n    if args.run_n is not None: cfg.kbf.run.name = args.run_n    \n    cfg.kbf.dashboard.pw =  args.dashboard_pw \n    \n    if cfg.kbf.volume.get(\"pvc\", None) is not None:\n        import kfp.dsl as dsl\n        mode = cfg.kbf.volume.pvc.mode\n        if mode == 'VOLUME_MODE_RWO':\n            cfg.kbf.volume.pvc.mode = dsl.VOLUME_MODE_RWO\n        elif mode == 'VOLUME_MODE_RWM':\n            cfg.kbf.volume.pvc.mode = dsl.VOLUME_MODE_RWM\n        elif mode == 'VOLUME_MODE_ROM':\n            cfg.kbf.volume.pvc.mode = dsl.VOLUME_MODE_ROM\n            \n       \n            \ndef comman_set(cfg):\n    if CONFIGS['pipeline'] is not None:     # when run only by kubeflow pipeline\n        if CONFIGS['pipeline'].kbf.volume.get(\"pvc\", None) is not None:\n            cfg.path.volume = CONFIGS['pipeline'].kbf.volume.pvc.mount_path\n        \n        \ndef set_cfg_record(args, cfg):\n    assert cfg.dvc.record.train == cfg.record.train_dataset\n    assert cfg.dvc.record.val == cfg.record.val_dataset\n    \n    comman_set(cfg)\n    \n     \ndef set_cfg_train(args, cfg):\n    # set config of model to training \n    assert args.model, f\"Model to be trained must be specified!!\\n\"\\\n        f\"add `--model` option when entering the command.\"  \n    \n    if args.katib:\n        cfg.katib = True\n\n        # set 0 when running for katib or in pod (not have enough shared memory) \n        cfg.data.workers_per_gpu = 0\n\n    comman_set(cfg)\n    \n    map_cfg = Config.fromfile(MAP_CONFIG)\n    models_cfg_path = osp.join(os.getcwd(), \n                                map_cfg.dir.config.name, \n                                map_cfg.dir.config.models)       \n                             \n    \n    model_cfg = Config.fromfile(osp.join(models_cfg_path, f\"{args.model}.py\")) \n\n    for key, item in cfg.model.get(args.model).items():\n        sub_cfg = Config.fromfile(osp.join(models_cfg_path, key, f\"{item}.py\"))\n        if key == 'backbone':\n            model_cfg.model.backbone = sub_cfg.get(key)\n            \n        if key == 'neck':\n            model_cfg.model.neck = sub_cfg.get(key)\n    cfg.model = model_cfg.model\n    \n    if args.name_db is not None: cfg.db.db = args.name_db \n    if args.user_db is not None: cfg.db.user = args.user_db \n    \n    if args.epoch is not None: cfg.max_epochs = args.epoch\n\n    if args.lr is not None: cfg.optimizer.lr = float(args.lr)\n   \n    if cfg.model.backbone.type == \"SwinTransformer\":\n        if args.swin_drop_rate is not None : \n            cfg.model.backbone.drop_rate = float(args.swin_drop_rate)\n            assert 0.<=cfg.model.backbone.drop_rate and cfg.model.backbone.drop_rate < 0.999\n        if args.swin_window_size is not None : \n            cfg.model.backbone.window_size = int(args.swin_window_size)\n            assert cfg.model.backbone.window_size in [3, 5, 7, 9, 11, 13, 15]\n        if args.swin_mlp_ratio is not None : \n            cfg.model.backbone.mlp_ratio = int(args.swin_mlp_ratio)\n            assert cfg.model.backbone.mlp_ratio in [i for i in range(10)] \n         \n\n    # If get dataset with dvc, load the paths from the database.\n    # And all paths were set by dvc config\n    if cfg.get('dvc', None) is not None:\n        if args.cfg_pipeline is not None:\n            cfg.data.train.data_root = cfg.data.val.data_root = osp.join(cfg.git.dataset.repo,\n                                                                         cfg.dvc.record.dir,\n                                                                         cfg.dvc.category)\n            \n            \n            cfg.data.train.ann_file = osp.join(cfg.data.train.data_root, cfg.dvc.record.train)\n            cfg.data.val.ann_file = osp.join(cfg.data.val.data_root, cfg.dvc.record.val)  \n        else:\n            cfg.pop('dvc')\n  \n    if args.pm_dilation is not None: cfg.model.backbone.pm_dilation = args.pm_dilation\n    if args.drop_rate is not None: cfg.model.backbone.drop_rate = args.drop_rate\n    if args.drop_path_rate is not None: cfg.model.backbone.drop_path_rate = args.drop_path_rate\n    if args.attn_drop_rate is not None: cfg.model.backbone.attn_drop_rate = args.attn_drop_rate    \n    \n    if CONFIGS['pipeline'] is not None:     # when run only by kubeflow pipeline\n        if CONFIGS['pipeline'].kbf.volume.get('pvc', None) is not None:\n            for i, hook_cfg in enumerate(cfg.hook_configs):\n                if hook_cfg.type == \"TensorBoard_Hook\":\n                    cfg.hook_configs[i].pvc_dir = osp.join(CONFIGS['pipeline'].kbf.volume.pvc.mount_path,\n                                                          cfg.hook_configs[i].pvc_dir)\n                    break\n\n\ndef set_cfg_test(args, cfg):\n    if args.model_path is not None: cfg.model_path = args.model_path  \n    \n    print(f\"test: {cfg.get('data_root', None)}\")\n    \n\ndef sef_cfg_evaluate(args, cfg):\n    if args.model_path is not None: cfg.model_path = args.model_path\n\n    # If get dataset with dvc, load the paths from the database.\n    # And all paths were set by dvc config\n    if cfg.get('dvc', None) is not None:\n        if args.cfg_pipeline is not None:\n            # why set `cfg.data.train.data_root` even unused in `evaluate_op.py`?\n            # To prevent conflicts between configs in `combine_config`\n            cfg.data.train.data_root = cfg.data.val.data_root = osp.join(cfg.git.dataset.repo,\n                                                                         cfg.dvc.record.dir,\n                                                                         cfg.dvc.category)\n            \n            \n            cfg.data.train.ann_file = osp.join(cfg.data.train.data_root, cfg.dvc.record.train)\n            cfg.data.val.ann_file = osp.join(cfg.data.val.data_root, cfg.dvc.record.val) \n        else:\n            cfg.pop('dvc')\n    \n    \n\n\nCONFIG_SET_FUNCTION = dict(\n    pipeline = set_cfg_pipeline,\n    record = set_cfg_record,\n    train = set_cfg_train,\n    test = set_cfg_test,\n    evaluate = sef_cfg_evaluate\n)\n\n\ndef set_config(args):\n    \"\"\" \n        cfg arguments determines which component be run.\n        Components that matching cfg arguments which got `None` are excluded from the pipeline.\n        cfg arguments: is chooses in [args.cfg_train, args.cfg_record]\n    Args:\n        args : argparse\n    \"\"\"\n\n    if args.katib:\n        if args.model is None:\n            args.model = 'MaskRCNN' \n        if args.cfg_train is None:\n            args.cfg_train = 'config/train_cfg.py'\n\n \n    if (args.cfg_pipeline is not None) and (args.pipeline_v is not None) and (args.dashboard_pw is not None):\n        print(\"Run with kubeflow pipeline\")\n        CONFIGS['pipeline'] = args.cfg_pipeline\n        \n    elif (args.cfg_pipeline is None) and (args.pipeline_v is None) and (args.dashboard_pw is None):\n        print(f\"Run without kubeflow pipleine\")\n        CONFIGS['pipeline'] = None\n    else:\n        raise ValueError(f\"To run in pipeline of kubeflow, config, version and password of pipeline must be set.\\n\"\\\n                         f\"add options --cfg_pipeline, --pipeline_v, --dashboard_pw\")\n           \n    CONFIGS['train'] = args.cfg_train\n    CONFIGS['record'] = args.cfg_record\n    CONFIGS['test'] = args.cfg_test\n    CONFIGS['evaluate'] = args.cfg_eval \n\n    for key, func in CONFIG_SET_FUNCTION.items(): \n        if CONFIGS[key] is not None:\n            # Assign config only included in args \n            config =  Config.fromfile(CONFIGS[key])\n            func(args, config)\n        else: config = None\n        # CONFIGS[key] = False or Config\n        # if False, components matching the key will be passed from the pipeline.\n        # >>    example\n        # >>    CONFIGS[record] = False\n        # >>    `record_op` component will be passed from the pipeline.\n        CONFIGS[key] = config\n\n\n"
  },
  {
    "repo": "tanle2694/kubeflow-resnet-tf-pipeline",
    "file_path": "pipeline/src/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/tanle2694/kubeflow-resnet-tf-pipeline/master/pipeline/src/pipeline.py",
    "content": "# Copyright (c) 2019, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport kfp.dsl as dsl\nimport datetime\nimport os\nfrom kubernetes import client as k8s_client\n\n\n# Modify image='<image>' in each op to match IMAGE in the build.sh of its corresponding component\n\ndef PreprocessOp(name, input_dir, output_dir):\n    return dsl.ContainerOp(\n        name=name,\n        image='preprocess-image',\n        arguments=[\n            '--input_dir', input_dir,\n            '--output_dir', output_dir,\n        ],\n        file_outputs={'output': '/output.txt'}\n    )\n\n\ndef TrainOp(name, input_dir, output_dir, model_name, model_version, epochs):\n    return dsl.ContainerOp(\n        name=name,\n        image='train-image',\n        arguments=[\n            '--input_dir', input_dir,\n            '--output_dir', output_dir,\n            '--model_name', model_name,\n            '--model_version', model_version,\n            '--epochs', epochs\n        ],\n        file_outputs={'output': '/output.txt'}\n    )\n\n\ndef InferenceServerLauncherOp(name, input_dir, trtserver_name):\n    return dsl.ContainerOp(\n        name=name,\n        image='inference-server-launcher-image',\n        arguments=[\n            '--trtserver_name', trtserver_name,\n            '--model_path', input_dir,\n        ],\n        file_outputs={'output': '/output.txt'}\n    )\n\n\ndef WebappLauncherOp(name, trtserver_name, model_name, model_version, webapp_prefix, webapp_port):\n    return dsl.ContainerOp(\n        name=name,\n        image='webapp-launcher-image',\n        arguments=[\n            '--workflow_name', '{{workflow.name}}',\n            '--trtserver_name', trtserver_name,\n            '--model_name', model_name,\n            '--model_version', str(model_version),\n            '--webapp_prefix', webapp_prefix,\n            '--webapp_port', str(webapp_port)\n        ],\n        file_outputs={}\n    )\n\n\n@dsl.pipeline(\n    name='resnet_cifar10_pipeline',\n    description='Demonstrate an end-to-end training & serving pipeline using ResNet and CIFAR-10'\n)\ndef resnet_pipeline(\n    raw_data_dir='/mnt/workspace/raw_data',\n    processed_data_dir='/mnt/workspace/processed_data',\n    model_dir='/mnt/workspace/saved_model',\n    epochs=50,\n    trtserver_name='trtis',\n    model_name='resnet_graphdef',\n    model_version=1,\n    webapp_prefix='webapp',\n    webapp_port=80\n):\n\n    persistent_volume_name = 'nvidia-workspace'\n    persistent_volume_path = '/mnt/workspace'\n\n    op_dict = {}\n\n    op_dict['preprocess'] = PreprocessOp(\n        'preprocess', raw_data_dir, processed_data_dir)\n\n    op_dict['train'] = TrainOp(\n        'train', op_dict['preprocess'].output, model_dir, model_name, model_version, epochs)\n    op_dict['train'].execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n    op_dict['deploy_inference_server'] = InferenceServerLauncherOp(\n        'deploy_inference_server', op_dict['train'].output, trtserver_name)\n\n    op_dict['deploy_webapp'] = WebappLauncherOp(\n        'deploy_webapp', op_dict['deploy_inference_server'].output, model_name, model_version, webapp_prefix, webapp_port)\n\n    for _, container_op in op_dict.items():\n        container_op.add_volume(k8s_client.V1Volume(\n            host_path=k8s_client.V1HostPathVolumeSource(\n                path=persistent_volume_path),\n            name=persistent_volume_name))\n        container_op.add_volume_mount(k8s_client.V1VolumeMount(\n            mount_path=persistent_volume_path,\n            name=persistent_volume_name))\n\n\nif __name__ == '__main__':\n    import kfp.compiler as compiler\n    compiler.Compiler().compile(resnet_pipeline, __file__ + '.tar.gz')\n"
  },
  {
    "repo": "gnanimail/kubeflow-pipeline-aircraft-emission",
    "file_path": "pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/gnanimail/kubeflow-pipeline-aircraft-emission/main/pipeline.py",
    "content": "import kfp\nfrom kfp import dsl\nfrom kfp import compiler\n\ndef preprocess_op():\n\n    return dsl.ContainerOp(\n        name='Preprocess Data',\n        image='gcr.io/psychic-kite-329307/ae_pipeline_preprocessing',\n        arguments=[],\n        file_outputs={\n            'x_train': '/app/x_train.npy',\n            'x_test': '/app/x_test.npy',\n            'y_train': '/app/y_train.npy',\n            'y_test': '/app/y_test.npy',\n        }\n    )\n\ndef train_op(x_train, y_train):\n\n    return dsl.ContainerOp(\n        name='Train Model',\n        image='gcr.io/psychic-kite-329307/ae_pipeline_train',\n        arguments=[\n            '--x_train', x_train,\n            '--y_train', y_train\n        ],\n        file_outputs={\n            'model': '/app/model.pkl'\n        }\n    )\n\ndef test_op(x_test, y_test, model):\n\n    return dsl.ContainerOp(\n        name='Test Model',\n        image='gcr.io/psychic-kite-329307/ae_pipeline_test',\n        arguments=[\n            '--x_test', x_test,\n            '--y_test', y_test,\n            '--model', model\n        ],\n        file_outputs={}\n    )\n\ndef deploy_model_op(model):\n\n    return dsl.ContainerOp(\n        name='Deploy Model',\n        image='gcr.io/psychic-kite-329307/ae_pipeline_api',\n        arguments=[\n            '--model', model\n        ]\n    )\n\n@dsl.pipeline(\n   name='Aircraft Emission',\n   description='An example pipeline that trains and logs a regression model.'\n)\ndef aircraft_emission_pipeline():\n    _preprocess_op = preprocess_op()\n    \n    _train_op = train_op(\n        dsl.InputArgumentPath(_preprocess_op.outputs['x_train']),\n        dsl.InputArgumentPath(_preprocess_op.outputs['y_train'])\n    ).after(_preprocess_op)\n\n    _test_op = test_op(\n        dsl.InputArgumentPath(_preprocess_op.outputs['x_test']),\n        dsl.InputArgumentPath(_preprocess_op.outputs['y_test']),\n        dsl.InputArgumentPath(_train_op.outputs['model'])\n    ).after(_train_op)\n\n    deploy_model_op(\n        dsl.InputArgumentPath(_train_op.outputs['model'])\n    ).after(_test_op)\n\n#client = kfp.Client()\n#client.create_run_from_pipeline_func(boston_pipeline, arguments={})\ncompiler.Compiler().compile(aircraft_emission_pipeline, 'aircraft_emission_pipeline.yaml')\n"
  },
  {
    "repo": "Deeksha-5/Kubeflow-Model-Train-Pipeline",
    "file_path": "autoML.py",
    "raw_url": "https://raw.githubusercontent.com/Deeksha-5/Kubeflow-Model-Train-Pipeline/main/autoML.py",
    "content": "import kfp\nfrom kfp import dsl\nfrom kfp.components import func_to_container_op\nfrom google.oauth2 import service_account\n\ncredentials = service_account.Credentials.from_service_account_file('./pipeline-key.json')\n\nPROJECT_ID = 'MY_PROJECT_ID'\nCOMPUTE_REGION = 'MY_COMPUTE_REGION'\n\n\n@func_to_container_op\ndef create_dataset(Dname):\n    import subprocess\n\n    def install(name):\n        subprocess.call(['pip', 'install', name])\n    install('google-cloud-automl')\n    import google.cloud.automl as automl\n    client = automl.AutoMlClient(credentials=credentials)\n    project_location = client.location_path(PROJECT_ID, COMPUTE_REGION)\n    DATASET_NAME = Dname\n    dataset_metadata = {}\n    my_dataset = {\"display_name\": DATASET_NAME, \"image_object_detection_dataset_metadata\": dataset_metadata, }\n    response = client.create_dataset(project_location, my_dataset)\n    print(\"Dataset name: {}\".format(response.name))\n    print(\"Dataset id: {}\".format(response.name.split(\"/\")[-1]))\n    print(\"Dataset display name: {}\".format(response.display_name))\n    print(\"Image classification dataset metadata:\")\n    print(\"\\t{}\".format(response.image_classification_dataset_metadata))\n    print(\"Dataset example count: {}\".format(response.example_count))\n    dataset_id = response.name.split(\"/\")[-1]\n    print(type(dataset_id))\n    return dataset_id\n\n\n@func_to_container_op\ndef import_items(Id, url):\n    import subprocess\n\n    def install(name):\n        subprocess.call(['pip', 'install', name])\n    install('google-cloud-automl')\n    import google.cloud.automl as automl\n    client = automl.AutoMlClient(credentials=credentials)\n    dataset_full_id = client.dataset_path(PROJECT_ID, COMPUTE_REGION, Id)\n    CSV_DATASET = url\n    input_config = {\"gcs_source\": {\"input_uris\": [CSV_DATASET]}}\n    response = client.import_data(dataset_full_id, input_config)\n    print(\"Data imported. {}\".format(response.result()))\n\n\n@func_to_container_op\ndef train_model(NAME, did):\n    import subprocess\n\n    def install(name):\n        subprocess.call(['pip', 'install', name])\n    install('google-cloud-automl')\n    import google.cloud.automl as automl\n    client = automl.AutoMlClient(credentials=credentials)\n    project_location = client.location_path(PROJECT_ID, COMPUTE_REGION)\n    MODEL_NAME = NAME\n    my_model = {\"display_name\": MODEL_NAME, \"dataset_id\": did, \"image_object_detection_model_metadata\": {}}\n    response = client.create_model(project_location, my_model)\n    print(\"Training operation name: {}\".format(response.operation.name))\n    print(\"Training done. {}\".format(response.result()))\n    model_id = response.result().name.split(\"/\")[-1]\n    print(model_id)\n\n\n@dsl.pipeline(\n    name='AutoML Vision pipeline',\n    description='A pipeline with AutoML Image Classification model training \\\n    steps.'\n)\ndef sequential_pipeline(dname='NewDataset', url='gs://ml-pipeline/test.csv', mname='NewModel'):\n    did = create_dataset(dname)\n    import_items(did, url)\n    train_model(mname, did)\n\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(sequential_pipeline, __file__ + '.tar.gz')\n"
  },
  {
    "repo": "PascalSchroederDE/kf-sample-pipeline",
    "file_path": "ml_test_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/PascalSchroederDE/kf-sample-pipeline/master/ml_test_pipeline.py",
    "content": "import kfp\nfrom kfp import components\nfrom kfp import dsl\nfrom kfp import onprem\n\n\ndef train_op(epochs, validations, workers, pvc_path, trainset, input, filenames, target,train_size, learn_rate, output):\n  return dsl.ContainerOp(\n    name='Train',\n    image='pascalschroeder/ml-train-test',\n    arguments=[\n      '--epochs', epochs,\n      '--validations', validations,\n      '--workers', workers,\n      '--pvc_path', pvc_path,\n      '--trainset', trainset,\n      '--input', input,\n      '--filenames', filenames,\n      '--target', target,\n      '--train_size', train_size,\n      '--learn_rate', learn_rate,\n      '--output', output\n    ],\n    file_outputs={\n        'output': '/output.txt'\n     } \n  )\n\ndef load_op(workers, pvc_path, testset, input, filenames, target, model, result):\n  return dsl.ContainerOp(\n    name='Load',\n    image='pascalschroeder/ml-load-test',\n    arguments=[\n      '--workers', workers,\n      '--pvc_path', pvc_path,\n      '--testset', testset,\n      '--input', input,\n      '--filenames', filenames,\n      '--target', target,\n      '--model', model,\n      '--output', result\n    ],\n  )\n\n\n@dsl.pipeline(\n  name='ML Test Pipeline',\n  description='Test'\n)\ndef train_pipeline(output=\"/mnt/model.h5\", result=\"/mnt/results.txt\", pvc_name=\"train-vol\", pvc_path=\"/mnt\", epochs=30, validations=10, trainset='/cut', testset='/cut', input='/train.csv', filenames='id', target='has_scratch', train_size=0.8, learn_rate=0.0001, workers=2):\n  train = train_op(epochs, validations, workers,  pvc_path, trainset, input, filenames, target, train_size, learn_rate, output).apply(onprem.mount_pvc(\"train-vol\", 'local-storage', \"/mnt\"))\n  load = load_op(workers, pvc_path, testset, input, filenames, target, train.outputs['output'], result).apply(onprem.mount_pvc(\"train-vol\", 'local-storage', \"/mnt\"))\n\nif __name__ == '__main__':\n  kfp.compiler.Compiler().compile(train_pipeline, __file__ + '.tar.gz')"
  },
  {
    "repo": "PascalSchroederDE/kf-sample-pipeline",
    "file_path": "pipeline-fashion-mnist.py",
    "raw_url": "https://raw.githubusercontent.com/PascalSchroederDE/kf-sample-pipeline/master/pipeline-fashion-mnist.py",
    "content": "import kfp\nfrom kfp import components\nfrom kfp import dsl\nfrom kfp import onprem\n\ndef data_prep_op(dataset_location, output):\n  return dsl.ContainerOp(\n    name='Data preparation',\n    image='pascalschroeder/kf-dataprep',\n    arguments=[\n      '--dataset_location', dataset_location,\n      '--output', output\n    ],\n    file_outputs={\n      'output': '/prepdf_output.txt'\n    }\n  )\n\ndef feature_eng_op(prep_dataset_location, output):\n  return dsl.ContainerOp(\n    name='Feature Engineering',\n    image='pascalschroeder/kf-featureeng',\n    arguments=[\n      '--dataset_location', prep_dataset_location,\n      '--output', output\n    ],\n    file_outputs={\n      'output': '/findf_output.txt'\n    }\n  )\n\ndef data_split_op(impr_dataset_location, test_size, random_state, output_train_img, output_train_label, output_test_img, output_test_label):\n  return dsl.ContainerOp(\n    name='Data Split',\n    image='pascalschroeder/kf-datasplit',\n    arguments=[\n      '--dataset_location', impr_dataset_location,\n      '--test_size', test_size,\n      '--random_state', random_state,\n      '--output_train_img', output_train_img,\n      '--output_train_label', output_train_label,\n      '--output_test_img', output_test_img,\n      '--output_test_label', output_test_label\n    ],\n    file_outputs={\n      'train_img': '/trainimg.txt',\n      'train_label': '/trainlabel.txt',\n      'test_img': '/testimg.txt',\n      'test_label': '/testlabel.txt',\n    }\n  )\n\ndef model_build_op(input_shape_height, input_shape_width, num_units, num_outputs, activation_l2, activation_l3, optimizer, loss, metrics, model_output):\n  return dsl.ContainerOp(\n    name='Model building',\n    image='pascalschroeder/kf-modelbuild',\n    arguments=[\n      '--input_shape_height', input_shape_height,\n      '--input_shape_width', input_shape_width,\n      '--num_units', num_units,\n      '--num_outputs', num_outputs,\n      '--activation_l2', activation_l2,\n      '--activation_l3', activation_l3,\n      '--optimizer', optimizer,\n      '--loss', loss,\n      '--metrics', metrics,\n      '--output', model_output\n    ],\n    file_outputs={\n      'output_model_loc': '/model.txt',\n    }\n  )\n\ndef model_download_op(input_shape_height, input_shape_width, model_output):\n  return dsl.ContainerOp(\n    name='Model download',\n    image='pascalschroeder/kf-modeldownload',\n    arguments=[\n      '--input_shape_height', input_shape_height,\n      '--input_shape_width', input_shape_width,\n      '--output', model_output\n    ],\n    file_outputs={\n      'output_model_loc': '/model.txt',\n    }\n  )\n\ndef model_train_op(input_train_img, input_train_label, input_shape_height, input_shape_width, model_location, epochs, model_output):\n  return dsl.ContainerOp(\n    name='Model training',\n    image='pascalschroeder/kf-modeltrain',\n    arguments=[\n      '--input_train_img', input_train_img,\n      '--input_train_label', input_train_label,\n      '--input_shape_height', input_shape_height,\n      '--input_shape_width', input_shape_width,\n      '--model_location', model_location,\n      '--epochs', epochs,\n      '--output', model_output\n    ],\n    file_outputs={\n      'output_model_loc': '/trained_model.txt',\n    }\n  )\n\ndef model_eval_op(input_test_img, input_test_label, input_shape_height, input_shape_width, model_location, result_output):\n  return dsl.ContainerOp(\n    name='Model evaluation',\n    image='pascalschroeder/kf-modeleval',\n    arguments=[\n      '--input_test_img', input_test_img,\n      '--input_test_label', input_test_label,\n      '--input_shape_height', input_shape_height,\n      '--input_shape_width', input_shape_width,\n      '--model_location', model_location,\n      '--output', result_output\n    ],\n    output_artifact_paths={\n            'mlpipeline-ui-metadata': '/mlpipeline-ui-metadata.json',\n    },\n  )\n\n@dsl.pipeline(\n  name='ML Test Pipeline',\n  description='Test'\n)\n\ndef pipeline(dataset_location='/mnt/data/manipulated_fashion_mnist.csv', test_size=0.3, random_state=42, input_shape_height=28, input_shape_width=28, use_pretrained_model='False', model_units_num=128, model_outputs_num=10, model_activation_func_layer2='relu', model_activation_func_layer3='softmax', optimizer='adam', loss='binary_crossentropy', metrics='accuracy', num_epochs=10, location_prepared_dataset='/mnt/data/prep_fashion_mnist.csv', location_improved_dataset='/mnt/data/impr_fasion_mnist.csv', location_training_images='/mnt/data/train_img.csv', location_training_labels='/mnt/data/train_labels.csv', location_test_images='/mnt/data/test_img.csv', location_test_labels='/mnt/data/test_labels.csv', location_base_model='/mnt/model/base_model.h5', location_trained_model='/mnt/model/trained_model.h5'):\n  data_preparation = data_prep_op(dataset_location, location_prepared_dataset).apply(onprem.mount_pvc(\"fashion-mnist-vol\", 'local-storage', \"/mnt\"))\n  feature_engineering = feature_eng_op(data_preparation.outputs['output'], location_improved_dataset).apply(onprem.mount_pvc(\"fashion-mnist-vol\", 'local-storage', \"/mnt\"))\n  data_split = data_split_op(feature_engineering.outputs['output'], test_size, random_state, location_training_images, location_training_labels, location_test_images, location_test_labels).apply(onprem.mount_pvc(\"fashion-mnist-vol\", 'local-storage', \"/mnt\"))\n  \n  with dsl.Condition(use_pretrained_model == 'True'):\n    model_building = model_download_op(input_shape_height, input_shape_width, location_base_model).apply(onprem.mount_pvc(\"fashion-mnist-vol\", 'local-storage', \"/mnt\"))\n    model_training = model_train_op(data_split.outputs['train_img'], data_split.outputs['train_label'], input_shape_height, input_shape_width, model_building.outputs['output_model_loc'], num_epochs, location_trained_model).apply(onprem.mount_pvc(\"fashion-mnist-vol\", 'local-storage', \"/mnt\"))\n    model_evaluation = model_eval_op(data_split.outputs['test_img'], data_split.outputs['test_label'], input_shape_height, input_shape_width, model_training.outputs['output_model_loc'], '/mlpipeline-ui-metadata.json').apply(onprem.mount_pvc(\"fashion-mnist-vol\", 'local-storage', \"/mnt\"))\n\n\n  with dsl.Condition(use_pretrained_model == 'False'):\n    model_building = model_build_op(input_shape_height, input_shape_width, model_units_num, model_outputs_num, model_activation_func_layer2, model_activation_func_layer3, optimizer, loss, metrics, location_base_model).apply(onprem.mount_pvc(\"fashion-mnist-vol\", 'local-storage', \"/mnt\"))\n    model_training = model_train_op(data_split.outputs['train_img'], data_split.outputs['train_label'], input_shape_height, input_shape_width, model_building.outputs['output_model_loc'], num_epochs, location_trained_model).apply(onprem.mount_pvc(\"fashion-mnist-vol\", 'local-storage', \"/mnt\"))\n    model_evaluation = model_eval_op(data_split.outputs['test_img'], data_split.outputs['test_label'], input_shape_height, input_shape_width, model_training.outputs['output_model_loc'], '/mlpipeline-ui-metadata.json').apply(onprem.mount_pvc(\"fashion-mnist-vol\", 'local-storage', \"/mnt\"))\n\n\n\nif __name__ == '__main__':\n  kfp.compiler.Compiler().compile(pipeline, __file__ + '.tar.gz')"
  },
  {
    "repo": "stackdemos/got-image-classification",
    "file_path": "components/golang/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/got-image-classification/master/components/golang/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef processing_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('GOLANG_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=[script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n"
  },
  {
    "repo": "stackdemos/got-image-classification",
    "file_path": "components/training/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/got-image-classification/master/components/training/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef training_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('TRAINING_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=['/usr/local/bin/python', script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n\n"
  },
  {
    "repo": "drbwa/kfp-hello-world",
    "file_path": "kfp_hello_world/kfp_hello_world.py",
    "raw_url": "https://raw.githubusercontent.com/drbwa/kfp-hello-world/main/kfp_hello_world/kfp_hello_world.py",
    "content": "from kfp import dsl\nfrom kfp import compiler\nfrom kfp import Client\n\n# One component used in a simple pipeline. Compiles pipeline to YAML and creates \n# a run from the pipeline YAML.\n\n@dsl.component\ndef say_hello(name: str) -> str:\n    hello_text = f\"Hello, {name}!\"\n    print(hello_text)\n    return hello_text\n\n\n@dsl.pipeline\ndef hello_pipeline(recipient: str) -> str:\n    hello_task = say_hello(name=recipient)\n    return hello_task.output\n\n\ndef compile(pipeline_name: str):\n    compiler.Compiler().compile(hello_pipeline, pipeline_name)\n\n\ndef run(endpoint: str, pipeline_name: str) -> str:\n    client = Client(host=endpoint)\n    run = client.create_run_from_pipeline_package(\n        pipeline_name, \n        arguments={\n            \"recipient\": \"World\",\n        },\n    )\n    url = f\"{endpoint}/#/runs/details/{run.run_id}\"\n    return url\n\n\ndef main():\n    pipeline_name = \"pipeline.yaml\"\n    endpoint = \"http://localhost:8080\"\n    compile(pipeline_name)\n    url = run(endpoint, pipeline_name)\n    print(url)\n\n\nif __name__ == \"__main__\":\n    main()\n"
  },
  {
    "repo": "drbwa/kfp-hello-world",
    "file_path": "kfp_hello_world/kfp_hello_world2.py",
    "raw_url": "https://raw.githubusercontent.com/drbwa/kfp-hello-world/main/kfp_hello_world/kfp_hello_world2.py",
    "content": "from kfp import dsl\nfrom kfp import Client\n\n# Simple pipeline that chains to invocations of a single component to each\n# other. Does not produce KFP IR through compilation and instead creates a run\n# directly from the pipeline function defined here.\n\n@dsl.component\ndef addition_component(num1: int, num2: int) -> int:\n    return num1 + num2\n\n@dsl.pipeline(name='addition-pipeline')\ndef my_pipeline(a: int, b: int, c: int = 10):\n    add_task_1 = addition_component(num1=a, num2=b)\n    add_task_2 = addition_component(num1=add_task_1.output, num2=c)\n\ndef run(endpoint: str, pipeline) -> str:\n    client = Client(host=endpoint)\n    run = client.create_run_from_pipeline_func(\n        pipeline, \n        arguments={\n            \"a\": 1,\n            \"b\": 2\n        },\n    )\n    url = f\"{endpoint}/#/runs/details/{run.run_id}\"\n    return url\n\ndef main():\n    endpoint = \"http://localhost:8080\"\n    pipeline = my_pipeline\n    url = run(endpoint, pipeline)\n    print(url)\n\nif __name__ == \"__main__\":\n    main()"
  },
  {
    "repo": "hh10/Minimal-Kubeflow-Pipeline-Template",
    "file_path": "kf_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/hh10/Minimal-Kubeflow-Pipeline-Template/main/kf_pipeline.py",
    "content": "import os\nimport sys\nimport kfp\nfrom kfp import dsl\nfrom kubernetes.client.models import V1Volume, V1VolumeMount\n\n_IMAGE = \"eu.gcr.io/grand-kingdom-352313/test_proj_image2\"  # change to your project/image path\n_PERSISTENT_VOL_CLAIM_PATH = \"/mnt/pvolume\"\n\n\ndef create_volume(size: str, vol_res_name: str = '', datasource_res_name: str = '', last_op=None):\n    assert not vol_res_name or not datasource_res_name, print(\"Provide either 'vol_res_name' for new volume creation or 'datasource_res_name' for cloning it, not both\")\n\n    vname, data_source, access_mode = f\"shared-volume-{size}\", None, dsl.VOLUME_MODE_RWO\n    if datasource_res_name:\n        # a PVC claim to clone is provided\n        data_source = \"{{workflow.name}}-\" + datasource_res_name\n        vol_res_name = datasource_res_name + \"-clone\"\n        vname += \"-clone\"\n        access_mode = dsl.VOLUME_MODE_ROM\n\n    vop = dsl.VolumeOp(\n        name=vname,\n        resource_name=vol_res_name,\n        storage_class=\"standard-rwo\",\n        data_source=data_source,\n        modes=access_mode,\n        size=size,\n    )\n    return vop.after(last_op) if last_op else vop, vol_res_name\n\n\ndef release_pvc(pvc_res_name: str, last_op):\n    pvc_release_cmd = 'kubectl delete pvc {{workflow.name}}-%s -n kubeflow --wait=false && \\\n                       kubectl patch pvc {{workflow.name}}-%s -n kubeflow -p \\'{\"metadata\":{\"finalizers\": []}}\\' --type=merge' % (pvc_res_name, pvc_res_name)\n    return dsl.ContainerOp(\n        name=\"release-shared-volume\",\n        image=\"google/cloud-sdk:216.0.0\",\n        command=[\"bash\", \"-c\"],\n        arguments=[pvc_release_cmd]).after(last_op)\n\n\ndef clone_repo(vop: dsl.VolumeOp, git_repo: str):\n    repo_name = \"cloned_repo\"\n    repo_path = os.path.join(_PERSISTENT_VOL_CLAIM_PATH, repo_name)\n    cases_outpath = \"/tmp/test.config\"\n    get_cases_args = f\"rm -r {repo_path}; \\\n                       cat /root/.ssh/id_rsa; \\\n                       echo git clone {git_repo} {repo_path}; \\\n                       git clone {git_repo} {repo_path}; \\\n                       ls {repo_path}; \\\n                       python3 -c \\\"import os; import json; json.dump([os.path.join('{repo_path}', f) for f in os.listdir('{repo_path}') if os.path.isfile(os.path.join('{repo_path}', f))], open('{cases_outpath}', 'w'))\\\"; \\\n                       cat {cases_outpath};\"\n    return dsl.ContainerOp(\n        name=\"get-files-from-repo\",\n        image=_IMAGE,\n        command=[\"bash\", \"-c\"],\n        arguments=[f\"{get_cases_args}\"],\n        file_outputs={\"config\": \"/tmp/test.config\"},\n        pvolumes={_PERSISTENT_VOL_CLAIM_PATH: vop.volume},\n    )\n\n\ndef node_op(filepath: str, gcs_results_location: str, vop_clone: dsl.VolumeOp, vol_clone_res_name: str):\n    srun = dsl.ContainerOp(\n        name=\"single-file-upload\",\n        image=_IMAGE,\n        command=[\"bash\", \"-c\"],\n        arguments=[f'ls /mnt/pvolume/cloned_repo && sleep 15 && gsutil cp \"{filepath}\" \"{gcs_results_location}\"'],\n    )\n    # set these resource requests/limits to ensure that each pod is assigned to a single node/machine \n    srun.set_cpu_request(\"500m\")\n    srun.set_cpu_limit(\"800m\")\n    srun.add_volume(\n        V1Volume(\n            name=vop_clone.name,\n            persistent_volume_claim={\"claimName\": \"{{workflow.name}}-\" + vol_clone_res_name, \"readOnly\": True},\n        )\n    )\n    srun.add_volume_mount(V1VolumeMount(mount_path=_PERSISTENT_VOL_CLAIM_PATH, name=vop_clone.name, read_only=True))\n    return srun\n\n\n@dsl.pipeline(name=\"Verification pipeline\", description=\"A simple pipeline that fetches a repo and uploads its files to a GCP bucket in parallel.\")\ndef si_verpipeline(\n    git_repo: str = \"git@github.com:hh10/AIHack2022.git\",\n    gcs_results_location: str = \"gs://si-testbucket-1\",\n    run_label: str = \"default\",\n):\n    vol_size = \"1Gi\"\n    vop, vol_res_name = create_volume(vol_size, vol_res_name=\"pvc\")\n    repo_files = clone_repo(vop, git_repo)\n    \n    gcs_results_location = f\"{gcs_results_location}/{run_label}/\"\n    vop_clone, vol_clone_res_name = create_volume(vol_size, datasource_res_name=vol_res_name, last_op=repo_files)\n    with dsl.ParallelFor(repo_files.output).after(vop_clone) as filepath:\n        single_result = node_op(filepath, gcs_results_location, vop_clone, vol_clone_res_name)\n    # cleanup pv(c)s\n    vop_clone_release = release_pvc(vol_clone_res_name, single_result)\n    release_pvc(vol_res_name, vop_clone_release)\n\n\nif __name__ == \"__main__\":\n    assert len(sys.argv) == 2, print(\"only one input, i.e., output yaml path should be provided.\")\n    output_path = sys.argv[1]\n    kfp.compiler.Compiler().compile(si_verpipeline, output_path)\n    os.system(\n        \"sed -E -i \\\"s/^( *)add: \\\\['- SYS_ADMIN'\\\\]/\\\\1add:\\\\n\\\\1- SYS_ADMIN/g\\\" {pipeline}\".format(\n            pipeline=output_path\n        )\n    )\n"
  },
  {
    "repo": "shahriar0999/ml-pipeline-with-kubeflow",
    "file_path": "kubeflow_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/shahriar0999/ml-pipeline-with-kubeflow/main/kubeflow_pipeline.py",
    "content": "from typing import Dict, List\nfrom kfp import dsl\nfrom kfp import compiler\nimport kfp\nfrom kfp.dsl import Input, Output, Dataset, Model, component\n\n# Step 1: Load Dataset\n@dsl.component(base_image=\"python:3.9\")\ndef load_data(output_csv: Output[Dataset]):\n    import subprocess\n    subprocess.run([\"pip\", \"install\", \"pandas\", \"scikit-learn\"], check=True)\n\n    from sklearn.datasets import load_iris\n    import pandas as pd\n    iris = load_iris()\n    df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n    df['target'] = iris.target\n\n    # Save the dataset to the output artifact path\n    df.to_csv(output_csv.path, index=False)\n\n# Step 2: Preprocess Data\n@dsl.component(base_image=\"python:3.9\")\ndef preprocess_data(input_csv: Input[Dataset], output_train: Output[Dataset], output_test: Output[Dataset], \n                    output_ytrain: Output[Dataset], output_ytest: Output[Dataset]):\n    import subprocess\n    subprocess.run([\"pip\", \"install\", \"pandas\", \"scikit-learn\"], check=True)\n\n    import pandas as pd\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.model_selection import train_test_split\n\n    # Load dataset\n    df = pd.read_csv(input_csv.path)\n\n    # Debug: Check for NaN values\n    print(\"Initial dataset shape:\", df.shape)\n    print(\"Missing values before preprocessing:\\n\", df.isnull().sum())\n\n    # Handle missing values\n    if df.isnull().values.any():\n        print(\"Missing values detected. Handling them...\")\n        df = df.dropna()  # Drop rows with any NaN values\n    \n    # Validate that there are no NaNs in the target column\n    assert not df['target'].isnull().any(), \"Target column contains NaN values after handling missing values.\"\n\n    features = df.drop(columns=['target'])\n    target = df['target']\n\n    # Standardize features\n    scaler = StandardScaler()\n    scaled_features = scaler.fit_transform(features)\n\n    # Train-test split\n    X_train, X_test, y_train, y_test = train_test_split(scaled_features, target, test_size=0.2, random_state=42)\n\n    # Debug: Validate splits\n    print(\"Shapes after train-test split:\")\n    print(\"X_train:\", X_train.shape, \"X_test:\", X_test.shape) \n    print(\"y_train:\", y_train.shape, \"y_test:\", y_test.shape)\n    print(\"Missing values in y_train:\", y_train.isnull().sum())\n\n    # Ensure no NaNs in the split data\n    assert not y_train.isnull().any(), \"y_train contains NaN values.\"\n    assert not y_test.isnull().any(), \"y_test contains NaN values.\"\n\n    # Create DataFrames for train and test sets\n    X_train_df = pd.DataFrame(X_train, columns=features.columns)\n    print(\"X_train_df:\", X_train_df) \n\n    y_train_df = pd.DataFrame(y_train) \n    print(\"y_train_df: \", y_train_df)  \n\n    X_test_df = pd.DataFrame(X_test, columns=features.columns)\n    print(\"X_test_df:\", X_test_df) \n\n    y_test_df = pd.DataFrame(y_test) \n    print(\"y_test_df: \", y_test_df) \n\n    # Save processed train and test data\n    X_train_df.to_csv(output_train.path, index=False)  \n    X_test_df.to_csv(output_test.path, index=False)\n\n    y_train_df.to_csv(output_ytrain.path, index=False)  \n    y_test_df.to_csv(output_ytest.path, index=False) \n\n# Step 3: Train Model\n@dsl.component(base_image=\"python:3.9\")\ndef train_model(train_data: Input[Dataset], ytrain_data: Input[Dataset], model_output: Output[Model]):\n\n    import subprocess\n    subprocess.run([\"pip\", \"install\", \"pandas\", \"scikit-learn\", \"joblib\"], check=True)\n\n    import pandas as pd\n    from sklearn.linear_model import LogisticRegression\n    from joblib import dump\n\n    # Load training data\n    train_df = pd.read_csv(train_data.path)\n    print(\"Shape of train_df:\", train_df.shape)\n    print(\"train_df:\", train_df)\n    X_train = train_df \n\n    y_train = pd.read_csv(ytrain_data.path)\n    print(\"Shape of ytrain_df:\", y_train.shape)\n    print(\"y_train_df:\", y_train)\n\n    # Debug: Validate splits\n    print(\"Shapes of X_train and y_train: \")\n    print(\"X_train:\", X_train.shape)\n    print(\"y_train:\", y_train.shape) \n    print(\"Missing values in X_train:\", X_train.isnull().sum())\n    print(\"Missing values in y_train:\", y_train.isnull().sum()) \n\n    # Ensure no NaN values\n    assert not X_train.isnull().values.any(), \"X_train contains NaN values.\"\n    assert not y_train.isnull().values.any(), \"y_train contains NaN values.\" \n\n    # Train model\n    model = LogisticRegression()\n    model.fit(X_train, y_train)\n\n    # Save model\n    dump(model, model_output.path)\n\n# Step 4: Evaluate Model\n@dsl.component(base_image=\"python:3.9\")\ndef evaluate_model(test_data: Input[Dataset], ytest_data: Input[Dataset], model: Input[Model], metrics_output: Output[Dataset]):\n    import subprocess\n    subprocess.run([\"pip\", \"install\", \"pandas\", \"scikit-learn\", \"matplotlib\", \"joblib\"], check=True)\n\n    import pandas as pd\n    from sklearn.metrics import classification_report, confusion_matrix\n    import matplotlib.pyplot as plt\n    from joblib import load\n\n    # Load test data\n    X_test = pd.read_csv(test_data.path)\n\n    y_test = pd.read_csv(ytest_data.path)  \n\n    # Load model\n    model = load(model.path)\n\n    # Predict\n    y_pred = model.predict(X_test)\n\n    # Generate metrics\n    report = classification_report(y_test, y_pred, output_dict=True)\n    cm = confusion_matrix(y_test, y_pred)\n\n    # Save metrics to a file\n    metrics_path = metrics_output.path\n    with open(metrics_path, 'w') as f:\n        f.write(str(report))\n\n    # Plot confusion matrix\n    plt.figure(figsize=(8, 6))\n    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n    plt.title('Confusion Matrix')\n    plt.colorbar()\n    plt.xlabel('Predicted Label')\n    plt.ylabel('True Label')\n    plt.savefig(metrics_path.replace('.txt', '.png'))\n\n# Define the pipeline\n@dsl.pipeline(name=\"ml-pipeline\")\ndef ml_pipeline():\n    # Step 1: Load Dataset\n    load_op = load_data()\n\n    # Step 2: Preprocess Data\n    preprocess_op = preprocess_data(input_csv=load_op.outputs[\"output_csv\"])\n\n    # Step 3: Train Model\n    train_op = train_model(train_data=preprocess_op.outputs[\"output_train\"], ytrain_data=preprocess_op.outputs[\"output_ytrain\"])\n\n    # Step 4: Evaluate Model\n    evaluate_op = evaluate_model(test_data=preprocess_op.outputs[\"output_test\"], ytest_data=preprocess_op.outputs[\"output_ytest\"], model=train_op.outputs[\"model_output\"]) \n\n# Compile the pipeline\nif __name__ == \"__main__\":\n    compiler.Compiler().compile(pipeline_func=ml_pipeline, package_path=\"kubeflow_pipeline.yaml\")\n"
  },
  {
    "repo": "quynhtl/build-pipeline-on-Kubeflow",
    "file_path": "pipeline01.py",
    "raw_url": "https://raw.githubusercontent.com/quynhtl/build-pipeline-on-Kubeflow/main/pipeline01.py",
    "content": "\nfrom kfp import dsl\nfrom kfp import compiler\nimport os\n\n\n@dsl.pipeline(name='My Pipeline', description='A pipeline with 3 steps')\ndef my_pipeline():\n    data_prep = dsl.ContainerOp(\n        name='Data Preparation',\n        image='quynhtl/today_code1:latest',\n        command=['python', 'download.py'],\n        file_outputs = {\"output\": \"/data\"}\n    )\n\n    model_train = dsl.ContainerOp(\n        name='Model Training',\n        image='quynhtl/today_code1:latest',\n        command=['python', 'model_train.py'],\n        arguments=['--input-folder', data_prep.output]\n    )\n    model_train.after(data_prep)\n\n    # model_version = dsl.ContainerOp(\n    #     name='Model Versioning',\n    #     image='quynhtl/today_code1:latest',\n    #     command=['python', 'version_data.py'],\n    #     arguments=['--model-dir', model_train.outputs['model_path'], '--version-dir', '/version']\n    # )\n    # model_version.after(model_train)\n\n\nif __name__ == '__main__':\n    compiler.Compiler().compile(my_pipeline, 'my_pipeline.zip')"
  },
  {
    "repo": "quynhtl/build-pipeline-on-Kubeflow",
    "file_path": "pipeline_base/test1.py",
    "raw_url": "https://raw.githubusercontent.com/quynhtl/build-pipeline-on-Kubeflow/main/pipeline_base/test1.py",
    "content": "\nimport kfp\nfrom kfp import dsl\nimport kfp.components as comp\n\n\n@comp.create_component_from_func\ndef echo_op():\n    print(\"Hello world\")\n\n@dsl.pipeline(\n    name='my-first-pipeline',\n    description='A hello world pipeline.'\n)\ndef hello_world_pipeline():\n    echo_task = echo_op()\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(hello_world_pipeline, __file__ + '.yaml')"
  },
  {
    "repo": "ruteee/kubeflow-pipeline-gcp-deploy",
    "file_path": "source/main.py",
    "raw_url": "https://raw.githubusercontent.com/ruteee/kubeflow-pipeline-gcp-deploy/main/source/main.py",
    "content": "import logging\nimport sys\nfrom kfp.v2 import compiler, dsl\nfrom kfp.v2.dsl import Input, Output, Dataset, Model\nfrom google.cloud import aiplatform\n\n\nlogging.basicConfig(level=\"INFO\", stream=sys.stdout)\n\ndef compile_pipeline(pipeline_func):   \n    compiler.Compiler().compile(\n        pipeline_func=pipeline_func,\n        package_path=\"./tmp/my_pipeline.json\")\n    \n\n@dsl.component(\n    packages_to_install=[\n        \"ucimlrepo==0.0.7\",\n        \"fastparquet==2023.7.0\"\n    ],\n    base_image=\"python:3.9\")\ndef load_data(dataset: Output[Dataset]):\n    \"\"\"\n    Get iris dataset from UCI reposiory\n    Returns: \n        df_data - Dataframe containing 4 features \n        regarding iris dataset and the target\n    \"\"\"\n    import logging\n    from ucimlrepo import fetch_ucirepo\n\n    logging.info(\"Getting Dataset\")\n    data_iris = fetch_ucirepo(id=53) \n    df_data = data_iris.data.features \n    df_data.rename(columns = {\n        'sepal length' : 'sepal_length',\n        'sepal width' : 'sepal_width',\n        'petal length' : 'petal_length',\n        'petal width': 'petal_width'\n    }, inplace=True)\n    target_array = data_iris.data.targets['class']\n\n    df_data['target'] = target_array\n\n    df_data.to_csv(dataset.path)\n\n\n@dsl.component(packages_to_install=[\"scikit-learn==1.5.2\"], base_image=\"python:3.9\")\ndef set_training_pipeline(pipeline_out: Output[Model]):\n    \"\"\"\n    Defines training pipeline steps and returns the pipeline\n    \"\"\"\n    import joblib\n    from sklearn.impute import SimpleImputer\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.pipeline import Pipeline\n    from sklearn.preprocessing import StandardScaler\n    \n    pipeline = Pipeline(steps = [\n        ('Imputer', SimpleImputer(strategy='mean', keep_empty_features=True)),\n        ('normalization', StandardScaler()),\n        ('estimator', LogisticRegression() )\n        ]\n    )\n    joblib.dump(pipeline, pipeline_out.path)\n\n\n@dsl.component(packages_to_install=[\"scikit-learn==1.5.2\"], base_image=\"python:3.9\")\ndef train_model(\n    dataset: Input[Dataset],\n    pipeline: Input[Model],\n    output_model: Output[Model]\n)-> Model:\n    import logging\n    import joblib\n    import pandas as pd\n    from sklearn.metrics import accuracy_score\n    from sklearn.model_selection import GridSearchCV, train_test_split\n\n    dataset = pd.read_csv(dataset.path)\n    logging.info(f\"Spliting dataset\")\n    X_train, X_test, y_train, y_test = train_test_split(\n        dataset.drop(columns=\"target\"),\n        dataset[\"target\"],\n        test_size=0.2, \n        random_state=14)\n\n    logging.info(f\"Fittig model with train data\")\n\n    parameters = {\n        'estimator__solver': ['newton-cg'],\n        'estimator__tol': [ 0.0001, 0.003, 0.01],\n        'estimator__penalty': [None, 'l2'],\n    }\n\n    model = GridSearchCV(estimator=pipeline,\n                            param_grid=parameters,\n                            scoring= {\"AUC\": \"roc_auc_ovr\"},\n                            refit=\"AUC\",\n                            cv=5,\n                            verbose=1,\n                            error_score='raise')\n    \n    pipeline = joblib.load(pipeline)\n    model = pipeline.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n\n    logging.info(f\"Computing scores\")\n    model_score = model.score(X_test, y_test)\n    logging.info(f\"Model AUC Score: {model_score}\")\n\n    test_acc_score = accuracy_score(y_test, y_pred)\n    logging.info(f\"Accuracy test score: {test_acc_score}\")\n\n    logging.info(\"Saving model\")\n    joblib.dump(model, output_model.path)\n\n\nPIPELINE_ROOT=\"/tmp\"\n@dsl.pipeline(\n    name=\"my-pipeline-\",\n    description=\"A class project\",\n    pipeline_root=PIPELINE_ROOT\n)\ndef my_pipeline_func():\n    load_data_component = load_data()\n    set_training_pipe_component = set_training_pipeline(\n        \n    ).after(load_data_component)\n\n    fit_model_component = train_model(\n        dataset = load_data_component.output,\n        pipeline = set_training_pipe_component.outputs['pipeline_out']\n    ).after(set_training_pipe_component)\n\n\ndef execute_pipeline():\n    compile_pipeline(my_pipeline_func)\n    PIPELINE_ROOT = \"./tmp\"\n    aiplatform.init(project=\"personal-448814\",\n                    location=\"us-central1\",\n                    staging_bucket=(\n                        f\"gs://kfp_bucket_pipeline/{PIPELINE_ROOT}/\"\n                    ))\n    job = aiplatform.PipelineJob(\n        display_name=\"A pipeline for a class project\",\n        template_path=f\"{PIPELINE_ROOT}/my_pipeline.json\",\n        pipeline_root=f\"gs://kfp_bucket_pipeline/{PIPELINE_ROOT}/\",\n        project=\"personal-448814\",\n        location=\"us-central1\",\n        enable_caching=False\n        \n    )\n    job.run(\n        service_account=(\n            \"personal@personal-448814.iam.gserviceaccount.com\"\n        )\n    )\n"
  },
  {
    "repo": "lajd/local_kubeflow",
    "file_path": "examples/pytorch_lightning_cifar10/pl_train_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/lajd/local_kubeflow/main/examples/pytorch_lightning_cifar10/pl_train_pipeline.py",
    "content": "#!/usr/bin/env python3\n# Ref: https://pytorch-lightning.readthedocs.io/en/stable/notebooks/lightning_examples/cifar10-baseline.html\n\nimport kfp.dsl as kfp\n\n\ndef training_op(\n        lr: float=0.05,\n        momentum: float=0.9,\n        wd: float=5e-4,\n        max_lr: float=0.1,\n        batch_size: int = 64,\n        num_workers: int = 8,\n        max_epochs: int = 30,\n        step_name: str = 'training'\n):\n  return kfp.ContainerOp(\n    name=step_name,\n    image='lajd94/kubeflow_examples:pytorch_lightning_example',\n    command=['/bin/bash', '-c'],\n    arguments=[\n      f'python train.py'\n      f' --lr {lr}'\n      f' --momentum {momentum}'\n      f' --wd {wd}'\n      f' --max_lr {max_lr}'\n      f' --batch_size {batch_size}'\n      f' --num_workers {num_workers}'\n      f' --max_epochs {max_epochs}'\n    ],\n    # TODO: Handle directory output\n    # file_outputs={'output': '/opt/model/lightning_logs/'},\n  )\n\n\n@kfp.pipeline(\n  name='Pipeline Example',\n  description='Demonstrate the Kubeflow pipelines SDK'\n)\n\ndef kubeflow_training(\n    lr: kfp.PipelineParam = kfp.PipelineParam(name='lr', value=0.05),\n    momentum: kfp.PipelineParam = kfp.PipelineParam(name='momentum', value=0.9),\n    wd: kfp.PipelineParam = kfp.PipelineParam(name='wd', value=5e-4),\n    max_lr: kfp.PipelineParam = kfp.PipelineParam(name='max_lr', value=0.1),\n    batch_size: kfp.PipelineParam = kfp.PipelineParam(name='batch_size', value=64),\n    num_workers: kfp.PipelineParam = kfp.PipelineParam(name='num_workers', value=8),\n    max_epochs: kfp.PipelineParam = kfp.PipelineParam(name='max_epochs', value=30),\n  ):\n\n  training = training_op(lr, momentum, wd, max_lr, batch_size, num_workers, max_epochs)\n\n\nif __name__ == '__main__':\n  import kfp.compiler as compiler\n  compiler.Compiler().compile(kubeflow_training, __file__ + '.tar.gz')\n"
  },
  {
    "repo": "lajd/local_kubeflow",
    "file_path": "examples/simple_pipeline/simple_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/lajd/local_kubeflow/main/examples/simple_pipeline/simple_pipeline.py",
    "content": "#!/usr/bin/env python3\n# Ref: https://github.com/kubeflow/examples/tree/master/demos/simple_pipeline\n\nimport kfp.dsl as kfp\n\n\ndef training_op(learning_rate: float,\n                num_layers: int,\n                optimizer='ftrl',\n                step_name='training'):\n  return kfp.ContainerOp(\n    name=step_name,\n    image='katib/mxnet-mnist-example',\n    command=['python', '/mxnet/example/image-classification/train_mnist.py'],\n    arguments=[\n      '--batch-size', '64',\n      '--lr', learning_rate,\n      '--num-layers', num_layers,\n      '--optimizer', optimizer\n    ],\n    file_outputs={'output': '/etc/timezone'},\n  )\n\n\ndef postprocessing_op(output,\n                      step_name='postprocessing'):\n  return kfp.ContainerOp(\n    name=step_name,\n    image='library/bash:4.4.23',\n    command=['sh', '-c'],\n    arguments=['echo \"%s\"' % output]\n  )\n\n\n@kfp.pipeline(\n  name='Pipeline Example',\n  description='Demonstrate the Kubeflow pipelines SDK'\n)\n\ndef kubeflow_training(\n  learning_rate: kfp.PipelineParam = kfp.PipelineParam(name='learningrate', value=0.1),\n  num_layers: kfp.PipelineParam = kfp.PipelineParam(name='numlayers', value='2'),\n  optimizer: kfp.PipelineParam = kfp.PipelineParam(name='optimizer', value='ftrl')):\n\n  training = training_op(learning_rate, num_layers, optimizer)\n  postprocessing = postprocessing_op(training.output)  # pylint: disable=unused-variable\n\n\nif __name__ == '__main__':\n  import kfp.compiler as compiler\n  compiler.Compiler().compile(kubeflow_training, __file__ + '.tar.gz')\n"
  },
  {
    "repo": "lajd/local_kubeflow",
    "file_path": "examples/spark/spark_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/lajd/local_kubeflow/main/examples/spark/spark_pipeline.py",
    "content": "# Ref: https://github.com/sbakiu/kubeflow-spark/blob/main/kubeflow_pipeline.py\n\nimport json\nimport time\nimport yaml\n\nimport kfp.components as comp\nimport kfp.dsl as dsl\n\nSPARK_COMPLETED_STATE = \"COMPLETED\"\nSPARK_APPLICATION_KIND = \"sparkapplications\"\nSPARK_JOB_YAML_PATH = \"./spark_job.yaml\"\nK8S_GET_COMPONENT_PATH = \"./k8s_get_component.yaml\"\nK8S_APPLY_COMPONENT_PATH = \"./k8s_apply_component.yaml\"\n\n\ndef get_spark_job_definition():\n    \"\"\"\n    Read Spark Operator job manifest file and return the corresponding dictionary and\n    add some randomness in the job name\n    :return: dictionary defining the spark job\n    \"\"\"\n    # Read manifest file\n    with open(SPARK_JOB_YAML_PATH,  \"r\") as stream:\n        spark_job_manifest = yaml.safe_load(stream)\n\n    # Add epoch time in the job name\n    epoch = int(time.time())\n    spark_job_manifest[\"metadata\"][\"name\"] = spark_job_manifest[\"metadata\"][\"name\"].format(epoch=epoch)\n\n    return spark_job_manifest\n\n\ndef print_op(msg):\n    \"\"\"\n    Op to print a message.\n    \"\"\"\n    return dsl.ContainerOp(\n        name=\"Print message.\",\n        image=\"alpine:3.6\",\n        command=[\"echo\", msg],\n    )\n\n\n@dsl.graph_component  # Graph component decorator is used to annotate recursive functions\ndef graph_component_spark_app_status(input_application_name):\n    k8s_get_op = comp.load_component_from_file(K8S_GET_COMPONENT_PATH)\n    check_spark_application_status_op = k8s_get_op(\n        name=input_application_name,\n        kind=SPARK_APPLICATION_KIND\n    )\n    # Remove cache\n    check_spark_application_status_op.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n\n    time.sleep(5)\n    with dsl.Condition(check_spark_application_status_op.outputs[\"applicationstate\"] != SPARK_COMPLETED_STATE):\n        graph_component_spark_app_status(check_spark_application_status_op.outputs[\"name\"])\n\n\n@dsl.pipeline(\n    name=\"Spark Operator job pipeline\",\n    description=\"Spark Operator job pipeline\"\n)\ndef spark_job_pipeline():\n\n    # Load spark job manifest\n    spark_job_definition = get_spark_job_definition()\n\n    # Load the kubernetes apply component\n    k8s_apply_op = comp.load_component_from_file(K8S_APPLY_COMPONENT_PATH)\n\n    # Execute the apply command\n    spark_job_op = k8s_apply_op(object=json.dumps(spark_job_definition))\n\n    # Fetch spark job name\n    spark_job_name = spark_job_op.outputs[\"name\"]\n\n    # Remove cache for the apply operator\n    spark_job_op.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n\n    spark_application_status_op = graph_component_spark_app_status(spark_job_op.outputs[\"name\"])\n    spark_application_status_op.after(spark_job_op)\n\n    print_message = print_op(f\"Job {spark_job_name} is completed.\")\n    print_message.after(spark_application_status_op)\n    print_message.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n\n\nif __name__ == \"__main__\":\n    # Compile the pipeline\n    import kfp.compiler as compiler\n    import logging\n    logging.basicConfig(level=logging.INFO)\n    pipeline_func = spark_job_pipeline\n    pipeline_filename = pipeline_func.__name__ + \".yaml\"\n    compiler.Compiler().compile(pipeline_func, pipeline_filename)\n    logging.info(f\"Generated pipeline file: {pipeline_filename}.\")\n"
  },
  {
    "repo": "awskosehy/fashion_mnist_kubeflow_pipeline",
    "file_path": "pipeline/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/awskosehy/fashion_mnist_kubeflow_pipeline/main/pipeline/pipeline.py",
    "content": "import kfp\nimport kfp.dsl as dsl\nimport requests\nimport os\nimport json\nimport config\n\nfrom kfp import components\nfrom kfp import onprem\nfrom kubernetes.client.models import V1EnvVar\n\ncreate_pytorchjob_fashion_mnist_katib_experiment_op = kfp.components.load_component_from_file(\n    \"/src/2_katib/component.yaml\"\n)\ncreate_tensorboard_launcher_op = kfp.components.load_component_from_file(\n    \"/src/3_tensorboard/component.yaml\"\n)\ncreate_model_path_launcher_op = kfp.components.load_component_from_file(\n    \"/src/4_model_path/component.yaml\"\n)\n\ndef extract_best_trial_name(katib_results) -> str:\n    \"\"\" Extract the best trial name.\n      \n    Args:\n    katib_results: The JSON object formatted the hyperparameter set of the best experiment trial result\n\n    Returns:\n    best_trial_name: string which contain the best experiment trial name\n    \"\"\"\n    import json\n    import pprint\n    katib_results_json = json.loads(katib_results)\n    best_trial_name = katib_results_json[\"currentOptimalTrial\"][\"bestTrialName\"] + '-master-0'\n\n    return best_trial_name\n\n\n@dsl.pipeline(\n    name=config.PIPELINE_NAME,\n    description=config.DESCRIPTION\n)\n\ndef create_pytorch_fashion_mnist_pipeline():\n    fashion_mnist_katib_experiment = create_pytorchjob_fashion_mnist_katib_experiment_op(\n        experiment_name=config.EXPERIMENT_NAME,\n        experiment_namespace=config.EXPERIMENT_NAMESPACE,\n        dataset_path=config.DATASET_PATH,\n        log_dir=config.MODEL_DIR,\n        max_trial_count=config.MAX_TRIAL_COUNT,\n        max_failed_trial_count=config.MAX_FAILED_TRIAL_COUNT,\n        parallel_trial_count=config.PARALLEL_TRIAL_COUNT,\n        epochs=config.EPOCHS,\n        loss_goal=config.LOSS_GOAL,\n        min_learning_rate=config.MIN_LR,\n        max_learning_rate=config.MAX_LR,\n        min_momentum=config.MIN_MOMENTUM,\n        max_momentum=config.MAX_MOMENTUM,\n        pytorch_fashion_mnist_image=config.PYTORCH_FAHION_MNIST_IMAGE,\n    )\n\n    tensorboard_pipeline = create_tensorboard_launcher_op(\n        s3_path = 's3://tensorboard' + config.LOG_DIR,\n    ).apply(onprem.mount_pvc('minio-pvc-volume','minio-pv-volume','/workspace/minio')) \\\n    .add_env_variable(V1EnvVar(name='S3_ENDPOINT', value=config.S3_ENDPOINT)) \\\n    .add_env_variable(V1EnvVar(name='AWS_ENDPOINT_URL', value=config.AWS_ENDPOINT_URL)) \\\n    .add_env_variable(V1EnvVar(name='AWS_ACCESS_KEY_ID', value=config.AWS_ACCESS_KEY_ID)) \\\n    .add_env_variable(V1EnvVar(name='AWS_SECRET_ACCESS_KEY', value=config.AWS_SECRET_ACCESS_KEY)) \\\n    .add_env_variable(V1EnvVar(name='AWS_REGION', value=config.AWS_REGION)) \\\n    .add_env_variable(V1EnvVar(name='S3_USE_HTTPS', value='0')) \\\n    .add_env_variable(V1EnvVar(name='SE_VERIFY_SSL', value='0'))\n\n    extract_best_trial_name_op =components.func_to_container_op(extract_best_trial_name)\n    best_trial_name = extract_best_trial_name_op(\n        fashion_mnist_katib_experiment.output\n    ).after(fashion_mnist_katib_experiment)\n\n    model_path = create_model_path_launcher_op(\n        model_path = config.MINIO_ADDR + config.MINIO_PATH + str(best_trial_name.output) + '/'\n    ).after(best_trial_name)\n\nif __name__==\"__main__\":\n    # resolve kfp_server_api.exceptions.ApiException: (400) NAMESPACE is empty issue\n    os.system('mkdir -p ~/.config/kfp')\n    context = {\n        \"namespace\": \"admin\"\n    }\n\n    with open(\"context.json\", \"w\") as write_file:\n        json.dump(context, write_file)\n    os.system('mv context.json ~/.config/kfp/context.json')\n    \n    pipeline_func = create_pytorch_fashion_mnist_pipeline\n\n    session = requests.Session()\n    response = session.get(config.HOST)\n\n    headers = {\n        \"Content-Type\": \"application/x-www-form-urlencoded\",\n    }\n\n    data = {\"login\": config.USERNAME, \"password\": config.PASSWORD}\n    session.post(response.url, headers=headers, data=data)\n    print(session.cookies.get_dict())\n    session_cookie = session.cookies.get_dict()[\"authservice_session\"]\n\n    print(f'session_cookie: {session_cookie}')\n\n    from kubernetes import client as k8s_client\n    pipeline_conf = kfp.dsl.PipelineConf()\n    pipeline_conf.set_image_pull_secrets([k8s_client.V1ObjectReference(name=\"registry-credentials\")])\n\n    # Compile pipeline to generate compressed YAML definition of the pipeline.\n    kfp_client=kfp.Client(\n        host=f\"{config.HOST}/pipeline\",\n        cookies=f\"authservice_session={session_cookie}\",\n        namespace=config.NAMESPACE\n    )\n    run_id=kfp_client.create_run_from_pipeline_func(pipeline_func,\n                                                    arguments={},\n                                                    pipeline_conf=pipeline_conf,\n                                                    )\n "
  },
  {
    "repo": "anilkharde/FlexiKubeflowPipelineSolutions",
    "file_path": "pipeline/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/anilkharde/FlexiKubeflowPipelineSolutions/main/pipeline/pipeline.py",
    "content": "import kfp\nfrom kfp import dsl\nfrom kfp.compiler import Compiler\nimport kfp.components as components\nimport requests\nfrom kubernetes import client as k8s_client\n\nclass FlexiPipeline():\n    def __init__(self, config_path):\n        \"\"\"\n        Initialize the FlexiPipeline class with the configuration path.\n        \n        Args:\n            config_path (str): Path to the JSON configuration file.\n        \"\"\"\n        self.config_data = self.read_json(config_path)\n       \n    def read_json(self, path):\n        \"\"\"\n        Read and parse a JSON file from the given path.\n        \n        Args:\n            path (str): Path to the JSON file.\n        \n        Returns:\n            dict: Parsed JSON data.\n        \"\"\"\n        import json\n        with open(path, 'r') as file:\n            cf = json.load(file)\n        import pprint\n        pprint.pprint(cf)\n        return cf\n    \n    @staticmethod\n    def pre_process():\n        \"\"\"\n        Pre-processing function to be executed before the main processing.\n        \"\"\"\n        import time\n        print(\"Pre processing...\")\n        time.sleep(5)\n        print(\"Pre processing done...\")\n\n    @staticmethod\n    def custom_process(config_data: dict, image_id: str):\n        \"\"\"\n        Custom processing function to process each image.\n        \n        Args:\n            config_data (dict): Configuration data.\n            image_id (str): ID of the image to be processed.\n        \"\"\"\n        import os\n        import sys\n        import logging\n            \n        code_path = config_data['pipeline_config']['code_path']\n        logging.info(os.listdir(code_path))\n        sys.path.append(code_path)\n\n        # Import the custom processing module\n        from process import CustomProcess\n\n        try:\n            # Process implementation\n            print(\"code_path: \", code_path)\n            print(f\"Running Custom Process for image id {image_id}...\")\n            obj = CustomProcess(5)\n            param = config_data['process_input']['input_parameter']\n            print(param)\n        except Exception as e:\n            print(\"Exception\", e)\n\n    @staticmethod\n    def post_process():\n        \"\"\"\n        Post-processing function to be executed after the main processing.\n        \"\"\"\n        import time\n        print(\"Post processing...\")\n        time.sleep(5)\n        print(\"Post processing done...\")     \n    \n    def get_pre_process_fn(self):\n        \"\"\"\n        Get the pre-processing function.\n        \n        Returns:\n            function: Pre-processing function.\n        \"\"\"\n        return self.pre_process\n    \n    def get_custom_process_fn(self):\n        \"\"\"\n        Get the custom processing function.\n        \n        Returns:\n            function: Custom processing function.\n        \"\"\"\n        return self.custom_process\n    \n    def get_post_process_fn(self):\n        \"\"\"\n        Get the post-processing function.\n        \n        Returns:\n            function: Post-processing function.\n        \"\"\"\n        return self.post_process\n    \n    def pipeline(self, image_ids):\n        \"\"\"\n        Define the pipeline for processing images.\n        \n        Args:\n            image_ids (list): List of image IDs to be processed.\n        \n        Returns:\n            function: The pipeline function.\n        \"\"\"\n        \n        config_data = self.config_data\n        \n        pre_process_fn = self.get_pre_process_fn()\n        custom_process_fn = self.get_custom_process_fn()\n        post_process_fn = self.get_post_process_fn()\n\n        @dsl.pipeline(\"Process Name\", \"Process Description\")\n        def custom_process_pipeline(self):\n\n            packages_installation = config_data['pipeline_config']['packages_to_install']\n           \n            # Define the code PVC (Persistent Volume Claim) --------------------------------------------------------------------------------\n            code_mount = k8s_client.V1VolumeMount(\n                    name=config_data['pipeline_config']['pvc']['name'], \n                    mount_path= config_data['pipeline_config']['pvc']['mount_path'])\n            code_volume = k8s_client.V1Volume(\n                    name=config_data['pipeline_config']['pvc']['name'], \n                    persistent_volume_claim=k8s_client.V1PersistentVolumeClaimVolumeSource(\n                        claim_name=config_data['pipeline_config']['pvc']['name']))\n            \n            # Pre-process task\n            create_pre_process_op = components.create_component_from_func(\n                                                func=pre_process_fn,\n                                                base_image=\"python:3.9\"\n                                                )\n            create_pre_process_task = create_pre_process_op()\n            create_pre_process_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\" # Disable caching\n            create_pre_process_task.set_retry(1, policy=\"Always\", backoff_duration=\"2m\")\n\n            # Custom process tasks for each image ID\n            create_custom_process_op = components.create_component_from_func(\n                                                func=custom_process_fn,\n                                                base_image=\"python:3.9\",\n                                                packages_to_install=packages_installation\n                                                )\n            tasks = []\n            for image_id in image_ids:\n                task = create_custom_process_op(\n                    config_data=config_data, \n                    image_id=image_id\n                    )  \\\n                    .set_memory_request(config_data['pipeline_config']['custom_process']['memory_request']) \\\n                    .set_cpu_request(config_data['pipeline_config']['custom_process']['cpu_request']) \\\n                    .set_memory_limit(config_data['pipeline_config']['custom_process']['memory_limit']) \\\n                    .set_cpu_limit(config_data['pipeline_config']['custom_process']['cpu_limit'])\n                task.add_volume_mount(code_mount)\n                task.add_volume(code_volume)\n                task.execution_options.caching_strategy.max_cache_staleness = \"P0D\" # Disable caching\n                \n                task.set_display_name(f\"Process for image id {image_id}\")\n                task.set_retry(1, policy=\"Always\", backoff_duration=\"2m\")\n                task.after(create_pre_process_task)\n                tasks.append(task)\n\n            # Post-process task\n            create_post_process_op = components.create_component_from_func(\n                                                func=post_process_fn,\n                                                base_image=\"python:3.9\"\n                                                )\n            create_post_process_task = create_post_process_op()\n            create_post_process_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\" # Disable caching\n            create_post_process_task.set_retry(1, policy=\"Always\", backoff_duration=\"2m\")\n\n            # Ensure the post-process task runs after all custom process tasks\n            for task in tasks:\n                create_post_process_task.after(task)\n\n        return custom_process_pipeline\n    \n    def create_run(self, image_ids):\n        \"\"\"\n        Create and start a new run of the pipeline.\n        \n        Args:\n            image_ids (list): List of image IDs to be processed.\n        \n        Returns:\n            str: The run ID of the created pipeline run.\n        \"\"\"\n        import datetime\n        import requests\n\n        kubeflow_endpoint = self.config_data['pipeline_config']['kubeflow_endpoint']\n        username = self.config_data['pipeline_config']['username']\n        password = self.config_data['pipeline_config']['password']\n\n        # Authenticate with the Kubeflow endpoint\n        session = requests.Session()\n        response = session.get(kubeflow_endpoint)\n        headers = {\n            \"Content-Type\": \"application/x-www-form-urlencoded\",\n        }\n        data = {\"login\": username, \"password\": password}\n        session.post(response.url, headers=headers, data=data)\n        session_cookie = session.cookies.get_dict()[\"authservice_session\"]\n\n        # Create a client for the Kubeflow Pipelines\n        client = kfp.Client(host=f\"{kubeflow_endpoint}/pipeline\", cookies=f\"authservice_session={session_cookie}\")\n        print(\"Image_ids in create run:\", image_ids)\n\n        try: \n            # Generate a timestamp for the run name\n            timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n            # Create and start the pipeline run\n            run_id = client.create_run_from_pipeline_func(\n                                self.pipeline(image_ids), {},\n                                run_name=f\"custom_process_run_{timestamp}\",\n                                experiment_name=\"custom-process-exp\").run_id\n            print(\"Custom process run created successfully:\", run_id)\n            return run_id\n        except Exception as e:\n            print(\"Exception:\", e)"
  },
  {
    "repo": "ImranRiazChohan/ml_pipeline_using_kubeflow",
    "file_path": "pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/ImranRiazChohan/ml_pipeline_using_kubeflow/main/pipeline.py",
    "content": "import kfp\r\nfrom kfp import dsl\r\n\r\n\r\ndef preprocess_op():\r\n\r\n    return dsl.ContainerOp(\r\n        name='Preprocess Data',\r\n        image='usamachohan76/ml_pipeline_usingkubeflow:latest',\r\n        arguments=[],\r\n        file_outputs={\r\n            'x_train': '/app/x_train.npy',\r\n            'x_test': '/app/x_test.npy',\r\n            'y_train': '/app/y_train.npy',\r\n            'y_test': '/app/y_test.npy',\r\n        }\r\n    )\r\ndef train_op(x_train, y_train):\r\n\r\n    return dsl.ContainerOp(\r\n        name='Train Model',\r\n        image='usamachohan76/ml_pipeline_usingkubeflow:latest',\r\n        arguments=[\r\n            '--x_train', x_train,\r\n            '--y_train', y_train\r\n        ],\r\n        file_outputs={\r\n            'model': '/app/model.pkl'\r\n        }\r\n    )\r\n\r\n\r\ndef test_op(x_test, y_test, model):\r\n    return dsl.ContainerOp(\r\n        name='Test Model',\r\n        image='usamachohan76/ml_pipeline_usingkubeflow:latest',\r\n        arguments=[\r\n            '--x_test', x_test,\r\n            '--y_test', y_test,\r\n            '--model', model\r\n        ],\r\n        file_outputs={\r\n            'mean_squared_error': '/app/output.txt'\r\n        }\r\n    )\r\n\r\n\r\ndef deploy_model_op(model):\r\n    return dsl.ContainerOp(\r\n        name='Deploy Model',\r\n        image='usamachohan76/ml_pipeline_usingkubeflow:latest',\r\n        arguments=[\r\n            '--model', model\r\n        ]\r\n    )\r\n\r\n\r\n@dsl.pipeline(\r\n    name='Boston Housing Pipeline',\r\n    description='An example pipeline that trains and logs a regression model.'\r\n)\r\ndef boston_pipeline():\r\n    _preprocess_op = preprocess_op()\r\n\r\n    _train_op = train_op(\r\n        dsl.InputArgumentPath(_preprocess_op.outputs['x_train']),\r\n        dsl.InputArgumentPath(_preprocess_op.outputs['y_train'])\r\n    ).after(_preprocess_op)\r\n\r\n    _test_op = test_op(\r\n        dsl.InputArgumentPath(_preprocess_op.outputs['x_test']),\r\n        dsl.InputArgumentPath(_preprocess_op.outputs['y_test']),\r\n        dsl.InputArgumentPath(_train_op.outputs['model'])\r\n    ).after(_train_op)\r\n\r\n    deploy_model_op(\r\n        dsl.InputArgumentPath(_train_op.outputs['model'])\r\n    ).after(_test_op)\r\n\r\nclient = kfp.Client()\r\nclient.create_run_from_pipeline_func(boston_pipeline, arguments={})\r\n"
  },
  {
    "repo": "vyomagg/poc_kubeflow_regression_pipeline",
    "file_path": "pipeline/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/vyomagg/poc_kubeflow_regression_pipeline/develop/pipeline/pipeline.py",
    "content": "import kfp\nfrom kfp import dsl\nimport yaml\n\n\ndef extract_data_op(train_samples, test_samples):\n    return dsl.ContainerOp(\n        name='Extract Data',\n        image='vyomagg/regression_pipeline_extract_data:latest',\n        arguments=[\n            '--train_samples', train_samples,\n            '--test_samples', test_samples\n        ],\n        file_outputs={\n            'out_train': '/app/data/raw/train.pkl',\n            'out_test': '/app/data/raw/test.pkl'\n        }\n    )\n\n\ndef prepare_op(input_train, input_test, co_relation_threshold):\n    return dsl.ContainerOp(\n        name='Prepare Data',\n        image='vyomagg/regression_pipeline_prepare:latest',\n        arguments=[\n            '--input_train', input_train,\n            '--input_test', input_test,\n            '--co_relation_threshold', co_relation_threshold\n        ],\n        file_outputs={\n            'out_train': '/app/data/prepared/train.pkl',\n            'out_test': '/app/data/prepared/test.pkl'\n        }\n    )\n\n\ndef train_op(input_train, fit_intercept , normalize, n_jobs, copy_X):\n    return dsl.ContainerOp(\n        name='Train Model',\n        image='vyomagg/regression_pipeline_train:latest',\n        arguments=[\n            '--input_train', input_train,\n            '--fit_intercept', fit_intercept,\n            '--normalize',normalize,\n            '--n_jobs',n_jobs,\n            '--copy_X', copy_X\n        ],\n        file_outputs={\n            'train_model': '/app/model/Regression_checkpoints/best.pkl'\n        }\n    )\n\n\ndef evaluate_op(data_path, model_ckpt_dir, metrics):\n    return dsl.ContainerOp(\n        name='Test Model',\n        image='vyomagg/regression_pipeline_evaluate:latest',\n        arguments=[\n            '--data_path', data_path,\n            '--model_ckpt_dir', model_ckpt_dir,\n            '--metrics', metrics\n        ],\n        file_outputs={\n            'mean_squared_error': '/app/results/train_stats_mse.json',\n            'root_mean_squared_error': '/app/results/train_stats_rmse.json',\n            'mean_absolute_error': '/app/results/train_stats_mae.json',\n            'r_square_error': 'app/results/train_stats_rsquare.json'\n        }\n    )\n\n\ndef deploy_model_op(model):\n    return dsl.ContainerOp(\n        name='Deploy Model',\n        image='vyomagg/regression_pipeline_deploy_model:latest',\n        arguments=[\n            '--model', model\n        ]\n    )\n\n\n@dsl.pipeline(\n    name='Regression Kubeflow Pipeline',\n    description='An example pipeline that trains and logs a regression model.'\n)\ndef regression_pipeline(train_samples, test_samples, co_relation_threshold, fit_intercept,\n                        normalize, n_jobs, copy_X, metrics ) :\n\n    _extract_op = extract_data_op(train_samples, test_samples)\n\n    _prepare_op = prepare_op(\n        dsl.InputArgumentPath(_extract_op.outputs['out_train']),\n        dsl.InputArgumentPath(_extract_op.outputs['out_test']),\n        co_relation_threshold\n    ).after(_extract_op)\n\n    _train_op = train_op(\n        dsl.InputArgumentPath(_prepare_op.outputs['out_train']),\n        fit_intercept, normalize, n_jobs, copy_X\n    ).after(_prepare_op)\n\n    _evaluate_op = evaluate_op(\n        dsl.InputArgumentPath(_prepare_op.outputs['out_test']),\n        dsl.InputArgumentPath(_train_op.outputs['train_model']),\n        metrics\n    ).after(_train_op)\n\n    deploy_model_op(\n        dsl.InputArgumentPath(_train_op.outputs['train_model'])\n    ).after(_evaluate_op)\n\n\n#kfp.compiler.Compiler().compile(regression_pipeline, 'regression_pipeline.zip')\n\n## Global Parameters\nparams = yaml.safe_load(open('../params.yaml'))\nextract_params = params['extract']\nprepare_params = params['prepare']\ntrain_params = params['train']\nevaluate_params = params['evaluate']\n\n\narguments = { 'train_samples' : extract_params['train_samples'], 'test_samples' : extract_params['test_samples'] ,\n            'co_relation_threshold' : prepare_params['co_relation_threshold'] , 'fit_intercept' : train_params['fit_intercept'],\n            'normalize' : train_params['normalize'], 'n_jobs' : train_params['n_jobs'] , 'copy_X' : train_params['copy_X'],\n            'metrics' : evaluate_params['metrics']}\n\n\n## Auto Execution of pipeline\nclient = kfp.Client(host='http://127.0.0.1:8081', namespace='kubeflow')\nclient.create_run_from_pipeline_func(regression_pipeline, arguments= arguments)\n"
  },
  {
    "repo": "AlbughdadiM/kubeflow-pipeline-crop-classification",
    "file_path": "pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/AlbughdadiM/kubeflow-pipeline-crop-classification/master/pipeline.py",
    "content": "import kfp\nfrom kfp import dsl\nfrom kfp.components import  InputPath\n\n\n\n\n\n@dsl.pipeline(name='advanced-crop-classification-pipeline', description='Classify crops extracted from RPG.')\ndef crop_classification_pipeline(json_img: str,shp:str,cross_validation: bool = False,iterations: int = 2,cv: int = 2):\n\n    def compare_models(xgboost_csv : InputPath(str), lstm_csv : InputPath(str)) -> str:\n        import pandas as pd\n        xgb_df = pd.read_csv(xgboost_csv)\n        xgb_acc = xgb_df['precision'][2]\n        \n        lstm_df = pd.read_csv(lstm_csv)\n        lstm_acc = lstm_df['precision'][2]\n        \n        if xgb_acc>=lstm_acc:\n            print (\"XGBoost model will be used for serving\")\n            return \"XGB\"\n        else:\n            print (\"LSTM will be used for serving\")\n            return \"LSTM\"\n\n    # create components from yaml manifest \n    download_img = kfp.components.load_component_from_file('process_img/process_img.yaml')\n    temporal_stats = kfp.components.load_component_from_file('temporal_stats/temporal_stats.yaml')\n    preprocess = kfp.components.load_component_from_file('preprocess_data/preprocess_data.yaml')\n    xgboost_classif = kfp.components.load_component_from_file('extreme_gradient_boost/extreme_gradient_boost.yaml')\n    lstm_classif = kfp.components.load_component_from_file('lstm/lstm.yaml')\n    compare = kfp.components.create_component_from_func(\n                        func=compare_models,\n                        base_image='python:3.7', \n                        #output_component_file='compare_models.yaml', \n                        packages_to_install=['pandas==0.24'],\n                    )\n\n    # Run first task\n    download_task = download_img(json_img,shp)\n    #download_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n\n    # create temporal stats from results of the previous task\n    temporal_task = temporal_stats(download_task.output)\n    #temporal_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n\n    # preprocess data \n    preprocess_task = preprocess(temporal_task.output)\n    #preprocess_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n\n    # classification with XGBoost\n    xgboost_task = xgboost_classif(preprocess_task.output,cross_validation,iterations,cv)\n    #xgboost_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n\n    # classification with LSTM\n    lstm_task = lstm_classif(preprocess_task.output,cross_validation,iterations,cv)\n    #lstm_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n\n    # compare models\n    compare_task = compare(xgboost_task.outputs['Report'],lstm_task.outputs['Report'])\n    compare_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n   \n\n\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(crop_classification_pipeline, 'advanced-crop-classification-pipeline.yaml')\n"
  },
  {
    "repo": "biswaroop1547/skit-pipelines",
    "file_path": "skit_pipelines/utils/k8s.py",
    "raw_url": "https://raw.githubusercontent.com/biswaroop1547/skit-pipelines/main/skit_pipelines/utils/k8s.py",
    "content": "from kfp.dsl import PipelineConf\n\nimport skit_pipelines.constants as const\nfrom skit_pipelines.api import models\n\n\ndef get_pipeline_config_kfp(pipeline_name):\n    ## since currently gpu node doesn't support all integrations with db,\n    ## we set default pipeline nodeselector to be CPU node\n    ## while setting gpu node only for the required component in pipeline.\n\n    # node_type = models.PodNodeSelectorMap[pipeline_name]\n    # if node_type == const.CPU_NODE_LABEL:\n    return PipelineConf().set_default_pod_node_selector(\n        label_name=const.POD_NODE_SELECTOR_LABEL,\n        value=const.CPU_NODE_LABEL,  # node_type\n    )\n"
  },
  {
    "repo": "biswaroop1547/skit-pipelines",
    "file_path": "skit_pipelines/pipelines/asr_tune/__init__.py",
    "raw_url": "https://raw.githubusercontent.com/biswaroop1547/skit-pipelines/main/skit_pipelines/pipelines/asr_tune/__init__.py",
    "content": "import os\n\nimport kfp\n\nfrom skit_pipelines import constants as pipeline_constants\nfrom skit_pipelines.components import (\n    asr_tune_op,\n    create_true_transcript_labels_op,\n    download_directory_from_s3_op,\n    download_file_from_s3_op,\n    extract_true_transcript_labels_to_txt_op,\n    fetch_tagged_dataset_op,\n    process_true_transcript_labels_op,\n    slack_notification_op,\n    upload2s3_op,\n)\n\nBUCKET = pipeline_constants.BUCKET\n\n\n@kfp.dsl.pipeline(\n    name=\"ASR Language Model Tune Pipeline\",\n    description=\"Tunes LM on provided corpus using the val_corpus for validation.\",\n)\ndef asr_tune(\n    *,\n    lang: str,\n    base_model_path: str,\n    general_lm_path: str,\n    target_model_path: str,\n    corpus_path: str = \"\",\n    val_corpus_path: str = \"\",\n    corpus_tog_job_ids: str = \"\",\n    val_corpus_tog_job_ids: str = \"\",\n    augment_wordlist_path: str = \"\",\n    remove_wordlist_path: str = \"\",\n    storage_options: str = '{\"type\": \"s3\",\"bucket\": \"vernacular-asr-models\"}',\n    notify: str = \"\",\n    channel: str = \"\",\n    slack_thread: str = \"\",\n):\n    \"\"\"\n    TODO: Docstring.\n    \"\"\"\n    augment_wordlist_op = download_file_from_s3_op(\n        storage_path=augment_wordlist_path, empty_possible=True\n    )\n    remove_wordlist_op = download_file_from_s3_op(\n        storage_path=remove_wordlist_path, empty_possible=True\n    )\n\n    # create a component that can make sure:\n    # 1. target_model_path does not already exist.\n    # 2. target_model_path is a valid s3 path.\n    # TODO: check_s3_path_does_not_exist_op(target_model_path)\n\n    base_model_op = download_directory_from_s3_op(storage_path=base_model_path)\n    general_lm_op = download_file_from_s3_op(storage_path=general_lm_path)\n\n    with kfp.dsl.Condition(corpus_path == \"\", \"corpus_path\"):\n        corpus_op = fetch_tagged_dataset_op(\n            job_id=corpus_tog_job_ids,\n        )\n        val_corpus_op = fetch_tagged_dataset_op(\n            job_id=val_corpus_tog_job_ids,\n        )\n        true_label_column = \"transctipt_y\"\n        corpus_op = create_true_transcript_labels_op(\n            corpus_op.outputs[\"output\"], true_label_column\n        )\n        corpus_op = process_true_transcript_labels_op(\n            corpus_op.outputs[\"output\"],\n            true_label_column,\n        )\n        corpus_op = extract_true_transcript_labels_to_txt_op(\n            corpus_op.outputs[\"output\"], true_label_column\n        )\n        val_corpus_op = create_true_transcript_labels_op(\n            val_corpus_op.outputs[\"output\"], true_label_column\n        )\n        val_corpus_op = process_true_transcript_labels_op(\n            val_corpus_op.outputs[\"output\"],\n            true_label_column,\n        )\n        val_corpus_op = extract_true_transcript_labels_to_txt_op(\n            val_corpus_op.outputs[\"output\"], true_label_column\n        )\n        tune_op = asr_tune_op(\n            corpus_op.outputs[\"output\"],\n            val_corpus_op.outputs[\"output\"],\n            augment_wordlist_op.outputs[\"output\"],\n            remove_wordlist_op.outputs[\"output\"],\n            base_model_op.outputs[\"output\"],\n            general_lm_op.outputs[\"output\"],\n            lang=lang,\n        ).set_ephemeral_storage_limit(\"20G\")\n        tune_op.execution_options.caching_strategy.max_cache_staleness = (\n            \"P0D\"  # disables caching\n        )\n        upload = upload2s3_op(\n            path_on_disk=tune_op.outputs[\"output\"],\n            output_path=target_model_path,\n            storage_options=storage_options,\n            ext=\"\",\n            upload_as_directory=True,\n        ).after(tune_op)\n        upload.execution_options.caching_strategy.max_cache_staleness = (\n            \"P0D\"  # disables caching\n        )\n    with kfp.dsl.Condition(corpus_tog_job_ids == \"\", \"corpus_tog_job_ids\"):\n        corpus_op_2 = download_file_from_s3_op(storage_path=corpus_path)\n        val_corpus_op_2 = download_file_from_s3_op(storage_path=val_corpus_path)\n        tune_op_2 = asr_tune_op(\n            corpus_op_2.outputs[\"output\"],\n            val_corpus_op_2.outputs[\"output\"],\n            augment_wordlist_op.outputs[\"output\"],\n            remove_wordlist_op.outputs[\"output\"],\n            base_model_op.outputs[\"output\"],\n            general_lm_op.outputs[\"output\"],\n            lang=lang,\n        ).set_ephemeral_storage_limit(\"20G\")\n        tune_op_2.execution_options.caching_strategy.max_cache_staleness = (\n            \"P0D\"  # disables caching\n        )\n        upload_2 = upload2s3_op(\n            path_on_disk=tune_op_2.outputs[\"output\"],\n            output_path=target_model_path,\n            storage_options=storage_options,\n            ext=\"\",\n            upload_as_directory=True,\n        ).after(tune_op_2)\n        upload_2.execution_options.caching_strategy.max_cache_staleness = (\n            \"P0D\"  # disables caching\n        )\n\n    with kfp.dsl.Condition(notify != \"\", \"notify\").after(\n        upload, upload_2\n    ) as upload_check:\n        notification_text = f\"The ASR Tuning pipeline is completed.\"\n        tune_notif = slack_notification_op(\n            notification_text, channel=channel, cc=notify, thread_id=slack_thread\n        )\n        tune_notif.execution_options.caching_strategy.max_cache_staleness = (\n            \"P0D\"  # disables caching\n        )\n\n\n__all__ = [\"asr_tune\"]\n"
  },
  {
    "repo": "biswaroop1547/skit-pipelines",
    "file_path": "skit_pipelines/pipelines/eval_asr_pipeline/__init__.py",
    "raw_url": "https://raw.githubusercontent.com/biswaroop1547/skit-pipelines/main/skit_pipelines/pipelines/eval_asr_pipeline/__init__.py",
    "content": "import kfp\n\nfrom skit_pipelines import constants as pipeline_constants\nfrom skit_pipelines.components import (\n    create_true_transcript_labels_op,\n    create_utterances_op,\n    download_csv_from_s3_op,\n    gen_asr_metrics_op,\n    process_true_transcript_labels_op,\n    slack_notification_op,\n    upload2s3_op,\n)\n\nUTTERANCES = pipeline_constants.UTTERANCES\nTRANSCRIPT_Y = pipeline_constants.TRANSCRIPT_Y\nBUCKET = pipeline_constants.BUCKET\nINTENT = pipeline_constants.INTENT\n\n\n@kfp.dsl.pipeline(\n    name=\"ASR Transcription vs Transcription tags Eval Pipeline\",\n    description=\"Produces asr metrics for the transcriptions present against transcription tags.\",\n)\ndef eval_asr_pipeline(\n    *,\n    s3_path_data: str,\n    org_id: str,\n    notify: str = \"\",\n    channel: str = \"\",\n    true_label_column: str = \"transcript_y\",\n    pred_label_column: str = \"utterances\",\n    slack_thread: str = \"\",\n):\n    \"\"\"\n    Evaluates ASR transcriptions using transcription tags.\n\n    .. _p_eval_asr_pipeline:\n\n    Example payload to invoke this pipeline via slack integrations:\n\n        @charon run eval_asr_pipeline\n\n        .. code-block:: python\n\n            {\n                \"s3_path_data\": \"s3://bucket-name/data/\",\n                \"org_id\": \"org\"\n            }\n\n    :param s3_path_data: S3 path to a tagged dataset (.csv).\n    :type s3_path_data: str\n    :param org_id: reference path to save the metrics.\n    :type org_id: str\n    :param channel: The slack channel to send the notification, defaults to \"\"\n    :type channel: str, optional\n    :param slack_thread: The slack thread to send the notification, defaults to \"\"\n    :type slack_thread: str, optional\n    \"\"\"\n    tagged_data_op = download_csv_from_s3_op(storage_path=s3_path_data)\n\n    # Create true label column\n    preprocess_data_op = create_utterances_op(tagged_data_op.outputs[\"output\"]).after(\n        tagged_data_op\n    )\n\n    # Create utterance column\n    preprocess_step_2_data_op = create_true_transcript_labels_op(\n        preprocess_data_op.outputs[\"output\"], true_label_column\n    ).after(preprocess_data_op)\n\n    preprocess_step_3_data_op = process_true_transcript_labels_op(\n        preprocess_step_2_data_op.outputs[\"output\"],\n        true_label_column,\n    ).after(preprocess_step_2_data_op)\n\n    asr_metrics_op = gen_asr_metrics_op(\n        preprocess_step_3_data_op.outputs[\"output\"],\n        true_label_column=true_label_column,\n        pred_label_column=pred_label_column,\n    )\n\n    # produce test set metrics.\n    upload_metrics = upload2s3_op(\n        path_on_disk=asr_metrics_op.outputs[\"output\"],\n        reference=org_id,\n        file_type=\"asr-metrics\",\n        bucket=BUCKET,\n        ext=\"\",\n        upload_as_directory=True,\n    )\n    upload_metrics.execution_options.caching_strategy.max_cache_staleness = (\n        \"P0D\"  # disables caching\n    )\n\n    with kfp.dsl.Condition(notify != \"\", \"notify\").after(upload_metrics) as asr_check:\n        notification_text = f\"Here are the ASR eval results.\"\n        code_block = f\"aws s3 cp {upload_metrics.output} .\"\n        asr_notif = slack_notification_op(\n            notification_text,\n            channel=channel,\n            cc=notify,\n            code_block=code_block,\n            thread_id=slack_thread,\n        )\n        asr_notif.execution_options.caching_strategy.max_cache_staleness = (\n            \"P0D\"  # disables caching\n        )\n\n\n__all__ = [\"eval_asr_pipeline\"]\n"
  },
  {
    "repo": "biswaroop1547/skit-pipelines",
    "file_path": "skit_pipelines/pipelines/evaluate_slu/__init__.py",
    "raw_url": "https://raw.githubusercontent.com/biswaroop1547/skit-pipelines/main/skit_pipelines/pipelines/evaluate_slu/__init__.py",
    "content": "import kfp\n\nfrom skit_pipelines import constants as pipeline_constants\nfrom skit_pipelines.components import (\n    download_csv_from_s3_op,\n    download_yaml_op,\n    fetch_tagged_dataset_op,\n    evalution_slu_from_repo_op,\n    slack_notification_op,\n    upload2s3_op,\n)\n\nUTTERANCES = pipeline_constants.UTTERANCES\nINTENT_Y = pipeline_constants.INTENT_Y\nBUCKET = pipeline_constants.BUCKET\nCSV_FILE = pipeline_constants.CSV_FILE\nCPU_NODE_LABEL = pipeline_constants.CPU_NODE_LABEL\nGPU_NODE_LABEL = pipeline_constants.GPU_NODE_LABEL\nNODESELECTOR_LABEL = pipeline_constants.POD_NODE_SELECTOR_LABEL\n\n\n@kfp.dsl.pipeline(\n    name=\"SLU evaluating Pipeline\",\n    description=\"Evaluate an existing SLU model.\",\n)\n\ndef evaluate_slu(\n    *,\n    repo_name: str,\n    repo_branch: str = \"master\",\n    compare_branch: str = \"master\",\n    job_ids: str = \"\",\n    test_dataset_path: str = \"\",\n    labelstudio_project_ids: str = \"\",\n    job_start_date: str = \"\",\n    job_end_date: str = \"\",\n    remove_intents: str = \"\",\n    alias_yaml_path: str = \"\",\n    notify: str = \"\",\n    channel: str = \"\",\n    slack_thread: str = \"\",\n    core_slu_repo_name: str = \"core-slu-service\",\n    core_slu_repo_branch: str = \"master\",\n    customization_repo_name: str = \"customization\",\n    customization_repo_branch: str = \"master\",\n):\n    \"\"\"\n    A pipeline to evaluate an existing SLU model.\n\n    .. _p_evaluate_slu:\n\n    Example payload to invoke via slack integrations:\n\n    A minimal example:\n\n        @charon run evaluate_slu\n\n        .. code-block:: python\n\n            {\n                \"repo_name\": \"slu_repo_name\",\n                \"labelstudio_project_ids\": \"10,13\",\n                \"test_dataset_path\":\"s3://bucket/data.csv\"\n            }\n\n\n    A full available parameters example:\n\n        @charon run evaluate_slu\n\n        .. code-block:: python\n\n            {\n                \"repo_name\": \"slu_repo_name\",\n                \"repo_branch\": \"master\",\n                \"test_dataset_path\": \"s3://bucket-name/path1/to1/data1.csv,s3://bucket-name/path2/to2/data2.csv\",\n                \"job_ids\": \"4011,4012\",\n                \"labelstudio_project_ids\": \"10,13\",\n                \"job_start_date\": \"2022-08-01\",\n                \"job_end_date\": \"2022-09-19\",\n                \"remove_intents\": \"_confirm_,_oos_,audio_speech_unclear,ood\",\n                \"alias_yaml_path\": \"intents/oppo/alias.yaml\"\n            }\n\n\n    :param repo_name: SLU repository name under /vernacularai/ai/clients org in gitlab.\n    :type repo_name: str\n\n    :param repo_branch: The branch name in the SLU repository one wants to use, defaults to master.\n    :type repo_name: str, optional\n\n    :param test_dataset_path: The S3 URI or the S3 key for the tagged dataset (can be multiple - comma separated).\n    :type dataset_path: str, optional\n\n    :param job_ids: The job ids as per tog. Optional if labestudio_project_ids is provided.\n    :type job_ids: str\n\n    :param labelstudio_project_ids: The labelstudio project id (this is a number) since this is optional, defaults to \"\".\n    :type labelstudio_project_ids: str\n\n    :param job_start_date: The start date range (YYYY-MM-DD) to filter tagged data.\n    :type job_start_date: str, optional\n\n    :param job_end_date: The end date range (YYYY-MM-DD) to filter tagged data\n    :type job_end_date: str, optional\n\n    :param remove_intents: Comma separated list of intents to remove from dataset while training.\n    :type remove_intents: str, optional\n\n    :param alias_yaml_path: eevee's intent_report alias.yaml, refer docs `here <https://skit-ai.github.io/eevee/metrics/intents.html#aliasing>`_ . Upload your yaml to eevee-yamls repository `here <https://github.com/skit-ai/eevee-yamls>`_ & pass the relative path of the yaml from base of the repository.\n    :type alias_yaml_path: str, optional\n\n    :param core_slu_repo_name: Name of repository for core slu service. Defaults to core-slu-service\n    :type core_slu_repo_name: str, optional\n\n    :param core_slu_repo_branch: Branch to check out for core slu repository. Defaults to master\n    :type core_slu_repo_branch: str, optional\n\n    :param customization_repo_name: Name of repository for customization service. Defaults to customization\n    :type customization_repo_name: str, optional\n\n    :param customization_repo_branch: Branch to check out for customization service repository. Defaults to master\n    :type customization_repo_branch: str, optional\n\n    :param notify: Whether to send a slack notification, defaults to \"\"\n    :type notify: str, optional\n\n    :param channel: The slack channel to send the notification, defaults to \"\"\n    :type channel: str, optional\n\n    :param slack_thread: The slack thread to send the notification, defaults to \"\"\n    :type slack_thread: str, optional\n\n    \"\"\"\n\n    tagged_s3_data_op = download_csv_from_s3_op(storage_path=test_dataset_path)\n    tagged_s3_data_op.execution_options.caching_strategy.max_cache_staleness = (\n        \"P0D\"  # disables caching\n    )\n\n    tagged_job_data_op = fetch_tagged_dataset_op(\n        job_id=job_ids,\n        project_id=labelstudio_project_ids,\n        task_type=\"conversation\",\n        timezone=\"Asia/Kolkata\",\n        start_date=job_start_date,\n        end_date=job_end_date,\n        empty_possible=True,\n    )\n    tagged_job_data_op.execution_options.caching_strategy.max_cache_staleness = (\n        \"P0D\"  # disables caching\n    )\n\n    downloaded_alias_yaml_op = download_yaml_op(\n        git_host_name=pipeline_constants.GITLAB,\n        yaml_path=alias_yaml_path,\n    )\n\n    validate_evaluation_setup_op = evalution_slu_from_repo_op(\n        tagged_s3_data_op.outputs[\"output\"],\n        tagged_job_data_op.outputs[\"output\"],\n        downloaded_alias_yaml_op.outputs[\"output\"],\n        bucket=BUCKET,\n        repo_name=repo_name,\n        compare_branch=compare_branch,\n        branch=repo_branch,\n        remove_intents=remove_intents,\n        validate_setup=True,\n        customization_repo_name=customization_repo_name,\n        customization_repo_branch=customization_repo_branch,\n        core_slu_repo_name=core_slu_repo_name,\n        core_slu_repo_branch=core_slu_repo_branch,\n    ).set_ephemeral_storage_limit(\"20G\")\n    validate_evaluation_setup_op.display_name = \"Validate Evaluation Setup\"\n\n    evaluate_op = evalution_slu_from_repo_op(\n        tagged_s3_data_op.outputs[\"output\"],\n        tagged_job_data_op.outputs[\"output\"],\n        downloaded_alias_yaml_op.outputs[\"output\"],\n        bucket=BUCKET,\n        repo_name=repo_name,\n        compare_branch=compare_branch,\n        branch=repo_branch,\n        remove_intents=remove_intents,\n        customization_repo_name=customization_repo_name,\n        customization_repo_branch=customization_repo_branch,\n        core_slu_repo_name=core_slu_repo_name,\n        core_slu_repo_branch=core_slu_repo_branch,\n    ).after(validate_evaluation_setup_op)\n    evaluate_op.set_gpu_limit(1).add_node_selector_constraint(\n        label_name=NODESELECTOR_LABEL, value=GPU_NODE_LABEL\n    )\n\n    comparison_upload_cf = upload2s3_op(\n        path_on_disk=evaluate_op.outputs[\"comparison_classification_report\"],\n        reference=repo_name,\n        file_type=\"comparison_classification_report\",\n        bucket=BUCKET,\n        ext=CSV_FILE,\n    )\n    comparison_upload_cf.execution_options.caching_strategy.max_cache_staleness = (\n        \"P0D\"  # disables caching\n    )\n\n    comparison_upload_cm = upload2s3_op(\n        path_on_disk=evaluate_op.outputs[\"comparison_confusion_matrix\"],\n        reference=repo_name,\n        file_type=\"comparison_confusion_matrix\",\n        bucket=BUCKET,\n        ext=CSV_FILE,\n    )\n\n    with kfp.dsl.Condition(notify != \"\", name=\"slack_notify\").after(\n        comparison_upload_cf, comparison_upload_cm\n    ):\n        notification_text = f\"Here's the IRR report.\"\n        code_block = f\"aws s3 cp {comparison_upload_cf.output} .\"\n        irr_notif = slack_notification_op(\n            notification_text,\n            channel=channel,\n            cc=notify,\n            code_block=code_block,\n            thread_id=slack_thread,\n        )\n        irr_notif.execution_options.caching_strategy.max_cache_staleness = (\n            \"P0D\"  # disables caching\n        )\n\n        notification_text = f\"Here's the confusion matrix.\"\n        code_block = f\"aws s3 cp {comparison_upload_cm.output} .\"\n        cm_notif = slack_notification_op(\n            notification_text,\n            channel=channel,\n            cc=notify,\n            code_block=code_block,\n            thread_id=slack_thread,\n        )\n        cm_notif.execution_options.caching_strategy.max_cache_staleness = (\n            \"P0D\"  # disables caching\n        )\n\n\n__all__ = [\"evaluate_slu\"]\n"
  },
  {
    "repo": "biswaroop1547/skit-pipelines",
    "file_path": "skit_pipelines/pipelines/fetch_calls_pipeline/__init__.py",
    "raw_url": "https://raw.githubusercontent.com/biswaroop1547/skit-pipelines/main/skit_pipelines/pipelines/fetch_calls_pipeline/__init__.py",
    "content": "import kfp\n\nfrom skit_pipelines import constants as pipeline_constants\nfrom skit_pipelines.components import fetch_calls_op, slack_notification_op\n\nUSE_FSM_URL = pipeline_constants.USE_FSM_URL\nREMOVE_EMPTY_AUDIOS = False if USE_FSM_URL else True\n\n\n@kfp.dsl.pipeline(\n    name=\"Fetch Calls Pipeline\",\n    description=\"fetches calls from production db with respective arguments\",\n)\ndef fetch_calls_pipeline(\n    lang: str,\n    start_date: str = \"\",\n    end_date: str = \"\",\n    client_id: str = \"\",\n    ignore_callers: str = \"\",\n    reported: bool = False,\n    template_id: str = \"\",\n    use_case: str = \"\",\n    flow_name: str = \"\",\n    min_duration: str = \"\",\n    asr_provider: str = \"\",\n    states: str = \"\",\n    intents: str = \"\",\n    call_quantity: int = 200,\n    call_type: str = \"\",\n    remove_empty_audios: bool = REMOVE_EMPTY_AUDIOS,\n    notify: str = \"\",\n    channel: str = \"\",\n    slack_thread: str = \"\",\n    use_fsm_url: bool = False,\n    flow_ids: str = \"\",\n):\n    \"\"\"\n    A pipeline to randomly sample calls for a given voice-bot project.\n\n    .. _p_fetch_calls_pipeline:\n\n\n    Example payload to invoke this pipeline via slack integrations:\n\n        @charon run fetch_calls_pipeline\n\n        .. code-block:: python\n\n            {\n                \"client_id\": 1,\n                \"start_date\": \"2020-01-01\",\n                \"lang\": \"en\",\n                \"end_date\": \"2020-01-01\",\n                \"reported\": false,\n                \"call_quantity\": 200\n            }\n\n    :param client_id: The comma separated client ids as per fsm db.\n    :type client_id: str, optional\n    :param start_date: The start date range to filter calls in YYYY-MM-DD format.\n    :type start_date: str\n    :param lang: The language code of the calls to filter. eg: en, hi, ta, te, etc.\n    :type lang: str\n    :param end_date: The end date range to filter calls in YYYY-MM-DD format.\n    :type end_date: str\n    :param ignore_callers: Comma separated list of callers to ignore, defaults to \"\"\n    :type ignore_callers: str, optional\n    :param reported: Pick only reported calls, defaults to False\n    :type reported: bool\n    :param template_id: The flow template id to filter calls, defaults to \"\"\n    :type template_id: str, optional\n    :param use_case: Voice bot project's use-case, defaults to \"\"\n    :type use_case: str, optional\n    :param flow_name: Identifier for a whole/part of a voicebot conversation flow, defaults to \"\"\n    :type flow_name: str, optional\n    :param min_duration: Call duration filter, defaults to \"\"\n    :type min_duration: str, optional\n    :param asr_provider: The ASR vendor (google/VASR), defaults to \"\"\n    :type asr_provider: str, optional\n    :param states: Filter calls in a comma separated list of states, defaults to \"\"\n    :type states: str, optional\n    :param intents: Filter turns in sampled calls from a comma separated list of intents, defaults to \"\"\n    :type intents: str, optional\n    :param call_quantity: Number of calls to sample, defaults to 200\n    :type call_quantity: int, optional\n    :param call_type: inbound, outbound vs subtesting call filters. We can currently choose only one of these, or defaults to \"INBOUND\" and \"OUTBOUND\" both\n    :type call_type: str, optional\n    :param remove_empty_audios: to remove calls with call audios being empty/broken, defaults to True\n    :type remove_empty_audios: bool\n    :param notify: A comma separated list of slack ids: \"@apples, @orange.fruit\" etc, defaults to \"\"\n    :type notify: str, optional\n    :param channel: The slack channel to send the notification, defaults to \"\"\n    :type channel: str, optional\n    :param slack_thread: The slack thread to send the notification, defaults to \"\"\n    :type slack_thread: str, optional\n    :param use_fsm_url: Whether to use turn audio url from fsm or s3 path., defaults to False\n    :type use_fsm_url: bool, optional\n    :param flow_id: Id for a whole/part of a voicebot conversation flow, defaults to \"\"\n    :type flow_id: str, optional\n    \"\"\"\n    \n    calls = fetch_calls_op(\n        client_id=client_id,\n        start_date=start_date,\n        end_date=end_date,\n        lang=lang,\n        call_quantity=call_quantity,\n        call_type=call_type,\n        ignore_callers=ignore_callers,\n        reported=reported,\n        template_id=template_id,\n        use_case=use_case,\n        flow_name=flow_name,\n        min_duration=min_duration,\n        asr_provider=asr_provider,\n        intents=intents,\n        states=states,\n        remove_empty_audios=remove_empty_audios,\n        use_fsm_url=USE_FSM_URL or use_fsm_url,\n        flow_ids=flow_ids\n    )\n    calls.execution_options.caching_strategy.max_cache_staleness = (\n        \"P0D\"  # disables caching\n    )\n\n    with kfp.dsl.Condition(notify != \"\", \"notify\").after(calls) as check1:\n        notification_text = f\"Finished a request for {call_quantity} calls. Fetched from {start_date} to {end_date} for {client_id=}.\"\n        code_block = f\"aws s3 cp {calls.output} .\"\n        task_no_cache = slack_notification_op(\n            notification_text,\n            channel=channel,\n            cc=notify,\n            code_block=code_block,\n            thread_id=slack_thread,\n        )\n        task_no_cache.execution_options.caching_strategy.max_cache_staleness = (\n            \"P0D\"  # disables caching\n        )\n\n\n__all__ = [\"fetch_calls_pipeline\"]\n"
  },
  {
    "repo": "biswaroop1547/skit-pipelines",
    "file_path": "skit_pipelines/pipelines/fetch_n_tag_turns_and_calls/__init__.py",
    "raw_url": "https://raw.githubusercontent.com/biswaroop1547/skit-pipelines/main/skit_pipelines/pipelines/fetch_n_tag_turns_and_calls/__init__.py",
    "content": "import kfp\n\nfrom skit_pipelines import constants as pipeline_constants\nfrom skit_pipelines.components import (\n    fetch_calls_for_slots_op,\n    fetch_calls_op,\n    fetch_gpt_intent_prediction_op,\n    org_auth_token_op,\n    slack_notification_op,\n    tag_calls_op,\n)\n\nUSE_FSM_URL = pipeline_constants.USE_FSM_URL\nREMOVE_EMPTY_AUDIOS = False if USE_FSM_URL else True\n\n\n@kfp.dsl.pipeline(\n    name=\"Fetch and push for tagging turns & calls pipeline\",\n    description=\"fetches calls from production db (or an s3_path) with respective arguments and uploads turns & calls to labelstudio for tagging intent, entities, slots & call metrics.\",\n)\ndef fetch_n_tag_turns_and_calls(\n    org_id: str,\n    lang: str,\n    client_id: str = \"\",\n    data_label: str = \"\",\n    start_date: str = \"\",\n    end_date: str = \"\",\n    labelstudio_project_id: str = \"\",\n    call_project_id: str = \"\",\n    ignore_callers: str = \"\",\n    template_id: str = \"\",\n    use_case: str = \"\",\n    flow_name: str = \"\",\n    min_duration: str = \"\",\n    asr_provider: str = \"\",\n    states: str = \"\",\n    intents: str = \"\",\n    reported: bool = False,\n    call_quantity: int = 200,\n    call_type: str = \"\",\n    start_date_offset: int = 0,\n    end_date_offset: int = 0,\n    start_time_offset: int = 0,\n    end_time_offset: int = 0,\n    calls_file_s3_path: str = \"\",\n    notify: str = \"\",\n    channel: str = \"\",\n    slack_thread: str = \"\",\n    use_fsm_url: bool = False,\n    remove_empty_audios: bool = REMOVE_EMPTY_AUDIOS,\n    use_assisted_annotation: bool = False,\n    flow_ids: str = \"\"\n):\n    \"\"\"\n    A pipeline to randomly sample calls and upload for annotating turns for intents & entities and annotating calls for slots & call level metrics.\n\n    .. _p_fetch_n_tag_turns_and_calls:\n\n    Example payload to invoke via slack integrations:\n\n        @charon run fetch_n_tag_turns_and_calls\n\n        .. code-block:: python\n\n            {\n                \"client_id\": 41,\n                \"org_id\": 34,\n                \"lang\": \"en\",\n                \"start_date\": \"2022-11-10\",\n                \"end_date\": \"2022-11-11\",\n                \"labelstudio_project_id\": 195,\n                \"call_project_id\": 194,\n                \"data_label\": \"Client\"\n            }\n\n    To use labelstudio:\n\n        @charon run fetch_n_tag_turns_and_calls\n\n        .. code-block:: python\n\n            {\n                \"org_id\": 34,\n                \"client_id\": 41,\n                \"start_date\": \"2022-09-16\",\n                \"end_date\": \"2022-09-19\",\n                \"lang\": \"en\",\n                \"reported\": false,\n                \"call_quantity\": 1000,\n                \"flow_name\" : \"indigo_domain_tuning_english\"\n                \"labelstudio_project_id\": \"135\",\n                \"call_project_id\": 194\n            }\n\n    :param client_id: The comma separated client ids as per fsm db.\n    :type client_id: str, optional\n\n    :param org_id: The organization id as per api-gateway.\n    :type org_id: str\n\n    :param labelstudio_project_id: The labelstudio project id for turn level tagging (intent & entities) (this is a number) since this is optional, defaults to \"\".\n    :type labelstudio_project_id: str\n\n    :param call_project_id: The labelstudio project id for call level tagging (slots & call metrics) (this is a number) since this is optional, defaults to \"\".\n    :type call_project_id: str\n\n    :param data_label: A label to identify the source of a datapoint\n    :type data_label: str, optional. Defaults to \"Live\"\n\n    :param start_date: The start date range to filter calls in YYYY-MM-DD format.\n    :type start_date: str\n\n    :param lang: The language code of the calls to filter. eg: en, hi, ta, te, etc.\n    :type lang: str\n\n    :param end_date: The end date range to filter calls in YYYY-MM-DD format.\n    :type end_date: str\n\n    :param ignore_callers: Comma separated list of callers to ignore, defaults to \"\"\n    :type ignore_callers: str, optional\n\n    :param reported: Pick only reported calls, defaults to False\n    :type reported: bool\n\n    :param template_id: The flow template id to filter calls, defaults to \"\"\n    :type template_id: str, optional\n\n    :param use_case: Voice bot project's use-case, defaults to \"\"\n    :type use_case: str, optional\n\n    :param flow_name: Identifier for a whole/part of a voicebot conversation flow, defaults to \"\"\n    :type flow_name: str, optional\n\n    :param min_duration: Call duration filter, defaults to \"\"\n    :type min_duration: str, optional\n\n    :param asr_provider: The ASR vendor (google/VASR), defaults to \"\"\n    :type asr_provider: str, optional\n\n    :param states: Filter calls in a comma separated list of states, defaults to \"\"\n    :type states: str, optional\n\n    :param intents: Filter turns in sampled calls from a comma separated list of intents, defaults to \"\"\n    :type intents: str, optional\n\n    :param start_date_offset: Offset the start date by an integer value, defaults to 0\n    :type start_date_offset: int, optional\n\n    :param end_date_offset: Offset the end date by an integer value, defaults to 0\n    :type end_date_offset: int, optional\n\n    :param start_time_offset: Offset the start time by an integer value, defaults to 0\n    :type start_time_offset: int, optional\n\n    :param end_time_offset: Offset the end time by an integer value, defaults to 0\n    :type end_time_offset: int, optional\n\n    :param calls_file_s3_path: The s3_path to upload the turns from instead of querying from FSM_db, defaults to \"\"\n    :type calls_file_s3_path: str, optional\n\n    :param call_quantity: Number of calls to sample, defaults to 200\n    :type call_quantity: int, optional\n\n    :param call_type: INBOUND, OUTBOUND, or CALL_TEST call filters. We can currently choose only one of these, or defaults to \"INBOUND\" and \"OUTBOUND\" both\n    :type call_type: str, optional\n\n    :param notify: Whether to send a slack notification, defaults to \"\"\n    :type notify: str, optional\n\n    :param channel: The slack channel to send the notification, defaults to \"\"\n    :type channel: str, optional\n\n    :param slack_thread: The slack thread to send the notification, defaults to \"\"\n    :type slack_thread: float, optional\n\n    :param use_fsm_url: Whether to use turn audio url from fsm or s3 path., defaults to False\n    :type use_fsm_url: bool, optional\n\n    :param remove_empty_audios: Whether to turns of empty audio., defaults to False\n    :type remove_empty_audios: bool, optional\n\n    :param use_assisted_annotation: Whether to use GPT for intent prediction, only applicable to US collections, defaults to False\n    :type use_assisted_annotation: bool, optional\n    \n    :param flow_ids: Id for a whole/part of a voicebot conversation flow, defaults to \"\"\n    :type flow_ids: str, optional\n    \"\"\"\n    calls = fetch_calls_op(\n        client_id=client_id,\n        start_date=start_date,\n        end_date=end_date,\n        lang=lang,\n        call_quantity=call_quantity,\n        call_type=call_type,\n        start_date_offset=start_date_offset,\n        end_date_offset=end_date_offset,\n        start_time_offset=start_time_offset,\n        end_time_offset=end_time_offset,\n        ignore_callers=ignore_callers,\n        reported=reported,\n        template_id=template_id,\n        use_case=use_case,\n        flow_name=flow_name,\n        min_duration=min_duration,\n        asr_provider=asr_provider,\n        intents=intents,\n        states=states,\n        calls_file_s3_path=calls_file_s3_path,\n        use_fsm_url=USE_FSM_URL or use_fsm_url,\n        remove_empty_audios=remove_empty_audios,\n        flow_ids=flow_ids\n    )\n\n    calls.execution_options.caching_strategy.max_cache_staleness = (\n        \"P0D\"  # disables caching\n    )\n\n    auth_token = org_auth_token_op(org_id)\n    auth_token.execution_options.caching_strategy.max_cache_staleness = (\n        \"P0D\"  # disables caching\n    )\n\n    with kfp.dsl.Condition(calls.output != \"\", \"calls_found\").after(calls):\n        # Get intent response from GPT for qualifying turns\n        gpt_response_path = fetch_gpt_intent_prediction_op(\n            s3_file_path=calls.output, use_assisted_annotation=use_assisted_annotation\n        )\n\n        # uploads data for turn level intent, entity & transcription tagging\n        tag_turns_output = tag_calls_op(\n            input_file=gpt_response_path.output,\n            project_id=labelstudio_project_id,\n            data_label=data_label,\n        )\n\n        fetch_slot_and_calls_output = fetch_calls_for_slots_op(\n            untagged_records_path=calls.output,\n            org_id=org_id,\n            language_code=lang,\n            start_date=start_date,\n            end_date=end_date,\n        )\n\n        # uploads data for call & slot level tagging to labelstudio\n        tag_calls_output = tag_calls_op(\n            input_file=fetch_slot_and_calls_output.output,\n            call_project_id=call_project_id,\n            data_label=data_label,\n        )\n\n        with kfp.dsl.Condition(notify != \"\", \"notify\").after(tag_turns_output):\n            df_sizes = tag_turns_output.outputs[\"df_sizes\"]\n            errors = tag_turns_output.outputs[\"errors\"]\n\n            notification_text = f\"\"\"Finished a request for {call_quantity} calls. Fetched from {start_date} to {end_date} for {client_id=}.\n            Uploaded {getattr(calls, 'output')} ({df_sizes}, {org_id=}) for tagging to {labelstudio_project_id=}.\"\"\"\n            notification_text += f\"\\nErrors: {errors}\" if errors else \"\"\n\n            task_no_cache = slack_notification_op(\n                notification_text, channel=channel, cc=notify, thread_id=slack_thread\n            )\n            task_no_cache.execution_options.caching_strategy.max_cache_staleness = (\n                \"P0D\"  # disables caching\n            )\n\n            df_sizes2 = tag_calls_output.outputs[\"df_sizes\"]\n            errors2 = tag_calls_output.outputs[\"errors\"]\n\n            notification_text = f\"\"\"Finished a request for {call_quantity} calls. Fetched from {start_date} to {end_date} for {client_id=}.\n            Uploaded {getattr(fetch_slot_and_calls_output, 'output')} ({df_sizes2}, {org_id=}) for call & slot tagging to {call_project_id=}.\"\"\"\n            notification_text += f\"\\nErrors: {errors2}\" if errors else \"\"\n\n            task_no_cache2 = slack_notification_op(\n                notification_text, channel=channel, cc=notify, thread_id=slack_thread\n            )\n            task_no_cache2.execution_options.caching_strategy.max_cache_staleness = (\n                \"P0D\"  # disables caching\n            )\n\n    with kfp.dsl.Condition(calls.output == \"\", \"no_calls\").after(calls):\n        with kfp.dsl.Condition(notify != \"\", \"notify\").after(calls):\n            notification_text = f\"\"\"No calls could be found from {start_date} to {end_date} for {client_id=}.\n                        Please verify the parameters you have used or refer to the debugging guide on Notion.\"\"\"\n\n            task_no_cache2 = slack_notification_op(\n                notification_text, channel=channel, cc=notify, thread_id=slack_thread\n            )\n            task_no_cache2.execution_options.caching_strategy.max_cache_staleness = (\n                \"P0D\"  # disables caching\n            )\n\n\n__all__ = [\"fetch_n_tag_turns_and_calls\"]\n"
  },
  {
    "repo": "biswaroop1547/skit-pipelines",
    "file_path": "skit_pipelines/pipelines/fetch_tagged_calls_dataset/__init__.py",
    "raw_url": "https://raw.githubusercontent.com/biswaroop1547/skit-pipelines/main/skit_pipelines/pipelines/fetch_tagged_calls_dataset/__init__.py",
    "content": "import tempfile\n\nimport kfp\n\nfrom skit_pipelines import constants as pipeline_constants\nfrom skit_pipelines.components import (\n    fetch_tagged_dataset_op,\n    slack_notification_op,\n    upload2s3_op,\n)\n\n\n@kfp.dsl.pipeline(\n    name=\"Fetch Tagged Dataset Pipeline\",\n    description=\"fetches tagged dataset from tog with respective arguments\",\n)\ndef fetch_tagged_calls_dataset(\n    org_id: str,\n    job_id: str = \"\",\n    labelstudio_project_id: str = \"\",\n    start_date: str = \"\",\n    end_date: str = \"\",\n    timezone: str = \"Asia/Kolkata\",\n    task_type: str = \"conversation\",\n    notify: str = \"\",\n    channel: str = \"\",\n    slack_thread: str = \"\",\n):\n    \"\"\"\n    A pipeline to fetch tagged dataset.\n\n    .. _p_fetch_tagged_calls_dataset:\n\n    Example payload to invoke via slack integrations:\n\n        @charon run fetch_tagged_calls_dataset\n\n        .. code-block:: python\n\n            {\n                \"org_id\": 1,\n                \"job_id\": \"4011\",\n                \"start_date\": \"2020-01-01\",\n                \"end_date\": \"2020-01-01\"\n            }\n\n    To use labelstudio:\n\n        @charon run fetch_tagged_calls_dataset\n\n        .. code-block:: python\n\n            {\n                \"org_id\": 1,\n                \"labelstudio_project_id\": \"40\",\n                \"start_date\": \"2020-01-01\",\n                \"end_date\": \"2020-01-01\"\n            }\n\n    :param org_id: reference path to save the metrics.\n    :type org_id: str\n    :param job_ids: The job ids as per tog. Optional if labestudio project id is provided.\n    :type job_id: str\n    :param labelstudio_project_id: The labelstudio project id (this is a number) since this is optional, defaults to \"\".\n    :type labelstudio_project_id: str\n    :param start_date: The start date range (YYYY-MM-DD) to filter tagged data.\n    :type start_date: str\n    :param end_date: The end date range (YYYY-MM-DD) to filter tagged data\n    :type end_date: str\n    :param timezone: The timezone to apply for multi-region datasets, defaults to \"Asia/Kolkata\"\n    :type timezone: str, optional\n    :param task_type: https://github.com/skit-ai/skit-labels#task-types, defaults to \"conversation\"\n    :type task_type: str, optional\n    :param notify: A comma separated list of slack ids: \"@apples, @orange.fruit\" etc, defaults to \"\"\n    :type notify: str, optional\n    :param channel: The slack channel to send the notification, defaults to \"\"\n    :type channel: str, optional\n    :param slack_thread: The slack thread to send the notification, defaults to \"\"\n    :type slack_thread: str, optional\n    \"\"\"\n    tagged_df = fetch_tagged_dataset_op(\n        job_id=job_id,\n        project_id=labelstudio_project_id,\n        task_type=task_type,\n        timezone=timezone,\n        start_date=start_date,\n        end_date=end_date,\n    )\n    tagged_df.execution_options.caching_strategy.max_cache_staleness = (\n        \"P0D\"  # disables caching\n    )\n    s3_upload = upload2s3_op(\n        path_on_disk=tagged_df.outputs[\"output\"],\n        reference=f\"datasets/{org_id}_{job_id}\",\n        file_type=f\"tagged\",\n        bucket=pipeline_constants.KUBEFLOW_SANDBOX_BUCKET,\n        ext=\".csv\",\n    )\n\n    notification_text = f\"Here is your data for {org_id=} and {job_id=}.\"\n    code_block = f\"aws s3 cp {s3_upload.output} .\"\n    with kfp.dsl.Condition(notify != \"\", \"notify\").after(s3_upload) as check1:\n        task_no_cache = slack_notification_op(\n            notification_text,\n            code_block=code_block,\n            cc=notify,\n            channel=channel,\n            thread_id=slack_thread,\n        )\n        task_no_cache.execution_options.caching_strategy.max_cache_staleness = (\n            \"P0D\"  # disables caching\n        )\n\n\n__all__ = [\"fetch_tagged_calls_dataset\"]\n"
  },
  {
    "repo": "biswaroop1547/skit-pipelines",
    "file_path": "skit_pipelines/pipelines/fetch_tagged_data_from_labelstore/__init__.py",
    "raw_url": "https://raw.githubusercontent.com/biswaroop1547/skit-pipelines/main/skit_pipelines/pipelines/fetch_tagged_data_from_labelstore/__init__.py",
    "content": "import kfp\n\nfrom skit_pipelines import constants as pipeline_constants\nfrom skit_pipelines.components import (\n    fetch_tagged_data_label_store_op,\n    slack_notification_op,\n    upload2s3_op,\n)\n\n\n@kfp.dsl.pipeline(\n    name=\"Fetch annotated data from label store\",\n    description=\"A pipeline aimed at querying intent, entity, and transcriptions that happen across Skit\",\n)\ndef fetch_tagged_data_from_labelstore(\n    flow_id: str,\n    start_date: str = \"\",\n    end_date: str = \"\",\n    limit: int = 2000,\n    notify: str = \"\",\n    channel: str = \"\",\n    slack_thread: str = \"\",\n    data_labels: str = \"\",\n):\n    \"\"\"\n    A pipeline aimed at querying intent, entity, and transcriptions that happen across Skit\n\n    .. _p_fetch_tagged_data_from_labelstore:\n\n    Example payload to invoke via slack integrations:\n\n        @charon run fetch_tagged_data_from_labelstore\n\n        .. code-block:: python\n\n            {\n                \"flow_id\": \"294\",\n                \"limit\": 20,\n                \"start_date\": \"2022-11-12\",\n                \"end_date\": \"2022-11-16\",\n                \"data_labels\": \"Client, Live\"\n            }\n\n    :param flow_id: The id of the flow from which annotated data should be queried\n    :type flow_id: str\n\n    :param start_date: The start date range (YYYY-MM-DD) to filter tagged data. defaults to yesterday\n    :type start_date: str, optional\n\n    :param end_date: The end date range (YYYY-MM-DD) to filter tagged data, defaults to today\n    :type end_date: str, optional\n\n    :param limit: Number of annotations to fetch, defaults to 2000\n    :type limit: int, optional\n\n    :param notify: Whether to send a slack notification, defaults to \"\"\n    :type notify: str, optional\n\n    :param channel: The slack channel to send the notification, defaults to \"\"\n    :type channel: str, optional\n\n    :param slack_thread: The slack thread to send the notification, defaults to \"\"\n    :type slack_thread: float, optional\n\n    :param data_labels: Comma seperated data labels to filter, defaults to \"\"\n    :type data_labels: str, optional\n    \"\"\"\n    tagged_df = fetch_tagged_data_label_store_op(\n        flow_id=flow_id,\n        start_date=start_date,\n        end_date=end_date,\n        limit=limit,\n        data_labels=data_labels,\n    )\n\n    tagged_df.execution_options.caching_strategy.max_cache_staleness = (\n        \"P0D\"  # disables caching\n    )\n\n    s3_upload = upload2s3_op(\n        path_on_disk=tagged_df.outputs[\"output\"],\n        reference=f\"{flow_id}-{start_date}-{end_date}\",\n        file_type=f\"annotations-with-call-context\",\n        bucket=pipeline_constants.KUBEFLOW_SANDBOX_BUCKET,\n        ext=\".csv\",\n    )\n\n    s3_upload.execution_options.caching_strategy.max_cache_staleness = (\n        \"P0D\"  # disables caching\n    )\n\n    notification_text = (\n        f\"Here is your data for {flow_id=} and date_range: {start_date=}, {end_date=}.\"\n    )\n    code_block = f\"aws s3 cp {s3_upload.output} .\"\n    with kfp.dsl.Condition(notify != \"\", \"notify\").after(s3_upload) as check1:\n        task_no_cache = slack_notification_op(\n            notification_text,\n            code_block=code_block,\n            cc=notify,\n            channel=channel,\n            thread_id=slack_thread,\n        )\n        task_no_cache.execution_options.caching_strategy.max_cache_staleness = (\n            \"P0D\"  # disables caching\n        )\n\n\n__all__ = [\"fetch_tagged_data_from_labelstore\"]\n"
  },
  {
    "repo": "biswaroop1547/skit-pipelines",
    "file_path": "skit_pipelines/pipelines/fetch_tagged_entity_dataset/__init__.py",
    "raw_url": "https://raw.githubusercontent.com/biswaroop1547/skit-pipelines/main/skit_pipelines/pipelines/fetch_tagged_entity_dataset/__init__.py",
    "content": "import kfp\n\nfrom skit_pipelines import constants as pipeline_constants\nfrom skit_pipelines.components import (\n    fetch_tagged_dataset_op,\n    modify_entity_dataset_op,\n    slack_notification_op,\n    upload2s3_op,\n)\n\n\n@kfp.dsl.pipeline(\n    name=\"Fetch Tagged Entities Dataset Pipeline\",\n    description=\"fetches tagged entity dataset from tog & does few modifications for eval\",\n)\ndef fetch_tagged_entity_dataset(\n    org_id: str,\n    job_id: str = \"\",\n    labelstudio_project_id: str = \"\",\n    start_date: str = \"\",\n    end_date: str = \"\",\n    timezone: str = \"Asia/Kolkata\",\n    task_type: str = \"conversation\",\n    notify: str = \"\",\n    channel: str = \"\",\n    slack_thread: str = \"\",\n):\n    \"\"\"\n    A pipeline to fetch tagged entity dataset wiht modifications ready for eval.\n\n    .. _p_fetch_tagged_entity_dataset:\n\n    Example payload to invoke via slack integrations:\n\n        @charon run fetch_tagged_entity_dataset\n\n        .. code-block:: python\n\n            {\n                \"org_id\": 1,\n                \"job_id\": \"4011\",\n                \"start_date\": \"2020-01-01\",\n                \"end_date\": \"2020-01-01\"\n            }\n\n    To use labelstudio:\n\n        @charon run fetch_tagged_entity_dataset\n\n        .. code-block:: python\n\n            {\n                \"org_id\": 1,\n                \"labelstudio_project_id\": \"40\",\n                \"start_date\": \"2020-01-01\",\n                \"end_date\": \"2020-01-01\"\n            }\n\n    :param org_id: reference path to save the metrics.\n    :type org_id: str\n    :param job_ids: The job ids as per tog. Optional if labestudio project id is provided.\n    :type job_id: str\n    :param labelstudio_project_id: The labelstudio project id (this is a number) since this is optional, defaults to \"\".\n    :type labelstudio_project_id: str\n    :param start_date: The start date range (YYYY-MM-DD) to filter tagged data.\n    :type start_date: str\n    :param end_date: The end date range (YYYY-MM-DD) to filter tagged data\n    :type end_date: str\n    :param timezone: The timezone to apply for multi-region datasets, defaults to \"Asia/Kolkata\"\n    :type timezone: str, optional\n    :param task_type: https://github.com/skit-ai/skit-labels#task-types, defaults to \"conversation\"\n    :type task_type: str, optional\n    :param notify: A comma separated list of slack ids: \"@apples, @orange.fruit\" etc, defaults to \"\"\n    :type notify: str, optional\n    :param channel: The slack channel to send the notification, defaults to \"\"\n    :type channel: str, optional\n    :param slack_thread: The slack thread to send the notification, defaults to \"\"\n    :type slack_thread: str, optional\n    \"\"\"\n\n    tagged_df = fetch_tagged_dataset_op(\n        job_id=job_id,\n        project_id=labelstudio_project_id,\n        task_type=task_type,\n        timezone=timezone,\n        start_date=start_date,\n        end_date=end_date,\n    )\n    tagged_df.execution_options.caching_strategy.max_cache_staleness = (\n        \"P0D\"  # disables caching\n    )\n\n    modified_df = modify_entity_dataset_op(\n        tagged_df.outputs[\"output\"],\n        tog_job_id=job_id,\n        labelstudio_project_id=labelstudio_project_id,\n        timezone=timezone,\n    )\n    modified_df.execution_options.caching_strategy.max_cache_staleness = (\n        \"P0D\"  # disables caching\n    )\n\n    s3_upload = upload2s3_op(\n        path_on_disk=modified_df.outputs[\"output\"],\n        reference=f\"{org_id}_{job_id}\",\n        file_type=f\"tagged_entity\",\n        bucket=pipeline_constants.KUBEFLOW_SANDBOX_BUCKET,\n        ext=\".csv\",\n    )\n\n    notification_text = f\"Here is your tagged entity data for {org_id=} and {job_id=}.\"\n    code_block = f\"aws s3 cp {s3_upload.output} .\"\n    with kfp.dsl.Condition(notify != \"\", \"notify\").after(s3_upload) as check1:\n        task_no_cache = slack_notification_op(\n            notification_text,\n            code_block=code_block,\n            cc=notify,\n            channel=channel,\n            thread_id=slack_thread,\n        )\n        task_no_cache.execution_options.caching_strategy.max_cache_staleness = (\n            \"P0D\"  # disables caching\n        )\n\n\n__all__ = [\"fetch_tagged_entity_dataset\"]\n"
  },
  {
    "repo": "biswaroop1547/skit-pipelines",
    "file_path": "skit_pipelines/pipelines/generate_and_tag_conversations/__init__.py",
    "raw_url": "https://raw.githubusercontent.com/biswaroop1547/skit-pipelines/main/skit_pipelines/pipelines/generate_and_tag_conversations/__init__.py",
    "content": "import kfp\nfrom kfp.components import OutputPath\nfrom typing import Optional\n\nfrom skit_pipelines import constants as pipeline_constants\nfrom skit_pipelines.components import (\n    upload2s3_op,\n    zip_file_and_notify_op,\n    slack_notification_op,\n    validate_and_add_situations_to_db_op,\n    final_conversation_generator_op,\n    upload_conv_to_label_studio_op,\n    upload_conversation_data_to_metrics_db_op,\n)\n\n\n@kfp.dsl.pipeline(\n    name=\"Generate and tag conversations\",\n    description=\"Generate final conversations based on the situation data provided and upload it to labelstudio for tagging\",\n)\ndef generate_and_tag_conversations(\n    *,\n    situations: str = \"\",\n    scenario: str = \"\",\n    scenario_category: str = \"\",\n    s3_links_to_prompts: str = \"\",\n    llm_trainer_repo_name: str = \"LLMtrainer\",\n    llm_trainer_repo_branch: str = \"main\",\n    model: str = 'gpt-4',\n    n_iter: int = 1,\n    n_choice: int = 2,\n    temperature: float = 0.99,\n    client_id: str,\n    template_id: str,\n    labelstudio_project_id: str,\n    data_label: str = \"\",\n    project_name: str = \"\",\n    notify: str = \"\",\n    channel: str = \"\",\n    slack_thread: str = \"\"\n    ):\n    \"\"\"\n    A pipeline to generate and tag conversations given a situation\n    \n    .. _p_generate_and_tag_conversations:\n\n    Example payload to invoke via slack integrations:\n\n    A minimal example:\n\n        @charon run generate_and_tag_conversations\n\n        .. code-block:: python\n\n            {   \"situations\" : \"The user disputes the debt, so the agent transfers the call to the agent :: The user cannot pay any amount as they have a difficult situation, so the agent hangs up the call. \",\n                \"scenario\" : \"Test scenario\",\n                \"scenario_category\" : \"Test scenario category\",\n                \"llm_trainer_repo_branch\" : \"refactor-data-gen-script\",\n                \"client_id\" : \"85\",\n                \"template_id\" : \"0\",\n                \"labelstudio_project_id\" : \"95\",\n                \"s3_links_to_prompts\": \"s3://kubeflow-us-cluster/pipeline_uploads/prompt/test_prompt.txt\",\n                \"data_label\" : \"UAT\",\n                \"project_name\" : \"test project name\"\n            }\n\n\n    A full available parameters example:\n\n        @charon run generate_and_tag_conversations\n\n        .. code-block:: python\n\n            {   \"situations\" : \"The user disputes the debt, so the agent transfers the call to the agent :: The user cannot pay any amount as they have a difficult situation, so the agent hangs up the call. \",\n                \"scenario\" : \"Test scenario\",\n                \"scenario_category\" : \"Test scenario category\",\n                \"llm_trainer_repo_branch\" : \"refactor-data-gen-script\",\n                \"client_id\" : \"85\",\n                \"template_id\" : \"0\",\n                \"labelstudio_project_id\" : \"95\",\n                \"s3_links_to_prompts\": \"s3://kubeflow-us-cluster/pipeline_uploads/prompt/test_prompt.txt\",\n                \"data_label\" : \"UAT\",\n                \"project_name\" : \"test project name\"\n            }\n    \n    :param situations: The situations for generating the conversations, use delimiter :: to pass multiple situations\n    :type situations: optional\n\n    :param scenario: The scenario linked to the situation\n    :type scenario: optional\n    \n    :param scenario_category: The scenarios category\n    :type scenario_category: optional\n    \n    :param prompt: Prompt to the model for data generation\n    type prompt: str\n    \n    :param s3_links_to_prompts: s3 links to the prompt to the model for data generation\n    :type s3_links_to_prompts: str\n    \n    :param output_dir: The output directory where the generated conversations gets stored\n    :type output_dir: str\n\n    :param filename: Acts as a prfix to the default naming used\n    :type filename: str\n\n    :param llm_trainer_repo_name: The conversation generation repo name in Github.\n    :type llm_trainer_repo_name: str\n    \n    :param llm_trainer_repo_branch: The branch name in the conversation generation repo to use , defaults to main.\n    :type llm_trainer_repo_branch: str, optional\n    \n    :param model: Optional model to be used for generating data \n    :type model: str\n    \n    :param n_iter: No of times we make iterate on scenarios list to generate conversations\n    type n_iter: int\n    \n    :param n_choice: No of convs generated in a single time from a scenario.\n    type n_choice: int\n    \n    :param temperature: Temperature\n    type temperature: float\n    \n    :param client_id: id of the client for which data is being generated\n    :type client_id : str\n    \n    :param template_id: template id for which data is being generated\n    :type template_id : str\n    \n    :param project_name: project name to distinguish between various experiments\n    :type project_name : str\n    \n    :param notify: Whether to send a slack notification, defaults to \"\"\n    :type notify: str, optional\n\n    :param channel: The slack channel to send the notification, defaults to \"\"\n    :type channel: str, optional\n\n    :param slack_thread: The slack thread to send the notification, defaults to \"\"\n    :type slack_thread: str, optional\n\n    \"\"\"\n    \n    validate_situations = validate_and_add_situations_to_db_op(situations=situations,\n                                                         scenario=scenario ,  \n                                                         scenario_category=scenario_category)\n    \n    situations_id_info = validate_situations.outputs['situation_mapping_info']\n    conv_s3_dir_name = f'llm_artifacts/generated_conversations/{client_id}_{template_id}'\n    \n    conv_generation_output= final_conversation_generator_op(situation_info_list=situations_id_info,\n                                                        s3_links_to_prompts = s3_links_to_prompts,\n                                                        n_iter=n_iter,\n                                                        n_choice=n_choice,\n                                                        temperature=temperature,\n                                                        model=model,\n                                                        llm_trainer_repo_name=llm_trainer_repo_name,\n                                                        llm_trainer_repo_branch=llm_trainer_repo_branch,\n                                                    )\n    conversations_dir = conv_generation_output.outputs[\"output\"]\n        \n    conversation_s3_upload = upload2s3_op(\n            path_on_disk=conversations_dir,\n            reference=conv_s3_dir_name,\n            bucket=pipeline_constants.KUBEFLOW_SANDBOX_BUCKET,\n            upload_as_directory=True,\n            ext=\"\"\n        )\n\n    tag_calls_output = upload_conv_to_label_studio_op(project_id=labelstudio_project_id, \n                                                      conversations_dir= conversations_dir, \n                                                      data_label=data_label, \n                                                      situations_id_info=situations_id_info)\n    \n    \n    upload_df_sizes = tag_calls_output.outputs[\"df_sizes\"]\n    upload_errors = tag_calls_output.outputs[\"errors\"] \n    \n    \n    with kfp.dsl.Condition(notify != \"\", \"notify\").after(conversation_s3_upload) as check1:\n        notification_text_1 = f\"Generated conversations are successfully uploaded to s3 for client_id  : {client_id}.\"\n        code_block = f\"aws s3 cp {conversation_s3_upload.output} .\"\n        prompt_s3_notif = slack_notification_op(\n            message=notification_text_1,\n            channel=channel,\n            cc=notify,\n            code_block=code_block,\n            thread_id=slack_thread,\n        )\n        \n        prompt_s3_notif.execution_options.caching_strategy.max_cache_staleness = (\n            \"P0D\"  # disables caching\n        )\n        \n        notification_text_2 = \"Here is the ZIP file generated by the Generate Sample conversations Pipeline.\"\n        zip_file_and_notify = zip_file_and_notify_op(\n                    path_on_disk = conversations_dir, \n                    message = notification_text_2,\n                    channel = channel,\n                    thread_id = slack_thread,\n                    file_title = 'generated_conversations',\n                    file_name = 'generated_conversations.zip',\n                    notify = notify,\n                    display_sample = True,\n                    ).after(prompt_s3_notif)\n        \n        zip_file_and_notify.execution_options.caching_strategy.max_cache_staleness = (\n            \"P0D\"  # disables caching\n        )\n    \n    with kfp.dsl.Condition(notify != \"\", \"notify\").after(tag_calls_output) as check2:\n\n        notification_text = f\"\"\"Uploaded the {upload_df_sizes} conversations for tagging to {labelstudio_project_id=}.\"\"\"\n        \n        notification_text += f\"\\nErrors: {upload_errors}\" if upload_errors else \"\"\n\n        task_no_cache = slack_notification_op(\n            notification_text, channel=channel, cc=notify, thread_id=slack_thread\n        )\n        task_no_cache.execution_options.caching_strategy.max_cache_staleness = (\n            \"P0D\"  # disables caching\n        )\n\n    with kfp.dsl.Condition(upload_errors == [], \"upload_to_metrics_db\").after(tag_calls_output) as check3:\n        upload_to_metrics_db_op = upload_conversation_data_to_metrics_db_op(situations_id_info=situations_id_info, client_id=client_id,\n                                                                            template_id=template_id, \n                                                                            generated_conversations_s3_link=conversation_s3_upload.output,\n                                                                            prompt_links_in_s3=s3_links_to_prompts, conv_directory=conversations_dir, \n                                                                            project_name=project_name)\n    \n\n__all__ = [\"generate_and_tag_conversations\"]\n"
  },
  {
    "repo": "biswaroop1547/skit-pipelines",
    "file_path": "skit_pipelines/pipelines/generate_sample_conversations/__init__.py",
    "raw_url": "https://raw.githubusercontent.com/biswaroop1547/skit-pipelines/main/skit_pipelines/pipelines/generate_sample_conversations/__init__.py",
    "content": "import kfp\nfrom kfp.components import OutputPath\nfrom typing import Optional\n\nfrom skit_pipelines import constants as pipeline_constants\nfrom skit_pipelines.components import (\n    sample_conversations_generator_op,\n    upload2s3_op,\n    zip_file_and_notify_op,\n    slack_notification_op\n)\n\n\n@kfp.dsl.pipeline(\n    name=\"Generate sample conversations\",\n    description=\"Generate sample conversations based on the situation data provided\",\n)\ndef generate_sample_conversations(\n    *,\n    situations: Optional[str],\n    s3_links_to_prompts: str = \"\",\n    filename: str = \"\",\n    llm_trainer_repo_name: str = \"LLMtrainer\",\n    llm_trainer_repo_branch: str = \"main\",\n    model: str = 'gpt-4',\n    n_iter: int = 1,\n    n_choice: int = 2,\n    temperature: float = 0.99,\n    notify: str = \"\",\n    channel: str = \"\",\n    slack_thread: str = \"\"\n    ):\n    \"\"\"\n    A pipeline to sample conversations given a situation\n    \n    .. _p_generate_sample_conversations:\n\n    Example payload to invoke via slack integrations:\n\n    A minimal example:\n\n        @charon run generate_sample_conversations\n\n        .. code-block:: python\n\n            {\n                \"situations\": \"The user wants to talk to a human agent, so the agent transfers the call\",\n                \"llm_trainer_repo_name\": \"LLMtrainer\",\n                \"llm_trainer_repo_branch\": \"main\"\n                }\n\n\n    A full available parameters example:\n\n        @charon run generate_sample_conversations\n\n        .. code-block:: python\n\n            {\n                \"situations\": \"The user disputes the debt, so the agent transfers the call to the agent :: The user cannot pay any amount as they have a difficult situation, so the agent hangs up the call. \"\n                \"llm_trainer_repo_name\": \"LLMtrainer\",\n                \"llm_trainer_repo_branch\": \"main\",\n            }\n\n    :param situations: The situations for generating the conversations\n    :type situations: optional\n    \n    :param prompt: Prompt to the model for data generation\n    type prompt: str\n\n    :param filename: Acts as a prfix to the default naming used\n    :type filename: str\n\n    :param llm_trainer_repo_name: The conversation generation repo name in Github.\n    :type llm_trainer_repo_name: str\n    \n    :param llm_trainer_repo_branch: The branch name in the conversation generation repo to use , defaults to main.\n    :type llm_trainer_repo_branch: str, optional\n    \n    :param model: Optional model to be used for generating data \n    :type model: str\n    \n    :param n_iter: No of times we make iterate on sub_scenarios list to generate conversations\n    type n_iter: int\n    \n    :param n_choice: No of convs generated in a single time from a scenario.\n    type n_choice: int\n    \n    :param temperature: Temperature\n    type temperature: float\n    \n    :param notify: Whether to send a slack notification, defaults to \"\"\n    :type notify: str, optional\n\n    :param channel: The slack channel to send the notification, defaults to \"\"\n    :type channel: str, optional\n\n    :param slack_thread: The slack thread to send the notification, defaults to \"\"\n    :type slack_thread: str, optional\n\n    \"\"\"\n    from skit_pipelines import constants as pipeline_constants\n    \n    prompt_generation = sample_conversations_generator_op(\n        situations=situations,\n        llm_trainer_repo_name=llm_trainer_repo_name,\n        llm_trainer_repo_branch=llm_trainer_repo_branch,\n        filename=filename,\n        model=model,\n        prompt_file_path=s3_links_to_prompts,\n        n_iter=n_iter,\n        n_choice=n_choice,\n        temperature=temperature\n    )\n\n    prompt_s3_upload = upload2s3_op(\n            path_on_disk=prompt_generation.outputs[\"output\"],\n            reference='llm_artifacts/generated_conversations/',\n            bucket=pipeline_constants.KUBEFLOW_SANDBOX_BUCKET,\n            upload_as_directory=True,\n            ext=\"\"\n        )\n\n    with kfp.dsl.Condition(notify != \"\", \"notify\").after(prompt_s3_upload):\n        notification_text_1 = f\"Generated conversations are successfully uploaded to s3.\"\n        code_block = f\"aws s3 cp {prompt_s3_upload.output} .\"\n        prompt_s3_notif = slack_notification_op(\n            message=notification_text_1,\n            channel=channel,\n            cc=notify,\n            code_block=code_block,\n            thread_id=slack_thread,\n        )\n        \n        prompt_s3_notif.execution_options.caching_strategy.max_cache_staleness = (\n            \"P0D\"  # disables caching\n        )\n        \n        notification_text_2 = \"Here is the ZIP file generated by the Generate Sample conversations Pipeline.\"\n        zip_file_and_notify = zip_file_and_notify_op(\n                    path_on_disk = prompt_generation.outputs[\"output\"], \n                    message = notification_text_2,\n                    channel = channel,\n                    thread_id = slack_thread,\n                    file_title = 'generated_conversations',\n                    file_name = 'generated_conversations.zip',\n                    notify = notify,\n                    display_sample = True,\n                    ).after(prompt_s3_notif)\n        \n        zip_file_and_notify.execution_options.caching_strategy.max_cache_staleness = (\n            \"P0D\"  # disables caching\n        )\n\n\n__all__ = [\"generate_sample_conversations\"]\n"
  },
  {
    "repo": "biswaroop1547/skit-pipelines",
    "file_path": "skit_pipelines/pipelines/invalidate_llm_situations_in_db/__init__.py",
    "raw_url": "https://raw.githubusercontent.com/biswaroop1547/skit-pipelines/main/skit_pipelines/pipelines/invalidate_llm_situations_in_db/__init__.py",
    "content": "from typing import Optional\n\nimport kfp\n\nfrom skit_pipelines.components import (\n    invalidate_situations_in_db_op,\n    slack_notification_op\n)\n\n\n@kfp.dsl.pipeline(\n    name=\"Invalidate situations\",\n    description=\"\"\"Sets conversations as invalid, thereby preventing it from being used for training\"\"\",\n)\ndef invalidate_llm_situations_in_db(\n    situation_id_list: str = \"\",\n    notify: str = \"\",\n    channel: str = \"\",\n    slack_thread: str = \"\",\n):\n    \"\"\"\n    A pipeline to discard situations by setting flag is_valid to False for situations that are no longer needed\n\n    .. invalidate_llm_situations_in_db:\n\n    Example payload to invoke via slack integrations:\n\n    @charon run invalidate_llm_situations_in_db\n\n    .. code-block:: python\n\n        {\n            \"situation_id_list\": \"1, 3, 5\"\n        }\n    \n    :param situation_id_list: A comma separated list of situation ids from the situation_scenario_mapper table: \"1, 2\" etc, defaults to \"\"\n    :type situation_id_list: str \n    :param notify: A comma separated list of slack ids: \"@apples, @orange.fruit\" etc, defaults to \"\"\n    :type notify: str, optional\n    :param channel: The slack channel to send the notification, defaults to \"\"\n    :type channel: str, optional\n    :param slack_thread: The slack thread to send the notification, defaults to \"\"\n    :type slack_thread: str, optional\n\n    \"\"\"\n\n    update_situation_validity = invalidate_situations_in_db_op(\n        situation_id=situation_id_list\n    )\n\n    with kfp.dsl.Condition(notify != \"\", \"notify\").after(update_situation_validity):\n        notification_text = f\"is_valid has been set to False for the situations : {situation_id_list}\"\n\n        task_no_cache = slack_notification_op(\n            notification_text,\n            cc=notify,\n            channel=channel,\n            thread_id=slack_thread,\n        )\n        task_no_cache.execution_options.caching_strategy.max_cache_staleness = (\n            \"P0D\"  # disables caching\n        )\n\n\n__all__ = [\"invalidate_llm_situations_in_db\"]\n"
  },
  {
    "repo": "biswaroop1547/skit-pipelines",
    "file_path": "skit_pipelines/pipelines/publish_compliance_breaches/__init__.py",
    "raw_url": "https://raw.githubusercontent.com/biswaroop1547/skit-pipelines/main/skit_pipelines/pipelines/publish_compliance_breaches/__init__.py",
    "content": "from typing import Optional\n\nimport kfp\n\nfrom skit_pipelines.components import (\n    fetch_calls_op,\n    identify_compliance_breaches_llm_op,\n    push_compliance_report_to_postgres_op,\n    slack_notification_op,\n)\n\n\n@kfp.dsl.pipeline(\n    name=\"Check calls for compliance breaches\",\n    description=\"\"\"Fetches sampled calls from production db and checks for potential compliance breaches (only \n        for US collections application\"\"\",\n)\ndef publish_compliance_breaches(\n    lang: str,\n    template_id: Optional[str] = None,\n    start_date: str = \"\",\n    end_date: str = \"\",\n    start_date_offset: int = 0,\n    end_date_offset: int = 0,\n    reported: bool = False,\n    call_quantity: int = 1000,\n    notify: str = \"\",\n    channel: str = \"\",\n    slack_thread: str = \"\",\n):\n    \"\"\"\n    A pipeline to sample calls in a given time range and check if there are any compliance breaches. A LLM model is\n    used to identify these breaches by sending entire conversations. The results are persisted in the 'ML_metrics'\n    database from where they can be queried whenever required.\n\n    .. _publish_compliance_breaches:\n\n    Example payload to invoke via slack integrations:\n\n    @charon run publish_compliance_breaches\n\n    .. code-block:: python\n\n        {\n            \"lang\": \"en\",\n            \"template_id\": 100,\n            \"start_date\": \"2022-11-10\",\n            \"end_date\": \"2022-11-11\",\n            \"reported\": false,\n            \"call_quantity\": 500\n        }\n\n    :param lang: The language code of the calls to filter. eg: en, hi, ta, te, etc.\n    :type lang: str\n    :param template_id: The flow template id to filter calls, defaults to \"\"\n    :type template_id: str, optional\n    :param start_date: The start date range to filter calls in YYYY-MM-DD format.\n    :type start_date: str\n    :param end_date: The end date range to filter calls in YYYY-MM-DD format.\n    :type end_date: str\n    :param start_date_offset: Number of days from current date to start querying calls\n    :type start_date_offset: int, optional\n    :param end_date_offset: Number of days from current date to stop querying calls\n    :type end_date_offset: int, optional\n    :param reported: Pick only reported calls, defaults to False\n    :type reported: bool\n    :param call_quantity: Number of calls to sample, defaults to 1000\n    :type call_quantity: int, optional\n    :param notify: A comma separated list of slack ids: \"@apples, @orange.fruit\" etc, defaults to \"\"\n    :type notify: str, optional\n    :param channel: The slack channel to send the notification, defaults to \"\"\n    :type channel: str, optional\n    :param slack_thread: The slack thread to send the notification, defaults to \"\"\n    :type slack_thread: str, optional\n\n    \"\"\"\n\n    calls = fetch_calls_op(\n        lang=lang,\n        start_date=start_date,\n        end_date=end_date,\n        call_quantity=call_quantity,\n        start_date_offset=start_date_offset,\n        end_date_offset=end_date_offset,\n        template_id=template_id,\n        reported=reported,\n        client_id=\"\",\n        remove_empty_audios=False,\n    )\n\n    compliance_breach_report = identify_compliance_breaches_llm_op(\n        s3_file_path=calls.output\n    )\n\n    push_to_postgres = push_compliance_report_to_postgres_op(\n        s3_file_path=compliance_breach_report.output\n    )\n\n    with kfp.dsl.Condition(notify != \"\", \"notify\").after(push_to_postgres):\n        notification_text = f\" Tried generating report for {call_quantity} calls. Among these {push_to_postgres.output} had compliance breaches\"\n        code_block = f\"aws s3 cp {compliance_breach_report.output} .\"\n\n        task_no_cache = slack_notification_op(\n            notification_text,\n            cc=notify,\n            channel=channel,\n            code_block=code_block,\n            thread_id=slack_thread,\n        )\n        task_no_cache.execution_options.caching_strategy.max_cache_staleness = (\n            \"P0D\"  # disables caching\n        )\n\n\n__all__ = [\"publish_compliance_breaches\"]\n"
  },
  {
    "repo": "biswaroop1547/skit-pipelines",
    "file_path": "skit_pipelines/pipelines/retrain_slu/__init__.py",
    "raw_url": "https://raw.githubusercontent.com/biswaroop1547/skit-pipelines/main/skit_pipelines/pipelines/retrain_slu/__init__.py",
    "content": "import kfp\n\nfrom skit_pipelines import constants as pipeline_constants\nfrom skit_pipelines.components import (\n    create_mr_op,\n    download_csv_from_s3_op,\n    download_yaml_op,\n    fetch_tagged_dataset_op,\n    file_contents_to_markdown_s3_op,\n    retrain_slu_from_repo_op,\n    slack_notification_op,\n    upload2s3_op,\n)\n\nUTTERANCES = pipeline_constants.UTTERANCES\nINTENT_Y = pipeline_constants.INTENT_Y\nBUCKET = pipeline_constants.BUCKET\nCSV_FILE = pipeline_constants.CSV_FILE\nCPU_NODE_LABEL = pipeline_constants.CPU_NODE_LABEL\nGPU_NODE_LABEL = pipeline_constants.GPU_NODE_LABEL\nNODESELECTOR_LABEL = pipeline_constants.POD_NODE_SELECTOR_LABEL\n\n\n@kfp.dsl.pipeline(\n    name=\"SLU retraining Pipeline\",\n    description=\"Retrains an existing SLU model.\",\n)\ndef retrain_slu(\n    *,\n    repo_name: str,\n    repo_branch: str = \"master\",\n    job_ids: str = \"\",\n    dataset_path: str = \"\",\n    custom_test_dataset_path: str = \"\",\n    labelstudio_project_ids: str = \"\",\n    job_start_date: str = \"\",\n    job_end_date: str = \"\",\n    remove_intents: str = \"\",\n    alias_yaml_path: str = \"\",\n    initial_training: bool = False,\n    use_previous_dataset: bool = True,\n    epochs: int = 10,\n    train_split_percent: int = 85,\n    stratify: bool = False,\n    target_mr_branch: str = \"sandbox\",\n    notify: str = \"\",\n    channel: str = \"\",\n    slack_thread: str = \"\",\n    core_slu_repo_name: str = \"core-slu-service\",\n    core_slu_repo_branch: str = \"master\",\n    customization_repo_name: str = \"customization\",\n    customization_repo_branch: str = \"master\",\n):\n    \"\"\"\n    A pipeline to retrain an existing SLU model.\n\n    .. _p_retrain_slu:\n\n    Example payload to invoke via slack integrations:\n\n    A minimal example:\n\n        @charon run retrain_slu\n\n        .. code-block:: python\n\n            {\n                \"repo_name\": \"slu_repo_name\",\n                \"labelstudio_project_ids\": \"10,13\"\n            }\n\n\n    A full available parameters example:\n\n        @charon run retrain_slu\n\n        .. code-block:: python\n\n            {\n                \"repo_name\": \"slu_repo_name\",\n                \"repo_branch\": \"master\",\n                \"dataset_path\": \"s3://bucket-name/path1/to1/data1.csv,s3://bucket-name/path2/to2/data2.csv\",\n                \"job_ids\": \"4011,4012\",\n                \"labelstudio_project_ids\": \"10,13\",\n                \"job_start_date\": \"2022-08-01\",\n                \"job_end_date\": \"2022-09-19\",\n                \"remove_intents\": \"_confirm_,_oos_,audio_speech_unclear,ood\",\n                \"alias_yaml_path\": \"intents/oppo/alias.yaml\",\n                \"use_previous_dataset\": True,\n                \"train_split_percent\": 85,\n                \"stratify\": False,\n                \"epochs\": 10,\n            }\n\n\n    Training an SLU for first time example:\n\n        @charon run retrain_slu\n\n        .. code-block:: python\n\n            {\n                \"repo_name\": \"slu_repo_name\",\n                \"repo_branch\": \"master\",\n                \"labelstudio_project_ids\": \"10,13\",\n                \"initial_training\": True\n            }\n\n\n    :param repo_name: SLU repository name under /vernacularai/ai/clients org in gitlab.\n    :type repo_name: str\n\n    :param repo_branch: The branch name in the SLU repository one wants to use, defaults to master.\n    :type repo_name: str, optional\n\n    :param dataset_path: The S3 URI or the S3 key for the tagged dataset (can be multiple - comma separated).\n    :type dataset_path: str, optional\n\n    :param custom_test_dataset_path: The S3 URI or the S3 key for the tagged dataset to be used for model evaluation (can be multiple - comma separated).\n    :type custom_test_dataset_path: str, optional\n\n    :param job_ids: The job ids as per tog. Optional if labestudio_project_ids is provided.\n    :type job_ids: str\n\n    :param labelstudio_project_ids: The labelstudio project id (this is a number) since this is optional, defaults to \"\".\n    :type labelstudio_project_ids: str\n\n    :param job_start_date: The start date range (YYYY-MM-DD) to filter tagged data.\n    :type job_start_date: str, optional\n\n    :param job_end_date: The end date range (YYYY-MM-DD) to filter tagged data\n    :type job_end_date: str, optional\n\n    :param remove_intents: Comma separated list of intents to remove from dataset while training.\n    :type remove_intents: str, optional\n\n    :param alias_yaml_path: eevee's intent_report alias.yaml, refer docs `here <https://skit-ai.github.io/eevee/metrics/intents.html#aliasing>`_ . Upload your yaml to eevee-yamls repository `here <https://github.com/skit-ai/eevee-yamls>`_ & pass the relative path of the yaml from base of the repository.\n    :type alias_yaml_path: str, optional\n\n    :param initial_training: Set to true only if you're training a model for the first time, defaults to False.\n    :type initial_training: bool, optional\n\n    :param use_previous_dataset: Before retraining combines new dataset with last dataset the model was trained on, defaults to True.\n    :type use_previous_dataset: bool, optional\n\n    :param train_split_percent: Percentage of new data one should train the model on, defaults to 85.\n    :type train_split_percent: int, optional\n\n    :param stratify: For stratified splitting of dataset into train and test set, defaults to False.\n    :type stratify: bool, optional\n\n    :param core_slu_repo_name: Name of repository for core slu service. Defaults to core-slu-service\n    :type core_slu_repo_name: str, optional\n\n    :param core_slu_repo_branch: Branch to check out for core slu repository. Defaults to master\n    :type core_slu_repo_branch: str, optional\n\n    :param customization_repo_name: Name of repository for customization service. Defaults to customization\n    :type customization_repo_name: str, optional\n\n    :param customization_repo_branch: Branch to check out for customization service repository. Defaults to master\n    :type customization_repo_branch: str, optional\n\n    :param target_mr_branch: Target branch against which the MR will be created. Defaults to sandbox\n    :type target_mr_branch: str, optional\n\n    :param notify: Whether to send a slack notification, defaults to \"\"\n    :type notify: str, optional\n\n    :param channel: The slack channel to send the notification, defaults to \"\"\n    :type channel: str, optional\n\n    :param slack_thread: The slack thread to send the notification, defaults to \"\"\n    :type slack_thread: str, optional\n\n    \"\"\"\n\n    tagged_s3_data_op = download_csv_from_s3_op(storage_path=dataset_path)\n    tagged_s3_data_op.execution_options.caching_strategy.max_cache_staleness = (\n        \"P0D\"  # disables caching\n    )\n\n    tagged_job_data_op = fetch_tagged_dataset_op(\n        job_id=job_ids,\n        project_id=labelstudio_project_ids,\n        task_type=\"conversation\",\n        timezone=\"Asia/Kolkata\",\n        start_date=job_start_date,\n        end_date=job_end_date,\n        empty_possible=True,\n    )\n    tagged_job_data_op.execution_options.caching_strategy.max_cache_staleness = (\n        \"P0D\"  # disables caching\n    )\n\n    custom_test_tagged_s3_data_op = download_csv_from_s3_op(\n        storage_path=custom_test_dataset_path, empty_possible=True\n    )\n    custom_test_tagged_s3_data_op.execution_options.caching_strategy.max_cache_staleness = (\n        \"P0D\"  # disables caching\n    )\n\n    downloaded_alias_yaml_op = download_yaml_op(\n        git_host_name=pipeline_constants.GITLAB,\n        yaml_path=alias_yaml_path,\n    )\n\n    validate_training_setup_op = retrain_slu_from_repo_op(\n        tagged_s3_data_op.outputs[\"output\"],\n        custom_test_tagged_s3_data_op.outputs[\"output\"],\n        tagged_job_data_op.outputs[\"output\"],\n        downloaded_alias_yaml_op.outputs[\"output\"],\n        bucket=BUCKET,\n        repo_name=repo_name,\n        branch=repo_branch,\n        remove_intents=remove_intents,\n        use_previous_dataset=use_previous_dataset,\n        train_split_percent=train_split_percent,\n        stratify=stratify,\n        epochs=epochs,\n        initial_training=initial_training,\n        labelstudio_project_ids=labelstudio_project_ids,\n        s3_paths=dataset_path,\n        validate_setup=True,\n        customization_repo_name=customization_repo_name,\n        customization_repo_branch=customization_repo_branch,\n        core_slu_repo_name=core_slu_repo_name,\n        core_slu_repo_branch=core_slu_repo_branch,\n    ).set_ephemeral_storage_limit(\"20G\")\n    validate_training_setup_op.display_name = \"Validate Training Setup\"\n\n    retrained_op = retrain_slu_from_repo_op(\n        tagged_s3_data_op.outputs[\"output\"],\n        custom_test_tagged_s3_data_op.outputs[\"output\"],\n        tagged_job_data_op.outputs[\"output\"],\n        downloaded_alias_yaml_op.outputs[\"output\"],\n        bucket=BUCKET,\n        repo_name=repo_name,\n        branch=repo_branch,\n        remove_intents=remove_intents,\n        use_previous_dataset=use_previous_dataset,\n        train_split_percent=train_split_percent,\n        stratify=stratify,\n        epochs=epochs,\n        initial_training=initial_training,\n        labelstudio_project_ids=labelstudio_project_ids,\n        s3_paths=dataset_path,\n        customization_repo_name=customization_repo_name,\n        customization_repo_branch=customization_repo_branch,\n        core_slu_repo_name=core_slu_repo_name,\n        core_slu_repo_branch=core_slu_repo_branch,\n    ).after(validate_training_setup_op)\n    retrained_op.set_gpu_limit(1).add_node_selector_constraint(\n        label_name=NODESELECTOR_LABEL, value=GPU_NODE_LABEL\n    )\n\n    comparison_upload_cf = upload2s3_op(\n        path_on_disk=retrained_op.outputs[\"comparison_classification_report\"],\n        reference=repo_name,\n        file_type=\"comparison_classification_report\",\n        bucket=BUCKET,\n        ext=CSV_FILE,\n    )\n    comparison_upload_cf.execution_options.caching_strategy.max_cache_staleness = (\n        \"P0D\"  # disables caching\n    )\n    comparison_classification_report_markdown_file_op = (\n        file_contents_to_markdown_s3_op(\n            ext=CSV_FILE,\n            path_on_disk=retrained_op.outputs[\"comparison_classification_report\"],\n            file_title=\"## Comparison Classification Report (latest,prod)\",\n        )\n    )\n\n    comparison_upload_cm = upload2s3_op(\n        path_on_disk=retrained_op.outputs[\"comparison_confusion_matrix\"],\n        reference=repo_name,\n        file_type=\"comparison_confusion_matrix\",\n        bucket=BUCKET,\n        ext=CSV_FILE,\n    )\n    comparison_confusion_report_markdown_file_op = (\n        file_contents_to_markdown_s3_op(\n            ext=CSV_FILE,\n            path_on_disk=retrained_op.outputs[\"comparison_confusion_matrix\"],\n            file_title=\"## Comparison Confusion Matrix (latest, prod)\",\n        )\n    )\n\n    mr_response_op = create_mr_op(\n        git_host_name=pipeline_constants.GITLAB,\n        repo_name=repo_name,\n        project_path=pipeline_constants.GITLAB_SLU_PROJECT_CONFIG_PATH,\n        target_branch=target_mr_branch,\n        source_branch=retrained_op.outputs[\"output\"],\n        mr_title=\"Auto retrained changes\",\n        s3_description_paths=f\"{comparison_classification_report_markdown_file_op.output}, {comparison_confusion_report_markdown_file_op.output}\",\n    )\n\n    with kfp.dsl.Condition(notify != \"\", \"notify\").after(retrained_op, mr_response_op):\n        notification_text = f\"Finished training {repo_name} SLU, please review <{mr_response_op.output}|this MR>\"\n        task_no_cache = slack_notification_op(\n            notification_text,\n            channel=channel,\n            cc=notify,\n            thread_id=slack_thread,\n        )\n        task_no_cache.execution_options.caching_strategy.max_cache_staleness = (\n            \"P0D\"  # disables caching\n        )\n\n    with kfp.dsl.Condition(notify != \"\", name=\"slack_notify\").after(\n        comparison_upload_cf, comparison_upload_cm\n    ):\n        notification_text = f\"Here's the IRR report.\"\n        code_block = f\"aws s3 cp {comparison_upload_cf.output} .\"\n        irr_notif = slack_notification_op(\n            notification_text,\n            channel=channel,\n            cc=notify,\n            code_block=code_block,\n            thread_id=slack_thread,\n        )\n        irr_notif.execution_options.caching_strategy.max_cache_staleness = (\n            \"P0D\"  # disables caching\n        )\n\n        notification_text = f\"Here's the confusion matrix.\"\n        code_block = f\"aws s3 cp {comparison_upload_cm.output} .\"\n        cm_notif = slack_notification_op(\n            notification_text,\n            channel=channel,\n            cc=notify,\n            code_block=code_block,\n            thread_id=slack_thread,\n        )\n        cm_notif.execution_options.caching_strategy.max_cache_staleness = (\n            \"P0D\"  # disables caching\n        )\n\n\n__all__ = [\"retrain_slu\"]\n"
  },
  {
    "repo": "biswaroop1547/skit-pipelines",
    "file_path": "skit_pipelines/pipelines/retrain_slu_old/__init__.py",
    "raw_url": "https://raw.githubusercontent.com/biswaroop1547/skit-pipelines/main/skit_pipelines/pipelines/retrain_slu_old/__init__.py",
    "content": "import kfp\n\nfrom skit_pipelines import constants as pipeline_constants\nfrom skit_pipelines.components import (\n    create_mr_op,\n    download_csv_from_s3_op,\n    download_repo_op,\n    download_yaml_op,\n    fetch_tagged_dataset_op,\n    file_contents_to_markdown_s3_op,\n    retrain_slu_from_repo_op_old,\n    slack_notification_op,\n    upload2s3_op,\n)\n\nUTTERANCES = pipeline_constants.UTTERANCES\nINTENT_Y = pipeline_constants.INTENT_Y\nBUCKET = pipeline_constants.BUCKET\nCSV_FILE = pipeline_constants.CSV_FILE\nCPU_NODE_LABEL = pipeline_constants.CPU_NODE_LABEL\nGPU_NODE_LABEL = pipeline_constants.GPU_NODE_LABEL\nNODESELECTOR_LABEL = pipeline_constants.POD_NODE_SELECTOR_LABEL\n\n\n@kfp.dsl.pipeline(\n    name=\"SLU retraining Pipeline\",\n    description=\"Retrains an existing SLU model.\",\n)\ndef retrain_slu_old(\n    *,\n    repo_name: str,\n    repo_branch: str = \"master\",\n    job_ids: str = \"\",\n    dataset_path: str = \"\",\n    labelstudio_project_ids: str = \"\",\n    job_start_date: str = \"\",\n    job_end_date: str = \"\",\n    remove_intents: str = \"\",\n    alias_yaml_path: str = \"\",\n    initial_training: bool = False,\n    use_previous_dataset: bool = True,\n    epochs: int = 10,\n    train_split_percent: int = 85,\n    stratify: bool = False,\n    target_mr_branch: str = \"sandbox\",\n    notify: str = \"\",\n    channel: str = \"\",\n    slack_thread: str = \"\",\n    customization_repo_name: str = \"slu-customization\",\n    customization_repo_branch: str = \"master\",\n):\n    \"\"\"\n    A pipeline to retrain an existing SLU model.\n\n    .. _p_retrain_slu:\n\n    Example payload to invoke via slack integrations:\n\n    A minimal example:\n\n        @charon run retrain_slu\n\n        .. code-block:: python\n\n            {\n                \"repo_name\": \"slu_repo_name\",\n                \"labelstudio_project_ids\": \"10,13\"\n            }\n\n\n    A full available parameters example:\n\n        @charon run retrain_slu\n\n        .. code-block:: python\n\n            {\n                \"repo_name\": \"slu_repo_name\",\n                \"repo_branch\": \"master\",\n                \"dataset_path\": \"s3://bucket-name/path1/to1/data1.csv,s3://bucket-name/path2/to2/data2.csv\",\n                \"job_ids\": \"4011,4012\",\n                \"labelstudio_project_ids\": \"10,13\",\n                \"job_start_date\": \"2022-08-01\",\n                \"job_end_date\": \"2022-09-19\",\n                \"remove_intents\": \"_confirm_,_oos_,audio_speech_unclear,ood\",\n                \"alias_yaml_path\": \"intents/oppo/alias.yaml\",\n                \"use_previous_dataset\": True,\n                \"train_split_percent\": 85,\n                \"stratify\": False,\n                \"epochs\": 10,\n            }\n\n\n    Training an SLU for first time example:\n\n        @charon run retrain_slu\n\n        .. code-block:: python\n\n            {\n                \"repo_name\": \"slu_repo_name\",\n                \"repo_branch\": \"master\",\n                \"labelstudio_project_ids\": \"10,13\",\n                \"initial_training\": True\n            }\n\n\n    :param repo_name: SLU repository name under /vernacularai/ai/clients org in gitlab.\n    :type repo_name: str\n\n    :param repo_branch: The branch name in the SLU repository one wants to use, defaults to master.\n    :type repo_name: str, optional\n\n    :param dataset_path: The S3 URI or the S3 key for the tagged dataset (can be multiple - comma separated).\n    :type dataset_path: str, optional\n\n    :param job_ids: The job ids as per tog. Optional if labestudio_project_ids is provided.\n    :type job_ids: str\n\n    :param labelstudio_project_ids: The labelstudio project id (this is a number) since this is optional, defaults to \"\".\n    :type labelstudio_project_ids: str\n\n    :param epochs: Number of epchs to train the model, defaults to 10\n    :type epochs: int, optional\n\n    :param job_start_date: The start date range (YYYY-MM-DD) to filter tagged data.\n    :type job_start_date: str, optional\n\n    :param job_end_date: The end date range (YYYY-MM-DD) to filter tagged data\n    :type job_end_date: str, optional\n\n    :param remove_intents: Comma separated list of intents to remove from dataset while training.\n    :type remove_intents: str, optional\n\n    :param alias_yaml_path: eevee's intent_report alias.yaml, refer docs `here <https://skit-ai.github.io/eevee/metrics/intents.html#aliasing>`_ . Upload your yaml to eevee-yamls repository `here <https://github.com/skit-ai/eevee-yamls>`_ & pass the relative path of the yaml from base of the repository.\n    :type alias_yaml_path: str, optional\n\n    :param initial_training: Set to true only if you're training a model for the first time, defaults to False.\n    :type initial_training: bool, optional\n\n    :param use_previous_dataset: Before retraining combines new dataset with last dataset the model was trained on, defaults to True.\n    :type use_previous_dataset: bool, optional\n\n    :param train_split_percent: Percentage of new data one should train the model on, defaults to 85.\n    :type train_split_percent: int, optional\n\n    :param stratify: For stratified splitting of dataset into train and test set, defaults to False.\n    :type stratify: bool, optional\n\n    :param notify: Whether to send a slack notification, defaults to \"\"\n    :type notify: str, optional\n\n    :param channel: The slack channel to send the notification, defaults to \"\"\n    :type channel: str, optional\n\n    :param slack_thread: The slack thread to send the notification, defaults to \"\"\n    :type slack_thread: str, optional\n\n    \"\"\"\n\n    tagged_s3_data_op = download_csv_from_s3_op(storage_path=dataset_path)\n    tagged_s3_data_op.execution_options.caching_strategy.max_cache_staleness = (\n        \"P0D\"  # disables caching\n    )\n\n    tagged_job_data_op = fetch_tagged_dataset_op(\n        job_id=job_ids,\n        project_id=labelstudio_project_ids,\n        task_type=\"conversation\",\n        timezone=\"Asia/Kolkata\",\n        start_date=job_start_date,\n        end_date=job_end_date,\n        empty_possible=True,\n    )\n    tagged_job_data_op.execution_options.caching_strategy.max_cache_staleness = (\n        \"P0D\"  # disables caching\n    )\n\n    downloaded_repo_op = download_repo_op(\n        git_host_name=pipeline_constants.GITLAB,\n        repo_name=repo_name,\n        project_path=pipeline_constants.GITLAB_SLU_PROJECT_PATH,\n    )\n\n    downloaded_repo_op.execution_options.caching_strategy.max_cache_staleness = (\n        \"P0D\"  # disables caching\n    )\n\n    # downloaded_customization_repo_op = download_repo_op(\n    #     repo_name=customization_repo_name,\n    # )\n    # downloaded_customization_repo_op.display_name = \"Download SLU customization repo\"\n\n    # downloaded_customization_repo_op.execution_options.caching_strategy.max_cache_staleness = (\n    #     \"P0D\"  # disables caching\n    # )\n\n    downloaded_alias_yaml_op = download_yaml_op(\n        git_host_name=pipeline_constants.GITLAB,\n        yaml_path=alias_yaml_path,\n    )\n\n    validate_training_setup_op = retrain_slu_from_repo_op_old(\n        tagged_s3_data_op.outputs[\"output\"],\n        tagged_job_data_op.outputs[\"output\"],\n        downloaded_repo_op.outputs[\"repo\"],\n        downloaded_alias_yaml_op.outputs[\"output\"],\n        bucket=BUCKET,\n        repo_name=repo_name,\n        branch=repo_branch,\n        remove_intents=remove_intents,\n        use_previous_dataset=use_previous_dataset,\n        train_split_percent=train_split_percent,\n        stratify=stratify,\n        epochs=epochs,\n        initial_training=initial_training,\n        job_ids=job_ids,\n        labelstudio_project_ids=labelstudio_project_ids,\n        s3_paths=dataset_path,\n        validate_setup=True,\n        customization_repo_name=customization_repo_name,\n        customization_repo_branch=customization_repo_branch,\n    )\n    validate_training_setup_op.display_name = \"Validate Training Setup\"\n\n    retrained_op = retrain_slu_from_repo_op_old(\n        tagged_s3_data_op.outputs[\"output\"],\n        tagged_job_data_op.outputs[\"output\"],\n        downloaded_repo_op.outputs[\"repo\"],\n        downloaded_alias_yaml_op.outputs[\"output\"],\n        bucket=BUCKET,\n        repo_name=repo_name,\n        branch=repo_branch,\n        remove_intents=remove_intents,\n        use_previous_dataset=use_previous_dataset,\n        train_split_percent=train_split_percent,\n        stratify=stratify,\n        epochs=epochs,\n        initial_training=initial_training,\n        job_ids=job_ids,\n        labelstudio_project_ids=labelstudio_project_ids,\n        s3_paths=dataset_path,\n        customization_repo_name=customization_repo_name,\n        customization_repo_branch=customization_repo_branch,\n    ).after(validate_training_setup_op)\n    retrained_op.set_gpu_limit(1).add_node_selector_constraint(\n        label_name=NODESELECTOR_LABEL, value=GPU_NODE_LABEL\n    )\n\n    # upload test set metrics.\n    upload_cf = upload2s3_op(\n        path_on_disk=retrained_op.outputs[\"output_classification_report\"],\n        reference=repo_name,\n        file_type=\"test_classification_report\",\n        bucket=BUCKET,\n        ext=CSV_FILE,\n    )\n\n    upload_cm = upload2s3_op(\n        path_on_disk=retrained_op.outputs[\"output_confusion_matrix\"],\n        reference=repo_name,\n        file_type=\"test_confusion_matrix\",\n        bucket=BUCKET,\n        ext=CSV_FILE,\n    )\n    upload_cm.execution_options.caching_strategy.max_cache_staleness = (\n        \"P0D\"  # disables caching\n    )\n\n    classification_report_markdown_file_op = file_contents_to_markdown_s3_op(\n        ext=CSV_FILE,\n        path_on_disk=retrained_op.outputs[\"output_classification_report\"],\n        file_title=\"## Classification Report\",\n    )\n\n    confusion_matrix_markdown_file_op = file_contents_to_markdown_s3_op(\n        ext=CSV_FILE,\n        path_on_disk=retrained_op.outputs[\"output_confusion_matrix\"],\n        file_title=\"## Confusion Matrix\",\n    )\n\n    mr_response_op = create_mr_op(\n        git_host_name=pipeline_constants.GITLAB,\n        repo_name=repo_name,\n        project_path=pipeline_constants.GITLAB_SLU_PROJECT_PATH,\n        target_branch=target_mr_branch,\n        source_branch=retrained_op.outputs[\"output\"],\n        mr_title=\"Auto retrained changes\",\n        s3_description_paths=f\"{classification_report_markdown_file_op.output},{confusion_matrix_markdown_file_op.output}\",\n    )\n\n    with kfp.dsl.Condition(notify != \"\", \"notify\").after(retrained_op, mr_response_op):\n        notification_text = f\"Finished training {repo_name} SLU, please review <{mr_response_op.output}|this MR>\"\n        task_no_cache = slack_notification_op(\n            notification_text,\n            channel=channel,\n            cc=notify,\n            thread_id=slack_thread,\n        )\n        task_no_cache.execution_options.caching_strategy.max_cache_staleness = (\n            \"P0D\"  # disables caching\n        )\n\n    with kfp.dsl.Condition(notify != \"\", name=\"slack_notify\").after(\n        upload_cf, upload_cm\n    ):\n        notification_text = f\"Here's the IRR report.\"\n        code_block = f\"aws s3 cp {upload_cf.output} .\"\n        irr_notif = slack_notification_op(\n            notification_text,\n            channel=channel,\n            cc=notify,\n            code_block=code_block,\n            thread_id=slack_thread,\n        )\n        irr_notif.execution_options.caching_strategy.max_cache_staleness = (\n            \"P0D\"  # disables caching\n        )\n\n        notification_text = f\"Here's the confusion matrix.\"\n        code_block = f\"aws s3 cp {upload_cm.output} .\"\n        cm_notif = slack_notification_op(\n            notification_text,\n            channel=channel,\n            cc=notify,\n            code_block=code_block,\n            thread_id=slack_thread,\n        )\n        cm_notif.execution_options.caching_strategy.max_cache_staleness = (\n            \"P0D\"  # disables caching\n        )\n\n\n__all__ = [\"retrain_slu_old\"]\n"
  },
  {
    "repo": "biswaroop1547/skit-pipelines",
    "file_path": "skit_pipelines/pipelines/transcription_pipeline/__init__.py",
    "raw_url": "https://raw.githubusercontent.com/biswaroop1547/skit-pipelines/main/skit_pipelines/pipelines/transcription_pipeline/__init__.py",
    "content": "import kfp\nimport pandas as pd\n\nfrom skit_pipelines import constants as pipeline_constants\nfrom skit_pipelines.components import (\n    audio_transcription_op,\n    download_audio_wavs_op,\n    download_csv_from_s3_op,\n    download_file_from_s3_op,\n    overlay_transcription_csv_op,\n    re_presign_s3_urls_op,\n    slack_notification_op,\n    upload2s3_op,\n)\n\nBUCKET = pipeline_constants.BUCKET\n\n\n@kfp.dsl.pipeline(\n    name=\"Transcription Pipeline\",\n    description=\"Transcribe the audio data using the mentioned ASR models\",\n)\ndef transcription_pipeline(\n    *,\n    data_s3_path: str,\n    config_s3_path: str,\n    audio_sample_rate: str = \"8k\",\n    audio_download_workers: int = 30,\n    transcription_concurrency: int = 8,\n    notify: str = \"\",\n    channel: str = \"\",\n    slack_thread: str = \"\",\n):\n\n    \"\"\"\n    A pipeline to transcribe the audio files present in a dataset using different ASRs.\n\n    .. _p_transcription_pipeline:\n\n    Example payload to invoke via slack integrations:\n\n        @charon run transcription_pipeline\n\n        .. code-block:: python\n\n            {\n\n            }\n\n    :param data_s3_path: S3 path of the data in CSV\n    :type data_s3_path: str\n    :param config_s3_path: the config yaml to be used by blaze. Refer to (https://github.com/skit-ai/blaze#config) for more info.\n    :type config_s3_path: str\n    :param audio_sample_rate: audio sample rate / frequency of output audios. (default \"8k\").\n    :type audio_sample_rate: str\n    :param audio_download_workers: maximum workers while downloading the audios (default 30).\n    :type audio_download_workers: int\n    :param transcription_concurrency: maximum workers while transcribing the audios (default 8).\n    :type transcription_concurrency: int\n\n    \"\"\"\n    # Download CSV files with audio\n    original_data_op = download_csv_from_s3_op(storage_path=data_s3_path)\n    original_data_op.execution_options.caching_strategy.max_cache_staleness = (\n        \"P0D\"  # disables caching\n    )\n\n    config_data_op = download_file_from_s3_op(storage_path=config_s3_path)\n    config_data_op.execution_options.caching_strategy.max_cache_staleness = (\n        \"P0D\"  # disables caching\n    )\n\n    # re-presign the s3 links present in .csv, so that they are accessible\n    # does presigning again only if the links are expired\n    re_presigned_op = re_presign_s3_urls_op(original_data_op.outputs[\"output\"])\n    re_presigned_op.execution_options.caching_strategy.max_cache_staleness = (\"P0D\")\n\n    # Download audio files from CSV\n    audio_wavs_op = download_audio_wavs_op(\n        re_presigned_op.outputs[\"output\"], audio_sample_rate, audio_download_workers\n    )\n\n    # Transcribing\n    transcribed_sqlite_op = audio_transcription_op(\n        audio_wavs_op.outputs[\"output\"],\n        config_data_op.outputs[\"output\"],\n        concurrency=transcription_concurrency,\n    )\n\n    # overlay the original csv (original_data_op) with the new transcriptions (transcribed_sqlite_op)\n    overlayed_data_op = overlay_transcription_csv_op(\n        transcribed_sqlite_op.outputs[\"output\"], original_data_op.outputs[\"output\"]\n    )\n\n    # Returning S3 path\n    audio_s3_path = upload2s3_op(\n        path_on_disk=overlayed_data_op.outputs[\"output\"], bucket=BUCKET, ext=\".csv\"\n    )\n\n    with kfp.dsl.Condition(notify != \"\", name=\"slack_notify\").after(\n        audio_s3_path\n    ) as audio_check:\n        notification_text = f\"Here's the CSV after transcription.\"\n        code_block = f\"aws s3 cp {audio_s3_path.output} .\"\n        audio_notif = slack_notification_op(\n            notification_text,\n            channel=channel,\n            cc=notify,\n            code_block=code_block,\n            thread_id=slack_thread,\n        )\n        audio_notif.execution_options.caching_strategy.max_cache_staleness = (\n            \"P0D\"  # disables caching\n        )\n\n\n__all__ = [\"transcription_pipeline\"]\n"
  },
  {
    "repo": "Taha-Cakir/Kubeflow-Pipelines-Deployment-GCP",
    "file_path": "model-deployment-kubeflow.py",
    "raw_url": "https://raw.githubusercontent.com/Taha-Cakir/Kubeflow-Pipelines-Deployment-GCP/main/model-deployment-kubeflow.py",
    "content": "from kfp.v2 import dsl\nfrom kfp.v2.dsl import (Input,Output,Metrics,component,Model)\nfrom google.cloud.aiplatform import pipeline_jobs\nfrom typing import NamedTuple\nfrom kfp.v2 import compiler\n\n\n@component(\npackages_to_install=[\"gcsfs\",\"pandas\",\"google-cloud-storage\"]\n)\ndef validate_input_ds(filename:str)-> NamedTuple(\"output\", [(\"input_validation\", str)]):\n\n    import logging\n    from google.cloud import storage\n    import pandas as pd\n\n    logging.basicConfig(level=logging.INFO)\n\n    logging.info(f\"Reading file: {filename}\")\n    df = pd.read_csv(filename)\n    expected_num_cols = 26\n    num_cols = len(df.columns)\n\n    logging.info(f\"Number of columns: {num_cols}\")\n    \n    input_validation=\"true\"\n    \n    if num_cols != expected_num_cols:\n        input_validation=\"false\"\n        \n    expected_col_names = ['destination', 'passanger', 'weather', 'temperature', 'time', 'coupon',\n                               'expiration', 'gender', 'age', 'maritalStatus', 'has_children',\n                               'education', 'occupation', 'income', 'car', 'Bar', 'CoffeeHouse',\n                               'CarryAway', 'RestaurantLessThan20', 'Restaurant20To50',\n                               'toCoupon_GEQ5min', 'toCoupon_GEQ15min', 'toCoupon_GEQ25min',\n                               'direction_same', 'direction_opp', 'Y']\n\n    if set(df.columns) != set(expected_col_names):\n        input_validation=\"false\"\n\n    return (input_validation,)\n\n\n@component(\npackages_to_install=[\"google-cloud-aiplatform\",\"gcsfs\",\"xgboost\",\"category_encoders\",\"imblearn\",\"pandas\",\"google-cloud-storage\"]\n)\ndef custom_training_job_component(\n    max_depth:int,\n    learning_rate:float,\n    n_estimators:int,\n    metrics: Output[Metrics]\n)->NamedTuple(\"output\", [(\"model_validation\", str)]):\n    import pandas as pd\n    from sklearn.metrics import precision_score, recall_score, roc_auc_score, accuracy_score\n    from sklearn.model_selection import train_test_split\n    from category_encoders import HashingEncoder\n    from imblearn.over_sampling import SMOTE\n    from xgboost import XGBClassifier\n    from google.cloud import storage\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(\"mlops-cakir-kubeflow-v1\")\n\n    def load_data(file_path):\n        df = pd.read_csv(file_path)\n        return df\n\n    def preprocess_data(df):\n\n        df = df.drop(columns=['car', 'toCoupon_GEQ5min', 'direction_opp'])\n        df = df.fillna(df.mode().iloc[0])\n        df = df.drop_duplicates()\n\n        df_dummy = df.copy()\n        age_list = []\n        for i in df['age']:\n            if i == 'below21':\n                age = '<21'\n            elif i in ['21', '26']:\n                age = '21-30'\n            elif i in ['31', '36']:\n                age = '31-40'\n            elif i in ['41', '46']:\n                age = '41-50'\n            else:\n                age = '>50'\n            age_list.append(age)\n        df_dummy['age'] = age_list\n\n        df_dummy['passanger_destination'] = df_dummy['passanger'].astype(str) + '-' + df_dummy['destination'].astype(str)\n        df_dummy['marital_hasChildren'] = df_dummy['maritalStatus'].astype(str) + '-' + df_dummy['has_children'].astype(str)\n        df_dummy['temperature_weather'] = df_dummy['temperature'].astype(str) + '-' + df_dummy['weather'].astype(str)\n        df_dummy = df_dummy.drop(columns=['passanger', 'destination', 'maritalStatus', 'has_children', 'temperature','weather', 'Y'])\n\n        df_dummy = pd.concat([df_dummy, df['Y']], axis = 1)\n        df_dummy = df_dummy.drop(columns=['gender', 'RestaurantLessThan20'])\n        df_le = df_dummy.replace({\n            'expiration':{'2h': 0, '1d' : 1},\n            'age':{'<21': 0, '21-30': 1, '31-40': 2, '41-50': 3, '>50': 4},\n            'education':{'Some High School': 0, 'High School Graduate': 1, 'Some college - no degree': 2,\n                         'Associates degree': 3, 'Bachelors degree': 4, 'Graduate degree (Masters or Doctorate)': 5},\n            'Bar':{'never': 0, 'less1': 1, '1~3': 2, '4~8': 3, 'gt8': 4},\n            'CoffeeHouse':{'never': 0, 'less1': 1, '1~3': 2, '4~8': 3, 'gt8': 4}, \n            'CarryAway':{'never': 0, 'less1': 1, '1~3': 2, '4~8': 3, 'gt8': 4}, \n            'Restaurant20To50':{'never': 0, 'less1': 1, '1~3': 2, '4~8': 3, 'gt8': 4},\n            'income':{'Less than $12500':0, '$12500 - $24999':1, '$25000 - $37499':2, '$37500 - $49999':3,\n                      '$50000 - $62499':4, '$62500 - $74999':5, '$75000 - $87499':6, '$87500 - $99999':7,\n                      '$100000 or More':8},\n            'time':{'7AM':0, '10AM':1, '2PM':2, '6PM':3, '10PM':4}\n        })\n\n        x = df_le.drop('Y', axis=1)\n        y = df_le.Y\n\n        return x, y\n\n    def train_model(x_train, y_train,max_depth,learning_rate,n_estimators):\n        \n        model = XGBClassifier(\n            max_depth=max_depth,\n            learning_rate=learning_rate,\n            n_estimators=n_estimators,\n            random_state=42,\n            use_label_encoder=False\n        )\n        model.fit(x_train, y_train)\n        return model\n\n    def evaluate_model(model, x_test, y_test, x_sm_train_hashing, y_sm_train):\n        y_pred = model.predict(x_test)\n        y_pred_proba = model.predict_proba(x_test)\n        y_pred_train = model.predict(x_sm_train_hashing)\n        y_pred_train_proba = model.predict_proba(x_sm_train_hashing)\n\n        accuracy = accuracy_score(y_test, y_pred)\n        precision = precision_score(y_test, y_pred)\n        recall = recall_score(y_test, y_pred)\n\n        # roc_auc_train_proba = roc_auc_score(y_sm_train, y_pred_train_proba[:, 1])\n        # roc_auc_test_proba = roc_auc_score(y_test, y_pred_proba[:, 1])\n\n        return accuracy,precision,recall\n\n    def encode_features(x, n_components=27):\n        hashing_ros_enc = HashingEncoder(cols=['passanger_destination', 'marital_hasChildren', 'occupation', 'coupon',\n                                               'temperature_weather'], n_components=n_components).fit(x)\n        x_test_hashing = hashing_ros_enc.transform(x.reset_index(drop=True))\n        return x_test_hashing\n\n    def oversample_data(x_train_hashing, y_train):\n        sm = SMOTE(random_state=42)\n        x_sm_train_hashing, y_sm_train = sm.fit_resample(x_train_hashing, y_train)\n        return x_sm_train_hashing, y_sm_train\n\n    def save_model_artifact(pipeline):\n        artifact_name = 'model.bst'\n        pipeline.save_model(artifact_name)\n        model_artifact = bucket.blob('mlops-recommendation/artifacts/'+artifact_name)\n        model_artifact.upload_from_filename(artifact_name)\n\n    input_file = \"gs://mlops-cakir-kubeflow-v1-kubeflow-v1/mlops-recommendation/in-vehicle-coupon-recommendation.csv\"\n    df = load_data(input_file)\n    x, y = preprocess_data(df)\n\n    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n\n    x_train.fillna(x_train.mode().iloc[0], inplace=True)\n    x_test.fillna(x_train.mode().iloc[0], inplace=True)\n    \n    model_name = 'xgboost'\n    print(\"Training and evaluating\", model_name, \"model:\")\n    x_train_hashing = encode_features(x_train)\n    x_test_hashing = encode_features(x_test)\n    x_sm_train_hashing, y_sm_train = oversample_data(x_train_hashing,y_train)\n\n    pipeline = train_model(x_sm_train_hashing,y_sm_train,max_depth,learning_rate,n_estimators)\n\n    accuracy,precision,recall = evaluate_model(pipeline,x_test_hashing,y_test,x_sm_train_hashing,y_sm_train)\n    metrics.log_metric(\"accurancy\", accuracy)\n    metrics.log_metric(\"precision\", precision)\n    metrics.log_metric(\"recall\", recall)\n    \n    model_validation = \"true\"\n    if accuracy>0.5 and precision>0.5 :\n        save_model_artifact(pipeline)\n        model_validation=\"true\"\n    else :\n        model_validation=\"false\"\n\n    return (model_validation,)\n\n\n\n@component(\n    packages_to_install=[\"google-cloud-aiplatform\"]\n)\ndef model_deployment()-> NamedTuple(\"endpoint\", [(\"endpoint\", str)]):\n    \n    from google.cloud import aiplatform\n    \n    aiplatform.init(project=\"cakir-kubeflow\", location=\"us-central1\", staging_bucket=\"gs://mlops-cakir-kubeflow-v1\")\n    \n    model = aiplatform.Model.upload(\n        display_name=\"mlops-recommendation-model\",\n        artifact_uri=\"gs://mlops-cakir-kubeflow-v1/mlops-recommendation/artifacts/\",\n        serving_container_image_uri = \"us-docker.pkg.dev/vertex-ai/prediction/xgboost-cpu.1-6:latest\",\n        sync=False\n    )\n    \n    DEPLOYED_NAME = \"coupon-model-endpoint\"\n    TRAFFIC_SPLIT = {\"0\": 100}\n    MIN_NODES = 1\n    MAX_NODES = 1\n\n    endpoint = model.deploy(\n        deployed_model_display_name=DEPLOYED_NAME,\n        traffic_split=TRAFFIC_SPLIT,\n        machine_type=\"n1-standard-4\",\n        min_replica_count=MIN_NODES,\n        max_replica_count=MAX_NODES\n    )\n\n\n@dsl.pipeline(\n    pipeline_root=\"gs://mlops-cakir-kubeflow-v1/coupon-pipeline-v1\",\n    name=\"coupon-model-training-pipeline\",\n)\ndef pipeline(\n    project: str = \"cakir-kubeflow\",\n    region: str = \"us-central1\"\n    ):\n    \n    max_depth=5\n    learning_rate=0.2\n    n_estimators=40\n    \n    file_name = \"gs://mlops-cakir-kubeflow-v1/mlops-recommendation/in-vehicle-coupon-recommendation.csv\"\n    input_validation_task = validate_input_ds(file_name)\n    \n    with dsl.Condition(input_validation_task.outputs[\"input_validation\"] == \"true\"):\n        model_training = custom_training_job_component(\n            max_depth=max_depth,\n            learning_rate=learning_rate,\n            n_estimators=n_estimators,\n        ).after(input_validation_task)\n        \n        with dsl.Condition(model_training.outputs[\"model_validation\"] == \"true\"):\n            task_deploy_model = model_deployment().after(model_training)\n\n\nif __name__ == \"__main__\":\n     # Create an argument parser\n    parser = argparse.ArgumentParser(description='Data Drift Script')\n    parser.add_argument('--display_name', type=str, help='pipeline display name')\n    parser.add_argument('--location', type=str, help='region of pipeline')\n\n    # Parse the command-line arguments\n    args = parser.parse_args()\n    location = args.location\n    display_name = args.display_name\n\n    compiler.Compiler().compile(pipeline_func=pipeline,package_path='coupon-pipeline-deploy-v1.json')\n\n    start_pipeline = pipeline_jobs.PipelineJob(\n        display_name=display_name,\n        template_path=\"mlops-recommendation-pipeline-deploy-v1.json\",\n        enable_caching=False,\n        location=args.test,\n    )\n\n    start_pipeline.run()"
  },
  {
    "repo": "kaiomurz/kubeflow-imdb",
    "file_path": "imdb_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/kaiomurz/kubeflow-imdb/main/imdb_pipeline.py",
    "content": "\nimport imp\nimport kfp\nimport kfp.components as comp\n\n\n######################################\n########## IMPORT FUNCTIONS ##########\n######################################\n\nfrom functions.download_and_save_data import download_and_save_data_from_s3\nfrom functions.shuffle import shuffle\nfrom functions.split_to_x_y import split_to_x_y\nfrom functions.encode_target import encode_target\nfrom functions.clean_text import clean_text\nfrom functions.preprocess_text import preprocess_text\nfrom functions.split_to_train_test import split_to_train_test\nfrom functions.create_and_train_model import create_and_train_model\nfrom functions.create_and_save_performance_artifacts import create_and_save_performance_artifacts\n\n# dsl-compile --py imdb_pipeline.py --output imdb_pipeline.yaml\n# https://github.com/kubeflow/pipelines/tree/sdk/release-1.8/samples/core\n\n\n#####################################################\n########## CONVERT FUNCTIONS TO COMPONENTS ##########\n#####################################################\n\nbase_image = \"kaiomurz/kubeflow-imdb:latest\"\n\ndownload_and_save_data_from_s3_op = kfp.components.create_component_from_func(\n    download_and_save_data_from_s3,\n    base_image=base_image\n)\n\nshuffle_op = kfp.components.create_component_from_func(\n    shuffle,\n    base_image=base_image\n)\n\nsplit_to_x_y_op = kfp.components.create_component_from_func(\n    split_to_x_y,\n    base_image=base_image\n)\n\nencode_target_op = kfp.components.create_component_from_func(\n    encode_target,\n    base_image=base_image\n)\n\nclean_text_op = kfp.components.create_component_from_func(\n    clean_text,\n    base_image=base_image\n)\n\npreprocess_text_op = kfp.components.create_component_from_func(\n    preprocess_text,\n    base_image=base_image\n)\n\nsplit_to_train_test_op = kfp.components.create_component_from_func(\n    split_to_train_test,\n    base_image=base_image    \n)\ncreate_and_train_model_op = kfp.components.create_component_from_func(\n    create_and_train_model,\n    base_image=base_image\n)\ncreate_and_save_performance_artifacts_op = kfp.components.create_component_from_func(\n    create_and_save_performance_artifacts,\n    base_image=base_image\n)\n\n# test_pickling_op = kfp.components.create_component_from_func(\n#     test_pickling,\n#     base_image=base_image\n# )\n# pickle_load_test_op = kfp.components.create_component_from_func(\n#     pickle_load_test,\n#     base_image=base_image\n# )\n# split_to_train_test_op = kfp.components.create_component_from_func(\n#     split_to_train_test,\n#     base_image=base_image\n# )\n\n\n\n###########################################\n########## CREATING THE PIPELINE ##########\n###########################################\n\n\n@kfp.dsl.pipeline(\n    name='imdb_pipeline',\n    description='Pipeline to train and serve sentiment analysis model on IMDB dataset'\n)\ndef imdb_pipeline(\n    aws_access_key_id: str,\n    aws_secret_access_key: str,\n    Bucket: str='imdb-kubeflow',\n    Key: str='data.csv',\n    y_heading: str='sentiment',\n    tokenizer_num_words: int=1000,\n    shuffle: bool=True,\n    split: float = 0.8,\n    fraction: float=0.1,\n    num_epochs: int=10\n    ):\n\n    download_and_save_data_from_s3_task = download_and_save_data_from_s3_op(\n        Bucket, \n        Key,\n        aws_access_key_id,\n        aws_secret_access_key\n    )\n\n    shuffle_task = shuffle_op(\n        download_and_save_data_from_s3_task.outputs['data_csv'],\n        shuffle,\n        fraction\n    )\n\n    split_to_x_y_task = split_to_x_y_op(\n        shuffle_task.outputs['out_data_csv'],\n        y_heading\n    )\n\n    encode_target_task = encode_target_op(\n        split_to_x_y_task.outputs['out_y_csv']\n    )\n\n    clean_text_task = clean_text_op(\n        split_to_x_y_task.outputs['out_X_csv']        \n    )\n\n    preprocess_text_task = preprocess_text_op(\n        clean_text_task.outputs['out_X_csv'],\n        tokenizer_num_words\n    )\n    split_to_train_test_task = split_to_train_test_op(\n        preprocess_text_task.outputs['out_X_pkl'],\n        encode_target_task.outputs['out_y_pkl'],\n        split\n    )\n    create_and_train_model_task = create_and_train_model_op(\n        split_to_train_test_task.outputs['out_X_train_pkl'],\n        split_to_train_test_task.outputs['out_y_train_pkl'],\n        tokenizer_num_words,\n        num_epochs\n    )\n    create_and_save_performance_artifacts_task = create_and_save_performance_artifacts_op(\n        create_and_train_model_task.outputs['model_pkl'],\n        split_to_train_test_task.outputs['out_X_test_pkl'],\n        split_to_train_test_task.outputs['out_y_test_pkl']\n    )\n\n    # test_pickling_task = test_pickling_op(\n    #     split_to_train_test_task.outputs['out_X_train_pkl'],\n    #     split_to_train_test_task.outputs['out_y_train_pkl'],\n    #     split_to_train_test_task.outputs['out_X_test_pkl'],\n    #     split_to_train_test_task.outputs['out_y_test_pkl']\n    # )\n\n    # pickle_load_test_task = pickle_load_test_op(\n    #     preprocess_text_task.outputs['out_X_pkl']\n    # )\n    # split_to_train_test_task = split_to_train_test_op(\n    #     download_and_save_data_from_s3_task.outputs['data_csv'],\n    #     split=0.8,\n    #     shuffle=True  \n    # )\n\n    # def pickle_load_test(\n#     in_X_pkl: comp.InputPath('PKL')\n#     ):\n#     import pickle\n#     X = pickle.load(open(in_X_pkl, 'rb'))    \n    \n\n# def test_pickling(\n#     in_X_train_pkl: comp.InputPath('PKL'),\n#     in_y_train_pkl: comp.InputPath('PKL'),\n#     in_X_test_pkl: comp.InputPath('PKL'),    \n#     in_y_test_pkl: comp.InputPath('PKL')\n#     ):\n#     import pickle\n#     import numpy\n\n#     X_train = pickle.load(open(in_X_train_pkl, 'rb'))\n#     y_train = pickle.load(open(in_y_train_pkl, 'rb'))\n#     X_test = pickle.load(open(in_X_test_pkl, 'rb'))\n#     y_test = pickle.load(open(in_y_test_pkl, 'rb'))\n\n#     objs = {\n#         'X train':X_train,\n#         'y train': y_train,\n#         'X test': X_test,\n#         'y test': y_test\n#         }\n\n#     for obj in objs:\n#         print(obj, type(objs[obj]), objs[obj].shape, \"\\n\")\n        \n#         for i in range(3):\n#             print(objs[obj][i], \"\\n\")\n#         print(\"#####################################################, \\n\\n\\n\")    \n"
  },
  {
    "repo": "aakashbajaj/Retinal-OCT-Kubeflow",
    "file_path": "pipelines/e2e_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/aakashbajaj/Retinal-OCT-Kubeflow/master/pipelines/e2e_pipeline.py",
    "content": "import kfp.dsl as dsl\nimport kfp.gcp as gcp\n\n\n@dsl.pipeline(\n  name='Retinal_OCT',\n  description='Retinal OCT detection'\n)\ndef dp_inf_pipe(\n  # Important Parameters on top\n\n  project_id: dsl.PipelineParam = dsl.PipelineParam(name='project-id', value=\"YOUR_PROJECT_ID\"),\n  inp_dir: dsl.PipelineParam = dsl.PipelineParam(name='input-dir', value='GCS_IMAGE_INPDIR_HERE'),\n  out_dir: dsl.PipelineParam = dsl.PipelineParam(name='data-dir', value='GCS_TFRECORD_OUTDIR_HERE'),\n  model_dir: dsl.PipelineParam = dsl.PipelineParam(name='model-dir', value='MODEL_CHECKPOINT_DIR_HERE'),\n  save_model_dir: dsl.PipelineParam = dsl.PipelineParam(name='save-model-dir', value=\"DIR_TO_EXPORT_SAVED_MODEL\"),\n  model_name: dsl.PipelineParam = dsl.PipelineParam(name='model-name', value='MODEL_NAME_FOR_SERVING (No spaces or underscores)'),\n  epochs: dsl.PipelineParam = dsl.PipelineParam(name='train-num-epochs', value=1),\n  batch_size: dsl.PipelineParam = dsl.PipelineParam(name='batch-size-train', value=32),\n\n  train_flag: dsl.PipelineParam = dsl.PipelineParam(name='train-flag', value=1),\n  dataprep_flag: dsl.PipelineParam = dsl.PipelineParam(name='dataprep-flag', value=0),\n\n  num_shards: dsl.PipelineParam = dsl.PipelineParam(name='num-shards', value=5),\n  split_flag: dsl.PipelineParam = dsl.PipelineParam(name='split-flag', value=2),\n  train_split: dsl.PipelineParam = dsl.PipelineParam(name='train-split', value=0.8),\n  seed: dsl.PipelineParam = dsl.PipelineParam(name='seed', value=123),\n  height: dsl.PipelineParam = dsl.PipelineParam(name='height', value=256),\n  width: dsl.PipelineParam = dsl.PipelineParam(name='width', value=256),\n  channels: dsl.PipelineParam = dsl.PipelineParam(name='channels', value=1),\n  \n  eval_steps: dsl.PipelineParam = dsl.PipelineParam(name='eval-steps', value=10000),\n  max_train_steps: dsl.PipelineParam = dsl.PipelineParam(name='max-train-steps', value=10000),\n  prefetch_buffer_size: dsl.PipelineParam = dsl.PipelineParam(name='prefetch-buffer', value=-1),\n\n  num_gpus_serve: dsl.PipelineParam = dsl.PipelineParam(name='num-gpus-serve', value=0),\n):\n\n  dataprep = dsl.ContainerOp(\n    name='dataprep',\n    image='gcr.io/speedy-aurora-193605/prep_tfr_df:latest',\n    arguments=[\"--input-dir\", inp_dir,\n      \"--output-dir\", out_dir,\n      \"--dataprep-flag\", dataprep_flag,\n      \"--num-shards\", num_shards,\n      \"--split-flag\", split_flag,\n      \"--train-split\", train_split,\n      \"--project-id\", project_id,\n      \"--seed\", seed,\n      \"--height\", height,\n      \"--width\", width,\n      ],\n      \n\n      ).apply(gcp.use_gcp_secret(secret_name='user-gcp-sa', secret_file_path_in_volume='/user-gcp-sa.json', volume_name='gcp-credentials-user-gcp-sa'))\n\n  train = dsl.ContainerOp(\n    name='train',\n    image='gcr.io/speedy-aurora-193605/cnn_train_dis:latest',\n    arguments=[\"--conv-dir\", out_dir,\n        \"--model-dir\", model_dir,\n        \"--save-model-dir\", save_model_dir,\n        \"--train-flag\", train_flag,\n        \"--num-epochs\", epochs,\n        \"--batch-size\", batch_size,\n        \"--max-train-steps\", max_train_steps,\n        \"--eval-steps\", eval_steps,\n        \n        \"--prefetch-buffer\", prefetch_buffer_size,\n        \"--height\", height,\n        \"--width\", width,\n        \"--channels\", channels,\n        ]\n    ).apply(gcp.use_gcp_secret(secret_name='user-gcp-sa', secret_file_path_in_volume='/user-gcp-sa.json', volume_name='gcp-credentials-user-gcp-sa'))\n\n  tensorboard = dsl.ContainerOp(\n    name='tensorboard',\n    image='gcr.io/speedy-aurora-193605/model-tensorboard:latest',\n    arguments=[\"--model-dir\", model_dir,\n      ],\n      ).apply(gcp.use_gcp_secret(secret_name='user-gcp-sa', secret_file_path_in_volume='/user-gcp-sa.json', volume_name='gcp-credentials-user-gcp-sa'))\n\n  tfserve = dsl.ContainerOp(\n    name='tfserve',\n    image='gcr.io/speedy-aurora-193605/retina-tfserve:latest',\n    arguments=[\"--model_name\", model_name,\n      \"--model_path\", save_model_dir,\n      \"--num_gpus\", num_gpus_serve,\n      ],\n      ).apply(gcp.use_gcp_secret(secret_name='admin-gcp-sa', secret_file_path_in_volume='/admin-gcp-sa.json', volume_name='gcp-credentials-admin-gcp-sa'))\n      \n  train.set_gpu_limit('2')\n  train.set_memory_request('8G')\n  train.set_cpu_request('4')\n  train.after(dataprep)\n  tfserve.after(train)\n  tensorboard.after(dataprep)\n\nif __name__ == '__main__':\n  import kfp.compiler as compiler\n  compiler.Compiler().compile(dp_inf_pipe, 'retinal_oct_fin.tar.gz')"
  },
  {
    "repo": "ZeynepRuveyda/MLOPS_Automation_SyntheticDataCreation_and_MatsimSimulation",
    "file_path": "kubernetes_last_codes/code/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/ZeynepRuveyda/MLOPS_Automation_SyntheticDataCreation_and_MatsimSimulation/master/kubernetes_last_codes/code/pipeline.py",
    "content": "from kfp import dsl\n\n@dsl.pipeline(\n    name='Hello World',\n    description='A pipeline to demonstrate use of PersistentVolumeClaim'\n)\ndef pvc_pipeline(log:str =\"/mnt/data/kubeflow\",\n                 data_folder_directory:str = \"\",\n                 output_folder_directory:str=\"\",\n                 tmp_folder_directory:str=\"\",\n                 sampling_rate:float=0.001):\n    #git clone\n    def clone_equasim():\n        return dsl.ContainerOp(\n        name = 'Git Clone Equasim',\n        image = 'zeynep02/pipeline-v0.0.4:latest',\n        command = 'python3',\n        arguments = [\n            \"/mnt/data/kubeflow/code/clone.py\",  \n            \"--log_dir\",\n            log\n           \n\n        ],\n        pvolumes={'/mnt/data/kubeflow': dsl.PipelineVolume(pvc='my-pvc')}\n    )\n    def edit_config():\n        return dsl.ContainerOp(\n        name = 'Edit Config Yml',\n        image = 'zeynep02/pipeline-v0.0.4:latest',\n        command = 'python3',\n        arguments = [\n            \"/mnt/data/kubeflow/code/edit_config.py\",\n            \"--log_dir\",\n            log,\n            \"--data_folder_directory\",\n            data_folder_directory,\n            \"--output_folder_directory\",\n            output_folder_directory,\n            \"--tmp_folder_directory\",\n            tmp_folder_directory,\n            \"--sampling_rate\",\n            sampling_rate\n\n        ],\n        pvolumes={'/mnt/data/kubeflow': dsl.PipelineVolume(pvc='my-pvc')}\n\n        )\n    \n    def synpp():\n        return dsl.ContainerOp(\n        name='Run synp',\n        image='zeynep02/pipeline-v0.0.4:latest',\n        command='python3',\n        arguments=[ \n            \"/mnt/data/kubeflow/code/synpp.py\",\n            \"--log_dir\",\n            log,],\n        pvolumes={'/mnt/data/kubeflow': dsl.PipelineVolume(pvc='my-pvc')}\n        )\n    \n    clone_git = clone_equasim()\n    clone_git.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n\n    update_config = edit_config()\n    update_config.execution_options.caching_strategy.max_cache_staleness = \"P0D\"   \n    update_config.after(clone_git)\n    synpp_run = synpp()\n    synpp_run.execution_options.caching_strategy.max_cache_staleness = \"P0D\" \n    synpp_run.after(update_config)\n\n\n\n    \n\nif __name__ == '__main__':\n    import kfp.compiler as compiler\n    compiler.Compiler().compile(pvc_pipeline, 'pipeline_v_4.yaml')\n"
  },
  {
    "repo": "ZeynepRuveyda/MLOPS_Automation_SyntheticDataCreation_and_MatsimSimulation",
    "file_path": "kubernetes_last_codes/code/pipeline_demo.py",
    "raw_url": "https://raw.githubusercontent.com/ZeynepRuveyda/MLOPS_Automation_SyntheticDataCreation_and_MatsimSimulation/master/kubernetes_last_codes/code/pipeline_demo.py",
    "content": "from kfp import dsl\n\ndef write():\n    return dsl.ContainerOp(\n        name='Write',\n        image='busybox',\n        command=['sh', '-c'],\n        arguments=['echo \"Hello\" > /mnt/data/kubeflow/hello_world.txt'],\n        pvolumes={'/mnt/data/kubeflow': dsl.PipelineVolume(pvc='my-pvc')}\n    )\n\ndef read():\n    return dsl.ContainerOp(\n        name='Read',\n        image='busybox',\n        command=['sh', '-c'],\n        arguments=['cat /mnt/data/kubeflow/hello_world.txt'],\n        pvolumes={'/mnt/data/kubeflow': dsl.PipelineVolume(pvc='my-pvc')}\n    )\n\n@dsl.pipeline(\n    name='Hello World',\n    description='A pipeline to demonstrate use of PersistentVolumeClaim'\n)\ndef pvc_pipeline():\n    producer = write()\n    consumer = read().after(producer)\n\nif __name__ == '__main__':\n    import kfp.compiler as compiler\n    compiler.Compiler().compile(pvc_pipeline, 'hello_world.yaml')\n"
  },
  {
    "repo": "ZeynepRuveyda/MLOPS_Automation_SyntheticDataCreation_and_MatsimSimulation",
    "file_path": "kubernetes_last_codes/code/pipeline_hello_world.py",
    "raw_url": "https://raw.githubusercontent.com/ZeynepRuveyda/MLOPS_Automation_SyntheticDataCreation_and_MatsimSimulation/master/kubernetes_last_codes/code/pipeline_hello_world.py",
    "content": "from kfp import dsl\n\ndef write():\n    return dsl.ContainerOp(\n        name='Write',\n        image='busybox',\n        command=['sh', '-c'],\n        arguments=['echo \"Hello\" > /mnt/data/kubeflow/hello_world.txt'],\n        pvolumes={'/mnt/data/kubeflow': dsl.PipelineVolume(pvc='my-pvc')}\n    )\n\ndef read():\n    return dsl.ContainerOp(\n        name='Read',\n        image='busybox',\n        command=['sh', '-c'],\n        arguments=['cat /mnt/data/kubeflow/hello_world.txt'],\n        pvolumes={'/mnt/data/kubeflow': dsl.PipelineVolume(pvc='my-pvc')}\n    )\n\n@dsl.pipeline(\n    name='Hello World',\n    description='A pipeline to demonstrate use of PersistentVolumeClaim'\n)\ndef pvc_pipeline():\n    producer = write()\n    consumer = read().after(producer)\n\nif __name__ == '__main__':\n    import kfp.compiler as compiler\n    compiler.Compiler().compile(pvc_pipeline, 'hello_world.yaml')\n"
  },
  {
    "repo": "SaschaDittmann/kubeflow-azurepipeline",
    "file_path": "code/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/SaschaDittmann/kubeflow-azurepipeline/master/code/pipeline.py",
    "content": "import kfp.dsl as dsl\nfrom kubernetes import client as k8s_client\n\n@dsl.pipeline(\n    name='Tacos vs. Burritos',\n    description='Simple TF CNN for binary classifier between burritos and tacos'\n)\ndef tacosandburritos_train(\n    tenant_id,\n    service_principal_id,\n    service_principal_password,\n    subscription_id,\n    resource_group,\n    workspace,\n    persistent_volume_name='azure',\n    persistent_volume_path='/mnt/azure',\n    data_download='https://github.com/SaschaDittmann/kubeflow-azurepipeline/raw/main/data/tacodata.zip',\n    epochs=5,\n    batch=32,\n    learning_rate=0.0001,\n    imagetag='latest',\n    model_name='tacosandburritos',\n    profile_name='tacoprofile',\n    service_name='tacosandburritos-service'\n):\n\n    operations = {}\n    image_size = 160\n    training_folder = 'train'\n    training_dataset = 'train.txt'\n    model_folder = 'model'\n\n    # preprocess data\n    operations['preprocess'] = dsl.ContainerOp(\n        name='preprocess',\n        image='bytesmith/kubeflow-azurepipeline:latest-preprocess',\n        command=['python'],\n        arguments=[\n            '/scripts/data.py',\n            '--base_path', persistent_volume_path,\n            '--data', training_folder,\n            '--target', training_dataset,\n            '--img_size', image_size,\n            '--zipfile', data_download\n        ]\n    )\n\n    #train\n    operations['training'] = dsl.ContainerOp(\n        name='training',\n        image='bytesmith/kubeflow-azurepipeline:latest-training',\n        command=['python'],\n        arguments=[\n            '/scripts/train.py',\n            '--base_path', persistent_volume_path,\n            '--data', training_folder, \n            '--epochs', epochs, \n            '--batch', batch, \n            '--image_size', image_size, \n            '--lr', learning_rate, \n            '--outputs', model_folder, \n            '--dataset', training_dataset\n        ]\n    )\n    operations['training'].after(operations['preprocess'])\n\n    # register model\n    operations['register'] = dsl.ContainerOp(\n        name='register',\n        image='bytesmith/kubeflow-azurepipeline:latest-register',\n        command=['python'],\n        arguments=[\n            '/scripts/register.py',\n            '--base_path', persistent_volume_path,\n            '--model', 'latest.h5',\n            '--model_name', model_name,\n            '--tenant_id', tenant_id,\n            '--service_principal_id', service_principal_id,\n            '--service_principal_password', service_principal_password,\n            '--subscription_id', subscription_id,\n            '--resource_group', resource_group,\n            '--workspace', workspace\n        ]\n    )\n    operations['register'].after(operations['training'])\n\n    operations['profile'] = dsl.ContainerOp(\n        name='profile',\n        image='bytesmith/kubeflow-azurepipeline:latest-profile',\n        command=['/bin/bash'],\n        arguments=[\n            '/scripts/profile.sh',\n            '-n', profile_name,\n            '-e', '/scripts/score.py',\n            '-d', '{ \"schemaVersion\": 1, \"datasetType\": \"Tabular\", \"parameters\": { \"path\": [ \"https://github.com/SaschaDittmann/kubeflow-azurepipeline/raw/master/data/profiledata.json\" ], \"sourceType\": \"json_lines_files\" }, \"registration\": { \"createNewVersion\": true, \"name\": \"tacosandburritos-dataset\", \"tags\": { \"mlops-system\": \"kubeflow\" } } }',\n            '-t', tenant_id,\n            '-r', resource_group,\n            '-w', workspace,\n            '-s', service_principal_id,\n            '-p', service_principal_password,\n            '-u', subscription_id,\n            '-b', persistent_volume_path\n        ]\n    )\n    operations['profile'].after(operations['register'])\n\n    operations['deploy'] = dsl.ContainerOp(\n        name='deploy',\n        image='bytesmith/kubeflow-azurepipeline:latest-deploy',\n        command=['/bin/bash'],\n        arguments=[\n            '/scripts/deploy.sh',\n            '-n', service_name,\n            '-e', '/scripts/score.py',\n            '-d', '/scripts/acideploymentconfig.json',\n            '-t', tenant_id,\n            '-r', resource_group,\n            '-w', workspace,\n            '-s', service_principal_id,\n            '-p', service_principal_password,\n            '-u', subscription_id,\n            '-b', persistent_volume_path\n        ]\n    )\n    operations['deploy'].after(operations['profile'])\n\n    for _, op in operations.items():\n        op.container.set_image_pull_policy(\"Always\")\n        op.add_volume(\n            k8s_client.V1Volume(\n                name='azure',\n                persistent_volume_claim=k8s_client.V1PersistentVolumeClaimVolumeSource(\n                    claim_name='azure-managed-disk')\n                )\n            ).add_volume_mount(k8s_client.V1VolumeMount(\n                mount_path='/mnt/azure', \n                name='azure')\n            )\n\n\nif __name__ == '__main__':\n   import kfp.compiler as compiler\n   compiler.Compiler().compile(tacosandburritos_train, __file__ + '.tar.gz')\n"
  },
  {
    "repo": "Epochex/MLOps_Kubeflow_TwinStream_Pipeline",
    "file_path": "kfp_pipeline/build_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/Epochex/MLOps_Kubeflow_TwinStream_Pipeline/main/kfp_pipeline/build_pipeline.py",
    "content": "# kfp_pipeline/build_pipeline.py\nimport pathlib, sys, kfp\nfrom kfp import dsl\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u5e38\u91cf & \u8def\u5f84 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nROOT  = pathlib.Path(__file__).resolve().parents[1]\nCOMP  = ROOT / \"kfp_pipeline\" / \"components\"\nIMAGE = \"hirschazer/flux_demo5:latest\"      # \u4f60\u7684\u4e1a\u52a1\u955c\u50cf\n\n# \u8ba9 Python \u627e\u5f97\u5230 components \u5305\nsys.path.append(str(COMP.parent))\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u5bfc\u5165\u5df2\u88c5\u9970\u597d\u7684\u7ec4\u4ef6\u51fd\u6570 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nfrom components.split_data          import split_data\nfrom components.offline_train       import offline_train\nfrom components.launch_katib        import launch_katib\nfrom components.apply_k8s_resource  import apply_k8s_resource\n\n# \u5982\u9700\u7edf\u4e00\u955c\u50cf\uff0c\u53ef\u5728\u8fd9\u91cc\u52a8\u6001\u8986\u5199\uff08\u4efb\u9009\uff09\nfor c in (split_data, offline_train, launch_katib):\n    c.component_spec.implementation.container.image = IMAGE\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 DAG \u5b9a\u4e49 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\n@dsl.pipeline(\n    name=\"twin-stream-pipeline\",\n    description=\"Katib HPO \u2192 Batch \u8bad\u7ec3 \u2192 Streaming Job\"\n)\ndef pipeline(\n    csv_uri: str = \"s3://katib-flux-demo/datasets/load_stimulus_global.csv\",\n):\n    # \u2460 40/60 \u5207\u5206\uff0c\u81ea\u52a8\u628a train/stream.csv \u4e0a\u4f20\u56de\u540c Bucket\n    split = split_data(csv_uri=csv_uri)\n\n    # \u2461 Katib \u968f\u673a\u641c\u7d22\n    hpo = launch_katib(\n        train_image = IMAGE,\n        train_csv   = split.outputs[\"train_csv\"],\n        out_dir     = \"/mnt/data\",\n    )\n\n    # \u2462 \u79bb\u7ebf Batch \u8bad\u7ec3\n    train = offline_train(\n        csv       = split.outputs[\"train_csv\"],\n        model_key = \"models/ann_batch_model.pth\",\n    ).after(hpo)\n\n    # \u2463 kubectl apply Producer + Consumer Job\n    apply_k8s_resource(\n        yaml_path = str(COMP / \"stream_job.yaml\"),\n    ).after(train)\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u7f16\u8bd1 + \u53ef\u9009\uff1a\u81ea\u52a8\u89e6\u53d1\u4e00\u6b21 Run \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 #\nif __name__ == \"__main__\":\n    pkg = str(ROOT / \"pipeline.json\")\n    kfp.compiler.Compiler().compile(\n        pipeline_func = pipeline,\n        package_path  = pkg,\n    )\n    print(\"\u2705  pipeline.json generated\")\n\n    # ----- \u81ea\u52a8\u89e6\u53d1\uff08\u53ef\u5220\u6389\uff09 -----\n    from kfp import Client\n    client = Client(host=\"http://ml-pipeline.kubeflow:8888\")  # in-cluster \u53ef\u7701 host\n    run = client.create_run_from_pipeline_package(\n        pipeline_file   = pkg,\n        arguments       = {\"csv_uri\": \"s3://katib-flux-demo/datasets/load_stimulus_global.csv\"},\n        experiment_name = \"twin-stream\",\n        run_name        = \"twin-stream-auto\",\n    )\n    print(\"\ud83d\ude80  triggered run:\", run.run_id)\n\n\n"
  },
  {
    "repo": "Epochex/MLOps_Kubeflow_TwinStream_Pipeline",
    "file_path": "kfp_pipeline/components/split_data.py",
    "raw_url": "https://raw.githubusercontent.com/Epochex/MLOps_Kubeflow_TwinStream_Pipeline/main/kfp_pipeline/components/split_data.py",
    "content": "# kfp_pipeline/components/split_data.py\nfrom kfp import dsl\nfrom kfp.dsl import OutputPath\nimport pandas as pd\nimport s3fs\nimport os\n\n@dsl.component(\n    base_image=\"python:3.10\",  # \u63a8\u8350\u81ea\u5b9a\u4e49\u955c\u50cf\u4ee5\u52a0\u901f\n    packages_to_install=[\"pandas\", \"s3fs\"]\n)\ndef split_data(\n    csv_uri: str,\n    train_csv: OutputPath(str),\n    stream_csv: OutputPath(str),\n    ratio: float = 0.4\n):\n    \"\"\"\n    \u7ec4\u4ef6\u529f\u80fd:\n    \u2022 csv_uri: \u5fc5\u987b\u662f s3://bucket/key.csv\n    \u2022 \u6309 ratio \u5207\u5206\u6570\u636e\uff0c\u8f93\u51fa train_csv / stream_csv\n\n    \u73af\u5883\u53d8\u91cf:\n    - MINIO_KEY\n    - MINIO_SECRET\n    - MINIO_ENDPOINT\n    \"\"\"\n    # 1\ufe0f\u20e3 \u8fde\u63a5 S3 (MinIO)\n    fs = s3fs.S3FileSystem(\n        key=os.getenv(\"MINIO_KEY\", \"minio\"),\n        secret=os.getenv(\"MINIO_SECRET\", \"minio123\"),\n        client_kwargs={\n            \"endpoint_url\": os.getenv(\"MINIO_ENDPOINT\", \"http://minio.kubeflow:9000\"),\n            \"region_name\": \"us-east-1\"  # \u901a\u5e38\u4fdd\u6301\u9ed8\u8ba4\uff0c\u5bf9 MinIO \u65e0\u5f3a\u5236\u8981\u6c42\n        }\n    )\n\n    # 2\ufe0f\u20e3 \u8bfb\u53d6\u6570\u636e\n    with fs.open(csv_uri, \"rb\") as f:\n        df = (\n            pd.read_csv(f)\n            .replace(['<not counted>', ' '], pd.NA)\n            .dropna()\n        )\n\n    # 3\ufe0f\u20e3 \u5207\u5206 & \u4fdd\u5b58\n    cut = int(len(df) * ratio)\n    df.iloc[:cut].to_csv(train_csv, index=False)\n    df.iloc[cut:].to_csv(stream_csv, index=False)\n\n    print(f\"[split] rows={len(df)}  ratio={ratio}\")\n    print(\"train_csv \u2192\", train_csv)\n    print(\"stream_csv\u2192\", stream_csv)\n\n\n\n"
  },
  {
    "repo": "Vishwajyoti/Pipelines",
    "file_path": "demo_kf_pipelines.py",
    "raw_url": "https://raw.githubusercontent.com/Vishwajyoti/Pipelines/master/demo_kf_pipelines.py",
    "content": "\"\"\"\nCreated on Tue Sep  3 14:58:04 2019\n\n@author: vispande2\n\"\"\"\n\n\nfrom kfp import compiler\nimport kfp.dsl as dsl\nimport kfp.gcp as gcp\nimport kfp.notebook\nimport sys\nimport json\n\nclass ObjectDict(dict):\n  def __getattr__(self, name):\n    if name in self:\n      return self[name]\n    else:\n      raise AttributeError(\"No such attribute: \" + name)\n\n\n@dsl.pipeline(\n  name='Basic KubeFlow Pipeline',\n  description='Feature Eng,Training,Testing & Deployment'\n)\ndef ftreng_train_test_and_deploy(\n        project='cohesive-gadget-166410',\n        bucket_uri='gs://vishwa/',\n        region='us-central1',\n        test_size=0.3,\n        file_name='Boston.csv',\n        target_var='target',\n        hyper_param=json.dumps({'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000],'max_features': ['auto', 'sqrt'],'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None],'min_samples_split': [2, 5, 10],'min_samples_leaf': [1, 2, 4],'bootstrap': [True, False]}),\n        search_type=1,\n        model_bucket_name='rf-vj-model',\n        model_name='rf-vj',\n        framework='scikit-learn',\n        version_key_word='rf_model_'\n        \n        ):\n# Step 1: create training dataset using Apache Beam on Cloud Dataflow\n    feature_eng = dsl.ContainerOp(\n            name='feature_eng',\n            # image needs to be a compile-time string\n            image='gcr.io/cohesive-gadget-166410/feature-eng-vj:latest',\n            arguments=[\n                    '--path',bucket_uri,\n                    '--filename',file_name,\n                    '--t_size',test_size\n                    ]\n            #,file_outputs={'bucket': '/output.txt'}\n            ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n\n# Step 2: Train the model and find best set of hyperparameter.\n    train = dsl.ContainerOp(\n            name='train',\n            # image needs to be a compile-time string\n            image='gcr.io/cohesive-gadget-166410/train-rf-vj:latest',\n            arguments=[\n                    '--path',bucket_uri,\n                    '--target',target_var,\n                    '--h_param',hyper_param,\n                    '--search_type',search_type\n                    ]\n            #,file_outputs={'jobname': '/output.txt'}\n            ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n    train.after(feature_eng)\n\n# Step 3: Train the model some more, but on the pipelines cluster itself\n    test = dsl.ContainerOp(\n            name='test',\n            # image needs to be a compile-time string\n            image='gcr.io/cohesive-gadget-166410/test-rf-vj:latest',\n            #image='gcr.io/cloud-training-demos/babyweight-pipeline-traintuned-trainer@sha256:3d73c805430a16d0675aeafa9819d6d2cfbad0f0f34cff5fb9ed4e24493bc9a8',\n            arguments=[\n                    '--path',bucket_uri,\n                    '--target',target_var\n                    ]\n            #,file_outputs={'train': '/output.txt'}\n            ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n    test.after(train)\n\n# Step 4: Deploy the trained model to Cloud ML Engine\n    deploy_cmle = dsl.ContainerOp(\n            name='deploycmle',\n            # image needs to be a compile-time string\n            image='gcr.io/cohesive-gadget-166410/deploy-rf-vj:latest',\n            arguments=[\n                    model_bucket_name,\n                    project,\n                    region,\n                    bucket_uri,\n                    framework,\n                    version_key_word,\n                    model_name   \n                    ]\n\n        ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n    deploy_cmle.after(test)\n\nif __name__ == '__main__':\n    filename=sys.argv[1]\n    compiler.Compiler().compile(ftreng_train_test_and_deploy,filename)\n"
  },
  {
    "repo": "II-VSB-II/TaxiClassificationKubeflowPipelinePVC",
    "file_path": "Taxi-Pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/II-VSB-II/TaxiClassificationKubeflowPipelinePVC/main/Taxi-Pipeline.py",
    "content": "import kfp\r\nfrom kfp import components\r\nfrom kfp import dsl\r\nfrom kfp import gcp\r\nfrom kfp import onprem\r\n\r\nplatform = 'local'\r\n\r\n#proxy=\"http://test:8080\"\r\nproxy = \"\"\r\n\r\ndataflow_tf_data_validation_op  = components.load_component_from_url('https://raw.githubusercontent.com/II-VSB-II/TaxiClassificationPipelinePVC/main/yamls/tfdv_component.yaml')\r\ndataflow_tf_transform_op        = components.load_component_from_url('https://raw.githubusercontent.com/II-VSB-II/TaxiClassificationPipelinePVC/main/yamls/tft_component.yaml')\r\ntf_train_op                     = components.load_component_from_url('https://raw.githubusercontent.com/II-VSB-II/TaxiClassificationPipelinePVC/main/yamls/dnntrainer_component.yaml')\r\ndataflow_tf_model_analyze_op    = components.load_component_from_url('https://raw.githubusercontent.com/II-VSB-II/TaxiClassificationPipelinePVC/main/yamls/tfma_component.yaml')\r\ndataflow_tf_predict_op          = components.load_component_from_url('https://raw.githubusercontent.com/II-VSB-II/TaxiClassificationPipelinePVC/main/yamls/predict_component.yaml')\r\n\r\nconfusion_matrix_op             = components.load_component_from_url('https://raw.githubusercontent.com/II-VSB-II/TaxiClassificationPipelinePVC/main/yamls/confusion_matrix_component.yaml')\r\nroc_op                          = components.load_component_from_url('https://raw.githubusercontent.com/II-VSB-II/TaxiClassificationPipelinePVC/main/yamls/roc_component.yaml')\r\n\r\nkubeflow_deploy_op              = components.load_component_from_url('https://raw.githubusercontent.com/II-VSB-II/TaxiClassificationPipelinePVC/main/yamls/deployer_component.yaml')\r\n\r\n@dsl.pipeline(\r\n  name='TFX Taxi Cab Classification Pipeline Example',\r\n  description='Example pipeline that does classification with model analysis based on a public BigQuery dataset.'\r\n)\r\ndef taxi_cab_classification(\r\n    project,\r\n    output=\"/mnt/shared\",\r\n    column_names='/mnt/shared/pipelines/column-names.json',\r\n    key_columns='trip_start_timestamp',\r\n    train='/mnt/shared/pipelines/train.csv',\r\n    evaluation='/mnt/shared/pipelines/eval.csv',\r\n    mode='local',\r\n    preprocess_module='/mnt/shared/pipelines/preprocessing.py',\r\n    learning_rate=0.1,\r\n    hidden_layer_size='1500',\r\n    steps=3000,\r\n    analyze_slice_column='trip_start_hour'\r\n):\r\n    output_template = str(output) + '/{{workflow.uid}}/{{pod.name}}/data'\r\n    target_lambda = \"\"\"lambda x: (x['target'] > x['fare'] * 0.2)\"\"\"\r\n    target_class_lambda = \"\"\"lambda x: 1 if (x['target'] > x['fare'] * 0.2) else 0\"\"\"\r\n\r\n    tf_server_name = 'taxi-cab-classification-model-{{workflow.uid}}'\r\n\r\n    if platform != 'GCP':\r\n        vop = dsl.VolumeOp(\r\n            name=\"create_pvc\",\r\n            resource_name=\"pipeline-pvc\",\r\n            modes=dsl.VOLUME_MODE_RWM,\r\n            size=\"1Gi\"\r\n        )\r\n        if proxy != \"\":\r\n            checkout = dsl.ContainerOp(\r\n            name=\"checkout\",\r\n            image=\"alpine/git:latest\",\r\n            command=[\"git\", \"clone\", \"https://github.com/II-VSB-II/TaxiClassificationKubeflowPipelinePVC.git\", str(output) + \"/pipelines\", \"-c\", \"http.proxy={}\".format(proxy)],\r\n        ).apply(onprem.mount_pvc(vop.outputs[\"name\"], 'local-storage', output))\r\n        else:\r\n            checkout = dsl.ContainerOp(\r\n            name=\"checkout\",\r\n            image=\"alpine/git:latest\",\r\n            command=[\"git\", \"clone\", \"https://github.com/II-VSB-II/TaxiClassificationKubeflowPipelinePVC.git\", str(output) + \"/pipelines\"],\r\n        ).apply(onprem.mount_pvc(vop.outputs[\"name\"], 'local-storage', output))\r\n        \r\n        checkout.after(vop)\r\n\r\n    validation = dataflow_tf_data_validation_op(\r\n        inference_data=train,\r\n        validation_data=evaluation,\r\n        column_names=column_names,\r\n        key_columns=key_columns,\r\n        gcp_project=project,\r\n        run_mode=mode,\r\n        validation_output=output_template,\r\n    )\r\n    if platform != 'GCP':\r\n        validation.after(checkout)\r\n\r\n    preprocess = dataflow_tf_transform_op(\r\n        training_data_file_pattern=train,\r\n        evaluation_data_file_pattern=evaluation,\r\n        schema=validation.outputs['schema'],\r\n        gcp_project=project,\r\n        run_mode=mode,\r\n        preprocessing_module=preprocess_module,\r\n        transformed_data_dir=output_template\r\n    )\r\n\r\n    training = tf_train_op(\r\n        transformed_data_dir=preprocess.output,\r\n        schema=validation.outputs['schema'],\r\n        learning_rate=learning_rate,\r\n        hidden_layer_size=hidden_layer_size,\r\n        steps=steps,\r\n        target='tips',\r\n        preprocessing_module=preprocess_module,\r\n        training_output_dir=output_template\r\n    )\r\n\r\n    analysis = dataflow_tf_model_analyze_op(\r\n        model=training.output,\r\n        evaluation_data=evaluation,\r\n        schema=validation.outputs['schema'],\r\n        gcp_project=project,\r\n        run_mode=mode,\r\n        slice_columns=analyze_slice_column,\r\n        analysis_results_dir=output_template\r\n    )\r\n\r\n    prediction = dataflow_tf_predict_op(\r\n        data_file_pattern=evaluation,\r\n        schema=validation.outputs['schema'],\r\n        target_column='tips',\r\n        model=training.output,\r\n        run_mode=mode,\r\n        gcp_project=project,\r\n        predictions_dir=output_template\r\n    )\r\n\r\n    cm = confusion_matrix_op(\r\n        predictions=prediction.output,\r\n        target_lambda=target_lambda,\r\n        output_dir=output_template\r\n    )\r\n\r\n    roc = roc_op(\r\n        predictions_dir=prediction.output,\r\n        target_lambda=target_class_lambda,\r\n        output_dir=output_template\r\n    )\r\n\r\n    if platform == 'GCP':\r\n        deploy = kubeflow_deploy_op(\r\n            model_dir=str(training.output) + '/export/export',\r\n            server_name=tf_server_name\r\n        )\r\n    else:\r\n        deploy = kubeflow_deploy_op(\r\n            cluster_name=project,\r\n            model_dir=str(training.output) + '/export/export',\r\n            pvc_name=vop.outputs[\"name\"],\r\n            server_name=tf_server_name\r\n        )\r\n\r\n    steps = [validation, preprocess, training, analysis, prediction, cm, roc, deploy]\r\n    for step in steps:\r\n        if platform == 'GCP':\r\n            step.apply(gcp.use_gcp_secret('user-gcp-sa'))\r\n        else:\r\n            step.apply(onprem.mount_pvc(vop.outputs[\"name\"], 'local-storage', output))\r\n\r\n\r\nif __name__ == '__main__':\r\n    kfp.compiler.Compiler().compile(taxi_cab_classification, \"TaxiPipelinePVC\" + '.zip')"
  },
  {
    "repo": "seoeun25/ml-ops",
    "file_path": "distributed_training/pytorch-op/mnist/pytorch_mnist_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/seoeun25/ml-ops/master/distributed_training/pytorch-op/mnist/pytorch_mnist_pipeline.py",
    "content": "import json\nfrom typing import NamedTuple\nfrom collections import namedtuple\nimport kfp\nimport kfp.dsl as dsl\nfrom kfp import components\nfrom kfp.dsl.types import Integer\n\ndef kfp_client():\n    \"\"\"\n    Returns Kubeflow pipelines client inside cluster.\n    \"\"\"\n    end_point=\"http://ml-pipeline.kubeflow.svc.cluster.local:8888\"\n    credentials = kfp.auth.ServiceAccountTokenVolumeCredentials(path=None)\n    client = kfp.Client(host=end_point, credentials=credentials)\n\n    return client\n\ndef base_image() -> str:\n    import os\n    import re\n\n    iam = os.environ.get(\"AWS_ROLE_ARN\")\n    account = re.findall(\"arn:aws:iam::(.*):role*\", iam)[0]\n    region = os.environ.get(\"AWS_REGION\")\n    base_image = \"{}.dkr.ecr.{}.amazonaws.com/aladin-runtime:anaconda-cpu\".format(account, region)\n    print(\"base_image = {}\".format(base_image))\n    return base_image\ndef get_current_namespace():\n    \"\"\"Returns current namespace if available, else kubeflow\"\"\"\n    try:\n        current_namespace = open(\n            \"/var/run/secrets/kubernetes.io/serviceaccount/namespace\"\n        ).read()\n    except:\n        current_namespace = \"kubeflow\"\n    return current_namespace\n\n\ndef create_worker_spec(\n        worker_num: int = 0\n) -> NamedTuple(\n    \"CreatWorkerSpec\", [(\"worker_spec\", dict)]\n):\n    from collections import namedtuple\n    \"\"\"\n    Creates pytorch-job worker spec\n    \"\"\"\n    worker = {}\n    if worker_num > 0:\n        worker = {\n            \"replicas\": worker_num,\n            \"restartPolicy\": \"OnFailure\",\n            \"template\": {\n                \"metadata\": {\n                    \"annotations\": {\n                        \"sidecar.istio.io/inject\": \"false\"\n                    }\n                },\n                \"spec\": {\n                    \"containers\": [\n                        {\n                            \"command\": [\n                                \"python\",\n                                \"/opt/mnist/src/mnist.py\"\n                            ],\n                            \"args\": [\n                                \"--backend\",\n                                \"gloo\",\n                            ],\n                            \"image\": \"public.ecr.aws/pytorch-samples/pytorch_dist_mnist:latest\",\n                            \"name\": \"pytorch\",\n                            # \"resources\": {\n                            #     \"requests\": {\n                            #         \"memory\": \"4Gi\",\n                            #         \"cpu\": \"2000m\",\n                            #         # Uncomment for GPU\n                            #         # \"nvidia.com/gpu\": 1,\n                            #     },\n                            #     \"limits\": {\n                            #         \"memory\": \"4Gi\",\n                            #         \"cpu\": \"2000m\",\n                            #         # Uncomment for GPU\n                            #         # \"nvidia.com/gpu\": 1,\n                            #     },\n                            # },\n                        }\n                    ]\n                },\n            },\n        }\n\n    worker_spec_output = namedtuple(\n        \"MyWorkerOutput\", [\"worker_spec\"]\n    )\n    return worker_spec_output(worker)\n\n\nworker_spec_op = components.func_to_container_op(\n    create_worker_spec,\n    base_image=base_image(),\n)\n\n\n@dsl.pipeline(\n    name=\"launch-kubeflow-pytorchjob\",\n    description=\"An example to launch pytorch.\",\n)\ndef mnist_train(\n        namespace: str = get_current_namespace(),\n        worker_replicas: int = 1,\n        ttl_seconds_after_finished: int = -1,\n        job_timeout_minutes: int = 60,\n        delete_after_done: bool = False,\n):\n    print(\"mnist_train_pipeline: namespace={}, worker_replicas={}, ttl_seconds_after_finished={}, job_timeout_minutes={}, delete_after_done={}\"\n          .format(namespace, worker_replicas, ttl_seconds_after_finished, job_timeout_minutes, delete_after_done))\n    pytorchjob_launcher_op = components.load_component_from_file(\n        \"../launcher/component.yaml\"\n    )\n\n    master = {\n        \"replicas\": 1,\n        \"restartPolicy\": \"OnFailure\",\n        \"template\": {\n            \"metadata\": {\n                \"annotations\": {\n                    # See https://github.com/kubeflow/website/issues/2011\n                    \"sidecar.istio.io/inject\": \"false\"\n                }\n            },\n            \"spec\": {\n                \"containers\": [\n                    {\n                        #To override default command\n                        \"command\": [\n                            \"python\",\n                            \"/opt/mnist/src/mnist.py\"\n                        ],\n                        \"args\": [\n                            \"--backend\",\n                            \"gloo\",\n                        ],\n                        # Or, create your own image from\n                        # https://github.com/kubeflow/pytorch-operator/tree/master/examples/mnist\n                        \"image\": \"public.ecr.aws/pytorch-samples/pytorch_dist_mnist:latest\",\n                        \"name\": \"pytorch\",\n                        # \"resources\": {\n                        #     \"requests\": {\n                        #         \"memory\": \"4Gi\",\n                        #         \"cpu\": \"2000m\",\n                        #         # Uncomment for GPU\n                        #         # \"nvidia.com/gpu\": 1,\n                        #     },\n                        #     \"limits\": {\n                        #         \"memory\": \"4Gi\",\n                        #         \"cpu\": \"2000m\",\n                        #         # Uncomment for GPU\n                        #         # \"nvidia.com/gpu\": 1,\n                        #     },\n                        # },\n                    }\n                ],\n                # If imagePullSecrets required\n                # \"imagePullSecrets\": [\n                #     {\"name\": \"image-pull-secret\"},\n                # ],\n            },\n        },\n    }\n\n    worker_spec_create = worker_spec_op(\n        worker_replicas\n    )\n\n    # Launch and monitor the job with the launcher\n    pytorchjob_launcher_op(\n        # Note: name needs to be a unique pytorchjob name in the namespace.\n        # Using RUN_ID_PLACEHOLDER is one way of getting something unique.\n        name=f\"pytorch-mnist-{kfp.dsl.RUN_ID_PLACEHOLDER}\",\n        namespace=namespace,\n        master_spec=master,\n        # pass worker_spec as a string because the JSON serializer will convert\n        # the placeholder for worker_replicas (which it sees as a string) into\n        # a quoted variable (eg a string) instead of an unquoted variable\n        # (number).  If worker_replicas is quoted in the spec, it will break in\n        # k8s.  See https://github.com/kubeflow/pipelines/issues/4776\n        worker_spec=worker_spec_create.outputs[\n            \"worker_spec\"\n        ],\n        ttl_seconds_after_finished=ttl_seconds_after_finished,\n        job_timeout_minutes=job_timeout_minutes,\n        delete_after_done=delete_after_done,\n    )\n\n\nif __name__ == \"__main__\":\n    import kfp.compiler as compiler\n\n    pipeline_file = \"pytorch_mnist_pipeline.yaml\"\n    print(\n        f\"Compiling pipeline as {pipeline_file}\"\n    )\n    compiler.Compiler(mode=kfp.dsl.PipelineExecutionMode.V1_LEGACY).compile(\n        mnist_train, pipeline_file\n    )\n    client = kfp_client()\n    client.upload_pipeline(pipeline_package_path=pipeline_file, pipeline_name=\"pytorch_mnist_pipeline\", description=\"pytorch_mnist_pipeline\")\n    print(f\"Created pipeline \")\n#     # To run:\n#     client = kfp.Client()\n#     run = client.create_run_from_pipeline_package(\n#         pipeline_file,\n#         arguments={},\n#         run_name=\"test pytorchjob run\"\n#     )\n#     print(f\"Created run {run}\")\n"
  },
  {
    "repo": "seoeun25/ml-ops",
    "file_path": "kubeflow_pipeline/samples/add/add.py",
    "raw_url": "https://raw.githubusercontent.com/seoeun25/ml-ops/master/kubeflow_pipeline/samples/add/add.py",
    "content": "###\n# $ dsl-compile --py add.py --output add_pipeline.yaml\n# Need manually upload add_pipeline.yaml on kubeflow-pipeline-ui\n#\n###\nimport kfp\nfrom kfp.components import create_component_from_func\n\ndef add(a: int, b: int) -> int:\n    ret = a + b\n    return ret\n\ndef substract(a: int, b: int) -> int:\n    ret = a - b\n    return ret\n\ndef multiply(a: int, b: int) -> int:\n    return a * b\n\nadd_op = create_component_from_func(add)\nsubstract_op = create_component_from_func(substract)\nmultiply_op = create_component_from_func(multiply)\n\nfrom kfp.dsl import pipeline\n@pipeline(name=\"add example\", description=\"example of addition calculation\")\ndef my_pipeline(a: int, b: int):\n    task_1 = add_op(a, b)\n    task_2 = substract_op(a, b)\n    task_3 = multiply_op(task_1.output, task_2.output)\n\n## $ dsl-compile --py add.py --output add_pipeline.yaml\n## upload add_pipeline.yaml on kubeflow-pipeline-ui"
  },
  {
    "repo": "seoeun25/ml-ops",
    "file_path": "kubeflow_pipeline/samples/add/add2.py",
    "raw_url": "https://raw.githubusercontent.com/seoeun25/ml-ops/master/kubeflow_pipeline/samples/add/add2.py",
    "content": "###\n# $ python add_2.py\n# Need manually upload add_pipeline_2.yaml on kubeflow-pipeline-ui\n#\n###\nimport kfp.compiler\nfrom kfp.components import create_component_from_func\n\ndef add(a: int, b: int) -> int:\n    ret = a + b\n    return ret\n\ndef substract(a: int, b: int) -> int:\n    ret = a - b\n    return ret\n\ndef multiply(a: int, b: int) -> int:\n    return a * b\n\nadd_op = create_component_from_func(add)\nsubstract_op = create_component_from_func(substract)\nmultiply_op = create_component_from_func(multiply)\n\nimport kfp.dsl as dsl\n@dsl.pipeline(name='Addition pipeline', description='An example pipeline that perform addition calculations')\ndef my_pipeline(a: int, b: int):\n    task_1 = add_op(a, b)\n    task_2 = substract_op(a, b)\n    task_3 = multiply_op(task_1.output, task_2.output)\n\nif __name__ == \"__main__\":\n    kfp.compiler.Compiler(mode=kfp.dsl.PipelineExecutionMode.V1_LEGACY).compile(my_pipeline, \"./add_pipeline_2.yaml\")\n"
  },
  {
    "repo": "seoeun25/ml-ops",
    "file_path": "kubeflow_pipeline/samples/add/add_cpu.py",
    "raw_url": "https://raw.githubusercontent.com/seoeun25/ml-ops/master/kubeflow_pipeline/samples/add/add_cpu.py",
    "content": "\"\"\"\n# $ python add_cpu.py\n# This will upload add_cpu.yaml. Check this on kubeflow-pipeline-ui\n#\n\"\"\"\n\nimport kfp\nfrom kfp.components import create_component_from_func\nfrom kubernetes.client import V1Toleration, V1Affinity, V1NodeSelector, V1NodeSelectorRequirement, V1NodeSelectorTerm\n\ndef kfp_client():\n    \"\"\"\n    Kubeflow pipelines client inside cluster.\n    \"\"\"\n    end_point=\"http://ml-pipeline.kubeflow.svc.cluster.local:8888\"\n    credentials = kfp.auth.ServiceAccountTokenVolumeCredentials(path=None)\n    client = kfp.Client(host=end_point, credentials=credentials)\n\n    return client\n\ndef add(a: float, b: float) -> float:\n    '''Calculates sum of two arguments'''\n    return a + b\n\nadd_op = create_component_from_func(add, output_component_file='add_component.yaml')\n\nimport kfp.dsl as dsl\n@dsl.pipeline(name=\"First Addition pipeline - python function base\", description=\"example of python function based pipeline\")\ndef add_pipeline(a='1', b='7',):\n    toleration = V1Toleration(\n        effect='NoSchedule',\n        key='cpu.aladin.skt/type',\n        value='common'\n    )\n    # toleration1 = V1Toleration(\n    #     effect='NoSchedule',\n    #     key='gpu.aladin.skt/type',\n    #     value='v100'\n    # )\n\n    first_add_task = add_op(a, 4)\n    first_add_task.add_toleration(toleration)\n    #first_add_task.add_toleration(toleration1)\n    first_add_task.set_cpu_request(\"4\").set_cpu_limit(\"4\").set_memory_request(\"16G\").set_memory_limit(\"16G\")\n    second_add_task = add_op(first_add_task.output, b)\n\n# Specify argument values for your pipeline run.\narguments = {'a': '7', 'b': '8'}\n\n# Create a pipeline run, using the client you initialized in a prior step.\nclient=kfp_client()\nkfp.compiler.Compiler(mode=kfp.dsl.PipelineExecutionMode.V1_LEGACY).compile(add_pipeline, \"./add_cpu.yaml\")\nclient.upload_pipeline(pipeline_package_path=\"./add_cpu.yaml\", pipeline_name=\"add_cpu\", description=\"Addition Python function based pipeline\")\n"
  },
  {
    "repo": "seoeun25/ml-ops",
    "file_path": "kubeflow_pipeline/samples/add/addition_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/seoeun25/ml-ops/master/kubeflow_pipeline/samples/add/addition_pipeline.py",
    "content": "\"\"\"\n# $ python addition_pipeline.py\n# This will upload addition_pipeline.yaml. Check this on kubeflow-pipeline-ui\n#\n\"\"\"\n\nimport kfp\nfrom kfp.components import create_component_from_func\n\ndef kfp_client():\n    \"\"\"\n    Kubeflow pipelines client inside cluster.\n    \"\"\"\n    end_point=\"http://ml-pipeline.kubeflow.svc.cluster.local:8888\"\n    credentials = kfp.auth.ServiceAccountTokenVolumeCredentials(path=None)\n    client = kfp.Client(host=end_point, credentials=credentials)\n\n    return client\n\ndef add(a: float, b: float) -> float:\n    '''Calculates sum of two arguments'''\n    return a + b\n\nadd_op = create_component_from_func(add, output_component_file='add_component.yaml')\n\nimport kfp.dsl as dsl\n@dsl.pipeline(name=\"First Addition pipeline - python function base\", description=\"example of python function based pipeline\")\ndef add_pipeline(\n        a='1',\n        b='7',\n):\n    # Passes a pipeline parameter and a constant value to the `add_op` factory\n    # function.\n    first_add_task = add_op(a, 4)\n    # Passes an output reference from `first_add_task` and a pipeline parameter\n    # to the `add_op` factory function. For operations with a single return\n    # value, the output reference can be accessed as `task.output` or\n    # `task.outputs['output_name']`.\n    second_add_task = add_op(first_add_task.output, b)\n\n# Specify argument values for your pipeline run.\narguments = {'a': '7', 'b': '8'}\n\n# Create a pipeline run, using the client you initialized in a prior step.\nclient=kfp_client()\n## experiments\nlist_experiments = client.list_experiments()\nprint(\"experiments\", )\nfor i in range(list_experiments.total_size):\n    print(list_experiments.experiments[i].id)\n# create experiment\n#client.create_experiment(name=\"add_org\", description=\"addition pipeline using python function base\")\n\n# creat run. not pipeline\n#client.create_run_from_pipeline_func(add_pipeline, arguments=arguments)\n# compile: create pipeline.yaml\nkfp.compiler.Compiler(mode=kfp.dsl.PipelineExecutionMode.V1_LEGACY).compile(add_pipeline, \"./addition_pipeline.yaml\")\nclient.upload_pipeline(pipeline_package_path=\"./addition_pipeline.yaml\", pipeline_name=\"addition_pipeline\", description=\"Addition Python function based pipeline\")\n"
  },
  {
    "repo": "seoeun25/ml-ops",
    "file_path": "kubeflow_pipeline/samples/edd_monitor/edd_checker.py",
    "raw_url": "https://raw.githubusercontent.com/seoeun25/ml-ops/master/kubeflow_pipeline/samples/edd_monitor/edd_checker.py",
    "content": "\"\"\"\nThis is pipeline to monitor whether edd data loading is successful.\n    $ python edd_checker.py\n\n    * create and upload edd_checker.yaml(pipeline) to kubeflow.\n    * check ede_monitor at kubeflow.\n\"\"\"\nimport kfp.compiler\nfrom kfp.components import create_component_from_func\nimport kfp\n\ndef kfp_client():\n    \"\"\"\n    Kubeflow pipelines client inside cluster.\n    \"\"\"\n    end_point=\"http://ml-pipeline.kubeflow.svc.cluster.local:8888\"\n    credentials = kfp.auth.ServiceAccountTokenVolumeCredentials(path=None)\n    client = kfp.Client(host=end_point, credentials=credentials)\n\n    return client\n\ndef base_image() -> str:\n    import os\n    import re\n\n    iam = os.environ.get(\"AWS_ROLE_ARN\")\n    account = re.findall(\"arn:aws:iam::(.*):role*\", iam)[0]\n    region = os.environ.get(\"AWS_REGION\")\n    # base_image = \"{}.dkr.ecr.{}.amazonaws.com/aladin-runtime:anaconda-cpu\".format(account, region)\n    base_image = \"{}.dkr.ecr.{}.amazonaws.com/aladin:runtime-common-cpu-202404r1\".format(account, region)\n    print(\"base_image = {}\".format(base_image))\n    return base_image\n\ndef apollo(cur_date: str, debug:bool, args: str) -> str:\n    from aladin import trino\n    from aladin import edd_util\n    from datetime import datetime\n\n    if cur_date is None or cur_date == \"\":\n        cur_date = datetime.today().strftime(\"%Y%m%d\")\n    print(\"cur_date = {}, debug={}, args = {}\".format(cur_date, debug, args))\n\n    def check_last_data(last_date: str, cur_date: str, days: int):\n        if debug:\n            return\n        if last_date < edd_util.date_add(cur_date, days).strftime(\"%Y%m%d\"):\n            raise Exception(\"cur_date = {}, last_date = {}. expected last_date = {} \".format(cur_date, last_date, edd_util.date_add(cur_date, days).strftime(\"%Y%m%d\")))\n\n    catalog_name = \"edd_hive\"\n    schema_name = \"apollo\"\n    def apollo_helper(cur_date: str, args: str):\n        table_name = \"luna_user\"\n        query = f\"SELECT count(*) as cnt FROM {catalog_name}.{schema_name}.{table_name} t \"\n        r1 = edd_util.fetchmany(conn=conn, query=query, size=5, column_name=[\"cnt\"])\n\n        table_name = \"luna_id_apollo_sub\"\n        query = f\"SELECT dt as p_dt FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by dt desc\"\n        r2 = edd_util.fetchmany(conn=conn, query=query, size=5, column_name=[\"p_dt\"])\n        check_last_data(r2, cur_date, -2)\n\n        table_name = \"luna_comm_log\"\n        query = f\"SELECT dt as p_dt FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by dt desc\"\n        r3 = edd_util.fetchmany(conn=conn, query=query, size=5, column_name=[\"p_dt\"])\n        check_last_data(r3, cur_date, -2)\n\n        table_name = \"user_context_log\"\n        query = f\"SELECT dt as p_dt, hh as p_hh FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by dt desc, hh desc\"\n        r4 = edd_util.fetchmany(conn, query=query, size=5, column_name=[\"p_dt\", \"p_hh\"])\n        check_last_data(r4, cur_date, -2)\n\n    conn = trino.connect(catalog=catalog_name, schema=schema_name)\n    try:\n        apollo_helper(cur_date, args)\n    except Exception as error:\n        print(\"-- Exception --\", error)\n        raise\n\n    # Close trino connection\n    conn.close()\n    print(\"Finish {}\".format(schema_name))\n    return \"ok\"\n\ndef cpm(previous:str, cur_date: str, debug:bool, args: str) -> str:\n    from aladin import trino\n    from aladin import edd_util\n    from datetime import datetime\n\n    if cur_date is None or cur_date == \"\":\n        cur_date = datetime.today().strftime(\"%Y%m%d\")\n    print(\"previous={}, cur_date = {}, debug={}, args = {}\".format(previous, cur_date, debug, args))\n\n    def check_last_data(last_date: str, cur_date: str, days: int):\n        if debug:\n            return\n        if last_date < edd_util.date_add(cur_date, days).strftime(\"%Y%m%d\"):\n            raise Exception(\"cur_date = {}, last_date = {}. expected last_date = {} \".format(cur_date, last_date, edd_util.date_add(cur_date, days).strftime(\"%Y%m%d\")))\n\n    catalog_name = \"edd_hive\"\n    schema_name = \"cpm\"\n\n    def cpm_helper(cur_date: str, args: str):\n        table_name = \"life_locationfeature_monthly\"\n        query = f\"SELECT ym as p_ym FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by ym desc\"\n        r1 = edd_util.fetchmany(conn=conn, query=query, size=5, column_name=[\"p_ym\"])\n        check_last_data(r1, cur_date, -60)\n\n        table_name = \"life_visit_poi_monthly\"\n        query = f\"SELECT ym as p_ym FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by ym desc\"\n        r2 = edd_util.fetchmany(conn=conn, query=query, size=5, column_name=[\"p_ym\"])\n        check_last_data(r2, cur_date, -60)\n\n    conn = trino.connect(catalog=catalog_name, schema=schema_name)\n    try:\n        cpm_helper(cur_date, args)\n    except Exception as error:\n        print(\"-- Exception --\", error)\n        raise\n\n    # Close trino connection\n    conn.close()\n    print(\"Finish {}\".format(schema_name))\n    return \"ok\"\n\ndef di_cpm(previous:str, cur_date: str, debug:bool, args: str) -> str:\n    from aladin import trino\n    from aladin import edd_util\n    from datetime import datetime\n\n    if cur_date is None or cur_date == \"\":\n        cur_date = datetime.today().strftime(\"%Y%m%d\")\n    print(\"previous={}, cur_date = {}, debug={}, args = {}\".format(previous, cur_date, debug, args))\n\n    def check_last_data(last_date: str, cur_date: str, days: int):\n        if debug:\n            return\n        if last_date < edd_util.date_add(cur_date, days).strftime(\"%Y%m%d\"):\n            raise Exception(\"cur_date = {}, last_date = {}. expected last_date = {} \".format(cur_date, last_date, edd_util.date_add(cur_date, days).strftime(\"%Y%m%d\")))\n\n    catalog_name = \"edd_hive\"\n    schema_name = \"di_cpm\"\n\n    def di_cpm_helper(cur_date: str, args: str):\n        table_name = \"base_tasa_rel_pred_monthly\"\n        query = f\"SELECT ym as p_ym FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by ym desc\"\n        r1 = edd_util.fetchmany(conn=conn, query=query, size=5, column_name=[\"p_ym\"])\n        check_last_data(r1, cur_date, -60)\n        r2 = edd_util.fetchmany(conn=conn, query=query, size=5, column_name=[\"p_ym\"])\n\n        table_name = \"fmly_hhld_pf_svc_monthly\"\n        query = f\"SELECT ym as p_ym FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by ym desc\"\n        r2 = edd_util.fetchmany(conn=conn, query=query, size=5, column_name=[\"p_ym\"])\n        check_last_data(r2, cur_date, -60)\n\n        table_name = \"fmly_pf_edge_monthly\"\n        query = f\"SELECT ym as p_ym FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by ym desc\"\n        r3 = edd_util.fetchmany(conn=conn, query=query, size=5, column_name=[\"p_ym\"])\n        check_last_data(r3, cur_date, -60)\n\n        table_name = \"general_pf_svc_monthly\"\n        query = f\"SELECT * FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by ym desc\"\n        r4 = edd_util.fetchmany(conn=conn, query=query, size=5, column_name=[\"ym\", \"cat1\", \"cat2\"])\n        check_last_data(r4, cur_date, -60)\n\n        table_name = \"seg_profile_inference_svc_monthly\"\n        query = f\"SELECT * FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by ym desc\"\n        r5 = edd_util.fetchmany(conn=conn, query=query, size=5, column_name=[\"ym\", \"age_group_cd\"])\n        check_last_data(r5, cur_date, -60)\n\n        table_name = \"seg_profile_seg_meta\"\n        query = f\"SELECT count(*) as cnt FROM {catalog_name}.{schema_name}.{table_name}\"\n        r6 = edd_util.fetchmany(conn=conn, query=query, size=5, column_name=[\"cnt\"])\n\n    conn = trino.connect(catalog=catalog_name, schema=schema_name)\n    try:\n        di_cpm_helper(cur_date, args)\n    except Exception as error:\n        print(\"-- Exception --\", error)\n        raise\n\n    # Close trino connection\n    conn.close()\n    print(\"Finish {}\".format(schema_name))\n    return \"ok\"\n\ndef ict_11st_11st(previous:str, cur_date: str, debug:bool, args: str) -> str:\n    from aladin import trino\n    from aladin import edd_util\n    from datetime import datetime\n\n    if cur_date is None or cur_date == \"\":\n        cur_date = datetime.today().strftime(\"%Y%m%d\")\n    print(\"previous={}, cur_date = {}, debug={}, args = {}\".format(previous, cur_date, debug, args))\n\n    def check_last_data(last_date: str, cur_date: str, days: int):\n        if debug:\n            return\n        if last_date < edd_util.date_add(cur_date, days).strftime(\"%Y%m%d\"):\n            raise Exception(\"cur_date = {}, last_date = {}. expected last_date = {} \".format(cur_date, last_date, edd_util.date_add(cur_date, days).strftime(\"%Y%m%d\")))\n\n    catalog_name = \"edd_hive\"\n    schema_name = \"ict_11st_11st\"\n\n    def ict_11st_11st_helper(cur_date: str, args: str):\n        table_name = \"tlounge_itg_agr_st11_dealings\"\n        query = f\"SELECT part_date FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by part_date desc\"\n        r1 = edd_util.fetchmany(conn=conn, query=query, size=5, column_name=[\"part_date\"])\n        check_last_data(r1, cur_date, -3)\n\n        table_name = \"tlounge_itg_agr_st11_member\"\n        query = f\"SELECT part_date FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by part_date desc\"\n        r2 = edd_util.fetchmany(conn=conn, query=query, size=5, column_name=[\"part_date\"])\n        check_last_data(r2, cur_date, -3)\n\n    conn = trino.connect(catalog=catalog_name, schema=schema_name)\n    try:\n        ict_11st_11st_helper(cur_date, args)\n    except Exception as error:\n        print(\"-- Exception --\", error)\n        raise\n    # Close trino connection\n    conn.close()\n    print(\"Finish {}\".format(schema_name))\n    return \"ok\"\n\ndef ict_skb_acc(previous:str, cur_date: str, debug:bool, args: str) -> str:\n    from aladin import trino\n    from aladin import edd_util\n    from datetime import datetime\n\n    if cur_date is None or cur_date == \"\":\n        cur_date = datetime.today().strftime(\"%Y%m%d\")\n    print(\"previous={}, cur_date = {}, debug={}, args = {}\".format(previous, cur_date, debug, args))\n\n    def check_last_data(last_date: str, cur_date: str, days: int):\n        if debug:\n            return\n        if last_date < edd_util.date_add(cur_date, days).strftime(\"%Y%m%d\"):\n            raise Exception(\"cur_date = {}, last_date = {}. expected last_date = {} \".format(cur_date, last_date, edd_util.date_add(cur_date, days).strftime(\"%Y%m%d\")))\n\n    catalog_name = \"edd_hive\"\n    schema_name = \"ict_skb_acc\"\n\n    def ict_skb_acc_helper(cur_date: str, args: str):\n        table_name = \"cc_svc_prst_month\"\n        query = f\"SELECT ym as p_ym FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by ym desc\"\n        r1 = edd_util.fetchmany(conn=conn, query=query, size=5, column_name=[\"p_ym\"])\n        check_last_data(r1, cur_date, -60)\n\n    conn = trino.connect(catalog=catalog_name, schema=schema_name)\n    try:\n        ict_skb_acc_helper(cur_date, args)\n    except Exception as error:\n        print(\"-- Exception --\", error)\n        raise\n    # Close trino connection\n    conn.close()\n    print(\"Finish {}\".format(schema_name))\n    return \"ok\"\n\ndef ict_skt_common(previous:str, cur_date: str, debug:bool, args: str) -> str:\n    from aladin import trino\n    from aladin import edd_util\n    from datetime import datetime\n\n    if cur_date is None or cur_date == \"\":\n        cur_date = datetime.today().strftime(\"%Y%m%d\")\n    print(\"previous={}, cur_date = {}, debug={}, args = {}\".format(previous, cur_date, debug, args))\n\n    def check_last_data(last_date: str, cur_date: str, days: int):\n        if debug:\n            return\n        if last_date < edd_util.date_add(cur_date, days).strftime(\"%Y%m%d\"):\n            raise Exception(\"cur_date = {}, last_date = {}. expected last_date = {} \".format(cur_date, last_date, edd_util.date_add(cur_date, days).strftime(\"%Y%m%d\")))\n\n    catalog_name = \"edd_hive\"\n    schema_name = \"ict_skt_common\"\n\n    def ict_skt_common_helper(cur_date: str, args: str):\n        table_name = \"ci_mst_u14\"\n        query = f\"SELECT svc_name FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" \"\n        edd_util.fetchmany(conn=conn, query=query, size=5, column_name=[\"svc_name\"])\n        query = f\"SELECT count(*) as cnt FROM {catalog_name}.{schema_name}.{table_name} where svc_name='mobile'\"\n        edd_util.fetchmany(conn=conn, query=query, size=5, column_name=[\"cnt\"])\n\n        table_name = \"ict_key_mst\"\n        query = f\"SELECT site FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" \"\n        edd_util.fetchmany(conn=conn, query=query, size=5, column_name=[\"site\"])\n        query = f\"SELECT count(*) as cnt FROM {catalog_name}.{schema_name}.{table_name} where site='11st'\"\n        edd_util.fetchmany(conn=conn, query=query, size=5, column_name=[\"cnt\"])\n\n        table_name = \"ict_key_mst_tmm\"\n        query = f\"SELECT site FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" \"\n        edd_util.fetchmany(conn=conn, query=query, size=5, column_name=[\"site\"])\n        query = f\"SELECT count(*) as cnt FROM {catalog_name}.{schema_name}.{table_name} where site='11st'\"\n        edd_util.fetchmany(conn=conn, query=query, size=5, column_name=[\"cnt\"])\n\n    conn = trino.connect(catalog=catalog_name, schema=schema_name)\n    try:\n        ict_skt_common_helper(cur_date, args)\n    except Exception as error:\n        print(\"-- Exception --\", error)\n        raise\n    # Close trino connection\n    conn.close()\n    print(\"Finish {}\".format(schema_name))\n    return \"ok\"\n\ndef ict_tmm_tmap(previous:str, cur_date: str, debug:bool, args: str) -> str:\n    from aladin import trino\n    from aladin import edd_util\n    from datetime import datetime\n\n    if cur_date is None or cur_date == \"\":\n        cur_date = datetime.today().strftime(\"%Y%m%d\")\n    print(\"previous={}, cur_date = {}, debug={}, args = {}\".format(previous, cur_date, debug, args))\n\n    def check_last_data(last_date: str, cur_date: str, days: int):\n        if debug:\n            return\n        if last_date < edd_util.date_add(cur_date, days).strftime(\"%Y%m%d\"):\n            raise Exception(\"cur_date = {}, last_date = {}. expected last_date = {} \".format(cur_date, last_date, edd_util.date_add(cur_date, days).strftime(\"%Y%m%d\")))\n\n    catalog_name = \"edd_hive\"\n    schema_name = \"ict_tmm_tmap\"\n\n    def ict_tmm_tmap_helper(cur_date: str, args: str):\n        table_name = \"tmap_favorate\"\n        query = f\"SELECT dt as p_dt FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by dt desc\"\n        r1 = edd_util.fetchmany(conn=conn, query=query, size=5, column_name=[\"p_dt\"])\n        check_last_data(r1, cur_date, -2)\n\n        table_name = \"tmap_poimeta\"\n        query = f\"SELECT dt as p_dt FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by dt desc\"\n        r2 = edd_util.fetchmany(conn=conn, query=query, size=5, column_name=[\"p_dt\"])\n        check_last_data(r2, cur_date, -2)\n\n        table_name = \"tmap_routehistory\"\n        query = f\"SELECT dt as p_dt, hh as p_hh FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by dt desc\"\n        r3 = edd_util.fetchmany(conn=conn, query=query, size=5, column_name=[\"p_dt\", \"p_hh\"])\n        check_last_data(r3, cur_date, -2)\n\n        table_name = \"tmap_rprsd\"\n        query = f\"SELECT dt as p_dt, hh as p_hh FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by dt desc\"\n        r4 = edd_util.fetchmany(conn=conn, query=query, size=5, column_name=[\"p_dt\", \"p_hh\"])\n        check_last_data(r4, cur_date, -2)\n\n    conn = trino.connect(catalog=catalog_name, schema=schema_name)\n    try:\n        ict_tmm_tmap_helper(cur_date, args)\n    except Exception as error:\n        print(\"-- Exception --\", error)\n        raise\n    # Close trino connection\n    conn.close()\n    print(\"Finish {}\".format(schema_name))\n    return \"ok\"\n\ndef litmus(previous:str, cur_date: str, debug:bool, args: str) -> str:\n    from aladin import trino\n    from aladin import edd_util\n    from datetime import datetime\n\n    if cur_date is None or cur_date == \"\":\n        cur_date = datetime.today().strftime(\"%Y%m%d\")\n    print(\"previous={}, cur_date = {}, debug={}, args = {}\".format(previous, cur_date, debug, args))\n\n    def check_last_data(last_date: str, cur_date: str, days: int):\n        if debug:\n            return\n        if last_date < edd_util.date_add(cur_date, days).strftime(\"%Y%m%d\"):\n            raise Exception(\"cur_date = {}, last_date = {}. expected last_date = {} \".format(cur_date, last_date, edd_util.date_add(cur_date, days).strftime(\"%Y%m%d\")))\n\n    catalog_name = \"edd_hive\"\n    schema_name = \"litmus\"\n\n    def litmus_helper(cur_date: str, args: str):\n        table_name = \"litmus_trip\"\n        query = f\"SELECT dt as p_dt FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by dt desc\"\n        r1 = edd_util.fetchmany(conn=conn, query=query, size=5, column_name=[\"p_dt\"])\n        check_last_data(r1, cur_date, -5)\n\n    conn = trino.connect(catalog=catalog_name, schema=schema_name)\n    try:\n        litmus_helper(cur_date, args)\n    except Exception as error:\n        print(\"-- Exception --\", error)\n        raise\n    # Close trino connection\n    conn.close()\n    print(\"Finish {}\".format(schema_name))\n    return \"ok\"\n\ndef loc_meta_raw(previous:str, cur_date: str, debug:bool, args: str) -> str:\n    from aladin import trino\n    from aladin import edd_util\n    from datetime import datetime\n\n    if cur_date is None or cur_date == \"\":\n        cur_date = datetime.today().strftime(\"%Y%m%d\")\n    print(\"previous={}, cur_date = {}, debug={}, args = {}\".format(previous, cur_date, debug, args))\n\n    def check_last_data(last_date: str, cur_date: str, days: int):\n        if debug:\n            return\n        if last_date < edd_util.date_add(cur_date, days).strftime(\"%Y%m%d\"):\n            raise Exception(\"cur_date = {}, last_date = {}. expected last_date = {} \".format(cur_date, last_date, edd_util.date_add(cur_date, days).strftime(\"%Y%m%d\")))\n\n    catalog_name = \"edd_hive\"\n    schema_name = \"loc_meta_raw\"\n\n    def loc_meta_raw_helper(cur_date: str, args: str):\n        table_name = \"enb_base\"\n        query = f\"SELECT dt as p_dt FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by dt desc\"\n        r1 = edd_util.fetchmany(conn=conn, query=query, size=5, column_name=[\"p_dt\"])\n        check_last_data(r1, cur_date, -2)\n\n    conn = trino.connect(catalog=catalog_name, schema=schema_name)\n    try:\n        loc_meta_raw_helper(cur_date, args)\n    except Exception as error:\n        print(\"-- Exception --\", error)\n        raise\n    # Close trino connection\n    conn.close()\n    print(\"Finish {}\".format(schema_name))\n    return \"ok\"\n\nbase_image = base_image()\napollo_op = create_component_from_func(apollo, base_image=base_image)\ncpm_op = create_component_from_func(cpm, base_image=base_image)\ndi_cpm_op = create_component_from_func(di_cpm, base_image=base_image)\nict_11st_11st_op = create_component_from_func(ict_11st_11st, base_image=base_image)\nict_skb_acc_op = create_component_from_func(ict_skb_acc, base_image=base_image)\nict_skt_common_op = create_component_from_func(ict_skt_common, base_image=base_image)\nict_tmm_tmap_op = create_component_from_func(ict_tmm_tmap, base_image=base_image)\nlitmus_op = create_component_from_func(litmus, base_image=base_image)\nloc_meta_raw_op = create_component_from_func(loc_meta_raw, base_image=base_image)\n\nimport kfp.dsl as dsl\n@dsl.pipeline(name='edd_checker_pipeline', description='EDD data loading monitor')\ndef my_pipeline(cur_date: str, debug: bool, args: str):\n    print(\"my_pipeline: cur_date={}, debug={}, args={}\".format(cur_date, debug, args))\n    data_op = dsl.VolumeOp(name=\"data-pvc\",\n                           resource_name=\"data-volume\",\n                           generate_unique_name=False,\n                           action='apply',\n                           size=\"10Gi\",\n                           modes=dsl.VOLUME_MODE_RWO)\n    task_1 = apollo_op(cur_date=cur_date, debug=debug, args=args).add_pvolumes({\"/data\": data_op.volume})\n    task_2 = cpm_op(previous=task_1.output, cur_date=cur_date, debug=debug, args=args)\n    task_3 = di_cpm_op(task_2.output, cur_date=cur_date, debug=debug, args=args)\n    task_4 = ict_11st_11st_op(task_3.output, cur_date=cur_date, debug=debug, args=args)\n    task_5 = ict_skb_acc_op(task_4.output, cur_date=cur_date, debug=debug, args=args)\n    task_6 = ict_skt_common_op(task_5.output, cur_date=cur_date, debug=debug, args=args)\n    task_7 = ict_tmm_tmap_op(task_6.output, cur_date=cur_date, debug=debug, args=args)\n    task_8 = litmus_op(task_7.output, cur_date=cur_date, debug=debug, args=args)\n    task_9 = loc_meta_raw_op(task_8.output, cur_date=cur_date, debug=debug, args=args)\n\nif __name__ == \"__main__\":\n    kfp.compiler.Compiler(mode=kfp.dsl.PipelineExecutionMode.V1_LEGACY).compile(my_pipeline, \"./edd_checker.yaml\")\n    client = kfp_client()\n    client.upload_pipeline(pipeline_package_path=\"./edd_checker.yaml\", pipeline_name=\"edd_checker\", description=\"EDD Checker pipeline\")\n\n"
  },
  {
    "repo": "seoeun25/ml-ops",
    "file_path": "kubeflow_pipeline/samples/edd_monitor/edd_monitor.py",
    "raw_url": "https://raw.githubusercontent.com/seoeun25/ml-ops/master/kubeflow_pipeline/samples/edd_monitor/edd_monitor.py",
    "content": "\"\"\"\nThis is pipeline to monitor whether edd data loading is successful.\n    $ python edd_monitor.py\n\n    * create and upload edd_monitor.yaml(pipeline) to kubeflow.\n    * check ede_monitor at kubeflow.\n\"\"\"\nimport kfp.compiler\nfrom kfp.components import create_component_from_func\nimport kfp\nfrom datetime import datetime\n\ndef kfp_client():\n    \"\"\"\n    Kubeflow pipelines client inside cluster.\n    \"\"\"\n    end_point=\"http://ml-pipeline.kubeflow.svc.cluster.local:8888\"\n    credentials = kfp.auth.ServiceAccountTokenVolumeCredentials(path=None)\n    client = kfp.Client(host=end_point, credentials=credentials)\n\n    return client\n\ndef base_image() -> str:\n    import os\n    import re\n\n    iam = os.environ.get(\"AWS_ROLE_ARN\")\n    account = re.findall(\"arn:aws:iam::(.*):role*\", iam)[0]\n    region = os.environ.get(\"AWS_REGION\")\n    base_image = \"{}.dkr.ecr.{}.amazonaws.com/aladin-runtime:anaconda-cpu\".format(account, region)\n    print(\"base_image = {}\".format(base_image))\n    return base_image\n\ndef apollo(cur_date: str, debug:bool, args: str) -> str:\n    import pandas as pd\n    from datetime import datetime, timedelta\n    if cur_date is None or cur_date == \"\":\n        cur_date = datetime.today().strftime(\"%Y%m%d\")\n    print(\"cur_date = {}, debug={}, args = {}\".format(cur_date, debug, args))\n\n    schema_name = \"apollo\"\n\n    def log_error(schema_name: str, query: str, error: str):\n        print(\"[{}], error={}, query={}\".format(schema_name, error, query))\n\n    def check_last_data(last_date: str, cur_date: str, days: int):\n        if debug:\n            return\n        if last_date < date_add(cur_date, days).strftime(\"%Y%m%d\"):\n            raise Exception(\"cur_date = {}, last_date = {}. expected last_date = {} \".format(cur_date, last_date, date_add(cur_date, days).strftime(\"%Y%m%d\")))\n    def date_add(cur_date:str, days: int) -> datetime:\n        \"\"\"\n        Convert date string to datetime.\n        ex> date_deleta(\"20231020\" -2): datetime object (2023-10-18 00:00:00)\n        :param cur_date: cur_date in %Y%m%d format\n        :param days: delta days. plus or minus days\n        :return: datetime\n        \"\"\"\n        return datetime.strptime(cur_date, \"%Y%m%d\") + timedelta(days=days)\n\n    def fetchmany(query: str, size: int, column_name: list) -> object:\n        \"\"\"\n        Fetch data.\n\n        Parameters\n        ----------\n        query: string. SQL.\n        size: int. fetch rows\n        column_name: list. fetch columns\n\n        Returns\n        -------\n        object: first row, first column data\n        \"\"\"\n        print(\"----------------------\")\n        print(query)\n        cursor.execute(query)\n        df = pd.DataFrame(cursor.fetchmany(size=size), columns=column_name)\n        print(\"rows =\", len(df))\n        print(\"columns = \", df.columns)\n        min_row = len(df)\n\n        for i in range(min_row):\n            projection = []\n            for j in range(len(column_name)):\n                projection.append(df.loc[i][column_name[j]])\n                if i == 0 and j == 0:\n                    result = df.loc[i][column_name[j]]\n            print(projection)\n\n        return result\n\n    catalog_name = \"edd_hive\"\n    def apollo_helper(cur_date: str, args: str):\n        table_name = \"luna_user\"\n        query = f\"SELECT count(*) as cnt FROM {catalog_name}.{schema_name}.{table_name} t \"\n        r1 = fetchmany(query, 5, [\"cnt\"])\n\n        table_name = \"luna_id_apollo_sub\"\n        query = f\"SELECT dt as p_dt FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by dt desc\"\n        r2 = fetchmany(query, 5, [\"p_dt\"])\n        check_last_data(r2, cur_date, -2)\n\n        table_name = \"luna_comm_log\"\n        query = f\"SELECT dt as p_dt FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by dt desc\"\n        r3 = fetchmany(query, 5, [\"p_dt\"])\n        check_last_data(r3, cur_date, -2)\n\n        table_name = \"user_context_log\"\n        query = f\"SELECT dt as p_dt, hh as p_hh FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by dt desc, hh desc\"\n        r4 = fetchmany(query, 5, [\"p_dt\", \"p_hh\"])\n        check_last_data(r4, cur_date, -2)\n\n    from aladin import trino\n    conn = trino.connect(catalog=catalog_name, schema=schema_name)\n    cursor = conn.cursor()\n    try:\n        apollo_helper(cur_date, args)\n    except Exception as error:\n        print(\"-- Exception --\", error)\n        raise\n\n    # Close trino connection\n    conn.close()\n    print(\"Finish {}\".format(schema_name))\n    return \"ok\"\n\ndef cpm(previous:str, cur_date: str, debug:bool, args: str) -> str:\n    import pandas as pd\n    from datetime import datetime, timedelta\n    if cur_date is None or cur_date == \"\":\n        cur_date = datetime.today().strftime(\"%Y%m%d\")\n    print(\"previous={}, cur_date = {}, debug={}, args = {}\".format(previous, cur_date, debug, args))\n\n    schema_name = \"cpm\"\n    def log_error(schema_name: str, query: str, error: str):\n        print(\"[{}], error={}, query={}\".format(schema_name, error, query))\n\n    def check_last_data(last_date: str, cur_date: str, days: int):\n        if debug:\n            return\n        if last_date < date_add(cur_date, days).strftime(\"%Y%m%d\"):\n            raise Exception(\"cur_date = {}, last_date = {}. expected last_date = {} \".format(cur_date, last_date, date_add(cur_date, days).strftime(\"%Y%m%d\")))\n    def date_add(cur_date:str, days: int) -> datetime:\n        return datetime.strptime(cur_date, \"%Y%m%d\") + timedelta(days=days)\n\n    def fetchmany(query: str, size: int, column_name: list) -> object:\n        print(\"----------------------\")\n        print(query)\n        cursor.execute(query)\n        df = pd.DataFrame(cursor.fetchmany(size=size), columns=column_name)\n        print(\"rows =\", len(df))\n        print(\"columns = \", df.columns)\n        min_row = len(df)\n\n        for i in range(min_row):\n            projection = []\n            for j in range(len(column_name)):\n                projection.append(df.loc[i][column_name[j]])\n                if i == 0 and j == 0:\n                    result = df.loc[i][column_name[j]]\n            print(projection)\n\n        return result\n\n    catalog_name = \"edd_hive\"\n    def cpm_helper(cur_date: str, args: str):\n        table_name = \"life_locationfeature_monthly\"\n        query = f\"SELECT ym as p_ym FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by ym desc\"\n        r1 = fetchmany(query=query, size=5, column_name=[\"p_ym\"])\n        check_last_data(r1, cur_date, -60)\n\n        table_name = \"life_visit_poi_monthly\"\n        query = f\"SELECT ym as p_ym FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by ym desc\"\n        r2 = fetchmany(query=query, size=5, column_name=[\"p_ym\"])\n        check_last_data(r2, cur_date, -60)\n\n    from aladin import trino\n    conn = trino.connect(catalog=catalog_name, schema=schema_name)\n    cursor = conn.cursor()\n    try:\n        cpm_helper(cur_date, args)\n    except Exception as error:\n        print(\"-- Exception --\", error)\n        raise\n\n    # Close trino connection\n    conn.close()\n    print(\"Finish {}\".format(schema_name))\n    return \"ok\"\n\ndef di_cpm(previous:str, cur_date: str, debug:bool, args: str) -> str:\n    import pandas as pd\n    from datetime import datetime, timedelta\n    if cur_date is None or cur_date == \"\":\n        cur_date = datetime.today().strftime(\"%Y%m%d\")\n    print(\"previous={}, cur_date = {}, debug={}, args = {}\".format(previous, cur_date, debug, args))\n\n    schema_name = \"di_cpm\"\n    def check_last_data(last_date: str, cur_date: str, days: int):\n        if debug:\n            return\n        if last_date < date_add(cur_date, days).strftime(\"%Y%m%d\"):\n            raise Exception(\"cur_date = {}, last_date = {}. expected last_date = {} \".format(cur_date, last_date, date_add(cur_date, days).strftime(\"%Y%m%d\")))\n    def date_add(cur_date:str, days: int) -> datetime:\n        return datetime.strptime(cur_date, \"%Y%m%d\") + timedelta(days=days)\n\n    def fetchmany(query: str, size: int, column_name: list) -> object:\n        print(\"----------------------\")\n        print(query)\n        cursor.execute(query)\n        df = pd.DataFrame(cursor.fetchmany(size=size), columns=column_name)\n        print(\"rows =\", len(df))\n        print(\"columns = \", df.columns)\n        min_row = len(df)\n\n        for i in range(min_row):\n            projection = []\n            for j in range(len(column_name)):\n                projection.append(df.loc[i][column_name[j]])\n                if i == 0 and j == 0:\n                    result = df.loc[i][column_name[j]]\n            print(projection)\n\n        return result\n\n    catalog_name = \"edd_hive\"\n    def di_cpm_helper(cur_date: str, args: str):\n        table_name = \"base_tasa_rel_pred_monthly\"\n        query = f\"SELECT ym as p_ym FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by ym desc\"\n        r1 = fetchmany(query=query, size=5, column_name=[\"p_ym\"])\n        check_last_data(r1, cur_date, -60)\n        r2 = fetchmany(query=query, size=5, column_name=[\"p_ym\"])\n\n        table_name = \"fmly_hhld_pf_svc_monthly\"\n        query = f\"SELECT ym as p_ym FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by ym desc\"\n        r2 = fetchmany(query=query, size=5, column_name=[\"p_ym\"])\n        check_last_data(r2, cur_date, -60)\n\n        table_name = \"fmly_pf_edge_monthly\"\n        query = f\"SELECT ym as p_ym FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by ym desc\"\n        r3 = fetchmany(query=query, size=5, column_name=[\"p_ym\"])\n        check_last_data(r3, cur_date, -60)\n\n        table_name = \"general_pf_svc_monthly\"\n        query = f\"SELECT * FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by ym desc\"\n        r4 = fetchmany(query=query, size=5, column_name=[\"ym\", \"cat1\", \"cat2\"])\n        check_last_data(r4, cur_date, -60)\n\n        table_name = \"seg_profile_inference_svc_monthly\"\n        query = f\"SELECT * FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by ym desc\"\n        r5 = fetchmany(query=query, size=5, column_name=[\"ym\", \"age_group_cd\"])\n        check_last_data(r5, cur_date, -60)\n\n        table_name = \"seg_profile_seg_meta\"\n        query = f\"SELECT count(*) as cnt FROM {catalog_name}.{schema_name}.{table_name}\"\n        r6 = fetchmany(query=query, size=5, column_name=[\"cnt\"])\n\n    from aladin import trino\n    conn = trino.connect(catalog=catalog_name, schema=schema_name)\n    cursor = conn.cursor()\n    try:\n        di_cpm_helper(cur_date, args)\n    except Exception as error:\n        print(\"-- Exception --\", error)\n        raise\n\n    # Close trino connection\n    conn.close()\n    print(\"Finish {}\".format(schema_name))\n    return \"ok\"\n\ndef ict_11st_11st(previous:str, cur_date: str, debug:bool, args: str) -> str:\n    import pandas as pd\n    from datetime import datetime, timedelta\n    if cur_date is None or cur_date == \"\":\n        cur_date = datetime.today().strftime(\"%Y%m%d\")\n    print(\"previous={}, cur_date = {}, debug={}, args = {}\".format(previous, cur_date, debug, args))\n\n    schema_name = \"ict_11st_11st\"\n    def check_last_data(last_date: str, cur_date: str, days: int):\n        if debug:\n            return\n        if last_date < date_add(cur_date, days).strftime(\"%Y%m%d\"):\n            raise Exception(\"cur_date = {}, last_date = {}. expected last_date = {} \".format(cur_date, last_date, date_add(cur_date, days).strftime(\"%Y%m%d\")))\n    def date_add(cur_date:str, days: int) -> datetime:\n        return datetime.strptime(cur_date, \"%Y%m%d\") + timedelta(days=days)\n\n    def fetchmany(query: str, size: int, column_name: list) -> object:\n        print(\"----------------------\")\n        print(query)\n        cursor.execute(query)\n        df = pd.DataFrame(cursor.fetchmany(size=size), columns=column_name)\n        print(\"rows =\", len(df))\n        print(\"columns = \", df.columns)\n        min_row = len(df)\n\n        for i in range(min_row):\n            projection = []\n            for j in range(len(column_name)):\n                projection.append(df.loc[i][column_name[j]])\n                if i == 0 and j == 0:\n                    result = df.loc[i][column_name[j]]\n            print(projection)\n\n        return result\n\n    catalog_name = \"edd_hive\"\n    def ict_11st_11st_helper(cur_date: str, args: str):\n        table_name = \"tlounge_itg_agr_st11_dealings\"\n        query = f\"SELECT part_date FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by part_date desc\"\n        r1 = fetchmany(query=query, size=5, column_name=[\"part_date\"])\n        check_last_data(r1, cur_date, -3)\n\n        table_name = \"tlounge_itg_agr_st11_member\"\n        query = f\"SELECT part_date FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by part_date desc\"\n        r2 = fetchmany(query=query, size=5, column_name=[\"part_date\"])\n        check_last_data(r2, cur_date, -3)\n\n    from aladin import trino\n    conn = trino.connect(catalog=catalog_name, schema=schema_name)\n    cursor = conn.cursor()\n    try:\n        ict_11st_11st_helper(cur_date, args)\n    except Exception as error:\n        print(\"-- Exception --\", error)\n        raise\n    # Close trino connection\n    conn.close()\n    print(\"Finish {}\".format(schema_name))\n    return \"ok\"\n\ndef ict_skb_acc(previous:str, cur_date: str, debug:bool, args: str) -> str:\n    import pandas as pd\n    from datetime import datetime, timedelta\n    if cur_date is None or cur_date == \"\":\n        cur_date = datetime.today().strftime(\"%Y%m%d\")\n    print(\"previous={}, cur_date = {}, debug={}, args = {}\".format(previous, cur_date, debug, args))\n\n    schema_name = \"ict_skb_acc\"\n    def check_last_data(last_date: str, cur_date: str, days: int):\n        if debug:\n            return\n        if last_date < date_add(cur_date, days).strftime(\"%Y%m%d\"):\n            raise Exception(\"cur_date = {}, last_date = {}. expected last_date = {} \".format(cur_date, last_date, date_add(cur_date, days).strftime(\"%Y%m%d\")))\n    def date_add(cur_date:str, days: int) -> datetime:\n        return datetime.strptime(cur_date, \"%Y%m%d\") + timedelta(days=days)\n\n    def fetchmany(query: str, size: int, column_name: list) -> object:\n        print(\"----------------------\")\n        print(query)\n        cursor.execute(query)\n        df = pd.DataFrame(cursor.fetchmany(size=size), columns=column_name)\n        print(\"rows =\", len(df))\n        print(\"columns = \", df.columns)\n        min_row = len(df)\n\n        for i in range(min_row):\n            projection = []\n            for j in range(len(column_name)):\n                projection.append(df.loc[i][column_name[j]])\n                if i == 0 and j == 0:\n                    result = df.loc[i][column_name[j]]\n            print(projection)\n\n        return result\n\n    catalog_name = \"edd_hive\"\n    def ict_skb_acc_helper(cur_date: str, args: str):\n        table_name = \"cc_svc_prst_month\"\n        query = f\"SELECT ym as p_ym FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by ym desc\"\n        r1 = fetchmany(query=query, size=5, column_name=[\"p_ym\"])\n        check_last_data(r1, cur_date, -60)\n\n    from aladin import trino\n    conn = trino.connect(catalog=catalog_name, schema=schema_name)\n    cursor = conn.cursor()\n    try:\n        ict_skb_acc_helper(cur_date, args)\n    except Exception as error:\n        print(\"-- Exception --\", error)\n        raise\n    # Close trino connection\n    conn.close()\n    print(\"Finish {}\".format(schema_name))\n    return \"ok\"\n\ndef ict_skt_common(previous:str, cur_date: str, debug:bool, args: str) -> str:\n    import pandas as pd\n    from datetime import datetime, timedelta\n    if cur_date is None or cur_date == \"\":\n        cur_date = datetime.today().strftime(\"%Y%m%d\")\n    print(\"previous={}, cur_date = {}, debug={}, args = {}\".format(previous, cur_date, debug, args))\n\n    schema_name = \"ict_skt_common\"\n    def check_last_data(last_date: str, cur_date: str, days: int):\n        if debug:\n            return\n        if last_date < date_add(cur_date, days).strftime(\"%Y%m%d\"):\n            raise Exception(\"cur_date = {}, last_date = {}. expected last_date = {} \".format(cur_date, last_date, date_add(cur_date, days).strftime(\"%Y%m%d\")))\n    def date_add(cur_date:str, days: int) -> datetime:\n        return datetime.strptime(cur_date, \"%Y%m%d\") + timedelta(days=days)\n\n    def fetchmany(query: str, size: int, column_name: list) -> object:\n        print(\"----------------------\")\n        print(query)\n        cursor.execute(query)\n        df = pd.DataFrame(cursor.fetchmany(size=size), columns=column_name)\n        print(\"rows =\", len(df))\n        print(\"columns = \", df.columns)\n        min_row = len(df)\n\n        for i in range(min_row):\n            projection = []\n            for j in range(len(column_name)):\n                projection.append(df.loc[i][column_name[j]])\n                if i == 0 and j == 0:\n                    result = df.loc[i][column_name[j]]\n            print(projection)\n\n        return result\n\n    catalog_name = \"edd_hive\"\n    def ict_skt_common_helper(cur_date: str, args: str):\n        table_name = \"ci_mst_u14\"\n        query = f\"SELECT svc_name FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" \"\n        fetchmany(query=query, size=5, column_name=[\"svc_name\"])\n        query = f\"SELECT count(*) as cnt FROM {catalog_name}.{schema_name}.{table_name} where svc_name='mobile'\"\n        fetchmany(query=query, size=5, column_name=[\"cnt\"])\n\n        table_name = \"ict_key_mst\"\n        query = f\"SELECT site FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" \"\n        fetchmany(query=query, size=5, column_name=[\"site\"])\n        query = f\"SELECT count(*) as cnt FROM {catalog_name}.{schema_name}.{table_name} where site='11st'\"\n        fetchmany(query=query, size=5, column_name=[\"cnt\"])\n\n        table_name = \"ict_key_mst_tmm\"\n        query = f\"SELECT site FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" \"\n        fetchmany(query=query, size=5, column_name=[\"site\"])\n        query = f\"SELECT count(*) as cnt FROM {catalog_name}.{schema_name}.{table_name} where site='11st'\"\n        fetchmany(query=query, size=5, column_name=[\"cnt\"])\n\n    from aladin import trino\n    conn = trino.connect(catalog=catalog_name, schema=schema_name)\n    cursor = conn.cursor()\n    try:\n        ict_skt_common_helper(cur_date, args)\n    except Exception as error:\n        print(\"-- Exception --\", error)\n        raise\n    # Close trino connection\n    conn.close()\n    print(\"Finish {}\".format(schema_name))\n    return \"ok\"\n\ndef ict_tmm_tmap(previous:str, cur_date: str, debug:bool, args: str) -> str:\n    import pandas as pd\n    from datetime import datetime, timedelta\n    if cur_date is None or cur_date == \"\":\n        cur_date = datetime.today().strftime(\"%Y%m%d\")\n    print(\"previous={}, cur_date = {}, debug={}, args = {}\".format(previous, cur_date, debug, args))\n\n    schema_name = \"ict_tmm_tmap\"\n    def check_last_data(last_date: str, cur_date: str, days: int):\n        if debug:\n            return\n        if last_date < date_add(cur_date, days).strftime(\"%Y%m%d\"):\n            raise Exception(\"cur_date = {}, last_date = {}. expected last_date = {} \".format(cur_date, last_date, date_add(cur_date, days).strftime(\"%Y%m%d\")))\n    def date_add(cur_date:str, days: int) -> datetime:\n        return datetime.strptime(cur_date, \"%Y%m%d\") + timedelta(days=days)\n\n    def fetchmany(query: str, size: int, column_name: list) -> object:\n        print(\"----------------------\")\n        print(query)\n        cursor.execute(query)\n        df = pd.DataFrame(cursor.fetchmany(size=size), columns=column_name)\n        print(\"rows =\", len(df))\n        print(\"columns = \", df.columns)\n        min_row = len(df)\n\n        for i in range(min_row):\n            projection = []\n            for j in range(len(column_name)):\n                projection.append(df.loc[i][column_name[j]])\n                if i == 0 and j == 0:\n                    result = df.loc[i][column_name[j]]\n            print(projection)\n\n        return result\n\n    catalog_name = \"edd_hive\"\n    def ict_tmm_tmap_helper(cur_date: str, args: str):\n        table_name = \"tmap_favorate\"\n        query = f\"SELECT dt as p_dt FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by dt desc\"\n        r1 = fetchmany(query, 5, [\"p_dt\"])\n        check_last_data(r1, cur_date, -2)\n\n        table_name = \"tmap_poimeta\"\n        query = f\"SELECT dt as p_dt FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by dt desc\"\n        r2 = fetchmany(query, 5, [\"p_dt\"])\n        check_last_data(r2, cur_date, -2)\n\n        table_name = \"tmap_routehistory\"\n        query = f\"SELECT dt as p_dt, hh as p_hh FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by dt desc\"\n        r3 = fetchmany(query, 5, [\"p_dt\", \"p_hh\"])\n        check_last_data(r3, cur_date, -2)\n\n        table_name = \"tmap_rprsd\"\n        query = f\"SELECT dt as p_dt, hh as p_hh FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by dt desc\"\n        r4 = fetchmany(query, 5, [\"p_dt\", \"p_hh\"])\n        check_last_data(r4, cur_date, -2)\n\n    from aladin import trino\n    conn = trino.connect(catalog=catalog_name, schema=schema_name)\n    cursor = conn.cursor()\n    try:\n        ict_tmm_tmap_helper(cur_date, args)\n    except Exception as error:\n        print(\"-- Exception --\", error)\n        raise\n    # Close trino connection\n    conn.close()\n    print(\"Finish {}\".format(schema_name))\n    return \"ok\"\n\ndef litmus(previous:str, cur_date: str, debug:bool, args: str) -> str:\n    import pandas as pd\n    from datetime import datetime, timedelta\n    if cur_date is None or cur_date == \"\":\n        cur_date = datetime.today().strftime(\"%Y%m%d\")\n    print(\"previous={}, cur_date = {}, debug={}, args = {}\".format(previous, cur_date, debug, args))\n\n    schema_name = \"litmus\"\n    def check_last_data(last_date: str, cur_date: str, days: int):\n        if debug:\n            return\n        if last_date < date_add(cur_date, days).strftime(\"%Y%m%d\"):\n            raise Exception(\"cur_date = {}, last_date = {}. expected last_date = {} \".format(cur_date, last_date, date_add(cur_date, days).strftime(\"%Y%m%d\")))\n    def date_add(cur_date:str, days: int) -> datetime:\n        return datetime.strptime(cur_date, \"%Y%m%d\") + timedelta(days=days)\n\n    def fetchmany(query: str, size: int, column_name: list) -> object:\n        print(\"----------------------\")\n        print(query)\n        cursor.execute(query)\n        df = pd.DataFrame(cursor.fetchmany(size=size), columns=column_name)\n        print(\"rows =\", len(df))\n        print(\"columns = \", df.columns)\n        min_row = len(df)\n\n        for i in range(min_row):\n            projection = []\n            for j in range(len(column_name)):\n                projection.append(df.loc[i][column_name[j]])\n                if i == 0 and j == 0:\n                    result = df.loc[i][column_name[j]]\n            print(projection)\n\n        return result\n\n    catalog_name = \"edd_hive\"\n    def litmus_helper(cur_date: str, args: str):\n        table_name = \"litmus_trip\"\n        query = f\"SELECT dt as p_dt FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by dt desc\"\n        r1 = fetchmany(query=query, size=5, column_name=[\"p_dt\"])\n        check_last_data(r1, cur_date, -5)\n\n    from aladin import trino\n    conn = trino.connect(catalog=catalog_name, schema=schema_name)\n    cursor = conn.cursor()\n    try:\n        litmus_helper(cur_date, args)\n    except Exception as error:\n        print(\"-- Exception --\", error)\n        raise\n    # Close trino connection\n    conn.close()\n    print(\"Finish {}\".format(schema_name))\n    return \"ok\"\n\ndef loc_meta_raw(previous:str, cur_date: str, debug:bool, args: str) -> str:\n    import pandas as pd\n    from datetime import datetime, timedelta\n    if cur_date is None or cur_date == \"\":\n        cur_date = datetime.today().strftime(\"%Y%m%d\")\n    print(\"previous={}, cur_date = {}, debug={}, args = {}\".format(previous, cur_date, debug, args))\n\n    schema_name = \"loc_meta_raw\"\n    def check_last_data(last_date: str, cur_date: str, days: int):\n        if debug:\n            return\n        if last_date < date_add(cur_date, days).strftime(\"%Y%m%d\"):\n            raise Exception(\"cur_date = {}, last_date = {}. expected last_date = {} \".format(cur_date, last_date, date_add(cur_date, days).strftime(\"%Y%m%d\")))\n    def date_add(cur_date:str, days: int) -> datetime:\n        return datetime.strptime(cur_date, \"%Y%m%d\") + timedelta(days=days)\n\n    def fetchmany(query: str, size: int, column_name: list) -> object:\n        print(\"----------------------\")\n        print(query)\n        cursor.execute(query)\n        df = pd.DataFrame(cursor.fetchmany(size=size), columns=column_name)\n        print(\"rows =\", len(df))\n        print(\"columns = \", df.columns)\n        min_row = len(df)\n\n        for i in range(min_row):\n            projection = []\n            for j in range(len(column_name)):\n                projection.append(df.loc[i][column_name[j]])\n                if i == 0 and j == 0:\n                    result = df.loc[i][column_name[j]]\n            print(projection)\n\n        return result\n\n    catalog_name = \"edd_hive\"\n    def loc_meta_raw_helper(cur_date: str, args: str):\n        table_name = \"enb_base\"\n        query = f\"SELECT dt as p_dt FROM {catalog_name}.{schema_name}.\\\"{table_name}$partitions\\\" order by dt desc\"\n        r1 = fetchmany(query=query, size=5, column_name=[\"p_dt\"])\n        check_last_data(r1, cur_date, -2)\n\n    from aladin import trino\n    conn = trino.connect(catalog=catalog_name, schema=schema_name)\n    cursor = conn.cursor()\n    try:\n        loc_meta_raw_helper(cur_date, args)\n    except Exception as error:\n        print(\"-- Exception --\", error)\n        raise\n    # Close trino connection\n    conn.close()\n    print(\"Finish {}\".format(schema_name))\n    return \"ok\"\n\nbase_image = base_image()\napollo_op = create_component_from_func(apollo, base_image=base_image)\ncpm_op = create_component_from_func(cpm, base_image=base_image)\ndi_cpm_op = create_component_from_func(di_cpm, base_image=base_image)\nict_11st_11st_op = create_component_from_func(ict_11st_11st, base_image=base_image)\nict_skb_acc_op = create_component_from_func(ict_skb_acc, base_image=base_image)\nict_skt_common_op = create_component_from_func(ict_skt_common, base_image=base_image)\nict_tmm_tmap_op = create_component_from_func(ict_tmm_tmap, base_image=base_image)\nlitmus_op = create_component_from_func(litmus, base_image=base_image)\nloc_meta_raw_op = create_component_from_func(loc_meta_raw, base_image=base_image)\n\nimport kfp.dsl as dsl\n@dsl.pipeline(name='edd_monitor_pipeline', description='EDD data loading monitor')\ndef my_pipeline(cur_date: str, debug: bool, args: str):\n    print(\"my_pipeline: cur_date={}, debug={}, args={}\".format(cur_date, debug, args))\n    data_op = dsl.VolumeOp(name=\"data-pvc\",\n                           resource_name=\"data-volume\",\n                           generate_unique_name=False,\n                           action='apply',\n                           size=\"10Gi\",\n                           modes=dsl.VOLUME_MODE_RWO)\n    task_1 = apollo_op(cur_date=cur_date, debug=debug, args=args).add_pvolumes({\"/data\": data_op.volume})\n    task_2 = cpm_op(previous=task_1.output, cur_date=cur_date, debug=debug, args=args)\n    task_3 = di_cpm_op(task_2.output, cur_date=cur_date, debug=debug, args=args)\n    task_4 = ict_11st_11st_op(task_3.output, cur_date=cur_date, debug=debug, args=args)\n    task_5 = ict_skb_acc_op(task_4.output, cur_date=cur_date, debug=debug, args=args)\n    task_6 = ict_skt_common_op(task_5.output, cur_date=cur_date, debug=debug, args=args)\n    task_7 = ict_tmm_tmap_op(task_6.output, cur_date=cur_date, debug=debug, args=args)\n    task_8 = litmus_op(task_7.output, cur_date=cur_date, debug=debug, args=args)\n    task_9 = loc_meta_raw_op(task_8.output, cur_date=cur_date, debug=debug, args=args)\n\nif __name__ == \"__main__\":\n    kfp.compiler.Compiler(mode=kfp.dsl.PipelineExecutionMode.V1_LEGACY).compile(my_pipeline, \"./edd_monitor.yaml\")\n    client = kfp_client()\n    client.upload_pipeline(pipeline_package_path=\"./edd_monitor.yaml\", pipeline_name=\"edd_monitor\", description=\"EDD Monitor pipeline\")\n\n"
  },
  {
    "repo": "seoeun25/ml-ops",
    "file_path": "kubeflow_pipeline/samples/iris_python/iris_python_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/seoeun25/ml-ops/master/kubeflow_pipeline/samples/iris_python/iris_python_pipeline.py",
    "content": "\"\"\"\nThis is kubeflow pipeline built using python sdk.\nWe define a stand-alone python function which is converted to pipeline component.\n\n1. create persistent volume on Kubeflow Volumes UI\n    name: test-data-volume\n2. Execute below command inside kubeflow cluster.\n    $ python iris_python_pipeline.py\nThis will create iris_python pipeline and upload it. Check this on Kubeflow Pipeline UI.\n\"\"\"\n\nimport kfp.compiler\nfrom kfp.components import create_component_from_func\nimport kfp\n\ndef kfp_client():\n    \"\"\"\n    Kubeflow pipelines client inside cluster.\n    \"\"\"\n    ## TODO os.environ\n    end_point=\"http://ml-pipeline.kubeflow.svc.cluster.local:8888\"\n    credentials = kfp.auth.ServiceAccountTokenVolumeCredentials(path=None)\n    client = kfp.Client(host=end_point, credentials=credentials)\n\n    return client\n\ndef base_image() -> str:\n    import os\n    import re\n\n    iam = os.environ.get(\"AWS_ROLE_ARN\")\n    account = re.findall(\"arn:aws:iam::(.*):role*\", iam)[0]\n    region = os.environ.get(\"AWS_REGION\")\n    base_image = \"{}.dkr.ecr.{}.amazonaws.com/aladin-runtime:anaconda-cpu\".format(account, region)\n    print(\"base_image={}\".format(base_image))\n    return base_image\n\ndef load_data(id_from: int, id_to: int) -> str:\n    print(\"data_from={}, data_to={}\".format(id_from, id_to))\n    import os\n    import pandas as pd\n    from pathlib import Path\n\n    catalog_name = \"aidp_bigquery\"\n    schema_name = \"aladin\"\n    label_table_name = 'iris'\n    metadata_table_name = 'iris_metadata'\n    columns = ['Id', 'SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm', 'Species']\n\n    def load_helper(id_from: int, id_to: int):\n        # Fetch iris dataset from trino (federated query)\n        query = f\"SELECT t.id, m.sepallengthcm, m.sepalwidthcm, m.petallengthcm, m.petalwidthcm, t.species \\\n        FROM {catalog_name}.{schema_name}.{label_table_name} t \\\n        JOIN {catalog_name}.{schema_name}.{metadata_table_name} m on m.id = t.id \\\n        WHERE m.id >= {id_from} AND m.id <= {id_to}\"\n\n        cursor = conn.cursor()\n        cursor.execute(query)\n\n        # Convert data to a pandas dataframe\n        df = pd.DataFrame(cursor.fetchall(), columns=columns)\n\n        # Write iris dataset to csv file\n        df.to_csv(iris_csv_file, index=False)\n\n    from aladin import trino\n    conn = trino.connect(catalog=catalog_name, schema=schema_name)\n    id_from = int(os.environ.get(\"data_id_from\", '1'))\n    id_to = int(os.environ.get(\"data_id_to\", '150'))\n    iris_csv_file = os.environ.get(\"iris_csv_file\", '/data/dataset/iris.csv')\n    print(\"iris_csv_file(os,environ)=\", iris_csv_file)\n    print(\"iris_csv_file(default)=\", os.environ.get(\"iris_csv_file\", \"default_csv\"))\n\n    Path(iris_csv_file).parent.mkdir(parents=True, exist_ok=True)\n    load_helper(id_from, id_to)\n    # Close trino connection\n    conn.close()\n    print(\"hello load_data. saved=\", iris_csv_file)\n    return iris_csv_file\n\ndef train_model(iris_csv_file: str) -> str:\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import LabelEncoder\n    from sklearn.metrics import accuracy_score\n    import pickle\n    import pandas as pd\n    from pathlib import Path\n    import os\n    def read_helper(iris_csv_file):\n        iris = pd.read_csv(iris_csv_file, sep=',')\n        print(iris.shape)\n        return iris\n    def get_train_test_data_helper(iris):\n        encode = LabelEncoder()\n        iris.Species = encode.fit_transform(iris.Species)\n\n        train , test = train_test_split(iris, test_size=0.2, random_state=0)\n        print('shape of training data : ', train.shape)\n        print('shape of testing data', test.shape)\n\n        X_train = train.drop(columns=['Species'], axis=1)\n        y_train = train['Species']\n        X_test = test.drop(columns=['Species'], axis=1)\n        y_test = test['Species']\n\n        return X_train, X_test, y_train, y_test\n\n    iris_csv_file = '/data/dataset/iris.csv'\n    model_pickle = '/data/models/model.pkl'\n\n    iris_data = read_helper(iris_csv_file)\n    iris_data.drop(columns='Id', inplace=True)\n\n    X_train, X_test, y_train, y_test = get_train_test_data_helper(iris_data)\n\n    model = LogisticRegression(max_iter=5000)\n    model.fit(X_train, y_train)\n\n    # Evaluate\n    y_pred = model.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n\n    # Create metric file to check in UI\n    from aladin import metric\n\n    metric.update(key='accuracy-score', value=accuracy, percent=True)\n    metric.update(key=\"power\", value=0.5, percent=False)\n    metric.dump()\n    # Save trained model\n    Path(model_pickle).parent.mkdir(parents=True, exist_ok=True)\n    with open(model_pickle, 'wb') as file:\n        pickle.dump(model, file)\n\n    import numpy as np\n\n    with open(\"/data/models/model.pkl\", \"rb\") as file:\n        m = pickle.load(file)\n    req = pd.DataFrame(columns=[\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"], data= [[5.6, 1.0, 3.5, 1.7]])\n    iris_type = {\n        0: 'setosa',\n        1: 'versicolor',\n        2: 'virginica'\n    }\n\n    proba = m.predict_proba(req)\n    print(iris_type[np.argmax(proba)])\n    print(round(max(proba[0]),4))\n\n    return model_pickle\n\ndef upload_to_msp(model_file: str):\n    import subprocess\n    import os\n    import time\n\n    print(\"pwd = \", subprocess.check_output(\"pwd\", shell=True))\n    output_path=\"./aladin_msp\"\n    model_path = \"/data/models\"\n    command = \"aladin msp init -i {}\".format(model_path)\n    result = subprocess.check_output(command, shell=True)\n    print(\"aladin msp init = \", result)\n\n    command = \"aladin msp deploy -d aladin_msp -n 'iris-python'\"\n    result = subprocess.check_output(command, shell=True)\n    print(\"aladin msp deploy = \", result)\n    time.sleep(1)\n    print(output_path, os.listdir(output_path))\n    print(\"{}/code\".format(output_path), os.listdir(\"{}/code\".format(output_path)))\n\nbase_image = base_image()\nload_data_op = create_component_from_func(load_data, base_image=base_image)\ntrain_model_op = create_component_from_func(train_model, base_image=base_image)\nupload_to_msp_op = create_component_from_func(upload_to_msp, base_image=base_image)\n\nimport kfp.dsl as dsl\n@dsl.pipeline(name='iris-python-pipeline', description='An example pipeline using python function.')\ndef my_pipeline(id_from: int, id_to: int):\n    print(\"my_pipeline: id_from={}, id_to={}\".format(id_from, id_to))\n    data_op = dsl.VolumeOp(name=\"data-pvc\",\n                           resource_name=\"test-data-volume\",\n                           generate_unique_name=False,\n                           action='apply',\n                           size=\"2Gi\",\n                           modes=dsl.VOLUME_MODE_RWO)\n    task_1 = load_data_op(id_from, id_to).add_pvolumes({\"/data\": data_op.volume})\n    task_2 = train_model_op(task_1.output).add_pvolumes({\"/data\": data_op.volume})\n    task_3 = upload_to_msp_op(task_2.output).add_pvolumes({\"/data\": data_op.volume})\n\nif __name__ == \"__main__\":\n    kfp.compiler.Compiler(mode=kfp.dsl.PipelineExecutionMode.V1_LEGACY).compile(my_pipeline, \"./iris_python.yaml\")\n    client = kfp_client()\n    client.upload_pipeline(pipeline_package_path=\"./iris_python.yaml\", pipeline_name=\"iris_python\", description=\"Iris python function based pipeline\")\n\n"
  },
  {
    "repo": "seoeun25/ml-ops",
    "file_path": "kubeflow_pipeline/samples/test/kfp_op_test.py",
    "raw_url": "https://raw.githubusercontent.com/seoeun25/ml-ops/master/kubeflow_pipeline/samples/test/kfp_op_test.py",
    "content": "\"\"\"\n$ python iris_python_pipeline.py\n# This will upload iris_python.yaml. Check this on kubeflow-pipeline-ui\n\"\"\"\n\nimport kfp.compiler\nfrom kfp.components import create_component_from_func\nimport kfp\n\ndef kfp_client():\n    \"\"\"\n    Kubeflow pipelines client inside cluster.\n    \"\"\"\n    ## TODO\n    end_point=\"http://ml-pipeline.kubeflow.svc.cluster.local:8888\"\n    credentials = kfp.auth.ServiceAccountTokenVolumeCredentials(path=None)\n    client = kfp.Client(host=end_point, credentials=credentials)\n\n    return client\ndef base_image() -> str:\n    import os\n    import re\n\n    iam = os.environ.get(\"AWS_ROLE_ARN\")\n    account = re.findall(\"arn:aws:iam::(.*):role*\", iam)[0]\n    region = os.environ.get(\"AWS_REGION\")\n    base_image = \"{}.dkr.ecr.{}.amazonaws.com/aladin-runtime:anaconda-cpu\".format(account, region)\n    print(\"base_image={}\".format(base_image))\n    return base_image\ndef volume_test(model_file: str):\n    import subprocess\n    import os\n\n    print(\"pwd = \", subprocess.check_output(\"pwd\", shell=True))\n\n    # list file and directories\n    data_path=\"/data\"\n    print(\"dir={} :: \".format(data_path), os.listdir(data_path))\n\ndef delete_file(file:str):\n    import subprocess\n    import os\n\n    print(\"pwd = \", subprocess.check_output(\"pwd\", shell=True))\n\n    # list file and directories\n    data_path=\"/data\"\n    print(\"dir={} :: \".format(data_path), os.listdir(data_path))\n\nbase_image = base_image()\nvolume_test_op = create_component_from_func(volume_test, base_image=base_image)\n\nimport kfp.dsl as dsl\n@dsl.pipeline(name='my-pipeline', description='An example pipeline')\ndef my_pipeline(id_from: int, id_to: int):\n    print(\"my_pipeline: id_from={}, id_to={}\".format(id_from, id_to))\n    data_op = dsl.VolumeOp(name=\"data-pvc\",\n                           resource_name=\"data-volume\",\n                           generate_unique_name=False,\n                           action='apply',\n                           size=\"2Gi\",\n                           modes=dsl.VOLUME_MODE_RWO)\n    task_1 = volume_test_op(\"models\").add_pvolumes({\"/data\": data_op.volume})\n\narguments = {'id_from': '7', 'id_to': '8'}\nif __name__ == \"__main__\":\n    client = kfp_client()\n    client.create_run_from_pipeline_func(my_pipeline, experiment_name=\"test-seoeun\", arguments=arguments)\n\n\n"
  },
  {
    "repo": "tryster7/FuelKFP",
    "file_path": "pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/tryster7/FuelKFP/master/pipeline.py",
    "content": "# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nKubeflow Pipelines MNIST example\n\nRun this script to compile pipeline\n\"\"\"\n\nimport kfp.dsl as dsl\nimport kfp.gcp as gcp\nimport kfp.onprem as onprem\n\nplatform = 'GCP'\n\n@dsl.pipeline(\n  name='Fuel',\n  description='Fuel Prediction pipeline.'\n)\ndef fuel_pipeline(bucket_name='gs://your-bucket/export',\n                   input_file='folder/file',\n                   output_folder='output folder',\n                   epochs = 10):\n  preprocess= dsl.ContainerOp(\n      name='preprocess',\n      image='gcr.io/kb-poc-262417/fuel:latest',\n      arguments=[\n          '--input_file', input_file,\n          '--output_folder', output_folder,\n          '--bucket_name', bucket_name\n          ]\n  )\n\n  train= dsl.ContainerOp(\n      name='train',\n      image='gcr.io/kb-poc-262417/fuel/train:latest',\n      arguments=[\n          '--bucket_name', bucket_name,\n          '--epochs', epochs\n          ]\n  )\n  train.after(preprocess)\n  \n  serve= dsl.ContainerOp(\n      name='serve',\n      image='gcr.io/kb-poc-262417/fuel/serve:latest',\n      arguments=[\n          '--bucket_name',bucket_name\n          ]\n  )\n\n  serve.after(train)\n\n  steps = [preprocess, train, serve]\n  for step in steps:\n    if platform == 'GCP':\n      step.apply(gcp.use_gcp_secret('user-gcp-sa'))\n    else:\n      step.apply(onprem.mount_pvc(pvc_name, 'local-storage', '/mnt'))\n\nif __name__ == '__main__':\n  import kfp.compiler as compiler\n  compiler.Compiler().compile(fuel_pipeline, __file__ + '.tar.gz')\n"
  },
  {
    "repo": "simanadler/kfp-components",
    "file_path": "samples/house_price_estimates/pipeline-argo.py",
    "raw_url": "https://raw.githubusercontent.com/simanadler/kfp-components/master/samples/house_price_estimates/pipeline-argo.py",
    "content": "import kfp.dsl as dsl\nimport kfp.components as components\n\n@dsl.pipeline(\n    name = \"Fybrik housing price estimate pipeline\",\n    description = \"Pipeline that provides data policy governed access to cataloged data, analyses data, trains model, and writes the results and catalogs them\"\n)\n\ndef get_current_namespace():\n    import kubernetes\n    import os\n\n    ns_path = \"/var/run/secrets/kubernetes.io/serviceaccount/namespace\"\n    if os.path.exists(ns_path):\n        with open(ns_path) as f:\n            return f.read().strip()\n    try:\n        _, active_context = kubernetes.config.list_kube_config_contexts()\n        return active_context[\"context\"][\"namespace\"]\n    except KeyError:\n        return \"kubeflow\"\n\ndef houseprice_pipeline(\n    test_dataset_id: str,\n    train_dataset_id: str\n):\n    # Get the namespace in which kfp is running\n    namespace = get_current_namespace()\n\n    # Get the ID of the run.  Make sure it's lower case and starts with a letter for our use\n    run_name = \"run-\" + dsl.RUN_ID_PLACEHOLDER.lower()\n\n    # Where to store parameters passed between workflow steps\n    result_name = \"submission-\" + str(run_name)\n\n    getDataEndpointsOp = components.load_component_from_file('../../get_data_endpoints/component.yaml')\n    getDataEndpointsStep = getDataEndpointsOp(train_dataset_id=train_dataset_id, test_dataset_id=test_dataset_id, namespace=namespace, run_name=run_name, result_name=result_name)\n    \n    visualizeTableOp = components.load_component_from_file('./visualize_table/component.yaml')\n    visualizeTableStep = visualizeTableOp(train_endpoint='%s'% getDataEndpointsStep.outputs['train_endpoint'], train_dataset_id=train_dataset_id, namespace=namespace)\n    visualizeTableStep.after(getDataEndpointsStep)\n\n    trainModelOp = components.load_component_from_file('./train_model/component.yaml')\n    trainModelStep = trainModelOp(train_endpoint_path='%s' % getDataEndpointsStep.outputs['train_endpoint'],\n                                test_endpoint_path='%s' % getDataEndpointsStep.outputs['test_endpoint'],\n                                result_name=result_name,\n                                result_endpoint_path='%s' % getDataEndpointsStep.outputs['result_endpoint'],\n                                train_dataset_id=train_dataset_id,\n                                test_dataset_id=test_dataset_id,\n                                namespace=namespace)\n    trainModelStep.after(visualizeTableStep)\n\n    sumbitResultOp = components.load_component_from_file('./submit_result/component.yaml')\n    submitResultStep = sumbitResultOp(getDataEndpointsStep.outputs['result_catalogid'])\n    submitResultStep.after(trainModelStep)\n \nif __name__ == '__main__':\n\n    # Set environment values to ensure persistent volumes used to pass parameters are allocated successfully \n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--test_dataset_id', type=str)\n    parser.add_argument('--train_dataset_id', type=str)\n \n    args = parser.parse_args()\n    import kfp.compiler as compiler\n    compiler.Compiler().compile(pipeline_func=houseprice_pipeline, package_path=__file__.replace('.py', '.yaml'))"
  },
  {
    "repo": "simanadler/kfp-components",
    "file_path": "samples/house_price_estimates/pipeline-tekton.py",
    "raw_url": "https://raw.githubusercontent.com/simanadler/kfp-components/master/samples/house_price_estimates/pipeline-tekton.py",
    "content": "import kfp.dsl as dsl\nimport kfp.components as components\n\n@dsl.pipeline(\n    name = \"Fybrik housing price estimate pipeline\",\n    description = \"Pipeline that provides data policy governed access to cataloged data, analyses data, trains model, and writes the results and catalogs them\"\n)\n\ndef get_current_namespace():\n    import kubernetes\n    import os\n\n    ns_path = \"/var/run/secrets/kubernetes.io/serviceaccount/namespace\"\n    if os.path.exists(ns_path):\n        with open(ns_path) as f:\n            return f.read().strip()\n    try:\n        _, active_context = kubernetes.config.list_kube_config_contexts()\n        return active_context[\"context\"][\"namespace\"]\n    except KeyError:\n        return \"kubeflow\"\n\ndef houseprice_pipeline(\n    test_dataset_id: str,\n    train_dataset_id: str\n):\n    # Get the namespace in which kfp is running\n    namespace = get_current_namespace()\n\n    # Get the ID of the run.  Make sure it's lower case and starts with a letter for our use\n    run_name = \"run-\" + dsl.RUN_ID_PLACEHOLDER.lower()\n\n    # Where to store parameters passed between workflow steps\n    result_name = \"submission-\" + str(run_name)\n\n    getDataEndpointsOp = components.load_component_from_file('../../get_data_endpoints/component.yaml')\n    getDataEndpointsStep = getDataEndpointsOp(train_dataset_id=train_dataset_id, test_dataset_id=test_dataset_id, namespace=namespace, run_name=run_name, result_name=result_name)\n    \n    visualizeTableOp = components.load_component_from_file('./visualize_table/component.yaml')\n    visualizeTableStep = visualizeTableOp(train_endpoint='%s'% getDataEndpointsStep.outputs['train_endpoint'], train_dataset_id=train_dataset_id, namespace=namespace)\n    visualizeTableStep.after(getDataEndpointsStep)\n\n    trainModelOp = components.load_component_from_file('./train_model/component.yaml')\n    trainModelStep = trainModelOp(train_endpoint_path='%s' % getDataEndpointsStep.outputs['train_endpoint'],\n                                test_endpoint_path='%s' % getDataEndpointsStep.outputs['test_endpoint'],\n                                result_name=result_name,\n                                result_endpoint_path='%s' % getDataEndpointsStep.outputs['result_endpoint'],\n                                train_dataset_id=train_dataset_id,\n                                test_dataset_id=test_dataset_id,\n                                namespace=namespace)\n    trainModelStep.after(visualizeTableStep)\n\n    sumbitResultOp = components.load_component_from_file('./submit_result/component.yaml')\n    submitResultStep = sumbitResultOp(getDataEndpointsStep.outputs['result_catalogid'])\n    submitResultStep.after(trainModelStep)\n \nif __name__ == '__main__':\n\n    # Set environment values to ensure persistent volumes used to pass parameters are allocated successfully \n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--test_dataset_id', type=str)\n    parser.add_argument('--train_dataset_id', type=str)\n\n    args = parser.parse_args()\n    from kfp_tekton.compiler import TektonCompiler\n \n    TektonCompiler().compile(houseprice_pipeline, __file__.replace('.py', '.yaml'))"
  },
  {
    "repo": "cliffvj/sparta-kubeflow",
    "file_path": "boston_housing/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/cliffvj/sparta-kubeflow/main/boston_housing/pipeline.py",
    "content": "import kfp\r\nfrom kfp import dsl\r\n\r\ndef preprocess_op():\r\n\r\n    return dsl.ContainerOp(\r\n        name='Preprocess Data',\r\n        image='cliffvj/boston_pipeline_preprocessing:latest',\r\n        arguments=[],\r\n        file_outputs={\r\n            'x_train': '/app/x_train.npy',\r\n            'x_test': '/app/x_test.npy',\r\n            'y_train': '/app/y_train.npy',\r\n            'y_test': '/app/y_test.npy',\r\n        }\r\n    )\r\n\r\ndef train_op(x_train, y_train):\r\n\r\n    return dsl.ContainerOp(\r\n        name='Train Model',\r\n        image='cliffvj/boston_pipeline_train:latest',\r\n        arguments=[\r\n            '--x_train', x_train,\r\n            '--y_train', y_train\r\n        ],\r\n        file_outputs={\r\n            'model': '/app/model.pkl'\r\n        }\r\n    )\r\n\r\ndef test_op(x_test, y_test, model):\r\n\r\n    return dsl.ContainerOp(\r\n        name='Test Model',\r\n        image='cliffvj/boston_pipeline_test:latest',\r\n        arguments=[\r\n            '--x_test', x_test,\r\n            '--y_test', y_test,\r\n            '--model', model\r\n        ],\r\n        file_outputs={\r\n            'mean_squared_error': '/app/output.txt'\r\n        }\r\n    )\r\n\r\ndef deploy_model_op(model):\r\n\r\n    return dsl.ContainerOp(\r\n        name='Deploy Model',\r\n        image='cliffvj/boston_pipeline_deploy_model:latest',\r\n        arguments=[\r\n            '--model', model\r\n        ]\r\n    )\r\n\r\n@dsl.pipeline(\r\n   name='Boston Housing Pipeline',\r\n   description='An example pipeline that trains and logs a regression model.'\r\n)\r\ndef boston_pipeline():\r\n    _preprocess_op = preprocess_op()\r\n    \r\n    _train_op = train_op(\r\n        dsl.InputArgumentPath(_preprocess_op.outputs['x_train']),\r\n        dsl.InputArgumentPath(_preprocess_op.outputs['y_train'])\r\n    ).after(_preprocess_op)\r\n\r\n    _test_op = test_op(\r\n        dsl.InputArgumentPath(_preprocess_op.outputs['x_test']),\r\n        dsl.InputArgumentPath(_preprocess_op.outputs['y_test']),\r\n        dsl.InputArgumentPath(_train_op.outputs['model'])\r\n    ).after(_train_op)\r\n\r\n    deploy_model_op(\r\n        dsl.InputArgumentPath(_train_op.outputs['model'])\r\n    ).after(_test_op)\r\n\r\nclient = kfp.Client()\r\nclient.create_run_from_pipeline_func(boston_pipeline, arguments={})"
  },
  {
    "repo": "deinal/jec-pipeline",
    "file_path": "pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/deinal/jec-pipeline/master/pipeline.py",
    "content": "import argparse\nimport uuid\nimport kfp\nfrom kfp import dsl\nimport http\nimport yaml\nimport json\n\n\ndef load_cookies(cookie_file, domain):\n    cookiejar = http.cookiejar.MozillaCookieJar(cookie_file)\n    cookiejar.load()\n    for cookie in cookiejar:\n        if cookie.domain == domain:\n            cookies = f'{cookie.name}={cookie.value}'\n            break\n    return cookies\n\ndef get_pipeline(name, description):\n    @dsl.pipeline(name=name, description=description)\n    def pipeline(\n        run_id: str,\n        s3_bucket: str,\n        data_train: str,\n        data_val: str,\n        data_test: str,\n        data_config: str,\n        network_config: str,\n        num_replicas: int,\n        num_gpus: int,\n        num_cpus: int,\n        memory: str,\n        delete_train_experiment: bool,\n        delete_export_job: bool,\n    ):\n\n        train = train_op(\n            id=run_id,\n            s3_bucket=s3_bucket,\n            num_replicas=num_replicas,\n            num_gpus=num_gpus,\n            num_cpus=num_cpus,\n            memory=memory,\n            data_train=data_train,\n            data_val=data_val,\n            data_test=data_test,\n            data_config=data_config,\n            network_config=network_config,\n            delete_experiment=delete_train_experiment,\n        )\n\n        export = export_op(\n            id=run_id,\n            s3_bucket=s3_bucket,\n            data_config=data_config,\n            network_config=network_config,\n            delete_job=delete_export_job,\n            pt_path=train.outputs['optimal_model_path'],\n            network_option=train.outputs['network_option'],\n        )\n\n        serve = serve_op(\n            model_name=run_id,\n            model_path=export.outputs['model_path'],\n        )\n\n    return pipeline\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='Pipeline Params')\n    parser.add_argument('--namespace', type=str, default='dholmber', \n                        help='Kubeflow namespace to run pipeline in')\n    parser.add_argument('--experiment-name', type=str, default='jec-experiment', \n                        help='name for KFP experiment on Kubeflow')\n    parser.add_argument('--num-replicas', type=int, default=1,\n                        help='number of nodes to train on')\n    parser.add_argument('--num-gpus', type=int, default=1,\n                        help='number of gpus per node, limit is 1')\n    parser.add_argument('--num-cpus', type=int, default=1, \n                        help='number of cpus to use')\n    parser.add_argument('--memory', type=str, default='12Gi', \n                        help='memory in gigabyte')                    \n    parser.add_argument('--data-config', type=str, default='data/jec_pfn_open.yaml', \n                        help='data configuration yaml file')\n    parser.add_argument('--network-config', type=str, default='networks/pfn_regressor_open.py', \n                        help='network architecture configuration file')\n    parser.add_argument('--s3-bucket', type=str, default='s3://jec-data', \n                        help='s3 bucket used by the pipeline for storing models and tensorboard log dirs')\n    parser.add_argument('--data-train', type=str, default='s3://jec-data/open/train/*.root',\n                        help='training data')\n    parser.add_argument('--data-val', type=str, default='s3://jec-data/open/val/*.root',\n                        help='validation data')\n    parser.add_argument('--data-test', type=str, default='s3://jec-data/open/test/*.root',\n                        help='test data')\n    parser.add_argument('--delete-train-experiment', action='store_true', default=False,\n                        help='whether or not to delete the hp tuning experiment once finished')\n    parser.add_argument('--delete-export-job', action='store_true', default=False,\n                        help='whether or not to delete the export job once finished')\n    args = parser.parse_args()\n\n    # Define pipeline variables\n    description = 'Jet Energy Corrections Pipeline'\n    network = args.network_config.split(\"/\")[-1].split(\".py\")[0].replace('_', '-')\n    run_id = f'{network}-{uuid.uuid4().hex[:6]}'\n    pipeline_name = f'jec-pipeline-{run_id}'\n    package_path = f'packages/{pipeline_name}.tar.gz'\n\n    # Import pipeline components\n    train_op = kfp.components.load_component_from_file('training/component.yaml')\n    export_op = kfp.components.load_component_from_file('exporting/component.yaml')\n    serve_op = kfp.components.load_component_from_file('serving/component.yaml')\n\n    # Get pipeline instance\n    pipeline = get_pipeline(pipeline_name, description)\n\n    # Compile pipeline\n    kfp.compiler.Compiler().compile(pipeline_func=pipeline, package_path=package_path)\n\n    # Load cookies to access Kubeflow at CERN\n    cookies = load_cookies(cookie_file='cookies.txt', domain='ml.cern.ch')\n    \n    # Load Kubeflow pipeline client\n    client = kfp.Client(host='https://ml.cern.ch/pipeline', cookies=cookies)\n\n    # Upload pipeline \n    client.upload_pipeline(pipeline_package_path=package_path, pipeline_name=pipeline_name, description=description)\n\n    # Create KFP experiment\n    experiment = client.create_experiment(name=args.experiment_name, namespace=args.namespace)\n\n    # Run pipeline\n    run = client.run_pipeline(\n        pipeline_package_path=package_path,\n        experiment_id=experiment.id,\n        job_name=f'run-{run_id}',\n        params={\n            'run_id': run_id,\n            's3_bucket': args.s3_bucket,\n            'data_train': args.data_train,\n            'data_val': args.data_val,\n            'data_test': args.data_test,\n            'data_config': args.data_config,\n            'network_config': args.network_config,\n            'num_replicas': args.num_replicas,\n            'num_gpus': args.num_gpus,\n            'num_cpus': args.num_cpus,\n            'memory': args.memory,\n            'delete_train_experiment': args.delete_train_experiment,\n            'delete_export_job': args.delete_export_job,\n        }\n    )\n\n    print('Deployed', pipeline_name)\n"
  },
  {
    "repo": "sol-demos-01/kubeflow-ml1-demo-app",
    "file_path": "components/golang/component.py",
    "raw_url": "https://raw.githubusercontent.com/sol-demos-01/kubeflow-ml1-demo-app/master/components/golang/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef processing_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('GOLANG_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=[script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n"
  },
  {
    "repo": "sol-demos-01/kubeflow-ml1-demo-app",
    "file_path": "components/training/component.py",
    "raw_url": "https://raw.githubusercontent.com/sol-demos-01/kubeflow-ml1-demo-app/master/components/training/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef training_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('TRAINING_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=['/usr/local/bin/python', script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n\n"
  },
  {
    "repo": "ajperry2/mapillary_pipeline",
    "file_path": "mapillary_pipeline/download_data_components.py",
    "raw_url": "https://raw.githubusercontent.com/ajperry2/mapillary_pipeline/main/mapillary_pipeline/download_data_components.py",
    "content": "from kfp.dsl import component, Output, Dataset\n\n\n@component()\ndef download_data(\n    url: str,\n    dataset: Output[Dataset]\n):\n    import urllib.request\n    import zipfile\n    import os\n    from pathlib import Path\n    dataset_path = Path(dataset.path)\n    dataset_path.mkdir(parents=True, exist_ok=True)\n    urllib.request.urlretrieve(url, f\"{dataset_path}/data.zip\")\n    with zipfile.ZipFile(dataset_path / \"data.zip\", 'r') as zip_ref:\n        zip_ref.extractall(dataset_path)\n    os.remove(dataset_path / \"data.zip\")\n\n"
  },
  {
    "repo": "ajperry2/mapillary_pipeline",
    "file_path": "mapillary_pipeline/download_data_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/ajperry2/mapillary_pipeline/main/mapillary_pipeline/download_data_pipeline.py",
    "content": "from kfp import dsl, kubernetes\n\nfrom .download_data_components import download_data\n\n\n@dsl.pipeline\ndef pipeline_func(url: str) -> None:\n\n    task = download_data(url=url).set_caching_options(enable_caching=False).ignore_upstream_failure()"
  },
  {
    "repo": "stackdemos/kf-overlay-acm",
    "file_path": "components/golang/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/kf-overlay-acm/master/components/golang/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef processing_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('GOLANG_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=[script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n"
  },
  {
    "repo": "stackdemos/kf-overlay-acm",
    "file_path": "components/training/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/kf-overlay-acm/master/components/training/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef training_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('TRAINING_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=['/usr/local/bin/python', script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n\n"
  },
  {
    "repo": "Shunpoco/kfp-sample",
    "file_path": "pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/Shunpoco/kfp-sample/main/pipeline.py",
    "content": "from types import FunctionType\n\nfrom kfp.compiler import Compiler\nfrom kfp import dsl\nimport kfp.components as comp\n\n\n@dsl.pipeline(\n    name=\"Test\",\n    description=\"test\",\n)\ndef pipeline():\n    c:FunctionType = comp.load_component_from_file(\"./components/task1/component.yaml\")\n\n    task1 = c(\n        input_path=\"hogehoge/fugafuga\",\n    )\n\n    c(\n        input_path=task1.outputs[\"output_path\"],\n    )\n\ndef main():\n    Compiler().compile(\n        pipeline_func=pipeline,\n        package_path=\"./pipeline.yaml\",\n    )\n\nif __name__ == \"__main__\":\n    main()\n"
  },
  {
    "repo": "kshokn2/kfp_VertexAI",
    "file_path": "run_kfp_vertexAI.py",
    "raw_url": "https://raw.githubusercontent.com/kshokn2/kfp_VertexAI/master/run_kfp_vertexAI.py",
    "content": "import os\nimport kfp\nfrom kfp import dsl, compiler\nfrom kfp.dsl import (Artifact, Dataset, Input, Output, Model, Metrics, Markdown, HTML, component, InputPath, OutputPath, PipelineTaskFinalStatus)\n\nfrom google_cloud_pipeline_components.types.artifact_types import VertexEndpoint\nfrom google_cloud_pipeline_components.v1.endpoint import EndpointCreateOp\nfrom google_cloud_pipeline_components.v1.custom_job import utils\nfrom google_cloud_pipeline_components.v1 import hyperparameter_tuning_job\nfrom google_cloud_pipeline_components.v1 hyperparameter_tuning_job import HyperparameterTuningJobRunOp\n\nfrom google.cloud import aiplatform\n\nmyparam1 = os.getenv(\"param1\")\n\n# \ub2e8\uc704 \ud14c\uc2a4\ud2b8\uc6a9\n# myparam1 = \"parameter1\"\n\nPROJECT_ID = \"my-project-id\"\nPROJECT_NUM = \"1234567890\"\nREGION = \"my-region\"\nPIPELINEJOB_SA = \"my-service-account@~~~~~\"\nPRIVATE_EP_VPC = \"my-vpc-name\"\n\nPIPELINE_ROOT = \"gs://my-mlops-bucket\"\nPIPELINE_NAME = \"my-pipeline-name\"\n\nBASE_IMAGE = \"my-docker-kfp_2_7_0-image\" # kfp 2.7.0 \uc124\uce58\ub41c \uae30\ubcf8 \ub3c4\ucee4 \uc774\ubbf8\uc9c0\n\ndef main():\n\n    @component(\n        base_image=BASE_IMAGE,\n        # packages_to_install=[\"install_library1==0.0.1\", \"install_library2==1.0.0\"]\n    )\n    def download_data(\n        # project: str,\n        # location: str,\n        bukcet_name: str,\n        download_path: OutputPath(\"Any\"),\n    ):\n        import os\n        import sys\n        import logging\n\n        # from google.cloud import storage\n        from google.cloud import logging as gc_logging\n\n        def download_bucket_with_transfer_manager(\n            bucket_name, blob_name_prefix, target_list=None, destination_directory=\"\", workers=8\n        ):\n            # https://cloud.google.com/storage/docs/samples/storage-transfer-manager-download-bucket?hl=ko\n\n            import os\n            import time\n            from google.cloud.storage import Client, transfer_manager\n\n            storage_client = Client()\n            bucket = storage_client.bucket(bucket_name)\n\n            if target_list is None:\n                blob_names = [blob.name.replace(blob_name_prefix, '') for blob in bucket.list_blobs(prefix=blob_name_prefix) if blob.name != blob_name_prefix]\n            else:\n                blob_names = target_list\n\n            results = transfer_manager.download_many_to_path(\n                bucket, blob_names, destination_directory=destination_directory, blob_name_prefix=blob_name_prefix, max_workers=workers)\n            \n            retry_blobs = []\n\n            for name, result in zip(blob_names, results):\n                if isinstance(result, Exception):\n                    # print(\"Failed to download {} due to exception: {}\".format(name, result))\n                    retry_blobs.append(name)\n                # else:\n                #     print(\"Downloaded {} to {}.\".format(name, destination_directory + name))\n\n            if len(retry_blobs) != 0:\n                retry_results = transfer_manager.download_many_to_path(\n                    bucket, retry_blobs, destination_directory=destination_directory, blob_name_prefix=blob_name_prefix, max_workers=workers)\n                \n                for name, result in zip(retry_blobs, retry_results):\n                    if isinstance(result, Exception):\n                        logging.error(\"Failed to download {} due to exception: {}\".format(name, result))\n                        os.remove(f'{destination_directory}/{name}')\n                        time.sleep(0.05)\n\n        workers = int(os.cpu_count()/2) if os.cpu_count() > 61 else os.cpu_count()\n        os.makedirs(download_path, exist_ok=True)\n\n        download_bucket_with_transfer_manager(\n            bucket_name=bukcet_name,\n            blob_name_prefix=f'my_blob/',\n            destination_directory=download_path,\n            workers=workers\n        )\n\n        if len(os.listdir(download_path)) == 0:\n            logging.error(\"download_data component failed..\")\n            sys.exit(1)\n\n        logging.info(\"download_data component completed..\")\n\n\n    @component(\n        base_image=BASE_IMAGE,\n        # packages_to_install=[\"install_library1==0.0.1\", \"install_library2==1.0.0\"]\n    )\n    def data_preparation(\n        # project: str,\n        # location: str,\n        rand_seed: int,\n        ratio: float,\n        batch_size: int,\n        label_files: str,\n        download_path: InputPath(\"Any\"),\n        trainset_path: OutputPath(\"Any\"),\n        validset_path: OutputPath(\"Any\"),\n        testset_path: OutputPath(\"Any\"),\n    ):\n        import os\n        import sys\n        import random\n        import numpy as np\n        from sklearn.model_selection import train_test_split\n        import tensorflow as tf\n        import logging\n\n        from google.cloud import storage\n        from google.cloud import logging as gc_logging\n\n        random.seed(rand_seed)\n\n        file_list = os.listdir(download_path)\n        \"\"\"\n        label \uc815\ubcf4 \uac00\uc838\uc624\uae30\n        \"\"\"\n\n        array_data = np.array([np.load(os.path.join(download_path, filenm+\".npy\")) for filenm in file_list])\n        array_class = label_files#\ub85c \ub9cc\ub4e4\uae30\n\n        # split dataset\n        train_data, temp_data, train_labels, temp_labels = train_test_split(\n            array_data, array_class, test_size=1-ratio, random_state=rand_seed, stratify=array_class)\n        \n        rest_ratio = 0.5\n        valid_data, test_data, valid_labels, test_labels = train_test_split(\n            temp_data, temp_labels, test_size=rest_ratio, random_state=rand_seed, stratify=temp_labels)\n        \n        # tf.data.Dataset \uc0dd\uc131\n        train_dataset = tf.data.Dataset.from_tensor_slices((train_data, train_labels))\n        train_dataset = train_dataset.shuffle(00).batch(batch_size)\n        \n        valid_dataset = tf.data.Dataset.from_tensor_slices((valid_data, valid_labels))\n        valid_dataset = valid_dataset.shuffle(00).batch(batch_size)\n        \n        test_dataset = tf.data.Dataset.from_tensor_slices((test_data, test_labels))\n        test_dataset = test_dataset.shuffle(00).batch(batch_size)\n\n        # artifacts\uc758 metadata\uc5d0 \uc800\uc7a5\n        train_dataset.save(trainset_path)\n        valid_dataset.save(validset_path)\n        test_dataset.save(testset_path)\n\n        logging.info(\"data_preparation component completed..\")\n\n\n    @component(\n        base_image=BASE_IMAGE,\n    )\n    def worker_pool_spec(\n        hpt_epochs: int,\n        hpt_image: str,\n        myparam1: str,\n    ) -> list:\n        CMDARGS = [\n            \"--epochs\", str(hpt_epochs),\n            \"--param1\", myparam1,\n        ]\n\n        worker_pool_spec = [\n            {\n                \"machine_spec\": {\n                    # gpu\n                    \"machine_type\": \"my-gcp-machine\",\n                    \"accelerator_type\": \"my-gcp-accelerator\",\n                    \"accelerator_count\": 1,\n                },\n                \"replica_count\": 1,\n                \"container_spec\": {\"image_uri\": hpt_image, \"args\": CMDARGS},\n            }\n        ]\n\n        logging.info(\"worker_pool_spec component completed..\")\n\n        return worker_pool_spec\n\n\n    @component(\n        base_image=BASE_IMAGE,\n        # packages_to_install=[\"google-cloud-aiplatform\", \"google-cloud-pipeline-components\", \"protobuf\"],\n    )\n    def GetBestTrialOp(\n        gcp_resources: str,\n        study_spec_metrics: list,\n    ) -> str:\n        import sys\n        import logging\n\n        from google.cloud import aiplatform\n        from google_cloud_pipeline_components.proto.gcp_resources_pb2 import GcpResources\n        from google.protobuf.json_format import Parse\n        from google.cloud.aiplatform_v1.types import study\n        from google.cloud import logging as gc_logging\n\n        api_endpoint_suffix = '-aiplatform.googleapis.com'\n        gcp_resources_proto = Parse(gcp_resources, GcpResources())\n        gcp_resources_split = gcp_resources_proto.resources[0].resource_uri.partition('projects')\n        resource_name = gcp_resources_split[1] + gcp_resources_split[2]\n        prefix_str = gcp_resources_split[0]\n        prefix_str = prefix_str[:prefix_str.find(api_endpoint_suffix)]\n        api_endpoint = prefix_str[(prefix_str.rfind('//') + 2):] + api_endpoint_suffix\n\n        client_options = {'api_endpoint': api_endpoint}\n        job_client = aiplatform.gapic.JobServiceClient(client_options=client_options)\n        response = job_client.get_hyperparameter_tuning_job(name=resource_name)\n        \n        trials = [study.Trial.to_json(trial) for trial in response.trials]\n\n        if len(study_spec_metrics) > 1:\n            raise RuntimeError('Unable to determine best parameters for multi-objective hyperparameter tuning.')\n            logging.error(\"Unable to determine best parameters for multi-objective hyperparameter tuning.\")\n            sys.exit(1)\n        trials_list = [study.Trial.from_json(trial) for trial in trials]\n        best_trial = None\n        goal = study_spec_metrics[0]['goal']\n        best_fn = None\n        if goal == study.StudySpec.MetricSpec.GoalType.MAXIMIZE:\n            best_fn = max\n        elif goal == study.StudySpec.MetricSpec.GoalType.MINIMIZE:\n            best_fn = min\n        best_trial = best_fn(\n            trials_list, key=lambda trial: trial.final_measurement.metrics[0].value)\n        \n        logging.info(\"GetBestTrialOp component completed..\")\n\n        return study.Trial.to_json(best_trial)\n\n\n    @component(\n        base_image=BASE_IMAGE,\n    )\n    def GetHyperparametersOp(\n        trial: str,\n    ) -> list:\n        from google.cloud.aiplatform_v1.types import study\n\n        trial_proto = study.Trial.from_json(trial)\n\n        return [ study.Trial.Parameter.to_json(param) for param in trial_proto.parameters ]\n    \n\n    @component(\n        base_image=BASE_IMAGE,\n    )\n    def training_model(\n        rand_seed: int,\n        training_params: dict,\n        best_params: list,\n        tensorboard_root: str,\n        dataset_train: InputPath(\"Any\"),\n        dataset_valid: InputPath(\"Any\"),\n        dataset_test: InputPath(\"Any\"),\n        model: Output[Model],\n    ):\n        import os\n        import sys\n        import json\n        import random\n        import tensorflow as tf\n        from tensorflow import keras\n        from tensorflow.keras import layers\n        from sklearn.utils import class_weight\n\n        from google.cloud import logging as gc_logging\n\n        import logging\n\n        random.seed(rand_seed)\n        learning_rate = json.loads(best_params[0])[\"value\"] # format(parma 1\uac1c\uc77c \ub54c): [{\"parameterId\": \"learning_rate\", \"value\": 0.00123}]\n\n        train_dataset = tf.data.Dataset.load(dataset_train)\n        valid_dataset = tf.data.Dataset.load(dataset_valid)\n        test_dataset = tf.data.Dataset.load(dataset_test)\n\n        os.environ['AIP_TENSORBOARD_LOG_DIR'] = tensorboard_root\n        gpus = tf.config.experimental.list_physical_devices('GPU')\n\n        # \ubaa8\ub378 \uc815\uc758\n        if len(gpus) > 1:\n            strategy = tf.distribute.MirroredStrategy()\n            with strategy.scope():\n                new_model = create_model()\n        else:\n            new_model = create_model()\n\n        # \ubaa8\ub378 \ucef4\ud30c\uc77c\n        new_model.complie(\n            loss = loss\uc124\uc815,\n            optimizer = opt\uc124\uc815(learning_rate=learning_rate),\n            metrics = [\uba54\ud2b8\ub9ad1, \uba54\ud2b8\ub9ad2]\n        )\n\n        # \ubaa8\ub378 \ud559\uc2b5\n        new_model.fit(\n            train_dataset,\n            validation_data=valid_dataset,\n            epochs = training_params[\"epochs\"],\n            verbose = 1,\n            # class_weight=class_weight,\n            callbacks = [\n                tf.keras.callbacks.Tensorboard(log_dir=os.environ['AIP_TENSORBOARD_LOG_DIR'], histogram_freq=1),\n                keras.callbacks.EarlyStopping(\uc124\uc815),\n                keras.callbacks.ModelCheckpoint(filepath=model.path, monitor=\uba54\ud2b8\ub9ad1, mode='max', save_best_only=True, save_wweights_only=True)\n            ]\n        )\n\n        logging.info(\"training_model component completed..\")\n\n\n    @component(\n        base_image=BASE_IMAGE,\n    )\n    def evaluate_model(\n        # project: str,\n        # location: str,\n        dataset_test: InputPath(\"Any\"),\n        model: Input[Model],\n    ) -> str:\n        from google.cloud import logging as gc_logging\n\n        import tensorflow as tf\n        import logging\n\n        \"\"\"\n        \ubaa8\ub378 \uc815\uc758 \ucf54\ub4dc\n        \"\"\"\n\n        load_model = create_model()\n        load_model.load_weights(model.path)\n\n        \"\"\"\n        \ubaa8\ub378 \ud3c9\uac00 \ucf54\ub4dc\n        \"\"\"\n\n        logging.info(\"evaluate_model component completed..\")\n\n        #\ud3c9\uac00 \ud1b5\uacfc\n        if contidion:\n            logging.info(\"Model evaluation criteria satisfied.\")\n            return \"true\" # \ubc30\ud3ec\n        \n        else:\n            logging.info(\"Model evaluation criteria not satisfied.\")\n            return \"false\" # \uc7ac\ud559\uc2b5\n\n\n    @component(\n        base_image=BASE_IMAGE,\n    )\n    def deploy_model(\n        project: str,\n        location: str,\n        model_name: str,\n        model: Input[Model],\n        endpoint: Input[VertexEndpoint],\n        upload_model: Output[Model],\n        deploy_model: Output[Model],\n    ):\n        from google.cloud import aiplatform\n        from google.cloud import logging as gc_logging\n\n        import logging\n\n        model_list = aiplatform.Model.list(\n            filter=f'display_name=\"{model_name}\"')\n        \n        if len(model_list) > 0:\n            # \ub4f1\ub85d\ub41c \ubaa8\ub378\uc5d0 \uc0c8 \ubc84\uc804 \uc0dd\uc131\n            my_model = model_list[0]\n            logging.info(f'Name of uploaded model: {my_model.resource_name}')\n\n            uploaded_model = aiplatform.Model.upload_tensorflow_saved_model(\n                saved_model_dir = model.path,\n                tensorflow_version = \"tf \ubc84\uc804\",\n                use_gpu = False,\n                parent_model = my_model.resource_name,\n                is_default_version = True,\n                version_description = f\"\ub4f1\ub85d\ud560 \ubaa8\ub378 \uc124\uba85\",\n            )\n        else:\n            # \uc0c8\ub85c\uc6b4 \ubaa8\ub378 \uc774\ub984\uc73c\ub85c \ub4f1\ub85d\n            logging.info(f'Name of New uploaded model: {my_model.resource_name}')\n\n            uploaded_model = aiplatform.Model.upload_tensorflow_saved_model(\n                saved_model_dir = model.path,\n                tensorflow_version = \"tf \ubc84\uc804\",\n                use_gpu = False,\n                parent_model = model_name,\n                is_default_version = True,\n                version_description = f\"\ub4f1\ub85d\ud560 \ubaa8\ub378 \uc124\uba85\",\n            )\n\n        created_private_ep = None\n        for i in aiplatform.PrivateEndpoint.list():\n            if i.resource_name == endpoint.metadata['resourceName']:\n                created_private_ep = i\n                break\n\n        if created_private_ep:\n            # \uc5d4\ub4dc\ud3ec\uc778\ud2b8 \uc7ac\uc0dd\uc131 \ud544\uc694\n            pass\n        else:\n            # \uc55e\uc5d0\uc11c \uc0dd\uc131\ud55c ep\uc5d0 \ubaa8\ub378 \ubc30\ud3ec\n            deployed_model = uploaded_model.deploy(\n                endpoint = created_private_ep,\n                machine_type = \"my-machine\",\n                # deployed_model_display_name = \"my-display-name\" # \ud544\uc694\uc2dc\n            )\n\n        upload_model.uri = uploaded_model.resource_name\n        deploy_model.uri = deployed_model.resource_name\n\n\n        logging.info(\"deploy_model component completed..\")\n\n\n    @component(\n        base_image=BASE_IMAGE,\n    )\n    def get_run_state(status: dict) -> str:\n        return status['state']\n        \n\n    @component(\n        base_image=BASE_IMAGE,\n    )\n    def logging_op(msg: str, severity: str):\n        from google.cloud import logging as gc_logging\n        import logging\n\n        if severity == \"WARNING\":\n            logging.warn(msg)\n        elif severity == \"ERROR\":\n            logging.error(msg)\n        else:\n            logging.info(msg)\n\n\n    @dsl.pipeline(name='conditional-notification')\n    def exit_op(status: PipelineTaskFinalStatus):\n        with dsl.If(get_run_state(status=status).output == \"FAILED\"):\n            # clearnup_op() # \ud30c\uc774\ud504\ub77c\uc778 \uc2e4\ud328\ud558\uba74 clearnup\n            logging_op(msg=\"\uc2e4\ud328 \uba54\uc2dc\uc9c0\", severity=\"ERROR\").set_display_name(\"fail-alarm\")\n        \n        with dsl.Else():\n            logging_op(msg=\"\uc131\uacf5 \uba54\uc2dc\uc9c0\", severity=\"INFO\").set_display_name(\"success-alarm\")\n\n    \n    @dsl.pipeline(\n        name=PIPELINE_NAME,\n        pipeline_root=PIPELINE_ROOT\n    )\n    def pipeline(\n        project_id: str,\n        project_num: str,\n        region: str,\n        vpc_network: str,\n        param1: str, # hpt\uc5d0 \uc0ac\uc6a9\ud558\ub294 \ud30c\ub77c\ubbf8\ud130 \uc608\uc2dc\n        tensorboard_root_try1: str,\n        create_private_ep_name: str,\n        upload_model_name: str,\n    ):\n        \"\"\"\n        \uac01\uc885 \ud544\uc694\ud55c \ud30c\ub77c\ubbf8\ud130\ub4e4 \uc124\uc815..\n        \"\"\"\n        rand_seed = 42\n        download_bukcet_name = \"my-bucket\"\n        hpt_container_image = \"hpyerparamter tuning image\"\n        hpt_epochs = 10\n        hpt_max_trial_count = 8\n        hpt_parallel_trial_count = 2\n\n        tr_label_files = \"my-training-label-file\"\n        tensorboard_id = \"1234567890\" # tensorboard id in gcp\n        sa_custom_training = \"my-service-account\"\n\n        training_params = {\n            \"batch_size\": 64,\n            \"ratio\": 0.8,\n            \"epochs\": 1000\n        }\n\n        # Data download\n        data_download_op = download_data(\n            # project = project_id,\n            # location = region,\n            bukcet_name = download_bukcet_name,\n        ).set_display_name(\"data-download\").set_caching_options(False)\n\n        # Data preparation\n        preparation_op = data_preparation(\n            # project = project_id,\n            # location = region,\n            rand_seed = rand_seed,\n            ratio = training_params[\"ratio\"],\n            batch_size = training_params[\"batch_size\"],\n            label_files = tr_label_files,\n            download_path = data_download_op.outputs[\"download_path\"],\n        ).set_display_name(\"data-preparation\").set_cpu_limit('8').set_memory_limit('16G').set_caching_options(False)\n\n        # worker_pool_specs, study_spec_metrics, study_spec_parameters\n        hpt_worker_pool_spec = worker_pool_spec(\n            hpt_epochs = hpt_epochs,\n            hpt_image = hpt_container_image,\n            myparam1 = param1,\n        ).set_caching_options(False)\n\n        hpt_study_spec_metrics = hyperparameter_tuning_job.serialize_metrics({\"\uba54\ud2b8\ub9ad1\": \"maximize\"})\n        \n        hpt_study_spec_parameters = hyperparameter_tuning_job.serialize_metrics(\n            {\n                \"learning_rate\": aiplatform.hyperparameter_tuning.DoubleParameterSpec(\n                    min=0.0001, max=0.01, scale=\"log\",\n                ),\n                # aiplatform.hyperparameter_tuning.DiscreteParameterSpec\n            }\n        )\n\n        # hyperparameter tuning\n        tuning_op = HyperparameterTuningJobRunOp(\n            display_name=\"hyperparameter-tuning\",\n            project = project_id,\n            location = region,\n            worker_pool_specs = hpt_worker_pool_spec.output,\n            study_spec_metrics = hpt_study_spec_metrics,\n            study_spec_parameters = hpt_study_spec_parameters,\n            max_trial_count = hpt_max_trial_count,\n            parallel_trial_count = hpt_parallel_trial_count,\n            base_output_directory = PIPELINE_ROOT,\n        ).after(preparation_op).set_caching_options(False)\n\n        best_trial_op = GetBestTrialOp(\n            gcp_resources = tuning_op.outputs[\"gcp_resources\"],\n            study_spec_metrics = hpt_study_spec_metrics,\n        ).set_display_name(\"get-best-trial\").set_caching_options(False)\n\n        best_param_op = GetHyperparametersOp(\n            trial = best_trial_op.output,\n        ).set_display_name(\"get-best-parameters\").set_caching_options(False)\n\n        tensorboard = aiplatform.Tensorboard(\n            tensorboard_name = tensorboard_id,\n            project = \"my-prj-id\", # \ubcc0\uc218\ub85c \ubc1b\uc73c\uba74 \uc548\ub40c\n            location = \"my-region\", # \ubcc0\uc218\ub85c \ubc1b\uc73c\uba74 \uc548\ub40c\n        )\n\n        # training\n        custom_job_training_op = utils.create_custom_training_job_op_from_component(\n            training_model,\n            tensorboard = tensorboard.resource_name,\n            base_output_directory = PIPELINE_ROOT,\n            service_account = sa_custom_training,\n            # gpu\n            machine_type = \"my-gcp-machine\",\n            accelerator_type = \"my-gcp-accelerator\",\n            accelerator_count = \"1\",\n            replica_count = 1\n        )\n        training_model_op = custom_job_training_op(\n            project = project_id, # \ud544\uc218\n            location = region, # \ud544\uc218\n            rand_seed = rand_seed,\n            training_params = training_params,\n            best_params = best_param_op.output, # ['{\"parameterId\":\"learning_rate\",\"value\":0.0001}']\n            tensorboard_root = tensorboard_root_try1, # f-string \ub4f1\uc73c\ub85c \uc870\ud569\ud558\uba74 \uc548\ub40c\n            dataset_train = preparation_op.outputs[\"trainset_path\"],\n            dataset_valid = preparation_op.outputs[\"validset_path\"],\n            dataset_test = preparation_op.outputs[\"testset_path\"],\n        ).set_display_name(\"train-model\").set_retry(num_retries=20, backoff_duration=\"60s\", backoff_factor=2, backoff_max_duration=\"3600s\").set_caching_options(False)\n\n        evaluate_op = evaluate_model(\n            # project = project_id,\n            # location = region,\n            dataset_test = preparation_op.outputs[\"testset_path\"],\n            model = training_model_op.outputs[\"model\"],\n        ).set_display_name(\"model-evaluation\").set_cpu_limit('8').set_memory_limit('16G').set_caching_options(False)\n\n        # \uc7ac\ud559\uc2b5\n        with kfp.dsl.If(evaluate_op.output == \"false\"):\n            new_seed = int(time.time())\n            print(f'Generate new seed. (value:{new_seed})')\n\n            \"\"\"\n            \uc7ac\ud559\uc2b5\ud558\ub294 \ucef4\ud3ec\ub10c\ud2b8(\ub610\ub294 \uc704\uc758 \ucef4\ud3ec\ub10c\ud2b8\ub4e4 \uc7ac\uc815\uc758)\n            \"\"\"\n\n            retry_evaluate_op = evaluate_model(\n                # project = project_id,\n                # location = region,\n                dataset_test = retry_preparation_op.outputs[\"testset_path\"],\n                model = retry_training_model_op.outputs[\"model\"],\n            ).set_display_name(\"model-evaluation\").set_cpu_limit('8').set_memory_limit('16G').set_caching_options(False)\n\n            # \uc7ac\ud559\uc2b5 \uc131\uacf5\uc73c\ub85c \ubc30\ud3ec\n            with kfp.dsl.If(retry_evaluate_op.output == \"true\"):\n                \"\"\"\n                \uc5d4\ub4dc\ud3ec\uc778\ud2b8 \uc0dd\uc131 \ubc0f \ubc30\ud3ec\ud558\ub294 \ucef4\ud3ec\ub10c\ud2b8\n                \"\"\"\n\n            # \uc7ac\ud559\uc2b5 \uc2e4\ud328\ub85c \ud30c\uc774\ud504\ub77c\uc778 \uc885\ub8cc\n            with kfp.dsl.Else():\n                logging_op(msg=\"\uc7ac\ud559\uc2b5 \uc2e4\ud328 \uba54\uc2dc\uc9c0\", severity=\"ERROR\").set_display_name(\"retraining-fail-alert\").set_caching_options(False)\n\n\n        with kfp.dsl.Else():\n            # \uc5d4\ub4dc\ud3ec\uc778\ud2b8 \uc0dd\uc131\n            create_endpoint_op = EndpointCreateOp(\n                display_name = create_private_ep_name,\n                project = project_id,\n                location = region,\n                network = vpc_network,\n            ).after(evaluate_op).set_display_name(\"create-private-endpoint\").set_caching_options(False)\n\n            model_deploy_op = deploy_model(\n                project = project_id,\n                location = region,\n                model_name = upload_model_name,\n                model = training_model_op.outputs[\"model\"],\n                endpoint = create_endpoint_op.outputs[\"endpoint\"],\n            ).set_display_name(\"deploy-model\").set_caching_options(False)\n\n    @dsl.pipeline(name='kfp-pipeline-exit-handler')\n    def pipeline_exit_handler(\n        project_id: str,\n        project_num: str,\n        region: str,\n        vpc_network: str,\n        param1: str, # hpt\uc5d0 \uc0ac\uc6a9\ud558\ub294 \ud30c\ub77c\ubbf8\ud130 \uc608\uc2dc\n        tensorboard_root_try1: str,\n        create_private_ep_name: str,\n        upload_model_name: str,\n    ):\n        exit_task = exit_op()\n\n        with dsl.ExitHandler(exit_task):\n            pipeline(\n                project_id = PROJECT_ID,\n                project_num = PROJECT_NUM,\n                region = REGION,\n                vpc_network = PRIVATE_EP_VPC,\n                param1 = myparam1,\n                tensorboard_root_try1 = tensorboard_root_try1,\n                create_private_ep_name = create_private_ep_name,\n                upload_model_name = upload_model_name,\n            )\n\n    yaml_file = \"./kfp_pipeline.yaml\"\n\n    compiler.Compiler().compile(\n        pipeline_func = pipeline_exit_handler,\n        package_path = yaml_file\n    )\n\n    aiplatform.init(project=PROJECT_ID, location=REGION)\n\n    job = aiplatform.PipelineJob(\n        display_name = PIPELINE_NAME,\n        template_path = yaml_file,\n        pipeline_root = PIPELINE_ROOT,\n        parameter_values = {\n            \"project_id\": PROJECT_ID,\n            \"project_num\": PROJECT_NUM,\n            \"region\": REGION,\n            \"vpc_network\": PRIVATE_EP_VPC,\n            \"param1\": myparam1, # cloud run\uc73c\ub85c \ub118\uaca8\ubc1b\ub294 \ud30c\ub77c\ubbf8\ud1301\n            \"tensorboard_root_try1\": f\"{PIPELINE_ROOT}/experiment/tensorboard_log/try1/\",\n            \"create_private_ep_name\": f\"my-private-endpoint-{241231}\",\n            \"upload_model_name\": f\"my-upload-model-{1}\",\n        }\n    )\n\n    job.submit(\n        service_account = PIPELINEJOB_SA,\n        network = PRIVATE_EP_VPC,\n    )\n\nif __name__ == \"__main__\":\n    main()"
  },
  {
    "repo": "niyushabaghayi/telecom_churn_kfp",
    "file_path": "telecom_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/niyushabaghayi/telecom_churn_kfp/dev/telecom_pipeline.py",
    "content": "import kfp\nfrom kfp import dsl\nfrom kfp.components import func_to_container_op\n\n@func_to_container_op\ndef show_results(ada_boost : float, logistic_regression : float, random_forest : float, svm : float, xg_boost : float) -> None:\n    # Given the outputs from decision_tree and logistic regression components\n    # the results are shown.\n\n    print(f\"Ada Boost (accuracy): {ada_boost}\")\n    print(f\"Logistic regression (accuracy): {logistic_regression}\")\n    print(f\"Random Forest (accuracy): {random_forest}\")\n    print(f\"SVM (accuracy): {svm}\")\n    print(f\"XG Boost (accuracy): {xg_boost}\")\n\n\n@dsl.pipeline(name=\"Telecom Churn Prediction\", description=\"Applies Several Models for classification problem.\")\ndef telecom_pipeline():\n\n    # Loads the yaml manifest for each component\n    ingestion = kfp.components.load_component_from_file(\"ingestion/ingestion.yaml\")\n    preprocess = kfp.components.load_component_from_file(\"preprocess/preprocess.yaml\")\n    ada_boost = kfp.components.load_component_from_file(\"ada_boost/ada_boost.yaml\")\n    logistic_regression = kfp.components.load_component_from_file(\"logistic_regression/logistic_regression.yaml\")\n    random_forest = kfp.components.load_component_from_file(\"random_forest/random_forest.yaml\")\n    svm = kfp.components.load_component_from_file(\"svm/svm.yaml\")\n    xg_boost = kfp.components.load_component_from_file(\"xg_boost/xg_boost.yaml\")\n\n    # Run ingestion task\n    ingestion_task = ingestion()\n\n    # Run preprocess task\n    preprocess_task = preprocess(ingestion_task.output)\n\n    # Run tasks Models given\n    # the output generated by \"ingestion_task\".\n    ada_boost_task = ada_boost(preprocess_task.output)\n    logistic_regression_task = logistic_regression(preprocess_task.output)\n    random_forest_task = random_forest(preprocess_task.output)\n    svm_task = svm(preprocess_task.output)\n    xg_boost_task = xg_boost(preprocess_task.output)\n\n    # Given the outputs from \"decision_tree\" and \"logistic_regression\"\n    # the component \"show_results\" is called to print the results.\n    show_results(ada_boost_task.output, logistic_regression_task.output, random_forest_task.output, svm_task.output, xg_boost_task.output)\n\n\n\nif __name__ == \"__main__\":\n    kfp.compiler.Compiler().compile(telecom_pipeline, \"TelecomChurn.yaml\")\n    # kfp.Client().create_run_from_pipeline_func(basic_pipeline, arguments={})"
  },
  {
    "repo": "kangkannnng/Final-Project",
    "file_path": "pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/kangkannnng/Final-Project/main/pipeline.py",
    "content": "import kfp\nfrom kfp import dsl\n\ndef mount_emptydir(name=None, mount_path='/tmp'):\n  from kubernetes import client as k8sc\n  if not name:\n    import uuid\n    name=str(uuid.uuid4())[:8]\n  \n  def _mount_emptydir(task):\n    task.add_volume(\n      k8sc.V1Volume(\n        name=name,\n        empty_dir=k8sc.V1EmptyDirVolumeSource()\n      )\n    )\n    task.add_volume_mount(\n      k8sc.V1VolumeMount(\n        name=name,\n        mount_path=mount_path\n      )\n    )\n    return task\n  \n  return _mount_emptydir\n\ndef preprocess_op():\n    return dsl.ContainerOp(\n        name='Preprocess Data',\n        image='kang/preprocess:v30'\n    )\n\n\ndef train_op():\n    return dsl.ContainerOp(\n        name='Train Model',\n        image='kang/train:v30'\n    )\n\n\ndef test_op():\n    return dsl.ContainerOp(\n        name='Test Model',\n        image='kang/test:v30'\n    )\n\ndef serve_op():\n    return dsl.ContainerOp(\n        name='Serve Model',\n        image='kang/serve:v10'\n    )\n\n\n@dsl.pipeline(\n    name='Translation Pipeline',\n    description='An example pipeline that translate from Chinese to English.'\n)\n\ndef translation_pipeline():\n    _preprocess_op = preprocess_op()\n    _preprocess_op.apply(mount_emptydir(mount_path='/tmp'))\n\n    _train_op = train_op().after(_preprocess_op)\n    _train_op.apply(mount_emptydir(mount_path='/tmp'))\n\n    _test_op = test_op().after(_train_op)\n    _test_op.apply(mount_emptydir(mount_path='/tmp'))\n\n    _serve_op = serve_op().after(_test_op)\n    _serve_op.apply(mount_emptydir(mount_path='/tmp')) \n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(translation_pipeline, __file__ + '.yaml')\n"
  },
  {
    "repo": "MuhamedAyoub/kubeflow-mlops",
    "file_path": "src/main.py",
    "raw_url": "https://raw.githubusercontent.com/MuhamedAyoub/kubeflow-mlops/master/src/main.py",
    "content": "from kfp.compiler import Compiler\n\nfrom kfp import dsl\n\nfrom src.pipelines.training_model.training_pipeline import model_training_component\nfrom src.pipelines.validation_model.validation_pipeline import model_validation_with_serializable_metrics\nfrom src.pipelines.data_ingestion.ingestion_pipeline import data_ingestion_component\nfrom src.pipelines.serving_model.serving_model import deploy_model_with_kserve_sdk\n\nenv = {\n    \"minio_bucket\": \"titanic-model\",\n    \"minio_endpoint\": \"minio.kubeflow:9000\",\n    \"minio_access_key\": \"minio\",\n    \"minio_secret_key\": \"minio123\"\n    \n}\n\n@dsl.pipeline(\n    name='Titanic Survival Prediction Pipeline',\n    description='Pipeline for training and validating a Titanic survival prediction model'\n)\n\ndef titanic_pipeline(\n        dataset_url: str,\n        output_dir: str,\n        test_size: float = 0.2,\n        random_state: int = 42\n):\n    \n    data_task = data_ingestion_component(\n        dataset_url=dataset_url,\n        output_dir=output_dir,\n        minio_access_key=env[\"minio_access_key\"],\n        minio_secret_key=env[\"minio_secret_key\"],\n        minio_endpoint=env[\"minio_endpoint\"],\n    )\n\n    model_task = model_training_component(\n        train_data_dir=data_task.output,\n        random_state=random_state,\n        output_dir=output_dir,\n        minio_access_key=env[\"minio_access_key\"],\n        minio_secret_key=env[\"minio_secret_key\"],\n        minio_endpoint=env[\"minio_endpoint\"],\n        test_size=test_size,\n        \n    )\n\n    validation_task = model_validation_with_serializable_metrics(\n        model_path=model_task.output,\n        test_data_dir=data_task.output,\n        minio_access_key=env[\"minio_access_key\"],\n        minio_secret_key=env[\"minio_secret_key\"],\n        minio_endpoint=env[\"minio_endpoint\"],\n        test_size=test_size,\n    )\n    \n    deploy_task = deploy_model_with_kserve_sdk(\n        minio_access_key=env[\"minio_access_key\"],\n        minio_endpoint=env[\"minio_endpoint\"],\n        minio_secret_key=env[\"minio_secret_key\"],\n        model_bucket=env[\"minio_bucket\"],\n        model_key=model_task.output,\n        model_name=\"logistic_model.pkl\",\n        namespace=\"kubeflow\",\n        \n    )\n    deploy_task.after(validation_task)\n# Create a pipeline run\n\ndef main():\n    \"\"\"\n    Compile the pipeline to YAML and optionally run it.\n    \"\"\"\n\n    # Compile the pipeline\n    pipeline_filename = 'titanic_pipeline.yaml'\n    Compiler().compile(\n        pipeline_func=titanic_pipeline,\n        package_path=pipeline_filename\n    )\n\n    print(f\"Pipeline compiled successfully to {pipeline_filename}\")\n\n\nif __name__ == '__main__':\n    main()\n"
  },
  {
    "repo": "MuhamedAyoub/kubeflow-mlops",
    "file_path": "src/test.py",
    "raw_url": "https://raw.githubusercontent.com/MuhamedAyoub/kubeflow-mlops/master/src/test.py",
    "content": "import kfp\nfrom kfp import dsl\n\n@dsl.pipeline(\n    name='Simple Pipeline',\n    description='A simple pipeline for testing kfp'\n)\ndef simple_pipeline():\n    op = dsl.ContainerSpec(\n        image='busybox',\n        command=['echo', 'Hello from Kubeflow Pipelines!']\n    )\n\n# Compile the pipeline to a YAML file\nkfp.compiler.Compiler().compile(simple_pipeline, 'simple_pipeline.yaml')\n\nprint(\"Pipeline compiled successfully!\")\n"
  },
  {
    "repo": "MuhamedAyoub/kubeflow-mlops",
    "file_path": "src/pipelines/serving_model/serving_model.py",
    "raw_url": "https://raw.githubusercontent.com/MuhamedAyoub/kubeflow-mlops/master/src/pipelines/serving_model/serving_model.py",
    "content": "from kfp.dsl import component\nfrom typing import NamedTuple\n\n@component(\n    packages_to_install=['kserve', 'boto3'],\n)\ndef deploy_model_with_kserve_sdk(\n    model_name: str,\n    model_bucket: str,\n    model_key: str,\n    minio_endpoint: str = 'minio:9000',\n    minio_access_key: str = 'minio',\n    minio_secret_key: str = 'minio123',\n    namespace: str = 'kubeflow'\n) -> NamedTuple('Outputs', [\n    ('inference_service_name', str),\n    ('status', str),\n]):\n    from kserve import KServeClient, constants\n    from kserve.models import (\n        V1beta1InferenceService,\n        V1beta1InferenceServiceSpec,\n        V1beta1PredictorSpec,\n        V1beta1SKLearnSpec,\n        V1beta1ModelSpec,\n    )\n    import boto3\n    from collections import namedtuple\n\n    # Validate the model exists in MinIO\n    s3 = boto3.client(\n        's3',\n        endpoint_url=f'http://{minio_endpoint}',\n        aws_access_key_id=minio_access_key,\n        aws_secret_access_key=minio_secret_key,\n        config=boto3.session.Config(signature_version='s3v4'),\n        verify=False\n    )\n\n    try:\n        s3.head_object(Bucket=model_bucket, Key=model_key)\n        print(f\"Model '{model_key}' found in bucket '{model_bucket}'.\")\n    except Exception as e:\n        raise RuntimeError(f\"Model not found: {e}\")\n\n    # Initialize the KServe client\n    kserve_client = KServeClient()\n\n    # Create the InferenceService spec using the KServe SDK\n    inference_service = V1beta1InferenceService(\n        api_version=constants.KSERVE_V1BETA1,\n        kind=constants.KSERVE_KIND,\n        metadata={\"name\": model_name, \"namespace\": namespace},\n        spec=V1beta1InferenceServiceSpec(\n            predictor=V1beta1PredictorSpec(\n                sklearn=V1beta1SKLearnSpec(\n                    storage_uri=f\"s3://{model_bucket}/{model_key}\",\n                    env=[\n                        {\"name\": \"AWS_ACCESS_KEY_ID\", \"value\": minio_access_key},\n                        {\"name\": \"AWS_SECRET_ACCESS_KEY\", \"value\": minio_secret_key},\n                        {\"name\": \"AWS_ENDPOINT_URL\", \"value\": f\"http://{minio_endpoint}\"},\n                        {\"name\": \"AWS_S3_FORCE_PATH_STYLE\", \"value\": \"true\"},\n                    ],\n                )\n            )\n        )\n    )\n\n    # Deploy the InferenceService\n    try:\n        kserve_client.create(inference_service)\n        print(f\"InferenceService '{model_name}' deployed successfully.\")\n        status = \"Success\"\n    except Exception as e:\n        print(f\"Failed to deploy InferenceService: {e}\")\n        status = \"Failed\"\n\n    # Return outputs\n    output = namedtuple('Outputs', ['inference_service_name', 'status'])\n    return output(model_name, status)\n \n \n \n \n \n\n\n\n"
  },
  {
    "repo": "MuhamedAyoub/kubeflow-mlops",
    "file_path": "src/pipelines/validation_model/validation_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/MuhamedAyoub/kubeflow-mlops/master/src/pipelines/validation_model/validation_pipeline.py",
    "content": "from kfp.dsl import component, Output, ClassificationMetrics,Markdown\nfrom typing import NamedTuple, Dict, Any\nimport io\n\n@component(\n    packages_to_install=['pandas', 'scikit-learn', 'minio', 'boto3'],\n)\ndef model_validation_with_serializable_metrics(\n    metrics: Output[ClassificationMetrics],\n    markdown: Output[Markdown],\n    model_path: str,\n    test_data_dir: str,\n    minio_endpoint: str = 'minio:9000',\n    minio_access_key: str = 'minio',\n    minio_secret_key: str = 'minio123',\n    test_size: float = 0.2,\n    random_state: int = 42\n) -> NamedTuple('Outputs', [\n    ('accuracy', float), \n    ('precision', float), \n    ('recall', float), \n]):\n    import pandas as pd\n    import joblib\n    import boto3\n    from io import BytesIO\n    from sklearn.metrics import (\n        accuracy_score, \n        precision_score, \n        recall_score, \n        classification_report, \n        confusion_matrix\n    )\n    from sklearn.model_selection import train_test_split\n    from collections import namedtuple\n    try: \n        # Create S3 client\n        s3 = boto3.client(\n            's3',\n            endpoint_url=f'http://{minio_endpoint}',\n            aws_access_key_id=minio_access_key,\n            aws_secret_access_key=minio_secret_key,\n            config=boto3.session.Config(signature_version='s3v4'),\n            verify=False\n        )\n\n        # Load data\n        data_obj = s3.get_object(Bucket='titanic-data', Key='titanic_data.csv')\n        data = pd.read_csv(data_obj['Body'])\n\n        # Load model directly from S3\n        model_obj = s3.get_object(Bucket='titanic-model', Key='logistic_model.pkl')\n        model_bytes = model_obj['Body'].read()\n        model = joblib.load(BytesIO(model_bytes))\n\n        # Prepare features\n        feature_columns = [\n            'Pclass', 'Age', 'SibSp', 'Parch', 'Fare', \n            'Sex_male', 'Embarked_Q', 'Embarked_S'\n        ]\n        X = data[feature_columns]\n        y = data['Survived']\n\n        # Split data\n        X_train, X_test, y_train, y_test = train_test_split(\n            X, y,\n            test_size=test_size,\n            random_state=random_state\n        )\n\n        # Predictions and metrics\n        y_pred = model.predict(X_test)\n        \n        # Compute metrics\n        accuracy = float(accuracy_score(y_test, y_pred))\n        precision = float(precision_score(y_test, y_pred, average='binary'))\n        recall = float(recall_score(y_test, y_pred, average='binary'))\n        \n        # Confusion Matrix\n        cm = confusion_matrix(y_test, y_pred)\n        metrics.log_confusion_matrix(['Not Survived', 'Survived'], cm.tolist())\n        \n        # classification report\n        report = classification_report(y_test, y_pred, output_dict=True)\n        \n        # use markdown to display classification report\n        markdown_content = f\"\"\"\n        # Classification Report\n        | Class | Precision | Recall | F1-Score | Support | \n        | --- | --- | --- | --- | --- |\n        | Not Survived | {report['0']['precision']} | {report['0']['recall']} | {report['0']['f1-score']} | {report['0']['support']} |\n        | Survived | {report['1']['precision']} | {report['1']['recall']} | {report['1']['f1-score']} | {report['1']['support']} |  \n        \"\"\"\n        with open(markdown.path, 'w') as f:\n            f.write(markdown_content)\n        \n        # Prepare output\n        output = namedtuple('Outputs', [\n            'accuracy', 'precision', 'recall'\n        ])\n        return output(\n            accuracy, \n            precision, \n            recall, \n        )\n    except Exception as e:\n        print(f\"Validation failed: {e}\")\n        raise RuntimeError(f\"Validation failed: {e}\")\n    "
  },
  {
    "repo": "bmorphism/kfsummit19",
    "file_path": "resnet_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/bmorphism/kfsummit19/master/resnet_pipeline.py",
    "content": "import kfp\nimport kfp.dsl as dsl\nfrom kfp.components import func_to_container_op\nfrom kfp.onprem import mount_pvc\n\nc= kfp.Client(\"127.0.0.1:8082/pipeline\", namespace=\"kfsummit\")\n\ndef PreprocessOp(name, input_dir, output_dir):\n    return dsl.ContainerOp(\n        name=name,\n        image='eu.gcr.io/kfsummit/preprocess_resnet',\n        arguments=[\n            '--input_dir', input_dir,\n            '--output_dir', output_dir,\n        ],\n        file_outputs={'output': '/output.txt'}\n    )\n\ndef TrainOp(name, input_dir, output_dir, model_name, model_version, epochs):\n    return dsl.ContainerOp(\n        name=name,\n        image='eu.gcr.io/kfsummit/train_resnet',\n        arguments=[\n            '--input_dir', input_dir,\n            '--output_dir', output_dir,\n            '--model_name', model_name,\n            '--model_version', model_version,\n            '--epochs', epochs\n        ],\n        file_outputs={'output': '/output.txt'}\n    )\n\n@dsl.pipeline(\n    name='resnet_cifar10_pipeline',\n    description='Demonstrate an end-to-end training & serving pipeline using ResNet and CIFAR-10'\n)\ndef resnet_pipeline(\n    raw_data_dir='/mnt/workspace/raw_data',\n    processed_data_dir='/mnt/workspace/processed_data',\n    model_dir='/mnt/workspace/saved_model',\n    epochs=50,\n    trtserver_name='trtis',\n    model_name='resnet_graphdef',\n    model_version=1,\n    webapp_prefix='webapp',\n    webapp_port=80\n):\n    pvc_name = 'kfsummit-workspace-read-claim'\n    volume_name = 'kfsummit-workspace'\n    volume_mount_path = '/mnt/workspace'\n\n    op_dict = {}\n\n    op_dict['preprocess'] = PreprocessOp(\n        'preprocess', raw_data_dir, processed_data_dir)\n\n    op_dict['train'] = TrainOp(\n\t'train', op_dict['preprocess'].output, model_dir, model_name, model_version, epochs)\n\n    for _, container_op in op_dict.items():\n        container_op.apply(mount_pvc(pvc_name, volume_name, volume_mount_path))"
  },
  {
    "repo": "anhdan/kfp-first-pipelines",
    "file_path": "l1-addition-pipeline/kfp_client.py",
    "raw_url": "https://raw.githubusercontent.com/anhdan/kfp-first-pipelines/main/l1-addition-pipeline/kfp_client.py",
    "content": "import kfp\nimport my_pipeline as mykfp\n\n# Connect to Kubeflow Pipelines cluster\nclient = kfp.Client( host='http://3.34.205.245:80' )\nprint( client.list_pipelines() )\n\n# Specify pipeline argument values\narguments = {'a': '7', 'b': '8'}\n\n# Create run for the addition pipeline created in my_pipeline.py\nrun = client.create_run_from_pipeline_func(\n    mykfp.add_pipeline,\n    arguments=arguments,\n    mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE,\n    experiment_name='my-addition-experiment'\n)\n"
  },
  {
    "repo": "anhdan/kfp-first-pipelines",
    "file_path": "l1-addition-pipeline/my_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/anhdan/kfp-first-pipelines/main/l1-addition-pipeline/my_pipeline.py",
    "content": "import kfp.dsl as dsl\n\nimport my_components as comps\n\n# Define a pipeline\n@dsl.pipeline(\n    name='Addition pipeline',\n    description='A toy pipeline that performs addition calculations.'\n)\ndef add_pipeline(\n    a: float = 1,\n    b: float = 7,\n):\n    first_add_task = comps.add(a, 4)\n    second_add_task = comps.add(first_add_task.output, b)"
  },
  {
    "repo": "anhdan/kfp-first-pipelines",
    "file_path": "l2-tf-pipeline/kfp_client.py",
    "raw_url": "https://raw.githubusercontent.com/anhdan/kfp-first-pipelines/main/l2-tf-pipeline/kfp_client.py",
    "content": "import kfp\nimport my_pipeline as mykfp\n\n# Connect to Kubeflow Pipelines cluster\nclient = kfp.Client( host='http://3.34.205.245:80' )\nprint( client.list_pipelines() )\n\n# Specify pipeline argument values\narguments = {'a': 7, 'b': 8}\n\n# Submit a pipeline run\nclient.create_run_from_pipeline_func(\n    mykfp.calc_pipeline,\n    arguments=arguments,\n    mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE)"
  },
  {
    "repo": "anhdan/kfp-first-pipelines",
    "file_path": "l2-tf-pipeline/my_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/anhdan/kfp-first-pipelines/main/l2-tf-pipeline/my_pipeline.py",
    "content": "import kfp.dsl as dsl\nimport my_components as comps\n\n@dsl.pipeline(\n   name='calculation-pipeline',\n   description='An example pipeline that performs arithmetic calculations.',\n)\ndef calc_pipeline(\n   a: float=1,\n   b: float=7,\n   c: float=17,\n):\n    # Passes a pipeline parameter and a constant value as operation arguments.\n    add_task = comps.add(a, 4) # The add_op factory function returns\n                            # a dsl.ContainerOp class instance. \n\n    # Passes the output of the add_task and a pipeline parameter as operation\n    # arguments. For an operation with a single return value, the output\n    # reference is accessed using `task.output` or\n    # `task.outputs['output_name']`.\n    divmod_task = comps.my_divmod(add_task.output, b)\n\n    # For an operation with multiple return values, output references are\n    # accessed as `task.outputs['output_name']`.\n    result_task = comps.add(divmod_task.outputs['quotient'], c)\n"
  },
  {
    "repo": "mpaul7/end-to-end-ai-pipelines-using-kubeflow",
    "file_path": "src/maikube/pipeline/feature_extraction.py",
    "raw_url": "https://raw.githubusercontent.com/mpaul7/end-to-end-ai-pipelines-using-kubeflow/main/src/maikube/pipeline/feature_extraction.py",
    "content": "from kfp.dsl import pipeline, get_pipeline_conf\n\nfrom src.maikube.components import load_component\n\n@pipeline(name=\"process NFStream pipeline\")\ndef process_nfstream_pipeline(input_bucket:str, input_pcap:str, output_bucket:str, output_file:str):\n    minio_download = load_component('minio_download')\n    minio_upload = load_component('minio_upload')\n    nfstream = load_component('nfs_feature_extractoion')\n\n    download_op = minio_download(f'kubeflow/{input_pcap_bucket}/{input_pcap_file}')\n    process_op = nfstream(download_op.output)\n    minio_upload(process_op.output, f'kubeflow/{output_bucket}/{output_file}')\n    get_pipeline_conf().set_image_pull_policy(policy=\"Never\")"
  },
  {
    "repo": "DharmitD/mlops-cicd-pipeline",
    "file_path": "dsl-convert.py",
    "raw_url": "https://raw.githubusercontent.com/DharmitD/mlops-cicd-pipeline/main/dsl-convert.py",
    "content": "import sys\nimport os\nimport ast\nimport re\nfrom kfp import dsl\nfrom kfp.dsl import Input, Output, Dataset, Model\n\ndef read_python_file(file_path):\n    \"\"\" Reads the content of the given Python file. \"\"\"\n    with open(file_path, \"r\") as f:\n        return f.read()\n\ndef extract_functions(code):\n    \"\"\" Extracts function definitions from Python code. \"\"\"\n    tree = ast.parse(code)\n    functions = {}\n\n    for node in tree.body:\n        if isinstance(node, ast.FunctionDef):\n            function_body = \"\\n\".join([ast.unparse(stmt) for stmt in node.body])\n            if function_body.strip():\n                args = [arg.arg for arg in node.args.args]\n                num_outputs = function_body.count(\"return \")\n\n                functions[node.name] = {\n                    \"body\": function_body,\n                    \"args\": args,\n                    \"num_outputs\": num_outputs\n                }\n\n    return functions\n\ndef extract_imports(code):\n    \"\"\" Extracts all import statements from Python code. \"\"\"\n    tree = ast.parse(code)\n    imports = []\n\n    for node in tree.body:\n        if isinstance(node, (ast.Import, ast.ImportFrom)):\n            imports.append(ast.unparse(node))\n\n    return \"\\n\".join(imports)\n\ndef convert_to_kfp_dsl(file_path):\n    \"\"\" Converts a given Python ML script into a Kubeflow Pipelines DSL format. \"\"\"\n    code = read_python_file(file_path)\n    functions = extract_functions(code)\n    imports = extract_imports(code)\n\n    if not functions:\n        print(\"No functions found in the provided script.\")\n        sys.exit(1)\n\n    dsl_code = \"\"\"\nfrom kfp import dsl\nfrom kfp.dsl import Input, Output, Dataset, Model\n\"\"\" + imports + \"\\n\"\n\n    component_templates = []\n    function_calls = []\n    previous_outputs = {}\n\n    for function_name, details in functions.items():\n        function_body = details[\"body\"]\n        function_args = details[\"args\"]\n        num_outputs = details[\"num_outputs\"]\n\n        kfp_args = \", \".join([f\"{arg}: Dataset\" for arg in function_args])\n\n        # Fix: Handling multiple outputs correctly\n        if function_name == \"preprocess_data\":\n            return_type = \"Tuple[Dataset, Dataset]\"\n            output_vars = [\"X_train\", \"y_train\"]\n        elif function_name == \"train_model\":\n            return_type = \"Model\"\n            output_vars = [\"model\"]\n        elif function_name == \"load_data\":\n            return_type = \"Dataset\"\n            output_vars = [\"df\"]\n        else:\n            return_type = \"Dataset\"\n            output_vars = [\"output\"]\n\n        outputs = \", \".join(output_vars)\n\n        component_templates.append(f\"\"\"\n@dsl.component(\n    base_image=\"python:3.9\",\n    packages_to_install=[\"pandas\", \"scikit-learn\", \"tensorflow\"]\n)\ndef {function_name}({kfp_args}) -> {return_type}:\n{re.sub(\"^\", \"    \", function_body, flags=re.MULTILINE)}\n\"\"\")\n\n        # Store outputs with correct variable names\n        if function_args:\n            input_args = []\n            for arg in function_args:\n                if arg in previous_outputs:\n                    input_args.append(f\"{previous_outputs[arg]}.output\")\n                else:\n                    print(f\"\u26a0\ufe0f Warning: Argument '{arg}' not found in previous outputs!\")\n                    input_args.append(\"MISSING_ARG\")\n\n            input_args_str = \", \".join(input_args)\n            function_calls.append(f\"    {outputs} = {function_name}({input_args_str})\")\n        else:\n            function_calls.append(f\"    {outputs} = {function_name}()\")\n\n        print(f\"\ud83d\udd39 Storing function output: {function_name} -> {outputs}\")\n\n        # Fix: Store multiple outputs correctly\n        for i, arg in enumerate(output_vars):\n            previous_outputs[arg] = output_vars[i]\n\n    dsl_code += \"\\n\".join(component_templates) + \"\\n\"\n\n    dsl_code += \"\"\"\n@dsl.pipeline(name=\"ml-pipeline\")\ndef ml_pipeline():\n\"\"\" + \"\\n\".join(function_calls) + \"\\n\"\n\n    dsl_code += \"\"\"\nif __name__ == \"__main__\":\n    from kfp import compiler\n    compiler.Compiler().compile(ml_pipeline, \"ml_pipeline.yaml\")\n\"\"\"\n\n    return dsl_code\n\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python dsl-convert.py <input_python_script>\")\n        sys.exit(1)\n\n    input_file = sys.argv[1]\n\n    if not os.path.exists(input_file):\n        print(f\"Error: The file '{input_file}' does not exist.\")\n        sys.exit(1)\n\n    dsl_output = convert_to_kfp_dsl(input_file)\n\n    output_file = \"generated_new_pipeline.py\"\n    with open(output_file, \"w\") as f:\n        f.write(dsl_output)\n\n    print(f\"\u2705 Successfully generated Kubeflow DSL: {output_file}\")\n"
  },
  {
    "repo": "DharmitD/mlops-cicd-pipeline",
    "file_path": "generated_new_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/DharmitD/mlops-cicd-pipeline/main/generated_new_pipeline.py",
    "content": "from click import Tuple\nfrom kfp import dsl\nfrom kfp.dsl import Input, Output, Dataset, Model\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nimport pickle\n\n@dsl.component(\n    base_image=\"python:3.9\",\n    packages_to_install=[\"pandas\", \"scikit-learn\", \"tensorflow\"]\n)\ndef load_data() -> Dataset:\n    df = pd.read_csv('data.csv')\n    return df\n\n\n@dsl.component(\n    base_image=\"python:3.9\",\n    packages_to_install=[\"pandas\", \"scikit-learn\", \"tensorflow\"]\n)\ndef preprocess_data(df: Dataset) -> Tuple[Dataset, Dataset]:\n    X = df.drop(columns=['target'])\n    y = df['target']\n    return train_test_split(X, y, test_size=0.2)\n\n\n@dsl.component(\n    base_image=\"python:3.9\",\n    packages_to_install=[\"pandas\", \"scikit-learn\", \"tensorflow\"]\n)\ndef train_model(X_train: Dataset, y_train: Dataset) -> Model:\n    model = LogisticRegression()\n    model.fit(X_train, y_train)\n    return model\n\n\n@dsl.component(\n    base_image=\"python:3.9\",\n    packages_to_install=[\"pandas\", \"scikit-learn\", \"tensorflow\"]\n)\ndef save_model(model: Dataset) -> Dataset:\n    with open('model.pkl', 'wb') as f:\n        pickle.dump(model, f)\n\n\n@dsl.pipeline(name=\"ml-pipeline\")\ndef ml_pipeline():\n    df = load_data()\n    X_train, y_train = preprocess_data(df.output)\n    model = train_model(X_train.output, y_train.output)\n    output = save_model(model.output)\n\nif __name__ == \"__main__\":\n    from kfp import compiler\n    compiler.Compiler().compile(ml_pipeline, \"ml_pipeline.yaml\")\n"
  },
  {
    "repo": "abzeefly/kubeflow-local-k8s",
    "file_path": "kfp/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/abzeefly/kubeflow-local-k8s/main/kfp/pipeline.py",
    "content": "import argparse\nimport kfp\n\nfrom kfp import components as comp\nfrom kfp.v2 import dsl\nfrom kfp.v2.compiler import Compiler\n\n\ndef parse_args():\n  \"\"\"Parse arguments.\"\"\"\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n      \"--gcp-project-id\",\n      type=str,\n      help=\"ID for the google cloud project to deploy the pipeline to.\",\n      required=True)\n  parser.add_argument(\n      \"--region\",\n      type=str,\n      help=\"Region in which to deploy the pipeline.\",\n      required=True)\n  parser.add_argument(\n      \"--pipeline-root\",\n      type=str,\n      help=\n      \"Path to artifact repository where Kubeflow Pipelines stores a pipeline\u2019s artifacts.\",\n      required=True)\n  parser.add_argument(\n      \"--component-artifact-root\",\n      type=str,\n      help=\n      \"Path to artifact repository where Kubeflow Pipelines components can store artifacts.\",\n      required=True)\n  parser.add_argument(\n      \"--dataflow-staging-root\",\n      type=str,\n      help=\"Path to staging directory for dataflow.\",\n      required=True)\n  parser.add_argument(\n      \"--beam-runner\",\n      type=str,\n      help=\"Beam runner: DataflowRunner or DirectRunner.\",\n      default=\"DirectRunner\")\n  return parser.parse_args()\n\n\n# arguments are parsed as a global variable so\n# they can be used in the pipeline decorator below\nARGS = parse_args()\nPIPELINE_ROOT = vars(ARGS)['pipeline_root']\n\n# [START load_kfp_components]\n# load the kfp components from their yaml files\nDataIngestOp = comp.load_component('components/ingestion/component.yaml')\nDataPreprocessingOp = comp.load_component(\n    'components/preprocessing/component.yaml')\nTrainModelOp = comp.load_component('components/train/component.yaml')\n# [END load_kfp_components]\n\n\n# [START define_kfp_pipeline]\n@dsl.pipeline(\n    pipeline_root=PIPELINE_ROOT,\n    name=\"beam-preprocessing-kfp-example\",\n    description=\"Pipeline to show an apache beam preprocessing example in KFP\")\ndef pipeline(\n    gcp_project_id: str,\n    region: str,\n    component_artifact_root: str,\n    dataflow_staging_root: str,\n    beam_runner: str):\n  \"\"\"KFP pipeline definition.\n\n  Args:\n      gcp_project_id (str): ID for the google cloud project to deploy the pipeline to.\n      region (str): Region in which to deploy the pipeline.\n      component_artifact_root (str): Path to artifact repository where Kubeflow Pipelines\n        components can store artifacts.\n      dataflow_staging_root (str): Path to staging directory for the dataflow runner.\n      beam_runner (str): Beam runner: DataflowRunner or DirectRunner.\n  \"\"\"\n\n  ingest_data_task = DataIngestOp(base_artifact_path=component_artifact_root)\n\n  data_preprocessing_task = DataPreprocessingOp(\n      ingested_dataset_path=ingest_data_task.outputs[\"ingested_dataset_path\"],\n      base_artifact_path=component_artifact_root,\n      gcp_project_id=gcp_project_id,\n      region=region,\n      dataflow_staging_root=dataflow_staging_root,\n      beam_runner=beam_runner)\n\n  train_model_task = TrainModelOp(\n      preprocessed_dataset_path=data_preprocessing_task.\n      outputs[\"preprocessed_dataset_path\"],\n      base_artifact_path=component_artifact_root)\n\n\n# [END define_kfp_pipeline]\n\nif __name__ == \"__main__\":\n  # [START compile_kfp_pipeline]\n  Compiler().compile(pipeline_func=pipeline, package_path=\"pipeline.json\")\n  # [END compile_kfp_pipeline]\n\n  run_arguments = vars(ARGS)\n  del run_arguments['pipeline_root']\n\n  # [START execute_kfp_pipeline]\n  client = kfp.Client(host=\"http://127.0.0.1/pipline\")\n  experiment = client.create_experiment(\"KFP orchestration example\")\n  run_result = client.run_pipeline(\n      experiment_id=experiment.id,\n      job_name=\"KFP orchestration job\",\n      pipeline_package_path=\"pipeline.json\",\n      params=run_arguments)\n  # [END execute_kfp_pipeline]"
  },
  {
    "repo": "stackdemos/clorox-kf",
    "file_path": "components/golang/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/clorox-kf/master/components/golang/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef processing_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('GOLANG_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=[script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n"
  },
  {
    "repo": "stackdemos/clorox-kf",
    "file_path": "components/training/component.py",
    "raw_url": "https://raw.githubusercontent.com/stackdemos/clorox-kf/master/components/training/component.py",
    "content": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef training_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('TRAINING_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=['/usr/local/bin/python', script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )\n\n"
  },
  {
    "repo": "mhash1m/kubeflow_kfp_workflow",
    "file_path": "xgboost_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/mhash1m/kubeflow_kfp_workflow/main/xgboost_pipeline.py",
    "content": "import os\nfrom typing import NamedTuple\n\nimport numpy as np\n\nimport kfp.components as comp\nfrom kfp import dsl, compiler,  Client\n\nfrom client import create_client\n\n# Define the data loading function\ndef load_data_op(random_state:int, test_size:float, \n    train_path: comp.OutputPath('CSV'),\n    test_path: comp.OutputPath('CSV')):\n\n    from sklearn.datasets import make_regression\n    from sklearn.model_selection import train_test_split\n    import pandas as pd\n    \n    # Load the Boston housing dataset\n    X, Y = make_regression()\n    data = pd.DataFrame(X)\n    data['Y'] = Y\n    # Split the data into training and test sets\n    train, test = train_test_split(data, test_size=test_size, random_state=random_state)\n    # Save to output paths\n    train.to_csv(train_path, index=False)\n    test.to_csv(test_path, index=False)\n\n# Define the model training function\ndef train_model_op(train_path: comp.InputPath('CSV'), model_path: comp.OutputPath('PKL')):# -> NamedTuple('Outputs', [('model', object), ('model_path', str)]):#, learning_rate: float, max_depth: int, subsample: float, n_estimators: int):\n    import pickle\n    import os\n    import xgboost as xgb\n    import pandas as pd\n\n    train_data = pd.read_csv(train_path)\n    \n    # Create the XGBoost model\n    xgb_model = xgb.XGBRegressor()\n\n    # Fit the model to the training data\n    xgb_model.fit(train_data.drop('Y', axis = 1), train_data['Y'])\n    \n    # Save model\n    pickle.dump(xgb_model, open(model_path, \"wb\"))\n\n# Define the model evaluation function\ndef evaluate_model_op(test_path: comp.InputPath('CSV'), model_path: comp.InputPath('PKL')) -> NamedTuple('Outputs', [\n  ('mlpipeline_metrics', 'Metrics'),\n]):\n    from sklearn.metrics import mean_squared_error\n    import pandas as pd\n    import pickle\n    import json\n\n    test_data = pd.read_csv(test_path)\n    xgb_model = pickle.load(open(model_path, \"rb\"))\n    # Make predictions on the test set\n    y_pred = xgb_model.predict(test_data.drop('Y', axis = 1))\n\n    # Calculate the mean squared error of the predictions\n    mse = mean_squared_error(test_data['Y'], y_pred)\n    metrics = {\n        'metrics': [{\n        'name': 'mse-score', # The name of the metric. Visualized as the column name in the runs table.\n        'numberValue':  mse, # The value of the metric. Must be a numeric value.\n        'format': \"RAW\",   # The optional format of the metric. Supported values are \"RAW\" (displayed in raw format) and \"PERCENTAGE\" (displayed in percentage format).\n        }]\n    }\n    return [json.dumps(metrics)]\n\ndef create_components():\n    data_op = comp.create_component_from_func(func=load_data_op, base_image='huanjason/scikit-learn', output_component_file='load_data_component.yaml')\n    train_op = comp.create_component_from_func(func=train_model_op, base_image='huanjason/scikit-learn', output_component_file='train_model_component.yaml')\n    eval_op = comp.create_component_from_func(func=evaluate_model_op, base_image='huanjason/scikit-learn', output_component_file='evaluate_model_component.yaml')\n    return data_op, train_op, eval_op\n\n# Define the pipeline\n@dsl.pipeline(\n    name=\"XGBoost Pipeline\",\n    description=\"A pipeline that trains an XGBoost model on the Boston housing dataset.\"\n)\ndef xgboost_pipeline(random_state: int =20, test_size: float =0.2):\n    data_op, train_op, eval_op=create_components()\n    data_prep = data_op(random_state, test_size)\n    trainer = train_op(data_prep.outputs['train'])\n    mse_score = eval_op(data_prep.outputs['test'], trainer.outputs['model']).output\n\nif __name__ == '__main__':\n    client = create_client()\n    client.create_run_from_pipeline_func(xgboost_pipeline, {}, experiment_name=\"Test XG_Boost\")\n"
  },
  {
    "repo": "rujual/telco_churn_pipeline",
    "file_path": "xgb sample pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/rujual/telco_churn_pipeline/master/xgb%20sample%20pipeline.py",
    "content": "# !/usr/bin/env python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport json\nimport kfp\nfrom kfp import components\nfrom kfp import dsl\nimport os\nimport subprocess\n\ndiagnose_me_op = components.load_component_from_url(\n'https://raw.githubusercontent.com/kubeflow/pipelines/566dddfdfc0a6a725b6e50ea85e73d8d5578bbb9/components/diagnostics/diagnose_me/component.yaml')\n\nconfusion_matrix_op = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/2df775a28045bda15372d6dd4644f71dcfe41bfe/components/local/confusion_matrix/component.yaml')\n\nroc_op = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/2df775a28045bda15372d6dd4644f71dcfe41bfe/components/local/roc/component.yaml')\n\ndataproc_create_cluster_op = components.load_component_from_url(\n'https://raw.githubusercontent.com/kubeflow/pipelines/2df775a28045bda15372d6dd4644f71dcfe41bfe/components/gcp/dataproc/create_cluster/component.yaml')\n\ndataproc_delete_cluster_op = components.load_component_from_url(\n'https://raw.githubusercontent.com/kubeflow/pipelines/2df775a28045bda15372d6dd4644f71dcfe41bfe/components/gcp/dataproc/delete_cluster/component.yaml')\n\ndataproc_submit_pyspark_op = components.load_component_from_url(\n'https://raw.githubusercontent.com/kubeflow/pipelines/2df775a28045bda15372d6dd4644f71dcfe41bfe/components/gcp/dataproc/submit_pyspark_job/component.yaml'\n)\n\ndataproc_submit_spark_op = components.load_component_from_url(\n'https://raw.githubusercontent.com/kubeflow/pipelines/2df775a28045bda15372d6dd4644f71dcfe41bfe/components/gcp/dataproc/submit_spark_job/component.yaml'\n)\n\n_PYSRC_PREFIX = 'gs://ml-pipeline-playground/dataproc-example'  # Common path to python src.\n\n_XGBOOST_PKG = 'gs://ml-pipeline-playground/xgboost4j-example-0.8-SNAPSHOT-jar-with-dependencies.jar'\n\n_TRAINER_MAIN_CLS = 'ml.dmlc.xgboost4j.scala.example.spark.XGBoostTrainer'\n\n_PREDICTOR_MAIN_CLS = 'ml.dmlc.xgboost4j.scala.example.spark.XGBoostPredictor'\n\n\ndef delete_directory_from_gcs(dir_path):\n    \"\"\"Delete a GCS dir recursively. Ignore errors.\"\"\"\n    try:\n        subprocess.call(['gsutil', '-m', 'rm', '-r', dir_path])\n    except:\n        pass\n\n\n# ! Please do not forget to enable the Dataproc API in your cluster https://console.developers.google.com/apis/api/dataproc.googleapis.com/overview\n\n# ================================================================\n# The following classes should be provided by components provider.\n\n\ndef dataproc_analyze_op(\n        project,\n        region,\n        cluster_name,\n        schema,\n        train_data,\n        output):\n    \"\"\"Submit dataproc analyze as a pyspark job.\n    :param project: GCP project ID.\n    :param region: Which zone to run this analyze.\n    :param cluster_name: Name of the cluster.\n    :param schema: GCS path to the schema.\n    :param train_data: GCS path to the training data.\n    :param output: GCS path to store the output.\n    \"\"\"\n    return dataproc_submit_pyspark_op(\n        project_id=project,\n        region=region,\n        cluster_name=cluster_name,\n        main_python_file_uri=os.path.join(_PYSRC_PREFIX, 'analyze_run.py'),\n        args=['--output', str(output), '--train', str(train_data), '--schema', str(schema)]\n    )\n\n\ndef dataproc_transform_op(\n        project,\n        region,\n        cluster_name,\n        train_data,\n        eval_data,\n        target,\n        analysis,\n        output\n):\n    \"\"\"Submit dataproc transform as a pyspark job.\n    :param project: GCP project ID.\n    :param region: Which zone to run this analyze.\n    :param cluster_name: Name of the cluster.\n    :param train_data: GCS path to the training data.\n    :param eval_data: GCS path of the eval csv file.\n    :param target: Target column name.\n    :param analysis: GCS path of the analysis results\n    :param output: GCS path to use for output.\n    \"\"\"\n\n    # Remove existing [output]/train and [output]/eval if they exist.\n    delete_directory_from_gcs(os.path.join(output, 'train'))\n    delete_directory_from_gcs(os.path.join(output, 'eval'))\n\n    return dataproc_submit_pyspark_op(\n        project_id=project,\n        region=region,\n        cluster_name=cluster_name,\n        main_python_file_uri=os.path.join(_PYSRC_PREFIX,\n                                          'transform_run.py'),\n        args=[\n            '--output',\n            str(output),\n            '--analysis',\n            str(analysis),\n            '--target',\n            str(target),\n            '--train',\n            str(train_data),\n            '--eval',\n            str(eval_data)\n        ])\n\n\ndef dataproc_train_op(\n        project,\n        region,\n        cluster_name,\n        train_data,\n        eval_data,\n        target,\n        analysis,\n        workers,\n        rounds,\n        output,\n        is_classification=True\n):\n    if is_classification:\n        config = 'gs://ml-pipeline-playground/trainconfcla.json'\n    else:\n        config = 'gs://ml-pipeline-playground/trainconfreg.json'\n\n    return dataproc_submit_spark_op(\n        project_id=project,\n        region=region,\n        cluster_name=cluster_name,\n        main_class=_TRAINER_MAIN_CLS,\n        spark_job=json.dumps({'jarFileUris': [_XGBOOST_PKG]}),\n        args=json.dumps([\n            str(config),\n            str(rounds),\n            str(workers),\n            str(analysis),\n            str(target),\n            str(train_data),\n            str(eval_data),\n            str(output)\n        ]))\n\n\ndef dataproc_predict_op(\n        project,\n        region,\n        cluster_name,\n        data,\n        model,\n        target,\n        analysis,\n        output\n):\n    return dataproc_submit_spark_op(\n        project_id=project,\n        region=region,\n        cluster_name=cluster_name,\n        main_class=_PREDICTOR_MAIN_CLS,\n        spark_job=json.dumps({'jarFileUris': [_XGBOOST_PKG]}),\n        args=json.dumps([\n            str(model),\n            str(data),\n            str(analysis),\n            str(target),\n            str(output)\n        ]))\n\n\n# =======================================================================\n\n@dsl.pipeline(\n    name='XGBoost Trainer',\n    description='A trainer that does end-to-end distributed training for XGBoost models.'\n)\ndef xgb_train_pipeline(\n        output='gs://{{kfp-default-bucket}}',\n        project='{{kfp-project-id}}',\n        diagnostic_mode='HALT_ON_ERROR',\n        rounds=5,\n):\n    output_template = str(output) + '/' + dsl.RUN_ID_PLACEHOLDER + '/data'\n    region = 'us-central1'\n    workers = 2\n    quota_check = [{'region': region, 'metric': 'CPUS', 'quota_needed': 12.0}]\n    train_data = 'gs://ml-pipeline-playground/sfpd/train.csv'\n    eval_data = 'gs://ml-pipeline-playground/sfpd/eval.csv'\n    schema = 'gs://ml-pipeline-playground/sfpd/schema.json'\n    true_label = 'ACTION'\n    target = 'resolution'\n    required_apis = 'dataproc.googleapis.com'\n    cluster_name = 'xgb-%s' % dsl.RUN_ID_PLACEHOLDER\n\n    # Current GCP pyspark/spark op do not provide outputs as return values, instead,\n    # we need to use strings to pass the uri around.\n    analyze_output = output_template\n    transform_output_train = os.path.join(output_template, 'train', 'part-*')\n    transform_output_eval = os.path.join(output_template, 'eval', 'part-*')\n    train_output = os.path.join(output_template, 'train_output')\n    predict_output = os.path.join(output_template, 'predict_output')\n\n    _diagnose_me_op = diagnose_me_op(\n        bucket=output,\n        execution_mode=diagnostic_mode,\n        project_id=project,\n        target_apis=required_apis,\n        quota_check=quota_check)\n\n    with dsl.ExitHandler(exit_op=dataproc_delete_cluster_op(\n            project_id=project,\n            region=region,\n            name=cluster_name\n    )):\n        _create_cluster_op = dataproc_create_cluster_op(\n            project_id=project,\n            region=region,\n            name=cluster_name,\n            initialization_actions=[\n                os.path.join(_PYSRC_PREFIX,\n                             'initialization_actions.sh'),\n            ],\n            image_version='1.2'\n        ).after(_diagnose_me_op)\n\n        _analyze_op = dataproc_analyze_op(\n            project=project,\n            region=region,\n            cluster_name=cluster_name,\n            schema=schema,\n            train_data=train_data,\n            output=output_template\n        ).after(_create_cluster_op).set_display_name('Analyzer')\n\n        _transform_op = dataproc_transform_op(\n            project=project,\n            region=region,\n            cluster_name=cluster_name,\n            train_data=train_data,\n            eval_data=eval_data,\n            target=target,\n            analysis=analyze_output,\n            output=output_template\n        ).after(_analyze_op).set_display_name('Transformer')\n\n        _train_op = dataproc_train_op(\n            project=project,\n            region=region,\n            cluster_name=cluster_name,\n            train_data=transform_output_train,\n            eval_data=transform_output_eval,\n            target=target,\n            analysis=analyze_output,\n            workers=workers,\n            rounds=rounds,\n            output=train_output\n        ).after(_transform_op).set_display_name('Trainer')\n\n        _predict_op = dataproc_predict_op(\n            project=project,\n            region=region,\n            cluster_name=cluster_name,\n            data=transform_output_eval,\n            model=train_output,\n            target=target,\n            analysis=analyze_output,\n            output=predict_output\n        ).after(_train_op).set_display_name('Predictor')\n\n        _cm_op = confusion_matrix_op(\n            predictions=os.path.join(predict_output, 'part-*.csv'),\n            output_dir=output_template\n        ).after(_predict_op)\n\n        _roc_op = roc_op(\n            predictions_dir=os.path.join(predict_output, 'part-*.csv'),\n            true_class=true_label,\n            true_score_column=true_label,\n            output_dir=output_template\n        ).after(_predict_op)\n\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(xgb_train_pipeline, __file__ + '.yaml')\n"
  },
  {
    "repo": "actions-marketplace-validations/f6wbl6_kubeflow-pipelines-deploy-action",
    "file_path": "example/example_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/actions-marketplace-validations/f6wbl6_kubeflow-pipelines-deploy-action/master/example/example_pipeline.py",
    "content": "from kfp import dsl\nfrom kfp.components import InputPath, OutputPath, func_to_container_op\n\n\ndef path_csv_pipeline(github_sha: str):\n    \"\"\"Making arbitrary Dataframe with specified columns and rows\"\"\"\n\n    @func_to_container_op\n    def make_csv(n_cols: int, n_rows: int, output_csv_path: OutputPath(\"CSV\")):\n        import subprocess\n        import random\n\n        subprocess.run([\"pip\", \"install\", \"pandas\"])\n        import pandas as pd\n\n        # make data\n        data = [\n            [random.random() for _ in range(n_cols)] for __ in range(n_rows)\n        ]\n        columns = [f\"col_{i}\" for i in range(n_cols)]\n        index = [f\"idx_{i}\" for i in range(n_rows)]\n        df = pd.DataFrame(\n            data=data,\n            columns=columns,\n            index=index,\n        )\n        df.to_csv(output_csv_path, index=True)\n        print(f\"File path: {output_csv_path}\")\n\n    @func_to_container_op\n    def read_csv(input_csv_path: InputPath(\"CSV\")):\n        import subprocess\n\n        subprocess.run([\"pip\", \"install\", \"pandas\"])\n        import pandas as pd\n\n        df = pd.read_csv(input_csv_path, index_col=0)\n        print(f\"input_csv_path: {input_csv_path}\")\n        print(f\"type: {type(input_csv_path)}\")\n        print(df.head())\n\n    # pipeline\n    @dsl.pipeline(\n        name=\"Sample pipeline\", description=\"Make a csv file and read it.\"\n    )\n    def pipeline(n_cols: int = 5, n_rows: int = 3):\n        make_csv_task = make_csv(n_cols, n_rows)\n        read_csv(input_csv=make_csv_task.outputs[\"output_csv\"])\n\n    return pipeline\n"
  },
  {
    "repo": "Abeshith/KubeFlow-HandsOn",
    "file_path": "pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/Abeshith/KubeFlow-HandsOn/main/pipeline.py",
    "content": "from kfp import dsl\r\nfrom kfp import compiler\r\nfrom kfp.dsl import Input, Output, Dataset, Model, component\r\nfrom typing import Dict, List\r\n\r\n\r\n### Step 1: Load DataSet \r\n@dsl.component(base_image=\"python:3.9\")\r\ndef load_data(output_csv : Output[Dataset]):\r\n    import subprocess\r\n    subprocess.run([\"pip\", \"install\", \"pandas\",\"scikit-learn\"], check=True)\r\n\r\n    import pandas as pd\r\n    from sklearn.datasets import load_iris\r\n\r\n    ## Load the iris dataset\r\n    data = load_iris()\r\n    df = pd.DataFrame(data.data, columns=data.feature_names)\r\n    df['target'] = data.target\r\n\r\n    ## save the dataset to a CSV file\r\n    df.to_csv(output_csv.path, index=False)\r\n\r\n### Step 2: Data Preprocessing\r\n@dsl.component(base_image=\"python:3.9\")\r\ndef preprocess_data(input_csv: Input[Dataset], output_train: Output[Dataset],output_test: Output[Dataset],\r\n                    output_ytrain: Output[Dataset], output_ytest: Output[Dataset]):\r\n    import subprocess\r\n    subprocess.run([\"pip\", \"install\", \"pandas\",\"scikit-learn\"], check=True)\r\n\r\n    import pandas as pd\r\n    from sklearn.preprocessing import StandardScaler\r\n    from sklearn.model_selection import train_test_split\r\n\r\n    df = pd.read_csv(input_csv.path)\r\n\r\n    # Debug: Check for NaN values\r\n    print(\"Initial dataset shape:\", df.shape)\r\n    print(\"Missing values before preprocessing:\\n\", df.isnull().sum())\r\n\r\n    # Handle missing values\r\n    if df.isnull().values.any():\r\n        print(\"Missing values detected. Handling them...\")\r\n        df = df.dropna()  # Drop rows with any NaN values\r\n    \r\n    # Validate that there are no NaNs in the target column\r\n    assert not df['target'].isnull().any(), \"Target column contains NaN values after handling missing values.\"\r\n\r\n    features = df.drop(columns=['target'])\r\n    target = df['target']\r\n\r\n    # Standardize features\r\n    scaler = StandardScaler()\r\n    scaled_features = scaler.fit_transform(features)\r\n\r\n    # Train-test split\r\n    X_train, X_test, y_train, y_test = train_test_split(scaled_features, target, test_size=0.2, random_state=42)\r\n\r\n    # Debug: Validate splits\r\n    print(\"Shapes after train-test split:\")\r\n    print(\"X_train:\", X_train.shape, \"X_test:\", X_test.shape) \r\n    print(\"y_train:\", y_train.shape, \"y_test:\", y_test.shape)\r\n    print(\"Missing values in y_train:\", y_train.isnull().sum())\r\n\r\n    # Ensure no NaNs in the split data\r\n    assert not y_train.isnull().any(), \"y_train contains NaN values.\"\r\n    assert not y_test.isnull().any(), \"y_test contains NaN values.\"\r\n\r\n    # Create DataFrames for train and test sets\r\n    X_train_df = pd.DataFrame(X_train, columns=features.columns)\r\n    print(\"X_train_df:\", X_train_df) \r\n\r\n    y_train_df = pd.DataFrame(y_train) \r\n    print(\"y_train_df: \", y_train_df)  \r\n\r\n    X_test_df = pd.DataFrame(X_test, columns=features.columns)\r\n    print(\"X_test_df:\", X_test_df) \r\n\r\n    y_test_df = pd.DataFrame(y_test) \r\n    print(\"y_test_df: \", y_test_df) \r\n\r\n    # Save processed train and test data\r\n    X_train_df.to_csv(output_train.path, index=False)  \r\n    X_test_df.to_csv(output_test.path, index=False)\r\n\r\n    y_train_df.to_csv(output_ytrain.path, index=False)  \r\n    y_test_df.to_csv(output_ytest.path, index=False) \r\n\r\n### Step 3: Train Model \r\n@dsl.component(base_image=\"python:3.9\")\r\ndef train_model(train_data: Input[Dataset], ytrain_data: Input[Dataset], model_output: Output[Model]):\r\n\r\n    import subprocess\r\n    subprocess.run([\"pip\", \"install\", \"pandas\", \"scikit-learn\", \"joblib\"], check=True)\r\n\r\n    import pandas as pd\r\n    from sklearn.linear_model import LogisticRegression\r\n    from joblib import dump\r\n\r\n    # Load training data\r\n    train_df = pd.read_csv(train_data.path)\r\n    print(\"Shape of train_df:\", train_df.shape)\r\n    print(\"train_df:\", train_df)\r\n    X_train = train_df \r\n\r\n    y_train = pd.read_csv(ytrain_data.path)\r\n    print(\"Shape of ytrain_df:\", y_train.shape)\r\n    print(\"y_train_df:\", y_train)\r\n\r\n    # Debug: Validate splits\r\n    print(\"Shapes of X_train and y_train: \")\r\n    print(\"X_train:\", X_train.shape)\r\n    print(\"y_train:\", y_train.shape) \r\n    print(\"Missing values in X_train:\", X_train.isnull().sum())\r\n    print(\"Missing values in y_train:\", y_train.isnull().sum()) \r\n\r\n    # Ensure no NaN values\r\n    assert not X_train.isnull().values.any(), \"X_train contains NaN values.\"\r\n    assert not y_train.isnull().values.any(), \"y_train contains NaN values.\" \r\n\r\n    # Train model\r\n    model = LogisticRegression()\r\n    model.fit(X_train, y_train)\r\n\r\n    # Save model\r\n    dump(model, model_output.path)\r\n\r\n### Step 4: Evaluate Model\r\n@dsl.component(base_image=\"python:3.9\")\r\ndef evaluate_model(test_data: Input[Dataset], ytest_data: Input[Dataset], model: Input[Model], metrics_output: Output[Dataset]):\r\n    import subprocess\r\n    subprocess.run([\"pip\", \"install\", \"pandas\", \"scikit-learn\", \"matplotlib\", \"joblib\"], check=True)\r\n\r\n    import pandas as pd\r\n    from sklearn.metrics import classification_report, confusion_matrix\r\n    import matplotlib.pyplot as plt\r\n    from joblib import load\r\n\r\n    # Load test data\r\n    X_test = pd.read_csv(test_data.path)\r\n\r\n    y_test = pd.read_csv(ytest_data.path)  \r\n\r\n    # Load model\r\n    model = load(model.path)\r\n\r\n    # Predict\r\n    y_pred = model.predict(X_test)\r\n\r\n    # Generate metrics\r\n    report = classification_report(y_test, y_pred, output_dict=True)\r\n    cm = confusion_matrix(y_test, y_pred)\r\n\r\n    # Save metrics to a file\r\n    metrics_path = metrics_output.path\r\n    with open(metrics_path, 'w') as f:\r\n        f.write(str(report))\r\n\r\n    # Plot confusion matrix\r\n    plt.figure(figsize=(8, 6))\r\n    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\r\n    plt.title('Confusion Matrix')\r\n    plt.colorbar()\r\n    plt.xlabel('Predicted Label')\r\n    plt.ylabel('True Label')\r\n    plt.savefig(metrics_path.replace('.txt', '.png'))\r\n\r\n### Define the pipeline\r\n@dsl.pipeline(name=\"ml-pipeline\")\r\ndef ml_pipeline():\r\n    # Step 1: Load Dataset\r\n    load_op = load_data()\r\n\r\n    # Step 2: Preprocess Data\r\n    preprocess_op = preprocess_data(input_csv=load_op.outputs[\"output_csv\"])\r\n\r\n    # Step 3: Train Model\r\n    train_op = train_model(train_data=preprocess_op.outputs[\"output_train\"], ytrain_data=preprocess_op.outputs[\"output_ytrain\"])\r\n\r\n    # Step 4: Evaluate Model\r\n    evaluate_op = evaluate_model(test_data=preprocess_op.outputs[\"output_test\"], ytest_data=preprocess_op.outputs[\"output_ytest\"], model=train_op.outputs[\"model_output\"]) \r\n\r\n### Compile the pipeline\r\nif __name__ == \"__main__\":\r\n    compiler.Compiler().compile(pipeline_func=ml_pipeline, package_path=\"kubeflow_pipeline.yaml\")\r\n"
  },
  {
    "repo": "bbrowning/docling-kfp-demo",
    "file_path": "docling_convert_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/bbrowning/docling-kfp-demo/main/docling_convert_pipeline.py",
    "content": "# SPDX-License-Identifier: Apache-2.0\n\nfrom typing import List\n\nfrom kfp import dsl\n\nPYTHON_BASE_IMAGE = \"python:3.10\"\n\n@dsl.component(\n    base_image=PYTHON_BASE_IMAGE,\n    packages_to_install=[\"gitpython\"],\n)\ndef import_test_pdfs(\n    output_path: dsl.OutputPath(\"Directory\"),\n):\n    import os\n    import shutil\n    from git import Repo\n\n    docling_github_repo = \"https://github.com/DS4SD/docling/\"\n    full_repo_path = os.path.join(output_path, \"docling\")\n    Repo.clone_from(docling_github_repo, full_repo_path, branch=\"v2.25.0\")\n\n    # Copy some tests pdf up to the root of our output folder\n    pdfs_path = os.path.join(full_repo_path, \"tests\", \"data\", \"pdf\")\n    shutil.copytree(pdfs_path, output_path, dirs_exist_ok=True)\n\n    # Delete the rest of the docling repo, leaving only the PDFs\n    shutil.rmtree(full_repo_path)\n\n@dsl.component(\n    base_image=PYTHON_BASE_IMAGE,\n)\ndef create_pdf_splits(\n    input_path: dsl.InputPath(\"Directory\"),\n    num_splits: int,\n) -> List[List[str]]:\n    import pathlib\n\n    # Split our entire directory of pdfs into n batches, where n == num_splits\n    all_pdfs = [path.name for path in pathlib.Path(input_path).glob(\"*.pdf\")]\n    splits = [all_pdfs[i::num_splits] for i in range(num_splits)]\n    return splits\n\n# A Docling container built from\n# https://github.com/DS4SD/docling/blob/v2.25.0/Dockerfile\n@dsl.component(\n    base_image=\"quay.io/bbrowning/docling-kfp:v2.25.0\",\n)\ndef docling_convert(\n    input_path: dsl.InputPath(\"Directory\"),\n    pdf_split: List[str],\n    output_path: dsl.OutputPath(\"Directory\"),\n):\n    import pathlib\n    import os\n\n    from docling_core.types.doc import ImageRefMode\n    from docling.datamodel.base_models import ConversionStatus, InputFormat\n    from docling.datamodel.pipeline_options import PdfPipelineOptions\n    from docling.document_converter import DocumentConverter, PdfFormatOption\n\n    input_path = pathlib.Path(input_path)\n    output_path = pathlib.Path(output_path)\n    output_path.mkdir(parents=True, exist_ok=True)\n\n    input_pdfs = [input_path / name for name in pdf_split]\n\n    pipeline_options = PdfPipelineOptions()\n    pipeline_options.generate_page_images = True\n\n    doc_converter = DocumentConverter(\n        format_options={\n            InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n        }\n    )\n\n    conv_results = doc_converter.convert_all(\n        input_pdfs,\n        raises_on_error=True,\n    )\n\n    for conv_res in conv_results:\n        # TODO: handle errors, record success/failure somewhere - via\n        # calling some API, writing to some shared storage, or\n        # something else that each parallel task can do independently\n        doc_filename = conv_res.input.file.stem\n        output_json_path = pathlib.Path(output_path) / f\"{doc_filename}.json\"\n        conv_res.document.save_as_json(\n            output_json_path,\n            image_mode=ImageRefMode.PLACEHOLDER,\n        )\n\n@dsl.pipeline()\ndef convert_pipeline():\n    importer = import_test_pdfs()\n\n    pdf_splits = create_pdf_splits(\n        input_path=importer.output,\n        num_splits=3,\n    )\n\n    with dsl.ParallelFor(pdf_splits.output) as pdf_split:\n        docling_convert(\n            input_path=importer.output,\n            pdf_split=pdf_split,\n        )\n\nif __name__ == '__main__':\n    import kfp\n    output_yaml = \"docling_pipeline.yaml\"\n    kfp.compiler.Compiler().compile(convert_pipeline, output_yaml)\n    print(f\"\\nDocling pipeline compiled to {output_yaml}\")\n"
  },
  {
    "repo": "PranavAI2050/Human-Activity-Classification",
    "file_path": "kubeflow_human_activity_recognition.py",
    "raw_url": "https://raw.githubusercontent.com/PranavAI2050/Human-Activity-Classification/main/kubeflow_human_activity_recognition.py",
    "content": "# Import the modules you will use\nimport kfp\n\n# For creating the pipeline\nfrom kfp.v2 import dsl\n\n# For building components\nfrom kfp.v2.dsl import component\n\n# Type annotations for the component artifacts\nfrom kfp.v2.dsl import (\n    Input,\n    Output,\n    Artifact,\n    Dataset,\n    Model,\n    Metrics\n)\n\nfrom kfp import compiler\n\n#data ingestion and formatting\n\n@component(\n    packages_to_install=[\"pandas\", \"openpyxl\", \"scikit-learn\", \"numpy\"],\n    base_image=\"python:3.8\",\n    output_component_file=\"clean_data_component.yaml\"\n)\ndef clean_data(path: str, output_csv: Output[Dataset]):\n    \n    def convert_to_float(x):\n        try:\n            return np.float(x)\n        except:\n            return 0.0\n    \n    column_names = ['user-id', 'activity', 'timestamp', 'x-acc', 'y-acc', 'z-acc']\n    df = pd.read_csv(path, header=None, names=column_names)\n    df['z-acc'].replace(regex=True, inplace=True, to_replace=r';', value=r'')\n    df['z-acc'] = df['z-acc'].apply(convert_to_float)\n    \n    df.dropna(axis=0, how='any', inplace=True)\n    df.to_csv(output_csv.path, index=False)\n\n    \n#spliting the data into test and train\n\n@component(\n    packages_to_install=[\"pandas\", \"scikit-learn\"],\n    output_component_file=\"split_data_component.yaml\"\n)\ndef split_data(input_csv: Input[Dataset], train_csv: Output[Dataset], test_csv: Output[Dataset]):\n    import pandas as pd\n    from sklearn.model_selection import train_test_split\n    df = pd.read_csv(input_csv.path)\n    \n    total_rows = len(df)\n    split_index = int(0.7 * total_rows)\n    \n    train_df = df.iloc[:split_index]   \n    test_df = df.iloc[split_index:] \n    \n    train_df.to_csv(train_csv.path, index=False)\n    test_df.to_csv(test_csv.path, index=False)\n    \n#converting the format of data to time series for prediction of activity \n#based on the mode of activity in respective window\n\n@component(\n    packages_to_install=[\"pandas\", \"numpy\", \"scipy\", \"scikit-learn\"],\n    output_component_file=\"transform_data_component.yaml\"\n)\ndef preprocess_data(\n    input_train_csv: Input[Dataset], \n    input_test_csv: Input[Dataset], \n    output_train_x: Output[Artifact], \n    output_test_x: Output[Artifact],\n    output_train_y: Output[Artifact], \n    output_test_y: Output[Artifact]\n):\n    from scipy import stats\n    from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n    import numpy as np\n    import pandas as pd\n    import pickle\n\n    LABELS = ['Downstairs', 'Jogging', 'Sitting', 'Standing', 'Upstairs', 'Walking']\n    TIME_PERIODS = 80\n    STEP_DISTANCE = 40\n    LABEL = 'activity'\n    N_FEATURES = 3\n    acc_cols = ['x-acc', 'y-acc', 'z-acc']\n\n    train = pd.read_csv(input_train_csv.path)\n    test = pd.read_csv(input_test_csv.path)\n\n    for col in acc_cols:\n        scaler = MinMaxScaler()\n        train[col] = scaler.fit_transform(train[[col]])\n        test[col] = scaler.transform(test[[col]])\n\n    label_encoder = LabelEncoder()\n    train.loc[:, LABEL] = label_encoder.fit_transform(train[LABEL].values.ravel())\n    test.loc[:, LABEL] = label_encoder.transform(test[LABEL].values.ravel())\n\n    def create_segments_and_labels(df, time_period, step_distance, label_name):\n        segments = []\n        labels = []\n        for i in range(0, len(df) - time_period, step_distance):\n            xs = df['x-acc'].values[i: i + time_period]\n            ys = df['y-acc'].values[i: i + time_period]\n            zs = df['z-acc'].values[i: i + time_period]\n\n            label = stats.mode(df[label_name][i: i + time_period])[0][0]\n            segments.append([xs, ys, zs])\n            labels.append(label)\n        reshaped_segments = np.asarray(segments, dtype=np.float32).reshape(-1, time_period, N_FEATURES)\n        labels = np.asarray(labels)\n        return reshaped_segments, labels\n\n    X_train, Y_train = create_segments_and_labels(train, TIME_PERIODS, STEP_DISTANCE, LABEL)\n    X_test, Y_test = create_segments_and_labels(test, TIME_PERIODS, STEP_DISTANCE, LABEL)\n\n    with open(output_train_x.path, 'wb') as f:\n        pickle.dump(X_train, f)\n    with open(output_test_x.path, 'wb') as f:\n        pickle.dump(X_test, f)\n\n    with open(output_train_y.path, 'wb') as f:\n        pickle.dump(Y_train, f)\n    with open(output_test_y.path, 'wb') as f:\n        pickle.dump(Y_test, f)\n\n#tranning a conv-1d model on the train_data \n\n@component(\n    packages_to_install=[\"tensorflow\", \"pandas\"],\n    output_component_file=\"train_model_component.yaml\"\n)\ndef train_model(\n    input_train_x: Input[Artifact], \n    input_train_y: Input[Artifact], \n    output_model: Output[Model], \n    output_history: Output[Artifact]\n):\n    import tensorflow as tf\n    from tensorflow.keras import models, layers\n    from tensorflow.keras.utils import to_categorical\n    import pickle\n\n    with open(input_train_x.path, \"rb\") as file:\n        train_X = pickle.load(file)\n\n    with open(input_train_y.path, \"rb\") as file:\n        train_Y = pickle.load(file)\n\n    Y_one_hot = to_categorical(train_Y)\n\n    def model_builder(train_X):\n        model = models.Sequential()\n        model.add(layers.Conv1D(160, 12, input_shape=(train_X.shape[1], train_X.shape[2]), activation='relu'))\n        model.add(layers.Conv1D(128, 10, activation='relu'))\n        model.add(layers.Conv1D(96, 8, activation='relu'))\n        model.add(layers.Conv1D(64, 6, activation='relu'))\n        model.add(layers.GlobalMaxPooling1D())\n        model.add(layers.Dropout(0.5))\n        model.add(layers.Dense(6, activation='softmax'))\n        print(model.summary())\n        return model\n\n    model = model_builder(train_X)\n    model.compile(optimizer='rmsprop',\n                  loss='categorical_crossentropy',\n                  metrics=['accuracy'])\n\n    history = model.fit(train_X, Y_one_hot, epochs=25, batch_size=1024)\n    model.save(output_model.path)\n\n    with open(output_history.path, \"wb\") as file:\n        pickle.dump(history.history, file)\n\n#evaluation the model on test data and return metrics as logs of pipeline output\n\n@component(\n    packages_to_install=[\"tensorflow\", \"pandas\"],\n    output_component_file=\"eval_model_component.yaml\"\n)\ndef eval_model(\n    input_model: Input[Model], \n    input_history: Input[Artifact], \n    input_test_x: Input[Artifact], \n    input_test_y: Input[Artifact], \n    MLPipeline_Metrics: Output[Metrics]\n):\n    import tensorflow as tf\n    from tensorflow.keras.utils import to_categorical\n    import pickle\n\n    model = tf.keras.models.load_model(input_model.path)\n\n    with open(input_test_x.path, \"rb\") as file:\n        test_X = pickle.load(file)\n\n    with open(input_test_y.path, \"rb\") as file:\n        test_Y = pickle.load(file)\n\n    Y_one_hot = to_categorical(test_Y)\n\n    loss_value, accuracy = model.evaluate(test_X, Y_one_hot)\n    output_string = f\"Loss: {loss_value:.4f}, Accuracy: {accuracy:.4f}\"\n\n    MLPipeline_Metrics.log_metric(\"categorical_crossentropy_loss\", loss_value)\n    MLPipeline_Metrics.log_metric(\"accuracy\", accuracy)\n\n#completely defining the pipeline dag /structure\n\n@dsl.pipeline(\n    name=\"Human Activity Recognition Pipeline\",\n)\ndef kfp_pipeline(data_path: str):\n    \n    clean_data_task = clean_data(path=data_path)\n    \n    split_data_task = split_data(input_csv=clean_data_task.outputs['output_csv'])\n    \n    preprocess_data_task = preprocess_data(\n        input_train_csv=split_data_task.outputs['train_csv'],\n        input_test_csv=split_data_task.outputs['test_csv']\n    )\n    \n    train_model_task = train_model(\n        input_train_x=preprocess_data_task.outputs[\"output_train_x\"],\n        input_train_y=preprocess_data_task.outputs[\"output_train_y\"]\n    )\n    \n    eval_model_task = eval_model(\n        input_model=train_model_task.outputs[\"output_model\"],\n        input_history=train_model_task.outputs[\"output_history\"],\n        input_test_x=preprocess_data_task.outputs[\"output_test_x\"],\n        input_test_y=preprocess_data_task.outputs[\"output_test_y\"]\n    )\n\n\n#compiling the pipeline into yaml to run on kubeflow\nif __name__ == \"__main__\":\n    compiler.Compiler().compile(kfp_pipeline, 'pipeline.yaml')\n"
  },
  {
    "repo": "mozilla-ai/kfp-discovery",
    "file_path": "doc-to-podcast/generate_pipeline_manual.py",
    "raw_url": "https://raw.githubusercontent.com/mozilla-ai/kfp-discovery/main/doc-to-podcast/generate_pipeline_manual.py",
    "content": "from kfp import dsl, compiler, components\n\n\ndownload_document = components.load_component_from_file(\"./_components/downloader.yaml\")\ntransform_document = components.load_component_from_file(\"./_components/transformer.yaml\")\nscriptwriter = components.load_component_from_file(\"./_components/scriptwriter.yaml\")\nperformer = components.load_component_from_file(\"./_components/performer.yaml\")\n\n\n@dsl.pipeline\ndef document_to_podcast(\n    document_url: str,\n    file_type: str = None,\n    audio_format: str = None,\n    host_name: str = None,\n    cohost_name: str = None,\n    host_voice_profile: str = None,\n    cohost_voice_profile: str = None,\n    text_to_text_model: str = None,\n    text_to_speech_model: str = None,\n):\n    \"\"\"Convert a document to a podcast.\n\n    This pipeline downloads a document, processes it, converts it to a script,\n    and finally converts the script to speech (podcast).\n\n    Args:\n        :param document_url: Path to the input document.\n        :param file_type: The file type of the input document. e.g. .html, .txt, .pdf.\n        :param audio_format: Output podcast file type .e.g. WAV, MP3.\n        :param host_name: Name of the host.\n        :param cohost_name: Name of the co-host.\n        :param host_voice_profile: Voice profile for the host.\n        :param cohost_voice_profile: Voice profile for the co-host.\n        :param text_to_text_model: The text-to-speech model to use for script writing.\n        :param text_to_speech_model: The text-to-speech model to use for performing the podcast.\n    \"\"\"\n    download_document_step = download_document(document_url=document_url)\n    download_document_step.set_caching_options(False)\n\n    process_data_step = transform_document(\n        file_path=download_document_step.outputs['downloaded_file_path'],\n        file_type=file_type,\n    ).after(download_document_step)\n    process_data_step.set_caching_options(False)\n\n    scriptwriter_step = scriptwriter(\n        processed_document=process_data_step.outputs['processed_document'],\n        host_name=host_name,\n        cohost_name=cohost_name,\n        model=text_to_text_model,\n    ).after(process_data_step)\n    scriptwriter_step.set_accelerator_type(\"nvidia.com/gpu\")\n    scriptwriter_step.set_accelerator_limit(1)\n    scriptwriter_step.set_caching_options(False)\n\n    performer_step = performer(\n        podcast_script=scriptwriter_step.outputs['podcast_script'],\n        host_voice_profile=host_voice_profile,\n        cohost_voice_profile=cohost_voice_profile,\n        model=text_to_speech_model,\n        audio_format=audio_format,\n    ).after(scriptwriter_step)\n    performer_step.set_accelerator_type(\"nvidia.com/gpu\")\n    performer_step.set_accelerator_limit(1)\n    performer_step.set_caching_options(False)\n\n\nif __name__ == \"__main__\":\n    compiler.Compiler().compile(document_to_podcast, package_path='document_to_podcast.yaml')"
  },
  {
    "repo": "iampatgrady/terraform-vertexai-helloworld",
    "file_path": "pipeline/components.py",
    "raw_url": "https://raw.githubusercontent.com/iampatgrady/terraform-vertexai-helloworld/main/pipeline/components.py",
    "content": "# ./pipeline/components.py\nfrom kfp.dsl import component\n\n@component(\n    base_image=\"python:3.12\", # Specify a base image for reproducibility\n)\ndef produce_message_component(\n    input_text: str,\n) -> str:\n    \"\"\"\n    A simple KFP component that takes an input string, appends a message,\n    logs it, and returns the new string.\n    \"\"\"\n    # For real MLOps, you might load data, preprocess, train, or predict here.\n    # For this \"Hello World\", we just manipulate a string.\n    processed_message = f\"{input_text} - from KFP component\"\n    \n    return processed_message"
  },
  {
    "repo": "iampatgrady/terraform-vertexai-helloworld",
    "file_path": "pipeline/hello_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/iampatgrady/terraform-vertexai-helloworld/main/pipeline/hello_pipeline.py",
    "content": "# ./pipeline/hello_pipeline.py\nfrom kfp import dsl\nfrom .components import produce_message_component\n\n@dsl.pipeline(\n    name=\"minimal-hello-world-pipeline\",\n    description=\"A minimal Vertex AI pipeline that produces a Hello World message, orchestrated by Terraform.\"\n)\ndef minimal_hello_pipeline(\n    # Terraform will pass a value for this.\n    message_to_produce: str \n):\n    \"\"\"\n    Defines the Hello World KFP pipeline structure.\n    It consists of a single component that processes an input message.\n    \"\"\"\n    # Call the component, passing the pipeline parameter to its input.\n    producer_task = produce_message_component(\n        input_text=message_to_produce\n    )\n    # In more complex pipelines, you would chain multiple components here."
  },
  {
    "repo": "Yeshwanththota/MLOps_project5_mlflow_Dagshub_minikube_kubectl_Kubeflow_pipelines_Dockerhub",
    "file_path": "kubeflow_pipeline/mlops_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/Yeshwanththota/MLOps_project5_mlflow_Dagshub_minikube_kubectl_Kubeflow_pipelines_Dockerhub/main/kubeflow_pipeline/mlops_pipeline.py",
    "content": "import kfp\nfrom kfp import dsl\n\ndef data_processing_op():\n    return dsl.ContainerOp(\n        name='Data Processing',\n        image='yeshwanththota33/mlopsproject:latest',\n        command=['python', 'src/data_processing.py'],)\ndef model_training_op():\n    return dsl.ContainerOp(\n        name='Model Training',\n        image='yeshwanththota33/mlopsproject:latest',\n        command=['python', 'src/model_training.py'],)\n\n@dsl.pipeline(\n    name='MLOps Pipeline',\n    description='A MLOps pipeline for data processing and model training.'\n)\n\ndef mlops_pipeline():\n    data_processing = data_processing_op()\n    model_training = model_training_op().after(data_processing)\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(mlops_pipeline, 'mlops_pipeline.yaml')"
  },
  {
    "repo": "yuanchi2807/kfp-codeflare",
    "file_path": "kubeflowpipeline/kfp-ray.py",
    "raw_url": "https://raw.githubusercontent.com/yuanchi2807/kfp-codeflare/main/kubeflowpipeline/kfp-ray.py",
    "content": "# Copyright 2020 kubeflow.org\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom kfp import dsl\nfrom kfp import components\n\ndef deploy_ray_cluster():\n    print(\"lauching a Ray cluster when this KFP operator is deployed\")\n    from test import launch_ray\n    launch_ray()\n\n\ndeploy_ray_cluster_op = components.create_component_from_func(\n    deploy_ray_cluster,\n    base_image='us.icr.io/cil15-shared-registry/preprocessing-pipelines/kfp/kfp-codeflare:0')\n\n@dsl.pipeline(\n    name='test_KFP_deploy_ray_cluster',\n    description='Testing how to use KFP pipeline to deploy a Ray cluster.'\n)\ndef kfp_ray_deployment_pipeline():\n    deploy_ray_cluster_op()\n\n\nif __name__ == '__main__':\n    from kfp_tekton.compiler import TektonCompiler\n    TektonCompiler().compile(kfp_ray_deployment_pipeline, __file__.replace('.py', '.yaml'))\n"
  },
  {
    "repo": "AlexIoannides/kfp-component-lib",
    "file_path": "tests/test_pipelines.py",
    "raw_url": "https://raw.githubusercontent.com/AlexIoannides/kfp-component-lib/main/tests/test_pipelines.py",
    "content": "\"\"\"Tests that components can be assembled into pipeline DAGs that compile.\"\"\"\nfrom pathlib import Path\n\nfrom kfp import compiler, dsl\n\nfrom kfp_component_lib.components import make_numeric_dataset\n\n\n@dsl.pipeline\ndef synthetic_data_pipeline(n_rows: int = 1000) -> None:\n    \"\"\"Create synthetic datasets.\"\"\"\n    task_1 = make_numeric_dataset(n_rows=n_rows)\n    task_2 = make_numeric_dataset(n_rows=n_rows)\n    task_2.after(task_1)\n\n\ndef test_synthetic_data_pipeline_compiles():\n    compiled_pipeline_file = \"pipeline.json\"\n    try:\n        compiler.Compiler().compile(\n            pipeline_func=synthetic_data_pipeline, package_path=compiled_pipeline_file\n        )\n        assert True\n    except Exception:\n        assert False\n    finally:\n        compiled_pipeline_path = Path(compiled_pipeline_file)\n        if compiled_pipeline_path.exists():\n            compiled_pipeline_path.unlink()\n"
  },
  {
    "repo": "MOLOZISE/MLOps_Formation",
    "file_path": "kubeflow_flowers_train_to_mlflow_pipeline_yaml.py",
    "raw_url": "https://raw.githubusercontent.com/MOLOZISE/MLOps_Formation/main/kubeflow_flowers_train_to_mlflow_pipeline_yaml.py",
    "content": "\n# 1. kubeflow pipeline \ud615\uc2dd\uc744 \ub530\ub984\n# 2. mlflow\uc5d0\uc11c \ubaa8\ub378\uc774 \uc800\uc7a5\ub418\ub3c4\ub85d \ud568\n# 3. how : inference\uc5d0\uc11c \ubaa8\ub378 \ub85c\ub529 -> \ub85c\ub529 \ubd80\ubd84\ub9cc \ucd94\ucd9c\ud558\uc5ec \ub85c\ub529 \ud6c4 \ubc14\ub85c \uc800\uc7a5\ud558\ub294 \ubc29\uc2dd\n# 4. \uc0ac\uc804\uc791\uc5c5 : minio mlflow bucket \ub0b4 \ubaa8\ub378 \uc5c5\ub85c\ub529 -> \ubaa8\ub378 \ub85c\ub529 ->\n# mlflow \ud615\uc2dd\uc73c\ub85c \ubaa8\ub378 \uc800\uc7a5(\ud150\uc11c\ud50c\ub85c \ucf00\ub77c\uc2a4, \ud30c\uc774\ud1a0\uce58 \ud615\uc2dd\uc744 \ud655\uc778) -> Seldon Core\ub85c \ubc30\ud3ec\n\n\n# SVC \ubaa8\ub378 Train\n# import pandas as pd\n# from sklearn.datasets import load_iris\n# from sklearn.svm import SVC\n#\n# iris = load_iris()\n#\n# data = pd.DataFrame(iris[\"data\"], columns=iris[\"feature_names\"])\n# target = pd.DataFrame(iris[\"target\"], columns=[\"target\"])\n#\n# clf = SVC(kernel=\"rbf\")\n# clf.fit(data, target)\n\n\n# MLFlow Infos\n\n# from mlflow.models.signature import infer_signature\n# from mlflow.utils.environment import _mlflow_conda_env\n#\n# input_example = data.sample(1)\n# signature = infer_signature(data, clf.predict(data))\n# conda_env = _mlflow_conda_env(additional_pip_deps=[\"dill\", \"pandas\", \"scikit-learn\"])\n\n# Save MLFlow Infos\n\n# from mlflow.sklearn import save_model\n#\n# save_model(\n#     sk_model=clf,\n#     path=\"svc\",\n#     serialization_format=\"cloudpickle\",\n#     conda_env=conda_env,\n#     signature=signature,\n#     input_example=input_example,\n# )\n\n# MLFlow on Server\n\n# import mlflow\n#\n# with mlflow.start_run():\n#     mlflow.log_artifact(\"svc/\")\nfrom functools import partial\n\nimport kfp\nfrom kfp.components import InputPath, OutputPath, create_component_from_func\nfrom kfp.dsl import pipeline\n\n\n@partial(\n    create_component_from_func,\n    packages_to_install=[\"tflite-model-maker\", \"numpy\"],\n)\ndef load_flower_data( # gan data path\n        data_path: OutputPath(\"csv\"),\n        target_path: OutputPath(\"csv\"),\n):\n\n    import pandas as pd\n    from keras.datasets.fashion_mnist import load_data\n\n    # minio \uc811\uadfc?\n    import os\n    os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = \"http://minio-service.kubeflow.svc:9000\"\n    os.environ[\"AWS_ACCESS_KEY_ID\"] = \"minio\"\n    os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"minio123\"\n\n    # minio\ub0b4 mlflow \ud3f4\ub354\n    # flowers = pd.read_csv(\"flowers.csv\")\n    # iris = load_iris()\n    #\n    # data = pd.DataFrame(flowers[\"data\"], columns=iris[\"feature_names\"])\n    # target = pd.DataFrame(flowers[\"target\"], columns=[\"target\"])\n    #\n    # data.to_csv(data_path, index=False)\n    # target.to_csv(target_path, index=False)\n\n    data_path = tf.keras.utils.get_file(\n        'flower_photos',\n        'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n        untar=True)\n\n    data = DataLoader.from_folder(data_path)\n    train_data, test_data = data.split(0.9)\n\n\n@partial(\n    create_component_from_func,\n    packages_to_install=[\"dill\", \"pandas\", \"scikit-learn\", \"mlflow\"],\n)\ndef train_from_csv(\n        train_data_path: InputPath(\"csv\"),\n        train_target_path: InputPath(\"csv\"),\n        model_path: OutputPath(\"dill\"),\n        input_example_path: OutputPath(\"dill\"),\n        signature_path: OutputPath(\"dill\"),\n        conda_env_path: OutputPath(\"dill\"),\n        kernel: str,\n):\n    import dill\n    import pandas as pd\n    from sklearn.svm import SVC\n\n    from mlflow.models.signature import infer_signature\n    from mlflow.utils.environment import _mlflow_conda_env\n\n    train_data = pd.read_csv(train_data_path)\n    train_target = pd.read_csv(train_target_path)\n\n    clf = SVC(kernel=kernel)\n    clf.fit(train_data, train_target)\n\n    with open(model_path, mode=\"wb\") as file_writer:\n        dill.dump(clf, file_writer)\n\n    input_example = train_data.sample(1)\n    with open(input_example_path, \"wb\") as file_writer:\n        dill.dump(input_example, file_writer)\n\n    signature = infer_signature(train_data, clf.predict(train_data))\n    with open(signature_path, \"wb\") as file_writer:\n        dill.dump(signature, file_writer)\n\n    conda_env = _mlflow_conda_env(\n        additional_pip_deps=[\"dill\", \"pandas\", \"scikit-learn\"]\n    )\n    with open(conda_env_path, \"wb\") as file_writer:\n        dill.dump(conda_env, file_writer)\n\n\n@partial(\n    create_component_from_func,\n    packages_to_install=[\"dill\", \"pandas\", \"scikit-learn\", \"mlflow\", \"boto3\", \"tqdm\", ],\n)\ndef upload_sklearn_model_to_mlflow(\n        model_name: str,\n        model_path: InputPath(\"dill\"),\n        input_example_path: InputPath(\"dill\"),\n        signature_path: InputPath(\"dill\"),\n        conda_env_path: InputPath(\"dill\"),\n):\n    import os\n    import dill\n    from mlflow.sklearn import save_model\n    from mlflow.tensorflow import load_model\n\n    from mlflow.tracking.client import MlflowClient\n\n    os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = \"http://minio-service.kubeflow.svc:9000\"\n    os.environ[\"AWS_ACCESS_KEY_ID\"] = \"minio\"\n    os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"minio123\"\n\n    client = MlflowClient(\"http://mlflow-server-service.mlflow-system.svc:5000\")\n\n    with open(model_path, mode=\"rb\") as file_reader:\n        clf = dill.load(file_reader)\n\n    with open(input_example_path, \"rb\") as file_reader:\n        input_example = dill.load(file_reader)\n\n    with open(signature_path, \"rb\") as file_reader:\n        signature = dill.load(file_reader)\n\n    with open(conda_env_path, \"rb\") as file_reader:\n        conda_env = dill.load(file_reader)\n\n    # \uc5f0\uacb0\ub41c \uc0c1\ud0dc\n\n\n    save_model(\n        sk_model=clf,\n        path=model_name,\n        serialization_format=\"cloudpickle\",\n        conda_env=conda_env,\n        signature=signature,\n        input_example=input_example,\n    )\n    run = client.create_run(experiment_id=\"0\")\n    client.log_artifact(run.info.run_id, model_name)\n\n\n@pipeline(name=\"mlflow_pipeline\")\ndef mlflow_pipeline(kernel: str, model_name: str):\n    iris_data = load_iris_data()\n    model = train_from_csv(\n        train_data=iris_data.outputs[\"data\"],\n        train_target=iris_data.outputs[\"target\"],\n        kernel=kernel,\n    )\n    _ = upload_sklearn_model_to_mlflow(\n        model_name=model_name,\n        model=model.outputs[\"model\"],\n        input_example=model.outputs[\"input_example\"],\n        signature=model.outputs[\"signature\"],\n        conda_env=model.outputs[\"conda_env\"],\n    )\n\n\nif __name__ == \"__main__\":\n    kfp.compiler.Compiler().compile(mlflow_pipeline, \"mlflow_pipeline.yaml\")"
  },
  {
    "repo": "MOLOZISE/MLOps_Formation",
    "file_path": "kubeflow_mlflow_pre_trained_model_pipeline_yaml.py",
    "raw_url": "https://raw.githubusercontent.com/MOLOZISE/MLOps_Formation/main/kubeflow_mlflow_pre_trained_model_pipeline_yaml.py",
    "content": "\n# 1. kubeflow pipeline \ud615\uc2dd\uc744 \ub530\ub984\n# 2. mlflow\uc5d0\uc11c \ubaa8\ub378\uc774 \uc800\uc7a5\ub418\ub3c4\ub85d \ud568\n# 3. how : inference\uc5d0\uc11c \ubaa8\ub378 \ub85c\ub529 -> \ub85c\ub529 \ubd80\ubd84\ub9cc \ucd94\ucd9c\ud558\uc5ec \ub85c\ub529 \ud6c4 \ubc14\ub85c \uc800\uc7a5\ud558\ub294 \ubc29\uc2dd\n# 4. \uc0ac\uc804\uc791\uc5c5 : minio mlflow bucket \ub0b4 \ubaa8\ub378 \uc5c5\ub85c\ub529 -> \ubaa8\ub378 \ub85c\ub529 ->\n# mlflow \ud615\uc2dd\uc73c\ub85c \ubaa8\ub378 \uc800\uc7a5(\ud150\uc11c\ud50c\ub85c \ucf00\ub77c\uc2a4, \ud30c\uc774\ud1a0\uce58 \ud615\uc2dd\uc744 \ud655\uc778) -> Seldon Core\ub85c \ubc30\ud3ec\n\n\n# SVC \ubaa8\ub378 Train\n# import pandas as pd\n# from sklearn.datasets import load_iris\n# from sklearn.svm import SVC\n#\n# iris = load_iris()\n#\n# data = pd.DataFrame(iris[\"data\"], columns=iris[\"feature_names\"])\n# target = pd.DataFrame(iris[\"target\"], columns=[\"target\"])\n#\n# clf = SVC(kernel=\"rbf\")\n# clf.fit(data, target)\n\n\n# MLFlow Infos\n\n# from mlflow.models.signature import infer_signature\n# from mlflow.utils.environment import _mlflow_conda_env\n#\n# input_example = data.sample(1)\n# signature = infer_signature(data, clf.predict(data))\n# conda_env = _mlflow_conda_env(additional_pip_deps=[\"dill\", \"pandas\", \"scikit-learn\"])\n\n# Save MLFlow Infos\n\n# from mlflow.sklearn import save_model\n#\n# save_model(\n#     sk_model=clf,\n#     path=\"svc\",\n#     serialization_format=\"cloudpickle\",\n#     conda_env=conda_env,\n#     signature=signature,\n#     input_example=input_example,\n# )\n\n# MLFlow on Server\n\n# import mlflow\n#\n# with mlflow.start_run():\n#     mlflow.log_artifact(\"svc/\")\nfrom functools import partial\n\nimport kfp\nfrom kfp.components import InputPath, OutputPath, create_component_from_func\nfrom kfp.dsl import pipeline\n\n\n@partial(\n    create_component_from_func,\n    packages_to_install=[\"pandas\", \"scikit-learn\"],\n)\ndef load_gan_data( # gan data path\n        data_path: OutputPath(\"csv\"),\n        target_path: OutputPath(\"csv\"),\n):\n\n    import pandas as pd\n    from sklearn.datasets import load_iris\n\n    # minio \uc811\uadfc?\n    import os\n    os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = \"http://minio-service.kubeflow.svc:9000\"\n    os.environ[\"AWS_ACCESS_KEY_ID\"] = \"minio\"\n    os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"minio123\"\n\n    # minio\ub0b4 mlflow \ud3f4\ub354\n\n    iris = load_iris()\n\n    data = pd.DataFrame(iris[\"data\"], columns=iris[\"feature_names\"])\n    target = pd.DataFrame(iris[\"target\"], columns=[\"target\"])\n\n    data.to_csv(data_path, index=False)\n    target.to_csv(target_path, index=False)\n\n\n@partial(\n    create_component_from_func,\n    packages_to_install=[\"dill\", \"pandas\", \"scikit-learn\", \"mlflow\"],\n)\ndef train_from_csv(\n        train_data_path: InputPath(\"csv\"),\n        train_target_path: InputPath(\"csv\"),\n        model_path: OutputPath(\"dill\"),\n        input_example_path: OutputPath(\"dill\"),\n        signature_path: OutputPath(\"dill\"),\n        conda_env_path: OutputPath(\"dill\"),\n        kernel: str,\n):\n    import dill\n    import pandas as pd\n    from sklearn.svm import SVC\n\n    from mlflow.models.signature import infer_signature\n    from mlflow.utils.environment import _mlflow_conda_env\n\n    train_data = pd.read_csv(train_data_path)\n    train_target = pd.read_csv(train_target_path)\n\n    clf = SVC(kernel=kernel)\n    clf.fit(train_data, train_target)\n\n    with open(model_path, mode=\"wb\") as file_writer:\n        dill.dump(clf, file_writer)\n\n    input_example = train_data.sample(1)\n    with open(input_example_path, \"wb\") as file_writer:\n        dill.dump(input_example, file_writer)\n\n    signature = infer_signature(train_data, clf.predict(train_data))\n    with open(signature_path, \"wb\") as file_writer:\n        dill.dump(signature, file_writer)\n\n    conda_env = _mlflow_conda_env(\n        additional_pip_deps=[\"dill\", \"pandas\", \"scikit-learn\"]\n    )\n    with open(conda_env_path, \"wb\") as file_writer:\n        dill.dump(conda_env, file_writer)\n\n\n@partial(\n    create_component_from_func,\n    packages_to_install=[\"dill\", \"pandas\", \"scikit-learn\", \"mlflow\", \"boto3\", \"tqdm\", ],\n)\ndef upload_sklearn_model_to_mlflow(\n        model_name: str,\n        model_path: InputPath(\"dill\"),\n        input_example_path: InputPath(\"dill\"),\n        signature_path: InputPath(\"dill\"),\n        conda_env_path: InputPath(\"dill\"),\n):\n    import os\n    import dill\n    from mlflow.sklearn import save_model\n    from mlflow.tensorflow import load_model\n\n    from mlflow.tracking.client import MlflowClient\n\n    os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = \"http://minio-service.kubeflow.svc:9000\"\n    os.environ[\"AWS_ACCESS_KEY_ID\"] = \"minio\"\n    os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"minio123\"\n\n    client = MlflowClient(\"http://mlflow-server-service.mlflow-system.svc:5000\")\n\n    with open(model_path, mode=\"rb\") as file_reader:\n        clf = dill.load(file_reader)\n\n    with open(input_example_path, \"rb\") as file_reader:\n        input_example = dill.load(file_reader)\n\n    with open(signature_path, \"rb\") as file_reader:\n        signature = dill.load(file_reader)\n\n    with open(conda_env_path, \"rb\") as file_reader:\n        conda_env = dill.load(file_reader)\n\n    # \uc5f0\uacb0\ub41c \uc0c1\ud0dc\n\n\n    save_model(\n        sk_model=clf,\n        path=model_name,\n        serialization_format=\"cloudpickle\",\n        conda_env=conda_env,\n        signature=signature,\n        input_example=input_example,\n    )\n    run = client.create_run(experiment_id=\"0\")\n    client.log_artifact(run.info.run_id, model_name)\n\n\n@pipeline(name=\"mlflow_pipeline\")\ndef mlflow_pipeline(kernel: str, model_name: str):\n    iris_data = load_iris_data()\n    model = train_from_csv(\n        train_data=iris_data.outputs[\"data\"],\n        train_target=iris_data.outputs[\"target\"],\n        kernel=kernel,\n    )\n    _ = upload_sklearn_model_to_mlflow(\n        model_name=model_name,\n        model=model.outputs[\"model\"],\n        input_example=model.outputs[\"input_example\"],\n        signature=model.outputs[\"signature\"],\n        conda_env=model.outputs[\"conda_env\"],\n    )\n\n\nif __name__ == \"__main__\":\n    kfp.compiler.Compiler().compile(mlflow_pipeline, \"mlflow_pipeline.yaml\")"
  },
  {
    "repo": "MOLOZISE/MLOps_Formation",
    "file_path": "01_3_5_8_example/example_kubeflow_pipeline_yaml.py",
    "raw_url": "https://raw.githubusercontent.com/MOLOZISE/MLOps_Formation/main/01_3_5_8_example/example_kubeflow_pipeline_yaml.py",
    "content": "import kfp\nfrom kfp.components import create_component_from_func\nfrom kfp.dsl import pipeline\n\n@create_component_from_func\ndef print_and_return_number(number: int) -> int:\n    print(number)\n    return number\n\n@create_component_from_func\ndef sum_and_print_numbers(number_1: int, number_2: int):\n    print(number_1 + number_2)\n\n@pipeline(name=\"example_pipeline\")\ndef example_pipeline(number_1: int, number_2: int):\n    number_1_result = print_and_return_number(number_1)\n    number_2_result = print_and_return_number(number_2)\n    sum_result = sum_and_print_numbers(\n        number_1=number_1_result.output, number_2=number_2_result.output\n    )\n\nif __name__ == \"__main__\":\n    kfp.compiler.Compiler().compile(example_pipeline, \"example_pipeline.yaml\")"
  },
  {
    "repo": "MOLOZISE/MLOps_Formation",
    "file_path": "02_Iris_example/complex_kubeflow_pipeline_yaml.py",
    "raw_url": "https://raw.githubusercontent.com/MOLOZISE/MLOps_Formation/main/02_Iris_example/complex_kubeflow_pipeline_yaml.py",
    "content": "from kfp.components import InputPath, OutputPath, create_component_from_func\nfrom functools import partial\n\n@partial(\n    create_component_from_func,\n    packages_to_install=[\"dill==0.3.4\", \"pandas==1.3.4\", \"scikit-learn==1.0.1\"],\n)\ndef train_from_csv(\n    train_data_path: InputPath(\"csv\"),\n    train_target_path: InputPath(\"csv\"),\n    model_path: OutputPath(\"dill\"),\n    kernel: str,\n):\n    import dill\n    import pandas as pd\n\n    from sklearn.svm import SVC\n\n    train_data = pd.read_csv(train_data_path)\n    train_target = pd.read_csv(train_target_path)\n\n    clf = SVC(kernel=kernel)\n    clf.fit(train_data, train_target)\n\n    with open(model_path, mode=\"wb\") as file_writer:\n        dill.dump(clf, file_writer)\n\nfrom functools import partial\nfrom kfp.components import InputPath, OutputPath, create_component_from_func\n\n\n@partial(\n    create_component_from_func,\n    packages_to_install=[\"dill==0.3.4\", \"pandas==1.3.4\", \"scikit-learn==1.0.1\"],\n)\ndef load_iris_data(\n    data_path: OutputPath(\"csv\"),\n    target_path: OutputPath(\"csv\"),\n):\n    import pandas as pd\n    from sklearn.datasets import load_iris\n\n    iris = load_iris()\n\n    data = pd.DataFrame(iris[\"data\"], columns=iris[\"feature_names\"])\n    target = pd.DataFrame(iris[\"target\"], columns=[\"target\"])\n\n    data.to_csv(data_path, index=False)\n    target.to_csv(target_path, index=False)\n\n\n### \ud30c\uc774\ud504\ub77c\uc778\n\nfrom kfp.dsl import pipeline\n\n@pipeline(name=\"complex_pipeline\")\ndef complex_pipeline(kernel: str):\n    iris_data = load_iris_data()\n    model = train_from_csv(\n        train_data=iris_data.outputs[\"data\"],\n        train_target=iris_data.outputs[\"target\"],\n        kernel=kernel,\n    )\nimport kfp\n\nif __name__ == \"__main__\":\n    kfp.compiler.Compiler().compile(complex_pipeline, \"complex_pipeline.yaml\")\n\n# from functools import partial\n# from kfp.components import InputPath, OutputPath, create_component_from_func\n#\n# @partial(\n#     create_component_from_func,\n#     packages_to_install=[\"dill==0.3.4\", \"pandas==1.3.4\", \"scikit-learn==1.0.1\"],\n# )\n# def train_from_csv(\n#     train_data_path: InputPath(\"csv\"),\n#     train_target_path: InputPath(\"csv\"),\n#     model_path: OutputPath(\"dill\"),\n#     kernel: str,\n# ):\n#     import dill\n#     import pandas as pd\n#\n#     from sklearn.svm import SVC\n#\n#     train_data = pd.read_csv(train_data_path)\n#     train_target = pd.read_csv(train_target_path)\n#\n#     clf = SVC(kernel=kernel)\n#     clf.fit(train_data, train_target)\n#\n#     with open(model_path, mode=\"wb\") as file_writer:\n#         dill.dump(clf, file_writer)\n#\n# if __name__ == \"__main__\":\n#     train_from_csv.component_spec.save(\"train_from_csv.yaml\")"
  },
  {
    "repo": "MOLOZISE/MLOps_Formation",
    "file_path": "03_Iris_mlflow_example/mlfow_kubeflow_pipeline_yaml.py",
    "raw_url": "https://raw.githubusercontent.com/MOLOZISE/MLOps_Formation/main/03_Iris_mlflow_example/mlfow_kubeflow_pipeline_yaml.py",
    "content": "from functools import partial\n\nimport kfp\nfrom kfp.components import InputPath, OutputPath, create_component_from_func\nfrom kfp.dsl import pipeline\n\n\n@partial(\n    create_component_from_func,\n    packages_to_install=[\"pandas\", \"scikit-learn\"],\n)\ndef load_iris_data(\n        data_path: OutputPath(\"csv\"),\n        target_path: OutputPath(\"csv\"),\n):\n    import pandas as pd\n    from sklearn.datasets import load_iris\n\n    iris = load_iris()\n\n    data = pd.DataFrame(iris[\"data\"], columns=iris[\"feature_names\"])\n    target = pd.DataFrame(iris[\"target\"], columns=[\"target\"])\n\n    data.to_csv(data_path, index=False)\n    target.to_csv(target_path, index=False)\n\n\n@partial(\n    create_component_from_func,\n    packages_to_install=[\"dill\", \"pandas\", \"scikit-learn\", \"mlflow\"],\n)\ndef train_from_csv(\n        train_data_path: InputPath(\"csv\"),\n        train_target_path: InputPath(\"csv\"),\n        model_path: OutputPath(\"dill\"),\n        input_example_path: OutputPath(\"dill\"),\n        signature_path: OutputPath(\"dill\"),\n        conda_env_path: OutputPath(\"dill\"),\n        kernel: str,\n):\n    import dill\n    import pandas as pd\n    from sklearn.svm import SVC\n\n    from mlflow.models.signature import infer_signature\n    from mlflow.utils.environment import _mlflow_conda_env\n\n    train_data = pd.read_csv(train_data_path)\n    train_target = pd.read_csv(train_target_path)\n\n    clf = SVC(kernel=kernel)\n    clf.fit(train_data, train_target)\n\n    with open(model_path, mode=\"wb\") as file_writer:\n        dill.dump(clf, file_writer)\n\n    input_example = train_data.sample(1)\n    with open(input_example_path, \"wb\") as file_writer:\n        dill.dump(input_example, file_writer)\n\n    signature = infer_signature(train_data, clf.predict(train_data))\n    with open(signature_path, \"wb\") as file_writer:\n        dill.dump(signature, file_writer)\n\n    conda_env = _mlflow_conda_env(\n        additional_pip_deps=[\"dill\", \"pandas\", \"scikit-learn\"]\n    )\n    with open(conda_env_path, \"wb\") as file_writer:\n        dill.dump(conda_env, file_writer)\n\n\n@partial(\n    create_component_from_func,\n    packages_to_install=[\"dill\", \"pandas\", \"scikit-learn\", \"mlflow\", \"boto3\"],\n)\ndef upload_sklearn_model_to_mlflow(\n        model_name: str,\n        model_path: InputPath(\"dill\"),\n        input_example_path: InputPath(\"dill\"),\n        signature_path: InputPath(\"dill\"),\n        conda_env_path: InputPath(\"dill\"),\n):\n    import os\n    import dill\n    from mlflow.sklearn import save_model\n\n    from mlflow.tracking.client import MlflowClient\n\n    os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = \"http://minio-service.kubeflow.svc:9000\"\n    os.environ[\"AWS_ACCESS_KEY_ID\"] = \"minio\"\n    os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"minio123\"\n\n    client = MlflowClient(\"http://mlflow-server-service.mlflow-system.svc:5000\")\n\n    with open(model_path, mode=\"rb\") as file_reader:\n        clf = dill.load(file_reader)\n\n    with open(input_example_path, \"rb\") as file_reader:\n        input_example = dill.load(file_reader)\n\n    with open(signature_path, \"rb\") as file_reader:\n        signature = dill.load(file_reader)\n\n    with open(conda_env_path, \"rb\") as file_reader:\n        conda_env = dill.load(file_reader)\n\n    save_model(\n        sk_model=clf,\n        path=model_name,\n        serialization_format=\"cloudpickle\",\n        conda_env=conda_env,\n        signature=signature,\n        input_example=input_example,\n    )\n    run = client.create_run(experiment_id=\"0\")\n    client.log_artifact(run.info.run_id, model_name)\n\n\n@pipeline(name=\"mlflow_pipeline\")\ndef mlflow_pipeline(kernel: str, model_name: str):\n    iris_data = load_iris_data()\n    model = train_from_csv(\n        train_data=iris_data.outputs[\"data\"],\n        train_target=iris_data.outputs[\"target\"],\n        kernel=kernel,\n    )\n    _ = upload_sklearn_model_to_mlflow(\n        model_name=model_name,\n        model=model.outputs[\"model\"],\n        input_example=model.outputs[\"input_example\"],\n        signature=model.outputs[\"signature\"],\n        conda_env=model.outputs[\"conda_env\"],\n    )\n\n\nif __name__ == \"__main__\":\n    kfp.compiler.Compiler().compile(mlflow_pipeline, \"mlflow_pipeline.yaml\")"
  },
  {
    "repo": "MOLOZISE/MLOps_Formation",
    "file_path": "04_kogpt2_mlflow_example[failed]/kogpt2_mlflow_pipeline_yaml.py",
    "raw_url": "https://raw.githubusercontent.com/MOLOZISE/MLOps_Formation/main/04_kogpt2_mlflow_example%5Bfailed%5D/kogpt2_mlflow_pipeline_yaml.py",
    "content": "from functools import partial\n\nimport kfp\nfrom kfp.components import InputPath, OutputPath, create_component_from_func\nfrom kfp.dsl import pipeline\n\n\n@partial(\n    create_component_from_func,\n    base_image=\"ghcr.io/molozise/kogpt2:latest\",\n)\ndef kogpt2_app(\n):\n    import torch\n    import string\n    import streamlit as st\n    from transformers import GPT2LMHeadModel, PreTrainedTokenizerFast\n\n    @st.cache(allow_output_mutation=True)\n    def get_model():\n        model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n        model.eval()\n        return model\n\n    tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n                                                        bos_token='</s>',\n                                                        eos_token='</s>',\n                                                        unk_token='<unk>',\n                                                        pad_token='<pad>',\n                                                        mask_token='<mask>')\n\n    default_text = \"korean is always busy\"\n\n    N_SENT = 3\n\n    model = get_model()\n    st.title(\"KoGPT2 Demo Page(ver 2.0)\")\n\n    st.markdown(\"\"\"\n    ### model\n    | Model       |  # of params |   Type   | # of layers  | # of heads | ffn_dim | hidden_dims | \n    |--------------|:----:|:-------:|--------:|--------:|--------:|--------------:|\n    | `KoGPT2` |  125M  |  Decoder |   12     | 12      | 3072    | 768 | \n    ### sampling method\n    - greedy sampling\n    - max out length : 128/1,024\n    ## Conditional Generation\n    \"\"\")\n\n    text = st.text_area(\"Input Text:\", value=default_text)\n    st.write(text)\n    punct = ('!', '?', '.')\n\n    if text:\n        st.markdown(\"## Predict\")\n        with st.spinner('processing..'):\n            print(f'input > {text}')\n            input_ids = tokenizer(text)['input_ids']\n            gen_ids = model.generate(torch.tensor([input_ids]),\n                                     max_length=128,\n                                     repetition_penalty=2.0)\n            generated = tokenizer.decode(gen_ids[0, :].tolist()).strip()\n            if generated != '' and generated[-1] not in punct:\n                for i in reversed(range(len(generated))):\n                    if generated[i] in punct:\n                        break\n                generated = generated[:(i + 1)]\n            print(f'KoGPT > {generated}')\n        st.write(generated)\n\n@partial(\n    create_component_from_func,\n    base_image=\"ghcr.io/molozise/kogpt2:latest\",\n)\ndef kogpt2_upload_config(\n        model_path: OutputPath(\"dill\"),\n        input_example_path: OutputPath(\"dill\"),\n        signature_path: OutputPath(\"dill\"),\n        conda_env_path: OutputPath(\"dill\"),\n):\n    import torch\n    import string\n    from transformers import GPT2LMHeadModel, PreTrainedTokenizerFast\n\n    import dill\n    import pandas as pd\n\n    from mlflow.models.signature import infer_signature\n    from mlflow.utils.environment import _mlflow_conda_env\n\n    tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n                                                        bos_token='</s>',\n                                                        eos_token='</s>',\n                                                        unk_token='<unk>',\n                                                        pad_token='<pad>',\n                                                        mask_token='<mask>')\n\n    default_text = \"korean is always busy\"\n\n    N_SENT = 3\n    punct = ('!', '?', '.')\n\n    model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n    model.eval()\n\n    def generating(model, text):\n        input_ids = tokenizer(text)['input_ids']\n        gen_ids = model.generate(torch.tensor([input_ids]),\n                                 max_length=128,\n                                 repetition_penalty=2.0)\n        generated = tokenizer.decode(gen_ids[0, :].tolist()).strip()\n        if generated != '' and generated[-1] not in punct:\n            for i in reversed(range(len(generated))):\n                if generated[i] in punct:\n                    break\n            generated = generated[:(i + 1)]\n        return generated\n\n    with open(model_path, mode=\"wb\") as file_writer:\n        dill.dump(model, file_writer)\n\n    input_example = default_text\n    with open(input_example_path, \"wb\") as file_writer:\n        dill.dump(input_example, file_writer)\n\n    signature = infer_signature(default_text, generating(model, default_text))\n    with open(signature_path, \"wb\") as file_writer:\n        dill.dump(signature, file_writer)\n\n    conda_env = _mlflow_conda_env(\n        additional_pip_deps=[\"dill\", \"torch\", \"transformers\"]\n    )\n    with open(conda_env_path, \"wb\") as file_writer:\n        dill.dump(conda_env, file_writer)\n\n@partial(\n    create_component_from_func,\n    base_image=\"ghcr.io/molozise/kogpt2:latest\",\n)\ndef kogpt2_upload_to_mlflow(\n        model_name: str,\n        model_path: InputPath(\"dill\"),\n        input_example_path: InputPath(\"dill\"),\n        signature_path: InputPath(\"dill\"),\n        conda_env_path: InputPath(\"dill\"),\n):\n\n    import os\n    import dill\n    from mlflow.pytorch import save_model\n\n    from mlflow.tracking.client import MlflowClient\n\n    os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = \"http://minio-service.kubeflow.svc:9000\"\n    os.environ[\"AWS_ACCESS_KEY_ID\"] = \"minio\"\n    os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"minio123\"\n\n    client = MlflowClient(\"http://mlflow-server-service.mlflow-system.svc:5000\")\n\n    with open(model_path, mode=\"rb\") as file_reader:\n        kogpt2 = dill.load(file_reader)\n\n    with open(input_example_path, \"rb\") as file_reader:\n        input_example = dill.load(file_reader)\n\n    with open(signature_path, \"rb\") as file_reader:\n        signature = dill.load(file_reader)\n\n    with open(conda_env_path, \"rb\") as file_reader:\n        conda_env = dill.load(file_reader)\n\n    save_model(pytorch_model=kogpt2, path=model_name, conda_env=conda_env, signature=signature, pip_requirements=[\"torch\", \"transformers\"])\n\n    run = client.create_run(experiment_id=\"1\")\n    client.log_artifact(run.info.run_id, model_name)\n\n@pipeline(name=\"mlflow_pipeline kogpt2\")\ndef mlflow_pipeline(model_name: str):\n    _ = kogpt2_app()\n    model = kogpt2_upload_config()\n    _ = kogpt2_upload_to_mlflow(\n        model_name=model_name,\n        model=model.outputs[\"model\"],\n        input_example=model.outputs[\"input_example\"],\n        signature=model.outputs[\"signature\"],\n        conda_env=model.outputs[\"conda_env\"],\n    )\n\n\nif __name__ == \"__main__\":\n    kfp.compiler.Compiler().compile(mlflow_pipeline, \"mlflow_pipeline_kogpt2.yaml\")"
  },
  {
    "repo": "MOLOZISE/MLOps_Formation",
    "file_path": "flowers/flowers_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/MOLOZISE/MLOps_Formation/main/flowers/flowers_pipeline.py",
    "content": "import kfp\nfrom kfp import dsl\nfrom kfp import onprem\ndef preprocess_op(pvc_name, volume_name, volume_mount_path):\n    return dsl.ContainerOp(\n        name='Preprocess Data',\n        image='molozise/kfp-flowers-preprocess:latest',\n        arguments=['--data-path', volume_mount_path,\n                   '--img-size', 224],\n    ).apply(onprem.mount_pvc(pvc_name, volume_name=volume_name, volume_mount_path=volume_mount_path))\ndef hyp_op(pvc_name, volume_name, volume_mount_path, device):\n    return dsl.ContainerOp(\n        name='Hyperparameter Tuning',\n        image='molozise/kfp-flowers-hyperparameter:latest',\n        arguments=['--data-path', volume_mount_path,\n                   '--device', device],\n    ).apply(onprem.mount_pvc(pvc_name, volume_name=volume_name, volume_mount_path=volume_mount_path))\ndef train_op(pvc_name, volume_name, volume_mount_path, device):\n    return dsl.ContainerOp(\n        name='Train Model',\n        image='molozise/kfp-flowers-train:latest',\n        arguments=['--data-path', volume_mount_path,\n                   '--img-size', 224,\n                   '--model-name', 'surface-ConvNeXt-T',\n                   '--device', device]\n    ).apply(onprem.mount_pvc(pvc_name, volume_name=volume_name, volume_mount_path=volume_mount_path)).set_gpu_limit(4)\ndef test_op(pvc_name, volume_name, volume_mount_path, model_path, device):\n    return dsl.ContainerOp(\n        name='Test Model',\n        image='molozise/kfp-flowers-test:latest',\n        arguments=['--data-path', volume_mount_path,\n                   '--img-size', 224,\n                   '--model-path', model_path,\n                   '--device', device]\n    ).apply(onprem.mount_pvc(pvc_name, volume_name=volume_name, volume_mount_path=volume_mount_path)).set_gpu_limit(4)\n@dsl.pipeline(\n    name='Flowers Pipeline',\n    description=''\n)\ndef surface_pipeline(mode_hyp_train_test: str,\n                     preprocess_yes_no: str,\n                     model_path: str,\n                     device: str):\n    pvc_name = \"workspace-flowers\"\n    volume_name = 'pipeline'\n    volume_mount_path = '/home/jovyan'\n    with dsl.Condition(preprocess_yes_no == 'yes'):\n        _preprocess_op = preprocess_op(pvc_name, volume_name, volume_mount_path)\n    with dsl.Condition(mode_hyp_train_test == 'hyp'):\n        _hyp_op = hyp_op(pvc_name, volume_name, volume_mount_path, device).after(_preprocess_op)\n    with dsl.Condition(mode_hyp_train_test == 'train'):\n        _train_op = train_op(pvc_name, volume_name, volume_mount_path, device).after(_preprocess_op)\n    with dsl.Condition(mode_hyp_train_test == 'test'):\n        _train_op = test_op(pvc_name, volume_name, volume_mount_path, model_path, device).after(_preprocess_op)\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(surface_pipeline, './flowers_pipeline.yaml')"
  },
  {
    "repo": "MOLOZISE/MLOps_Formation",
    "file_path": "flowers_gcp/flowers_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/MOLOZISE/MLOps_Formation/main/flowers_gcp/flowers_pipeline.py",
    "content": "#!pip3 install -U kfp\nimport kfp\nimport kfp.components as comp\nfrom kfp import dsl\nfrom kfp import compiler\nfrom kfp.components import func_to_container_op\nimport time\nimport datetime\n# Function for determine deployment\nimport kfp\nfrom kfp import dsl\n\ndef preprocess_op(volume_mount_path):\n    return dsl.ContainerOp(\n        name='Preprocess Data',\n        image='molozise/kfp-flowers-gcp-preprocess:latest',\n        arguments=['--data-path', volume_mount_path,\n                   '--img-size', 224],\n    )\ndef hyp_op(volume_mount_path, device):\n    return dsl.ContainerOp(\n        name='Hyperparameter Tuning',\n        image='molozise/kfp-flowers-gcp-hyperparameter:latest',\n        arguments=['--data-path', volume_mount_path,\n                   '--device', device],\n    )\ndef train_op(volume_mount_path, device):\n    return dsl.ContainerOp(\n        name='Train Model',\n        image='molozise/kfp-flowers-gcp-train:latest',\n        arguments=['--data-path', volume_mount_path,\n                   '--img-size', 224,\n                   '--model-name', 'flowers-ConvNeXt-T',\n                   '--device', device]\n    )\ndef test_op(volume_mount_path, model_path, device):\n    return dsl.ContainerOp(\n        name='Test Model',\n        image='molozise/kfp-flowers-gcp-test:latest',\n        arguments=['--data-path', volume_mount_path,\n                   '--img-size', 224,\n                   '--model-path', model_path,\n                   '--device', device]\n    )\n@dsl.pipeline(\n    name='Flowers Pipeline',\n    description=''\n)\ndef flowers_pipeline(mode_hyp_train_test: str,\n                     preprocess_yes_no: str,\n                     model_path: str,\n                     device: str):\n    PIPELINE_HOST = \"15bf934d55d3d679-dot-us-central1.pipelines.googleusercontent.com\"  # Kubeflow Pipeline URL\n    WORK_BUCKET = \"gs://vertex-ai-example-368505-kubeflowpipelines-default\"  # Cloud Storage Bucket\n    EXPERIMENT_NAME = \"Flowers Classification Experiment\"  # Experiment Name\n    volume_mount_path = WORK_BUCKET\n    with dsl.Condition(preprocess_yes_no == 'yes'):\n        _preprocess_op = preprocess_op(volume_mount_path)\n    with dsl.Condition(mode_hyp_train_test == 'hyp'):\n        _hyp_op = hyp_op(volume_mount_path, device).after(_preprocess_op)\n    with dsl.Condition(mode_hyp_train_test == 'train'):\n        _train_op = train_op(volume_mount_path, device).after(_preprocess_op)\n    with dsl.Condition(mode_hyp_train_test == 'test'):\n        _train_op = test_op(volume_mount_path, model_path, device).after(_preprocess_op)\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(surface_pipeline, './flowers_pipeline.yaml')"
  },
  {
    "repo": "vfcarida/CI-CD-Pipeline-for-machine-learning-with-online-training-in-Kubeflow",
    "file_path": "models/sklearn_spacy_text/pipeline/pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/vfcarida/CI-CD-Pipeline-for-machine-learning-with-online-training-in-Kubeflow/master/models/sklearn_spacy_text/pipeline/pipeline.py",
    "content": "#!/usr/bin/env python3\n\n# Copyright 2019 Google LLC. This software is provided as-is, without warranty\n# or representation for any use or purpose. Your use of it is subject to your\n# agreement with Google.\n\nimport kfp\nfrom kfp import components\nfrom kfp import dsl\nfrom kfp import gcp\n\n# confusion_matrix_op = components.load_component_from_url(\n#     'https://raw.githubusercontent.com/kubeflow/pipelines/eb830cd73ca148e5a1a6485a9374c2dc068314bc/components/local/confusion_matrix/component.yaml'\n# )\n# roc_op = components.load_component_from_url(\n#     'https://raw.githubusercontent.com/kubeflow/pipelines/eb830cd73ca148e5a1a6485a9374c2dc068314bc/components/local/roc/component.yaml'\n# )\n\n\ndef train_op(train_data, eval_data, output):\n  return dsl.ContainerOp(\n      name='train_sklearn_spacy_text_model',\n      image='gcr.io/ml-cicd/sklearn_spacy_text_trainer:latest',\n      arguments=[\n          train_data,\n          eval_data,\n          output,\n      ],\n      file_outputs={\n          'model_path': '/output.txt',\n      })\n\n\ndef deploy_op(model_name, model_path, model_version):\n  return dsl.ContainerOp(\n      name='deploy_sklearn_spacy_text',\n      image='gcr.io/ml-cicd/serving_deployer:latest',\n      arguments=[\n          model_name,\n          model_path,\n          model_version,\n      ],\n      file_outputs={\n          'output': '/output.txt',\n      })\n\n\n# =======================================================================\n\n\n@dsl.pipeline(\n    name='SKLearn Model Pipeline',\n    description='A pipeline that does end-to-end training and deployment for SKLearn models.')\ndef train_deploy_pipeline(\n    model_path='gs://ml-cicd/models/sklearn_spacy_text',\n    train_data='gs://ml-cicd/data/sklearn_spacy_text/train/train.csv',\n    eval_data='gs://ml-cicd/data/sklearn_spacy_text/train/train.csv',\n    model_version='000'\n):\n  model_name = 'sklearn-spacy-text'\n\n  output_path = \"{}/{}/{}\".format(model_path, model_name, model_version)\n\n  train_operation = train_op(train_data, eval_data, output_path).apply(\n                          gcp.use_gcp_secret('user-gcp-sa'))\n\n  deploy_operation = deploy_op(model_name,\n                               train_operation.outputs[\"model_path\"],\n                               model_version)\n\n  # predict_op = predict_op(\n  #     project,\n  #     region,\n  #     create_cluster_op.output,\n  #     transform_op.outputs['eval'],\n  #     train_op.output,\n  #     target,\n  #     analyze_op.output,\n  #     output_template\n  # ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n\n  # confusion_matrix_task = confusion_matrix_op(\n  #     predict_op.output,\n  #     output_template\n  # ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n\n  # roc_task = roc_op(\n  #     predictions_dir=predict_op.output,\n  #     true_class=true_label,\n  #     true_score_column=true_label,\n  #     output_dir=output_template\n  # ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n\n\nif __name__ == '__main__':\n  kfp.compiler.Compiler().compile(train_deploy_pipeline, __file__ + '.zip')\n"
  },
  {
    "repo": "ateevwasalreadytaken/kfp_jupyter_notebook",
    "file_path": "kfp_nb_submit.py",
    "raw_url": "https://raw.githubusercontent.com/ateevwasalreadytaken/kfp_jupyter_notebook/master/kfp_nb_submit.py",
    "content": "import datetime\r\nimport kfp.compiler as compiler\r\nimport kfp.dsl as dsl\r\nimport kfp\r\nimport os\r\n\r\ntmp_dir = 'temp' #Temporary directory here\r\nimage = 'image' #Docker Image for pipeline here\r\n\r\nif not os.path.exists(tmp_dir):\r\n    os.makedirs(tmp_dir)\r\n    \r\ndef run_notebook(input_notebook: str, output_notebook: str,\r\n                 cpu: str, memory: str, gpu = None, vendor = None):\r\n    def demo_op(input_notebook: str, output_notebook: str):\r\n        return dsl.ContainerOp(\r\n            name='papermill',\r\n            image=image,\r\n            command=['sh', '-c'],\r\n            pvolumes={\"Mount\": dsl.PipelineVolume(pvc=\"xyz\",name='xyz')},  #Mount here and replace xyz\r\n            arguments=['papermill $0 $1', input_notebook, output_notebook]\r\n        )\r\n    @dsl.pipeline(\r\n        name='papermill demo',\r\n        description='executing notebooks demo'\r\n    )\r\n    def pipeline_func(input_notebook: str, output_notebook: str):\r\n    \r\n        demo_task = demo_op(input_notebook, output_notebook)\r\n        if gpu != None:\r\n            if vendor != None:\r\n                demo_task.set_gpu_limit(gpu, vendor) #default vendor is NVIDIA\r\n            else:\r\n                demo_task.set_gpu_limit(gpu) #number\r\n        demo_task.set_memory_limit(memory) #number followed by 'G' or 'M' etc.\r\n        demo_task.set_cpu_limit(cpu) #number, optionally followed by m indicateing 1/1000\r\n        \r\n        \r\n    filename = tmp_dir + '/demo{dt:%Y%m%d_%H%M%S}.pipeline.tar.gz'.format(dt=datetime.datetime.now())\r\n    print('filename: {}'.format(filename))\r\n    compiler.Compiler().compile(pipeline_func, filename)\r\n    client = kfp.Client()\r\n    experiment = client.create_experiment('name') #name of experiment\r\n    arguments = {'input_notebook': input_notebook, 'output_notebook': output_notebook}\r\n    run_name = 'name' #name of demo run\r\n    run_result = client.run_pipeline(experiment.id, run_name, filename, arguments)\r\n"
  },
  {
    "repo": "hbelmiro/kfp-test-cache-pipeline",
    "file_path": "pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/hbelmiro/kfp-test-cache-pipeline/main/pipeline.py",
    "content": "from kfp import dsl\n\n\n@dsl.component\ndef comp() -> str:\n    from datetime import datetime\n    now = datetime.now()\n    print(now)\n    return now.isoformat()\n\n\n@dsl.pipeline\ndef my_pipeline() -> str:\n    return comp().output\n"
  },
  {
    "repo": "amanknoldus/llm-dolly-v2-3b-fine-tuning-kubeflow-template",
    "file_path": "pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/amanknoldus/llm-dolly-v2-3b-fine-tuning-kubeflow-template/master/pipeline.py",
    "content": "import logging\nfrom kfp.v2 import compiler\nimport kfp\n\nfrom components.process_data import process_data\nfrom components.serve_model import serve_model_component\nfrom components.train_model import fine_tune_model\nfrom components.upload_model import upload_container\nfrom constants import (PIPELINE_DESCRIPTION, PIPELINE_NAME, PIPELINE_ROOT_GCS, ORIGINAL_MODEL_NAME, \\\n                       SAVE_MODEL_BUCKET_NAME, REGION, DATASET_BUCKET, MODEL_DISPLAY_NAME, SERVING_IMAGE, \\\n                       STAGING_BUCKET, COMPONENT_EXECUTION, DATASET_NAME, SERVING_IMAGE_TRIGGER, SERVICE_ACCOUNT_ML,\n                       DEPLOYED_MODEL_DETAILS_FILE,\n                       PIPELINE_JSON_FILE)\n\nlogging.basicConfig(level=logging.DEBUG)\nlogger = logging.getLogger(__name__)\n\n\n@kfp.dsl.pipeline(name=PIPELINE_NAME,\n                  description=PIPELINE_DESCRIPTION,\n                  pipeline_root=PIPELINE_ROOT_GCS)\ndef pipeline(\n        project_id: str,\n        job_id: str\n):\n    \"\"\"Dataset Processing\"\"\"\n    process_data_task = process_data(DATASET_BUCKET, DATASET_NAME).set_display_name(\"Data_Processing\")\n\n    \"\"\"Fine Tune Model Pipeline\"\"\"\n    train_model_task = fine_tune_model(process_data_task.outputs[\"dataset\"],\n                                       ORIGINAL_MODEL_NAME,\n                                       SAVE_MODEL_BUCKET_NAME,\n                                       COMPONENT_EXECUTION) \\\n        .after(process_data_task) \\\n        .set_display_name(\"Dolly Fine Tuning\") \\\n        .set_cpu_request(\"8\") \\\n        .set_memory_limit(\"32G\")\n\n    \"\"\"Upload model package\"\"\"\n    upload_model_task = upload_container(project_id=project_id,\n                                         trigger_id=SERVING_IMAGE_TRIGGER,\n                                         component_execution=COMPONENT_EXECUTION) \\\n        .after(train_model_task) \\\n        .set_display_name(\"Model_Upload\")\n\n    \"\"\"Serve Model To Endpoint\"\"\"\n    serve_model_component(project_id,\n                          REGION,\n                          STAGING_BUCKET,\n                          SERVING_IMAGE,\n                          MODEL_DISPLAY_NAME,\n                          COMPONENT_EXECUTION,\n                          SERVICE_ACCOUNT_ML,\n                          save_model_details_bucket=DATASET_BUCKET,\n                          model_details_file_name=DEPLOYED_MODEL_DETAILS_FILE) \\\n        .after(upload_model_task) \\\n        .set_display_name(\"Serve_Model\") \\\n        .set_cpu_request(\"8\") \\\n        .set_memory_limit(\"32G\")\n\n\ndef compile_pipeline(pipeline_template_name=f'{PIPELINE_JSON_FILE}'):\n    compiler.Compiler().compile(\n        pipeline_func=pipeline,\n        package_path=pipeline_template_name\n    )\n    return None\n\n\nif __name__ == \"__main__\":\n    compile_pipeline()\n"
  },
  {
    "repo": "nomnomnonono/ML-Pipeline-of-Paper-Category-Classification",
    "file_path": "pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/nomnomnonono/ML-Pipeline-of-Paper-Category-Classification/main/pipeline.py",
    "content": "import datetime\nimport os\n\nfrom dotenv import load_dotenv\nfrom google.cloud import aiplatform\nfrom kfp import compiler, components, dsl\n\nload_dotenv(\".env\")\nPROJECT_ID = os.environ.get(\"GCP_PROJECT_ID\")\nAR_REPOSITORY_NAME = os.environ.get(\"AR_REPOSITORY_NAME\")\nLOCATION = os.environ.get(\"LOCATION\")\nSOURCE_CSV_URI = os.environ.get(\"SOURCE_CSV_URI\")\nROOT_BUCKET = os.environ.get(\"ROOT_BUCKET\")\nPIPELINE_NAME = os.environ.get(\"PIPELINE_NAME\")\n\n\n@dsl.pipeline(\n    name=PIPELINE_NAME,\n    description=\"Vertex Piplines sample\",\n    pipeline_root=ROOT_BUCKET,\n)\ndef pipeline() -> None:\n    preprocess_op = components.load_component_from_file(\n        \"components/preprocess/component.yaml\"\n    )\n    preprocess_task = preprocess_op(src_csv=SOURCE_CSV_URI)\n\n    train_op = components.load_component_from_file(\"components/train/component.yaml\")\n    train_task = train_op(dataset=preprocess_task.outputs[\"dataset\"])\n    train_task.custom_job_spec = {\n        \"displayName\": train_task.name,\n        \"jobSpec\": {\n            \"workerPoolSpecs\": [\n                {\n                    \"machineSpec\": {\"machineType\": \"n1-standard-2\"},\n                    \"replicaCount\": 1,\n                }\n            ],\n        },\n    }\n\n    evaluate_op = components.load_component_from_file(\n        \"components/evaluate/component.yaml\"\n    )\n    _ = evaluate_op(\n        dataset=preprocess_task.outputs[\"dataset\"],\n        artifact=train_task.outputs[\"artifact\"],\n    )\n\n    deploy_op = components.load_component_from_file(\"components/deploy/component.yaml\")\n    _ = deploy_op(\n        artifact=train_task.outputs[\"artifact\"],\n        model_name=\"ml-pipeline-arxiv-paper-model\",\n        serving_container_image_uri=f\"asia-northeast1-docker.pkg.dev/{PROJECT_ID}/{AR_REPOSITORY_NAME}/serving:latest\",\n        serving_container_environment_variables='{\"APP_MODULE\": \"server:app\"}',\n        serving_container_ports=80,\n        endpoint_name=\"ml-pipeline-arxiv-paper-endpoint\",\n        deploy_name=\"ml-pipeline-arxiv-paper-deploy\",\n        machine_type=\"n1-standard-2\",\n        min_replicas=1,\n        project=PROJECT_ID,\n        location=LOCATION,\n    )\n\n\ncompiler.Compiler().compile(\n    pipeline_func=pipeline, package_path=\"ml-pipeline-arxiv-paper.json\"\n)\n\njob = aiplatform.PipelineJob(\n    display_name=\"ml-pipeline-arxiv-paper\",\n    template_path=\"ml-pipeline-arxiv-paper.json\",\n    job_id=PIPELINE_NAME + f\"-{datetime.datetime.now().strftime('%Y%m%d%H%M%S%f')[:-4]}\",\n    pipeline_root=ROOT_BUCKET,\n    enable_caching=False,\n    project=PROJECT_ID,\n    location=LOCATION,\n)\n\njob.submit()\n"
  },
  {
    "repo": "hbelmiro/kfp-importer-minio-demo",
    "file_path": "pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/hbelmiro/kfp-importer-minio-demo/main/pipeline.py",
    "content": "from kfp import dsl\nfrom kfp.dsl import Input, Dataset\n\n\n@dsl.component(base_image=\"python:3.10\")\ndef read_dataset(dataset: Input[Dataset]):\n    with open(dataset.path, \"r\") as f:\n        data = f.read()\n    print(\"Dataset content:\", data)\n\n\n@dsl.pipeline\ndef pipeline():\n    importer_task = dsl.importer(\n        artifact_uri=\"minio://mlpipeline/artifacts/input/raw_transaction_datasource.csv\",\n        artifact_class=dsl.Dataset,\n        reimport=True)\n\n    read_dataset(dataset=importer_task.output)\n"
  },
  {
    "repo": "hbelmiro/kfp-print-env-var-demo",
    "file_path": "pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/hbelmiro/kfp-print-env-var-demo/main/pipeline.py",
    "content": "from kfp import dsl\n\n\n@dsl.component(base_image=\"quay.io/hbelmiro/kfp-print-env-var-demo:latest\")\ndef comp(env_var: str) -> str:\n    import os\n\n    value = os.getenv(env_var, \"\")\n\n    if value == \"\":\n        raise Exception(\"Env var is not set\")\n\n    return value\n\n\n@dsl.pipeline\ndef my_pipeline(env_var: str) -> str:\n    comp_task = comp(env_var=env_var)\n    comp_task.set_caching_options(False)\n    return comp_task.output\n"
  },
  {
    "repo": "Taring-Community/kfp-k8s-getstarted",
    "file_path": "first-pipeline/addition_pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/Taring-Community/kfp-k8s-getstarted/main/first-pipeline/addition_pipeline.py",
    "content": "import kfp\nfrom kfp import dsl\nfrom kfp.v2.dsl import (\n    component,\n    Input,\n    Output,\n    Dataset,\n    Metrics,\n)\n\n# Connect to Kubeflow Pipelines\nclient = kfp.Client( host='http://3.34.205.245:80' )\nprint( client.list_pipelines() )\n\n#===================== COMPONENT =====================\n# First component\n@component\ndef add( a: float, b: float ) -> float:\n    print( 'Adding two numbers' )\n    return a + b\n\n#===================== PIPELINE =====================\n# Pipeline definition\n@dsl.pipeline(\n    name='addition-pipeline',\n    description='A toy pipeline that performs addition calculations.'\n    # pipeline_root='gs://kubeflow-pipeline-data/addition-pipeline',\n)\ndef add_pipeline(\n    a: float = 1,\n    b: float = 7,\n):\n    first_add_task = add( a, 4 )\n\n    second_add_task = add( first_add_task.output, b )\n\n# Specify pipeline argument values\narguments = { 'a': '7', 'b': 8 }\n\n# Submit a pipeline run using the v2 compatible mode\nclient.create_run_from_pipeline_func(\n    add_pipeline,\n    arguments=arguments\n)"
  },
  {
    "repo": "Saoussen-CH/production-ready-MLOps-on-GCP",
    "file_path": "components/tests/conftest.py",
    "raw_url": "https://raw.githubusercontent.com/Saoussen-CH/production-ready-MLOps-on-GCP/main/components/tests/conftest.py",
    "content": "import pytest\nimport kfp.dsl\nimport json\n\n\n@pytest.fixture(autouse=True)\ndef mock_kfp_artifact(mocker):\n    \"\"\"\n    This fixture mocks the Artifact object (and thus any derived\n    classes i.e Dataset, Model, etc.)\n    to return the URI as the path.\n\n    Unit tests set the URI of artifacts, however, KFP components use Artifact.path to\n    retrieve paths to files. If a URI doesn't start with gs:// or minio:// or s3://,\n    the path with be None. This behaviour is avoided by mocking the Artifact._get_path\n    method.\n\n    Args:\n        mocker: Used to patch the _get_path method in `kfp.dsl.Artifact`.\n\n    Returns:\n        None\n    \"\"\"\n\n    def _get_path(self):\n        return self.uri\n\n    # mock the _get_path method of Artifact which is used by the property path\n    mocker.patch.object(kfp.dsl.Artifact, \"_get_path\", _get_path)\n\n\n@pytest.fixture\ndef mock_output_model(mocker):\n    return mocker.MagicMock()\n\n\n@pytest.fixture\ndef mock_model_list(mocker):\n    return mocker.patch(\"google.cloud.aiplatform.Model.list\")\n\n\n@pytest.fixture\ndef mock_job_service_client(mocker):\n    return mocker.patch(\n        \"google.cloud.aiplatform_v1beta1.services.job_service.JobServiceClient\"\n    )\n\n\n@pytest.fixture\ndef mock_create_batch_prediction_job(mock_job_service_client):\n    return mock_job_service_client.return_value.create_batch_prediction_job\n\n\n@pytest.fixture\ndef mock_get_batch_prediction_job(mock_job_service_client):\n    return mock_job_service_client.return_value.get_batch_prediction_job\n\n\n@pytest.fixture\ndef mock_model_class(mocker):\n    return mocker.patch(\"google.cloud.aiplatform.Model\")\n\n\n@pytest.fixture\ndef mock_model(tmp_path):\n    return type(\n        \"MockModel\",\n        (object,),\n        {\n            \"uri\": str(tmp_path / \"model-uri\"),\n            \"metadata\": {\"resourceName\": \"model-resource-name\"},\n        },\n    )()\n\n\n@pytest.fixture\ndef mock_dataset(tmp_path):\n    return type(\"MockDataset\", (object,), {\"uri\": str(tmp_path / \"test-data-uri\")})()\n\n\n@pytest.fixture\ndef mock_metrics(tmp_path):\n    metrics_path = tmp_path / \"metrics.json\"\n    metrics = {\"problemType\": \"classification\", \"accuracy\": 0.9}\n    with open(metrics_path, \"w\") as f:\n        json.dump(metrics, f)\n    return type(\"MockMetrics\", (object,), {\"path\": str(metrics_path)})()\n\n\n@pytest.fixture\ndef mock_model_service_client(mocker):\n    return mocker.patch(\"google.cloud.aiplatform_v1.ModelServiceClient\")\n"
  },
  {
    "repo": "Saoussen-CH/production-ready-MLOps-on-GCP",
    "file_path": "components/tests/test_lookup_model_op.py",
    "raw_url": "https://raw.githubusercontent.com/Saoussen-CH/production-ready-MLOps-on-GCP/main/components/tests/test_lookup_model_op.py",
    "content": "# test_lookup_model.py\nfrom kfp.dsl import Model\nimport pytest\nimport logging\nimport components\n\nlookup_model_op = components.lookup_model_op.python_func\n\n\ndef test_lookup_single_model_found(mock_model_list, mock_output_model, tmp_path):\n    \"\"\"\n    Assert lookup_model produces expected resource name, and that list method is\n    called with the correct arguments.\n    \"\"\"\n    mock_path = str(tmp_path / \"model\")\n    mock_model_instance = mock_output_model\n    mock_model_instance.resource_name = \"my-model-resource-name\"\n    mock_model_instance.uri = mock_path\n    mock_model_list.return_value = [mock_model_instance]\n\n    found_model_resource_name, _ = lookup_model_op(\n        model_name=\"my-model\",\n        location=\"us-central1\",\n        project=\"my-project-id\",\n        fail_on_model_not_found=False,\n        model=Model(uri=mock_path),\n    )\n\n    assert found_model_resource_name == \"my-model-resource-name\"\n\n    mock_model_list.assert_called_once_with(\n        filter='display_name=\"my-model\"',\n        location=\"us-central1\",\n        project=\"my-project-id\",\n    )\n\n\ndef test_lookup_model_no_model_found(mock_model_list, tmp_path, caplog):\n    \"\"\"\n    Checks that when there are no models and fail_on_model_found = False,\n    lookup_model returns an empty string.\n    \"\"\"\n    mock_model_list.return_value = []\n\n    with caplog.at_level(logging.ERROR):\n        model_resource_name, training_dataset = lookup_model_op(\n            model_name=\"my-model\",\n            location=\"us-central1\",\n            project=\"my-project-id\",\n            fail_on_model_not_found=False,\n            model=Model(uri=str(tmp_path / \"model\")),\n        )\n\n    assert model_resource_name == \"\"\n    assert training_dataset == {}\n    assert \"No model found with name\" in caplog.text\n\n\ndef test_lookup_model_fail_on_model_not_found(mock_model_list, tmp_path):\n    \"\"\"\n    Checks that when there are no models and fail_on_model_found = True,\n    lookup_model raises a RuntimeError.\n    \"\"\"\n    mock_model_list.return_value = []\n\n    with pytest.raises(RuntimeError, match=\"Failed as model was not found\"):\n        lookup_model_op(\n            model_name=\"my-model\",\n            location=\"europe-west4\",\n            project=\"my-project-id\",\n            fail_on_model_not_found=True,\n            model=Model(uri=str(tmp_path / \"model\")),\n        )\n\n\ndef test_multiple_models_found(mock_model_list, mock_output_model, tmp_path):\n    \"\"\"\n    Checks that when multiple models are found, lookup_model raises a RuntimeError.\n    \"\"\"\n    mock_path = str(tmp_path / \"model\")\n    mock_model_instance1 = mock_output_model\n    mock_model_instance1.resource_name = \"my-model-resource-name-1\"\n    mock_model_instance1.uri = mock_path\n\n    mock_model_instance2 = mock_output_model\n    mock_model_instance2.resource_name = \"my-model-resource-name-2\"\n    mock_model_instance2.uri = mock_path\n\n    mock_model_list.return_value = [mock_model_instance1, mock_model_instance2]\n\n    with pytest.raises(\n        RuntimeError, match=\"Multiple models with name my-model were found.\"\n    ):\n        lookup_model_op(\n            model_name=\"my-model\",\n            location=\"europe-west4\",\n            project=\"my-project-id\",\n            fail_on_model_not_found=False,\n            model=Model(uri=mock_path),\n        )\n"
  },
  {
    "repo": "Saoussen-CH/production-ready-MLOps-on-GCP",
    "file_path": "components/tests/test_model_batch_predict_op.py",
    "raw_url": "https://raw.githubusercontent.com/Saoussen-CH/production-ready-MLOps-on-GCP/main/components/tests/test_model_batch_predict_op.py",
    "content": "import pytest\nfrom kfp.dsl import Model\nfrom google.cloud.aiplatform_v1beta1.types import JobState\n\nimport components\n\nmodel_batch_predict_op = components.model_batch_predict_op.python_func\n\nSKEW_THRESHOLD = {\"defaultSkewThreshold\": {\"value\": 0.001}}\nTRAIN_DATASET = {\n    \"gcsSource\": {\"uris\": [\"gs://file.csv\"]},\n    \"dataFormat\": \"csv\",\n    \"targetField\": \"col\",\n}\n\n\n@pytest.mark.parametrize(\n    (\n        \"source_format, destination_format, source_uri, monitoring_training_dataset, \"\n        \"monitoring_alert_email_addresses, monitoring_skew_config\"\n    ),\n    [\n        (\"bigquery\", \"bigquery\", \"bq://a.b.c\", None, None, None),\n        (\"csv\", \"csv\", '[\"gs://file.csv\"]', None, None, None),\n        (\"csv\", \"csv\", '[\"gs://file.csv\"]', TRAIN_DATASET, [], SKEW_THRESHOLD),\n        (\"csv\", \"csv\", '[\"gs://file.csv\"]', TRAIN_DATASET, [\"a@b.com\"], SKEW_THRESHOLD),\n    ],\n)\ndef test_model_batch_predict_successful(\n    mock_create_batch_prediction_job,\n    mock_get_batch_prediction_job,\n    tmp_path,\n    source_format,\n    destination_format,\n    source_uri,\n    monitoring_training_dataset,\n    monitoring_alert_email_addresses,\n    monitoring_skew_config,\n):\n    \"\"\"\n    Test model_batch_predict_op function for a successful batch\n    prediction job creation with different parameter configurations.\n    \"\"\"\n    mock_create_batch_prediction_job.return_value.name = (\n        \"projects/my-project/locations/us-central1/batchPredictionJobs/123456789\"\n    )\n    mock_get_batch_prediction_job.return_value.state = JobState.JOB_STATE_SUCCEEDED\n\n    job_display_name = \"test-batch-prediction-job\"\n    location = \"us-central1\"\n    project = \"my-project\"\n    destination_uri = \"gs://destination-uri\"\n    gcp_resources_path = str(tmp_path / \"gcp_resources.json\")\n\n    (gcp_resources,) = model_batch_predict_op(\n        model=Model(\n            uri=\"gs://model-uri\", metadata={\"resourceName\": \"model-resource-name\"}\n        ),\n        gcp_resources=gcp_resources_path,\n        job_display_name=job_display_name,\n        location=location,\n        project=project,\n        source_uri=source_uri,\n        destination_uri=destination_uri,\n        source_format=source_format,\n        destination_format=destination_format,\n        monitoring_training_dataset=monitoring_training_dataset,\n        monitoring_alert_email_addresses=monitoring_alert_email_addresses,\n        monitoring_skew_config=monitoring_skew_config,\n    )\n\n    assert gcp_resources is not None\n    with open(gcp_resources_path, \"r\") as f:\n        content = f.read()\n        assert \"BatchPredictionJob\" in content\n        assert (\n            \"projects/my-project/locations/us-central1/batchPredictionJobs/123456789\"\n            in content\n        )\n    mock_create_batch_prediction_job.assert_called_once()\n    mock_get_batch_prediction_job.assert_called_once()\n\n\ndef test_model_batch_predict_failed(\n    mock_create_batch_prediction_job, mock_get_batch_prediction_job, tmp_path\n):\n    \"\"\"\n    Test the model_batch_predict_op function for a failed batch prediction job creation.\n    \"\"\"\n    mock_create_batch_prediction_job.return_value.name = (\n        \"projects/my-project/locations/us-central1/batchPredictionJobs/123456789\"\n    )\n    mock_get_batch_prediction_job.return_value.state = JobState.JOB_STATE_FAILED\n\n    job_display_name = \"test-batch-prediction-job\"\n    location = \"us-central1\"\n    project = \"my-project\"\n    source_uri = \"gs://source-uri\"\n    destination_uri = \"gs://destination-uri\"\n    source_format = \"jsonl\"\n    destination_format = \"jsonl\"\n    gcp_resources_path = str(tmp_path / \"gcp_resources.json\")\n\n    with pytest.raises(RuntimeError):\n        model_batch_predict_op(\n            model=Model(\n                uri=\"gs://model-uri\", metadata={\"resourceName\": \"model-resource-name\"}\n            ),\n            gcp_resources=gcp_resources_path,\n            job_display_name=job_display_name,\n            location=location,\n            project=project,\n            source_uri=source_uri,\n            destination_uri=destination_uri,\n            source_format=source_format,\n            destination_format=destination_format,\n            machine_type=\"n1-standard-2\",\n            starting_replica_count=1,\n            max_replica_count=1,\n        )\n"
  },
  {
    "repo": "Saoussen-CH/production-ready-MLOps-on-GCP",
    "file_path": "components/tests/test_upload_best_model_op.py",
    "raw_url": "https://raw.githubusercontent.com/Saoussen-CH/production-ready-MLOps-on-GCP/main/components/tests/test_upload_best_model_op.py",
    "content": "import json\nimport logging\nfrom kfp.dsl import Dataset, Metrics, Model\nfrom unittest import mock\nfrom google.protobuf.json_format import ParseDict\nfrom google.cloud.aiplatform_v1 import ModelEvaluation\nfrom google_cloud_pipeline_components.types.artifact_types import VertexModel\n\nimport components\n\nupload_model = components.upload_best_model_op.python_func\n\n\ndef test_model_upload_no_champion(\n    mock_model_class, mock_model_service_client, caplog, tmp_path\n):\n    caplog.set_level(logging.INFO)\n\n    metrics_json = json.dumps(\n        {\n            \"problemType\": \"regression\",\n            \"auc\": 0.4,\n            \"accuracy\": 0.80,\n        }\n    )\n    metrics_file_path = tmp_path / \"metrics.json\"\n    metrics_file_path.write_text(metrics_json)\n\n    mock_model_class.list.return_value = []\n\n    model = Model(uri=\"dummy-model-uri\")\n    serving_container_image = \"dummy_image:latest\"\n    model_name = \"dummy-model-name\"\n    model_description = \"dummy model_description\"\n    vertex_model = VertexModel.create(\n        name=model_name, uri=\"chall_uri\", model_resource_name=\"chall_resource_name\"\n    )\n    project = \"dummy-project\"\n    location = \"dummy-location\"\n    model_evaluation = Metrics(uri=\"\")\n    model_evaluation.path = str(metrics_file_path)\n    eval_metric = \"auc\"\n    eval_lower_is_better = False\n\n    pipeline_job_id = \"dummy-pipeline-job-id\"\n    test_dataset = Dataset(uri=\"test-dataset-uri\")\n    evaluation_name = \"dummy evaluation name\"\n\n    upload_model(\n        model=model,\n        model_description=model_description,\n        serving_container_image=serving_container_image,\n        vertex_model=vertex_model,\n        project=project,\n        location=location,\n        model_eval_metrics=model_evaluation,\n        eval_metric=eval_metric,\n        eval_lower_is_better=eval_lower_is_better,\n        model_name=model_name,\n        pipeline_job_id=pipeline_job_id,\n        test_data=test_dataset,\n        evaluation_name=evaluation_name,\n    )\n\n    # Check that model lookup is performed, and no existing model is found\n    mock_model_class.list.assert_called_once_with(\n        filter=f'display_name=\"{model_name}\"',\n        location=location,\n        project=project,\n    )\n    assert \"found 0 models\" in caplog.text\n\n    # Check no model comparison occurs\n    assert \"wins\" not in caplog.text\n\n    # Check model upload call\n    mock_model_class.upload.assert_called_once_with(\n        display_name=model_name,\n        description=model_description,\n        artifact_uri=\"dummy-model-uri\",\n        serving_container_image_uri=serving_container_image,\n        parent_model=None,\n        is_default_version=True,\n    )\n\n    # check model output URI\n    assert vertex_model.uri == f\"https://{location}-aiplatform.googleapis.com/v1/\" + (\n        str(mock_model_class.upload.return_value.versioned_resource_name)\n    )\n\n    # check evaluation import\n    mock_model_service_client.return_value.import_model_evaluation.assert_called_once_with(  # noqa\n        parent=mock_model_class.upload.return_value.versioned_resource_name,\n        model_evaluation=mock.ANY,\n    )\n\n\ndef test_model_upload_challenger_wins(\n    mock_model_class, mock_model_service_client, caplog, tmp_path, mocker\n):\n    caplog.set_level(logging.INFO)\n\n    metrics_json = json.dumps(\n        {\n            \"problemType\": \"regression\",\n            \"auc\": 0.4,\n            \"accuracy\": 0.80,\n        }\n    )\n    metrics_file_path = tmp_path / \"metrics.json\"\n    metrics_file_path.write_text(metrics_json)\n\n    # create mock champion model\n    mock_champion_model = mocker.Mock()\n    mock_champion_model.version_id = \"123\"\n    dummy_champion_eval = ModelEvaluation()\n    dummy_champion_metrics = {\n        \"auc\": 0.2,\n        \"f1\": 0.7,\n    }\n    message_dict = {\n        \"displayName\": \"Previously imported evaluation\",\n        \"metricsSchemaUri\": \"gs://google-cloud-aiplatform/schema/modelevaluation/regression_metrics_1.0.0.yaml\",  # noqa\n        \"metrics\": dummy_champion_metrics,\n        \"metadata\": {\n            \"pipeline_job_id\": \"dummy-pipeline-id\",\n            \"evaluation_dataset_type\": \"gcs\",\n            \"evaluation_dataset_path\": [\"dummy-gcs-uri\"],\n        },\n    }\n    ParseDict(message_dict, dummy_champion_eval._pb)\n    mock_champion_model.get_model_evaluation.return_value._gca_resource = (\n        dummy_champion_eval\n    )\n    mock_champion_model.resource_name = \"dummy-champion-resource-name\"\n    mock_model_class.list.return_value = [mock_champion_model]\n\n    # create mock challenger model\n    model = Model(uri=\"dummy-model-uri\")\n    serving_container_image = \"dummy_image:latest\"\n    model_name = \"dummy-model-name\"\n    model_description = \"dummy model_description\"\n    vertex_model = VertexModel.create(\n        name=model_name, uri=\"chall_uri\", model_resource_name=\"chall_resource_name\"\n    )\n    project = \"dummy-project\"\n    location = \"dummy-location\"\n    model_evaluation = Metrics(uri=\"\")\n    model_evaluation.path = str(metrics_file_path)\n    eval_metric = \"auc\"\n    eval_lower_is_better = False\n\n    pipeline_job_id = \"dummy-pipeline-job-id\"\n    test_dataset = Dataset(uri=\"test-dataset-uri\")\n    evaluation_name = \"dummy evaluation name\"\n\n    upload_model(\n        model=model,\n        model_description=model_description,\n        serving_container_image=serving_container_image,\n        vertex_model=vertex_model,\n        project=project,\n        location=location,\n        model_eval_metrics=model_evaluation,\n        eval_metric=eval_metric,\n        eval_lower_is_better=eval_lower_is_better,\n        model_name=model_name,\n        pipeline_job_id=pipeline_job_id,\n        test_data=test_dataset,\n        evaluation_name=evaluation_name,\n    )\n\n    # Check that model lookup is performed, and no existing model is found\n    mock_model_class.list.assert_called_once_with(\n        filter=f'display_name=\"{model_name}\"',\n        location=location,\n        project=project,\n    )\n    assert \"is being challenged by new model\" in caplog.text\n\n    # Check challenger wins in model comparison\n    assert \"Challenger wins!\" in caplog.text\n\n    # Check model upload call\n    mock_model_class.upload.assert_called_once_with(\n        display_name=model_name,\n        description=model_description,\n        artifact_uri=\"dummy-model-uri\",\n        serving_container_image_uri=serving_container_image,\n        parent_model=mock_champion_model.resource_name,\n        is_default_version=True,\n    )\n\n    # check model output URI\n    assert vertex_model.uri == f\"https://{location}-aiplatform.googleapis.com/v1/\" + (\n        str(mock_model_class.upload.return_value.versioned_resource_name)\n    )\n\n    # check evaluation import\n    mock_model_service_client.return_value.import_model_evaluation.assert_called_once_with(  # noqa\n        parent=mock_model_class.upload.return_value.versioned_resource_name,\n        model_evaluation=mock.ANY,\n    )\n\n\ndef test_model_upload_champion_wins(\n    mock_model_class, mock_model_service_client, caplog, tmp_path, mocker\n):\n    caplog.set_level(logging.INFO)\n\n    metrics_json = json.dumps(\n        {\n            \"problemType\": \"regression\",\n            \"auc\": 0.4,\n            \"accuracy\": 0.80,\n        }\n    )\n    metrics_file_path = tmp_path / \"metrics.json\"\n    metrics_file_path.write_text(metrics_json)\n\n    # Create mock champion model\n    mock_champion_model = mocker.Mock()\n    mock_champion_model.version_id = \"123\"\n    dummy_champion_eval = ModelEvaluation()\n    dummy_champion_metrics = {\n        \"auc\": 0.8,\n        \"f1\": 0.7,\n    }\n    message_dict = {\n        \"displayName\": \"Previously imported evaluation\",\n        \"metricsSchemaUri\": \"gs://google-cloud-aiplatform/schema/modelevaluation/regression_metrics_1.0.0.yaml\",  # noqa\n        \"metrics\": dummy_champion_metrics,\n        \"metadata\": {\n            \"pipeline_job_id\": \"dummy-pipeline-id\",\n            \"evaluation_dataset_type\": \"gcs\",\n            \"evaluation_dataset_path\": [\"dummy-gcs-uri\"],\n        },\n    }\n    ParseDict(message_dict, dummy_champion_eval._pb)\n    mock_champion_model.get_model_evaluation.return_value._gca_resource = (\n        dummy_champion_eval\n    )\n    mock_champion_model.resource_name = \"dummy-champion-resource-name\"\n    mock_model_class.list.return_value = [mock_champion_model]\n\n    # Create mock challenger model\n    model = Model(uri=\"dummy-model-uri\")\n    serving_container_image = \"dummy_image:latest\"\n    model_name = \"dummy-model-name\"\n    model_description = \"dummy model_description\"\n    vertex_model = VertexModel.create(\n        name=model_name, uri=\"chall_uri\", model_resource_name=\"chall_resource_name\"\n    )\n    project = \"dummy-project\"\n    location = \"dummy-location\"\n    model_evaluation = Metrics(uri=\"\")\n    model_evaluation.path = str(metrics_file_path)\n    eval_metric = \"auc\"\n    eval_lower_is_better = False\n\n    pipeline_job_id = \"dummy-pipeline-job-id\"\n    test_dataset = Dataset(uri=\"test-dataset-uri\")\n    evaluation_name = \"dummy evaluation name\"\n\n    upload_model(\n        model=model,\n        model_description=model_description,\n        serving_container_image=serving_container_image,\n        vertex_model=vertex_model,\n        project=project,\n        location=location,\n        model_eval_metrics=model_evaluation,\n        eval_metric=eval_metric,\n        eval_lower_is_better=eval_lower_is_better,\n        model_name=model_name,\n        pipeline_job_id=pipeline_job_id,\n        test_data=test_dataset,\n        evaluation_name=evaluation_name,\n    )\n\n    # Check that model lookup is performed, and the existing champion model is found\n    mock_model_class.list.assert_called_once_with(\n        filter=f'display_name=\"{model_name}\"',\n        location=location,\n        project=project,\n    )\n    assert \"is being challenged by new model\" in caplog.text\n\n    # Check champion wins in model comparison\n    assert \"Champion wins!\" in caplog.text\n\n    # Check model upload call\n    mock_model_class.upload.assert_called_once_with(\n        display_name=model_name,\n        description=model_description,\n        artifact_uri=\"dummy-model-uri\",\n        serving_container_image_uri=serving_container_image,\n        parent_model=mock_champion_model.resource_name,\n        is_default_version=False,\n    )\n\n    # Check model output URI\n    assert vertex_model.uri == f\"https://{location}-aiplatform.googleapis.com/v1/\" + (\n        str(mock_model_class.upload.return_value.versioned_resource_name)\n    )\n\n    # check evaluation import\n    mock_model_service_client.return_value.import_model_evaluation.assert_called_once_with(  # noqa\n        parent=mock_model_class.upload.return_value.versioned_resource_name,\n        model_evaluation=mock.ANY,\n    )\n"
  },
  {
    "repo": "Saoussen-CH/production-ready-MLOps-on-GCP",
    "file_path": "components/src/components/extract_table_to_gcs_op.py",
    "raw_url": "https://raw.githubusercontent.com/Saoussen-CH/production-ready-MLOps-on-GCP/main/components/src/components/extract_table_to_gcs_op.py",
    "content": "from kfp.dsl import Dataset, Artifact, component, Input, Output\n\n\n@component(\n    base_image=\"python:3.10.14\", packages_to_install=[\"google-cloud-bigquery==3.24.0\"]\n)\ndef extract_table_to_gcs_op(\n    bq_table: Input[Artifact],\n    dataset: Output[Dataset],\n    location: str = \"US\",\n) -> None:\n    \"\"\"\n    Extract a Big Query table into Google Cloud Storage.\n    \"\"\"\n\n    import google.cloud.bigquery as bq\n\n    project_id = bq_table.metadata[\"projectId\"]\n    dataset_id = bq_table.metadata[\"datasetId\"]\n    table_id = bq_table.metadata[\"tableId\"]\n\n    # Get the table generated on the previous component\n    full_table_id = f\"{project_id}.{dataset_id}.{table_id}\"\n    table = bq.table.Table(table_ref=full_table_id)\n\n    # Initiate the Big Query client to connect with the project\n    # job_config = bq.job.ExtractJobConfig(**{})\n    client = bq.client.Client(project=project_id, location=location)\n\n    # Submit the extract table job to store on GCS\n    extract_job = client.extract_table(table, dataset.uri)\n\n    # Wait for the extract job to complete\n    extract_job.result()\n"
  },
  {
    "repo": "Saoussen-CH/production-ready-MLOps-on-GCP",
    "file_path": "components/src/components/get_custom_job_results_op.py",
    "raw_url": "https://raw.githubusercontent.com/Saoussen-CH/production-ready-MLOps-on-GCP/main/components/src/components/get_custom_job_results_op.py",
    "content": "from kfp.dsl import component, Metrics, Output, Model\n\n\n@component(\n    base_image=\"python:3.10.14\",\n    packages_to_install=[\n        \"google-cloud-pipeline-components==2.14.1\",\n        \"google-cloud-aiplatform==1.55.0\",\n    ],\n)\ndef get_custom_job_results_op(\n    project: str,\n    location: str,\n    job_resource: str,\n    model: Output[Model],\n    metrics: Output[Metrics],\n):\n    import json\n    import shutil\n    from pathlib import Path\n    import google.cloud.aiplatform as aip\n    from google.protobuf.json_format import Parse\n    from google_cloud_pipeline_components.proto.gcp_resources_pb2 import GcpResources\n\n    aip.init(project=project, location=location)\n\n    training_gcp_resources = Parse(job_resource, GcpResources())\n    custom_job_id = training_gcp_resources.resources[0].resource_uri\n    custom_job_name = custom_job_id[custom_job_id.find(\"project\") :]\n\n    job_resource = aip.CustomJob.get(custom_job_name).gca_resource\n\n    job_base_dir = job_resource.job_spec.base_output_directory.output_uri_prefix\n\n    job_base_dir_fuse = job_base_dir.replace(\"gs://\", \"/gcs/\")\n    model_uri_fuse = model.uri.replace(\"gs://\", \"/gcs/\")\n\n    shutil.copytree(\n        f\"{job_base_dir_fuse}/model\", Path(model_uri_fuse), dirs_exist_ok=True\n    )\n\n    with open(f\"{job_base_dir_fuse}/metrics/metrics.json\") as fh:\n        metrics_dict = json.load(fh)\n\n    for k, v in metrics_dict.items():\n        metrics.log_metric(k, v)\n\n    with open(metrics.path, \"w\") as fh:\n        json.dump(metrics_dict, fh)\n\n    shutil.rmtree(f\"{job_base_dir_fuse}/model\")\n    shutil.rmtree(f\"{job_base_dir_fuse}/metrics\")\n"
  },
  {
    "repo": "Saoussen-CH/production-ready-MLOps-on-GCP",
    "file_path": "components/src/components/get_hyperparameter_tuning_results_op.py",
    "raw_url": "https://raw.githubusercontent.com/Saoussen-CH/production-ready-MLOps-on-GCP/main/components/src/components/get_hyperparameter_tuning_results_op.py",
    "content": "from kfp.dsl import component\n\n\n@component(\n    base_image=\"python:3.10.14\",\n    packages_to_install=[\n        \"google-cloud-pipeline-components==2.14.1\",\n        \"google-cloud-aiplatform==1.55.0\",\n    ],\n)\ndef get_hyperparameter_tuning_results_op(\n    project: str, location: str, job_resource: str, study_spec_metrics: list\n) -> dict:\n    import google.cloud.aiplatform as aip\n    from google_cloud_pipeline_components.proto.gcp_resources_pb2 import GcpResources\n    from google.protobuf.json_format import Parse\n    from google.cloud.aiplatform_v1.types import study\n\n    aip.init(project=project, location=location)\n\n    gcp_resources_proto = Parse(job_resource, GcpResources())\n    tuning_job_id = gcp_resources_proto.resources[0].resource_uri\n    tuning_job_name = tuning_job_id[tuning_job_id.find(\"project\") :]\n\n    job_resource = aip.HyperparameterTuningJob.get(tuning_job_name).gca_resource\n\n    trials = job_resource.trials\n\n    if len(study_spec_metrics) > 1:\n        raise RuntimeError(\n            \"Unable to determine best parameters for multi-objective hyperparameter tuning.\"  # noqa: E501\n        )\n\n    goal = study_spec_metrics[0][\"goal\"]\n    if goal == study.StudySpec.MetricSpec.GoalType.MAXIMIZE:\n        best_fn = max\n    elif goal == study.StudySpec.MetricSpec.GoalType.MINIMIZE:\n        best_fn = min\n    best_trial = best_fn(\n        trials, key=lambda trial: trial.final_measurement.metrics[0].value\n    )\n\n    return {p.parameter_id: p.value for p in best_trial.parameters}\n"
  },
  {
    "repo": "Saoussen-CH/production-ready-MLOps-on-GCP",
    "file_path": "components/src/components/get_training_args_dict_op.py",
    "raw_url": "https://raw.githubusercontent.com/Saoussen-CH/production-ready-MLOps-on-GCP/main/components/src/components/get_training_args_dict_op.py",
    "content": "from kfp.dsl import Input, component, Dataset\n\n\n@component(base_image=\"python:3.10.14\")\ndef get_training_args_dict_op(\n    train_data: Input[Dataset],\n    valid_data: Input[Dataset],\n    test_data: Input[Dataset],\n    hypertune: bool,\n) -> dict:\n    return dict(\n        train_data=train_data.path,\n        valid_data=valid_data.path,\n        test_data=test_data.path,\n        hypertune=hypertune,\n    )\n"
  },
  {
    "repo": "Saoussen-CH/production-ready-MLOps-on-GCP",
    "file_path": "components/src/components/get_workerpool_spec_op.py",
    "raw_url": "https://raw.githubusercontent.com/Saoussen-CH/production-ready-MLOps-on-GCP/main/components/src/components/get_workerpool_spec_op.py",
    "content": "from kfp.dsl import component\n\n\n@component(base_image=\"python:3.10.14\")\ndef get_workerpool_spec_op(\n    worker_pool_specs: list,\n    args: dict = {},\n    hyperparams: dict = {},\n    env: dict = {},\n) -> list:\n    for spec in worker_pool_specs:\n        if \"args\" not in spec[\"container_spec\"]:\n            spec[\"container_spec\"][\"args\"] = []\n        for k, v in args.items():\n            spec[\"container_spec\"][\"args\"].append(f\"--{k.replace('_', '-')}={v}\")\n        for k, v in hyperparams.items():\n            spec[\"container_spec\"][\"args\"].append(f\"--{k.replace('_', '-')}={v}\")\n\n        if env:\n            if \"env\" not in spec[\"container_spec\"]:\n                spec[\"container_spec\"][\"env\"] = []\n            for k, v in env.items():\n                spec[\"container_spec\"][\"env\"].append(dict(name=k, value=v))\n\n    return worker_pool_specs\n"
  },
  {
    "repo": "Saoussen-CH/production-ready-MLOps-on-GCP",
    "file_path": "components/src/components/lookup_model_op.py",
    "raw_url": "https://raw.githubusercontent.com/Saoussen-CH/production-ready-MLOps-on-GCP/main/components/src/components/lookup_model_op.py",
    "content": "from kfp.dsl import component, Output, Model\nfrom typing import NamedTuple\n\n\n@component(\n    base_image=\"python:3.10.14\",\n    packages_to_install=[\"google-cloud-aiplatform==1.55.0\"],\n)\ndef lookup_model_op(\n    model_name: str,\n    location: str,\n    project: str,\n    model: Output[Model],\n    fail_on_model_not_found: bool = False,\n) -> NamedTuple(\"Outputs\", [(\"model_resource_name\", str), (\"training_dataset\", dict)]):  # noqa: E501 # type: ignore\n    \"\"\"\n    Fetch a model given a model name (display name) and export to GCS.\n\n    Args:\n        model_name (str): display name of the model\n        location (str): location of the Google Cloud project\n        project (str): project id of the Google Cloud project\n        model (Output[Model]): a Vertex AI model\n        fail_on_model_not_found (bool): if set to True, raise runtime error if\n            model is not found\n\n    Returns:\n        str: Resource name of the found model. Empty string if model not found.\n    \"\"\"\n\n    import json\n    import logging\n    import os\n    from pathlib import Path\n    from google.cloud.aiplatform import Model\n\n    TRAINING_DATASET_INFO = \"training_dataset.json\"\n\n    logging.info(f\"listing models with display name {model_name}\")\n    models = Model.list(\n        filter=f'display_name=\"{model_name}\"',\n        location=location,\n        project=project,\n    )\n    logging.info(f\"found {len(models)} model(s)\")\n\n    training_dataset = {}\n    model_resource_name = \"\"\n    if len(models) == 0:\n        logging.error(\n            f\"No model found with name {model_name} \"\n            + f\"(project: {project} location: {location})\"\n        )\n        if fail_on_model_not_found:\n            raise RuntimeError(\"Failed as model was not found\")\n    elif len(models) == 1:\n        target_model = models[0]\n        model_resource_name = target_model.resource_name\n        logging.info(f\"model display name: {target_model.display_name}\")\n        logging.info(f\"model resource name: {target_model.resource_name}\")\n        logging.info(f\"model uri: {target_model.uri}\")\n        model.uri = target_model.uri\n        model.metadata[\"resourceName\"] = target_model.resource_name\n        logging.info(f\"target_model.resource_name : {target_model.resource_name} \")\n\n        path = Path(model.path) / TRAINING_DATASET_INFO\n        logging.info(f\"Reading training dataset metadata: {path}\")\n\n        if os.path.exists(path):\n            with open(path, \"r\") as fp:\n                training_dataset = json.load(fp)\n            logging.info(f\"Training dataset info for model monitoring path: {path}\")\n            logging.info(f\"Training dataset: {training_dataset}\")\n        else:\n            logging.warning(\"Training dataset metadata doesn't exist!\")\n    else:\n        raise RuntimeError(f\"Multiple models with name {model_name} were found.\")\n\n    return model_resource_name, training_dataset\n"
  },
  {
    "repo": "Saoussen-CH/production-ready-MLOps-on-GCP",
    "file_path": "components/src/components/model_batch_predict_op.py",
    "raw_url": "https://raw.githubusercontent.com/Saoussen-CH/production-ready-MLOps-on-GCP/main/components/src/components/model_batch_predict_op.py",
    "content": "from kfp.dsl import Input, Model, component, OutputPath\nfrom typing import List, NamedTuple\n\n\n@component(\n    base_image=\"python:3.10\",\n    packages_to_install=[\n        \"google-cloud-pipeline-components==2.14.1\",\n        \"google-cloud-aiplatform==1.55.0\",\n    ],\n)\ndef model_batch_predict_op(\n    model: Input[Model],\n    gcp_resources: OutputPath(str),  # type: ignore\n    job_display_name: str,\n    location: str,\n    project: str,\n    source_uri: str,\n    destination_uri: str,\n    source_format: str,\n    destination_format: str,\n    machine_type: str = \"n1-standard-2\",\n    starting_replica_count: int = 1,\n    max_replica_count: int = 1,\n    monitoring_training_dataset: dict = None,\n    monitoring_alert_email_addresses: List[str] = None,\n    notification_channels: List[str] = [],\n    monitoring_skew_config: dict = None,\n    instance_config: dict = None,\n) -> NamedTuple(\"Outputs\", [(\"gcp_resources\", str)]):  # type: ignore\n    \"\"\"\n    Trigger a batch prediction job and enable monitoring.\n\n    Args:\n        model (Input[Model]): Input model to use for calculating predictions.\n        job_display_name: Name of the batch prediction job.\n        location (str): location of the Google Cloud project. Defaults to None.\n        project (str): project id of the Google Cloud project. Defaults to None.\n        source_uri (str): bq:// URI or a list of gcs:// URIs to read input instances.\n        destination_uri (str): bq:// or gs:// URI to store output predictions.\n        source_format (str): E.g. \"bigquery\", \"jsonl\", \"csv\". See:\n            https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform_v1beta1.types.BatchPredictionJob.InputConfig\n        destination_format (str): E.g. \"bigquery\", \"jsonl\", \"csv\". See:\n            https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform_v1beta1.types.BatchPredictionJob.OutputConfig\n        machine_type (str): Machine type.\n        starting_replica_count (int): Starting replica count.\n        max_replica_count (int): Max replicat count.\n        monitoring_skew_config (dict): Configuration of training-serving skew. See:\n            https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform_v1beta1.types.ModelMonitoringObjectiveConfig.TrainingPredictionSkewDetectionConfig\n        monitoring_alert_email_addresses (List[str]):\n            Email addresses to send alerts to (optional).\n        notification_channels (List[str]):\n            Notification channels to send alerts to (optional).\n            Format: projects/<project>/notificationChannels/<notification_channel>\n        monitoring_training_dataset (dict): Metadata of training dataset. See:\n            https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform_v1beta1.types.ModelMonitoringObjectiveConfig.TrainingDataset\n        instance_config (dict): Configuration defining how to transform batch prediction\n            input instances to the instances that the Model accepts. See:\n            https://cloud.google.com/vertex-ai/docs/reference/rest/v1beta1/projects.locations.batchPredictionJobs#instanceconfig\n    Returns:\n        OutputPath: gcp_resources for Vertex AI UI integration.\n    \"\"\"\n\n    import logging\n    import time\n\n    from functools import partial\n    from google.protobuf.json_format import ParseDict, MessageToJson\n    from google.cloud.aiplatform_v1beta1.services.job_service import JobServiceClient\n    from google.cloud.aiplatform_v1beta1.types import (\n        BatchPredictionJob,\n        GetBatchPredictionJobRequest,\n    )\n    from google.cloud.aiplatform_v1beta1.types.job_state import JobState\n    from google_cloud_pipeline_components.container.v1.gcp_launcher.utils import (\n        error_util,\n    )\n    from google_cloud_pipeline_components.container.utils import execution_context\n    from google_cloud_pipeline_components.proto.gcp_resources_pb2 import GcpResources\n\n    def send_cancel_request(client: JobServiceClient, batch_job_uri: str):\n        logging.info(\"Sending BatchPredictionJob cancel request\")\n        client.cancel_batch_prediction_job(name=batch_job_uri)\n\n    def is_job_successful(job_state: JobState) -> bool:\n        _JOB_SUCCESSFUL_STATES = [\n            JobState.JOB_STATE_SUCCEEDED,\n        ]\n        _JOB_FAILED_STATES = [\n            JobState.JOB_STATE_FAILED,\n            JobState.JOB_STATE_CANCELLED,\n            JobState.JOB_STATE_EXPIRED,\n        ]\n\n        if job_state in _JOB_SUCCESSFUL_STATES:\n            logging.info(\n                f\"GetBatchPredictionJobRequest response state={job_state}. \"\n                \"Job completed\"\n            )\n            return True\n        elif job_state in _JOB_FAILED_STATES:\n            raise RuntimeError(\n                \"Job {} failed with error state: {}.\".format(response.name, job_state)\n            )\n        else:\n            logging.info(f\"Job {response.name} is in a non-final state {job_state}.\")\n        return False\n\n    _POLLING_INTERVAL_IN_SECONDS = 20\n    _CONNECTION_ERROR_RETRY_LIMIT = 5\n\n    api_endpoint = f\"{location}-aiplatform.googleapis.com\"\n\n    input_config = {\"instancesFormat\": source_format}\n    output_config = {\"predictionsFormat\": destination_format}\n    if source_format == \"bigquery\" and destination_format == \"bigquery\":\n        input_config[\"bigquerySource\"] = {\"inputUri\": source_uri}\n        output_config[\"bigqueryDestination\"] = {\"outputUri\": destination_uri}\n    else:\n        input_config[\"gcsSource\"] = {\"uris\": [source_uri]}\n        output_config[\"gcsDestination\"] = {\"outputUriPrefix\": destination_uri}\n\n    message = {\n        \"displayName\": job_display_name,\n        \"model\": model.metadata[\"resourceName\"],\n        \"inputConfig\": input_config,\n        \"outputConfig\": output_config,\n        \"dedicatedResources\": {\n            \"machineSpec\": {\"machineType\": machine_type},\n            \"startingReplicaCount\": starting_replica_count,\n            \"maxReplicaCount\": max_replica_count,\n        },\n    }\n\n    if instance_config:\n        message[\"instanceConfig\"] = instance_config\n\n    if monitoring_training_dataset and monitoring_skew_config:\n        logging.info(\"Adding monitoring config to request\")\n        if not monitoring_alert_email_addresses:\n            monitoring_alert_email_addresses = []\n\n        message[\"modelMonitoringConfig\"] = {\n            \"alertConfig\": {\n                \"emailAlertConfig\": {\"userEmails\": monitoring_alert_email_addresses},\n                \"notificationChannels\": notification_channels,\n                \"enableLogging\": True,\n            },\n            \"objectiveConfigs\": [\n                {\n                    \"trainingDataset\": monitoring_training_dataset,\n                    \"trainingPredictionSkewDetectionConfig\": monitoring_skew_config,\n                }\n            ],\n        }\n\n    request = ParseDict(message, BatchPredictionJob()._pb)\n\n    logging.info(f\"Submitting batch prediction job: {job_display_name}\")\n    logging.info(request)\n    client = JobServiceClient(client_options={\"api_endpoint\": api_endpoint})\n    response = client.create_batch_prediction_job(\n        parent=f\"projects/{project}/locations/{location}\",\n        batch_prediction_job=request,\n    )\n    logging.info(f\"Submitted batch prediction job: {response.name}\")\n\n    # output GCP resource for Vertex AI UI integration\n    batch_job_resources = GcpResources()\n    dr = batch_job_resources.resources.add()\n    dr.resource_type = \"BatchPredictionJob\"\n    dr.resource_uri = response.name\n    with open(gcp_resources, \"w\") as f:\n        f.write(MessageToJson(batch_job_resources))\n\n    with execution_context.ExecutionContext(\n        on_cancel=partial(\n            send_cancel_request,\n            api_endpoint,\n            response.name,\n        )\n    ):\n        retry_count = 0\n        while True:\n            try:\n                job_status_request = GetBatchPredictionJobRequest(\n                    {\"name\": response.name}\n                )\n                job_state = client.get_batch_prediction_job(\n                    request=job_status_request\n                ).state\n                retry_count = 0\n            except ConnectionError as err:\n                retry_count += 1\n                if retry_count <= _CONNECTION_ERROR_RETRY_LIMIT:\n                    logging.warning(\n                        f\"ConnectionError ({err}) encountered when polling job: \"\n                        f\"{response.name}. Retrying.\"\n                    )\n                else:\n                    error_util.exit_with_internal_error(\n                        f\"Request failed after {_CONNECTION_ERROR_RETRY_LIMIT} retries.\"\n                    )\n            if is_job_successful(job_state):\n                break\n            logging.info(\n                f\"Waiting for {_POLLING_INTERVAL_IN_SECONDS} seconds for next poll.\"\n            )\n            time.sleep(_POLLING_INTERVAL_IN_SECONDS)\n\n    # return GCP resource for Vertex AI UI integration\n    batch_job_resources = GcpResources()\n    dr = batch_job_resources.resources.add()\n    dr.resource_type = \"BatchPredictionJob\"\n    dr.resource_uri = response.name\n    gcp_resources = MessageToJson(batch_job_resources)\n\n    return (gcp_resources,)\n"
  },
  {
    "repo": "Saoussen-CH/production-ready-MLOps-on-GCP",
    "file_path": "components/src/components/upload_best_model_op.py",
    "raw_url": "https://raw.githubusercontent.com/Saoussen-CH/production-ready-MLOps-on-GCP/main/components/src/components/upload_best_model_op.py",
    "content": "from kfp.dsl import Dataset, Input, Metrics, Model, Output, component\nfrom google_cloud_pipeline_components.types.artifact_types import VertexModel\n\n\n@component(\n    base_image=\"python:3.10\",\n    packages_to_install=[\n        \"google-cloud-aiplatform==1.55.0\",\n        \"google-cloud-pipeline-components==2.14.1\",\n    ],\n)\ndef upload_best_model_op(\n    model: Input[Model],\n    test_data: Input[Dataset],\n    model_eval_metrics: Input[Metrics],\n    vertex_model: Output[VertexModel],\n    project: str,\n    location: str,\n    model_name: str,\n    eval_metric: str,\n    eval_lower_is_better: bool,\n    pipeline_job_id: str,\n    serving_container_image: str,\n    model_description: str = None,\n    evaluation_name: str = \"Imported evaluation\",\n) -> None:\n    \"\"\"\n    Args:\n        model (Model): Input challenger model.\n        test_data (Dataset): Test dataset used for evaluating challenger model.\n        vertex_model (VertexModel): Output model uploaded to Vertex AI Model Registry.\n        model_eval_metricsn (Metrics): Evaluation metrics of challenger model.\n        project (str): project id of the Google Cloud project.\n        location (str): location of the Google Cloud project.\n        pipeline_job_id (str):\n        model_name (str): Name of champion and challenger model in\n            Vertex AI Model Registry.\n        eval_metric (str): Metric name to compare champion and challenger on.\n        eval_lower_is_better (bool): Usually True for losses and\n            False for classification metrics.\n        serving_container_image (str): Container URI for serving the model.\n        model_description (str): Optional. Description of model.\n        evaluation_name (str): Optional. Name of evaluation results which are\n            displayed in the Vertex AI UI of the challenger model.\n    \"\"\"\n\n    import json\n    import logging\n    import google.cloud.aiplatform as aip\n    from google.protobuf.json_format import MessageToDict\n    from google.cloud.aiplatform_v1 import ModelEvaluation, ModelServiceClient\n    from google.protobuf.json_format import ParseDict\n\n    def lookup_model(model_name: str) -> aip.Model:\n        \"\"\"Look up model in model registry.\"\"\"\n        logging.info(f\"listing models with display name {model_name}\")\n        models = aip.Model.list(\n            filter=f'display_name=\"{model_name}\"',\n            location=location,\n            project=project,\n        )\n        logging.info(f\"found {len(models)} models\")\n\n        if len(models) == 0:\n            logging.info(\n                f\"No model found with name {model_name}\"\n                + f\"(project: {project} location: {location})\"\n            )\n            return None\n        elif len(models) == 1:\n            return models[0]\n        else:\n            raise RuntimeError(f\"Multiple models with name {model_name} were found.\")\n\n    def compare_models(\n        champion_metrics: dict,\n        challenger_metrics: dict,\n        eval_lower_is_better: bool,\n    ) -> bool:\n        \"\"\"Compare models by evaluating a primary metric.\"\"\"\n        logging.info(f\"Comparing {eval_metric} of models\")\n        logging.debug(f\"Champion metrics: {champion_metrics}\")\n        logging.debug(f\"Challenger metrics: {challenger_metrics}\")\n\n        m_champ = champion_metrics[eval_metric]\n        m_chall = challenger_metrics[eval_metric]\n        logging.info(f\"Champion={m_champ} Challenger={m_chall}\")\n\n        challenger_wins = (\n            (m_chall < m_champ) if eval_lower_is_better else (m_chall > m_champ)\n        )\n        logging.info(f\"{'Challenger' if challenger_wins else 'Champion'} wins!\")\n\n        return challenger_wins\n\n    def upload_model_to_registry(\n        is_default_version: bool, parent_model_uri: str = None\n    ) -> Model:\n        \"\"\"Upload model to registry.\"\"\"\n        logging.info(f\"Uploading model {model_name} (default: {is_default_version}\")\n        uploaded_model = aip.Model.upload(\n            display_name=model_name,\n            description=model_description,\n            artifact_uri=model.uri,\n            serving_container_image_uri=serving_container_image,\n            parent_model=parent_model_uri,\n            is_default_version=is_default_version,\n        )\n        logging.info(f\"Uploaded model {uploaded_model}\")\n\n        # Output google.VertexModel artifact\n        vertex_model.uri = (\n            f\"https://{location}-aiplatform.googleapis.com/v1/\"\n            f\"{uploaded_model.versioned_resource_name}\"\n        )\n        vertex_model.metadata[\"resourceName\"] = uploaded_model.versioned_resource_name\n\n        return uploaded_model\n\n    def import_evaluation(\n        parsed_metrics: dict,\n        challenger_model: aip.Model,\n        evaluation_name: str,\n    ) -> str:\n        \"\"\"Import model evaluation.\"\"\"\n        logging.info(f\"Evaluation metrics: {parsed_metrics}\")\n        problem_type = parsed_metrics.pop(\"problemType\")\n        schema = (\n            f\"gs://google-cloud-aiplatform/schema/modelevaluation/\"\n            f\"{problem_type}_metrics_1.0.0.yaml\"\n        )\n        evaluation = {\n            \"displayName\": evaluation_name,\n            \"metricsSchemaUri\": schema,\n            \"metrics\": parsed_metrics,\n            \"metadata\": {\n                \"pipeline_job_id\": pipeline_job_id,\n                \"evaluation_dataset_type\": \"gcs\",\n                \"evaluation_dataset_path\": [test_data.uri],\n            },\n        }\n\n        request = ParseDict(evaluation, ModelEvaluation()._pb)\n        logging.debug(f\"Request: {request}\")\n        challenger_name = challenger_model.versioned_resource_name\n        client = ModelServiceClient(\n            client_options={\"api_endpoint\": location + \"-aiplatform.googleapis.com\"}\n        )\n        logging.info(f\"Uploading model evaluation for {challenger_name}\")\n        response = client.import_model_evaluation(\n            parent=challenger_name,\n            model_evaluation=request,\n        )\n        logging.debug(f\"Response: {response}\")\n        return response.name\n\n    # Parse metrics to dict\n    with open(model_eval_metrics.path, \"r\") as f:\n        challenger_metrics = json.load(f)\n\n    champion_model = lookup_model(model_name=model_name)\n\n    challenger_wins = True\n    parent_model_uri = None\n    if champion_model is None:\n        logging.info(\"No champion model found, uploading new model.\")\n    else:\n        # Compare models\n        logging.info(\n            f\"Model default version {champion_model.version_id} \"\n            \"is being challenged by new model.\"\n        )\n        # Look up Vertex model evaluation for champion model\n        champion_eval = champion_model.get_model_evaluation()\n        champion_metrics = MessageToDict(champion_eval._gca_resource._pb)[\"metrics\"]\n\n        challenger_wins = compare_models(\n            champion_metrics=champion_metrics,\n            challenger_metrics=challenger_metrics,\n            eval_lower_is_better=eval_lower_is_better,\n        )\n        parent_model_uri = champion_model.resource_name\n\n    model = upload_model_to_registry(challenger_wins, parent_model_uri)\n\n    import_evaluation(\n        parsed_metrics=challenger_metrics,\n        challenger_model=model,\n        evaluation_name=evaluation_name,\n    )\n"
  },
  {
    "repo": "Saoussen-CH/production-ready-MLOps-on-GCP",
    "file_path": "pipelines/src/pipelines/prediction.py",
    "raw_url": "https://raw.githubusercontent.com/Saoussen-CH/production-ready-MLOps-on-GCP/main/pipelines/src/pipelines/prediction.py",
    "content": "from kfp import compiler, dsl\nfrom os import environ as env\nimport pathlib\n\nfrom components import (\n    lookup_model_op,\n    model_batch_predict_op,\n)\n\nfrom google_cloud_pipeline_components.v1.bigquery import BigqueryQueryJobOp\nfrom pipelines.utils.query import generate_query\n\n# set training-serving skew thresholds and emails to receive alerts:\nALERT_EMAILS = []\nNOTIFICATION_CHANNELS = []\nSKEW_THRESHOLDS = {\"defaultSkewThreshold\": {\"value\": 0.001}}\n# or set different thresholds per feature:\n# SKEW_THRESHOLDS = {\"skewThresholds\": {\"payment_type\": {\"value\": 0.001}}, ... }\n\n\n@dsl.pipeline(name=\"taxifare-batch-prediction-pipeline\")\ndef pipeline(\n    project: str = env.get(\"VERTEX_PROJECT_ID\"),\n    location: str = env.get(\"VERTEX_LOCATION\"),\n    bq_location: str = env.get(\"BQ_LOCATION\"),\n    bq_source_uri: str = \"bigquery-public-data.chicago_taxi_trips.taxi_trips\",\n    dataset: str = \"taxi_trips_dataset\",\n    timestamp: str = \"2022-12-01 00:00:00\",\n    use_latest_data: bool = True,  # Parameter to use the latest data or fixed timestamp\n    model_name: str = \"taxi-traffic-model\",\n    machine_type: str = \"n2-standard-4\",\n    min_replicas: int = 3,\n    max_replicas: int = 10,\n):\n    \"\"\"\n    Prediction pipeline which:\n     1. Looks up the default model version (champion).\n     2. Runs a batch prediction job with BigQuery as input and output\n     3. Optionally monitors training-serving skew\n\n    Args:\n        project (str): project id of the Google Cloud project\n        location (str): location of the Google Cloud project\n        bq_location (str): location of dataset in BigQuery\n        bq_source_uri (str): `<project>.<dataset>.<table>` of ingestion data in BigQuery\n        model_name (str): name of model\n        dataset (str): dataset id to store staging data & predictions in BigQuery\n        timestamp (str): Optional. Empty or a specific timestamp in ISO 8601 format\n            (YYYY-MM-DDThh:mm:ss.sss\u00b1hh:mm or YYYY-MM-DDThh:mm:ss).\n            If any time part is missing, it will be regarded as zero\n        use_latest_data (bool): Whether to use the latest available data\n        machine_type (str): Machine type to be used for Vertex Batch\n            Prediction. Example machine_types - n1-standard-4, n1-standard-16 etc.\n        min_replicas (int): Minimum no of machines to distribute the\n            Vertex Batch Prediction job for horizontal scalability\n        max_replicas (int): Maximum no of machines to distribute the\n            Vertex Batch Prediction job for horizontal scalability\n    \"\"\"\n\n    table = \"prep_prediction_table\"\n\n    queries_folder = pathlib.Path(__file__).parent / \"queries\"\n\n    prep_query = generate_query(\n        input_file=queries_folder / \"ingest_pred.sql\",\n        source=bq_source_uri,\n        location=bq_location,\n        dataset=f\"{project}.{dataset}\",\n        table_=table,\n        label=\"total_fare\",  # Assuming the label is 'total_fare'\n        start_timestamp=timestamp,\n        use_latest_data=use_latest_data,\n    )\n\n    prep_op = BigqueryQueryJobOp(\n        project=project,\n        location=\"US\",\n        query=prep_query,\n    ).set_display_name(\"Ingest & preprocess data\")\n\n    # lookup champion model\n    champion_model = lookup_model_op(\n        model_name=model_name,\n        location=location,\n        project=project,\n        fail_on_model_not_found=True,\n    ).set_display_name(\"Look up champion model\")\n\n    # batch predict from BigQuery to BigQuery\n    (\n        model_batch_predict_op(\n            model=champion_model.outputs[\"model\"],\n            job_display_name=\"taxi-fare-predict-job\",\n            location=location,\n            project=project,\n            source_uri=f\"bq://{project}.{dataset}.{table}\",\n            destination_uri=f\"bq://{project}.{dataset}\",\n            source_format=\"bigquery\",\n            destination_format=\"bigquery\",\n            instance_config={\n                \"instanceType\": \"object\",\n            },\n            machine_type=machine_type,\n            starting_replica_count=min_replicas,\n            max_replica_count=max_replicas,\n            monitoring_training_dataset=champion_model.outputs[\"training_dataset\"],\n            monitoring_alert_email_addresses=ALERT_EMAILS,\n            notification_channels=NOTIFICATION_CHANNELS,\n            monitoring_skew_config=SKEW_THRESHOLDS,\n        )\n        .after(prep_op)\n        .set_display_name(\"Run prediction job\")\n    )\n\n\nif __name__ == \"__main__\":\n    compiler.Compiler().compile(\n        pipeline_func=pipeline, package_path=\"taxifare-prediction-pipeline.yaml\"\n    )\n"
  },
  {
    "repo": "Saoussen-CH/production-ready-MLOps-on-GCP",
    "file_path": "pipelines/src/pipelines/training.py",
    "raw_url": "https://raw.githubusercontent.com/Saoussen-CH/production-ready-MLOps-on-GCP/main/pipelines/src/pipelines/training.py",
    "content": "import pathlib\nimport logging\n\nfrom components import (\n    extract_table_to_gcs_op,\n    get_custom_job_results_op,\n    get_training_args_dict_op,\n    get_workerpool_spec_op,\n    upload_best_model_op,\n    get_hyperparameter_tuning_results_op,\n)\n\nfrom os import environ as env\n\nfrom google_cloud_pipeline_components.v1.bigquery import BigqueryQueryJobOp\nfrom google_cloud_pipeline_components.v1.custom_job import CustomTrainingJobOp\nfrom google_cloud_pipeline_components.v1.hyperparameter_tuning_job import (\n    HyperparameterTuningJobRunOp,\n)\nfrom google_cloud_pipeline_components.v1 import hyperparameter_tuning_job\nfrom google.cloud.aiplatform import hyperparameter_tuning as hpt\n\nfrom kfp import compiler, dsl\n\nfrom pipelines.utils.query import generate_query\n\nbq_source_uri = \"bigquery-public-data.chicago_taxi_trips.taxi_trips\"\ndataset = \"prerocessing\"\ntable = \"taxi_fare\"\nlabel = \"total_fare\"\ntimestamp = \"2022-12-01 00:00:00\"\n\n\n# define the metric spec for hyperparameter tuning\n# for details:\n# https://cloud.google.com/vertex-ai/docs/reference/rest/v1/StudySpec#MetricSpec\nMETRIC_SPEC = dict(val_root_mean_squared_error=\"minimize\")\n\n# define the parameter specs for tuning\n# for details:\n# https://cloud.google.com/vertex-ai/docs/reference/rest/v1/StudySpec#ParameterSpec\nPARAMETER_SPEC = {\n    \"learning-rate\": hpt.DoubleParameterSpec(min=0.0001, max=1, scale=\"log\"),\n    \"batch-size\": hpt.DiscreteParameterSpec(values=[128, 256, 512], scale=\"linear\"),\n}\n\n\nPREDICTION_IMAGE = \"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-11:latest\"\n\n\n@dsl.pipeline(name=\"taxifare-training-pipeline\")\ndef pipeline(\n    project: str = env.get(\"VERTEX_PROJECT_ID\"),\n    location: str = env.get(\"VERTEX_LOCATION\"),\n    bq_location: str = env.get(\"BQ_LOCATION\"),\n    bq_source_uri: str = \"bigquery-public-data.chicago_taxi_trips.taxi_trips\",\n    dataset: str = \"taxi_trips_dataset\",\n    timestamp: str = \"2022-12-01 00:00:00\",  # Optional timestamp parameter\n    use_latest_data: bool = True,  # Parameter to use the latest data or fixed timestamp\n    base_output_dir: str = \"\",\n    training_job_display_name: str = \"\",\n    model_name: str = \"taxi-traffic-model\",\n):\n    \"\"\"\n    Training pipeline which:\n     1. Preprocesses data in BigQuery\n     2. Extracts data to Cloud Storage\n     3. Trains a model using a custom prebuilt container\n     4. Uploads the model to Model Registry\n     5. Evaluates the model against a champion model\n     6. Selects a new champion based on the primary metrics\n\n    Args:\n        project (str): project id of the Google Cloud project\n        location (str): location of the Google Cloud project\n        bq_location (str): location of dataset in BigQuery\n        bq_source_uri (str): `<project>.<dataset>.<table>` of ingestion data in BigQuery\n        model_name (str): name of model\n        dataset (str): dataset id to store staging data & predictions in BigQuery\n        timestamp (str): Optional. Empty or a specific timestamp in ISO 8601 format\n            (YYYY-MM-DDThh:mm:ss.sss\u00b1hh:mm or YYYY-MM-DDThh:mm:ss).\n            If any time part is missing, it will be regarded as zero.\n        use_latest_data (bool): Whether to use the latest available data\n        base_output_dir (str): base output directory for the training job\n        training_job_display_name (str): display name for the training job\n        model_name (str): name of the model\n    \"\"\"\n    PRIMARY_METRIC = \"rootMeanSquaredError\"\n    queries_folder = pathlib.Path(__file__).parent / \"queries\"\n\n    preprocessed_table = \"preprocessed_data\"\n\n    training_image = env.get(\"TRAINING_IMAGE\")\n\n    logging.info(f\"Training image URI: {training_image}\")\n    # define the workerpool spec for the custom jobs\n    # (https://cloud.google.com/vertex-ai/docs/reference/rest/v1/CustomJobSpec)\n    WORKER_POOL_SPECS = [\n        dict(\n            machine_spec=dict(\n                machine_type=\"n1-standard-4\",\n            ),\n            replica_count=1,\n            container_spec=dict(\n                image_uri=training_image,\n            ),\n        )\n    ]\n\n    # Generate the preprocessing query\n    prep_query = generate_query(\n        input_file=queries_folder / \"ingest.sql\",\n        source=bq_source_uri,\n        location=bq_location,\n        dataset=f\"{project}.{dataset}\",\n        table_=preprocessed_table,\n        label=label,\n        start_timestamp=timestamp,\n        use_latest_data=use_latest_data,\n    )\n\n    prep_op = BigqueryQueryJobOp(\n        project=project,\n        location=\"US\",\n        query=prep_query,\n    ).set_display_name(\"Ingest & preprocess data\")\n\n    split_train_query = generate_query(\n        input_file=queries_folder / \"repeatable_splitting.sql\",\n        source_dataset=f\"{project}.{dataset}\",\n        source_table=preprocessed_table,\n        num_lots=10,\n        lots=tuple(range(8)),\n    )\n\n    split_valid_query = generate_query(\n        input_file=queries_folder / \"repeatable_splitting.sql\",\n        source_dataset=f\"{project}.{dataset}\",\n        source_table=preprocessed_table,\n        num_lots=10,\n        lots=\"(8)\",\n    )\n\n    split_test_query = generate_query(\n        input_file=queries_folder / \"repeatable_splitting.sql\",\n        source_dataset=f\"{project}.{dataset}\",\n        source_table=preprocessed_table,\n        num_lots=10,\n        lots=\"(9)\",\n    )\n\n    split_train_data = (\n        BigqueryQueryJobOp(\n            project=project,\n            location=bq_location,\n            query=split_train_query,\n        )\n        .after(prep_op)\n        .set_display_name(\"Split train data\")\n    )\n\n    train_dataset = (\n        extract_table_to_gcs_op(bq_table=split_train_data.outputs[\"destination_table\"])\n        .after(split_train_data)\n        .set_display_name(\"Extract training data from BigQuery to GCS\")\n    )\n\n    split_valid_data = (\n        BigqueryQueryJobOp(\n            project=project,\n            location=bq_location,\n            query=split_valid_query,\n        )\n        .after(prep_op)\n        .set_display_name(\"Split valid data\")\n    )\n\n    valid_dataset = (\n        extract_table_to_gcs_op(bq_table=split_valid_data.outputs[\"destination_table\"])\n        .after(split_valid_data)\n        .set_display_name(\"Extract validation data from BigQuery to GCS\")\n    )\n\n    split_test_data = (\n        BigqueryQueryJobOp(\n            project=project,\n            location=bq_location,\n            query=split_test_query,\n        )\n        .after(prep_op)\n        .set_display_name(\"Split test data\")\n    )\n\n    test_dataset = (\n        extract_table_to_gcs_op(bq_table=split_test_data.outputs[\"destination_table\"])\n        .after(split_test_data)\n        .set_display_name(\"Extract test data from BigQuery to GCS\")\n    )\n\n    # define training args\n    args = dict(\n        train_data=train_dataset.outputs[\"dataset\"],\n        valid_data=valid_dataset.outputs[\"dataset\"],\n        test_data=test_dataset.outputs[\"dataset\"],\n        hypertune=True,\n    )\n\n    hypertune_args_step = get_training_args_dict_op(**args).set_display_name(\n        \"Get-Hypertune-Args\"\n    )\n\n    # create the workerpool spec for hyperparameter tuning\n    # dont provide hyperparams, because they are defined in the PARAMETER_SPEC\n    # and directly passed to the hyperparameter tuning job\n    hypertune_worker_pool_specs_step = get_workerpool_spec_op(\n        worker_pool_specs=WORKER_POOL_SPECS,\n        args=hypertune_args_step.output,\n    ).set_display_name(\"Get-Hypertune-Worker-Pool-Spec\")\n\n    # create the actual hyperparameter tuning job\n    # here you can choose how many trials to do and how many to run in parallel\n    hypertune_step = HyperparameterTuningJobRunOp(\n        display_name=\"hypertune-job\",\n        project=project,\n        location=location,\n        worker_pool_specs=hypertune_worker_pool_specs_step.output,\n        study_spec_metrics=hyperparameter_tuning_job.utils.serialize_metrics(\n            METRIC_SPEC\n        ),\n        study_spec_parameters=hyperparameter_tuning_job.utils.serialize_parameters(\n            PARAMETER_SPEC\n        ),\n        max_trial_count=6,\n        parallel_trial_count=2,\n        base_output_directory=f\"{base_output_dir}/hypertune-job\",\n    ).set_display_name(\"Hypertune-Job\")\n\n    # now we can extract the results of the hyperparameter tuning job\n    hypertune_results_step = get_hyperparameter_tuning_results_op(\n        project=project,\n        location=location,\n        job_resource=hypertune_step.output,\n        study_spec_metrics=hyperparameter_tuning_job.utils.serialize_metrics(\n            METRIC_SPEC\n        ),\n    ).set_display_name(\"Get-Hypertune-Results\")\n\n    # update our args dict for training\n    args.update(dict(hypertune=False))\n\n    # create the args dict\n    training_args_step = get_training_args_dict_op(**args).set_display_name(\n        \"Get-Training-Args\"\n    )\n\n    # create the workerpool spec for training\n    training_worker_pool_specs_step = get_workerpool_spec_op(\n        worker_pool_specs=WORKER_POOL_SPECS,\n        hyperparams=hypertune_results_step.output,\n        args=training_args_step.output,\n    ).set_display_name(\"Get-Training-Worker-Pool-Spec\")\n\n    # Train the model\n    custom_job_task = CustomTrainingJobOp(\n        project=project,\n        display_name=training_job_display_name,\n        worker_pool_specs=training_worker_pool_specs_step.output,\n        base_output_directory=f\"{base_output_dir}/training-job\",\n        location=location,\n    )\n\n    # now we can extract the training results\n    training_results_step = get_custom_job_results_op(\n        project=project, location=location, job_resource=custom_job_task.output\n    ).set_display_name(\"Get-Training-Results\")\n\n    upload_best_model_op(\n        project=project,\n        location=location,\n        model=training_results_step.outputs[\"model\"],\n        model_eval_metrics=training_results_step.outputs[\"metrics\"],\n        test_data=test_dataset.outputs[\"dataset\"],\n        eval_metric=PRIMARY_METRIC,\n        eval_lower_is_better=True,\n        serving_container_image=PREDICTION_IMAGE,\n        model_name=model_name,\n        model_description=\"Predict price of a taxi trip.\",\n        pipeline_job_id=\"{{$.pipeline_job_name}}\",\n    ).set_display_name(\"Upload model\")\n\n\nif __name__ == \"__main__\":\n    compiler.Compiler().compile(\n        pipeline_func=pipeline, package_path=\"taxifare-training-pipeline.yaml\"\n    )\n"
  },
  {
    "repo": "NashTech-Labs/Databricks-Dolly-LLM-Fine-Tuning",
    "file_path": "pipeline.py",
    "raw_url": "https://raw.githubusercontent.com/NashTech-Labs/Databricks-Dolly-LLM-Fine-Tuning/dolly-fine-tuning-and-serving/pipeline.py",
    "content": "import logging\nfrom kfp.v2 import compiler\nimport kfp\n\nfrom components.process_data import process_data\nfrom components.serve_model import serve_model_component\nfrom components.train_model import fine_tune_model\nfrom components.upload_model import upload_container\nfrom constants import (PIPELINE_DESCRIPTION, PIPELINE_NAME, PIPELINE_ROOT_GCS, ORIGINAL_MODEL_NAME, \\\n                       SAVE_MODEL_BUCKET_NAME, REGION, DATASET_BUCKET, MODEL_DISPLAY_NAME, SERVING_IMAGE, \\\n                       STAGING_BUCKET, COMPONENT_EXECUTION, DATASET_NAME, SERVING_IMAGE_TRIGGER, SERVICE_ACCOUNT_ML,\n                       DEPLOYED_MODEL_DETAILS_FILE,\n                       PIPELINE_JSON_FILE)\n\nlogging.basicConfig(level=logging.DEBUG)\nlogger = logging.getLogger(__name__)\n\n\n@kfp.dsl.pipeline(name=PIPELINE_NAME,\n                  description=PIPELINE_DESCRIPTION,\n                  pipeline_root=PIPELINE_ROOT_GCS)\ndef pipeline(\n        project_id: str,\n        job_id: str\n):\n    \"\"\"Dataset Processing\"\"\"\n    process_data_task = process_data(DATASET_BUCKET, DATASET_NAME).set_display_name(\"Data_Processing\")\n\n    \"\"\"Fine Tune Model Pipeline\"\"\"\n    train_model_task = fine_tune_model(process_data_task.outputs[\"dataset\"],\n                                       ORIGINAL_MODEL_NAME,\n                                       SAVE_MODEL_BUCKET_NAME,\n                                       COMPONENT_EXECUTION) \\\n        .after(process_data_task) \\\n        .set_display_name(\"Dolly Fine Tuning\") \\\n        .set_cpu_request(\"8\") \\\n        .set_memory_limit(\"32G\")\n\n    \"\"\"Upload model package\"\"\"\n    upload_model_task = upload_container(project_id=project_id,\n                                         trigger_id=SERVING_IMAGE_TRIGGER,\n                                         component_execution=COMPONENT_EXECUTION) \\\n        .after(train_model_task) \\\n        .set_display_name(\"Model_Upload\")\n\n    \"\"\"Serve Model To Endpoint\"\"\"\n    serve_model_component(project_id,\n                          REGION,\n                          STAGING_BUCKET,\n                          SERVING_IMAGE,\n                          MODEL_DISPLAY_NAME,\n                          COMPONENT_EXECUTION,\n                          SERVICE_ACCOUNT_ML,\n                          save_model_details_bucket=DATASET_BUCKET,\n                          model_details_file_name=DEPLOYED_MODEL_DETAILS_FILE) \\\n        .after(upload_model_task) \\\n        .set_display_name(\"Serve_Model\") \\\n        .set_cpu_request(\"8\") \\\n        .set_memory_limit(\"32G\")\n\n\ndef compile_pipeline(pipeline_template_name=f'{PIPELINE_JSON_FILE}'):\n    compiler.Compiler().compile(\n        pipeline_func=pipeline,\n        package_path=pipeline_template_name\n    )\n    return None\n\n\nif __name__ == \"__main__\":\n    compile_pipeline()\n"
  }
]