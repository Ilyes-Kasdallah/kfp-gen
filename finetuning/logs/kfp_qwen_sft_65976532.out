Node Python: 3.11.5
Torch CUDA: 12.2 Devices: 1
bnb libs: ['libbitsandbytes_cuda122.so', 'libbitsandbytes_cuda122_nocublaslt.so']
bitsandbytes: 0.43.3 OK
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.18.7
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
/home/ilyes/ENV311/lib/python3.11/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
/home/ilyes/ENV311/lib/python3.11/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
  0%|          | 0/42 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  2%|â–         | 1/42 [00:25<17:29, 25.60s/it]  5%|â–         | 2/42 [00:36<11:14, 16.85s/it]  7%|â–‹         | 3/42 [00:44<08:23, 12.92s/it] 10%|â–‰         | 4/42 [00:55<07:42, 12.17s/it] 12%|â–ˆâ–        | 5/42 [01:06<07:10, 11.63s/it] 14%|â–ˆâ–        | 6/42 [01:13<06:05, 10.15s/it] 17%|â–ˆâ–‹        | 7/42 [01:24<06:05, 10.44s/it] 19%|â–ˆâ–‰        | 8/42 [01:35<06:02, 10.66s/it] 21%|â–ˆâ–ˆâ–       | 9/42 [01:44<05:33, 10.10s/it] 24%|â–ˆâ–ˆâ–       | 10/42 [01:55<05:32, 10.39s/it] 26%|â–ˆâ–ˆâ–Œ       | 11/42 [02:06<05:26, 10.54s/it] 29%|â–ˆâ–ˆâ–Š       | 12/42 [02:14<04:49,  9.66s/it] 31%|â–ˆâ–ˆâ–ˆ       | 13/42 [02:25<04:51, 10.07s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 14/42 [02:34<04:39,  9.98s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 15/42 [02:41<03:57,  8.80s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 16/42 [02:52<04:08,  9.56s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 17/42 [03:03<04:08,  9.93s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 18/42 [03:11<03:44,  9.36s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 19/42 [03:22<03:47,  9.90s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 20/42 [03:31<03:33,  9.70s/it]                                                48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 20/42 [03:31<03:33,  9.70s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 21/42 [03:38<03:05,  8.82s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 22/42 [03:49<03:10,  9.52s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 23/42 [04:00<03:09,  9.95s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 24/42 [04:08<02:51,  9.51s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 25/42 [04:19<02:49,  9.97s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 26/42 [04:31<02:44, 10.30s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 27/42 [04:40<02:30, 10.03s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 28/42 [04:51<02:24, 10.34s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 29/42 [05:01<02:14, 10.38s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 30/42 [05:08<01:49,  9.10s/it]{'loss': 8.0652, 'grad_norm': 2.717799663543701, 'learning_rate': 5.782172325201155e-05, 'epoch': 1.34}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 31/42 [05:19<01:47,  9.73s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 32/42 [05:30<01:40, 10.08s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 33/42 [05:38<01:25,  9.55s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 34/42 [05:49<01:20, 10.00s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 35/42 [06:00<01:11, 10.24s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 36/42 [06:08<00:57,  9.53s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 37/42 [06:19<00:49,  9.99s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 38/42 [06:30<00:41, 10.31s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 39/42 [06:38<00:29,  9.71s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 40/42 [06:49<00:20, 10.12s/it]                                                95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 40/42 [06:49<00:20, 10.12s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 41/42 [06:59<00:10, 10.17s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 42/42 [07:07<00:00,  9.29s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 42/42 [07:08<00:00,  9.29s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 42/42 [07:08<00:00, 10.20s/it]
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         train/epoch â–â–‡â–ˆ
wandb:   train/global_step â–â–‡â–ˆ
wandb:     train/grad_norm â–ˆâ–
wandb: train/learning_rate â–ˆâ–
wandb:          train/loss â–ˆâ–
wandb: 
wandb: Run summary:
wandb:               total_flos 1.1996704760850432e+16
wandb:              train/epoch 2.81356
wandb:        train/global_step 42
wandb:          train/grad_norm 2.29103
wandb:      train/learning_rate 0.0
wandb:               train/loss 6.5791
wandb:               train_loss 7.29612
wandb:            train_runtime 428.4696
wandb: train_samples_per_second 0.826
wandb:   train_steps_per_second 0.098
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync run/kfp-Qwen2.5-7B-Instruct/wandb/offline-run-20250816_153804-21vcmlot
wandb: Find logs at: run/kfp-Qwen2.5-7B-Instruct/wandb/offline-run-20250816_153804-21vcmlot/logs
{'loss': 6.5791, 'grad_norm': 2.2910313606262207, 'learning_rate': 6.15582970243117e-07, 'epoch': 2.68}
{'train_runtime': 428.4696, 'train_samples_per_second': 0.826, 'train_steps_per_second': 0.098, 'train_loss': 7.296119190397716, 'epoch': 2.81}
