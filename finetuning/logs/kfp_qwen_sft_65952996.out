Node Python: 3.11.5
Torch CUDA: 12.2 Devices: 1
bnb libs: ['libbitsandbytes_cuda122.so', 'libbitsandbytes_cuda122_nocublaslt.so']
bitsandbytes: 0.43.3 OK
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: Tracking run with wandb version 0.18.7
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
/home/ilyes/ENV311/lib/python3.11/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/home/ilyes/ENV311/lib/python3.11/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
  0%|          | 0/42 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  2%|▏         | 1/42 [00:25<17:23, 25.45s/it]  5%|▍         | 2/42 [00:36<11:13, 16.84s/it]  7%|▋         | 3/42 [00:44<08:25, 12.96s/it] 10%|▉         | 4/42 [00:55<07:44, 12.22s/it] 12%|█▏        | 5/42 [01:06<07:12, 11.68s/it] 14%|█▍        | 6/42 [01:13<06:07, 10.22s/it] 17%|█▋        | 7/42 [01:24<06:07, 10.51s/it] 19%|█▉        | 8/42 [01:36<06:04, 10.73s/it] 21%|██▏       | 9/42 [01:45<05:35, 10.17s/it] 24%|██▍       | 10/42 [01:56<05:35, 10.47s/it] 26%|██▌       | 11/42 [02:07<05:29, 10.62s/it] 29%|██▊       | 12/42 [02:14<04:51,  9.73s/it] 31%|███       | 13/42 [02:25<04:53, 10.13s/it] 33%|███▎      | 14/42 [02:35<04:41, 10.05s/it] 36%|███▌      | 15/42 [02:41<03:59,  8.86s/it] 38%|███▊      | 16/42 [02:53<04:10,  9.63s/it] 40%|████      | 17/42 [03:04<04:09,  9.98s/it] 43%|████▎     | 18/42 [03:12<03:46,  9.43s/it] 45%|████▌     | 19/42 [03:23<03:49,  9.97s/it] 48%|████▊     | 20/42 [03:32<03:34,  9.76s/it]                                                48%|████▊     | 20/42 [03:32<03:34,  9.76s/it] 50%|█████     | 21/42 [03:39<03:06,  8.88s/it] 52%|█████▏    | 22/42 [03:50<03:11,  9.59s/it] 55%|█████▍    | 23/42 [04:01<03:10, 10.03s/it] 57%|█████▋    | 24/42 [04:10<02:52,  9.57s/it] 60%|█████▉    | 25/42 [04:21<02:50, 10.04s/it] 62%|██████▏   | 26/42 [04:32<02:45, 10.36s/it] 64%|██████▍   | 27/42 [04:42<02:31, 10.10s/it] 67%|██████▋   | 28/42 [04:53<02:25, 10.41s/it] 69%|██████▉   | 29/42 [05:03<02:15, 10.44s/it] 71%|███████▏  | 30/42 [05:09<01:49,  9.15s/it]{'loss': 8.065, 'grad_norm': 2.718278169631958, 'learning_rate': 5.782172325201155e-05, 'epoch': 1.34}
 74%|███████▍  | 31/42 [05:21<01:47,  9.82s/it] 76%|███████▌  | 32/42 [05:32<01:41, 10.16s/it] 79%|███████▊  | 33/42 [05:40<01:26,  9.63s/it] 81%|████████  | 34/42 [05:51<01:20, 10.08s/it] 83%|████████▎ | 35/42 [06:02<01:12, 10.33s/it] 86%|████████▌ | 36/42 [06:10<00:57,  9.60s/it] 88%|████████▊ | 37/42 [06:21<00:50, 10.06s/it] 90%|█████████ | 38/42 [06:32<00:41, 10.37s/it] 93%|█████████▎| 39/42 [06:41<00:29,  9.78s/it] 95%|█████████▌| 40/42 [06:52<00:20, 10.18s/it]                                                95%|█████████▌| 40/42 [06:52<00:20, 10.18s/it] 98%|█████████▊| 41/42 [07:02<00:10, 10.22s/it]100%|██████████| 42/42 [07:09<00:00,  9.35s/it]                                               100%|██████████| 42/42 [07:11<00:00,  9.35s/it]100%|██████████| 42/42 [07:11<00:00, 10.26s/it]
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         train/epoch ▁▇█
wandb:   train/global_step ▁▇█
wandb:     train/grad_norm █▁
wandb: train/learning_rate █▁
wandb:          train/loss █▁
wandb: 
wandb: Run summary:
wandb:               total_flos 1.1996704760850432e+16
wandb:              train/epoch 2.81356
wandb:        train/global_step 42
wandb:          train/grad_norm 2.29221
wandb:      train/learning_rate 0.0
wandb:               train/loss 6.579
wandb:               train_loss 7.29597
wandb:            train_runtime 431.0225
wandb: train_samples_per_second 0.821
wandb:   train_steps_per_second 0.097
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync run/kfp-Qwen2.5-7B-Instruct/wandb/offline-run-20250815_210317-os0t5h54
wandb: Find logs at: run/kfp-Qwen2.5-7B-Instruct/wandb/offline-run-20250815_210317-os0t5h54/logs
{'loss': 6.579, 'grad_norm': 2.2922096252441406, 'learning_rate': 6.15582970243117e-07, 'epoch': 2.68}
{'train_runtime': 431.0225, 'train_samples_per_second': 0.821, 'train_steps_per_second': 0.097, 'train_loss': 7.295967533474877, 'epoch': 2.81}
