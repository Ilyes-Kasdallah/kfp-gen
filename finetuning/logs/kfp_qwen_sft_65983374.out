Node Python: 3.11.5
Torch CUDA: 12.2 Devices: 1
bnb libs: ['libbitsandbytes_cuda122.so', 'libbitsandbytes_cuda122_nocublaslt.so']
bitsandbytes: 0.43.3 OK
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: Tracking run with wandb version 0.18.7
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
/home/ilyes/ENV311/lib/python3.11/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
/home/ilyes/ENV311/lib/python3.11/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
  0%|          | 0/56 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  2%|â–         | 1/56 [00:30<27:49, 30.35s/it]  4%|â–Ž         | 2/56 [00:42<17:32, 19.50s/it]  5%|â–Œ         | 3/56 [00:50<12:40, 14.36s/it]  7%|â–‹         | 4/56 [01:09<13:57, 16.10s/it]  9%|â–‰         | 5/56 [01:22<12:49, 15.08s/it] 11%|â–ˆ         | 6/56 [01:29<10:21, 12.43s/it] 12%|â–ˆâ–Ž        | 7/56 [01:48<11:41, 14.32s/it] 14%|â–ˆâ–        | 8/56 [02:01<11:12, 14.02s/it] 16%|â–ˆâ–Œ        | 9/56 [02:10<09:43, 12.42s/it] 18%|â–ˆâ–Š        | 10/56 [02:29<11:02, 14.40s/it] 20%|â–ˆâ–‰        | 11/56 [02:41<10:21, 13.82s/it] 21%|â–ˆâ–ˆâ–       | 12/56 [02:49<08:45, 11.95s/it] 23%|â–ˆâ–ˆâ–Ž       | 13/56 [03:02<08:52, 12.40s/it] 25%|â–ˆâ–ˆâ–Œ       | 14/56 [03:12<08:07, 11.60s/it] 27%|â–ˆâ–ˆâ–‹       | 15/56 [03:18<06:47,  9.94s/it] 29%|â–ˆâ–ˆâ–Š       | 16/56 [03:35<07:56, 11.91s/it] 30%|â–ˆâ–ˆâ–ˆ       | 17/56 [03:46<07:35, 11.69s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 18/56 [03:54<06:42, 10.60s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 19/56 [04:11<07:44, 12.55s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 20/56 [04:20<06:59, 11.66s/it]                                                36%|â–ˆâ–ˆâ–ˆâ–Œ      | 20/56 [04:20<06:59, 11.66s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 21/56 [04:27<05:56, 10.19s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 22/56 [04:46<07:15, 12.80s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 23/56 [05:00<07:17, 13.26s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 24/56 [05:09<06:18, 11.82s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/56 [05:28<07:12, 13.94s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 26/56 [05:43<07:05, 14.17s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 27/56 [05:52<06:09, 12.75s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 28/56 [06:09<06:28, 13.89s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/56 [06:19<05:47, 12.87s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 30/56 [06:25<04:41, 10.83s/it]{'loss': 7.4973, 'grad_norm': 2.672445297241211, 'learning_rate': 7.500000000000001e-05, 'epoch': 1.34}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 31/56 [06:44<05:32, 13.29s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 32/56 [06:58<05:19, 13.33s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 33/56 [07:06<04:32, 11.83s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 34/56 [07:23<04:55, 13.41s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 35/56 [07:34<04:26, 12.70s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 36/56 [07:42<03:45, 11.26s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 37/56 [08:00<04:13, 13.36s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 38/56 [08:12<03:54, 13.02s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 39/56 [08:21<03:17, 11.63s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 40/56 [08:38<03:32, 13.30s/it]                                                71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 40/56 [08:38<03:32, 13.30s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 41/56 [08:49<03:07, 12.53s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 42/56 [08:56<02:33, 10.96s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 43/56 [09:14<02:51, 13.17s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 44/56 [09:26<02:33, 12.81s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 45/56 [09:32<01:58, 10.75s/it]{'loss': 5.8899, 'grad_norm': 1.4950708150863647, 'learning_rate': 2.0142070414860704e-05, 'epoch': 2.68}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 46/56 [09:51<02:12, 13.26s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 47/56 [10:08<02:08, 14.29s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 48/56 [10:18<01:43, 12.88s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 49/56 [10:35<01:38, 14.12s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 50/56 [10:46<01:19, 13.18s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 51/56 [10:53<00:57, 11.47s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 52/56 [11:10<00:52, 13.07s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 53/56 [11:20<00:36, 12.09s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 54/56 [11:27<00:21, 10.66s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 55/56 [11:46<00:13, 13.03s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 56/56 [11:57<00:00, 12.69s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 56/56 [11:59<00:00, 12.69s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 56/56 [11:59<00:00, 12.85s/it]
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         train/epoch â–â–…â–ˆ
wandb:   train/global_step â–â–…â–ˆ
wandb:     train/grad_norm â–ˆâ–
wandb: train/learning_rate â–ˆâ–
wandb:          train/loss â–ˆâ–
wandb: 
wandb: Run summary:
wandb:               total_flos 2.277373446873907e+16
wandb:              train/epoch 3.74576
wandb:        train/global_step 56
wandb:          train/grad_norm 1.49507
wandb:      train/learning_rate 2e-05
wandb:               train/loss 5.8899
wandb:               train_loss 6.39549
wandb:            train_runtime 719.3841
wandb: train_samples_per_second 0.656
wandb:   train_steps_per_second 0.078
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync run/kfp-Qwen2.5-7B-Instruct/wandb/offline-run-20250816_225446-dd5w3f2g
wandb: Find logs at: run/kfp-Qwen2.5-7B-Instruct/wandb/offline-run-20250816_225446-dd5w3f2g/logs
{'train_runtime': 719.3841, 'train_samples_per_second': 0.656, 'train_steps_per_second': 0.078, 'train_loss': 6.395490646362305, 'epoch': 3.75}
