Node Python: 3.11.5
Torch CUDA: 12.2 Devices: 1
bnb libs: ['libbitsandbytes_cuda122.so', 'libbitsandbytes_cuda122_nocublaslt.so']
bitsandbytes: 0.43.3 OK
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: Tracking run with wandb version 0.18.7
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
/home/ilyes/ENV311/lib/python3.11/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/home/ilyes/ENV311/lib/python3.11/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
  0%|          | 0/56 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  2%|▏         | 1/56 [00:30<28:06, 30.67s/it]  4%|▎         | 2/56 [00:42<17:35, 19.56s/it]  5%|▌         | 3/56 [00:50<12:41, 14.37s/it]  7%|▋         | 4/56 [01:05<12:36, 14.55s/it]  9%|▉         | 5/56 [01:17<11:38, 13.69s/it] 11%|█         | 6/56 [01:24<09:35, 11.51s/it] 12%|█▎        | 7/56 [01:39<10:16, 12.59s/it] 14%|█▍        | 8/56 [01:52<10:13, 12.79s/it] 16%|█▌        | 9/56 [02:01<09:02, 11.55s/it] 18%|█▊        | 10/56 [02:16<09:37, 12.56s/it] 20%|█▉        | 11/56 [02:28<09:21, 12.47s/it] 21%|██▏       | 12/56 [02:36<08:03, 10.99s/it] 23%|██▎       | 13/56 [02:48<08:09, 11.39s/it] 25%|██▌       | 14/56 [02:58<07:36, 10.88s/it] 27%|██▋       | 15/56 [03:04<06:26,  9.42s/it] 29%|██▊       | 16/56 [03:18<07:15, 10.88s/it] 30%|███       | 17/56 [03:29<07:06, 10.93s/it] 32%|███▏      | 18/56 [03:37<06:22, 10.06s/it] 34%|███▍      | 19/56 [03:52<07:03, 11.44s/it] 36%|███▌      | 20/56 [04:01<06:30, 10.86s/it]                                                36%|███▌      | 20/56 [04:01<06:30, 10.86s/it] 38%|███▊      | 21/56 [04:08<05:36,  9.62s/it] 39%|███▉      | 22/56 [04:23<06:20, 11.20s/it] 41%|████      | 23/56 [04:36<06:29, 11.81s/it] 43%|████▎     | 24/56 [04:45<05:45, 10.79s/it] 45%|████▍     | 25/56 [05:00<06:12, 12.01s/it] 46%|████▋     | 26/56 [05:13<06:14, 12.48s/it] 48%|████▊     | 27/56 [05:23<05:34, 11.54s/it] 50%|█████     | 28/56 [05:37<05:44, 12.29s/it] 52%|█████▏    | 29/56 [05:47<05:16, 11.71s/it] 54%|█████▎    | 30/56 [05:53<04:20, 10.01s/it]{'loss': 7.643, 'grad_norm': 2.7942757606506348, 'learning_rate': 7.500000000000001e-05, 'epoch': 1.34}
 55%|█████▌    | 31/56 [06:08<04:47, 11.51s/it] 57%|█████▋    | 32/56 [06:20<04:41, 11.74s/it] 59%|█████▉    | 33/56 [06:29<04:06, 10.70s/it] 61%|██████    | 34/56 [06:43<04:18, 11.75s/it] 62%|██████▎   | 35/56 [06:54<04:01, 11.50s/it] 64%|██████▍   | 36/56 [07:01<03:27, 10.39s/it] 66%|██████▌   | 37/56 [07:16<03:42, 11.73s/it] 68%|██████▊   | 38/56 [07:28<03:32, 11.83s/it] 70%|██████▉   | 39/56 [07:37<03:02, 10.76s/it] 71%|███████▏  | 40/56 [07:51<03:11, 11.97s/it]                                                71%|███████▏  | 40/56 [07:51<03:11, 11.97s/it] 73%|███████▎  | 41/56 [08:02<02:53, 11.56s/it] 75%|███████▌  | 42/56 [08:09<02:23, 10.25s/it] 77%|███████▋  | 43/56 [08:24<02:31, 11.65s/it] 79%|███████▊  | 44/56 [08:36<02:20, 11.69s/it] 80%|████████  | 45/56 [08:42<01:49,  9.95s/it]{'loss': 6.0238, 'grad_norm': 1.6041007041931152, 'learning_rate': 2.0142070414860704e-05, 'epoch': 2.68}
 82%|████████▏ | 46/56 [08:57<01:54, 11.49s/it] 84%|████████▍ | 47/56 [09:11<01:50, 12.32s/it] 86%|████████▌ | 48/56 [09:21<01:31, 11.47s/it] 88%|████████▊ | 49/56 [09:35<01:27, 12.44s/it] 89%|████████▉ | 50/56 [09:46<01:11, 11.97s/it] 91%|█████████ | 51/56 [09:54<00:52, 10.60s/it] 93%|█████████▎| 52/56 [10:08<00:46, 11.74s/it] 95%|█████████▍| 53/56 [10:18<00:33, 11.13s/it] 96%|█████████▋| 54/56 [10:25<00:19,  9.97s/it] 98%|█████████▊| 55/56 [10:40<00:11, 11.49s/it]100%|██████████| 56/56 [10:52<00:00, 11.57s/it]                                               100%|██████████| 56/56 [10:53<00:00, 11.57s/it]100%|██████████| 56/56 [10:53<00:00, 11.67s/it]
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         train/epoch ▁▅█
wandb:   train/global_step ▁▅█
wandb:     train/grad_norm █▁
wandb: train/learning_rate █▁
wandb:          train/loss █▁
wandb: 
wandb: Run summary:
wandb:               total_flos 1.999351522215936e+16
wandb:              train/epoch 3.74576
wandb:        train/global_step 56
wandb:          train/grad_norm 1.6041
wandb:      train/learning_rate 2e-05
wandb:               train/loss 6.0238
wandb:               train_loss 6.52225
wandb:            train_runtime 653.517
wandb: train_samples_per_second 0.722
wandb:   train_steps_per_second 0.086
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync run/kfp-Qwen2.5-7B-Instruct/wandb/offline-run-20250816_220249-s7y3h0n0
wandb: Find logs at: run/kfp-Qwen2.5-7B-Instruct/wandb/offline-run-20250816_220249-s7y3h0n0/logs
{'train_runtime': 653.517, 'train_samples_per_second': 0.722, 'train_steps_per_second': 0.086, 'train_loss': 6.522247178213937, 'epoch': 3.75}
