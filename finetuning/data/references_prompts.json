[
  {
    "repo": "references_kfp_files",
    "file": "ferneutron_mlops_src_pipelines_components_utils.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `upload_model_pipeline` that performs the following:\n\nThis pipeline consists of **one** component: `upload_model`.\n\n**Component 1: `upload_model`**\n*   **Function:** Uploads a scikit-learn model to Google Cloud AI Platform.\n*   **Inputs:**\n    *   `model`: An `Input[Model]` representing the model artifact to be uploaded.\n*   **Outputs:** None explicitly defined, as it performs an action (model upload).\n*   **Base Image:** `gcr.io/deeplearning-platform-release/tf2-cpu.2-6:latest`\n*   **Packages to Install:** `google-cloud-aiplatform`\n*   **Internal Logic:**\n    *   Initializes the `google.cloud.aiplatform` client with `project=\"gsd-ai-mx-ferneutron\"` and `location=\"us-central1\"`.\n    *   Uses `aiplatform.Model.upload_scikit_learn_model_file` to upload the model.\n    *   The `model_file_path` is taken from `model.path`.\n    *   The `display_name` for the uploaded model is `\"IrisModelv3\"`.\n    *   The `project` for the upload is `\"gsd-ai-mx-ferneutron\"`.\n*   **Tools/Libraries Used:** `google-cloud-aiplatform`.\n\n**Control Flow:**\nThe pipeline executes the `upload_model` component. There are no dependencies or complex control flow constructs (like `after`, `parallelFor`, or conditional execution) as it's a single-component pipeline."
  },
  {
    "repo": "references_kfp_files",
    "file": "xxxtrillionarie_KubeFlow_MLOps_Pipelines_samples_test_legacy_data_passing.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `data_passing_pipeline` that performs a comprehensive demonstration of data passing mechanisms in KFP.\n\nThe pipeline consists of 15 components, categorized into \"Produce\" and \"Consume\" functions, showcasing various input/output types and consumption methods.\n\n**Pipeline Parameters:**\n* `anything_param`: Default value \"anything_param\"\n* `something_param`: Type \"Something\", Default value \"something_param\"\n* `string_param`: Type `str`, Default value \"string_param\"\n\n**Components and their functions:**\n\n**Produce Components (4):**\n1.  **`produce_anything`**:\n    *   **Function**: Writes the string \"produce_anything\" to an output file.\n    *   **Outputs**: `data_path` (untyped output path).\n2.  **`produce_something`**:\n    *   **Function**: Writes the string \"produce_something\" to an output file.\n    *   **Outputs**: `data_path` (output path of type \"Something\").\n3.  **`produce_something2`**:\n    *   **Function**: Returns the string \"produce_something2\" directly.\n    *   **Outputs**: Untyped string output (implicitly type \"Something\" due to annotation).\n4.  **`produce_string`**:\n    *   **Function**: Returns the string \"produce_string\" directly.\n    *   **Outputs**: String output of type `str`.\n\n**Consume Components - As Value (3):**\n1.  **`consume_anything_as_value`**:\n    *   **Function**: Prints the input data directly.\n    *   **Inputs**: `data` (untyped).\n2.  **`consume_something_as_value`**:\n    *   **Function**: Prints the input data directly.\n    *   **Inputs**: `data` (of type \"Something\").\n3.  **`consume_string_as_value`**:\n    *   **Function**: Prints the input data directly.\n    *   **Inputs**: `data` (of type `str`).\n\n**Consume Components - As File (3):**\n1.  **`consume_anything_as_file`**:\n    *   **Function**: Reads and prints the content of the input file.\n    *   **Inputs**: `data_path` (untyped input path).\n2.  **`consume_something_as_file`**:\n    *   **Function**: Reads and prints the content of the input file.\n    *   **Inputs**: `data_path` (input path of type \"Something\").\n3.  **`consume_string_as_file`**:\n    *   **Function**: Reads and prints the content of the input file.\n    *   **Inputs**: `data_path` (input path of type `str`).\n\n**Control Flow and Data Passing:**\n\nThe pipeline demonstrates 6 end-to-end data passing scenarios, covering constant values, pipeline parameters, and upstream component outputs, consumed both as values and as files.\n\n1.  **Passing constant value consumed as value:**\n    *   A task `consume_anything_as_value_from_const` is created by calling `consume_anything_as_value` with the string \"constant_value\".\n2.  **Passing constant value consumed as file:**\n    *   A task `consume_anything_as_file_from_const` is created by calling `consume_anything_as_file` with the string \"constant_value\".\n3.  **Passing pipeline parameter consumed as value:**\n    *   A task `consume_anything_as_value_from_param` is created by calling `consume_anything_as_value` with the `anything_param` pipeline parameter.\n    *   A task `consume_something_as_value_from_param` is created by calling `consume_something_as_value` with the `something_param` pipeline parameter.\n    *   A task `consume_string_as_value_from_param` is created by calling `consume_string_as_value` with the `string_param` pipeline parameter.\n4.  **Passing pipeline parameter consumed as file:**\n    *   A task `consume_anything_as_file_from_param` is created by calling `consume_anything_as_file` with the `anything_param` pipeline parameter.\n    *   A task `consume_something_as_file_from_param` is created by calling `consume_something_as_file` with the `something_param` pipeline parameter.\n    *   A task `consume_string_as_file_from_param` is created by calling `consume_string_as_file` with the `string_param` pipeline parameter.\n5.  **Passing task output consumed as value:**\n    *   A task `consume_anything_as_value_from_produce_anything` is created by calling `consume_anything_as_value` with the output of `produce_anything`.\n    *   A task `consume_something_as_value_from_produce_something` is created by calling `consume_something_as_value` with the output of `produce_something`.\n    *   A task `consume_something_as_value_from_produce_something2` is created by calling `consume_something_as_value` with the output of `produce_something2`.\n    *   A task `consume_string_as_value_from_produce_string` is created by calling `consume_string_as_value` with the output of `produce_string`.\n6.  **Passing task output consumed as file:**\n    *   A task `consume_anything_as_file_from_produce_anything` is created by calling `consume_anything_as_file` with the output of `produce_anything`.\n    *   A task `consume_something_as_file_from_produce_something` is created by calling `consume_something_as_file` with the output of `produce_something`.\n    *   A task `consume_string_as_file_from_produce_string` is created by calling `consume_string_as_file` with the output of `produce_string`.\n\n**Libraries/Tools Used:**\n*   `kfp.deprecated` for Kubeflow Pipelines definition.\n*   `kfp.deprecated.components` for creating components from Python functions."
  },
  {
    "repo": "references_kfp_files",
    "file": "xxxtrillionarie_KubeFlow_MLOps_Pipelines_samples_test_fail.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `fail-pipeline` that performs the following steps:\n\nThis pipeline consists of a single component.\n\nThe pipeline defines one component called `fail_op`, which is created from the Python function `fail`.\n- **`fail` function**: This function simulates a failure by exiting the Python process with an exit code of 1 (`sys.exit(1)`). Its purpose is to demonstrate a failing operation within a pipeline.\n- **Base Image**: The `fail_op` component uses the `alpine:latest` Docker image as its base.\n\n**Control Flow**:\n- The pipeline simply executes the `fail_op` component as `fail_task`. There are no dependencies or complex control flow structures. The pipeline is designed to always fail when this task is executed.\n\n**Tools/Libraries**:\n- The pipeline primarily uses the `kfp.deprecated` library for defining components and the pipeline structure.\n- The `sys` module is used within the `fail` function for exiting the process."
  },
  {
    "repo": "references_kfp_files",
    "file": "xxxtrillionarie_KubeFlow_MLOps_Pipelines_samples_test_metrics_visualization_v2_test.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `metrics_visualization_pipeline` that performs the following steps:\n\nThe pipeline consists of five components:\n1.  **`wine-classification`**: This component is responsible for performing wine classification.\n2.  **`iris-sgdclassifier`**: This component handles iris classification using an SGD Classifier.\n3.  **`digit-classification`**: This component performs digit classification.\n4.  **`html-visualization`**: This component generates an HTML visualization.\n5.  **`markdown-visualization`**: This component generates a Markdown visualization.\n\n**Control Flow and Dependencies:**\n\n*   The `html-visualization` component depends on the successful completion of `wine-classification`, `iris-sgdclassifier`, and `digit-classification`. It takes outputs (likely metrics or results) from these three classification components as inputs to generate the HTML visualization.\n*   Similarly, the `markdown-visualization` component also depends on the successful completion of `wine-classification`, `iris-sgdclassifier`, and `digit-classification`. It uses outputs from these classification components to generate the Markdown visualization.\n\n**Inputs and Outputs (where visible):**\n\n*   **`wine-classification`**:\n    *   **Outputs**: An artifact named `metrics` with specific metadata, including `display_name: 'metrics'` and `confidenceMetrics`. The `confidenceMetrics` contain a list of dictionaries, each with `confidenceThreshold`, `falsePositiveRate`, and `recall` values. These metrics suggest the component is outputting performance evaluation results for a classification model.\n*   **`iris-sgdclassifier`**:\n    *   **Outputs**: Similar to `wine-classification`, it outputs an artifact named `metrics` with `display_name: 'metrics'` and `confidenceMetrics` containing a list of dictionaries with `confidenceThreshold`, `falsePositiveRate`, and `recall`.\n*   **`digit-classification`**:\n    *   **Outputs**: Also outputs an artifact named `metrics` with `display_name: 'metrics'` and `confidenceMetrics` containing a list of dictionaries with `confidenceThreshold`, `falsePositiveRate`, and `recall`.\n*   **`html-visualization`**:\n    *   **Inputs**: Takes the `metrics` outputs from `wine-classification`, `iris-sgdclassifier`, and `digit-classification`.\n*   **`markdown-visualization`**:\n    *   **Inputs**: Takes the `metrics` outputs from `wine-classification`, `iris-sgdclassifier`, and `digit-classification`.\n\nThe pipeline aims to perform multiple classification tasks and then visualize their performance metrics using both HTML and Markdown formats."
  },
  {
    "repo": "references_kfp_files",
    "file": "xxxtrillionarie_KubeFlow_MLOps_Pipelines_samples_test_metrics_visualization_v2.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `test_metrics_visualization_v2` that performs machine learning tasks and visualizes metrics.\n\nThe pipeline consists of two components:\n1.  **`digit_classification`**: This component trains a Logistic Regression model on the Iris dataset, evaluates its accuracy using cross-validation and a test set, and logs the final accuracy as a metric.\n    *   **Functionality**: Loads the Iris dataset, performs data splitting and cross-validation, trains a `LogisticRegression` model, calculates and logs the accuracy metric.\n    *   **Libraries Used**: `sklearn` (specifically `model_selection`, `linear_model`, `datasets`, `metrics`).\n    *   **Outputs**: `metrics` (of type `kfp.dsl.Metrics`), which contains the logged accuracy.\n\n2.  **`wine_classification`**: This component trains a RandomForestClassifier on the Wine dataset, focusing on a binary classification problem for a specific label, and generates ROC curve data.\n    *   **Functionality**: Loads the Wine dataset, transforms the target for binary classification, splits data, trains a `RandomForestClassifier`, and generates predicted probabilities and classes for ROC curve calculation.\n    *   **Libraries Used**: `sklearn` (specifically `ensemble`, `metrics`, `datasets`, `model_selection`).\n    *   **Outputs**: `metrics` (of type `kfp.dsl.ClassificationMetrics`), intended to visualize classification-specific metrics like ROC curve data. Note that the provided code snippet is truncated and only shows the `fpr`, `tpr`, and `thresholds` calculation.\n\n**Control Flow**:\nThe pipeline runs the `digit_classification` and `wine_classification` components in parallel, as there are no explicit dependencies defined between them. Each component runs independently and outputs its respective metrics."
  },
  {
    "repo": "references_kfp_files",
    "file": "FernandoLpz_Kubeflow_Pipelines_pipeline.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `First Pipeline` that performs classification using Decision Tree and Logistic Regression.\n\nThis pipeline consists of **four** components:\n\n1.  **`download_data`**: This component is loaded from the `download_data/download_data.yaml` file. Its function is to download the necessary dataset for the classification problem. It has **no explicit inputs** and produces **one output** (likely the path or reference to the downloaded data).\n2.  **`decision_tree`**: This component is loaded from the `decision_tree/decision_tree.yaml` file. Its function is to train and evaluate a Decision Tree model. It takes **one input**, which is the output from the `download_data` component (the downloaded data). It produces **one output**, representing the accuracy or performance metric of the Decision Tree model (a float).\n3.  **`logistic_regression`**: This component is loaded from the `logistic_regression/logistic_regression.yaml` file. Its function is to train and evaluate a Logistic Regression model. It takes **one input**, which is also the output from the `download_data` component (the downloaded data). It produces **one output**, representing the accuracy or performance metric of the Logistic Regression model (a float).\n4.  **`show_results`**: This is a custom Python function component, defined using `@func_to_container_op`. Its function is to display the final results. It takes **two inputs**: `decision_tree` (a float representing the Decision Tree accuracy) and `logistic_regression` (a float representing the Logistic Regression accuracy). It has **no explicit outputs** but prints the accuracy scores to standard output.\n\nThe control flow of the pipeline is as follows:\n\n*   The `download_data` component runs first.\n*   Once `download_data` completes, its output is used as input for both the `decision_tree` and `logistic_regression` components, which run **in parallel**.\n*   After both `decision_tree` and `logistic_regression` components have completed, their respective outputs (accuracies) are passed as inputs to the `show_results` component, which runs last.\n\nThe pipeline utilizes the `kfp` library for defining and orchestrating the components. It loads component definitions from YAML files for the data download and model training steps, and uses a Python function for result display. No specific external machine learning libraries (like sklearn) or data platforms (like Snowflake) are explicitly mentioned within the pipeline definition itself, but they are likely used within the `.yaml` components."
  },
  {
    "repo": "references_kfp_files",
    "file": "9rince_kfp_samples_test_fail_parameter_value_missing.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `parameter_value_missing` that performs the following:\n\nThe pipeline has **one** component:\n*   **`echo`**: This component takes a single string input named `text` and simply echoes it to standard output. It uses the `alpine` container image and the `echo` command.\n\nThe pipeline definition `pipeline` takes one input parameter:\n*   `parameter` (type: `str`): This parameter is intended to be provided at pipeline submission time.\n\nThe control flow is as follows:\n*   The `echo_op` component is called, with its `text` input directly mapped to the pipeline's `parameter` input."
  },
  {
    "repo": "references_kfp_files",
    "file": "NusretOzates_autonlp-classification-kubeflow_pipeline_pipeline_runner.py",
    "structured_prompt": "I need the content of `pipeline/pipeline.py` to generate the prompt. The provided `run_pipeline` function *uses* that file, but it doesn't *define* the Kubeflow Pipeline itself.\n\nPlease provide the content of `pipeline/pipeline.py`, which should contain the `@dsl.pipeline` and `@component` definitions. Once I have that, I can generate the complete and accurate prompt."
  },
  {
    "repo": "references_kfp_files",
    "file": "riiid_krsh_tests_samples_have-pipeline-project_pipelines_pipeline-2_pipeline.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `pipeline-2` that performs the following:\n\nThis pipeline currently contains no components and thus has no specific functionality or control flow defined. It serves as a basic skeleton for a Kubeflow pipeline."
  },
  {
    "repo": "references_kfp_files",
    "file": "xxxtrillionarie_KubeFlow_MLOps_Pipelines_samples_test_fail_parameter_value_missing.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `parameter_value_missing` that performs the following steps:\n\nThis pipeline consists of a single component.\n\n**Components:**\n\n1.  **`echo_op`**:\n    *   **Function:** This component is an `Echo` component. It takes a string input and prints it to standard output.\n    *   **Inputs:** It takes one input:\n        *   `text`: A string value.\n    *   **Dependencies:** This component depends on the `parameter` input provided to the pipeline.\n\n**Control Flow:**\n\nThe `echo_op` component directly uses the `parameter` input provided to the main pipeline function.\n\n**Inputs to the Pipeline:**\n\n*   `parameter`: A string. This parameter is expected to be provided when the pipeline is submitted."
  },
  {
    "repo": "references_kfp_files",
    "file": "redhat-ai-services_kubeflow-pipelines-examples_pipelines_09_secrets_cm_pipeline.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `Env Vars Pipeline` that performs the following steps:\n\nThis pipeline consists of **two** parallel components:\n\n1.  **`print_envvar` (secret_print_task)**:\n    *   **Function:** This component takes an environment variable name (`env_var`) as input and prints its value.\n    *   **Inputs:** `env_var` (string)\n    *   **Outputs:** Prints the value of the specified environment variable to standard output.\n    *   **Configuration:**\n        *   The component is configured to **disable caching**.\n        *   It mounts a Kubernetes secret named `my-secret`.\n        *   Specifically, the key `my-secret-data` from `my-secret` is exposed as an environment variable named `my-secret-env-var` within the component's execution environment.\n        *   The `env_var` input to this component is set to `\"my-secret-env-var\"`.\n    *   **Base Image:** `image-registry.openshift-image-registry.svc:5000/openshift/python:latest`\n\n2.  **`print_envvar` (cm_print_task)**:\n    *   **Function:** This component also takes an environment variable name (`env_var`) as input and prints its value.\n    *   **Inputs:** `env_var` (string)\n    *   **Outputs:** Prints the value of the specified environment variable to standard output.\n    *   **Configuration:**\n        *   The component is configured to **disable caching**.\n        *   It mounts a Kubernetes ConfigMap named `my-configmap`.\n        *   Specifically, the key `my-configmap-data` from `my-configmap` is exposed as an environment variable named `my-cm-env-var` within the component's execution environment.\n        *   The `env_var` input to this component is set to `\"my-cm-env-var\"`.\n    *   **Base Image:** `image-registry.openshift-image-registry.svc:5000/openshift/python:latest`\n\n**Control Flow:**\nBoth `secret_print_task` and `cm_print_task` run in **parallel** as they do not have explicit dependencies on each other.\n\n**Tools/Libraries:**\nThe pipeline uses `kfp` for defining the pipeline and components, and `kubernetes` for interacting with Kubernetes resources like Secrets and ConfigMaps to expose them as environment variables. The `dotenv` library is used for loading environment variables locally for pipeline compilation and execution setup."
  },
  {
    "repo": "references_kfp_files",
    "file": "shawar8_sfcrime-prediction-kubeflow_training_functions.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `SF_Crime_Prediction_Training_Pipeline` that performs the following steps:\n\nThe pipeline consists of **two components**:\n\n1.  **`get_train_test_split` Component:**\n    *   **Function:** This component reads a DataFrame, splits it into training and testing sets (X and Y), and selects the split that minimizes data divergence (using Kolmogorov-Smirnov statistic) across specified continuous columns over `n_tries` attempts.\n    *   **Inputs:**\n        *   `df_path`: An `Input[Dataset]` representing the input DataFrame.\n        *   `label_column`: A string indicating the name of the target variable column.\n        *   `test_size`: A float specifying the proportion of the dataset to include in the test split.\n        *   `n_tries`: An integer representing the number of random splits to attempt to find the best one.\n    *   **Outputs:**\n        *   `output_x_train`: An `Output[Dataset]` containing the features for the training set.\n        *   `output_x_test`: An `Output[Dataset]` containing the features for the test set.\n        *   `output_y_train`: An `Output[Artifact]` containing the labels for the training set (serialized using pickle).\n        *   `output_y_test`: An `Output[Artifact]` containing the labels for the test set (serialized using pickle).\n        *   `divergence_output_dict`: An `Output[Artifact]` containing a dictionary of Kolmogorov-Smirnov statistics for continuous columns from the best split (serialized using pickle).\n    *   **Libraries/Tools:** `numpy`, `scikit-learn` (specifically `train_test_split`), `pandas`, `scipy`.\n    *   **Base Image:** `python:3.9`\n    *   **Output Component File:** `new_yamls/split_train_test.yaml`\n\n2.  **`prepare_data_for_training` Component:**\n    *   **Function:** This component is intended to perform data preparation steps for training, such as handling different data types (float, categorical, binary columns). *(Note: The provided code snippet for this component is incomplete, so its exact full functionality cannot be fully described. However, based on its name and input types, it processes the split datasets.)*\n    *   **Inputs:**\n        *   `x_train_input_path`: An `Input[Dataset]` representing the training features.\n        *   `x_test_input_path`: An `Input[Dataset]` representing the test features.\n        *   `float_cols`: A list of strings identifying float columns.\n        *   `cat_cols`: A list of strings identifying categorical columns.\n        *   `binary_cols`: A list of strings identifying binary columns.\n        *   *(Implicit/Inferred Outputs: Given the name, it's likely to output processed versions of x_train and x_test.)*\n    *   **Libraries/Tools:** `scikit-learn`, `pandas`, `numpy`.\n    *   **Base Image:** `python:3.9`\n    *   **Output Component File:** `new_yamls/prep_data_for_training.yaml`\n\n**Control Flow:**\n\n*   The `prepare_data_for_training` component should execute *after* `get_train_test_split` has successfully completed.\n*   The `prepare_data_for_training` component will use `x_train_input_path` and `x_test_input_path` as inputs, which should be the `output_x_train` and `output_x_test` from the `get_train_test_split` component, respectively.\n\n**General Notes:**\n\n*   The pipeline uses Kubeflow Pipelines V2 DSL (`kfp.v2.dsl`).\n*   The components specify `base_image` and `packages_to_install`.\n*   The pipeline integrates with custom `variables` and `common_fns` modules (though their content is not provided, their presence indicates potential shared configurations or utilities).\n*   The pipeline uses standard Python libraries for data manipulation and machine learning."
  },
  {
    "repo": "references_kfp_files",
    "file": "redhat-ai-services_kubeflow-pipelines-examples_pipelines_00_compiled_pipeline.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `add_pipeline` that performs a series of additions.\n\nThis pipeline consists of two components.\n\n**Component 1: `add`**\n- **Function:** Calculates the sum of two floating-point numbers.\n- **Inputs:**\n    - `a`: A float.\n    - `b`: A float.\n- **Output:** A float representing the sum.\n- **Base Image:** `image-registry.openshift-image-registry.svc:5000/openshift/python:latest`\n\n**Pipeline `add_pipeline` Details:**\n- **Description:** Takes an initial value `a`, adds 4 to it, and then takes the output of that first addition and adds a second value `b`.\n- **Inputs:**\n    - `a`: A float with a default value of `1.0`.\n    - `b`: A float with a default value of `7.0`.\n\n**Control Flow:**\n1. **`first_add_task`:** This task calls the `add` component.\n    - It takes the pipeline input `a` as its first argument.\n    - It uses a literal value `4.0` as its second argument `b`.\n2. **`second_add_task`:** This task also calls the `add` component.\n    - It takes the *output* of `first_add_task` as its first argument `a`.\n    - It takes the pipeline input `b` as its second argument.\n\nNo external tools or libraries beyond `kfp` for pipeline definition are explicitly used within the component logic itself."
  },
  {
    "repo": "references_kfp_files",
    "file": "akranga_anton-ml1_components_golang_component.py",
    "structured_prompt": "You provided Python code that defines helper functions `processing_op` and `http_download_op` to create `ContainerOp` instances. However, this code **does not define a Kubeflow Pipeline** using `@dsl.pipeline` or any `@component` decorators. It only provides building blocks.\n\nTherefore, I cannot generate a prompt that describes a Kubeflow Pipeline from the given code. The prompt structure I would use requires a complete pipeline definition.\n\nIf you can provide a Python file that defines a Kubeflow Pipeline using `@dsl.pipeline` and `@component` decorators, I would be happy to generate the appropriate prompt."
  },
  {
    "repo": "references_kfp_files",
    "file": "ArianFotouhi_kubeflowPipelineSpamDetector_script.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `SpamDetectorPipeline` that performs a machine learning workflow to classify SMS messages as spam or ham.\n\nThe pipeline consists of **six** components, each with a specific function:\n\n1.  **`extract_data`**:\n    *   **Function**: Downloads the SMS Spam Collection dataset from a UCI archive URL, extracts the data from a zip file, and saves it as a CSV file.\n    *   **Outputs**: A `dsl.OutputPath(str)` representing the path to the extracted CSV file (e.g., `/mnt/data/smsspamcollection.csv`).\n    *   **Tools/Libraries**: `requests`, `zipfile`, `io`, `pandas`.\n\n2.  **`preprocess_data`**:\n    *   **Function**: Reads the raw SMS data, adds two new features: `length` (length of the message) and `punct` (count of punctuation marks in the message). Saves the preprocessed data to a new CSV file.\n    *   **Inputs**: A `dsl.InputPath(str)` pointing to the raw CSV file generated by `extract_data`.\n    *   **Tools/Libraries**: `pandas`, `string`.\n\n3.  **`eda` (Exploratory Data Analysis)**:\n    *   **Function**: Performs basic exploratory data analysis on the preprocessed data. This includes checking for missing values, unique labels, and the distribution of 'ham' and 'spam' messages. It also generates and saves histograms to visualize the 'length' and 'punct' features for both categories, providing insights into their distributions.\n    *   **Inputs**: Reads the preprocessed data from a specific path (e.g., `/mnt/data/preprocessed_smsspamcollection.csv`).\n    *   **Tools/Libraries**: `pandas`, `matplotlib.pyplot`, `numpy`.\n\n4.  **`train_model`**:\n    *   **Function**: Trains a machine learning model to classify spam messages. It reads the preprocessed data, splits it into training and testing sets, creates a scikit-learn pipeline using `TfidfVectorizer` for text feature extraction and `RandomForestClassifier` for classification. The trained model is then saved using `joblib`.\n    *   **Inputs**: Reads the preprocessed data from a specific path (e.g., `/mnt/data/preprocessed_smsspamcollection.csv`).\n    *   **Outputs**: A `dsl.OutputPath(str)` representing the path to the saved model file (e.g., `/mnt/data/spam_detector_model.joblib`).\n    *   **Tools/Libraries**: `pandas`, `sklearn.model_selection`, `sklearn.pipeline`, `sklearn.ensemble`, `sklearn.feature_extraction.text`, `joblib`.\n\n5.  **`evaluate_model`**:\n    *   **Function**: Evaluates the performance of the trained model. It loads the saved model, re-reads the preprocessed data, splits it into training and testing sets, makes predictions on the test set, and calculates various classification metrics (e.g., accuracy, confusion matrix, classification report). It also generates and saves a confusion matrix plot.\n    *   **Inputs**: A `dsl.InputPath(str)` pointing to the trained model file from `train_model`. It also reads the preprocessed data from a specific path (e.g., `/mnt/data/preprocessed_smsspamcollection.csv`).\n    *   **Tools/Libraries**: `pandas`, `sklearn.model_selection`, `sklearn.metrics`, `matplotlib.pyplot`, `joblib`.\n\n6.  **`prediction_example`**:\n    *   **Function**: Demonstrates the trained model's capability by making predictions on new, unseen SMS messages. It loads the trained model, defines example messages, and uses the model to predict whether each message is 'ham' or 'spam'.\n    *   **Inputs**: A `dsl.InputPath(str)` pointing to the trained model file from `train_model`.\n    *   **Tools/Libraries**: `pandas`, `sklearn.pipeline`, `sklearn.feature_extraction.text`, `joblib`, `typing.List`.\n\n**Control Flow:**\n\n*   `preprocess_data` depends on `extract_data` (its input `file_path` is the output of `extract_data`).\n*   `eda` depends on `preprocess_data` (it reads the output file from `preprocess_data`).\n*   `train_model` depends on `preprocess_data` (it reads the output file from `preprocess_data`).\n*   `evaluate_model` depends on `train_model` (its input is the output model file from `train_model`).\n*   `prediction_example` depends on `train_model` (its input is the output model file from `train_model`).\n*   `eda`, `train_model` can run in parallel after `preprocess_data` completes.\n*   `evaluate_model` and `prediction_example` can run in parallel after `train_model` completes."
  },
  {
    "repo": "references_kfp_files",
    "file": "redhat-ai-services_kubeflow-pipelines-examples_pipelines_10_mount_pvc_pipeline.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `PVC Pipeline` that performs arithmetic operations and demonstrates mounting a Persistent Volume Claim (PVC) to a task.\n\nThe pipeline consists of **one** reusable component named `add`.\n\n**Component Details:**\n\n*   **`add` component:**\n    *   **Function:** Calculates the sum of two floating-point numbers.\n    *   **Inputs:**\n        *   `a` (float): The first number.\n        *   `b` (float): The second number.\n    *   **Output:** (float) The sum of `a` and `b`.\n    *   **Base Image:** `image-registry.openshift-image-registry.svc:5000/openshift/python:latest`\n\n**Pipeline Flow (`add_pipeline`):**\n\nThe pipeline takes two float parameters: `a` (defaulting to `1.0`) and `b` (defaulting to `7.0`).\n\n1.  **`first_add_task`:**\n    *   Calls the `add` component.\n    *   Inputs: `a` (from pipeline parameter `a`) and `b` (fixed value `4.0`).\n    *   **Control Flow:** Immediately after this task is defined, a Persistent Volume Claim (PVC) named `\"my-data\"` is mounted to this task at the path `\"/opt/data\"`. This is achieved using `kubernetes.mount_pvc`.\n\n2.  **`second_add_task`:**\n    *   Calls the `add` component.\n    *   Inputs: `a` (the output of `first_add_task`) and `b` (from pipeline parameter `b`).\n    *   **Control Flow:** This task implicitly depends on `first_add_task` as it uses its output.\n\n**Tools/Libraries Used:**\n\n*   `kfp.dsl` for pipeline and component definition.\n*   `kfp.kubernetes` for PVC mounting functionality."
  },
  {
    "repo": "references_kfp_files",
    "file": "anifort_kubeflow-pipelines-mlops_pipeline_src_pipeline.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `A Simple CI pipeline` that performs data preprocessing.\n\nThe pipeline consists of a single component:\n1.  **`tfidfvec_op` (vectorizing):** This component takes `data_path` (defaulting to \"gs://kubeflow_pipelines_sentiment/data/test.csv\") and `vectorizer_gcs_location` (defaulting to \"gs://kubeflow_pipelines_sentiment/assets/x.pkl\") as inputs. Its function is to perform TF-IDF vectorization on the input data. It implicitly produces an output named `local_output` (though not explicitly used in a subsequent step, it's defined as an output). This component utilizes Google Cloud Platform (GCP) secrets by applying `gcp.use_gcp_secret('user-gcp-sa')`.\n\nThe pipeline's control flow is straightforward, executing the single `tfidfvec_op` component. There are no explicit dependencies or conditional logic present in the provided active code.\n\nThe pipeline implicitly uses `kfp` and `kfp.dsl` for pipeline definition, and `kfp.gcp` for GCP integration. It relies on a `ComponentStore` to load components, specifically `tfidf-vectoriser`, indicating it's a pre-built or custom component. The specific libraries used within the `tfidf-vectoriser` component (e.g., `sklearn` for TF-IDF) are not directly visible in this pipeline definition but would reside within the component's implementation."
  },
  {
    "repo": "references_kfp_files",
    "file": "shawar8_sfcrime-prediction-kubeflow_data_ingest_fns.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `SF_Crime_Data_Ingestion_and_Preprocessing` that performs data ingestion and preprocessing for crime prediction.\n\nThe pipeline consists of **two** components:\n\n1.  **`preprocess_data`**:\n    *   **Function**: This component takes raw crime data, preprocesses it by dropping duplicates, filtering out irrelevant categories ('Case Closure'), handling missing values, standardizing date/time formats, calculating time differences between incident and report, extracting date/time features (year, month, hour, weekend flag), and filtering data based on a `max_date_training` for training data.\n    *   **Inputs**:\n        *   `df_path` (Input[Dataset]): The path to the raw input DataFrame.\n        *   `to_keep` (list): A list of columns to retain in the DataFrame.\n        *   `data` (str): A string indicating the data type, either 'serving' or 'training', which affects date/time parsing and filtering logic.\n    *   **Outputs**:\n        *   `df_output_path` (Output[Dataset]): The path where the preprocessed DataFrame will be saved.\n    *   **Libraries**: Uses `pandas`, `numpy`, `datetime`, and `pytz`.\n    *   **Base Image**: `python:3.9`\n\n2.  **`map_label`**:\n    *   **Function**: This component takes a preprocessed DataFrame and applies a mapping to specific incident categories, specifically consolidating different 'Human Trafficking' labels into a single 'Human Trafficking' category.\n    *   **Inputs**:\n        *   `df_path` (Input[Dataset]): The path to the input DataFrame requiring label mapping.\n    *   **Outputs**:\n        *   `mapped_df_path` (Output[Dataset]): The path where the DataFrame with mapped labels will be saved.\n    *   **Libraries**: Uses `pandas`.\n    *   **Base Image**: `python:3.9`\n\n**Control Flow**:\nThe pipeline's control flow is sequential: the `map_label` component should execute **after** the `preprocess_data` component. The `map_label` component will receive the output `Dataset` from `preprocess_data` as its input.\n\n**Additional Details**:\n*   The components are defined using `@component` from `kfp.v2.dsl`.\n*   The pipeline should import necessary modules like `kfp`, `kfp.dsl`, `kfp.v2`, `kfp.v2.dsl`, `Input`, `Output`, `Dataset`, and `Artifact`.\n*   Ensure that the Kubeflow SDK v2 compatible syntax is used for defining components and the pipeline.\n*   The components should generate their respective YAML files in a directory named `new_yamls`."
  },
  {
    "repo": "references_kfp_files",
    "file": "rakesh283343_kubeflow-sample-pipelines_core_parallel_join_parallel_join.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `Parallel pipeline` that performs the following steps:\n\nThis pipeline downloads two text files from GCS in parallel and then prints their concatenated content.\n\nIt consists of **three components**:\n\n1.  **`GCS - Download`**: This component downloads a file from a specified GCS URL.\n    *   It takes one input: `url` (string), which is the GCS path to the file.\n    *   It produces one output: `data` (file path), which points to the downloaded file at `/tmp/results.txt`.\n    *   It uses the `google/cloud-sdk:279.0.0` image and executes a `gsutil cat` command to download the file.\n\n2.  **`GCS - Download`** (second instance): This is another instance of the same GCS download component, running in parallel to the first.\n    *   It takes one input: `url` (string), which is the GCS path to the file.\n    *   It produces one output: `data` (file path), which points to the downloaded file at `/tmp/results.txt`.\n    *   It uses the `google/cloud-sdk:279.0.0` image and executes a `gsutil cat` command to download the file.\n\n3.  **`echo`**: This component takes two text inputs and prints them.\n    *   It takes two inputs: `text1` (string) and `text2` (string).\n    *   It does not produce any visible outputs.\n    *   It uses the `library/bash:4.4.23` image and executes a `echo` command to print the inputs.\n\n**Control Flow**:\n\n*   The pipeline defines two input parameters:\n    *   `url1` (string, default: `gs://ml-pipeline/sample-data/shakespeare/shakespeare1.txt`)\n    *   `url2` (string, default: `gs://ml-pipeline/sample-data/shakespeare/shakespeare2.txt`)\n*   The first two `GCS - Download` components (`download1_task` and `download2_task`) run **in parallel**.\n    *   `download1_task` takes `url1` as its input.\n    *   `download2_task` takes `url2` as its input.\n*   The `echo` component (`echo_task`) runs **after** both `download1_task` and `download2_task` have completed.\n    *   Its first input (`text1`) is the `data` output from `download1_task`.\n    *   Its second input (`text2`) is the `data` output from `download2_task`.\n\nThe pipeline's overall description is \"Download two messages in parallel and prints the concatenated result.\""
  },
  {
    "repo": "references_kfp_files",
    "file": "rakesh283343_kubeflow-pipelines-clv_pipelines_batch_predict_pipeline.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `CLV Batch Predict` that performs Customer Lifetime Value (CLV) batch prediction using BigQuery for feature engineering and AutoML Tables for batch inference.\n\nThe pipeline consists of four components:\n\n1.  **`Load sales transactions`**: This component, implemented by `load_sales_transactions_op` (which wraps the `load_sales_transactions` function), is responsible for loading sales transactions data.\n    *   **Inputs:** `project_id`, `source_gcs_path`, `source_bq_table`, `dataset_location`, `dataset_name`, and `table_id` (mapped to `transactions_table_name`).\n    *   **Outputs:** This component likely outputs the loaded sales transaction data (though not explicitly shown in the provided snippet).\n\n2.  **`Generate the feature engineering query`**: This component, implemented by `prepare_feature_engineering_query_op` (which wraps the `prepare_feature_engineering_query` function), prepares the BigQuery SQL query for feature engineering.\n    *   **Inputs:** `project_id`, `threshold_date`, `predict_end`, `max_monetary`, `query_template_uri`, `transactions_table_name`, `features_table_name`, and `bq_dataset_name`.\n    *   **Outputs:** It outputs the generated BigQuery query string.\n\n3.  **`Engineer features`**: This component, implemented by `engineer_features_op` (loaded from `bigquery/query` from the component store), executes the BigQuery query to engineer features.\n    *   **Inputs:** It takes the query string from the `Generate the feature engineering query` component.\n    *   **Outputs:** It outputs the engineered features, likely into a BigQuery table.\n    *   **Dependencies:** This component depends on the successful completion and output of `Generate the feature engineering query`.\n\n4.  **`Batch predict`**: This component, implemented by `batch_predict_op` (loaded from `aml-batch-predict` from the component store), performs batch prediction using the engineered features and an AutoML Tables model.\n    *   **Inputs:** It takes `aml_model_id`, `destination_prefix`, `aml_compute_region`, and the engineered features (presumably from the `Engineer features` component).\n    *   **Outputs:** It outputs the prediction results.\n    *   **Dependencies:** This component depends on the successful completion and output of `Engineer features`.\n\n**Control Flow:**\n\n*   `Load sales transactions` runs first, independently.\n*   `Generate the feature engineering query` runs concurrently with `Load sales transactions`.\n*   `Engineer features` runs *after* `Generate the feature engineering query` completes, using its output as input.\n*   `Batch predict` runs *after* `Engineer features` completes, using its output as input.\n\n**Pipeline Parameters:**\n\nThe pipeline accepts the following parameters:\n*   `project_id` (string)\n*   `source_gcs_path` (string)\n*   `staging_gcs_path` (string)\n*   `source_bq_table` (string)\n*   `bq_dataset_name` (string)\n*   `threshold_date` (string)\n*   `predict_end` (string)\n*   `max_monetary` (float)\n*   `aml_model_id` (string)\n*   `destination_prefix` (string)\n*   `features_table_name` (string, with a default value from `argument_defaults`)\n*   `transactions_table_name` (string, with a default value from `argument_defaults`)\n*   `dataset_location` (string, with a default value from `argument_defaults`)\n*   `aml_compute_region` (string, with a default value from `argument_defaults`)\n*   `query_template_uri` (string, with a default value from `argument_defaults`)\n\n**Tools/Libraries:**\n\n*   **Kubeflow Pipelines (KFP)**: Used for defining the pipeline (`@dsl.pipeline`, `@component`), creating lightweight components (`kfp.components.func_to_container_op`), and loading pre-built components from a component store (`kfp.components.ComponentStore`).\n*   **Google Cloud Platform (GCP)**: Integration with BigQuery for data querying and AutoML Tables for model prediction.\n*   **`pathlib` and `yaml`**: Used for loading pipeline settings from `settings.yaml`.\n*   **Helper components**: `load_sales_transactions` and `prepare_feature_engineering_query` are custom Python functions wrapped as lightweight components.\n*   **`bigquery/query` (from component store)**: A pre-built BigQuery component.\n*   **`aml-batch-predict` (from component store)**: A pre-built AutoML Tables batch prediction component."
  },
  {
    "repo": "references_kfp_files",
    "file": "stackdemos_kf4_components_training_component.py",
    "structured_prompt": "This prompt cannot be generated as the provided Python code does not define a Kubeflow Pipeline using `@dsl.pipeline` and `@component` decorators. It only contains helper functions for creating `ContainerOp` instances. \n\nTo generate the prompt, I would need a Python file that defines a Kubeflow pipeline structure with components and their dependencies using the `kfp.dsl` decorators."
  },
  {
    "repo": "references_kfp_files",
    "file": "paul-sud_demo-pipeline_kubeflow_toy.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `Paul_Sud_Demo_Pipeline` that performs a bioinformatics quality control and plotting workflow.\n\nThe pipeline consists of two components:\n1.  **`trim`**: This component performs quality trimming on a FASTQ file using Trimmomatic.\n    *   **Inputs**:\n        *   `fastq` (str): The path to the input FASTQ file.\n        *   `leading` (int): Quality threshold for leading bases.\n        *   `trailing` (int): Quality threshold for trailing bases.\n        *   `minlen` (int): Minimum read length after trimming.\n        *   `sliding_window` (str): Sliding window parameters (e.g., \"4:15\").\n    *   **Outputs**:\n        *   `trimmed_fastq` (OutputPath(\"TrimmedFastq\")): The path to the trimmed FASTQ file.\n    *   **Function**: It executes the `Trimmomatic-0.38.jar` tool to trim low-quality bases from the input FASTQ file. It uses a temporary `trimmed.fastq.gz` file and then renames it to the Kubeflow-provided output path.\n    *   **Base Image**: `quay.io/encode-dcc/demo-pipeline:template`\n\n2.  **`plot`**: This component generates quality score plots for both the original and trimmed FASTQ files.\n    *   **Inputs**:\n        *   `fastq` (str): The path to the original FASTQ file.\n        *   `trimmed_fastq` (InputPath(\"TrimmedFastq\")): The path to the trimmed FASTQ file, received from the `trim` component.\n        *   `bar_color` (str): Color for bars in the plot.\n        *   `flier_color` (str): Color for fliers (outliers) in the plot.\n        *   `plot_color` (str): General plot color.\n    *   **Outputs**:\n        *   `plot` (OutputPath(\"Plot\")): The path to the generated quality score PNG plot.\n    *   **Function**: It copies the `trimmed_fastq` to a temporary `trimmed.fastq.gz` file and then executes a Python script `/software/demo-pipeline/src/plot_fastq_scores.py` to generate quality score plots. It then renames the generated PNG file to the Kubeflow-provided output path.\n    *   **Base Image**: `quay.io/encode-dcc/demo-pipeline:template`\n\n**Control Flow**:\nThe `plot` component is dependent on the `trim` component. The `trimmed_fastq` output of the `trim` component is passed as the `trimmed_fastq` input to the `plot` component.\n\n**Pipeline Parameters**:\nThe pipeline should expose the following parameters to the user:\n*   `fastq_file` (str): The input FASTQ file.\n*   `leading_trim` (int): Leading trim quality. Default to `3`.\n*   `trailing_trim` (int): Trailing trim quality. Default to `3`.\n*   `min_length` (int): Minimum read length. Default to `36`.\n*   `sliding_window_params` (str): Sliding window parameters. Default to `\"4:15\"`.\n*   `bar_chart_color` (str): Bar color for plots. Default to `\"blue\"`.\n*   `flier_chart_color` (str): Flier color for plots. Default to `\"red\"`.\n*   `plot_line_color` (str): Plot line color. Default to `\"green\"`.\n\nThe pipeline should instantiate the `trim` and `plot` components and link them accordingly, passing the pipeline parameters to the respective component inputs."
  },
  {
    "repo": "references_kfp_files",
    "file": "stackdemos_specs-kf5_components_golang_component.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `GoLang_Compile_Demo` that performs the following steps:\n\nThis pipeline consists of **three** components:\n\n1.  **`download-code`**:\n    *   **Function:** Downloads a GoLang source code file from a specified URL.\n    *   **Inputs:**\n        *   `url`: A string URL pointing to the GoLang source code (e.g., `https://raw.githubusercontent.com/kubeflow/pipelines/master/samples/core/container_build/testdata/main.go`).\n        *   `download_to`: A string path where the file should be downloaded within the container (e.g., `/go/src/github.com/go-kubeflow/pipelines/cmd/main.go`).\n        *   `md5sum`: A string representing the expected MD5 checksum of the downloaded file for integrity verification (e.g., `380dd7283626d708f579512353a25d2c`).\n    *   **Outputs:** None explicitly defined, but the effect is a downloaded file at `download_to`.\n    *   **Tools/Libraries:** Uses `curl` for downloading and `md5sum` for verification. The image used is `appropriate/curl`.\n\n2.  **`compile-golang`**:\n    *   **Function:** Compiles the downloaded GoLang source code into an executable.\n    *   **Inputs:**\n        *   `script`: A string path to the GoLang source file to be compiled (e.g., `/go/src/github.com/go-kubeflow/pipelines/cmd/main.go`).\n        *   `arguments`: A list of strings for additional arguments to the compilation command (e.g., `['build', '-o', '/go/bin/go-program', '/go/src/github.com/go-kubeflow/pipelines/cmd/main.go']`).\n    *   **Outputs:** None explicitly defined, but the effect is a compiled executable.\n    *   **Tools/Libraries:** Uses the GoLang compiler. The image used is a GoLang base image, typically defined in the environment (e.g., `GOLANG_IMAGE`).\n    *   **Dependencies:** This component must run `after` the `download-code` component, ensuring the source code is available.\n\n3.  **`run-golang`**:\n    *   **Function:** Executes the compiled GoLang program.\n    *   **Inputs:**\n        *   `script`: A string path to the compiled GoLang executable (e.g., `/go/bin/go-program`).\n        *   `arguments`: A list of strings for additional arguments to the executable (e.g., `[]`).\n    *   **Outputs:** None explicitly defined, but the effect is the execution of the program.\n    *   **Tools/Libraries:** Executes the compiled GoLang binary. The image used is the same GoLang base image as `compile-golang`.\n    *   **Dependencies:** This component must run `after` the `compile-golang` component, ensuring the executable is ready.\n\n**Control Flow:**\nThe pipeline executes sequentially: `download-code` -> `compile-golang` -> `run-golang`. Each subsequent component is dependent on the successful completion of its predecessor."
  },
  {
    "repo": "references_kfp_files",
    "file": "9rince_kfp_samples_test_cache_v2_test.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `Two-step-pipeline` that performs a data preprocessing and a model training step.\n\nThe pipeline consists of two components:\n1.  **`preprocess`**:\n    *   **Functionality**: Simulates a data preprocessing step.\n    *   **Inputs**:\n        *   `uri` (str): A URI string.\n        *   `some_int` (int): An integer parameter.\n    *   **Outputs**:\n        *   `output_dataset_one` (Dataset): An output dataset artifact.\n        *   `output_parameter_one` (int): An output parameter.\n    *   **Implementation Details**: This component logs the input `uri` and `some_int`, and outputs `some_int` as a parameter and `output_dataset_one` as an artifact with a dummy value. It uses the `kfp.dsl.OutputPath` and `kfp.dsl.Output` types for artifact and parameter outputs, respectively.\n\n2.  **`train_op`**:\n    *   **Functionality**: Simulates a model training step.\n    *   **Inputs**:\n        *   `dataset` (Dataset): An input dataset artifact.\n        *   `num_steps` (int): An integer parameter representing the number of training steps.\n    *   **Outputs**:\n        *   `model` (Model): An output model artifact.\n    *   **Implementation Details**: This component logs the `num_steps` and the path of the input `dataset`. It produces a dummy `model` artifact. It uses the `kfp.dsl.InputPath` and `kfp.dsl.OutputPath` types for artifact inputs and outputs, respectively.\n\n**Control Flow**:\n*   The `preprocess` component runs first.\n*   The `train_op` component depends on the successful completion of `preprocess`. It uses the `output_dataset_one` artifact from `preprocess` as its `dataset` input and the `output_parameter_one` from `preprocess` as its `num_steps` input.\n\n**Pipeline Inputs**:\n*   `uri` (str): Defaults to 'gs://my-bucket/some-data'.\n*   `some_int` (int): Defaults to 10.\n\n**Decorators**: The pipeline uses `@dsl.pipeline` and `@dsl.component`.\n**Libraries**: The pipeline utilizes `kfp.dsl` for component and pipeline definition."
  },
  {
    "repo": "references_kfp_files",
    "file": "tonouchi510_kfp-project_pipelines_optuna-pipeline_optuna-pipeline.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `optuna pipeline` that performs hyperparameter optimization and sends notifications.\n\nThe pipeline consists of **two** main components:\n1.  **`optuna-worker`**: This component executes the core hyperparameter optimization logic using the Optuna library.\n    *   **Inputs**:\n        *   `pipeline_name` (string, default: \"optuna-pipeline\"): Name of the current pipeline.\n        *   `bucket_name` (string, default: \"\"): GCS bucket name for storing artifacts.\n        *   `job_id` (string, default: \"{{JOB_ID}}\"): Unique identifier for the current job run.\n        *   `n_trials` (integer, default: 100): Number of optimization trials to run.\n        *   `n_jobs` (integer, default: 5): Number of parallel jobs for Optuna.\n        *   `training_pipeline_name` (string, default: \"head-pose-pipeline\"): Name of the training pipeline to be optimized.\n        *   `dataset` (string, default: \"\"): Identifier for the dataset being used.\n        *   `epochs` (integer, default: 5): Number of training epochs per trial.\n    *   **Function**: Orchestrates hyperparameter optimization studies using Optuna, likely by calling a separate training pipeline for each trial.\n    *   **Resource Configuration**:\n        *   Runs on a node pool with the label `\"cloud.google.com/gke-nodepool\": \"cpu-pool\"`.\n        *   Includes a `cloudsqlproxy` sidecar container (image: `gcr.io/cloudsql-docker/gce-proxy:1.14`) to connect to a Cloud SQL instance. The sidecar command connects to a MySQL instance specified by environment variables `GCP_PROJECT`, `GCP_REGION`, and `DB_NAME`.\n        *   Configured to retry up to 2 times in case of failure.\n\n2.  **`slack-notification`**: This component sends a notification to Slack.\n    *   **Inputs**:\n        *   `pipeline_name` (string): Name of the pipeline.\n        *   `job_id` (string): Identifier of the job.\n        *   `message` (string, default: \"Status: {{workflow.status}}\"): The message to be sent, including the workflow status.\n    *   **Function**: Provides status updates or alerts to a Slack channel.\n    *   **Resource Configuration**:\n        *   Runs on a node pool with the label `\"cloud.google.com/gke-nodepool\": \"main-pool\"`.\n\n**Control Flow**:\nThe pipeline uses an `dsl.ExitHandler`. The `slack-notification` component is executed as the `exit_op`, meaning it will run **after** the `optuna-worker` component completes, regardless of success or failure. The `optuna-worker` component is the main execution step within the pipeline's context.\n\nThe pipeline is compiled into `optuna-pipeline.yaml`."
  },
  {
    "repo": "references_kfp_files",
    "file": "google_vertex-pipelines-boilerplate_src_pipelines_sample_pipeline.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `sample-pipeline` that performs the following steps:\n\nThe pipeline consists of a single component:\n1.  **`_save_message_to_file`**: This component takes two string inputs: `message` and `gcs_filepath`. Its function is to save the provided `message` content to the specified file path in Google Cloud Storage (`gcs_filepath`). It uses `python:3.10` as its base image and requires the `cloudpathlib==0.10.0` package to interact with GCS.\n\nThe pipeline's control flow is straightforward:\n-   The `_save_message_to_file` component is executed directly with the `message` and `gcs_filepath` parameters passed to the pipeline.\n\nThe pipeline definition includes `kfp` and `kfp.v2.dsl` for pipeline and component creation. The `cloudpathlib` library is used within the component for GCS interactions."
  },
  {
    "repo": "references_kfp_files",
    "file": "pavan-kumar-99_ml-ops_components_train-plot.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `train-plot-pipeline` that performs the following steps:\n\nThe pipeline will consist of two components:\n\n1.  **`traintest_op`**: This component is loaded from an external URL (`https://raw.githubusercontent.com/pavan-kumar-99/ml-ops/master/components/test-train.yaml`). Its function is to split a dataset into training and testing sets.\n\n2.  **`TrainPlot`**: This component is a Python function-based component that uses `sklearn` for model training and evaluation, and `matplotlib` for plotting.\n    *   **Functionality**:\n        *   It initializes and trains a `LinearRegression` model using `X_train` and `y_train`.\n        *   It makes predictions on `X_test` to get `y_pred`.\n        *   It generates two scatter plots using `matplotlib.pyplot`:\n            *   \"Salary vs Experience (Training set)\" showing `X_train` vs `y_train` and the regression line.\n            *   \"Salary vs Experience (Test set)\" showing `X_test` vs `y_test` and the regression line (using the training set's regression line for comparison).\n        *   It calculates several regression metrics: Mean Squared Error (MSE), Mean Absolute Error (MAE), and Explained Variance Score (EVS) using `sklearn.metrics`.\n        *   It prints `X_train`, a `params` variable (which is not explicitly defined in the provided snippet's `TrainPlot` but implied to be an input or global), and the calculated metrics.\n    *   **Inputs/Outputs**: The provided code snippet for `TrainPlot` implicitly expects `X_train`, `y_train`, `X_test`, and `y_test` to be available in its scope. These would typically be outputs from the preceding `traintest_op` component. The component itself does not explicitly define formal KFP inputs/outputs in the provided `create_component_from_func` decorator usage.\n    *   **Tools/Libraries**: `sklearn` (for `LinearRegression`, `f1_score`, `precision_score`, `recall_score`, `confusion_matrix`, `mean_squared_error`, `mean_absolute_error`, `explained_variance_score`), `matplotlib.pyplot`.\n\n**Control Flow**:\n\n*   The `TrainPlot` component should run `after` the `traintest_op` component has successfully completed, implicitly receiving the training and testing data splits as inputs from `traintest_op`.\n\nThe pipeline should be defined using the `@dsl.pipeline` decorator, and the Python components should be defined using `kfp.components.create_component_from_func` or by loading external component definitions."
  },
  {
    "repo": "references_kfp_files",
    "file": "speg03_kfp-toolbox_tests_test_pipeline_parser.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `echo-pipeline` that performs the following steps:\n\nThe pipeline has a single component.\n\n**Components:**\n\n1.  **`echo` component:**\n    *   **Function:** This component is designed to return a simple string, \"hello, world\".\n    *   **Outputs:** It returns a string value.\n\n**Pipeline Parameters:**\n\nThe pipeline accepts the following input parameters:\n\n*   `no_default_param`: An integer (int) with no default value.\n*   `int_param`: An integer (int) with a default value of `1`.\n*   `float_param`: A floating-point number (float) with a default value of `1.5`.\n*   `str_param`: A string (str) with a default value of `\"string_value\"`.\n*   `bool_param`: A boolean (bool) with a default value of `True`.\n*   `list_param`: A list of integers (`List[int]`) with a default value of `[1, 2, 3]`.\n*   `dict_param`: A dictionary with string keys and integer values (`Dict[str, int]`) with a default value of `{\"key\": 4}`.\n\n**Control Flow:**\n\n*   The pipeline simply calls the `echo` component. There are no dependencies or complex control flow mechanisms (like `after`, `parallelFor`, or conditional execution).\n\n**Libraries/Tools:**\n\n*   The pipeline utilizes the `kfp` library for defining components and pipelines.\n*   Standard Python `typing` module for type hints (e.g., `List`, `Dict`)."
  },
  {
    "repo": "references_kfp_files",
    "file": "AnsealArt_MLOps-Kubeflow_kubeflow-california-housing-pipeline_src_pipeline.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `California Housing prediction Pipeline` that performs a machine learning workflow for California Housing predictions using an SGDRegressor.\n\nThe pipeline consists of **four components**:\n\n1.  **`Download and preprocess data`**:\n    *   **Function**: Downloads the California Housing dataset, preprocesses it, and splits it into training and testing sets.\n    *   **Inputs**:\n        *   `output_path`: A string specifying the base output directory for generated files.\n        *   `run_date`: A string representing the current timestamp, used for organizing output files.\n        *   `test_size`: A float specifying the proportion of the dataset to be used as test data.\n    *   **Outputs**:\n        *   `x_train`: Path to the preprocessed training features (NumPy array).\n        *   `x_test`: Path to the preprocessed testing features (NumPy array).\n        *   `y_train`: Path to the preprocessed training labels (NumPy array).\n        *   `y_test`: Path to the preprocessed testing labels (NumPy array).\n    *   **Tool/Library**: Uses a custom Docker image `dzieciolfilipit/kf_ch_preprocess_data:1.1`.\n\n2.  **`Train SGD Regressor`**:\n    *   **Function**: Trains an SGD Regressor model using the preprocessed training data.\n    *   **Inputs**:\n        *   `x_train`: Path to the training features from the `preprocess` component.\n        *   `y_train`: Path to the training labels from the `preprocess` component.\n        *   `output_path`: A string specifying the base output directory.\n        *   `run_date`: A string representing the current timestamp.\n    *   **Outputs**:\n        *   `model_path`: Path to the trained SGD Regressor model (pickle file).\n    *   **Tool/Library**: Uses a custom Docker image `dzieciolfilipit/kf_ch_train_model:1.1` and likely relies on `scikit-learn` internally for SGDRegressor.\n\n3.  **`Test model and get MSE metric`**:\n    *   **Function**: Evaluates the trained model on the test dataset and calculates the Mean Squared Error (MSE).\n    *   **Inputs**:\n        *   `x_test`: Path to the testing features from the `preprocess` component.\n        *   `y_test`: Path to the testing labels from the `preprocess` component.\n        *   `model_path`: Path to the trained model from the `train` component.\n        *   `output_path`: A string specifying the base output directory.\n        *   `run_date`: A string representing the current timestamp.\n    *   **Outputs**:\n        *   `mean_squared_error`: Path to a text file containing the calculated MSE value.\n    *   **Tool/Library**: Uses a custom Docker image `dzieciolfilipit/kf_ch_test_model:1.1` and likely relies on `scikit-learn` internally for MSE calculation.\n\n4.  **`Deploy model for inference`**:\n    *   **Function**: Deploys the trained model for inference, potentially conditionally based on the calculated MSE.\n    *   **Inputs**:\n        *   `model_path`: Path to the trained model from the `train` component.\n        *   `mse_path`: Path to the Mean Squared Error metric from the `test` component.\n    *   **Outputs**: None explicitly defined in the provided snippet.\n    *   **Tool/Library**: Uses a custom Docker image `dzieciolfilipit/kf_ch_deploy_model:1.1`.\n\n**Control Flow**:\n\n*   The `Download and preprocess data` component runs first.\n*   The `Train SGD Regressor` component depends on and runs **after** `Download and preprocess data`. It consumes `x_train` and `y_train` produced by the preprocessing step.\n*   The `Test model and get MSE metric` component depends on and runs **after** `Train SGD Regressor`. It consumes `x_test` and `y_test` from preprocessing and the `model_path` from training.\n*   The `Deploy model for inference` component depends on and runs **after** both `Train SGD Regressor` (for `model_path`) and `Test model and get MSE metric` (for `mse_path`).\n\n**Pipeline Parameters**:\n\nThe pipeline accepts the following input parameters:\n*   `test_size`: A float, likely for the data split.\n*   `output_path`: A string, for output file storage.\n*   `deployment_threshhold_mse`: A float, likely a threshold for conditional deployment.\n\nThe pipeline uses `datetime.now().strftime(\"%Y%m%d%H%M%S\")` to generate a `run_date` string internally for organizing outputs."
  },
  {
    "repo": "references_kfp_files",
    "file": "03sarath_Kubeflow-pipelines-mlops_pipeline.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `First Pipeline` that performs classification using Decision Tree and Logistic Regression.\n\nThis pipeline consists of **four** components:\n1.  **`download`**: This component is loaded from the YAML manifest `download_data/download_data.yaml`. Its function is to download the necessary dataset. It takes no explicit inputs and produces a single output which is passed to subsequent components.\n2.  **`decision_tree`**: This component is loaded from the YAML manifest `decision_tree/decision_tree.yaml`. Its function is to train and evaluate a Decision Tree model. It takes the output of the `download` component as its input and produces a single output representing the accuracy (a float value) of the Decision Tree model.\n3.  **`logistic_regression`**: This component is loaded from the YAML manifest `logistic_regression/logistic_regression.yaml`. Its function is to train and evaluate a Logistic Regression model. It takes the output of the `download` component as its input and produces a single output representing the accuracy (a float value) of the Logistic Regression model.\n4.  **`show_results`**: This is a custom Python-based component (defined using `@func_to_container_op`). Its function is to display the accuracy results from both the Decision Tree and Logistic Regression models. It takes two float inputs: `decision_tree` (the accuracy from the `decision_tree` component) and `logistic_regression` (the accuracy from the `logistic_regression` component). It prints these values to standard output.\n\nThe control flow of the pipeline is as follows:\n- The `download` component runs first.\n- Once the `download` component completes, the `decision_tree` and `logistic_regression` components run **in parallel**, both taking the output of `download` as their input.\n- After both `decision_tree` and `logistic_regression` components have completed, the `show_results` component runs, taking the respective accuracy outputs from `decision_tree` and `logistic_regression` as its inputs.\n\nThe pipeline utilizes the `kfp` library for defining and compiling the pipeline. It loads external component definitions from YAML files."
  },
  {
    "repo": "references_kfp_files",
    "file": "xxxtrillionarie_KubeFlow_MLOps_Pipelines_samples_test_after.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `pipeline-with-after` that performs the following steps:\n\nThis pipeline consists of **three** components, all of which use the same reusable `Print Text` component.\n\nHere's the detailed breakdown of each component and the pipeline's control flow:\n\n**Reusable Component Definition:**\nFirst, define a reusable Kubeflow component named `Print Text`.\n*   **Function:** This component takes a single string input named `text` and prints it to standard output.\n*   **Implementation:** It uses an `alpine` container and executes a shell command `echo \"$0\"` where `$0` is replaced by the input `text`.\n\n**Pipeline Components and Control Flow:**\n\n1.  **`task1`**\n    *   **Function:** This task uses the `Print Text` component to print the string '1st task'.\n    *   **Inputs:** `text='1st task'`\n    *   **Dependencies:** None (it runs first).\n\n2.  **`task2`**\n    *   **Function:** This task uses the `Print Text` component to print the string '2nd task'.\n    *   **Inputs:** `text='2nd task'`\n    *   **Dependencies:** It must run **after** `task1` completes successfully.\n\n3.  **`task3`**\n    *   **Function:** This task uses the `Print Text` component to print the string '3rd task'.\n    *   **Inputs:** `text='3rd task'`\n    *   **Dependencies:** It must run **after** both `task1` and `task2` complete successfully.\n\nThe pipeline uses the `@dsl.pipeline` and `@component` decorators from the `kfp` library. The `compiler.Compiler()` is used to compile the pipeline function `my_pipeline` into a Kubeflow pipeline package, specifying `dummy_root` as the pipeline root and saving the output to a JSON file."
  },
  {
    "repo": "references_kfp_files",
    "file": "9rince_kfp_samples_test_legacy_data_passing.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `data_passing_pipeline` that demonstrates various data passing mechanisms.\n\nThe pipeline comprises **13 components**:\n\n**Producer Components:**\n1.  **`produce_anything`**: This component writes the string \"produce_anything\" to an `OutputPath()` named `data_path`.\n2.  **`produce_something`**: This component writes the string \"produce_something\" to an `OutputPath(\"Something\")` named `data_path`.\n3.  **`produce_something2`**: This component returns the string \"produce_something2\" as an output of type `Something`.\n4.  **`produce_string`**: This component returns the string \"produce_string\" as an output of type `str`.\n\n**Consumer Components (Consume as Value):**\n1.  **`consume_anything_as_value`**: This component takes an input `data` of unspecified type and prints its value.\n2.  **`consume_something_as_value`**: This component takes an input `data` of type `Something` and prints its value.\n3.  **`consume_string_as_value`**: This component takes an input `data` of type `str` and prints its value.\n\n**Consumer Components (Consume as File):**\n1.  **`consume_anything_as_file`**: This component takes an `InputPath()` named `data_path`, reads its content, and prints it.\n2.  **`consume_something_as_file`**: This component takes an `InputPath('Something')` named `data_path`, reads its content, and prints it.\n3.  **`consume_string_as_file`**: This component takes an `InputPath(str)` named `data_path`, reads its content, and prints it.\n\n**Pipeline Structure and Data Flow:**\n\nThe pipeline accepts three parameters:\n*   `anything_param` (default: \"anything_param\")\n*   `something_param` of type `Something` (default: \"something_param\")\n*   `string_param` of type `str` (default: \"string_param\")\n\nThe pipeline performs the following steps:\n\n**1. Producer Task Instantiation:**\n*   A task `produced_anything` is created from the `produce_anything` component. Its output is captured.\n*   A task `produced_something` is created from the `produce_something` component. Its output is captured.\n*   The code snippet provided is incomplete for `produce_something2` and `produce_string`. Assuming they are also instantiated and their outputs captured for subsequent consumption.\n\n**2. Data Passing Demonstrations (Incomplete from provided code, but based on the problem description):**\n\nThe prompt should complete the logic for all 6 data passing scenarios mentioned in the original problem description. For example, for the \"Consume as Value\" and \"Consume as File\" components, the prompts would describe how they are called with:\n\n*   Constant values\n*   Pipeline parameters\n*   Outputs from upstream producer components\n\n**Control Flow:**\nAll tasks are expected to run in parallel by default, with data dependencies dictating the execution order (e.g., a consumer task will only run after its required producer task has completed). There are no explicit `after` or `parallelFor` constructs mentioned in the provided snippet.\n\n**Libraries/Tools:**\nThe pipeline uses `kfp` for pipeline definition and component creation, and standard Python file I/O operations."
  },
  {
    "repo": "references_kfp_files",
    "file": "kubeflow_pipelines_samples_v2_parallel_consume_upstream.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `loop_consume_upstream` that performs the following steps:\n\nThis pipeline consists of **four** components: `split_input`, `create_file`, `read_file`, and `print_input`.\n\n**Component Breakdown:**\n\n1.  **`split_input`**:\n    *   **Function:** Takes a comma-separated string `input` and returns a `list` of strings by splitting the input.\n    *   **Inputs:** `input` (string).\n    *   **Outputs:** A `list` of strings.\n\n2.  **`create_file`**:\n    *   **Function:** Creates a file at the specified `Output[Artifact]` path and writes the `content` string into it.\n    *   **Inputs:** `file` (Output[Artifact]), `content` (string).\n    *   **Outputs:** Writes to the provided `Output[Artifact]`.\n\n3.  **`read_file`**:\n    *   **Function:** Reads the content of an `Input[Artifact]` file, prints its content to standard output, and then returns the file's path as a string.\n    *   **Inputs:** `file` (Input[Artifact]).\n    *   **Outputs:** A string (the file path).\n\n4.  **`print_input`**:\n    *   **Function:** Iterates through a given `list` of strings and prints each item prefixed with \"Input item: \".\n    *   **Inputs:** `input` (list).\n    *   **Outputs:** None (prints to stdout).\n\n**Pipeline Control Flow:**\n\n1.  The pipeline starts by invoking the `split_input` component, providing the string `'component1,component2,component3'` as its `input`. This operation is named `model_ids_split_op`.\n2.  `model_ids_split_op` has caching disabled and its display name is set to `'same display mame'`.\n3.  A `dsl.ParallelFor` loop is initiated, iterating over the `output` of `model_ids_split_op`. Each item from this list is assigned to the loop variable `model_id`.\n4.  Inside the `ParallelFor` loop:\n    *   The `create_file` component is called, providing the current `model_id` from the loop as its `content`. This operation is named `create_file_op`.\n    *   `create_file_op` has caching disabled and its display name is set to `'same display name'`.\n    *   The `read_file` component is then called, taking the `file` output from the `create_file_op` as its input. This demonstrates consuming an output from an upstream component within the same loop iteration. This operation is named `read_file_op`.\n    *   `read_file_op` has caching disabled and its display name is set to `'same display name'`.\n5.  After the `ParallelFor` loop completes (i.e., `print_input_op` runs after the entire parallel execution), the `print_input` component is called. It takes the original `output` from the `model_ids_split_op` (which was the input to the `ParallelFor` loop) as its `input`. This operation is named `print_input_op`.\n6.  `print_input_op` has caching disabled and its display name is set to `'same display name'`.\n\n**Libraries Used:**\n\n*   `kfp` (Kubeflow Pipelines SDK)\n*   `kfp.dsl` (for defining components, pipelines, and types like `Artifact`, `Input`, `Output`, `ParallelFor`)."
  },
  {
    "repo": "references_kfp_files",
    "file": "stackdemos_secrets-tetst-01_components_training_component.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `stackdemos_secrets-tetst-01_training_pipeline` that performs the following steps:\n\nThis pipeline consists of **three** components.\n\n1.  **`training_op`**: This component is a generic container operation designed to execute Python training scripts.\n    *   **Inputs**:\n        *   `script`: (Required) The path to the Python script to be executed.\n        *   `image`: (Optional) The Docker image to use for the container. If not provided and running in an IPython environment, it attempts to use a global `TRAINING_IMAGE` variable.\n        *   `arguments`: (Optional, list) A list of command-line arguments to pass to the script.\n        *   `file_outputs`: (Optional, dictionary) A dictionary specifying file paths within the container that should be captured as output artifacts.\n    *   **Function**: It creates a `ContainerOp` with a sanitized name derived from the script name, uses the specified image, and executes the Python script with provided arguments.\n    *   **Tools/Libraries**: Uses `os` and `re` for path and string manipulation.\n\n2.  **`http_download_op`**: This component downloads a file from a URL and performs an MD5 checksum verification.\n    *   **Inputs**:\n        *   `url`: (Required) The URL of the file to download.\n        *   `download_to`: (Required) The local path where the file should be downloaded.\n        *   `md5sum`: (Required) The expected MD5 checksum of the downloaded file.\n    *   **Function**: It uses a `curl` command within an `appropriate/curl` Docker image to download the file. Before downloading, it checks if a file with the same MD5 sum already exists at `download_to` and skips the download if it does. If not, it downloads the file and creates necessary directories.\n    *   **Tools/Libraries**: Uses `curl` and `md5sum` (via `awk`).\n\n3.  **`_is_ipython`**: This is a helper function, not a direct Kubeflow component.\n    *   **Function**: It checks if the current execution environment is an IPython notebook.\n    *   **Tools/Libraries**: Uses `IPython`.\n\n**Control Flow**:\nThe provided code defines the individual components (`training_op`, `http_download_op`) and a helper function (`_is_ipython`). The pipeline definition itself (i.e., how these components are orchestrated with `after`, `parallelFor`, etc.) is not present in this snippet. The prompt assumes these are callable functions that would be used within an `@dsl.pipeline` decorated function to construct the workflow. The pipeline's name is inferred from the file path."
  },
  {
    "repo": "references_kfp_files",
    "file": "9rince_kfp_samples_test_metrics_visualization_v1_test.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `metrics_visualization_v1_pipeline` that performs metrics visualization.\n\nThe pipeline consists of five components, all executed in parallel:\n\n1.  **`table-visualization`**: This component likely generates a table for visualization. Its output is an artifact named `mlpipeline_ui_metadata`.\n\n2.  **`markdown-visualization`**: This component likely generates Markdown content for visualization. Its output is an artifact named `mlpipeline_ui_metadata`.\n\n3.  **`roc-visualization`**: This component is responsible for visualizing a Receiver Operating Characteristic (ROC) curve. Its output is an artifact named `mlpipeline_ui_metadata`.\n\n4.  **`html-visualization`**: This component generates HTML content for visualization. Its output is an artifact named `mlpipeline_ui_metadata`.\n\n5.  **`confusion-visualization`**: This component visualizes a confusion matrix. Its output is an artifact named `mlpipeline_ui_metadata`.\n\nAll five components appear to operate independently and in parallel, with no explicit dependencies or control flow specified between them beyond their individual execution. The common output artifact name `mlpipeline_ui_metadata` suggests that each component contributes specific UI metadata for display in the Kubeflow Pipelines UI."
  },
  {
    "repo": "references_kfp_files",
    "file": "kubeflow_pipelines_samples_test_fail_v2.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `fail-pipeline` that performs the following:\n\nThis pipeline consists of a single component:\n1.  **`fail`**: This component's function is to simulate a failure by immediately exiting with a non-zero status code (specifically, `sys.exit(1)`). It does not take any inputs or produce any outputs.\n\nThe control flow is straightforward:\n- The `fail_task` is an instance of the `fail` component and is executed directly within the pipeline.\n\nNo specific external tools or libraries beyond standard Python `sys` module for exiting are used within the component logic visible in the definition."
  },
  {
    "repo": "references_kfp_files",
    "file": "redhat-ai-services_kubeflow-pipelines-examples_pipelines_04_artifact_pipeline.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `Artifact Pipeline` that performs the following steps:\n\nThis pipeline consists of **two components**:\n\n1.  **`create_artifact`**:\n    *   **Function**: This component creates a Python `list` (represented as a string \"1, 2, 3, 4\") and serializes it using `pickle`.\n    *   **Outputs**: It produces one output: `my_artifact` of type `dsl.Output[dsl.Artifact]`, which stores the pickled data.\n    *   **Tools/Libraries**: Uses the `pickle` library for serialization.\n    *   **Base Image**: `image-registry.openshift-image-registry.svc:5000/openshift/python:latest`.\n\n2.  **`consume_artifact`**:\n    *   **Function**: This component reads and deserializes the artifact created by the `create_artifact` component, then prints its content.\n    *   **Inputs**: It consumes one input: `my_artifact` of type `dsl.Input[dsl.Artifact]`.\n    *   **Tools/Libraries**: Uses the `pickle` library for deserialization.\n    *   **Base Image**: `image-registry.openshift-image-registry.svc:5000/openshift/python:latest`.\n\n**Control Flow**:\n*   The `create_artifact` component runs first.\n*   The `consume_artifact` component runs *after* `create_artifact` and depends on its `my_artifact` output. Specifically, `consume_artifact` takes `create_artifact_task.outputs[\"my_artifact\"]` as its input."
  },
  {
    "repo": "references_kfp_files",
    "file": "Mastercard_mastercard-labs-ml-pipeline_santander-trnx-classification_release.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `Santander Customer Transaction Prediction Release Pipeline` that performs the release of a trained classification model for Santander customer transactions.\n\nThe pipeline has a dynamic number of components, with a base of **2 or 3 components**, depending on the `platform` variable:\n\n**Pipeline Parameters:**\n* `output`: String, used for path configuration.\n* `project`: String, typically representing a GCP project ID or Kubernetes cluster name.\n\n**Control Flow and Component Details:**\n\n1.  **Platform Conditional Logic (`if platform != 'GCP'`):**\n    *   **If `platform` is NOT 'GCP' (e.g., On-premise Kubernetes):**\n        *   **`create_pvc` (dsl.VolumeOp):**\n            *   **Function:** Creates a Persistent Volume Claim (PVC) named \"pipeline-pvc\" with `RWM` (ReadWriteMany) mode and \"1Gi\" size.\n            *   **Outputs:** `name` (of the created PVC).\n        *   **`checkout` (dsl.ContainerOp):**\n            *   **Function:** Clones the `https://github.com/kubeflow/pipelines.git` repository into a directory specified by the `output` pipeline parameter (e.g., `/pipelines`).\n            *   **Image:** `alpine/git:latest`\n            *   **Command:** `[\"git\", \"clone\", \"https://github.com/kubeflow/pipelines.git\", str(output) + \"/pipelines\"]`\n            *   **Dependencies:** Runs `after` `create_pvc`.\n            *   **Volume Mounts:** Mounts the PVC created by `create_pvc` at `local-storage` path, corresponding to the `output` parameter.\n\n2.  **`kubeflow_deploy_op` (Loaded Component):**\n    *   **Function:** Deploys the machine learning model to a serving infrastructure (likely TensorFlow Serving).\n    *   **Component Definition Source:** Loaded from `pipeline_steps/serving/deployer/component.yaml`.\n    *   **Dynamic Inputs based on `platform`:**\n        *   **If `platform` is 'GCP':**\n            *   `model_dir`: Set to `gs://kubeflow-pipelines-demo/tfx/0b22081a-ed94-11e9-81fb-42010a800160/santander-customer-transaction-prediction-95qxr-268134926/data/export/export`\n            *   `server_name`: Set to `kfdemo-service`\n        *   **If `platform` is NOT 'GCP':**\n            *   `cluster_name`: Set to the `project` pipeline parameter.\n            *   `model_dir`: Set to `gs://kubeflow-pipelines-demo/tfx/0b22081a-ed94-11e9-81fb-42010a800160/santander-customer-transaction-prediction-95qxr-268134926/data/export/export`\n            *   `pvc_name`: Set to the `name` output of the `create_pvc` component.\n            *   `server_name`: Set to `kfdemo-service`\n\n3.  **`webapp` (dsl.ContainerOp):**\n    *   **Function:** Launches a web application.\n    *   **Image:** `us.gcr.io/kf-pipelines/ml-pipeline-webapp-launcher:v0.3`\n    *   **Arguments:** `[\"--model_name\", \"santanderapp\"]`\n    *   **Dependencies:** Runs `after` `kubeflow_deploy_op`.\n\n**Post-Component Execution (Loop for `deploy` and `webapp`):**\n\n*   For both the `deploy` and `webapp` components:\n    *   **If `platform` is 'GCP':** Applies `gcp.use_gcp_secret('user-gcp-sa')` to enable GCP service account authentication.\n    *   **If `platform` is NOT 'GCP':** Applies `onprem.mount_pvc(vop.outputs[\"name\"], 'local-storage', output)` to mount the previously created PVC.\n\n**Libraries/Frameworks Used:**\n*   Kubeflow Pipelines (kfp, dsl, components)\n*   Google Cloud Platform (gcp) utilities for Kubeflow\n*   On-premise (onprem) utilities for Kubeflow\n*   Git (alpine/git image)\n*   A custom component defined in `component.yaml` for model deployment.\n*   Potentially TensorFlow Serving (implied by `tf_server_name` and model export paths)."
  },
  {
    "repo": "references_kfp_files",
    "file": "Ark-kun_pipeline_components_samples_core_continue_training_from_prod_continue_training_from_prod.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `continuous_training_pipeline` that performs a continuous training scenario for a machine learning model, specifically addressing the training of new models starting from a production model if one exists.\n\nThe pipeline takes the following parameters:\n- `model_dir_uri` (str): GCS URI of a directory where models and model pointers should be stored.\n- `training_start_date` (str, default: '2019-02-01'): Start date for the training dataset.\n- `training_end_date` (str, default: '2019-03-01'): End date for the training dataset.\n- `testing_start_date` (str, default: '2019-01-01'): Start date for the testing dataset.\n- `testing_end_date` (str, default: '2019-02-01'): End date for the testing dataset.\n\nThe pipeline consists of the following components, connected sequentially to form two main branches for training and evaluation:\n\n**Data Preparation Branch:**\n\n1.  **`training_data`**: Uses `chicago_taxi_dataset_op` to fetch training data.\n    -   **Inputs**: `where` (based on `training_start_date` and `training_end_date`).\n    -   **Outputs**: Training dataset (CSV).\n\n2.  **`testing_data`**: Uses `chicago_taxi_dataset_op` to fetch testing data.\n    -   **Inputs**: `where` (based on `testing_start_date` and `testing_end_date`).\n    -   **Outputs**: Testing dataset (CSV).\n\n3.  **`training_data_for_xgboost`**: Uses `pandas_transform_csv_op` to transform the training data.\n    -   **Inputs**: `table` (from `training_data.output`), `transform_code` (Python code for data transformation, including feature selection and one-hot encoding).\n    -   **Outputs**: Transformed training data.\n\n4.  **`testing_data_for_xgboost`**: Uses `pandas_transform_csv_op` to transform the testing data.\n    -   **Inputs**: `table` (from `testing_data.output`), `transform_code` (same Python code as for `training_data_for_xgboost`).\n    -   **Outputs**: Transformed testing data.\n\n5.  **`test_data_no_header`**: Uses `drop_header_op` to remove the header from the transformed testing data.\n    -   **Inputs**: `table` (from `testing_data_for_xgboost.transformed_table`).\n    -   **Outputs**: Testing data without header.\n\n**Model Training and Evaluation Branch:**\n\nThis branch is designed to first check for an existing production model and warm-start training if one is found.\n\n1.  **`download_prod_model_pointer`**: Uses `download_from_gcs_op` to attempt downloading the production model pointer.\n    -   **Inputs**: `uri` (constructed from `model_dir_uri` and a fixed filename `prod_model.txt`).\n    -   **Outputs**: Production model pointer content.\n    -   **Control Flow**: This component is optional and its failure is handled.\n\n2.  **`download_prod_model`**: Uses `download_from_gcs_op` to download the actual production model if the pointer was successfully downloaded.\n    -   **Inputs**: `uri` (from `download_prod_model_pointer.output`).\n    -   **Outputs**: Production model artifact.\n    -   **Control Flow**: `after(download_prod_model_pointer)`. This component is optional.\n\n3.  **`train_model`**: Uses `xgboost_train_on_csv_op` to train an XGBoost model.\n    -   **Inputs**:\n        -   `training_data` (from `training_data_for_xgboost.transformed_table`).\n        -   `label_column` ('target').\n        -   `model_artifact` (optional, from `download_prod_model.output` if available, for warm-starting).\n    -   **Outputs**: Trained XGBoost model.\n\n4.  **`predict_test_data`**: Uses `xgboost_predict_on_csv_op` to make predictions on the testing data.\n    -   **Inputs**:\n        -   `model` (from `train_model.model`).\n        -   `data` (from `test_data_no_header.output`).\n    -   **Outputs**: Predictions (CSV).\n\n5.  **`calculate_metrics`**: Uses `calculate_regression_metrics_from_csv_op` to calculate regression metrics.\n    -   **Inputs**:\n        -   `predictions` (from `predict_test_data.predictions`).\n        -   `targets` (from `testing_data_for_xgboost.transformed_table` with `target` column selected).\n    -   **Outputs**: Regression metrics (e.g., `mean_absolute_error`).\n\n6.  **`upload_model_to_unique_uri`**: Uses `upload_to_gcs_unique_op` to upload the newly trained model to a unique GCS URI.\n    -   **Inputs**: `source` (from `train_model.model`), `uri_prefix` (constructed from `model_dir_uri` and 'models/').\n    -   **Outputs**: Unique GCS URI of the uploaded model.\n\n7.  **`upload_model_to_prod`**: Uses `upload_to_gcs_op` to set the newly trained model as the production model by updating the `prod_model.txt` pointer.\n    -   **Inputs**:\n        -   `source` (from `upload_model_to_unique_uri.output`).\n        -   `uri` (constructed from `model_dir_uri` and 'prod_model.txt').\n    -   **Control Flow**: `after(calculate_metrics)`. This step implies that the model is only promoted to production after successful evaluation.\n\n**External Component Libraries Used:**\n\nThe pipeline leverages several external Kubeflow components loaded from URLs, including:\n-   `chicago_taxi_dataset_op` for dataset retrieval.\n-   `xgboost_train_on_csv_op` and `xgboost_predict_on_csv_op` for XGBoost model operations.\n-   `pandas_transform_csv_op` for data transformation using Pandas.\n-   `drop_header_op` for table manipulation.\n-   `calculate_regression_metrics_from_csv_op` for metric calculation.\n-   `download_from_gcs_op`, `upload_to_gcs_op`, and `upload_to_gcs_unique_op` for Google Cloud Storage interactions.\n\nThe pipeline ensures that training can be warm-started from an existing production model, and a newly trained model is promoted to production only after its performance is evaluated."
  },
  {
    "repo": "references_kfp_files",
    "file": "redhat-ai-services_kubeflow-pipelines-examples_pipelines_06_visualization_pipeline.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `Metadata Example Pipeline` that performs a demonstration of Kubeflow UI visualizations.\n\nThis pipeline consists of **one component**:\n\n1.  **`confusion_matrix_viz`**:\n    *   **Functionality**: This component simulates the generation of a confusion matrix. It creates a hardcoded confusion matrix in CSV format and writes it to an output path. Crucially, it also generates `mlpipeline-ui-metadata.json` which contains the necessary metadata for Kubeflow UI to render a confusion matrix visualization based on the generated CSV.\n    *   **Inputs**: None.\n    *   **Outputs**:\n        *   `mlpipeline_ui_metadata_path` (type: `dsl.OutputPath`): The path where the UI metadata JSON file will be stored. This file is essential for rendering the visualization in the Kubeflow UI.\n        *   `confusion_matrix_path` (type: `dsl.OutputPath`): The path where the confusion matrix data (CSV string) will be stored.\n    *   **Libraries/Tools Used**: `json` for creating the UI metadata.\n    *   **Base Image**: `image-registry.openshift-image-registry.svc:5000/openshift/python:latest`.\n\n**Control Flow**:\nThe `visualization_pipeline` directly invokes the `confusion_matrix_viz` component. There are no explicit dependencies or complex control flow structures (like `parallelFor` or `after`) as it's a single-step pipeline."
  },
  {
    "repo": "references_kfp_files",
    "file": "iQuantC_Kubeflow-pipeline_kubeflow_pipeline.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `IrisClassificationPipeline` that performs a machine learning workflow for classifying the Iris dataset.\n\nThe pipeline consists of **four** components:\n\n1.  **`load_data`**:\n    *   **Function**: Loads the Iris dataset using `sklearn.datasets.load_iris`, converts it into a pandas DataFrame, and saves it to a CSV file.\n    *   **Outputs**:\n        *   `output_csv`: A `Dataset` artifact containing the raw Iris data.\n    *   **Libraries**: `pandas`, `scikit-learn`.\n\n2.  **`preprocess_data`**:\n    *   **Function**: Loads the raw dataset, handles any missing values (by dropping rows), standardizes features using `StandardScaler`, and splits the data into training and testing sets (80% train, 20% test) using `train_test_split` with `random_state=42`. It includes debug print statements and assertions for data validation.\n    *   **Inputs**:\n        *   `input_csv`: A `Dataset` artifact representing the raw Iris data from `load_data`.\n    *   **Outputs**:\n        *   `output_train`: A `Dataset` artifact containing the preprocessed training features (X_train).\n        *   `output_test`: A `Dataset` artifact containing the preprocessed testing features (X_test).\n        *   `output_ytrain`: A `Dataset` artifact containing the training labels (y_train).\n        *   `output_ytest`: A `Dataset` artifact containing the testing labels (y_test).\n    *   **Libraries**: `pandas`, `scikit-learn`.\n\n3.  **`train_model`**:\n    *   **Function**: Trains a machine learning model using the preprocessed training data. The specific model type is not specified but it should accept features and labels for training.\n    *   **Inputs**:\n        *   `input_train`: A `Dataset` artifact containing the training features (X_train) from `preprocess_data`.\n        *   `input_ytrain`: A `Dataset` artifact containing the training labels (y_train) from `preprocess_data`.\n    *   **Outputs**:\n        *   `output_model`: A `Model` artifact representing the trained machine learning model.\n    *   **Libraries**: `pandas`, `scikit-learn`.\n\n4.  **`evaluate_model`**:\n    *   **Function**: Evaluates the trained model using the preprocessed testing data and outputs evaluation metrics.\n    *   **Inputs**:\n        *   `input_model`: A `Model` artifact representing the trained model from `train_model`.\n        *   `input_test`: A `Dataset` artifact containing the testing features (X_test) from `preprocess_data`.\n        *   `input_ytest`: A `Dataset` artifact containing the testing labels (y_test) from `preprocess_data`.\n    *   **Outputs**:\n        *   `output_metrics`: A `ClassificationMetrics` artifact containing the evaluation metrics (e.g., accuracy, precision, recall, F1-score).\n    *   **Libraries**: `pandas`, `scikit-learn`.\n\n**Control Flow**:\n*   `load_data` is the initial step and runs first.\n*   `preprocess_data` runs *after* `load_data` completes and uses its output (`output_csv`) as its input.\n*   `train_model` runs *after* `preprocess_data` completes and uses its outputs (`output_train`, `output_ytrain`) as its inputs.\n*   `evaluate_model` runs *after* both `preprocess_data` and `train_model` complete. It depends on the trained model from `train_model` and the test data (`output_test`, `output_ytest`) from `preprocess_data`.\n\nEach component should use `python:3.9` as its `base_image` and install necessary libraries (`pandas`, `scikit-learn`) using `subprocess.run([\"pip\", \"install\", ...])` within the component's execution."
  },
  {
    "repo": "references_kfp_files",
    "file": "9rince_kfp_samples_test_legacy_exit_handler_test.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `download_and_print` that performs the following steps:\n\nThis pipeline consists of a single component.\n\nThe primary function of this pipeline is to demonstrate the use of an exit handler with a legacy Kubeflow Pipelines V1 execution mode.\n\n**Components:**\n\n1.  **`download_and_print`**: This is the main component and also serves as the pipeline's entry point. Its specific function is to download some content (implied by the name `download_and_print` which suggests fetching data) and then print it.\n    *   **Inputs**: The code snippet does not explicitly show any `dsl.Input` or `dsl.Parameter` for this component within the provided file. It's likely its inputs are defined internally within the `legacy_exit_handler.py` file or it operates without external inputs.\n    *   **Outputs**: The code snippet does not explicitly show any `dsl.Output` for this component. The \"print\" aspect suggests side effects (e.g., logging to stdout).\n\n**Control Flow:**\n\n*   The pipeline has a simple linear flow: the `download_and_print` component executes.\n*   The pipeline is configured to run in `kfp.dsl.PipelineExecutionMode.V1_LEGACY` mode.\n*   This pipeline is designed to include an implicit or explicit exit handler, which is a mechanism to execute a cleanup or finalization task regardless of whether the main pipeline run succeeds or fails. While the exit handler itself is not directly defined in this snippet, the context implies its presence and functionality, likely within the `legacy_exit_handler.py` module.\n\n**Libraries/Tools:**\n\n*   `kfp`: Used for defining the Kubeflow Pipeline and specifying its execution mode.\n*   The `download_and_print` component likely utilizes standard Python libraries for file downloading and printing. No external data science or machine learning libraries (e.g., sklearn, pandas) are explicitly indicated by this snippet."
  },
  {
    "repo": "references_kfp_files",
    "file": "9rince_kfp_samples_test_metrics_visualization_v2.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `MetricsVisualizationPipeline` that performs a series of machine learning classification tasks and visualizes their results.\n\nThe pipeline consists of two components:\n1.  **`digit_classification`**:\n    *   **Function:** This component trains and evaluates a Logistic Regression model on the Iris dataset. It performs cross-validation and calculates the accuracy on a test set.\n    *   **Outputs:** It logs a single metric named `accuracy` (a float value representing percentage accuracy) to an `Output[Metrics]` artifact.\n    *   **Libraries/Tools:** It uses `sklearn` for dataset loading, model selection, and metric calculation.\n    *   **Base Image:** `python:3.9`\n\n2.  **`wine_classification`**:\n    *   **Function:** This component trains and evaluates a RandomForestClassifier on the Wine dataset. It transforms the problem into a binary classification task and calculates ROC curve data.\n    *   **Outputs:** It logs classification metrics (specifically ROC curve data) to an `Output[ClassificationMetrics]` artifact.\n    *   **Libraries/Tools:** It uses `sklearn` for dataset loading, model training, and metric calculation.\n    *   **Base Image:** `python:3.9`\n\n**Control Flow:**\nBoth `digit_classification` and `wine_classification` components run in parallel within the pipeline. There are no explicit dependencies between them."
  },
  {
    "repo": "references_kfp_files",
    "file": "stackdemos_ml-kbf_components_training_component.py",
    "structured_prompt": "I apologize, but the provided Python code (`stackdemos_ml-kbf_components_training_component.py`) defines *Kubeflow component functions* (`training_op`, `http_download_op`) but **does not define a Kubeflow Pipeline** using `@dsl.pipeline`.\n\nTo generate a prompt for regenerating a Kubeflow Pipeline, I need to see the code that actually defines the pipeline structure, including:\n- `@dsl.pipeline` decorator and its `name`\n- `@component` decorated functions (if any, though the provided code uses `ContainerOp` directly)\n- How the components are called and linked together (e.g., `component_a.after(component_b)`)\n- Any pipeline inputs or outputs\n\n**If you can provide the Kubeflow Pipeline definition code, I will be happy to generate the requested prompt.**\n\n---\n\n**Example of what I'd need to see (conceptual):**\n\n```python\nfrom kfp import dsl\nfrom kfp.dsl import ContainerOp # Assuming training_op and http_download_op are available\n\n# Assume training_op and http_download_op are defined as in your original snippet\n\n@dsl.pipeline(\n    name='MyAwesomePipeline',\n    description='A pipeline to download data and train a model.'\n)\ndef my_awesome_pipeline(\n    data_url: str = 'http://example.com/data.csv',\n    data_md5: str = 'abc123def456',\n    model_script: str = 'train.py',\n    training_image: str = 'my-training-image:latest'\n):\n    download_task = http_download_op(\n        url=data_url,\n        download_to='/data/input.csv',\n        md5sum=data_md5\n    )\n\n    train_task = training_op(\n        script=model_script,\n        image=training_image,\n        arguments=[\n            '--input-path', download_task.output, # Example: passing output\n            '--output-model', '/model/output.pkl'\n        ],\n        file_outputs={'trained_model': '/model/output.pkl'}\n    )\n\n    # ... other tasks\n```"
  },
  {
    "repo": "references_kfp_files",
    "file": "s102401002_kubeflowPipeline0722_src_hypertension_prediction.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `HypertensionPredictionPipeline` that performs a machine learning workflow for predicting hypertension.\n\nThis pipeline consists of **three** components:\n\n1.  **`load_data`**:\n    *   **Function:** Loads the `hypertension_data.csv` dataset from a specified URL, performs data cleaning, removes unnecessary columns (`cp`, `thal`), handles missing values by dropping rows with \"N/A\" or NaN in critical columns, and filters out `restecg` values equal to 2.\n    *   **Outputs:**\n        *   `data_output`: An `Artifact` containing the cleaned DataFrame.\n    *   **Tools/Libraries:** `pandas==2.2.2`.\n\n2.  **`prepare_data`**:\n    *   **Function:** Reads the cleaned data, splits it into features (X) and target (Y) arrays, and then further splits the data into training, testing, and validation sets using `train_test_split`. The initial split is 80% train, 20% test. The 20% test set is then split again into 50% test and 50% validation.\n    *   **Inputs:**\n        *   `data_input`: An `Input[Artifact]` representing the cleaned data from the `load_data` component.\n    *   **Outputs:**\n        *   `X_train_output`: An `Artifact` containing the training features.\n        *   `X_test_output`: An `Artifact` containing the testing features.\n        *   `Y_train_output`: An `Artifact` containing the training targets.\n        *   `Y_test_output`: An `Artifact` containing the testing targets.\n        *   `X_val_output`: An `Artifact` containing the validation features.\n        *   `Y_val_output`: An `Artifact` containing the validation targets.\n    *   **Tools/Libraries:** `pandas==2.2.2`, `scikit-learn==1.5.1`.\n    *   **Dependencies:** This component depends on the successful completion of the `load_data` component.\n\n3.  **`train_model`**: (This component is not fully provided in the snippet, but based on typical ML workflows, it would follow `prepare_data`.)\n    *   **Function:** (Assumed) Trains a machine learning model (e.g., Logistic Regression, RandomForest) using the training data (`X_train`, `Y_train`).\n    *   **Inputs:**\n        *   `X_train_input`: An `Input[Artifact]` for training features.\n        *   `Y_train_input`: An `Input[Artifact]` for training targets.\n    *   **Outputs:**\n        *   `model_output`: A `Model` artifact representing the trained model.\n    *   **Tools/Libraries:** (Assumed) `scikit-learn` and potentially other ML libraries.\n    *   **Dependencies:** This component depends on the successful completion of the `prepare_data` component.\n\n**Control Flow:**\nThe pipeline execution flow is sequential:\n*   `load_data` runs first.\n*   `prepare_data` runs after `load_data` completes, taking its output as input.\n*   (Assumed) `train_model` runs after `prepare_data` completes, taking its training data outputs as input.\n\nAll components should use `python:3.9` as their base image."
  },
  {
    "repo": "references_kfp_files",
    "file": "stackdemos_jj-test2_components_golang_component.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `stackdemos-jj-test2-components-golang-component` that performs the following:\n\nThis pipeline does not use the `@dsl.pipeline` decorator directly but defines two utility functions, `processing_op` and `http_download_op`, which are intended to be used within a Kubeflow pipeline definition. These functions create `ContainerOp` instances.\n\n**Components (as functions returning `ContainerOp`):**\n\n1.  **`processing_op`**:\n    *   **Function:** This is a templated function designed to encapsulate similar container operations. It creates a `ContainerOp` with a dynamically generated name based on the script name, uses a specified Docker image (which can be derived from an IPython environment variable `GOLANG_IMAGE`), executes a given script as its command, and accepts a list of arguments and file outputs.\n    *   **Inputs:**\n        *   `script` (string): The path to the script to be executed.\n        *   `image` (string, optional): The Docker image to use. If not provided and running in IPython, it attempts to get it from `IPython.get_ipython().user_ns.get('GOLANG_IMAGE')`.\n        *   `arguments` (list of strings, optional): Command-line arguments to pass to the script.\n        *   `file_outputs` (dict, optional): A dictionary mapping output names to file paths.\n    *   **Outputs:** Returns a `kfp.dsl.ContainerOp` instance.\n    *   **Logic:**\n        *   It checks if `image` is provided; if not, and it's running in an IPython environment, it tries to retrieve the image from the `GOLANG_IMAGE` user namespace variable.\n        *   If `image` is still missing, it raises a `ValueError`.\n        *   The `ContainerOp`'s name is derived from the `script` name by converting it to lowercase and replacing non-alphanumeric characters with hyphens.\n\n2.  **`http_download_op`**:\n    *   **Function:** This component performs an HTTP download of a specified URL to a target path, including an MD5 sum pre-check to avoid re-downloading.\n    *   **Inputs:**\n        *   `url` (string): The URL of the file to download.\n        *   `download_to` (string): The local path where the file should be downloaded.\n        *   `md5sum` (string): The expected MD5 checksum of the file.\n    *   **Outputs:** Returns a `kfp.dsl.ContainerOp` instance.\n    *   **Logic:**\n        *   Uses the `appropriate/curl` Docker image.\n        *   Executes a shell script (`sh -c`) that first checks if the existing file at `download_to` has the expected `md5sum`.\n        *   If the MD5 sums match, it prints a \"Skipping\" message.\n        *   Otherwise, it uses `curl` to download the file, creating directories as needed, and saving it to `download_to`.\n\n**Control Flow:**\nThese are utility functions, not a complete pipeline definition. Therefore, there are no explicit dependencies or control flow within these functions themselves. They are designed to be called sequentially or in parallel from a higher-level `@dsl.pipeline` function, where the actual `after()` or `parallelFor` constructs would be defined.\n\n**Tools/Libraries Used:**\n*   `kfp.dsl.ContainerOp` for defining container operations.\n*   `urllib.parse` for URL parsing (though not directly used in the provided `ContainerOp` definitions, `urlparse` is imported).\n*   `os` for operating system-dependent functionality (specifically `os.path.splitext`).\n*   `re` for regular expressions (specifically `re.sub` for naming).\n*   `IPython` for checking if running in a notebook environment and accessing user namespaces.\n*   `curl` command-line tool (within `http_download_op`)."
  },
  {
    "repo": "references_kfp_files",
    "file": "e2fyi_kfx_kfx_dsl__transformers.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `demo` that performs the following steps:\n\nThis pipeline consists of **two components**.\n\n1.  **Component `echo`**:\n    *   **Function**: Takes a string `text` as input, prints it to standard output, and returns the same string.\n    *   **Inputs**: `text` (string).\n    *   **Outputs**: A string output.\n    *   **Implementation**: This is a Python function decorated with `@kfp.dsl.components.func_to_container_op`.\n\n2.  **Pipeline `demo`**:\n    *   **Function**: Orchestrates the execution of the `echo` component.\n    *   **Inputs**: `text` (string).\n    *   **Control Flow**:\n        *   An instance of the `echo` component, named `op1`, is created and executed with the pipeline input `text`.\n        *   Another instance of the `echo` component, named `op2`, is created and executed with a formatted string `\"%s-%s\" % text` as its input.\n        *   A global `ContainerOpTransform` is applied to all components in the pipeline:\n            *   It sets resource limits: CPU to \"500m\", memory to \"1G\" (request) and \"4G\" (limit).\n            *   It sets the image pull policy to \"Always\".\n            *   It sets an environment variable `ENV` to \"production\".\n            *   It sets an environment variable `AWS_ACCESS_KEY` from a Kubernetes secret named \"aws\" with key \"access_key\".\n            *   It sets an annotation `iam.amazonaws.com/role` to \"some-arn\".\n    *   **Tools/Libraries**: `kfp` (Kubeflow Pipelines SDK), `kubernetes.client` (for Kubernetes resource specifications)."
  },
  {
    "repo": "references_kfp_files",
    "file": "redhat-ai-services_kubeflow-pipelines-examples_pipelines_07_container_components_pipeline.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `container-pipeline` that performs an addition operation.\n\nThis pipeline has 1 component:\n1.  **`add`**: This component is designed to perform an addition. It utilizes a custom container image `quay.io/rhiap/kubeflow-example:latest` and executes a Python script located at `components/add.py` within that container. This component doesn't explicitly expose inputs or outputs in the provided DSL definition, but its purpose is implied to perform an addition.\n\nThe pipeline accepts two floating-point parameters:\n-   `a`: Default value is `1.0`.\n-   `b`: Default value is `7.0`.\n\nThe control flow is sequential: the `add` component is the sole task in the pipeline.\n\nThis pipeline uses the `kfp.dsl.ContainerOp` function for defining container-based components. It also implicitly uses Python for the component's internal logic as indicated by the command arguments."
  },
  {
    "repo": "references_kfp_files",
    "file": "tuhinaprasad28_Machine-Learning-Pipeline-using-Kubeflow_code_rep_localHost_Kubeflow_Pipelines-master_pipeline.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `First Pipeline` that performs machine learning classification.\n\nThe pipeline consists of **four** components:\n1.  **`download`**: This component is loaded from the YAML manifest file `download_data/download_data.yaml`. Its function is to download the necessary data for the machine learning tasks. It takes no explicit inputs and produces an output, which is the downloaded data.\n2.  **`decision_tree`**: This component is loaded from the YAML manifest file `decision_tree/decision_tree.yaml`. Its function is to train a Decision Tree model. It takes the output from the `download` component as input and is expected to output a floating-point value representing the model's accuracy.\n3.  **`logistic_regression`**: This component is loaded from the YAML manifest file `logistic_regression/logistic_regression.yaml`. Its function is to train a Logistic Regression model. It also takes the output from the `download` component as input and is expected to output a floating-point value representing the model's accuracy.\n4.  **`show_results`**: This is a Python function-based component decorated with `@func_to_container_op`. Its function is to display the accuracy results from both the Decision Tree and Logistic Regression models. It takes two floating-point inputs: `decision_tree` (accuracy from the Decision Tree model) and `logistic_regression` (accuracy from the Logistic Regression model). It prints these values to the console and has no explicit output.\n\nThe control flow of the pipeline is as follows:\n- The `download` component runs first.\n- Once the `download` component completes, both the `decision_tree` and `logistic_regression` components run **in parallel**, taking the output of `download` as their respective inputs.\n- After both `decision_tree` and `logistic_regression` components have finished executing, the `show_results` component runs, taking the accuracy outputs from `decision_tree` and `logistic_regression` as its inputs.\n\nThe pipeline utilizes the `kfp` library for defining and compiling the pipeline. It does not explicitly use other external machine learning libraries like scikit-learn or data platforms like Snowflake within the pipeline definition itself, but these might be implicitly used within the components defined by the YAML files. The pipeline is compiled to `FirstPipeline.yaml`."
  },
  {
    "repo": "references_kfp_files",
    "file": "rakesh283343_kubeflow-pipelines-clv_pipelines_train_pipeline.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `CLV Training` that performs Customer Lifetime Value (CLV) prediction using BigQuery for feature engineering and AutoML Tables for model training.\n\nThe pipeline consists of 7 components:\n\n1.  **`load_sales_transactions`**: This component loads sales transaction data.\n    *   **Inputs**: `project_id` (project ID), `source_gcs_path` (GCS path to source data), `transactions_table_name` (name of the transactions table), `bq_dataset_name` (BigQuery dataset name), `dataset_location` (BigQuery dataset location).\n    *   **Outputs**: `sales_transactions_bq_table_id` (BigQuery table ID for sales transactions).\n    *   **Function**: Ingests sales transaction data from GCS into a BigQuery table.\n\n2.  **`prepare_feature_engineering_query`**: This component prepares the SQL query for feature engineering.\n    *   **Inputs**: `project_id`, `sales_transactions_bq_table_id` (from `load_sales_transactions`), `threshold_date` (date for data partitioning), `predict_end` (prediction end date), `max_monetary` (maximum monetary value), `features_table_name` (name for the engineered features table), `bq_dataset_name`, `dataset_location`.\n    *   **Outputs**: `feature_engineering_query` (the SQL query string), `feature_engineering_table_id` (BigQuery table ID for features).\n    *   **Function**: Constructs a BigQuery SQL query to extract and engineer features relevant for CLV prediction.\n\n3.  **`engineer_features`**: This component executes the feature engineering query using BigQuery.\n    *   **Inputs**: `query` (the SQL query from `prepare_feature_engineering_query`), `project_id`, `output_dataset_id` (BigQuery dataset for output), `output_table_id` (BigQuery table for output), `dataset_location`.\n    *   **Outputs**: None directly visible from the snippet, but it creates the BigQuery table specified by `output_table_id`.\n    *   **Function**: Runs the generated SQL query in BigQuery to create the features table.\n\n4.  **`import_dataset`**: This component imports the engineered features dataset into AutoML Tables.\n    *   **Inputs**: `bq_source` (BigQuery table ID of the engineered features table), `dataset_display_name` (display name for the AutoML dataset), `project_id`, `region` (AutoML compute region).\n    *   **Outputs**: `dataset_id` (AutoML dataset ID).\n    *   **Function**: Creates and imports a dataset in AutoML Tables from the BigQuery features table.\n\n5.  **`train_model`**: This component trains a machine learning model using AutoML Tables.\n    *   **Inputs**: `dataset_id` (AutoML dataset ID from `import_dataset`), `model_display_name` (display name for the AutoML model), `target_column_name` (the column to predict), `excluded_features` (features to exclude from training), `optimization_objective` (optimization objective for training), `budget_milli_node_hours` (training budget), `project_id`, `region`.\n    *   **Outputs**: `model_id` (AutoML model ID), `model_evaluation_metrics` (evaluation metrics of the trained model).\n    *   **Function**: Initiates and monitors a model training job in AutoML Tables.\n\n6.  **`deploy_model`**: This component deploys the trained model from AutoML Tables.\n    *   **Inputs**: `model_id` (AutoML model ID from `train_model`), `project_id`, `region`.\n    *   **Outputs**: `endpoint_id` (AutoML endpoint ID).\n    *   **Function**: Deploys the trained AutoML model to an endpoint for predictions.\n\n7.  **`log_metrics`**: This component logs the model evaluation metrics.\n    *   **Inputs**: `metrics` (model evaluation metrics from `train_model`), `model_name` (name of the model).\n    *   **Outputs**: None directly visible.\n    *   **Function**: Records the performance metrics of the trained model, likely for tracking and analysis.\n\n**Control Flow:**\n\nThe pipeline execution flow is sequential with explicit dependencies:\n*   `load_sales_transactions` runs first.\n*   `prepare_feature_engineering_query` depends on the output of `load_sales_transactions`.\n*   `engineer_features` depends on the output of `prepare_feature_engineering_query`.\n*   `import_dataset` depends on the successful completion of `engineer_features`.\n*   `train_model` depends on the output of `import_dataset`.\n*   `deploy_model` depends on the output of `train_model`.\n*   `log_metrics` depends on the output of `train_model`.\n\n**Tools/Libraries Used:**\n\n*   **Kubeflow Pipelines (kfp)**: For defining and orchestrating the pipeline.\n*   **BigQuery**: For data loading and feature engineering.\n*   **AutoML Tables**: For dataset import, model training, and model deployment.\n*   **Google Cloud Storage (GCS)**: For source data.\n*   **Python `pathlib` and `yaml`**: For loading pipeline settings.\n*   **`helper_components`**: Custom Python functions (`load_sales_transactions`, `prepare_feature_engineering_query`) defined as lightweight components.\n*   **`kfp.gcp`**: For GCP-specific functionality (though not explicitly shown in the component definitions, it's imported).\n\nThe pipeline should accept the following parameters: `project_id`, `source_gcs_path`, `staging_gcs_path`, `source_bq_table`, `bq_dataset_name`, `threshold_date`, `predict_end`, `max_monetary`, `features_table_name` (with a default), `transactions_table_name` (with a default), `dataset_location` (with a default), `aml_dataset_name` (with a default), `aml_model_name` (with a default), `aml_compute_region` (with a default), `train_budget` (with a default), `target_column_name` (with a default), `features_to_exclude` (with a default), `optimization_objective` (with a default), and `primary_metric` (with a default). The specific default values for these parameters should be loaded from a `settings.yaml` file."
  },
  {
    "repo": "references_kfp_files",
    "file": "speg03_kfp-toolbox_tests_test_decorators.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `echo-pipeline` that performs the following steps:\n\nThe pipeline consists of a single component:\n1.  **`echo`**: This component is defined by the `@dsl.component()` decorator and is decorated with `@spec`.\n    *   **Function**: Returns the string \"hello, world\".\n    *   **Resource Specifications**:\n        *   `name`: \"Echo Component\"\n        *   `cpu`: \"2\"\n        *   `memory`: \"16G\"\n        *   `gpu`: \"1\"\n        *   `accelerator`: \"NVIDIA_TESLA_T4\"\n        *   `caching`: `True` (enabled)\n    *   **Outputs**: A string.\n\n**Control Flow**: The `echo` component runs directly within the pipeline.\n\n**Tools/Libraries Used**: `kfp`, `yaml`. The `@spec` decorator from `kfp_toolbox.decorators` is used to define resource requirements and metadata for the component."
  },
  {
    "repo": "references_kfp_files",
    "file": "rakesh283343_kubeflow-sample-pipelines_core_volume_ops_readme_volume_ops.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `VolumeOp Basic` that performs operations demonstrating basic `VolumeOp` usage.\n\nThe pipeline consists of two components:\n1.  **`create-pvc` (VolumeOp):** This component is responsible for creating a Persistent Volume Claim (PVC).\n    *   It is named `create-pvc`.\n    *   Its resource name is `my-pvc`.\n    *   It uses the `RWO` (ReadWriteOnce) access mode.\n    *   Its size is dynamically determined by the pipeline input `size`.\n    *   It exposes its created volume as an output.\n\n2.  **`cop` (ContainerOp):** This component is a containerized operation that writes data to the created volume.\n    *   It is named `cop`.\n    *   It uses the `library/bash:4.4.23` Docker image.\n    *   It executes the command `sh -c \"echo foo > /mnt/file1\"`, which writes the string \"foo\" to a file named `file1` within the `/mnt` directory.\n    *   **Control Flow:** This component explicitly depends on the `create-pvc` component and mounts its created volume at the `/mnt` path within its container.\n\nThe pipeline takes one input:\n*   `size`: A string representing the desired size for the PVC (e.g., \"1Gi\", \"500Mi\")."
  },
  {
    "repo": "references_kfp_files",
    "file": "ksalama_kubeflow-examples_kfp-cloudbuild_pipeline_workflow.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `A Simple CI pipeline` that performs basic arithmetic operations to demonstrate CI using CloudBuild.\n\nThe pipeline takes three integer inputs: `x_value` (default 1), `y_value` (default 1), and `z_value` (default 1).\n\nIt consists of **three** components:\n\n1.  **`my_add`**: This component is named 'Add x and y' and takes `x_value` and `y_value` as inputs. It produces an output named `sum`.\n2.  **`my_divide`**: This component is named 'Divide sum by z' and takes the `sum` output from the first `add_op` and `z_value` as inputs. This component is executed only if the `sum` from the first `add_op` is not equal to zero. It produces two outputs: `quotient` and `remainder`.\n3.  **`my_add` (second instance)**: This component is named 'Add quotient and remainder' and takes the `quotient` and `remainder` outputs from the `my_divide` component as inputs. This component is dependent on and executed after the `my_divide` component.\n\nThe control flow is as follows:\n*   The first `my_add` component (`add_step`) executes initially with `x_value` and `y_value` from the pipeline inputs.\n*   A `kfp.dsl.Condition` checks if the `sum` output of `add_step` is not equal to zero.\n*   If the condition is met, the `my_divide` component (`divide_step`) executes, taking `sum` from `add_step` and `z_value` as inputs.\n*   Immediately following `divide_step` (and nested within the same condition block), the second `my_add` component (`add_step2`) executes, using the `quotient` and `remainder` outputs from `divide_step`.\n\nThe pipeline utilizes pre-defined components loaded from a `ComponentStore` with `local_search_paths=['components']`. The specific components used are `my_add` and `my_divide`."
  },
  {
    "repo": "references_kfp_files",
    "file": "stackdemos_kf-flatcar2_components_golang_component.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `GoLang_pipeline` that performs the following steps:\n\n**Pipeline Description:**\n\nThis pipeline consists of **two** distinct components designed to download an artifact and then process it using a GoLang-based container.\n\n**Components:**\n\n1.  **`download-artifact` (http_download_op):**\n    *   **Function:** Downloads an artifact from a specified URL using `curl`.\n    *   **Inputs:**\n        *   `url`: The URL of the artifact to download.\n        *   `download_to`: The local path where the artifact should be saved.\n        *   `md5sum`: The expected MD5 checksum of the artifact for integrity verification.\n    *   **Logic:** It first checks if a file with the same name and matching MD5 sum already exists at `download_to`. If so, it skips the download. Otherwise, it uses `curl` to download the file to the specified path.\n    *   **Image:** Uses the `appropriate/curl` Docker image.\n\n2.  **`[script-name]` (processing_op):**\n    *   **Function:** Executes a GoLang script. The name of this component is dynamically derived from the `script` parameter by converting it to lowercase and replacing non-alphanumeric characters with hyphens.\n    *   **Inputs:**\n        *   `script`: The path to the GoLang script to be executed. This script is passed as the `command` to the container.\n        *   `image`: The Docker image to use for running the GoLang script. This can optionally be retrieved from a global `GOLANG_IMAGE` variable if running in a Jupyter Notebook. It is a mandatory input otherwise.\n        *   `arguments`: A list of arguments to pass to the GoLang script.\n        *   `file_outputs`: A dictionary defining expected file outputs from the component.\n    *   **Outputs:** The specific outputs are determined by the `file_outputs` parameter.\n    *   **Logic:** It creates a `ContainerOp` that runs the specified `script` using the provided `image` and `arguments`.\n\n**Control Flow:**\n\nThere is no explicit dependency or control flow defined between these two components in the provided code snippet. The `processing_op` and `http_download_op` are standalone functions that define Kubeflow components. When constructing the pipeline, an `after` dependency would typically be used if the `processing_op` needed the output of the `http_download_op`.\n\n**Tools/Libraries:**\n\n*   **Python Libraries:** `kfp.dsl`, `urllib.parse`, `os`, `re`, `IPython` (optional, for notebook environment detection).\n*   **External Tools (within containers):** `curl`, `awk` (used in `http_download_op`).\n*   **GoLang:** The `processing_op` is designed to run GoLang scripts, implying a GoLang runtime/compiler would be present in the specified `image`."
  },
  {
    "repo": "references_kfp_files",
    "file": "omkarakaya_kubeflow-recommender_pipeline.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `Kubeflow Pipeline Test` that performs a machine learning workflow for an XGBoost training scenario.\n\nThis pipeline consists of **three components**:\n\n1.  **`create_pvc` (VolumeOp)**:\n    *   **Function**: Creates a Persistent Volume Claim (PVC) named `my-pvc` with `1Gi` storage in `ReadWriteOnce` mode. This volume will be shared by subsequent components.\n    *   **Inputs**: None (implicit configuration parameters).\n    *   **Outputs**: A `dsl.PipelineVolume` object representing the created PVC.\n\n2.  **`preprocess` (ContainerOp)**:\n    *   **Function**: Executes a preprocessing step for the machine learning pipeline. It uses a custom image `gcr.io/compose-flask/hub:v6`.\n    *   **Inputs**:\n        *   `project` (parameter): Passed from the pipeline's `project` input.\n        *   `mode`: Hardcoded to `cloud`.\n    *   **Outputs**:\n        *   `output`: A file containing the preprocessed output, located at `/tmp/output.txt` within the container.\n    *   **Dependencies**: Requires the `create_pvc` volume operation to complete successfully and mounts the created volume at `/data`.\n\n3.  **`build` (ContainerOp)**:\n    *   **Function**: Executes a build step, likely for model building or further data preparation. It uses a custom image `gcr.io/compose-flask/build:v6`.\n    *   **Inputs**:\n        *   `project` (parameter): Passed from the pipeline's `project` input.\n        *   `mode`: Hardcoded to `cloud`.\n        *   `preprocessed`: Receives the `output` from the `preprocess` component.\n        *   `preprocessed2`: Receives the `output` from the `preprocess` component (note: `preprocess.output` is an alias for `preprocess.outputs['output']`).\n    *   **Outputs**:\n        *   `output`: A file containing the build output, located at `/tmp/output.txt` within the container.\n    *   **Dependencies**: Requires both the `create_pvc` volume operation and the `preprocess` component to complete successfully. It also mounts the `create_pvc` volume at `/data`.\n\n**Pipeline Inputs**:\n\n*   `output` (parameter): A generic output parameter, not explicitly used by the components in the provided snippet.\n*   `project` (parameter): A string specifying the GCP project.\n*   `region` (parameter, default: `us-central1`): A string specifying the GCP region.\n*   `train_data` (parameter, default: `gs://ml-pipeline-playground/sfpd/train.csv`): GCS path to the training data.\n*   `eval_data` (parameter, default: `gs://ml-pipeline-playground/sfpd/eval.csv`): GCS path to the evaluation data.\n*   `schema` (parameter, default: `gs://ml-pipeline-playground/sfpd/schema.json`): GCS path to the data schema.\n*   `target` (parameter, default: `resolution`): The name of the target column.\n*   `rounds` (parameter, default: `200`): Number of training rounds for XGBoost.\n*   `workers` (parameter, default: `2`): Number of workers for training.\n*   `true_label` (parameter, default: `ACTION`): The true label for the target.\n\n**Control Flow**:\n\n1.  `create_pvc` runs first.\n2.  `preprocess` starts *after* `create_pvc` has completed.\n3.  `build` starts *after* both `create_pvc` and `preprocess` have completed.\n\nThe pipeline utilizes custom container images `gcr.io/compose-flask/hub:v6` and `gcr.io/compose-flask/build:v6` for its `ContainerOp` steps. It also demonstrates the use of `dsl.VolumeOp` for shared persistent storage."
  },
  {
    "repo": "references_kfp_files",
    "file": "anupr567_kubeflow_pipeline_kfp_pipeline.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `First Pipeline` that performs data extraction, preprocessing, and trains two different classification models.\n\nThe pipeline consists of **four components**:\n1.  **`extract_data`**: This component is responsible for extracting the initial data. It takes no explicit inputs and produces an output that serves as the input for the next stage.\n2.  **`preprocess_data`**: This component performs preprocessing on the extracted data. It takes the output from the `extract_data` component as its input. Notably, caching is disabled for this component (`max_cache_staleness = \"P0D\"`), ensuring it always re-executes. Its output is the preprocessed data, which is then used by the subsequent model training components.\n3.  **`logistic_regression`**: This component trains a logistic regression model. It takes the preprocessed data from the `preprocess_data` component as its input.\n4.  **`random_forest_classifier`**: This component trains a random forest classifier model. It also takes the preprocessed data from the `preprocess_data` component as its input.\n\n**Control Flow:**\n- The `extract_data` component runs first.\n- The `preprocess_data` component runs **after** `extract_data` and consumes its output.\n- Both `logistic_regression` and `random_forest_classifier` components run **in parallel** after `preprocess_data`, and both consume the output of `preprocess_data`.\n\n**Tools/Libraries:**\n- The pipeline is built using the `kfp` (Kubeflow Pipelines) SDK.\n- Each component (`extract_data`, `preprocess_data`, `logistic_regression`, `random_forest_classifier`) is loaded from a YAML manifest file, indicating external component definitions.\n- The pipeline description is \"Applies extraction of data and pre processing it.\""
  },
  {
    "repo": "references_kfp_files",
    "file": "s102401002_kubeflowPipeline_kubeflowPipeline_parquet.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `DiabetesPredictionPipeline` that performs a machine learning workflow for diabetes prediction.\n\nThe pipeline consists of three components:\n1.  **`load_data`**:\n    *   **Function**: This component is responsible for loading data from a remote URL, performing initial data cleaning and preprocessing, and converting the processed data into Parquet format.\n    *   **Inputs**: None.\n    *   **Outputs**: `load_data_output` (a Parquet file containing the cleaned and preprocessed dataset).\n    *   **Tools/Libraries**: `pandas`, `numpy`, `requests`, `pyarrow`.\n    *   **Details**: It fetches a CSV from a GitHub URL, drops rows with 'No Info' in 'diabetes', selects relevant columns ('gender', 'age', 'bmi', 'HbA1c_level', 'blood_glucose_level', 'diabetes'), handles 'No Info' in numerical columns by converting to NaN and then filling NaNs with column means, and maps 'gender' categories ('Male', 'Female', 'Other') to numerical values, filtering out 'Other'. The final processed DataFrame is saved as a Parquet file.\n\n2.  **`prepare_data`**:\n    *   **Function**: This component takes the preprocessed data, splits it into features (X) and target (y), and then further splits these into training and testing sets.\n    *   **Inputs**: `data` (an `InputPath` representing the Parquet file output from `load_data`).\n    *   **Outputs**: `x_train_output`, `x_test_output`, `y_train_output`, `y_test_output` (all are `OutputPath` for Parquet files containing the respective data splits).\n    *   **Tools/Libraries**: `pandas`, `scikit-learn` (`train_test_split`), `pyarrow`.\n    *   **Details**: It reads the input Parquet file, separates 'diabetes' as the target variable `y` and the rest as features `x`. It then performs a 80/20 train-test split with `random_state=42`. All four resulting dataframes are saved as separate Parquet files.\n\n3.  **`train_model`**:\n    *   **Function**: This component trains a Logistic Regression model using the provided training data.\n    *   **Inputs**: `x_train` (an `InputPath` representing the training features from `prepare_data`), `y_train` (an `InputPath` representing the training target from `prepare_data`).\n    *   **Outputs**: `train_model_output` (an `OutputPath` for the serialized trained model).\n    *   **Tools/Libraries**: `pandas`, `scikit-learn` (`LogisticRegression`), `joblib`, `pyarrow`.\n    *   **Details**: It reads the `x_train` and `y_train` Parquet files, trains a `LogisticRegression` model, and then saves the trained model using `joblib`.\n\n**Control Flow**:\n*   The `prepare_data` component depends on the successful completion of `load_data` and consumes its output as input.\n*   The `train_model` component depends on the successful completion of `prepare_data` and consumes its `x_train_output` and `y_train_output` as inputs.\n\nAll components should use `python:3.9` as their base image.\nThe Kubeflow DSL decorators `@dsl.pipeline` and `@dsl.component` should be used.\nThe outputs should be saved as Parquet files where indicated."
  },
  {
    "repo": "references_kfp_files",
    "file": "hiruna72_miniKF_example_pipeline_small_pipeline.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `My pipeline` that performs the following steps:\n\nThe pipeline consists of three main components:\n1.  **`multiply`**: This component takes an `input_file` and a `multiplier` to perform a multiplication operation. It produces two outputs: `output_file` (the result of the multiplication) and `output_uri_in_file` (a URI pointing to the output file). It uses the Docker image `hiruna72/multiplier` and mounts a volume at `/data`.\n2.  **`concatenate`**: This component takes two input files, `input_file1` and `input_file2`, and concatenates them. It produces two outputs: `output_file` (the concatenated result) and `output_uri_in_file` (a URI pointing to the output file). It uses the Docker image `hiruna72/concatenate` and mounts a volume at `/data`.\n\nThe pipeline takes one parameter: `rok_url`.\n\nThe control flow is as follows:\n- The pipeline begins by defining a `pvc_op` which creates a Persistent Volume Claim (PVC) with a size of 1GB and a name `my-pvc`. This PVC is then used as a volume across subsequent tasks.\n- A **`multiply`** task named `multiply-op1` is executed.\n    - It takes an `input_file` which is a `kfp.dsl.PipelineParam` named `rok_url`.\n    - It uses a fixed `multiplier` value of `2`.\n    - It directs its `output_uri` to `/data/1.txt` and its `output_uri_in_file` to `/data/1.txt.url`.\n    - It uses the `pvc_op` as its volume.\n- A **`multiply`** task named `multiply-op2` is executed.\n    - It takes an `input_file` which is the `output_file` from `multiply-op1` (i.e., `/data/1.txt`).\n    - It uses a fixed `multiplier` value of `3`.\n    - It directs its `output_uri` to `/data/2.txt` and its `output_uri_in_file` to `/data/2.txt.url`.\n    - It uses the `pvc_op` as its volume.\n    - This task runs `after` `multiply-op1`.\n- A **`concatenate`** task named `concat-op` is executed.\n    - It takes `input_file1` as the `output_file` from `multiply-op1` (i.e., `/data/1.txt`).\n    - It takes `input_fi1e2` as the `output_file` from `multiply-op2` (i.e., `/data/2.txt`).\n    - It directs its `output_uri` to `/data/3.txt` and its `output_uri_in_file` to `/data/3.txt.url`.\n    - It uses the `pvc_op` as its volume.\n    - This task runs `after` both `multiply-op1` and `multiply-op2`.\n\nThe pipeline should be defined using the `@kfp.dsl.pipeline` decorator with the specified name and an empty description.\nThe component functions (`multiply` and `concatenate`) are defined as `kfp.dsl.ContainerOp` and use the specified Docker images, arguments, commands, file outputs, and pvolumes."
  },
  {
    "repo": "references_kfp_files",
    "file": "9rince_kfp_samples_test_lightweight_python_functions_v2_pipeline.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `lightweight_python_functions_v2_pipeline` that performs a data processing and machine learning workflow.\n\nThe pipeline consists of three components:\n1.  **`preprocess`**: This component serves as a dummy preprocessing step.\n    *   **Inputs**:\n        *   `message` (string): An input string parameter.\n        *   `empty_message` (string, optional): An input string parameter with an empty string default value.\n    *   **Outputs**:\n        *   `output_dataset_one` (Output[Dataset]): A metadata-rich handle for an output `Dataset` artifact. The component writes the `message` content to this dataset's path.\n        *   `output_dataset_two_path` (OutputPath('Dataset')): A locally accessible filepath for another output `Dataset` artifact. The component writes the `message` content to this path.\n        *   `output_parameter_path` (OutputPath(str)): A locally accessible filepath for an output string parameter. The component writes the `message` content to this path.\n        *   `output_bool_parameter_path` (OutputPath(bool)): A locally accessible filepath for an output boolean parameter. The component writes the string representation of `True` to this path.\n        *   `output_dict_parameter_path` (OutputPath(Dict[str, int])): A locally accessible filepath for an output dictionary parameter. The component writes a JSON string `{'A': 1, 'B': 2}` to this path.\n        *   `output_list_parameter_path` (OutputPath(List[str])): A locally accessible filepath for an output list parameter. The component writes a JSON string `['a', 'b', 'c']` to this path.\n    *   **Functionality**: It writes the input `message` to two different `Dataset` output paths and an output string parameter path. It also writes a boolean, a dictionary, and a list as output parameters. It uses the `json` library for serializing dictionary and list outputs.\n\n2.  **`train`**: This component performs a dummy training step.\n    *   **Inputs**:\n        *   `input_dataset_one` (Input[Dataset]): An input `Dataset` artifact, which is the `output_dataset_one` from the `preprocess` component.\n        *   `input_dataset_two_path` (InputPath('Dataset')): A locally accessible filepath for an input `Dataset` artifact, which is the `output_dataset_two_path` from the `preprocess` component.\n        *   `input_parameter` (str): An input string parameter, which is the `output_parameter_path` from the `preprocess` component.\n        *   `input_bool_parameter` (bool): An input boolean parameter, which is the `output_bool_parameter_path` from the `preprocess` component.\n        *   `input_dict_parameter` (Dict[str, int]): An input dictionary parameter, which is the `output_dict_parameter_path` from the `preprocess` component.\n        *   `input_list_parameter` (List[str]): An input list parameter, which is the `output_list_parameter_path` from the `preprocess` component.\n    *   **Outputs**:\n        *   `output_model` (Output[Model]): A metadata-rich handle for an output `Model` artifact. The component writes a dummy \"trained model\" string to this model's path.\n    *   **Functionality**: It reads data from the provided input datasets and parameters, printing their content to standard output. It then writes a dummy \"trained model\" string to the `output_model` path. It uses the `json` library for deserializing dictionary and list inputs.\n\nThe control flow of the pipeline is sequential:\n*   The `preprocess` component runs first, initialized with the `message` parameter set to \"Hello world\".\n*   The `train` component runs after `preprocess` completes, taking all its inputs from the corresponding outputs of the `preprocess` component.\n\nThe pipeline uses the `kfp.v2.dsl` module for defining components and the pipeline structure, and `kfp.v2.compiler` for compilation. It also utilizes standard Python libraries for file I/O and the `json` library for handling dictionary and list parameters."
  },
  {
    "repo": "references_kfp_files",
    "file": "shawar8_sfcrime-prediction-kubeflow_serving_fns.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `SF_Crime_Prediction_Batch_Serving` that performs batch inference for crime prediction.\n\nThe pipeline consists of **6 components**:\n\n1.  **`get_data`**:\n    *   **Function**: Collects batch serving data from the San Francisco OpenData Socrata API. It queries for incidents from two days prior to the current date.\n    *   **Inputs**: None.\n    *   **Outputs**:\n        *   `serving_data_path` (Output[Dataset]): Path to the CSV file containing the raw batch serving data.\n    *   **Tools/Libraries**: `sodapy`, `pandas`.\n    *   **Base Image**: `python:3.9`.\n\n2.  **`transform_cat_label_data`**:\n    *   **Function**: Transforms categorical features and labels in the serving data using pre-trained `OrdinalEncoder` and `LabelEncoder` models. It handles unknown categorical values by mapping them to 'UNK'. It downloads the encoders from Google Cloud Storage.\n    *   **Inputs**:\n        *   `df_path` (Input[Dataset]): Path to the raw serving data CSV file from the `get_data` component.\n        *   `cat_encoder_path` (str): Local path to save the downloaded categorical encoder.\n        *   `label_encoder_path` (str): Local path to save the downloaded label encoder.\n        *   `projectid` (str): Google Cloud Project ID.\n        *   `bucket_name` (str): Google Cloud Storage bucket name where encoders are stored.\n        *   `blob_path_cat_enc` (str): GCS blob path for the categorical encoder.\n        *   `blob_path_lab_enc` (str): GCS blob path for the label encoder.\n    *   **Outputs**:\n        *   `df_output_path` (Output[Dataset]): Path to the CSV file containing the transformed DataFrame.\n        *   `label_output_path` (Output[Artifact]): Path to the pickled transformed labels.\n    *   **Tools/Libraries**: `pandas`, `google-cloud-storage`, `scikit-learn`.\n    *   **Base Image**: `python:3.9`.\n\n3.  **`predict_crime_category`**:\n    *   **Function**: Loads a pre-trained XGBoost model and uses it to predict crime categories on the transformed serving data. It loads the model from Google Cloud Storage.\n    *   **Inputs**:\n        *   `df_path` (Input[Dataset]): Path to the transformed DataFrame from `transform_cat_label_data`.\n        *   `model_path` (str): Local path to save the downloaded model.\n        *   `projectid` (str): Google Cloud Project ID.\n        *   `bucket_name` (str): Google Cloud Storage bucket name where the model is stored.\n        *   `blob_path_model` (str): GCS blob path for the XGBoost model.\n    *   **Outputs**:\n        *   `predictions_path` (Output[Dataset]): Path to the CSV file containing the predictions.\n    *   **Tools/Libraries**: `pandas`, `xgboost`, `google-cloud-storage`.\n    *   **Base Image**: `python:3.9`.\n\n4.  **`evaluate_predictions`**:\n    *   **Function**: Evaluates the model's predictions against the true labels, calculating metrics such as accuracy, precision, recall, and F1-score.\n    *   **Inputs**:\n        *   `predictions_path` (Input[Dataset]): Path to the predictions CSV from `predict_crime_category`.\n        *   `true_labels_path` (Input[Artifact]): Path to the true labels (pickled) from `transform_cat_label_data`.\n    *   **Outputs**:\n        *   `metrics_output` (Output[Metrics]): Kubeflow Metrics object containing evaluation results.\n    *   **Tools/Libraries**: `pandas`, `sklearn`.\n    *   **Base Image**: `python:3.9`.\n\n5.  **`upload_predictions`**:\n    *   **Function**: Uploads the raw serving data, transformed data, and predictions to Google Cloud Storage.\n    *   **Inputs**:\n        *   `raw_data_path` (Input[Dataset]): Path to the raw serving data from `get_data`.\n        *   `transformed_data_path` (Input[Dataset]): Path to the transformed data from `transform_cat_label_data`.\n        *   `predictions_path` (Input[Dataset]): Path to the predictions from `predict_crime_category`.\n        *   `projectid` (str): Google Cloud Project ID.\n        *   `bucket_name` (str): Google Cloud Storage bucket name for uploads.\n    *   **Outputs**: None.\n    *   **Tools/Libraries**: `pandas`, `google-cloud-storage`.\n    *   **Base Image**: `python:3.9`.\n\n6.  **`send_slack_alert`**:\n    *   **Function**: Sends a Slack alert with the evaluation metrics.\n    *   **Inputs**:\n        *   `metrics_json` (str): JSON string of the evaluation metrics from `evaluate_predictions`.\n        *   `webhook_url` (str): Slack webhook URL for sending alerts.\n    *   **Outputs**: None.\n    *   **Tools/Libraries**: `requests`.\n    *   **Base Image**: `python:3.9`.\n\n**Control Flow and Dependencies**:\n\n*   The pipeline starts with `get_data`.\n*   `transform_cat_label_data` depends on the output of `get_data` (raw serving data).\n*   `predict_crime_category` depends on the transformed data output from `transform_cat_label_data`.\n*   `evaluate_predictions` depends on the predictions from `predict_crime_category` and the transformed labels from `transform_cat_label_data`.\n*   `upload_predictions` depends on the raw data from `get_data`, the transformed data from `transform_cat_label_data`, and the predictions from `predict_crime_category`.\n*   `send_slack_alert` depends on the metrics output from `evaluate_predictions`.\n\n**Pipeline Parameters**:\n\nThe pipeline will accept the following parameters:\n*   `projectid` (str): GCP Project ID.\n*   `bucket_name` (str): GCS bucket name.\n*   `blob_path_cat_enc` (str): GCS blob path for the categorical encoder.\n*   `blob_path_lab_enc` (str): GCS blob path for the label encoder.\n*   `blob_path_model` (str): GCS blob path for the trained model.\n*   `webhook_url` (str): Slack webhook URL.\n*   `cat_encoder_path` (str): Local path to store the downloaded categorical encoder.\n*   `label_encoder_path` (str): Local path to store the downloaded label encoder.\n*   `model_path` (str): Local path to store the downloaded model.\n\nThe components should utilize the `kfp.v2.dsl` decorators (`@component`, `Input`, `Output`, `Dataset`, `Artifact`, `Model`, `Metrics`). The base image for all components should be `python:3.9`. All component definitions should be output to a YAML file named `new_yamls/collect_batch_data.yaml`."
  },
  {
    "repo": "references_kfp_files",
    "file": "stackdemos_ghsumm2_components_training_component.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `stackdemos-ghsumm2-components-training-component` that defines two reusable container operation factory functions:\n\n**1. `training_op`:**\n   - **Purpose:** Encapsulates the execution of Python training scripts within a container.\n   - **Inputs:**\n     - `script` (str): The path to the Python script to be executed.\n     - `image` (str, optional): The Docker image to use for the container. If not provided and running in an IPython environment, it attempts to retrieve a global `TRAINING_IMAGE` variable. Raises a `ValueError` if `image` is still missing.\n     - `arguments` (list, optional): A list of command-line arguments to pass to the script.\n     - `file_outputs` (dict, optional): A dictionary defining named file outputs for the component.\n   - **Functionality:**\n     - Dynamically generates the component name by cleaning the script name (e.g., `my_script.py` becomes `my-script`).\n     - Sets the container `command` to execute the Python script using `/usr/local/bin/python`.\n     - Passes the provided `arguments` to the script.\n     - Maps `file_outputs` to the component's outputs.\n   - **Libraries/Tools:** `re`, `os` for name manipulation.\n\n**2. `http_download_op`:**\n   - **Purpose:** Downloads a file from a given URL using `curl` and performs an MD5 checksum pre-check.\n   - **Inputs:**\n     - `url` (str): The URL of the file to download.\n     - `download_to` (str): The destination path where the file should be saved.\n     - `md5sum` (str): The expected MD5 checksum of the file.\n   - **Functionality:**\n     - Sets the component name to `download-artifact`.\n     - Uses the `appropriate/curl` Docker image.\n     - Executes a shell command (`sh -c`) that:\n       - Checks if the calculated MD5 sum of `download_to` matches the provided `md5sum`.\n       - If they match, it prints a \"Skipping\" message.\n       - Otherwise, it uses `curl` with options `-#Lv --create-dirs -o` to download the file from `url` to `download_to`.\n   - **Libraries/Tools:** `curl`, `awk` (implicitly via shell command).\n\nThe pipeline does not define a top-level `@dsl.pipeline` function but provides these two utility functions to be used as building blocks in other Kubeflow Pipelines."
  },
  {
    "repo": "references_kfp_files",
    "file": "s102401002_kubeflowPipeline0722_src_compose.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `DistributedTrainingPipeline` that performs a machine learning workflow involving data loading, parameter parsing, distributed training with Katib, and Spark-based processing.\n\nThe pipeline consists of 7 components:\n\n1.  **`load_file_from_nas_to_minio`**:\n    *   **Function**: Loads CSV files (X_train, X_test, Y_train, Y_test) from specified input paths into Kubeflow Pipeline `Dataset` artifacts. It uses the `pandas` library for data reading and writing.\n    *   **Inputs**:\n        *   `x_train_input_path` (str)\n        *   `x_test_input_path` (str)\n        *   `y_train_input_path` (str)\n        *   `y_test_input_path` (str)\n    *   **Outputs**:\n        *   `x_train_output` (Output[Dataset])\n        *   `x_test_output` (Output[Dataset])\n        *   `y_train_output` (Output[Dataset])\n        *   `y_test_output` (Output[Dataset])\n    *   **Base Image**: `python:3.10-slim`\n    *   **Packages**: `pandas`\n\n2.  **`parse_input_json`**:\n    *   **Function**: Parses a JSON file containing input parameters for different machine learning methods (xgboost, random_forest, knn, lr). It extracts relevant metrics and logs them to separate Kubeflow Pipeline `Metrics` artifacts based on the `method` field. It uses the `json` library.\n    *   **Inputs**:\n        *   `json_file_path` (str)\n    *   **Outputs**:\n        *   `xgboost_input_metrics` (Output[Metrics])\n        *   `random_forest_input_metrics` (Output[Metrics])\n        *   `knn_input_metrics` (Output[Metrics])\n        *   `lr_input_metrics` (Output[Metrics])\n    *   **Base Image**: `python:3.10-slim`\n\n3.  **`run_xgboost_katib_experiment`**:\n    *   **Function**: Orchestrates a Katib experiment for XGBoost model training. It likely uses the input parameters to configure the Katib experiment (e.g., hyperparameter search space, objective metrics) and outputs the best parameters found by Katib. It uses `kubeflow-katib` and `kubernetes.client`.\n    *   **Inputs**:\n        *   `input_params_metrics` (Input[Metrics])\n    *   **Outputs**:\n        *   `best_params_metrics` (Output[Metrics])\n    *   **Base Image**: `python:3.10-slim`\n    *   **Packages**: `kubeflow-katib==0.17.0`\n\n4.  **`run_random_forest_katib_experiment`**:\n    *   **Function**: Orchestrates a Katib experiment for Random Forest model training, similar to the XGBoost component.\n    *   **Inputs**:\n        *   `input_params_metrics` (Input[Metrics])\n    *   **Outputs**:\n        *   `best_params_metrics` (Output[Metrics])\n    *   **Base Image**: `python:3.10-slim`\n    *   **Packages**: `kubeflow-katib==0.17.0`\n\n5.  **`run_knn_katib_experiment`**:\n    *   **Function**: Orchestrates a Katib experiment for K-Nearest Neighbors (KNN) model training, similar to the other Katib components.\n    *   **Inputs**:\n        *   `input_params_metrics` (Input[Metrics])\n    *   **Outputs**:\n        *   `best_params_metrics` (Output[Metrics])\n    *   **Base Image**: `python:3.10-slim`\n    *   **Packages**: `kubeflow-katib==0.17.0`\n\n6.  **`run_lr_katib_experiment`**:\n    *   **Function**: Orchestrates a Katib experiment for Logistic Regression (LR) model training, similar to the other Katib components.\n    *   **Inputs**:\n        *   `input_params_metrics` (Input[Metrics])\n    *   **Outputs**:\n        *   `best_params_metrics` (Output[Metrics])\n    *   **Base Image**: `python:3.10-slim`\n    *   **Packages**: `kubeflow-katib==0.17.0`\n\n7.  **`run_spark_job_with_best_params`**:\n    *   **Function**: This component is a custom container that executes a Spark job defined by a YAML manifest. It configures the Spark job to run on a Spark operator cluster within Kubernetes. It accepts the best parameters obtained from a Katib experiment and uses them to configure the Spark job. It uses the `kubernetes` and `kfp.kubernetes` libraries to manage Kubernetes resources. The component's `container` field defines a specific command execution using `sh -c` and references a custom image.\n    *   **Inputs**:\n        *   `best_params_metrics` (Input[Metrics])\n        *   `x_train` (Input[Dataset])\n        *   `x_test` (Input[Dataset])\n        *   `y_train` (Input[Dataset])\n        *   `y_test` (Input[Dataset])\n    *   **Outputs**: None explicitly defined in the provided snippet.\n    *   **Container Spec**:\n        *   Image: `spark:spark-py-operator`\n        *   Command: `sh -c`\n        *   Args: `python /app/main.py --x_train_path ${inputPath:x_train} --y_train_path ${inputPath:y_train} --x_test_path ${inputPath:x_test} --y_test_path ${inputPath:y_test}`. This command dynamically passes the input dataset paths to the Spark application.\n    *   **Kubernetes Configuration**: Sets a `timeout` for the Spark job, defines a `spark_driver_annotations` for `spark-operator.io/restart-policy` and `spark-operator.io/disable-monitoring-metrics`. It also defines `tolerations` for `dedicated` nodes.\n\n**Control Flow and Dependencies**:\n\n1.  The `load_file_from_nas_to_minio` component runs first, taking input paths and producing Dataset artifacts.\n2.  The `parse_input_json` component runs in parallel to `load_file_from_nas_to_minio`, taking a JSON file path and producing separate `Metrics` artifacts for each ML method's input parameters.\n3.  Four Katib experiment components (`run_xgboost_katib_experiment`, `run_random_forest_katib_experiment`, `run_knn_katib_experiment`, `run_lr_katib_experiment`) run in parallel. Each of these components `after` `parse_input_json` and takes the respective `input_params_metrics` output from `parse_input_json` as its input.\n4.  The `run_spark_job_with_best_params` component is designed to run `after` one of the Katib experiments (specifically, `run_xgboost_katib_experiment` is used in the provided `dsl.Condition` example, but it implies similar logic for others). It takes the `best_params_metrics` from a Katib experiment and the dataset outputs from `load_file_from_nas_to_minio` as inputs.\n5.  There is a `dsl.Condition` named `check-xgboost-params-if-exists` that checks if the `xgboost_input_metrics` output from `parse_input_json` exists and its value is not empty. If this condition is met, the `run_spark_job_with_best_params` component is executed, using the `best_params_metrics` from `run_xgboost_katib_experiment` and the dataset outputs.\n\n**Tools and Libraries Used**:\n*   `kfp` (Kubeflow Pipelines SDK)\n*   `pandas`\n*   `json`\n*   `yaml` (implicitly used by `get_spark_job_definition`)\n*   `kubeflow-katib`\n*   `kubernetes` (client for Kubernetes API interaction)\n*   Spark Operator (implied by `run_spark_job_with_best_params` and the `get_spark_job_definition` helper function)."
  },
  {
    "repo": "references_kfp_files",
    "file": "rakesh283343_kubeflow-sample-pipelines_core_retry_retry.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `Retry random failures` that performs the following:\n\nThe pipeline consists of **two** components, both of which are instances of a reusable `random_failure_op` function. This function creates a `ContainerOp` that simulates random failures.\n\n**Component 1: `op1`**\n*   **Function:** `op1` is an instance of `random_failure_op`. It is configured to randomly exit with one of the codes `0, 1, 2, 3`.\n*   **Retry Mechanism:** This component has a retry policy applied, allowing it to **retry up to 10 times** if it fails.\n\n**Component 2: `op2`**\n*   **Function:** `op2` is also an instance of `random_failure_op`. It is configured to randomly exit with one of the codes `0, 1`.\n*   **Retry Mechanism:** This component has a retry policy applied, allowing it to **retry up to 5 times** if it fails.\n\n**Control Flow:**\n*   The two components, `op1` and `op2`, run **in parallel** as there are no explicit dependencies defined between them. Each component's execution is independent, with its own retry logic.\n\n**Libraries/Tools:**\n*   The pipeline uses the `kfp` (Kubeflow Pipelines) SDK, specifically `kfp.dsl` for defining the pipeline and components.\n*   The `random_failure_op` uses a `python:alpine3.6` Docker image and executes a Python script to simulate random exits using the `random` and `sys` modules.\n\n**Pipeline Description:**\nThe pipeline's description is \"The pipeline includes two steps which fail randomly. It shows how to use ContainerOp(...).set_retry(...)\"."
  },
  {
    "repo": "references_kfp_files",
    "file": "mengdong_kubeflow-pipeline-nvidia-example_kubeflow-tf_JoC_end2end_serve.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `End2end Resnet50 Classification` that performs an end-to-end Kubeflow demonstration including TensorRT Inference Server, TF-AMP, and TensorRT.\n\nThe pipeline accepts the following parameters:\n- `num_iter` (default: 20)\n- `batch_size` (default: 1024)\n- `use_tf_amp` (default: 1)\n- `use_auto_loss_scaling` (default: 1)\n- `trtserver_name` (default: 'trtserver')\n- `model_name` (default: 'resnet_graphdef')\n- `model_version` (default: '1')\n- `webapp_prefix` (default: 'webapp')\n- `webapp_port` (default: '80')\n- `storage_bucket` (default: 'gs://dongm-kubeflow')\n- `ckpt_dir` (default: 'ckpt')\n- `mount_dir` (default: '/mnt/vol')\n- `model_dir` (default: 'model_repository')\n- `raw_data_dir` (default: 'raw_data')\n- `processed_data_dir` (default: 'processed_data')\n\nThe pipeline consists of at least one component:\n1.  **`preprocessing`**: This component is a `ContainerOp` that downloads raw data.\n    -   **Image**: `gcr.io/nvidia-sa-org/gcp-joc-end2end-demo-preprocessing`\n    -   **Command**: `python`\n    -   **Arguments**: `download.py --data_dir {mount_dir}/{raw_data_dir}`\n    -   **Resources**: Requires 1 GPU.\n    -   **Volumes**: Mounts a Persistent Volume Claim named `my-rw-pvc` to `/mnt/vol`."
  },
  {
    "repo": "references_kfp_files",
    "file": "tonouchi510_kfp-project_pipelines_hello-world-pipeline_hello-world-pipeline.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `hello-world-pipeline` that performs the following:\n\nThis pipeline consists of **one component**:\n1.  **`hello_op`**: This component takes a `message` as an input parameter. Its function is to output the provided message.\n    *   **Inputs**:\n        *   `message`: A string with a default value of \"hello world\".\n    *   **Resource Configuration**: This component is configured to use a preemptible node pool on GCP.\n    *   **Error Handling**: It has a retry policy configured to retry up to 2 times in case of failure.\n\nThe pipeline definition includes the following input parameters:\n*   `job_id`: A string with a default value of \"xxxx\".\n*   `message`: A string with a default value of \"hello world\".\n\nThere are no explicit dependencies or complex control flow (like parallelFor, or `after` conditions) between multiple components, as there is only one component in this pipeline. The `hello_op` component is executed directly.\n\nThe pipeline is compiled using `kfp.v2.compiler.Compiler` into a JSON package named \"hello-world-pipeline.json\"."
  },
  {
    "repo": "references_kfp_files",
    "file": "pharmbio_kubeflow-pipelines_kensert_CNN_pipeline.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `Kensert_CNN_test` that performs a CNN workflow.\n\nThe pipeline consists of one component at this point:\n1.  **`preprocessing`**: This component uses the `pharmbio/pipelines-kensert-preprocess:test` Docker image.\n    *   **Functionality**: It's responsible for the preprocessing step of the CNN workflow. It takes `model_type` and `checkpoint_preprocess` as arguments.\n    *   **Inputs**:\n        *   `model_type` (PipelineParam, default value: \"Inception_v3\"): Specifies the type of model to be used.\n        *   `checkpoint_preprocess` (PipelineParam, default value: \"false\"): A boolean flag indicating whether to use a checkpoint for preprocessing.\n        *   `workspace_name` (PipelineParam, default value: \"kensert_CNN\"): Passed as an environment variable `WORKFLOW_NAME` to the container.\n    *   **Outputs**: It outputs a file named `labels` which corresponds to `/home/output/bbbc014_labels.npy` within the container.\n    *   **Configuration**: The container has its `image_pull_policy` set to \"Always\".\n\n**Pipeline Parameters**:\nThe pipeline accepts the following input parameters:\n*   `model_type` (default: \"Inception_v3\")\n*   `artifact_bucket` (default: \"kensert_CNN\")\n*   `checkpoint_preprocess` (default: \"false\")\n*   `checkpoint_training` (default: \"false\")\n*   `checkpoint_evaluation` (default: \"false\")\n*   `workspace_name` (default: \"kensert_CNN\")\n*   `model_repo` (default: \"\")\n\n**Utility Functions**:\n*   `mount_pvc`: A utility function that can be applied to a `ContainerOp` to mount a `PersistentVolumeClaim`. It takes `pvc_name`, `volume_name`, `volume_mount_path`, and optional `volume_sub_path` as arguments.\n*   `set_resources`: A utility function to set resource requests and limits (memory, CPU, GPUs) for a container. It takes `memory_req`, `memory_lim`, `cpu_req`, `cpu_lim`, `gpus`, and the `container` object as arguments.\n\n**Control Flow**:\nThe `preprocessing` component is the initial and currently only step in the pipeline. There are no explicit dependencies or parallel executions defined yet, but the prompt should anticipate the addition of `training` and `evaluation` steps as hinted by the unused pipeline parameters."
  },
  {
    "repo": "references_kfp_files",
    "file": "riiid_krsh_tests_samples_have-pipeline-project_pipelines_pipeline-1_pipeline.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `pipeline-1` that performs the following:\n\nThis pipeline has no components or specific control flow defined in the provided code snippet. The pipeline is currently empty."
  },
  {
    "repo": "references_kfp_files",
    "file": "kubeflow_pipelines_samples_test_placeholder_if_v2.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `one-step-pipeline-with-if-placeholder-supply-both` that performs the following:\n\n**Pipeline Name:** `one-step-pipeline-with-if-placeholder-supply-both`\n\n**Number of Components:** 1\n\n**Components:**\n\n1.  **`component_op`**:\n    *   **Function:** This component is designed to echo a required input and conditionally echo two optional inputs based on their presence. If `optional_input_2` is not provided, it defaults to 'default value'.\n    *   **Inputs:**\n        *   `required_input` (String): A mandatory input.\n        *   `optional_input_1` (String, optional): An optional input.\n        *   `optional_input_2` (String, optional): An optional input.\n    *   **Implementation Details:**\n        *   Uses the `registry.k8s.io/busybox` image.\n        *   The command starts with `echo --arg0` followed by the value of `required_input`.\n        *   It includes a conditional `if` statement: if `optional_input_1` is present, it adds `--arg1` and its value to the command.\n        *   It includes another conditional `if` statement: if `optional_input_2` is present, it adds `--arg2` and its value. Otherwise (`else`), it adds `--arg2` and the string 'default value'.\n    *   **Usage within the pipeline:**\n        *   It is instantiated as `component` and receives the following inputs:\n            *   `required_input`: Connected to the pipeline parameter `input0` (default 'input0').\n            *   `optional_input_1`: Connected to the pipeline parameter `input1` (default 'input1').\n            *   `optional_input_2`: Connected to the pipeline parameter `input2` (default 'input2').\n\n**Control Flow:**\n*   The pipeline has three string parameters: `input0`, `input1`, and `input2`, all with default values.\n*   The `component_op` is executed once, receiving all three pipeline parameters as its corresponding inputs.\n\n---\n\nAdditionally, regenerate a Kubeflow Pipeline named `one-step-pipeline-with-if-placeholder-supply-none` that performs the following:\n\n**Pipeline Name:** `one-step-pipeline-with-if-placeholder-supply-none`\n\n**Number of Components:** 1\n\n**Components:**\n\n1.  **`component_op`**:\n    *   **Function:** This component is designed to echo a required input and conditionally echo two optional inputs based on their presence. If `optional_input_2` is not provided, it defaults to 'default value'.\n    *   **Inputs:**\n        *   `required_input` (String): A mandatory input.\n        *   `optional_input_1` (String, optional): An optional input.\n        *   `optional_input_2` (String, optional): An optional input.\n    *   **Implementation Details:**\n        *   Uses the `registry.k8s.io/busybox` image.\n        *   The command starts with `echo --arg0` followed by the value of `required_input`.\n        *   It includes a conditional `if` statement: if `optional_input_1` is present, it adds `--arg1` and its value to the command.\n        *   It includes another conditional `if` statement: if `optional_input_2` is present, it adds `--arg2` and its value. Otherwise (`else`), it adds `--arg2` and the string 'default value'.\n    *   **Usage within the pipeline:**\n        *   It is instantiated as `component` and receives only the `required_input`:\n            *   `required_input`: Connected to the pipeline parameter `input0` (default 'input0').\n        *   `optional_input_1` and `optional_input_2` are *not* provided in this pipeline definition.\n\n**Control Flow:**\n*   The pipeline has one string parameter: `input0` with a default value.\n*   The `component_op` is executed once, receiving only the `input0` pipeline parameter as its `required_input`. The optional inputs are intentionally omitted at compile-time."
  },
  {
    "repo": "references_kfp_files",
    "file": "litovn_kubeflow-autopipe_src_pipeline_manager.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `Pipeline_Manager` that performs a series of operations based on a dynamically loaded DAG configuration.\n\nThis pipeline consists of **multiple** components, with the exact number and names of components being determined at runtime by loading a YAML configuration file.\n\nHere's a breakdown of its functionality and structure:\n\n**1. Configuration Loading:**\n   - The pipeline uses a `load_dag_configuration` utility function to parse a YAML DAG file.\n   - This function takes `dag_path` as an input and returns three pieces of information: a list of `components`, a list of `dependencies`, and the `initial_input_media` file path.\n\n**2. Dynamic Component Creation:**\n   - A `create_component` utility function dynamically defines Kubeflow container components using `@dsl.container_component`.\n   - Each dynamically created component function is named after the `component_name` (with hyphens replaced by underscores).\n   - Each component takes `input_path` and `output_path` as string arguments.\n   - The components execute a Docker image in the format `{username}/{component_name}:latest`.\n   - The container command is `python main.py`, with arguments `-i` (for `input_path`) and `-o` (for `output_path`).\n\n**3. Component Setup and Execution:**\n   - The `setup_component` utility function is responsible for retrieving and configuring the dynamically created component functions.\n   - It takes `component_name`, `input_path`, `output_dir`, and `pvc_name` as arguments.\n   - Each component step in the pipeline will:\n     - Mount a Kubernetes Persistent Volume Claim (PVC) named `pvc_name` to the path `/data`.\n     - The `input_path` for a component will typically be the output path of a preceding component or the `initial_input_media`.\n     - The `output_path` for a component is constructed using `output_dir`.\n\n**4. Control Flow and Dependencies:**\n   - The pipeline's control flow is determined by the `dependencies` loaded from the DAG configuration.\n   - The pipeline dynamically constructs the execution graph, ensuring that components run in the correct order based on their defined dependencies.\n   - Specifically, if component `B` depends on component `A`, then `B` will run `after` `A`.\n\n**5. Inputs and Outputs:**\n   - **Pipeline Input:**\n     - `dag_path`: A string representing the path to the YAML DAG configuration file.\n     - `pvc_name`: A string specifying the name of the Persistent Volume Claim to be used.\n     - `username`: A string representing the Docker username used for component images.\n   - **Component Inputs:** Each dynamic component will receive `input_path` and `output_path` strings.\n   - **Component Outputs:** Each component is expected to save its output file in the specified `output_path` within the mounted PVC. The naming convention for output files is `output_name + \".tar.gz\"`, where `output_name` is derived from the base name of `output_path_dir`.\n\n**6. Libraries and Tools:**\n   - The pipeline heavily relies on `kfp` for Kubeflow Pipeline definition.\n   - `kfp.kubernetes.mount_pvc` is used for PVC mounting.\n   - `yaml` is used for loading DAG configurations.\n   - `os`, `time`, `subprocess`, `logging`, `argparse`, and `dotenv` are used for general Python operations, logging, and environment management within the utility functions.\n\n**Note:** The `get_output_filepath` function is commented out and not used, indicating a design decision to pass explicit output paths rather than relying on directory listing."
  },
  {
    "repo": "references_kfp_files",
    "file": "xxxtrillionarie_KubeFlow_MLOps_Pipelines_samples_test_lightweight_python_functions_v2_pipeline.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `data_passing_pipeline` that performs a data preprocessing and training workflow.\n\nThe pipeline consists of **two components**:\n1.  **`preprocess`**: This component performs a dummy preprocessing step.\n    *   **Inputs**:\n        *   `message` (string): An input parameter.\n        *   `empty_message` (string, optional): An input message that defaults to empty.\n    *   **Outputs**:\n        *   `output_dataset_one` (Output[Dataset]): A metadata-rich handle to an output artifact of type `Dataset`.\n        *   `output_dataset_two_path` (OutputPath('Dataset')): A locally accessible filepath for another output artifact of type `Dataset`.\n        *   `output_parameter_path` (OutputPath(str)): A locally accessible filepath for an output parameter of type string.\n        *   `output_bool_parameter_path` (OutputPath(bool)): A locally accessible filepath for an output parameter of type boolean.\n        *   `output_dict_parameter_path` (OutputPath(Dict[str, int])): A locally accessible filepath for an output parameter of type dictionary.\n        *   `output_list_parameter_path` (OutputPath(List[str])): A locally accessible filepath for an output parameter of type list.\n    *   **Functionality**: It writes the input `message` to `output_dataset_one` and `output_dataset_two_path`. It also writes the `message` to `output_parameter_path`, `True` to `output_bool_parameter_path`, `{'A': 1, 'B': 2}` (as JSON) to `output_dict_parameter_path`, and `['a', 'b', 'c']` (as JSON) to `output_list_parameter_path`. It uses the `json` library for serializing dictionary and list outputs.\n2.  **`train`**: This component performs a dummy training step.\n    *   **Inputs**:\n        *   `dataset_one` (Input[Dataset]): An input artifact of type `Dataset`.\n        *   `dataset_two_path` (InputPath('Dataset')): A locally accessible path for another input artifact of type `Dataset`.\n        *   `message` (Input[str]): An input parameter of type string.\n        *   `boolean` (Input[bool]): An input parameter of type boolean.\n        *   `dictionary` (Input[Dict[str, int]]): An input parameter of type dictionary.\n        *   `a_list` (Input[List[str]]): An input parameter of type list.\n    *   **Outputs**: None explicitly defined in the provided snippet, but it implies consumption of inputs.\n    *   **Functionality**: It reads content from `dataset_one.path` and `dataset_two_path`. It also prints the `message`, `boolean`, `dictionary`, and `a_list` inputs. It uses the `json` library for deserializing dictionary and list inputs.\n\n**Control Flow**:\nThe pipeline executes sequentially. The `train` component depends on the outputs of the `preprocess` component.\n*   The `dataset_one` input of `train` receives the `output_dataset_one` from `preprocess`.\n*   The `dataset_two_path` input of `train` receives the `output_dataset_two_path` from `preprocess`.\n*   The `message` input of `train` receives the `output_parameter_path` from `preprocess`.\n*   The `boolean` input of `train` receives the `output_bool_parameter_path` from `preprocess`.\n*   The `dictionary` input of `train` receives the `output_dict_parameter_path` from `preprocess`.\n*   The `a_list` input of `train` receives the `output_list_parameter_path` from `preprocess`.\n\n**Pipeline Inputs**:\nThe `data_passing_pipeline` takes a single input parameter:\n*   `message` (str): Defaults to \"Hello world!\".\n\n**Tools/Libraries**: The components utilize the `json` standard library for serializing/deserializing dictionary and list data. The pipeline definition itself uses the `kfp` library, specifically `kfp.dsl` for `@dsl.pipeline`, `@component`, `Input`, `InputPath`, `Output`, and `OutputPath` decorators/types."
  },
  {
    "repo": "references_kfp_files",
    "file": "stackdemos_jjtest-ml_components_training_component.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `stackdemos-jjtest-ml-components-training-component` that performs the following steps:\n\nThe pipeline consists of two primary component definitions: `training_op` and `http_download_op`.\n\n1.  **`training_op` Component**:\n    *   **Function**: Encapsulates a container operation for running a Python training script. It dynamically sets the container image.\n    *   **Inputs**:\n        *   `script` (Python string): Path to the Python script to be executed.\n        *   `image` (Python string, optional): Docker image to use for the container. If not provided and running in an IPython environment, it attempts to retrieve `TRAINING_IMAGE` from the IPython user namespace.\n        *   `arguments` (Python list, optional): A list of command-line arguments to pass to the script.\n        *   `file_outputs` (Python dictionary, optional): Specifies output files to be captured by Kubeflow.\n    *   **Output**: Returns a `kfp.dsl.ContainerOp` instance.\n    *   **Logic**:\n        *   Checks if `image` is provided. If not, and running in IPython, tries to get `TRAINING_IMAGE` from `IPython.get_ipython().user_ns`.\n        *   Raises a `ValueError` if `image` is still missing.\n        *   The component's name is derived from the `script` parameter, with non-alphanumeric characters replaced by hyphens.\n        *   The container command is `/usr/local/bin/python` followed by the `script` path.\n\n2.  **`http_download_op` Component**:\n    *   **Function**: Downloads a file from a given URL using `curl` and performs an MD5 checksum pre-check to avoid re-downloading if the file already exists and matches the checksum.\n    *   **Inputs**:\n        *   `url` (Python string): The URL of the file to download.\n        *   `download_to` (Python string): The local path where the file should be saved.\n        *   `md5sum` (Python string): The expected MD5 checksum of the file.\n    *   **Output**: Returns a `kfp.dsl.ContainerOp` instance.\n    *   **Logic**:\n        *   The component is named `download-artifact`.\n        *   It uses the `appropriate/curl` Docker image.\n        *   The command executed is `sh -c`.\n        *   The arguments contain a shell script that first checks if the `md5sum` of the existing `download_to` file matches the provided `md5sum`. If they match, it prints a \"Skipping\" message. Otherwise, it downloads the file using `curl -#Lv --create-dirs -o {download_to} {url}`.\n\n**Control Flow**:\nThe provided Python file defines component factory functions, not a complete pipeline structure using `@dsl.pipeline`. Therefore, there are no explicit dependencies, parallel execution, or `after` conditions defined at this level. These functions are designed to be called within a larger Kubeflow pipeline definition to instantiate the actual components."
  },
  {
    "repo": "references_kfp_files",
    "file": "xxxtrillionarie_KubeFlow_MLOps_Pipelines_samples_test_fail_test.py",
    "structured_prompt": "Here's the prompt you requested, designed for an LLM to regenerate the Kubeflow Pipeline:\n\nGenerate a Kubeflow Pipeline named `fail-pipeline` that performs the following:\n\nThis pipeline consists of **one** component:\n- **Component `fail`**: This component is designed to fail during its execution. Its primary function is to demonstrate a failing pipeline scenario. It does not take any explicit inputs or produce any explicit outputs that are passed between components.\n\nThe control flow of the pipeline is straightforward:\n- The `fail` component executes directly without any dependencies on other tasks.\n\nNo specific external tools or libraries like sklearn or Snowflake are explicitly used or demonstrated within this pipeline's component logic; its purpose is to show a basic failure."
  },
  {
    "repo": "references_kfp_files",
    "file": "rakesh283343_kubeflow-sample-pipelines_core_hello_world_hello_world.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `My first pipeline` that performs the following:\n\nThis pipeline consists of a single component.\n\nThe component is named `echo` and its function is to print \"hello world\" to standard output. It uses the `library/bash:4.4.23` Docker image and executes the command `sh -c 'echo \"hello world\"'`.\n\nThe control flow is straightforward; there are no complex dependencies or parallel executions as it's a single-component pipeline."
  },
  {
    "repo": "references_kfp_files",
    "file": "akranga_anton-ml1_components_training_component.py",
    "structured_prompt": "This is not a Kubeflow pipeline definition. It's a Python file containing helper functions (`_is_ipython`, `training_op`, `http_download_op`) that could be used *within* a Kubeflow pipeline to define components.\n\nTo provide a prompt for an LLM, I need an actual Kubeflow pipeline decorated with `@dsl.pipeline` and `@component`.\n\n**Please provide the actual Kubeflow pipeline code, including the `@dsl.pipeline` and `@component` decorators, for me to generate the prompt.**"
  },
  {
    "repo": "references_kfp_files",
    "file": "aiffel-socar-cv_kubeflow-pipeline_baseline_retrain_pipeline.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `retrain` that performs model retraining based on new data availability.\n\nThis pipeline consists of two main components:\n1.  **`check_cnt`**: This component, implemented as a Python function, is responsible for counting the number of newly added images in a specified Google Cloud Storage bucket.\n    *   **Inputs**:\n        *   `bucket_name` (str): The name of the GCS bucket to check.\n        *   `data_type` (str): A string indicating the type of data (its specific usage is not shown in the provided snippet but is part of the function signature).\n    *   **Outputs**:\n        *   A `NamedTuple` with two fields:\n            *   `count` (int): The calculated number of images.\n            *   `type` (str): The `data_type` input passed through.\n    *   **Functionality**: Connects to Google Cloud Storage (`google.cloud.storage`) to list blobs within a specified prefix (`\"originals\"`) in the `\"images-original\"` bucket and returns the count of these blobs, excluding the root directory.\n\n2.  **`retrain_op`**: This component is a `ContainerOp` that executes a retraining script.\n    *   **Image**: `us-west1-docker.pkg.dev/viai/retrain:v1.0`\n    *   **Arguments**:\n        *   `--data-dir`: The directory containing training data (represented by the `data_dir` variable).\n        *   `--torch-export-dir`: The directory where the trained model should be exported (represented by the `model_export_dir` variable).\n    *   **Functionality**: Executes a Python script `retrain.py` within the container image to perform the model retraining process.\n\nThe pipeline flow is as follows:\n*   The `check_cnt` component is executed first.\n*   A **conditional step** is introduced: The `retrain_op` component is only executed if the `count` output from the `check_cnt` component is greater than 100.\n*   **Parameters**: The pipeline accepts the following parameters:\n    *   `name` (default: \"retrain\")\n    *   `training_image` (default: \"gcr.io/aiffel-gn-3/retrain:latest\")\n    *   `training_namespace` (default: \"kubeflow\")\n    *   `model_export_dir` (default: \"gs://model-cpt/\")\n*   **GCP Integration**: If the `platform` variable is set to \"GCP\" (which is assumed outside the pipeline definition), the `retrain_op` component will apply a GCP secret named `user-gcp-sa` for authentication.\n\n**Libraries used**: `kfp`, `kfp.dsl`, `kfp.gcp`, `google.cloud.storage`."
  },
  {
    "repo": "references_kfp_files",
    "file": "rakesh283343_kubeflow-sample-pipelines_core_pipeline_transformers_pipeline_transformers.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `Pipeline transformer` that performs the following:\n\nThis pipeline demonstrates how to apply a common transformation to all operations within the pipeline using a pipeline transformer.\n\nIt consists of **two** components:\n\n1.  **`Print`**: This component, instantiated twice as `op1` and `op2`, prints a specified message.\n    *   The first instance (`op1`) prints \"hey, what are you up to?\".\n    *   The second instance (`op2`) prints \"train my model.\".\n\n**Control Flow and Transformations**:\n\n*   Both `op1` and `op2` run independently.\n*   Before the pipeline execution, a pipeline-level transformer function named `add_annotation` is applied to all operations within the pipeline.\n*   The `add_annotation` function adds a pod annotation with the name `hobby` and value `football` to each operation it transforms.\n\n**Inputs/Outputs**:\n\n*   Each `Print` component takes a string message as input.\n*   There are no explicit outputs passed between components.\n\n**Libraries/Tools**:\n\n*   `kfp` (Kubeflow Pipelines SDK) for defining the pipeline and components.\n*   `alpine:3.6` Docker image is used for the `Print` component."
  },
  {
    "repo": "references_kfp_files",
    "file": "stackdemos_kfappx_components_golang_component.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `stackdemos_kfappx_components_golang_component` that performs the following:\n\nThe pipeline consists of two primary utility functions designed to generate `ContainerOp` instances:\n\n1.  **`processing_op`**:\n    *   **Function**: This function serves as a template to create a `ContainerOp` for executing a given script within a specified container image. It encapsulates common parameters for containerized processing steps.\n    *   **Inputs**:\n        *   `script` (string): The path to the script to be executed within the container.\n        *   `image` (string, optional): The Docker image to use for the container. If not provided and running in an IPython environment, it attempts to retrieve the image from `IPython.get_ipython().user_ns.get('GOLANG_IMAGE')`. A `ValueError` is raised if `image` is not provided and cannot be inferred.\n        *   `arguments` (list of strings, optional): A list of command-line arguments to pass to the script. Defaults to an empty list.\n        *   `file_outputs` (dictionary, optional): A dictionary mapping output names to file paths that will be captured as component outputs. Defaults to an empty dictionary.\n    *   **Outputs**: Returns a `kfp.dsl.ContainerOp` instance.\n    *   **Logic**:\n        *   Dynamically sets the component name by cleaning the script name (converting to lowercase and replacing non-alphanumeric characters with hyphens).\n        *   Uses `script` as the primary command to execute.\n        *   Passes `arguments` and `file_outputs` directly to the `ContainerOp`.\n    *   **Dependencies/Tools**: Utilizes `kfp.dsl.ContainerOp`, `os`, `re`, and `IPython` (conditionally for `_is_ipython` helper function).\n\n2.  **`http_download_op`**:\n    *   **Function**: This function creates a `ContainerOp` to download a file from a given URL using `curl`, with an optional MD5 checksum pre-check to avoid re-downloading if the file already exists and matches the checksum.\n    *   **Inputs**:\n        *   `url` (string): The URL of the file to download.\n        *   `download_to` (string): The local path where the file should be downloaded.\n        *   `md5sum` (string): The expected MD5 checksum of the file. If the file at `download_to` already exists and its MD5 matches this `md5sum`, the download will be skipped.\n    *   **Outputs**: Returns a `kfp.dsl.ContainerOp` instance.\n    *   **Logic**:\n        *   Sets the component name to 'download-artifact'.\n        *   Uses the `appropriate/curl` Docker image.\n        *   Executes a shell script (`sh -c`) that first checks if the `md5sum` of the existing `download_to` file matches the provided `md5sum`. If they match, it prints a skipping message. Otherwise, it uses `curl` to download the file, creating directories as needed, and outputs verbose information.\n    *   **Dependencies/Tools**: Utilizes `kfp.dsl.ContainerOp` and `urllib.parse` (though `urlparse` is imported, it's not directly used in the provided function logic for this specific op), and relies on standard Unix tools like `test`, `awk`, and `curl` within the container image.\n\nThe pipeline itself is defined by these two reusable component functions, which would typically be called within a `@dsl.pipeline` decorated function to compose a complete workflow."
  },
  {
    "repo": "references_kfp_files",
    "file": "Mastercard_mastercard-labs-ml-pipeline_pipeline_steps_training_katib-launcher_kubeflow_katib_launcher_op.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `Katib Launcher` that performs hyperparameter tuning.\n\nThe pipeline consists of a single component:\n1.  **`StudyJob-Launcher`**: This component launches a Kubeflow Katib StudyJob.\n    *   **Inputs**:\n        *   `name` (string): The name for the Katib StudyJob.\n        *   `namespace` (string): The Kubernetes namespace where the StudyJob will be created.\n        *   `optimizationtype` (string): The type of optimization (e.g., `maximize`, `minimize`).\n        *   `objectivevaluename` (string): The name of the metric to optimize.\n        *   `optimizationgoal` (float): The target value for the objective metric.\n        *   `requestcount` (int): The number of trials to request.\n        *   `metricsnames` (string): Comma-separated list of metrics to collect.\n        *   `parameterconfigs` (string): JSON string representing the hyperparameter configuration.\n        *   `nasConfig` (string): JSON string representing Neural Architecture Search configuration (if applicable).\n        *   `workertemplatepath` (string): Path to the worker template YAML file.\n        *   `mcollectortemplatepath` (string): Path to the metrics collector template YAML file.\n        *   `suggestionspec` (string): JSON string representing the Katib Suggestion specification.\n        *   `studyjob_timeout_minutes` (int): Timeout for the StudyJob in minutes.\n        *   `delete` (boolean, default: `True`): Whether to delete the StudyJob after completion.\n        *   `output_file` (string, default: `/output.txt`): The path for the output file containing hyperparameter results.\n    *   **Outputs**:\n        *   `hyperparameter` (string): The content of the `output_file`, which typically contains the best hyperparameters found.\n    *   **Image**: `liuhougangxa/ml-pipeline-kubeflow-studyjob:latest`\n    *   **Purpose**: Orchestrates the execution of a Katib hyperparameter tuning experiment, managing its lifecycle and extracting the results.\n\nThe control flow is sequential, as there is only one component. The pipeline leverages the Kubeflow Katib tool for hyperparameter optimization."
  },
  {
    "repo": "references_kfp_files",
    "file": "levitomer_kubeflow-pipeline-demo_pipeline_pipeline.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `Pipeline` that performs an end-to-end machine learning workflow including data preprocessing, model training, model testing, and model deployment.\n\nThe pipeline consists of four components:\n\n1.  **`Preprocess Data`**:\n    *   **Function**: Preprocesses raw data into training and testing sets.\n    *   **Inputs**: None.\n    *   **Outputs**:\n        *   `x_train` (file path: `/app/x_train.npy`)\n        *   `x_test` (file path: `/app/x_test.npy`)\n        *   `y_train` (file path: `/app/y_train.npy`)\n        *   `y_test` (file path: `/app/y_test.npy`)\n    *   **Image**: `tomerlev/pipeline_preprocessing:latest`\n\n2.  **`Train Model`**:\n    *   **Function**: Trains a machine learning model using the preprocessed training data.\n    *   **Inputs**:\n        *   `x_train` (from `Preprocess Data`)\n        *   `y_train` (from `Preprocess Data`)\n    *   **Outputs**:\n        *   `model` (file path: `/app/model.pkl`)\n    *   **Image**: `tomerlev/pipeline_train:latest`\n\n3.  **`Test Model`**:\n    *   **Function**: Evaluates the trained model using the preprocessed test data.\n    *   **Inputs**:\n        *   `x_test` (from `Preprocess Data`)\n        *   `y_test` (from `Preprocess Data`)\n        *   `model` (from `Train Model`)\n    *   **Outputs**:\n        *   `mean_squared_error` (file path: `/app/output.txt`)\n    *   **Image**: `tomerlev/pipeline_test:latest`\n\n4.  **`Deploy Model`**:\n    *   **Function**: Deploys the trained model.\n    *   **Inputs**:\n        *   `model` (from `Train Model`)\n    *   **Outputs**: None.\n    *   **Image**: `tomerlev/pipeline_deploy_model:latest`\n\n**Control Flow**:\n*   `Train Model` runs after `Preprocess Data` completes.\n*   `Test Model` runs after `Train Model` completes.\n*   `Deploy Model` runs after `Test Model` completes.\n\nThe pipeline should be compiled to `./pipeline.yaml` and then uploaded to a Kubeflow Pipelines instance with the name \"pipeline\"."
  },
  {
    "repo": "references_kfp_files",
    "file": "stackdemos_kcdemo_components_training_component.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `KCDemoPipeline` that performs the following steps:\n\nThe pipeline consists of three components:\n1.  **`download-artifact`**: This component is a `ContainerOp` that downloads a file from a specified URL to a local path.\n    *   **Function**: Downloads an artifact using `curl` and performs an MD5 checksum pre-check to avoid re-downloading if the file already exists and matches the checksum.\n    *   **Inputs**:\n        *   `url`: The URL of the file to download.\n        *   `download_to`: The local path where the file should be saved.\n        *   `md5sum`: The expected MD5 checksum of the downloaded file for validation.\n    *   **Tool/Library**: Uses the `appropriate/curl` Docker image and `sh` for execution.\n\n2.  **`_is_ipython`**: This is a helper function that checks if the code is running within an IPython environment (e.g., Jupyter Notebook).\n    *   **Function**: Returns `True` if `IPython` is imported and available, `False` otherwise.\n    *   **Note**: This is a utility function used internally by `training_op` and is not a standalone pipeline component.\n\n3.  **`training_op`**: This component is a `ContainerOp` designed for running a Python training script.\n    *   **Function**: Executes a Python script within a specified Docker image, passing arguments and defining file outputs. It dynamically determines the image if running in an IPython environment and `TRAINING_IMAGE` is set.\n    *   **Inputs**:\n        *   `script`: The path to the Python script to execute.\n        *   `image`: The Docker image to use for the container. If not provided, it attempts to get it from `IPython.get_ipython().user_ns.get('TRAINING_IMAGE')` if in an IPython environment.\n        *   `arguments`: A list of command-line arguments to pass to the script.\n        *   `file_outputs`: A dictionary mapping output names to file paths that should be captured as component outputs.\n    *   **Naming Convention**: The component name is derived from the `script` parameter by converting it to lowercase and replacing non-alphanumeric characters with hyphens.\n    *   **Dependencies**: Internally uses the `_is_ipython` helper function.\n\n**Control Flow**:\nThe components (`download-artifact` and `training_op`) are independent functions that can be called to create `ContainerOp` instances. The `_is_ipython` function is a helper used internally by `training_op`. There are no explicit dependencies or sequential/parallel execution defined within this prompt, as the provided code only defines the *components themselves*, not a pipeline orchestrating them. A future pipeline would orchestrate these components using `.after()` or by passing outputs as inputs."
  },
  {
    "repo": "references_kfp_files",
    "file": "tonouchi510_kfp-project_pipelines_simple-training-pipeline_simple-training-pipeline.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `simple-training-pipeline` that performs an image classification training process.\n\nThe pipeline comprises three components:\n1.  **`training`**: This component is responsible for the core machine learning training. It takes the following inputs: `pipeline_name`, `bucket_name`, `job_id`, `global_batch_size`, `epochs`, `learning_rate`, `dataset`, `model_type`, `image_size`, and `num_classes`. It should be named \"training\" in the pipeline graph. This component utilizes GCP's preemptible nodepool and a TPU with 8 v3 cores running TensorFlow 2.8.0. It should be configured to retry up to 2 times upon failure.\n2.  **`tb-observer`**: This component is used to observe and visualize training logs, likely generating a TensorBoard instance. It takes `pipeline_name`, `bucket_name`, `job_id`, and `tblog_dir` as inputs. It should be named \"tboard\" in the pipeline graph. This component also utilizes GCP's preemptible nodepool.\n3.  **`slack-notification`**: This component sends notifications to Slack. It takes `pipeline_name`, `job_id`, and `message` as inputs. The message should dynamically include the workflow status (`\"Status: {{workflow.status}}\"`).\n\n**Control Flow:**\nThe pipeline uses a `dsl.ExitHandler` to ensure that the `slack-notification` component is always executed at the end of the pipeline, regardless of whether the other steps succeed or fail.\nInside the `ExitHandler`, the `training` component runs first. Upon its completion (success or failure), the `slack-notification` component is triggered as the exit handler.\nThe `tb-observer` component runs in parallel with or after the `training` component, as there is no explicit `after` dependency specified, but it logically depends on training logs being generated.\n\n**Inputs and Outputs:**\nThe pipeline takes the following parameters:\n*   `pipeline_name` (str, default: \"simple-training-pipeline\")\n*   `bucket_name` (str, default: \"kfp-project\")\n*   `job_id` (str, default: \"{{JOB_ID}}\")\n*   `model_type` (str, default: \"resnet\")\n*   `global_batch_size` (int, default: 1024)\n*   `epochs` (int, default: 30)\n*   `lr` (float, default: 0.001)\n*   `image_size` (int, default: 64)\n*   `num_classes` (int, default: 100)\n*   `dataset` (str, default: \"gs://kfp-project/datasets/mnist\")\n\nThe components are assumed to be loaded from a `kfp.components.ComponentStore` with `local_search_paths=[\"pipelines/simple-training-pipeline\", \"components\"]`.\n\n**Tools and Libraries:**\nThe pipeline heavily leverages Kubeflow Pipelines (KFP) for orchestration and Google Cloud Platform (GCP) services (specifically preemptible nodepools and TPUs) for execution. It also implicitly uses TensorFlow (version 2.8.0) within the TPU environment for the training component."
  },
  {
    "repo": "references_kfp_files",
    "file": "ZoieD_kfp-resnet_pipeline_src_pipeline.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `resnet_cifar10_pipeline` that performs an end-to-end training and serving workflow using ResNet and CIFAR-10.\n\nThe pipeline consists of four components: `PreprocessOp`, `TrainOp`, `InferenceServerLauncherOp`, and `WebappLauncherOp`.\n\n1.  **`PreprocessOp`**:\n    *   **Function**: Preprocesses raw data.\n    *   **Inputs**: `name` (string), `input_dir` (string), `output_dir` (string).\n    *   **Outputs**: A file output named `output` from `/output.txt`.\n    *   **Image**: `zdou001/only_tests:preprocess-image`.\n    *   **Arguments**: `--input_dir` mapped to the `input_dir` parameter and `--output_dir` mapped to the `output_dir` parameter.\n\n2.  **`TrainOp`**:\n    *   **Function**: Trains a ResNet model.\n    *   **Inputs**: `name` (string), `input_dir` (string), `output_dir` (string), `model_name` (string), `model_version` (integer), `epochs` (integer).\n    *   **Outputs**: A file output named `output` from `/output.txt`.\n    *   **Image**: `zdou001/only_tests:train-image`.\n    *   **Arguments**: `--input_dir`, `--output_dir`, `--model_name`, `--model_version`, and `--epochs` mapped to their respective parameters.\n\n3.  **`InferenceServerLauncherOp`**:\n    *   **Function**: Launches an inference server (Triton Inference Server).\n    *   **Inputs**: `name` (string), `input_dir` (string), `trtserver_name` (string).\n    *   **Outputs**: A file output named `output` from `/output.txt`.\n    *   **Image**: `zdou001/only_tests:inference-server-launcher-image`.\n    *   **Arguments**: `--trtserver_name` and `--model_path` (mapped to `input_dir`) mapped to their respective parameters.\n\n4.  **`WebappLauncherOp`**:\n    *   **Function**: Launches a web application for inference.\n    *   **Inputs**: `name` (string), `trtserver_name` (string), `model_name` (string), `model_version` (integer), `webapp_prefix` (string), `webapp_port` (integer).\n    *   **Outputs**: No explicit file outputs.\n    *   **Image**: `zdou001/only_tests:webapp-launcher-image`.\n    *   **Arguments**: `--workflow_name` (using Kubeflow's built-in `{{workflow.name}}`), `--trtserver_name`, `--model_name`, `--model_version`, `--webapp_prefix`, and `--webapp_port` mapped to their respective parameters.\n\nThe pipeline definition `resnet_pipeline` takes the following parameters:\n*   `raw_data_dir` (default: `/mnt/workspace/raw_data`)\n*   `processed_data_dir` (default: `/mnt/workspace/processed_data`)\n*   `model_dir` (default: `/mnt/workspace/saved_model`)\n*   `epochs` (default: `50`)\n*   `trtserver_name` (default: `trtis`)\n*   `model_name` (default: `resnet_graphdef`)\n*   `model_version` (default: `1`)\n*   `webapp_prefix` (default: `webapp`)\n*   `webapp_port` (default: `80`)\n\nThe pipeline should define a persistent volume named `nvidia-workspace` and mount it to `/mnt/workspace` for all components.\n\n**Control Flow:**\n1.  The `PreprocessOp` named `preprocess` should run first, taking `raw_data_dir` as `input_dir` and `processed_data_dir` as `output_dir`.\n2.  The `TrainOp` named `train` should run after `preprocess` completes. It takes the output of `preprocess` as its `input_dir`, `model_dir` as `output_dir`, and the pipeline parameters `model_name`, `model_version`, and `epochs`.\n3.  The `InferenceServerLauncherOp` named `trtserver` should run after `train` completes. It takes the output of `train` as its `input_dir` and the pipeline parameter `trtserver_name`.\n4.  The `WebappLauncherOp` named `webapp` should run after `trtserver` completes. It takes the pipeline parameters `trtserver_name`, `model_name`, `model_version`, `webapp_prefix`, and `webapp_port`.\n\nEach component must have its image specified as shown above. The `datetime` and `os` libraries are imported but not explicitly used in the provided pipeline structure; `kubernetes` client (aliased as `k8s_client`) is also imported, indicating potential use for volume-related configurations. Ensure the definition of the persistent volume and its application to each task is correct."
  },
  {
    "repo": "references_kfp_files",
    "file": "xxxtrillionarie_KubeFlow_MLOps_Pipelines_samples_test_metrics_visualization_v1.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `metrics-visualization-v1-pipeline` that performs a series of visualization tasks.\n\nThe pipeline consists of five components, all running in parallel:\n\n1.  **`confusion_visualization`**: This component, imported from `../core/visualization/confusion_matrix`, is responsible for generating a confusion matrix visualization. It does not take any explicit inputs visible in the pipeline definition.\n\n2.  **`html_visualization`**: This component, imported from `../core/visualization/html`, generates an HTML visualization. It takes one string input, initialized with an empty string `\"\"`.\n\n3.  **`markdown_visualization`**: This component, imported from `../core/visualization/markdown`, generates a Markdown visualization. It does not take any explicit inputs visible in the pipeline definition.\n\n4.  **`roc_visualization`**: This component, imported from `../core/visualization/roc`, generates a Receiver Operating Characteristic (ROC) curve visualization. It does not take any explicit inputs visible in the pipeline definition.\n\n5.  **`table_visualization`**: This component, imported from `../core/visualization/table`, generates a table visualization. It does not take any explicit inputs visible in the pipeline definition.\n\nThere are no explicit dependencies or control flow mechanisms like `after` or `parallelFor` defined between these components; they are all instantiated directly within the pipeline function, implying they can run in parallel.\n\nThe pipeline primarily utilizes internal `kfp.deprecated.dsl` features and custom visualization components imported from relative paths (`../core/visualization/`). No external libraries like `sklearn` or external services like `Snowflake` are explicitly used within the pipeline definition itself."
  },
  {
    "repo": "references_kfp_files",
    "file": "akranga_machine-learning1_components_golang_component.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `GolangComponent` that performs the following steps:\n\nThis pipeline consists of two reusable utility components: `processing_op` and `http_download_op`. These components are designed to be flexible and are not directly chained within a single, fixed pipeline definition in the provided code. Instead, they serve as building blocks that can be used independently or combined in various ways to construct pipelines.\n\nHere's a breakdown of each component:\n\n**Component 1: `processing_op`**\n*   **Function:** This component is a generic template for running a containerized script. It encapsulates the common logic for creating `ContainerOp` instances in Kubeflow.\n*   **Inputs:**\n    *   `script` (str): The path to the script to be executed within the container.\n    *   `image` (str, optional): The Docker image to use for the container. If not provided and running in an IPython environment, it attempts to fetch `GOLANG_IMAGE` from the IPython user namespace.\n    *   `arguments` (list, optional): A list of arguments to pass to the script. Defaults to an empty list.\n    *   `file_outputs` (dict, optional): A dictionary defining file outputs for the component. Defaults to an empty dictionary.\n*   **Outputs:** None explicitly defined as file outputs, but the component returns a `ContainerOp` object which can have outputs configured via `file_outputs`.\n*   **Tools/Libraries:** `os`, `re`, `IPython` (conditionally for `_is_ipython` check).\n*   **Control Flow:** It performs a check to determine if it's running in an IPython environment to dynamically set the `image` if not provided. It raises a `ValueError` if `image` is still missing.\n\n**Component 2: `http_download_op`**\n*   **Function:** This component is designed to download a file from a specified URL using `curl` and includes an MD5 checksum pre-check to avoid re-downloading if the file already exists and matches the checksum.\n*   **Inputs:**\n    *   `url` (str): The URL of the file to download.\n    *   `download_to` (str): The local path where the file should be saved.\n    *   `md5sum` (str): The expected MD5 checksum of the file.\n*   **Outputs:** None explicitly defined as file outputs. The operation's primary \"output\" is the downloaded file on the container's filesystem.\n*   **Tools/Libraries:** `appropriate/curl` Docker image is used, executing `sh -c` commands involving `curl` and `awk`.\n*   **Control Flow:** The `command` arguments include a shell script that first checks if the `md5sum` of the existing file at `download_to` matches the provided `md5sum`. If they match, it prints a message and skips the download; otherwise, it proceeds with the `curl` download.\n\n**Overall Pipeline Structure:**\n\nThe provided Python file defines **reusable Kubeflow `ContainerOp` factory functions** rather than a single, complete `@dsl.pipeline` decorated function. To generate a pipeline that *uses* these, one would typically define a separate `@dsl.pipeline` that calls these functions to construct its steps. For example:\n\n```python\nfrom kfp import dsl\n\n@dsl.pipeline(name='ExamplePipelineUsingUtilities')\ndef example_pipeline(\n    download_url: str = 'http://example.com/data.zip',\n    target_path: str = '/data/downloaded.zip',\n    expected_md5: str = 'abcdef1234567890abcdef1234567890',\n    processing_script: str = 'my_processing_script.sh',\n    processing_image: str = 'my_custom_image'\n):\n    # Step 1: Download data using http_download_op\n    download_task = http_download_op(\n        url=download_url,\n        download_to=target_path,\n        md5sum=expected_md5\n    )\n\n    # Step 2: Process the downloaded data using processing_op\n    # This step implicitly depends on the download_task completing\n    process_task = processing_op(\n        script=processing_script,\n        image=processing_image,\n        arguments=[f'--input={download_task.output}'] # Example of passing an output if download_op had one\n    ).after(download_task) # Explicit dependency\n\n    # Note: The original http_download_op doesn't have a direct .output for the file path,\n    # so a more robust approach might involve volume sharing or passing the path directly.\n    # The example above is illustrative of how dependencies would work.\n```"
  },
  {
    "repo": "references_kfp_files",
    "file": "ferneutron_mlops_src_pipelines_components_evaluation.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `ModelEvaluationPipeline` that performs the following:\n\nThe pipeline consists of a single component:\n1.  **`choose_best_model`**: This component takes two trained models (a Decision Tree and a Random Forest) and a test dataset as input. It evaluates both models on the test dataset, logs their respective accuracy scores, and then selects and saves the model with the higher accuracy as the `best_model`.\n\n**Inputs/Outputs:**\n*   **`choose_best_model` component:**\n    *   **Inputs:**\n        *   `test_dataset`: An `Input[Dataset]` containing the data for evaluation.\n        *   `decision_tree_model`: An `Input[Model]` representing the trained Decision Tree model.\n        *   `random_forest_model`: An `Input[Model]` representing the trained Random Forest model.\n    *   **Outputs:**\n        *   `metrics`: An `Output[Metrics]` object to log the accuracy scores of both models.\n        *   `best_model`: An `Output[Model]` representing the selected best performing model.\n\n**Control Flow:**\n*   The pipeline executes the `choose_best_model` component directly. There are no dependencies or advanced control flow mechanisms like `parallelFor` or `after` specified, as it's a single-component pipeline.\n\n**Tools/Libraries Used:**\n*   The `choose_best_model` component uses `pandas` for data handling, `joblib` for loading and saving models, and `sklearn.metrics.accuracy_score` for model evaluation.\n*   The base image for the component is `gcr.io/deeplearning-platform-release/tf2-cpu.2-6:latest`.\n*   The component explicitly installs `pandas==1.3.5` and `joblib==1.1.0`."
  },
  {
    "repo": "references_kfp_files",
    "file": "stackdemos_secrets-tetst-01_components_golang_component.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `GolangComponentTest` that performs the following steps:\n\nThe pipeline consists of 3 components:\n1.  **`download-golang-compiler`**: This component downloads a Go compiler.\n    *   It uses the `http_download_op` function.\n    *   **Inputs**:\n        *   `url`: A string representing the URL to download the Go compiler from.\n        *   `download_to`: A string specifying the path where the compiler should be downloaded.\n        *   `md5sum`: A string representing the MD5 checksum for integrity verification.\n    *   **Function**: Downloads the Go compiler executable from the provided URL to the specified path and verifies its integrity using the MD5 checksum.\n    *   **Tool/Library**: `curl` for downloading.\n\n2.  **`download-helloworld-source`**: This component downloads a \"Hello World\" Go source file.\n    *   It uses the `http_download_op` function.\n    *   **Inputs**:\n        *   `url`: A string representing the URL to download the Go source file from.\n        *   `download_to`: A string specifying the path where the source file should be downloaded.\n        *   `md5sum`: A string representing the MD5 checksum for integrity verification.\n    *   **Function**: Downloads the \"Hello World\" Go source file from the provided URL to the specified path and verifies its integrity using the MD5 checksum.\n    *   **Tool/Library**: `curl` for downloading.\n\n3.  **`golang-component`**: This component compiles and runs the \"Hello World\" Go program.\n    *   It uses the `processing_op` function.\n    *   **Inputs**:\n        *   `script`: A string representing the path to the Go program to be executed (`/usr/local/go/bin/go run /app/hello.go`).\n        *   `image`: A string specifying the Docker image to use for this component (e.g., `golang:1.16`).\n    *   **Function**: Executes the Go program using the Go compiler downloaded in the first step.\n    *   **Dependencies**: This component depends on both `download-golang-compiler` and `download-helloworld-source` completing successfully.\n\n**Control Flow**:\n*   `download-golang-compiler` and `download-helloworld-source` run in parallel.\n*   `golang-component` runs after both `download-golang-compiler` and `download-helloworld-source` have completed."
  },
  {
    "repo": "references_kfp_files",
    "file": "fybrik_kfp-components_samples_house_price_estimates_pipeline-argo.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `Fybrik housing price estimate pipeline` that performs the following steps to provide data policy governed access to cataloged data, analyze it, train a model, and write/catalog the results.\n\nThe pipeline takes two input parameters:\n- `test_dataset_id` (string)\n- `train_dataset_id` (string)\n\nIt consists of four main components and utilizes a helper function to determine the current Kubernetes namespace.\n\n**Components Breakdown:**\n\n1.  **`getDataEndpoints`**:\n    *   **Function:** Retrieves data endpoints for the training and testing datasets. It seems to handle the initial data access and potentially sets up where results will be stored and cataloged.\n    *   **Inputs:** `train_dataset_id`, `test_dataset_id`, `namespace` (determined dynamically by `get_current_namespace`), `run_name` (derived from `dsl.RUN_ID_PLACEHOLDER`), and `result_name` (derived from `run_name`).\n    *   **Outputs:** `train_endpoint`, `test_endpoint`, and `result_catalogid`.\n    *   **Component Definition:** Loaded from `../../get_data_endpoints/component.yaml`.\n\n2.  **`visualizeTable`**:\n    *   **Function:** Visualizes the training data.\n    *   **Inputs:** `train_endpoint` (obtained from `getDataEndpointsStep.outputs['train_endpoint']`), `train_dataset_id`, and `namespace`.\n    *   **Dependencies:** Runs *after* `getDataEndpointsStep`.\n    *   **Component Definition:** Loaded from `./visualize_table/component.yaml`.\n\n3.  **`trainModel`**:\n    *   **Function:** Trains a machine learning model using the provided training and testing data endpoints. It also specifies where to output the results.\n    *   **Inputs:** `train_endpoint_path` (obtained from `getDataEndpointsStep.outputs['train_endpoint']`), `test_endpoint_path` (obtained from `getDataEndpointsStep.outputs['test_endpoint']`), `result_name`, `result_endpoint_path` (obtained from `getDataEndpointsStep.outputs['result_endpoint']`), `train_dataset_id`, `test_dataset_id`, and `namespace`.\n    *   **Dependencies:** Runs *after* `visualizeTableStep`.\n    *   **Component Definition:** Loaded from `./train_model/component.yaml`.\n\n4.  **`submitResult`**:\n    *   **Function:** Submits and catalogs the results of the model training.\n    *   **Inputs:** `result_catalogid` (obtained from `getDataEndpointsStep.outputs['result_catalogid']`).\n    *   **Dependencies:** Runs *after* `trainModelStep`.\n    *   **Component Definition:** Loaded from `./submit_result/component.yaml`.\n\n**Control Flow:**\nThe pipeline executes sequentially: `getDataEndpoints` -> `visualizeTable` -> `trainModel` -> `submitResult`, with each step explicitly waiting for the completion of the previous one using `.after()` dependencies.\n\n**Helper Function:**\nA local Python function `get_current_namespace()` is used to dynamically determine the Kubernetes namespace where the pipeline is running. This function uses the `kubernetes` library and checks `kubeflow` specific paths. The result of this function is passed as an argument to the `getDataEndpoints` component.\n\n**Internal Variables:**\n- `namespace`: Stores the current Kubernetes namespace.\n- `run_name`: A string derived from `dsl.RUN_ID_PLACEHOLDER` and prefixed with \"run-\", ensuring it's lowercase and starts with a letter.\n- `result_name`: A string derived from `run_name` and prefixed with \"submission-\"."
  },
  {
    "repo": "references_kfp_files",
    "file": "muhammadyaseen_kubeflow-on-linode_kfp-examples_04_model-train-eval-pipeline_train_eval_pipeline.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `TrainEvalPipeline` that performs a machine learning model training, evaluation, and selection workflow.\n\nThe pipeline consists of **three components**:\n\n1.  **`train_eval_baseline_model`**: This component is a containerized step responsible for training and evaluating various machine learning models (e.g., baseline, logistic regression, gradient boosting, decision tree, including resampled versions).\n    *   **Inputs**:\n        *   `model_name` (str): The name of the model to train and evaluate.\n        *   `script` (str): The Python script to execute within the container.\n        *   `ip` (str): IP address for data storage/retrieval.\n        *   `port` (str): Port for data storage/retrieval.\n        *   `bucket_name` (str): The name of the S3-compatible bucket where data/models are stored.\n        *   `object_name` (str): The name of the object (e.g., file path) within the bucket.\n    *   **Function**: It executes a Python script within a `myaseende/my-scikit:latest` Docker image to perform model training and evaluation. It's expected to output evaluation metrics that will be consumed by subsequent steps.\n    *   **Tools/Libraries**: Uses a custom Docker image `myaseende/my-scikit:latest`.\n\n2.  **`show_best_model_info`**: This component retrieves and processes evaluation metrics for a specific model from an S3-compatible storage.\n    *   **Inputs**:\n        *   `model_name` (str): The name of the model whose information is to be retrieved.\n        *   `ip` (str): IP address for MinIO client connection.\n        *   `port` (str): Port for MinIO client connection.\n        *   `bucket_name` (str): The name of the S3-compatible bucket.\n        *   `object_name` (str): The object name (e.g., file path) within the bucket where metrics are stored.\n    *   **Output**: A `float` representing the evaluation metric for the specified model.\n    *   **Function**: Connects to a MinIO (S3-compatible) server, downloads a specified object (expected to contain metrics), and extracts a single float metric. It handles potential `S3Error` during MinIO operations.\n    *   **Tools/Libraries**: `pandas`, `minio==7.1.14`.\n\n3.  **`find_best_model_on_full_data`**: This component analyzes the evaluation metrics from various models to determine the best-performing one.\n    *   **Inputs**:\n        *   `baseline_metric` (float): Evaluation metric for the baseline model.\n        *   `lr_metric` (float): Evaluation metric for the Logistic Regression model.\n        *   `lr_resampled_metric` (float): Evaluation metric for the resampled Logistic Regression model.\n        *   `gbt_metric` (float): Evaluation metric for the Gradient Boosting Trees model.\n        *   `gbt_resampled_metric` (float): Evaluation metric for the resampled Gradient Boosting Trees model.\n        *   `dtree_metric` (float): Evaluation metric for the Decision Tree model.\n        *   `dtree_resampled_metric` (float): Evaluation metric for the resampled Decision Tree model.\n    *   **Output**: `experiment_summary` (Output[Markdown]): A Markdown-formatted summary detailing all model metrics and identifying the best model.\n    *   **Function**: Compares the provided metrics for seven different models, identifies the model with the highest metric, and generates a markdown table summarizing all results, including the best model.\n    *   **Tools/Libraries**: `pandas`.\n\nThe **control flow** of the pipeline is as follows:\n\n*   The `train_eval_baseline_model` component will be executed multiple times using a `parallelFor` loop. Each iteration will train and evaluate a specific model type. The loop will iterate over a list of dictionaries, where each dictionary defines the `name`, `script`, and `object_name` for a particular model to be trained. The `ip`, `port`, and `bucket_name` will be constant across all training tasks.\n*   For each model trained in the `parallelFor` loop, an instance of the `show_best_model_info` component will be executed **after** its corresponding `train_eval_baseline_model` task completes. This `show_best_model_info` component will retrieve the evaluation metric for that specific model.\n*   Once all individual `show_best_model_info` tasks have completed and their respective metrics are available, the `find_best_model_on_full_data` component will be executed. It will receive the outputs (metrics) from all instances of `show_best_model_info` as its inputs.\n\nThe pipeline accepts the following **parameters**:\n*   `ip_addr` (str): IP address for MinIO/data access, with a default value of \"172.17.0.1\".\n*   `port_num` (str): Port number for MinIO/data access, with a default value of \"9000\".\n*   `bucket_name` (str): The name of the S3 bucket, with a default value of \"kubeflow-pipelines\".\n*   `script_path` (str): The path to the training script, with a default value of \"src/main.py\".\n\nThe pipeline will use the `@dsl.pipeline` and `@component` decorators from `kfp.dsl`."
  },
  {
    "repo": "references_kfp_files",
    "file": "lynnmatrix_kfp-local_kfp_local_local_client_test.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `test-run-local-pipeline` that performs the following steps:\n\nThis pipeline consists of a single component.\n\n**Components:**\n\n1.  **`hello`**: This component takes a string input named `name` and returns a greeting string in the format \"hello {name}\".\n\n**Control Flow:**\n\nThe pipeline starts by executing the `hello` component, passing the `name` parameter received by the pipeline as its input.\n\n**Inputs:**\n\nThe pipeline accepts one input parameter:\n*   `name`: A string that will be used by the `hello` component.\n\n**Tools/Libraries:**\n\nThe pipeline utilizes standard Python functionalities and the `kfp.v2.dsl` module for Kubeflow pipeline definitions."
  },
  {
    "repo": "references_kfp_files",
    "file": "rakesh283343_kubeflow-sample-pipelines_core_loop_parameter_loop_parameter.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `my-pipeline` that performs the following steps:\n\nThis pipeline accepts one input parameter:\n- `loopidy_doop`: A list of dictionaries, defaulting to `[{'a': 1, 'b': 2}, {'a': 10, 'b': 20}]`. This parameter is used for parallel iteration.\n\nThe pipeline consists of three components:\n\n1.  **`my-out-cop0`**:\n    *   **Function**: Generates a JSON list of integers from 20 to 30 (inclusive) and writes it to a file.\n    *   **Image**: `python:alpine3.6`\n    *   **Output**: `out` (JSON list of integers)\n\n2.  **`my-in-cop1`**:\n    *   **Function**: Prints a string including a value from the current item in the parallel loop.\n    *   **Image**: `library/bash:4.4.23`\n    *   **Dependency**: This component runs *after* `my-out-cop0` has completed.\n    *   **Control Flow**: This component is executed within a `ParallelFor` loop that iterates over the `loopidy_doop` pipeline parameter. For each item in `loopidy_doop`, a new instance of `my-in-cop1` is run, accessing `item.a` from the current loop item.\n\n3.  **`my-out-cop2`**:\n    *   **Function**: Prints a string including the output generated by `my-out-cop0`.\n    *   **Image**: `library/bash:4.4.23`\n    *   **Input**: Consumes the `out` output from `my-out-cop0`.\n\nThe pipeline should be compiled to a YAML file."
  },
  {
    "repo": "references_kfp_files",
    "file": "stackdemos_kf-flatcar2_components_training_component.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `stackdemos_kf-flatcar2_components_training_component` that performs the following steps:\n\nThe pipeline should define two reusable container operation templates: `training_op` and `http_download_op`.\n\nThe `training_op` component:\n- **Functionality**: Serves as a template for various training-related container operations. It executes a Python script within a specified Docker image.\n- **Inputs**:\n    - `script` (str): The path to the Python script to be executed.\n    - `image` (str, optional): The Docker image to use for the container. If not provided, it attempts to retrieve `TRAINING_IMAGE` from the IPython user namespace if running in a notebook.\n    - `arguments` (list, optional): A list of arguments to pass to the Python script.\n    - `file_outputs` (dict, optional): A dictionary defining the file outputs of the component.\n- **Output**: A `kfp.dsl.ContainerOp` instance.\n- **Logic**:\n    - If `image` is not provided and running in an IPython environment, it attempts to retrieve `TRAINING_IMAGE` from `IPython.get_ipython().user_ns`.\n    - If `image` is still not provided, it raises a `ValueError`.\n    - The `ContainerOp`'s `name` is derived from the `script` parameter by converting it to lowercase, removing non-alphanumeric characters, and splitting the extension.\n    - The `command` is `['/usr/local/bin/python', script]`.\n    - The `arguments` and `file_outputs` are passed directly to the `ContainerOp`.\n- **Tools/Libraries**: Uses `os` and `re` for name manipulation. Checks for `IPython` for notebook environment detection.\n\nThe `http_download_op` component:\n- **Functionality**: Downloads a file from a given URL and performs an MD5 checksum pre-check to avoid re-downloading if the file already exists and matches the checksum.\n- **Inputs**:\n    - `url` (str): The URL of the file to download.\n    - `download_to` (str): The local path where the file should be saved.\n    - `md5sum` (str): The expected MD5 checksum of the file.\n- **Output**: A `kfp.dsl.ContainerOp` instance.\n- **Logic**:\n    - The `ContainerOp`'s `name` is fixed as 'download-artifact'.\n    - The `image` is `appropriate/curl`.\n    - The `command` is `['sh', '-c']`.\n    - The `arguments` contain a shell script that first checks if the `md5sum` of the existing file at `download_to` matches the provided `md5sum`. If it matches, it prints a message skipping the download. Otherwise, it uses `curl -#Lv --create-dirs -o {download_to} {url}` to download the file.\n- **Tools/Libraries**: Uses `curl` for downloading and `awk` for MD5 checksum extraction.\n\nNo explicit pipeline structure (e.g., dependencies, parallel execution) is defined, as these are reusable component definitions. The pipeline itself acts as a collection of these definitions."
  },
  {
    "repo": "references_kfp_files",
    "file": "kubeflow_pipelines_samples_test_two_step_with_uri_placeholder.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `two-step-with-uri-placeholders` that performs a two-step data transfer operation using Google Cloud Storage (GCS).\n\nThe pipeline consists of **two components**:\n\n1.  **`write-to-gcs`**:\n    *   **Function**: This component writes a given string message to a GCS file.\n    *   **Inputs**:\n        *   `msg` (String): The content to be written.\n    *   **Outputs**:\n        *   `artifact` (Artifact): The GCS file path where the message was written.\n    *   **Implementation**: It uses the `google/cloud-sdk:slim` Docker image and executes a shell command to `echo` the `msg` into a GCS location specified by the output URI placeholder.\n\n2.  **`read-from-gcs`**:\n    *   **Function**: This component reads content from a specified GCS file and prints it to standard output.\n    *   **Inputs**:\n        *   `artifact` (Artifact): The GCS file path to read from.\n    *   **Outputs**: None explicitly defined, but the content is printed.\n    *   **Implementation**: It uses the `google/cloud-sdk:slim` Docker image and executes a shell command to `gsutil cat` the content from the GCS location specified by the input URI placeholder.\n\n**Control Flow and Dependencies**:\n*   The `write-to-gcs` component runs first, taking the `msg` pipeline parameter as its input.\n*   The `read-from-gcs` component runs **after** `write-to-gcs` and is directly dependent on its output.\n*   The `artifact` output (GCS file path) from `write-to-gcs` is passed as the `artifact` input to `read-from-gcs`.\n\n**Pipeline Parameters**:\n*   The pipeline accepts a single string parameter named `msg`, with a default value of `'Hello world!'`.\n\n**Tools/Libraries**:\n*   The pipeline heavily leverages `gsutil` commands from the `google/cloud-sdk:slim` Docker image for GCS operations."
  },
  {
    "repo": "references_kfp_files",
    "file": "sbakiu_ml-kf-pipeline_tokenize_pipeline.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `Pipeline` that performs a machine learning workflow for training with a Reddit response dataset.\n\nThe pipeline has five components:\n1.  **`tokenize`**: This component is a `ContainerOp` that tokenizes the input data.\n    *   **Image**: Uses a custom image from ECR (`<ID>.dkr.ecr.eu-central-1.amazonaws.com/<registry>/tokenize:1.0.0`).\n    *   **Command**: `python -m src.steps.tokenize.pipeline_step`.\n    *   **Outputs**: Produces two file outputs: `tokenize_location` (path to tokenized data) and `labels_location` (path to labels data).\n    *   **Security**: Applies AWS secrets for access key and secret key.\n    *   **Pull Policy**: Sets image pull policy to `Always`.\n\n2.  **`vectorize`**: This component is a `ContainerOp` that performs TF-IDF vectorization.\n    *   **Image**: Uses the same custom image as `tokenize` (`<ID>.dkr.ecr.eu-central-1.amazonaws.com/<registry>/tokenize:1.0.0`).\n    *   **Command**: `python -m src.steps.tfidftransformer.pipeline_step`.\n    *   **Inputs**: Takes `tokenize_location` as input from the `tokenize` component.\n    *   **Outputs**: Produces two file outputs: `tfidftransformer_location` (path to vectorizer model) and `tfidfvectors_location` (path to vectorized data).\n    *   **Security**: Applies AWS secrets for access key and secret key.\n    *   **Pull Policy**: Sets image pull policy to `Always`.\n\n3.  **`logical regression`**: This component is a `ContainerOp` that trains a Logistic Regression classifier.\n    *   **Image**: Uses the same custom image as `tokenize` and `vectorize` (`<ID>.dkr.ecr.eu-central-1.amazonaws.com/<registry>/tokenize:1.0.0`).\n    *   **Command**: `python -m src.steps.lrclassifier.pipeline_step`.\n    *   **Inputs**: Takes `labels_location` from the `tokenize` component and `tfidfvectors_location` from the `vectorize` component.\n    *   **Outputs**: Produces one file output: `lr_model_location` (path to the trained LR model).\n    *   **Security**: Applies AWS secrets for access key and secret key.\n    *   **Pull Policy**: Sets image pull policy to `Always`.\n\n4.  **`Build tokenize Serving`**: This component is a `ContainerOp` responsible for building a serving image for the tokenizer.\n    *   **Image**: Uses a Kaniko executor image from ECR (`<ID>.dkr.ecr.eu-central-1.amazonaws.com/<registry>/kaniko-executor:1.0.0`).\n    *   **Arguments**:\n        *   `--dockerfile=Dockerfile`\n        *   `--build-arg=TOKENIZE_MODEL=<tokenize_location output from tokenize component>`\n        *   `--context=dir:///workspace`\n        *   `--destination=<ID>.dkr.ecr.eu-central-1.amazonaws.com/<registry>/tokenizeserving:1.0.0`\n    *   **Security**: Applies AWS secrets for access key and secret key.\n    *   **Pull Policy**: Sets image pull policy to `Always`.\n\n5.  **`Build vectorize Serving`**: This component is a `ContainerOp` responsible for building a serving image for the vectorizer.\n    *   **Image**: Uses the same Kaniko executor image as `Build tokenize Serving` (`<ID>.dkr.ecr.eu-central-1.amazonaws.com/<registry>/kaniko-executor:1.0.0`).\n    *   **Arguments**:\n        *   `--dockerfile=Dockerfile`\n        *   `--build-arg=VECTORIZE_MODEL=<tfidftransformer_location output from vectorize component>`\n        *   `--context=dir:///workspace`\n        *   `--destination=<ID>.dkr.ecr.eu-central-1.amazonaws.com/<registry>/vectorizeserving:1.0.0`\n    *   **Security**: Applies AWS secrets for access key and secret key.\n    *   **Pull Policy**: Sets image pull policy to `Always`.\n\n**Control Flow and Dependencies**:\n*   `vectorize_training_step` depends on `tokenize_training_step` as it consumes its output.\n*   `lr_training_step` depends on both `tokenize_training_step` and `vectorize_training_step` as it consumes their outputs.\n*   `tokenize_build_step` runs `after` `lr_training_step`.\n*   `vectorize_build_step` runs `after` `lr_training_step`.\n\n**Pipeline Input Parameters**:\n*   `input_data`: Default value `reddit_train.csv`.\n\n**General Pipeline Configuration**:\n*   The pipeline description is 'A pipeline for training with Reddit response dataset'.\n*   All components apply an AWS secret named 'aws-secret' for 'AWS_ACCESS_KEY_ID' and 'AWS_SECRET_ACCESS_KEY'.\n*   All components set the image pull policy to 'Always'.\n*   The pipeline leverages custom Docker images stored in an ECR registry (`<ID>.dkr.ecr.eu-central-1.amazonaws.com/<registry>`) with a consistent tag (`1.0.0`).\n*   The build steps use `kaniko-executor` for building Docker images."
  },
  {
    "repo": "references_kfp_files",
    "file": "s102401002_kubeflowPipeline0722_src_kubeflowPipeline_xgboost.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `DiabetesPredictionPipeline` that performs an end-to-end machine learning workflow for diabetes prediction.\n\nThe pipeline consists of three components:\n1.  **`load_data`**: This component is responsible for downloading, cleaning, and preprocessing the raw diabetes dataset.\n    *   **Function**: Downloads CSV files from specified URLs, consolidates them into a single DataFrame, renames columns to a standard format, removes rows with 'No Info' in the 'diabetes' column, selects relevant features (`gender`, `age`, `bmi`, `HbA1c_level`, `blood_glucose_level`, `diabetes`), drops rows with less than 4 non-NA values, maps 'gender' values (Male:0, Female:1), filters out 'Other' gender, and replaces 'No Info' values in numerical columns with their respective means.\n    *   **Inputs**: None.\n    *   **Outputs**: `data_output` (Artifact), which contains the preprocessed DataFrame in CSV format.\n    *   **Tools/Libraries**: Uses `pandas`.\n    *   **Base Image**: `python:3.9` with `pandas==2.2.2` installed.\n\n2.  **`prepare_data`**: This component splits the preprocessed data into training and testing sets.\n    *   **Function**: Reads the preprocessed DataFrame, separates features (X) from the target variable (y), and then splits them into training and testing sets using a 80/20 ratio.\n    *   **Inputs**: `data_input` (Input[Artifact]), which expects the preprocessed DataFrame from `load_data`.\n    *   **Outputs**:\n        *   `x_train_output` (Output[Artifact])\n        *   `x_test_output` (Output[Artifact])\n        *   `y_train_output` (Output[Artifact])\n        *   `y_test_output` (Output[Artifact])\n        Each output contains the respective DataFrame in CSV format.\n    *   **Tools/Libraries**: Uses `pandas` and `scikit-learn`.\n    *   **Base Image**: `python:3.9` with `pandas==2.2.2` and `scikit-learn==1.5.1` installed.\n\n3.  **(Implicit - based on common ML pipelines, you'd expect a model training/evaluation component)**: (You can add this if you'd like the LLM to infer a next step, otherwise, explicitly state it's a two-component pipeline based on the provided code.)\n\n**Control Flow**:\n*   The `load_data` component runs first.\n*   The `prepare_data` component runs `after` `load_data` and depends on its `data_output` as its `data_input`."
  },
  {
    "repo": "references_kfp_files",
    "file": "speg03_kfp-toolbox_src_kfp_toolbox_pipelines.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `timestamp-pipeline` that performs the following:\n\n**Pipeline Name:** `timestamp-pipeline`\n\n**Number of Components:** 1\n\n**Components:**\n\n1.  **`timestamp` component:**\n    *   **Function:** This component is responsible for generating a timestamp string.\n    *   **Inputs:**\n        *   `format` (str, default: `\"%Y%m%d%H%M%S\"`): Specifies the desired format for the timestamp string.\n        *   `prefix` (str, default: `\"\"`): A string to prepend to the timestamp.\n        *   `suffix` (str, default: `\"\"`): A string to append to the timestamp.\n        *   `separator` (str, default: `\"-\"`): A character or string to separate parts of the timestamp.\n        *   `tz_offset` (int, default: `0`): An integer representing the timezone offset in hours.\n    *   **Outputs:**\n        *   `time_string` (Artifact/String): The generated timestamp string.\n\n**Control Flow:**\n\n*   The `timestamp_pipeline` directly calls the `timestamp` component. There are no complex dependencies, parallel executions, or conditional logic involved. The output of the `timestamp` component (`time_string`) is then returned by the pipeline.\n\n**Tools/Libraries Used:**\n\n*   `kfp.v2.dsl`: Used for defining the Kubeflow Pipeline and its components.\n*   The `timestamp` component is imported from a local `components` module, suggesting it's a custom-defined Kubeflow component."
  },
  {
    "repo": "references_kfp_files",
    "file": "mengdong_kubeflow-pipeline-nvidia-example_kubeflow-tf_end2end_serve.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `End2end Resnet50 Classification` that performs an end-to-end Kubeflow demonstration using TensorRT Inference Server, TF-AMP, and TensorRT.\n\nThe pipeline should accept the following parameters with their default values:\n- `trtserver_name`: `trtserver` (string)\n- `model_name`: `resnet_graphdef` (string)\n- `model_version`: `1` (string)\n- `num_iter`: `10` (integer)\n- `batch_size`: `128` (integer)\n- `webapp_prefix`: `webapp` (string)\n- `webapp_port`: `80` (string)\n- `storage_bucket`: `gs://dongm-kubeflow` (string)\n- `ckpt_dir`: `ckpt` (string)\n- `mount_dir`: `/mnt/vol` (string)\n- `model_dir`: `test_model_repository` (string)\n- `raw_data_dir`: `raw_data` (string)\n- `processed_data_dir`: `processed_data` (string)\n\nThe pipeline consists of at least two components (the provided code snippet is incomplete, but we can infer the first two):\n\n1.  **`preprocessing`**:\n    *   **Function**: Preprocesses raw data.\n    *   **Image**: `gcr.io/nvidia-sa-org/gcp-end2end-demo-preprocessing`\n    *   **Command**: `python`\n    *   **Arguments**:\n        *   `/scripts/preprocess.py`\n        *   `--input_dir`: Constructed using `mount_dir` and `raw_data_dir` (e.g., `/mnt/vol/raw_data`).\n        *   `--output_dir`: Constructed using `mount_dir` and `processed_data_dir` (e.g., `/mnt/vol/processed_data`).\n    *   **Outputs**: No file outputs are explicitly defined (`file_outputs={}`).\n    *   **Resource Requirements**: Requests 1 GPU.\n    *   **Volume Mounts**: Attaches a `PersistentVolumeClaim` named `my-rw-pvc` to `/mnt/vol` using a `V1Volume` and `V1VolumeMount`.\n\n2.  **`training`**:\n    *   **Function**: Performs model training.\n    *   **Image**: `gcr.io/nvidia-sa-org/gcp-end2end-demo-training`\n    *   **Command**: `python`\n    *   **Arguments**: The snippet is incomplete, but based on typical MLOps pipelines, it would likely take inputs from the `preprocessing` step and potentially other pipeline parameters like `num_iter`, `batch_size`, `ckpt_dir`, etc.\n    *   **Dependencies**: This component is expected to run *after* the `preprocessing` component has completed successfully.\n\nThe pipeline should leverage `kfp.dsl` for pipeline definition and `kubernetes.client` for volume management. The `project_name` variable should be set to `nvidia-sa-org`."
  },
  {
    "repo": "references_kfp_files",
    "file": "rakesh283343_kubeflow-sample-pipelines_core_resource_ops_resource_ops.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `ResourceOp Basic` that performs a basic example of ResourceOp usage.\n\nThe pipeline consists of a single component:\n1.  **`test-step`**: This component is a `ResourceOp` that creates a Kubernetes Job resource.\n    *   **Function**: It deploys a Kubernetes Job defined by an inline JSON manifest. This Job's container uses the `k8s.gcr.io/busybox` image and executes the `/usr/bin/env` command, which prints out environment variables.\n    *   **Inputs**: The `k8s_resource` is provided as a JSON string representing the Kubernetes Job manifest.\n    *   **Outputs**: Not explicitly defined in the DSL, but the Job's output (stdout of the `/usr/bin/env` command) would be available in the step's logs.\n    *   **Control Flow**: This is the only step in the pipeline and runs directly.\n\nThe pipeline utilizes the `kfp.dsl.ResourceOp` class for Kubernetes resource management. No external libraries like sklearn or Snowflake are used within the pipeline components themselves. The `json` library is used to parse the Kubernetes manifest."
  },
  {
    "repo": "references_kfp_files",
    "file": "9rince_kfp_samples_test_lightweight_python_functions_v2_with_outputs.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `functions-with-outputs` that performs the following sequence of operations:\n\nThe pipeline consists of four components: `concat_message`, `add_numbers`, `output_artifact`, and `output_named_tuple`.\n\n1.  **`concat_message` component**:\n    *   **Function**: Concatenates two string inputs.\n    *   **Inputs**:\n        *   `first`: A string, provided by the pipeline parameter `first_message`.\n        *   `second`: A string, provided by the pipeline parameter `second_message`.\n    *   **Output**: A string, representing the concatenated message.\n\n2.  **`add_numbers` component**:\n    *   **Function**: Adds two integer inputs.\n    *   **Inputs**:\n        *   `first`: An integer, provided by the pipeline parameter `first_number`.\n        *   `second`: An integer, provided by the pipeline parameter `second_number`.\n    *   **Output**: An integer, representing the sum of the numbers.\n\n3.  **`output_artifact` component**:\n    *   **Function**: Generates a `Dataset` artifact containing a message repeated a specified number of times.\n    *   **Inputs**:\n        *   `number`: An integer, which is the output of the `add_numbers` component.\n        *   `message`: A string, which is the output of the `concat_message` component.\n    *   **Output**: A `Dataset` artifact.\n\n4.  **`output_named_tuple` component**:\n    *   **Function**: Takes a `Dataset` artifact as input and produces a named tuple containing a scalar string, metrics (in JSON format), and a `Model` artifact. It uses the contents of the input `Dataset` to construct the `Model` artifact.\n    *   **Inputs**:\n        *   `artifact`: An `Input[Dataset]`, which is the output of the `output_artifact` component.\n    *   **Output**: A `NamedTuple` with the following fields:\n        *   `scalar`: A `str`.\n        *   `metrics`: A `Metrics` object (containing accuracy).\n        *   `model`: A `Model` object.\n\n**Control Flow**:\n*   `concat_message` and `add_numbers` run in parallel.\n*   `output_artifact` depends on the completion of both `concat_message` and `add_numbers`, using their respective outputs as inputs.\n*   `output_named_tuple` depends on the completion of `output_artifact`, using its output as input.\n\nThe pipeline parameters are `first_message` (default 'first'), `second_message` (default 'second'), `first_number` (default 1), and `second_number` (default 2).\n\nThe pipeline should be compilable using `kfp.v2.compiler.Compiler`."
  },
  {
    "repo": "references_kfp_files",
    "file": "omerbsezer_Fast-Kubeflow_Project_Kubeflow_Pipeline_MLModels_pipeline.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `ML Models Pipeline` that performs machine learning model training and evaluation.\n\nThe pipeline consists of **seven** components:\n1.  **`download_data`**: This component is responsible for downloading the necessary dataset. Its input is not explicitly defined in the provided code, but it outputs the data used by subsequent ML model components. This component is loaded from `download_data/download_data.yaml`.\n2.  **`decision_tree`**: This component trains a Decision Tree classifier. It takes the output from `download_data` as its input and outputs the accuracy score of the Decision Tree model. This component is loaded from `decision_tree/decision_tree.yaml`.\n3.  **`logistic_regression`**: This component trains a Logistic Regression classifier. It takes the output from `download_data` as its input and outputs the accuracy score of the Logistic Regression model. This component is loaded from `logistic_regression/logistic_regression.yaml`.\n4.  **`svm`**: This component trains a Support Vector Machine (SVC) classifier. It takes the output from `download_data` as its input and outputs the accuracy score of the SVM model. This component is loaded from `svm/svm.yaml`.\n5.  **`naive_bayes`**: This component trains a Gaussian Naive Bayes classifier. It takes the output from `download_data` as its input and outputs the accuracy score of the Naive Bayes model. This component is loaded from `naive_bayes/naive_bayes.yaml`.\n6.  **`xg_boost`**: This component trains an XGBoost classifier. It takes the output from `download_data` as its input and outputs the accuracy score of the XGBoost model. This component is loaded from `xg_boost/xg_boost.yaml`.\n7.  **`show_results`**: This component is a Python function-based component (`@func_to_container_op`) that prints the accuracy results from all the machine learning models. It takes five float inputs: `decision_tree`, `logistic_regression`, `svm`, `naive_bayes`, and `xg_boost`, corresponding to the accuracy scores from the respective ML model components. It does not produce any visible output beyond printing to standard output.\n\nThe pipeline's control flow is as follows:\n- The `download_data` task runs first.\n- Once `download_data` completes, its output is used as input for the `decision_tree`, `logistic_regression`, `svm`, `naive_bayes`, and `xg_boost` tasks. These five tasks run in parallel, as they all depend only on the `download_data` task's completion and output.\n- After all five machine learning model tasks complete, their respective accuracy outputs are passed as arguments to the `show_results` component, which then executes to display the results.\n\nThe pipeline utilizes the `kfp` library for defining and compiling the Kubeflow Pipeline. The individual ML model components are loaded from external YAML files, suggesting that they might be pre-built Docker images or pre-defined Kubeflow component specifications. The `show_results` component is defined directly within the Python code using `@func_to_container_op`. The machine learning components likely use common ML libraries such as `scikit-learn` or `XGBoost`, although this is inferred and not explicitly stated in the provided pipeline definition."
  },
  {
    "repo": "references_kfp_files",
    "file": "dermatologist_kedro-multimodal_build_kubeflow_pipeline.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `Kedro pipeline` that performs the execution of a Kedro project.\n\nThis pipeline consists of a dynamic number of components, where each component corresponds to a node in a Kedro pipeline. The exact number of components is determined at runtime based on the structure of the Kedro pipeline being converted.\n\nEach component:\n- Represents a single Kedro node.\n- Is named after the cleaned version of the Kedro node's name (e.g., `feature-engineering` for a node named `feature_engineering`).\n- Takes no explicit inputs or outputs as all data handling is managed internally by Kedro.\n- Executes the `kedro run --node <node_name>` command within a specified Docker image.\n- Utilizes AWS credentials by applying `aws.use_aws_secret` to inject `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` from the Kubernetes secret `aws-secrets`.\n\nThe control flow of the pipeline is dynamically constructed:\n- Dependencies between components are established using the `.after()` method, reflecting the dependencies between Kedro nodes. If Kedro node A depends on Kedro node B, then the Kubeflow component for A will run `after` the component for B.\n- All components run sequentially or in parallel as dictated by the Kedro node dependency graph. There are no explicit `parallelFor` loops or other complex control flow mechanisms defined at the Kubeflow level, beyond the sequential execution based on `after` dependencies.\n\nThe pipeline itself, named `convert_kedro_pipeline_to_kfp`, orchestrates the creation of these dynamic components and their dependencies. It leverages Kedro's internal `node_dependencies` to build the Kubeflow component graph.\n\nThe pipeline definition uses the `kfp` library for defining components and the pipeline structure, and the `re` module for cleaning names. It integrates with Kedro to dynamically obtain the project's pipeline structure."
  },
  {
    "repo": "references_kfp_files",
    "file": "9rince_kfp_samples_test_metrics_visualization_v2_test.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `MetricsVisualizationPipeline` that performs classification tasks and visualizes their metrics.\n\nThe pipeline consists of five components:\n\n1.  **`wine_classification`**: This component performs a wine classification task.\n    *   It takes no visible inputs.\n    *   It produces a `metrics` output artifact, which contains `ConfidenceMetrics` with various `confidenceThreshold`, `falsePositiveRate`, and `recall` values. The `display_name` for this metric is \"metrics\".\n\n2.  **`iris_sgdclassifier`**: This component performs an Iris SGDClassifier task.\n    *   It takes no visible inputs.\n    *   It produces a `metrics` output artifact, which contains `ClassificationMetrics` including `accuracy`, `precision`, `recall`, `f1Score`, `falsePositiveRate`, `falseNegativeRate`, `truePositiveRate`, `trueNegativeRate`, and `areaUnderRocCurve`. The `display_name` for this metric is \"metrics\".\n\n3.  **`digit_classification`**: This component performs a digit classification task.\n    *   It takes no visible inputs.\n    *   It produces a `metrics` output artifact, which contains `RocCurveMetrics` with `falsePositiveRate` and `truePositiveRate` values, and also `ConfusionMatrixMetrics` with `labels` and `matrix` data. The `display_name` for this metric is \"metrics\".\n\n4.  **`html_visualization`**: This component creates an HTML visualization.\n    *   It takes no visible inputs.\n    *   It produces an `html_artifact` output artifact, which is an HTML file containing a simple \"Hello world\" string. The `display_name` for this artifact is \"HTML visualization\".\n\n5.  **`markdown_visualization`**: This component creates a Markdown visualization.\n    *   It takes no visible inputs.\n    *   It produces a `markdown_artifact` output artifact, which is a Markdown file containing a header \"## Hello world\". The `display_name` for this artifact is \"Markdown visualization\".\n\n**Control Flow:**\nAll five components run in parallel as they do not have any explicit dependencies on each other.\n\n**Tools/Libraries:**\nThe pipeline uses `kfp.dsl` for defining pipeline components and artifacts. The underlying classification components likely use machine learning libraries such as `scikit-learn` to generate the classification and ROC curve metrics. The visualization components likely generate simple HTML and Markdown content."
  },
  {
    "repo": "references_kfp_files",
    "file": "kubeflow_pipelines_samples_v2_collected_parameters.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `collected_param_pipeline` that performs the following steps:\n\nThis pipeline orchestrates a larger process by calling another nested pipeline.\n\n**Pipeline Structure and Components:**\n\nThis pipeline consists of **one** top-level component, which is a call to another Kubeflow Pipeline function.\n\n1.  **`collecting_parameters` (Nested Pipeline Call):**\n    *   **Function:** This component is a call to the `collecting_parameters` Kubeflow Pipeline function.\n    *   **Inputs:** It takes `model_ids` as input, which is a string set to `'s1,s2,s3'`.\n    *   **Outputs:** The output of this nested pipeline is collected and passed to the subsequent step.\n    *   **Caching:** Caching is explicitly disabled for this component (`dag.set_caching_options(False)`).\n\n2.  **`consume_ids` (Component):**\n    *   **Function:** This component is an instance of the `consume_ids` function. It iterates through a list of IDs and prints each one.\n    *   **Inputs:** It receives a list of strings (`ids`) which is the collected output from the `collecting_parameters` nested pipeline call (`dag.output`).\n    *   **Dependencies:** This component runs *after* the `collecting_parameters` nested pipeline has completed and its output is available.\n    *   **Caching:** Caching is explicitly disabled for this component (`consume_ids_op.set_caching_options(False)`).\n\n**Nested Pipeline Details (`collecting_parameters`):**\n\nThe `collected_param_pipeline` calls a nested pipeline named `collecting_parameters` which performs the following:\n\nThis nested pipeline has a total of **four** components: `split_ids`, `prepend_id`, `consume_single_id`, and `consume_ids`.\n\n*   **Pipeline Name:** `collecting_parameters`\n*   **Pipeline Inputs:** It accepts a single string input `model_ids` with a default empty string value.\n*   **Pipeline Outputs:** It returns a list of strings, specifically the `prepend_id_op.output` collected from the `ParallelFor` loop.\n\n**Nested Pipeline Components and Control Flow:**\n\n1.  **`split_ids` (Component):**\n    *   **Function:** Splits a comma-separated string of IDs into a list of strings.\n    *   **Inputs:** Takes `ids` (a string) from the pipeline input `model_ids`.\n    *   **Outputs:** Returns a `list` of strings.\n\n2.  **`ParallelFor` Loop:**\n    *   **Control Flow:** Iterates in parallel over each `model_id` in the `list` output from the `split_ids` component (`ids_split_op.output`).\n\n    *   **Inside the `ParallelFor` Loop:**\n        *   **`prepend_id` (Component):**\n            *   **Function:** Prepends the string 'model_id_' to a given content string.\n            *   **Inputs:** Takes `content` (a string) which is the current `model_id` from the `ParallelFor` loop.\n            *   **Outputs:** Returns a `string` (e.g., 'model_id_s1').\n            *   **Caching:** Caching is explicitly disabled for this component.\n        *   **`consume_single_id` (Component):**\n            *   **Function:** Consumes (prints) a single ID.\n            *   **Inputs:** Takes `id` (a string) which is the output from the `prepend_id` component (`prepend_id_op.output`).\n            *   **Dependencies:** Runs *after* `prepend_id_op` for the current `model_id`.\n            *   **Caching:** Caching is explicitly disabled for this component.\n\n3.  **`consume_ids` (Component):**\n    *   **Function:** Iterates through a list of IDs and prints each one.\n    *   **Inputs:** Takes a `List[str]` named `ids`. This list is a `dsl.Collected` object, meaning it gathers all the outputs of `prepend_id_op.output` from all iterations of the `ParallelFor` loop.\n    *   **Dependencies:** This component implicitly runs *after* the `ParallelFor` loop has completed all its iterations and collected all `prepend_id_op.output` values.\n    *   **Caching:** Caching is explicitly disabled for this component.\n\n**General Requirements:**\n\n*   Use `kfp.dsl` for pipeline and component definitions.\n*   The code should be runnable as a Kubeflow Pipeline.\n*   Include standard imports: `typing.List`, `kfp`, `kfp.dsl`.\n*   Include the `if __name__ == '__main__':` block to compile and run the pipeline using `kfp.Client`, creating a run for `collected_param_pipeline` with caching disabled."
  },
  {
    "repo": "references_kfp_files",
    "file": "kubeflow_pipelines_samples_test_after.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `pipeline-with-after` that performs the following steps:\n\nThis pipeline consists of **three** components, all of which are instances of a single reusable component named `Print Text`.\n\nThe `Print Text` component takes one input, `text` (String type), and prints its value. Its implementation uses an `alpine` container and executes a shell command to echo the input text. This component is loaded from text using `kfp.components.load_component_from_text`.\n\nThe control flow of the pipeline is as follows:\n1.  **`task1`**: This task is an instance of `Print Text` with the input `text` set to `'1st task'`. It runs without any dependencies.\n2.  **`task2`**: This task is an instance of `Print Text` with the input `text` set to `'2nd task'`. It has an explicit dependency on `task1`, meaning it will only start after `task1` has successfully completed.\n3.  **`task3`**: This task is an instance of `Print Text` with the input `text` set to `'3rd task'`. It has explicit dependencies on both `task1` and `task2`, meaning it will only start after both `task1` and `task2` have successfully completed.\n\nThe pipeline uses the `kfp` library for defining the pipeline and components, specifically `kfp.dsl` for the pipeline definition and `kfp.components` for loading the reusable component."
  },
  {
    "repo": "references_kfp_files",
    "file": "getindata_kedro-kubeflow_kedro_kubeflow_generators_one_pod_pipeline_generator.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `[PROJECT_NAME]` that performs the following:\n\nThe pipeline consists of a single component: `[PIPELINE_NAME]`.\n\n- **`[PIPELINE_NAME]` Component**:\n    - **Function**: This component executes a Kedro pipeline. It runs a Kedro command `kedro run --env [KEDRO_ENV] --pipeline [PIPELINE_NAME] --config config.yaml`.\n    - **Image**: Uses the `[IMAGE]` Docker image.\n    - **Image Pull Policy**: `[IMAGE_PULL_POLICY]`\n    - **Inputs**: It accepts a dictionary of parameters as arguments, derived from the merged Kedro context parameters (`params.keys()`).\n    - **Outputs**: It produces file outputs based on the Kedro catalog. For each catalog entry that has a `filepath` attribute and is a local file system path, and if `store_kedro_outputs_as_kfp_artifacts` is enabled in the run configuration, it maps the Kedro output name to its corresponding file path within the container (e.g., `/home/kedro/path/to/output.csv`).\n    - **Environment Variables**: Sets container environment variables, likely related to Kedro's execution.\n    - **Caching**: Configured with a `max_cache_staleness` set by `run_config.max_cache_staleness`.\n    - **Customization**: The operation is further customized based on the `image_pull_policy` and `run_config`.\n\n**Pipeline Control Flow**:\n- The pipeline defines an exit handler that will execute cleanup or post-pipeline actions, even if the main component fails. This handler is configured with the pipeline name, image, image pull policy, run configuration, and Kedro context.\n- The `[PIPELINE_NAME]` component is the only step within this exit handler's scope.\n\n**Global Pipeline Configuration**:\n- **Name**: The pipeline's name is dynamically set as `[PROJECT_NAME]`.\n- **Description**: The pipeline's description is dynamically set by `run_config.description`.\n- **TTL**: Sets `ttl_seconds_after_finished` for the pipeline run based on `run_config.ttl`.\n- **Parameters**: The pipeline itself can accept parameters if they are present in the merged Kedro context parameters.\n- **Tools/Libraries Used**: This pipeline heavily relies on **Kedro** for its execution logic and **KFP (Kubeflow Pipelines)** for orchestration. It interacts with Kedro's context, parameters, and catalog to define its operations and artifacts."
  },
  {
    "repo": "references_kfp_files",
    "file": "StatCan_aaw-kubeflow-mlops_pipeline_train_cnn_databricks.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `Tacos vs. Burritos` that performs a simple TensorFlow Convolutional Neural Network (CNN) training workflow.\n\nThe pipeline consists of **three primary components**:\n1.  **Databricks Data Processing**: This component is an `init_container` that runs a Databricks notebook.\n    *   **Function**: It's likely responsible for initial data preparation or triggering a Databricks job.\n    *   **Inputs**:\n        *   `r`: Takes the Kubeflow `RUN_ID_PLACEHOLDER` as an argument.\n        *   `p`: Takes a JSON string `{\"argument_one\":\"param one\",\"argument_two\":\"param two\"}` as an argument.\n    *   **Dependencies**: This component is the first to run and has an `init_container` named `callback` that sends a \"Training Started\" event.\n    *   **Tools/Libraries**: Integrates with Databricks using a custom secret helper `use_databricks_secret()`.\n\n2.  **TensorFlow Preprocess**: This component handles data preprocessing for the TensorFlow model.\n    *   **Function**: Prepares image data for training, including unzipping, resizing, and generating a dataset file.\n    *   **Inputs**:\n        *   `--base_path`: Uses `/mnt/azure` as the base path for data.\n        *   `--data`: Specifies `train` as the data folder.\n        *   `--target`: Specifies `train.txt` as the target dataset file.\n        *   `--img_size`: Sets the image size to `160`.\n        *   `--zipfile`: Takes the `dataset` pipeline parameter, which represents the zip file containing images.\n    *   **Dependencies**: This component runs **after** the \"Databricks Data Processing\" component.\n    *   **Tools/Libraries**: Uses Python for data processing.\n\n3.  **Exit Handler**: This component is a special `ExitHandler` that runs regardless of the pipeline's success or failure.\n    *   **Function**: Sends a \"Training Finished\" event to a specified callback URL.\n    *   **Inputs**:\n        *   `-d`: Takes a JSON payload generated by the `get_callback_payload` function, indicating a `TRAIN_FINISH_EVENT` and including workflow status, SHA, PR number, and run ID.\n        *   Callback URL: `kubemlopsbot-svc.kubeflow.svc.cluster.local:8080`.\n    *   **Control Flow**: Executes upon pipeline completion (success or failure) as part of the `dsl.ExitHandler`.\n\nThe pipeline also defines a utility function `get_callback_payload(event_type)` that generates a JSON payload for callback events, including `event_type`, `sha`, `pr_num`, `run_id`, and `status` (for finish events).\n\n**Overall Pipeline Flow**:\n*   The pipeline starts by initiating an `ExitHandler` to ensure a \"Training Finished\" callback is sent upon completion.\n*   Inside the `ExitHandler` context, a `callback` `init_container` sends a \"Training Started\" event.\n*   The \"Databricks Data Processing\" component runs first, initiated with the start callback.\n*   Upon successful completion of \"Databricks Data Processing\", the \"TensorFlow Preprocess\" component executes.\n*   Finally, the \"Exit Handler\" component is triggered when the pipeline finishes, regardless of the success of preceding steps.\n\n**Pipeline Parameters**:\n*   `resource_group`: String, no default.\n*   `workspace`: String, no default.\n*   `dataset`: String, no default (represents the data zip file).\n*   `token`: String, no default.\n\n**Kubernetes and Azure Integration**:\n*   The pipeline leverages Kubernetes client for potential future resource management.\n*   It uses custom secret helpers `use_databricks_secret()` and `use_azstorage_secret()` (though only `use_databricks_secret` is explicitly used in the provided snippet).\n*   It defines a `persistent_volume_path` as `/mnt/azure`.\n*   Container images are sourced from `k8scc01covidmlopsacr.azurecr.io/mlops`.\n*   Environment variables like `GITHUB_SHA` and `PR_NUM` are accessed within the `get_callback_payload` function."
  },
  {
    "repo": "references_kfp_files",
    "file": "9rince_kfp_samples_test_after.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `pipeline-with-after` that performs the following steps:\n\nThis pipeline consists of **three components**. All components use the `Print Text` component, which is loaded from a YAML-like string and prints a given text input.\n\n**Component 1: `task1`**\n*   **Function:** Prints the string \"1st task\".\n*   **Input:** `text='1st task'`\n\n**Component 2: `task2`**\n*   **Function:** Prints the string \"2nd task\".\n*   **Input:** `text='2nd task'`\n*   **Control Flow:** This task must execute **after** `task1` has completed successfully.\n\n**Component 3: `task3`**\n*   **Function:** Prints the string \"3rd task\".\n*   **Input:** `text='3rd task'`\n*   **Control Flow:** This task must execute **after** both `task1` and `task2` have completed successfully.\n\nThe pipeline utilizes the `kfp` library, specifically `kfp.v2.dsl` for pipeline definition and `kfp.v2.compiler` for compilation. The `Print Text` component is loaded using `components.load_component_from_text`."
  },
  {
    "repo": "references_kfp_files",
    "file": "pragadeeshraju_kubeflow-sample-pipeline_pipeline.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `Docker test` that performs the following steps:\n\n**Pipeline Description:**\nThe pipeline applies Decision Tree and Logistic Regression for a classification problem.\n\n**Components:**\nThe pipeline consists of **four** components:\n1.  **`getdata`**: This component is loaded from the YAML manifest located at `getdata/getdata.yaml`. Its function is to retrieve data.\n2.  **`reshapedata`**: This component is loaded from the YAML manifest located at `reshapedata/reshapedata.yaml`. Its function is to reshape the retrieved data.\n3.  **`modelbuilding`**: This component is loaded from the YAML manifest located at `modelbuilding/modelbuilding.yaml`. Its function is to build machine learning models, specifically applying Decision Tree and Logistic Regression.\n4.  **`kserve_op`**: This component is loaded from the URL `https://raw.githubusercontent.com/kubeflow/pipelines/master/components/kserve/component.yaml`. Its function is to deploy a model using KServe. It takes the following inputs:\n    *   `action`: 'apply'\n    *   `model_name`: 'tensorflow-sample'\n    *   `model_uri`: 's3://mlpipeline/mnistdocker/models/detect-digits/'\n    *   `namespace`: 'kubeflow-user-example-com'\n    *   `framework`: 'tensorflow'\n    *   `service_account`: 'sa-minio-kserve'\n\n**Control Flow (Dependencies):**\nThe pipeline executes sequentially with explicit dependencies:\n*   `step1` (executing `getdata`) runs first.\n*   `step2` (executing `reshapedata`) runs **after** `step1` completes.\n*   `step3` (executing `modelbuilding`) runs **after** `step2` completes.\n*   The `kserve_op` component runs **after** `step3` completes.\n\n**Compilation:**\nThe pipeline should be compiled into a YAML file named `dockerten1Pipeline.yaml`.\n\n**Tools/Libraries:**\nThe pipeline primarily uses Kubeflow Pipelines (`kfp`) for orchestration and leverages KServe for model deployment. The model building step implicitly uses libraries for Decision Tree and Logistic Regression, though specific libraries like scikit-learn are not explicitly mentioned in the provided snippet. The `model_uri` suggests interaction with S3-compatible storage."
  },
  {
    "repo": "references_kfp_files",
    "file": "9rince_kfp_samples_test_after_test.py",
    "structured_prompt": "I need the actual Python code for the `my_pipeline` function and its components to generate an accurate prompt. The provided snippet only shows how the pipeline is called, but not its definition.\n\nPlease provide the content of `9rince_kfp_samples_test_after_test.py` or at least the definitions of `my_pipeline` and any `@component` functions it uses, including their inputs, outputs, and internal logic (even if it's just `print` statements).\n\nOnce I have the full code, I can provide the detailed prompt."
  },
  {
    "repo": "references_kfp_files",
    "file": "kubeflow_pipelines_samples_test_placeholder_concat.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `one-step-pipeline-with-concat-placeholder` that performs the following steps:\n\n**Pipeline Overview:**\nThis pipeline consists of a single custom component that demonstrates the use of the `concat` placeholder within its arguments.\n\n**Components:**\n\n1.  **`Component with concat placeholder`**\n    *   **Function:** This component takes two string inputs, `input_one` and `input_two`, concatenates them with a literal `+` and `=three`, and then echoes the resulting string to a temporary file `/tmp/test`. It also asserts that the concatenated string is exactly `one+two=three`.\n    *   **Inputs:**\n        *   `input_one` (type: String): Provided with the value `'one'`.\n        *   `input_two` (type: String): Provided with the value `'two'`.\n    *   **Implementation Details:**\n        *   **Image:** `registry.k8s.io/busybox`\n        *   **Command:** `sh`, `-ec`\n        *   **Arguments:**\n            *   `echo \"$0\" > /tmp/test && [[ \"$0\" == 'one+two=three' ]]` (positional argument)\n            *   `concat: [{inputValue: input_one}, '+', {inputValue: input_two}, '=three']` (this argument uses the `concat` placeholder to construct a single string from `input_one`, `'+'`, `input_two`, and `'=three'`)\n\n**Control Flow:**\n*   The pipeline executes the `component_op` directly, passing the specified input values. There are no dependencies or complex control flow constructs (e.g., parallelFor, after).\n\n**Libraries Used:**\n*   `kfp.components` for loading the component definition.\n*   `kfp.dsl` for defining the pipeline."
  },
  {
    "repo": "references_kfp_files",
    "file": "tonouchi510_kfp-project_pipelines_head-pose-dataset-pipeline_head-pose-dataset-pipeline.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `head-pose-dataset pipeline` that performs the following steps to create a TFRecord dataset for a head-pose-pipeline.\n\nThe pipeline takes the following parameters:\n- `pipeline_name` (string, default: \"head-pose-dataset-pipeline\"): The name of the pipeline.\n- `bucket_name` (string, default: \"kfp-project\"): The GCS bucket name.\n- `job_id` (string, default: \"{{JOB_ID}}\"): The unique ID for the current job run.\n- `dataset` (string, default: \"\"): The path to the input dataset.\n- `chunk_size` (integer, default: 1000): The size of data chunks for splitting.\n- `valid_ratio` (float, default: 0.1): The ratio for the validation dataset.\n\nThe pipeline consists of three components:\n\n1.  **`data-chunk-spliter`**:\n    *   **Function:** Splits the input dataset into smaller chunks.\n    *   **Inputs:** `pipeline_name`, `bucket_name`, `job_id`, `dataset`, `chunk_size`.\n    *   **Outputs:** A list of chunk file paths.\n    *   **Configuration:**\n        *   Applied to use a preemptible node pool (`gcp.use_preemptible_nodepool()`).\n        *   Configured to retry up to 2 times (`set_retry(num_retries=2)`).\n\n2.  **`pose-annotation`**:\n    *   **Function:** Processes each data chunk to perform pose annotation and generate TFRecord files.\n    *   **Inputs:** `pipeline_name`, `bucket_name`, `job_id`, `valid_ratio`, `chunk_file` (an item from the output of `data-chunk-spliter`).\n    *   **Configuration:**\n        *   Applied to use a preemptible node pool (`gcp.use_preemptible_nodepool()`).\n        *   Configured to retry up to 2 times (`set_retry(num_retries=2)`).\n\n3.  **`slack-notification`**:\n    *   **Function:** Sends a Slack notification about the pipeline's status upon completion or failure.\n    *   **Inputs:** `pipeline_name`, `job_id`, `message` (set to \"Status: {{workflow.status}}\").\n\n**Control Flow:**\n\n*   The entire pipeline execution is wrapped in an `dsl.ExitHandler`. If the pipeline completes (successfully or with failure), the `slack_notification_op` component will be executed, sending a message including the workflow status.\n*   The `data-chunk-spliter` component (`split_task`) is executed first.\n*   Once `split_task` completes, its output (the list of chunk file paths) is used as the input for a `dsl.ParallelFor` loop.\n*   Inside the `dsl.ParallelFor` loop, the `pose-annotation` component is executed concurrently for each `item` (chunk file) produced by `split_task`.\n\nThe pipeline utilizes Kubeflow Pipelines (KFP) for orchestration and integrates with Google Cloud Platform (GCP) for resource management. Component definitions are loaded from a `kfp.components.ComponentStore`."
  },
  {
    "repo": "references_kfp_files",
    "file": "Ark-kun_pipeline_components_samples_core_train_until_good_train_until_good.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `train_until_good` that performs continuous model training and evaluation until a satisfactory model performance is achieved.\n\nThe pipeline consists of a total of **8 components**, including a recursive graph component.\n\n**Component Breakdown:**\n\n1.  **`chicago_taxi_dataset_op`**: This component loads the Chicago Taxi Trips dataset.\n    *   **Function:** Provides the raw dataset for the pipeline.\n    *   **Outputs:** `output_data` (CSV file containing the dataset).\n\n2.  **`pandas_transform_csv_op`**: This component transforms the CSV data using pandas.\n    *   **Function:** Likely performs data cleaning or feature engineering on the dataset.\n    *   **Inputs:** `dataframe` (from `chicago_taxi_dataset_op.output_data`), `transform_code` (Python code for transformation).\n    *   **Outputs:** `transformed_dataframe` (transformed CSV data).\n\n3.  **`drop_header_op` (Training Data)**: This component removes the header from the transformed training data.\n    *   **Function:** Prepares the training data for model consumption by removing the header row.\n    *   **Inputs:** `table` (from `pandas_transform_csv_op.output`), `has_header` (set to `True`).\n    *   **Outputs:** `table_without_header` (CSV data without header).\n\n4.  **`drop_header_op` (True Values)**: This component removes the header from the true values data.\n    *   **Function:** Isolates the true target values from the dataset, preparing them for evaluation.\n    *   **Inputs:** `table` (from `pandas_transform_csv_op.output`), `has_header` (set to `True`), `column_index` (set to `0`).\n    *   **Outputs:** `table_without_header` (CSV data representing true values without header).\n\n5.  **`xgboost_train_on_csv_op`**: This component performs initial XGBoost model training.\n    *   **Function:** Trains an initial model on the processed training data.\n    *   **Inputs:** `training_data` (from `drop_header_op` for training), `label_column` (set to `0`), `objective` (set to `'reg:squarederror'`), `num_iterations` (set to `50`).\n    *   **Outputs:** `model` (the trained XGBoost model).\n\n6.  **`train_until_low_error` (Graph Component)**: This is a recursive graph component that continuously trains, evaluates, and checks the model.\n    *   **Function:** It encapsulates a loop of model training, prediction, metric calculation, and conditional re-training based on error thresholds.\n    *   **Inputs:**\n        *   `starting_model` (initial model from `xgboost_train_on_csv_op.outputs['model']`).\n        *   `training_data` (from `drop_header_op` for training data).\n        *   `true_values` (from `drop_header_op` for true values).\n    *   **Internal Components & Logic:**\n        *   **`xgboost_train_on_csv_op` (inside graph)**: Trains the model further.\n            *   **Inputs:** `training_data`, `starting_model`, `label_column`, `objective`, `num_iterations`.\n            *   **Outputs:** `model`.\n        *   **`xgboost_predict_on_csv_op` (inside graph)**: Makes predictions using the current model.\n            *   **Inputs:** `data` (training data), `model`, `label_column`.\n            *   **Outputs:** `predictions`.\n        *   **`calculate_regression_metrics_from_csv_op` (inside graph)**: Calculates regression metrics.\n            *   **Inputs:** `true_values`, `predictions`.\n            *   **Outputs:** `metrics` (contains `mean_absolute_error`).\n        *   **`kfp.dsl.Condition`**: Checks if `metrics_task.outputs['mean_absolute_error']` is greater than `0.001`.\n            *   **If True:** The `train_until_low_error` graph component is recursively called with the newly trained model (`model`) as `starting_model`.\n            *   **If False:** The loop terminates, and the final `model` is returned.\n    *   **Outputs:** `output_model` (the final model when the error is low enough).\n\n7.  **`xgboost_predict_on_csv_op` (Final Prediction)**: This component makes final predictions using the best model.\n    *   **Function:** Uses the optimized model from the recursive training loop to make predictions on the original training data.\n    *   **Inputs:** `data` (from `drop_header_op` for training data), `model` (from `train_until_low_error.output`), `label_column` (set to `0`).\n    *   **Outputs:** `output` (the final predictions).\n\n8.  **`calculate_regression_metrics_from_csv_op` (Final Metrics)**: This component calculates the final regression metrics.\n    *   **Function:** Evaluates the performance of the best model based on its final predictions against the true values.\n    *   **Inputs:** `true_values` (from `drop_header_op` for true values), `predictions` (from `xgboost_predict_on_csv_op` final prediction).\n    *   **Outputs:** `metrics` (containing final regression metrics).\n\n**Control Flow:**\n\nThe pipeline executes sequentially:\n1.  `chicago_taxi_dataset_op` runs.\n2.  `pandas_transform_csv_op` runs, dependent on `chicago_taxi_dataset_op`.\n3.  Two instances of `drop_header_op` run in parallel, both dependent on `pandas_transform_csv_op`.\n4.  `xgboost_train_on_csv_op` (initial training) runs, dependent on the first `drop_header_op` (training data).\n5.  `train_until_low_error` (the recursive loop) runs, dependent on `xgboost_train_on_csv_op` (initial model) and both `drop_header_op` outputs.\n    *   **Crucially, the `train_until_low_error` graph component itself contains a conditional recursive call based on the `mean_absolute_error` metric. This forms a `while` loop, continuing training until the error condition is met.**\n6.  `xgboost_predict_on_csv_op` (final prediction) runs, dependent on the final model output from `train_until_low_error` and the first `drop_header_op` output (training data).\n7.  `calculate_regression_metrics_from_csv_op` (final metrics) runs, dependent on the final predictions from `xgboost_predict_on_csv_op` and the second `drop_header_op` output (true values).\n\n**Tools/Libraries:**\n\n*   **Kubeflow Pipelines (KFP):** Used for defining and orchestrating the pipeline.\n*   **Pandas:** Used within the `pandas_transform_csv_op` for data manipulation.\n*   **XGBoost:** Used for machine learning model training and prediction.\n*   **Custom Components:** Several components are loaded from external URLs (e.g., `Chicago_Taxi_Trips`, `XGBoost/Train`, `pandas/Transform_DataFrame`, `ml_metrics/Calculate_regression_metrics`, `tables/Remove_header`).\n\nThe goal is to generate the Python code for this Kubeflow Pipeline using the `@dsl.pipeline` and `@component` decorators, including the recursive graph component and its internal logic."
  },
  {
    "repo": "references_kfp_files",
    "file": "pragadeeshraju_kubeflow-sample-pipeline_pipeline2.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `KServe pipeline` that performs the following steps:\n\nThis pipeline consists of **one** component.\n\n**Pipeline Description:**\nA pipeline for KServe.\n\n**Pipeline Parameters:**\n* `action`: Default value 'apply'.\n* `model_name`: Default value 'tensorflow-sample'.\n* `model_uri`: Default value 's3://mlpipeline/mnistdocker/models/detect-digits/'.\n* `namespace`: Default value 'kubeflow-user-example-com'.\n* `framework`: Default value 'tensorflow'.\n* `service_account`: Default value 'sa-minio-kserve'.\n\n**Component Details:**\n\n1.  **Component Name:** `kserve_op`\n    *   **Function:** This component is loaded from the URL `https://raw.githubusercontent.com/kubeflow/pipelines/master/components/kserve/component.yaml`. It is responsible for interacting with KServe to deploy or manage a model.\n    *   **Inputs:**\n        *   `action`: Passed directly from the pipeline parameter `action`.\n        *   `model_name`: Passed directly from the pipeline parameter `model_name`.\n        *   `model_uri`: Passed directly from the pipeline parameter `model_uri`.\n        *   `namespace`: Passed directly from the pipeline parameter `namespace`.\n        *   `framework`: Passed directly from the pipeline parameter `framework`.\n        *   `service_account`: Passed directly from the pipeline parameter `service_account`.\n    *   **Outputs:** (Not explicitly defined in the provided code, but typical KServe components might output service URL, status, etc.)\n    *   **Tools/Libraries:** KServe component.\n\n**Control Flow:**\nThe `kserve_op` component runs directly, taking all its inputs from the pipeline's input parameters. There are no explicit dependencies, parallel execution, or conditional logic defined."
  },
  {
    "repo": "references_kfp_files",
    "file": "Alexander6463_Kubeflow_MNIST_pipeline_dev.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `End-to-End MNIST Pipeline` that performs a multi-step machine learning workflow including data download, model training, evaluation, export, and serving.\n\nThe pipeline consists of **six** main components:\n\n1.  **Download Dataset**: This component, implemented by the `download_dataset` function, takes `input_bucket` (string) and `dataset_name` (string) as inputs. It downloads the specified dataset and outputs the dataset artifact. Caching for this component is disabled (`max_cache_staleness = \"P0D\"`).\n2.  **Train Model**: This component, implemented by the `train_model` function, takes the dataset artifact output from the `Download Dataset` component as input. It trains a model and outputs the trained model artifact. Caching for this component is disabled.\n3.  **Evaluate Model**: This component, implemented by the `evaluate_model` function, takes the dataset artifact from `Download Dataset` and the trained model artifact from `Train Model` as inputs. It evaluates the model's performance and outputs the evaluation results. Caching for this component is disabled.\n4.  **Export Model**: This component, implemented by the `export_model` function, takes the trained model artifact from `Train Model`, the evaluation results from `Evaluate Model`, `export_bucket` (string), `model_name` (string), and `model_version` (string) as inputs. It exports the model to the specified location. Caching for this component is disabled.\n5.  **KFServing Deploy**: This component is loaded from a remote YAML definition (`https://raw.githubusercontent.com/kubeflow/pipelines/master/components/kubeflow/kfserving/component.yaml`). It takes `action` (string, set to \"apply\"), `model_uri` (string, constructed as `s3://{export_bucket}/{model_name}`), `model_name` (string, set to \"mnist\"), `namespace` (string, set to \"default\"), `framework` (string, set to \"tensorflow\"), and `watch_timeout` (string, set to \"300\") as inputs. Its purpose is to deploy the exported model using KFServing. This component explicitly depends on the completion of the `Export Model` component (`kfservingOp.after(exportOp)`). Caching for this component is disabled.\n\nThe pipeline defines the following default parameters:\n-   `input_bucket`: \"pipelines-tutorial-data\"\n-   `dataset_name`: \"datasets.tar.gz\"\n-   `export_bucket`: \"models\"\n-   `model_name`: \"mnist\"\n-   `model_version`: \"1\"\n\nAll user-defined components (`download_dataset`, `train_model`, `evaluate_model`, `export_model`) should use `drobnov1994/example:kubeflow_tensorflow_v1` as their `BASE_IMAGE`.\n\nThe pipeline also includes a global operation transformer named `op_transformer` that adds the `sidecar.istio.io/inject: false` pod annotation to all operations.\n\nThe pipeline should be run with a KFP client, connecting to `http://127.0.0.1:8080`, creating a run from the `train_and_serve` function with the specified arguments: `input_bucket` (from `INPUT_BUCKET` constant), `dataset_name` (\"datasets.tar.gz\"), `export_bucket` (\"models\"), `model_name` (\"mnist\"), and `model_version` (\"1\")."
  },
  {
    "repo": "references_kfp_files",
    "file": "stackdemos_ml-kbf_components_golang_component.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `stackdemos_ml-kbf_components_golang_component` that performs the following steps:\n\nThe pipeline does not have a `@dsl.pipeline` decorator visible in the provided code snippet, but it defines two reusable component factory functions: `processing_op` and `http_download_op`.\n\nIt uses two custom component definitions:\n\n1.  **`processing_op`**:\n    *   **Function**: A template function designed to create `ContainerOp` instances for processing tasks. It dynamically generates the component name from the script name, sets the container image, command, arguments, and file outputs.\n    *   **Inputs**:\n        *   `script` (string): The path to the script to be executed as the main command.\n        *   `image` (string, optional): The container image to use. If not provided and running in an IPython environment, it attempts to retrieve `GOLANG_IMAGE` from the IPython user namespace.\n        *   `arguments` (list, optional): A list of arguments to pass to the script.\n        *   `file_outputs` (dict, optional): A dictionary mapping output names to their file paths within the container.\n    *   **Outputs**: Returns a `kfp.dsl.ContainerOp` instance.\n    *   **Tools/Libraries**: Uses `re` for name sanitization and `os` for path manipulation. It also conditionally checks for `IPython` to fetch the image.\n\n2.  **`http_download_op`**:\n    *   **Function**: Creates a `ContainerOp` to download a file from a given URL using `curl`, with an `md5sum` pre-check for idempotency.\n    *   **Inputs**:\n        *   `url` (string): The URL of the file to download.\n        *   `download_to` (string): The destination path where the file should be saved.\n        *   `md5sum` (string): The expected MD5 checksum of the file.\n    *   **Outputs**: Returns a `kfp.dsl.ContainerOp` instance.\n    *   **Tools/Libraries**: Uses `appropriate/curl` as the container image. The command involves `sh`, `test`, `awk`, and `curl` for the download and checksum verification.\n\n**Control Flow**:\nThe provided code only defines component factory functions. It does not define a pipeline decorated with `@dsl.pipeline`, so there is no explicit control flow like `after` or `parallelFor` specified at this level. These functions are intended to be called within a larger Kubeflow pipeline definition to instantiate and connect components."
  },
  {
    "repo": "references_kfp_files",
    "file": "rakesh283343_kubeflow-sample-pipelines_core_loop_output_loop_output.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `my-pipeline` that performs the following sequence of operations:\n\nThe pipeline consists of three components.\n\n1.  **`my-out-cop0`**: This component uses the `python:alpine3.6` image. Its purpose is to generate a JSON list of integers ranging from 20 to 30 (inclusive) and output it to a file named `/tmp/out.json`. This output is then captured as the `out` artifact, which serves as an input for subsequent steps.\n\n2.  **`my-in-cop1`**: This component uses the `library/bash:4.4.23` image. It is executed in parallel (`dsl.ParallelFor`) for each item in the list produced by `my-out-cop0`. For each `item`, it simply prints a message indicating the processing of that specific item.\n\n3.  **`my-out-cop2`**: This component uses the `library/bash:4.4.23` image. Its purpose is to display the entire output list generated by the `my-out-cop0` component. It depends on `my-out-cop0` completing successfully to access its output.\n\nThe pipeline utilizes the `kfp` library for defining and compiling the pipeline. The control flow involves an initial component producing a list, followed by a parallel execution of another component over the items of that list, and finally, a component that consumes the initial list's full output."
  },
  {
    "repo": "references_kfp_files",
    "file": "II-VSB-II_TaxiClassificationKubeflowPipelineMinio_Taxi-Pipeline.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `TFX Taxi Cab Classification Pipeline Example` that performs classification with model analysis based on a public BigQuery dataset.\n\nThe pipeline consists of **7 distinct components** and utilizes several external YAML definitions loaded from a GitHub repository, specifically `II-VSB-II/TaxiClassificationKubeflowPipelineMinio`. It also uses `kfp` library features like `dsl.VolumeOp` and `dsl.ContainerOp`.\n\n**Pipeline Parameters:**\n*   `project` (string)\n*   `output` (string, default: `/mnt/shared`)\n*   `column_names` (string, default: `/mnt/shared/pipelines/column-names.json`)\n*   `key_columns` (string, default: `trip_start_timestamp`)\n*   `train` (string, default: `/mnt/shared/pipelines/train.csv`)\n*   `evaluation` (string, default: `/mnt/shared/pipelines/eval.csv`)\n*   `mode` (string, default: `local`)\n*   `preprocess_module` (string, default: `/mnt/shared/pipelines/preprocessing.py`)\n*   `learning_rate` (float, default: `0.1`)\n*   `hidden_layer_size` (string, default: `1500`)\n*   `steps` (integer, default: `3000`)\n*   `analyze_slice_column` (string, default: `trip_start_hour`)\n\n**Environment Variables:**\nThe pipeline requires the following environment variables to be set, likely for MinIO integration:\n*   `MINIO_SECRET_KEY` with value `minio123`\n*   `MINIO_ACCESS_KEY` with value `minio`\n*   `MINIO_ENDPOINT` with value `minio-service:9000`\n\n**Pipeline Logic and Component Definitions:**\n\nThe pipeline's execution flow is conditional based on the `platform` variable (not explicitly a pipeline parameter but determined outside the `@dsl.pipeline` decorator).\n\n**If `platform` is NOT 'GCP'**:\n1.  **`create_pvc` (dsl.VolumeOp):**\n    *   **Function:** Creates a Persistent Volume Claim (PVC) named `pipeline-pvc` with `RWM` (ReadWriteMany) mode and size `1Gi`. This PVC is intended to store shared data for the pipeline.\n    *   **Output:** A volume that can be mounted by subsequent components.\n\n2.  **`checkout` (dsl.ContainerOp):**\n    *   **Function:** This component is conditional and only runs if `proxy` is not an empty string. Its exact function is not fully visible in the provided snippet but implies a checkout operation (e.g., cloning a Git repository) perhaps through a proxy.\n    *   **Dependencies:** Depends on the successful creation of `create_pvc` (mounts its volume).\n\n**Common Components (regardless of `platform`):**\n\nThe following components are loaded from external YAML files and are likely custom containerized operations. Each component likely sets `minio_access_key`, `minio_secret_key`, and `minio_endpoint` environment variables, and mounts the `vop.volume` (if `platform` is not 'GCP') or similar shared storage.\n\n1.  **`dataflow-tf-data-validation` (from `tfdv_component.yaml`):**\n    *   **Function:** Performs data validation using TensorFlow Data Validation (TFDV).\n    *   **Inputs:** `data` (from `train` parameter), `evaluation_data` (from `evaluation` parameter), `column_names_path` (from `column_names` parameter), `key_columns` (from `key_columns` parameter).\n    *   **Outputs:** `schema` (output path derived from `output_template`).\n    *   **Dependencies:** If `platform` is not 'GCP', it depends on `create_pvc` and optionally `checkout`.\n\n2.  **`dataflow-tf-transform` (from `tft_component.yaml`):**\n    *   **Function:** Performs data transformation using TensorFlow Transform (TFT).\n    *   **Inputs:** `train_data` (from `train` parameter), `evaluation_data` (from `evaluation` parameter), `schema` (from `dataflow-tf-data-validation`'s `schema` output), `preprocess_module` (from `preprocess_module` parameter).\n    *   **Outputs:** `transformed_data_path`, `transformed_eval_data_path`, `transform_output_path`.\n    *   **Dependencies:** Depends on `dataflow-tf-data-validation`.\n\n3.  **`tf-train` (from `dnntrainer_component.yaml`):**\n    *   **Function:** Trains a Deep Neural Network (DNN) model.\n    *   **Inputs:** `transformed_data_path`, `transformed_eval_data_path`, `transform_output_path` (all from `dataflow-tf-transform`), `schema` (from `dataflow-tf-data-validation`), `learning_rate` (from `learning_rate` parameter), `hidden_layer_size` (from `hidden_layer_size` parameter), `steps` (from `steps` parameter), `target_lambda` (defined internally), `target_class_lambda` (defined internally).\n    *   **Outputs:** `model` (output path derived from `output_template`).\n    *   **Dependencies:** Depends on `dataflow-tf-transform`.\n\n4.  **`dataflow-tf-model-analyze` (from `tfma_component.yaml`):**\n    *   **Function:** Analyzes the trained model using TensorFlow Model Analysis (TFMA).\n    *   **Inputs:** `model` (from `tf-train`), `evaluation_data` (from `evaluation` parameter), `schema` (from `dataflow-tf-data-validation`), `analyze_slice_column` (from `analyze_slice_column` parameter).\n    *   **Dependencies:** Depends on `tf-train`.\n\n5.  **`dataflow-tf-predict` (from `predict_component.yaml`):**\n    *   **Function:** Performs predictions using the trained model.\n    *   **Inputs:** `model` (from `tf-train`), `data` (from `evaluation` parameter).\n    *   **Outputs:** `predictions` (output path derived from `output_template`).\n    *   **Dependencies:** Depends on `tf-train`.\n\n6.  **`confusion-matrix` (from `confusion_matrix_component.yaml`):**\n    *   **Function:** Calculates and visualizes the confusion matrix.\n    *   **Inputs:** `predictions` (from `dataflow-tf-predict`), `target_lambda` (defined internally), `target_class_lambda` (defined internally).\n    *   **Dependencies:** Depends on `dataflow-tf-predict`.\n\n7.  **`roc` (from `roc_component.yaml`):**\n    *   **Function:** Calculates and visualizes the Receiver Operating Characteristic (ROC) curve.\n    *   **Inputs:** `predictions` (from `dataflow-tf-predict`), `target_lambda` (defined internally), `target_class_lambda` (defined internally).\n    *   **Dependencies:** Depends on `dataflow-tf-predict`.\n\n**Control Flow:**\n*   `dataflow-tf-data-validation` is the initial component (after PVC creation/checkout if applicable).\n*   `dataflow-tf-transform` runs `after` `dataflow-tf-data-validation`.\n*   `tf-train` runs `after` `dataflow-tf-transform`.\n*   `dataflow-tf-model-analyze` runs `after` `tf-train`.\n*   `dataflow-tf-predict` runs `after` `tf-train`.\n*   `confusion-matrix` and `roc` run in parallel, both `after` `dataflow-tf-predict`.\n\n**Tools and Libraries:**\n*   Kubeflow Pipelines (kfp, dsl, components)\n*   TensorFlow Data Validation (TFDV)\n*   TensorFlow Transform (TFT)\n*   TensorFlow Model Analysis (TFMA)\n*   Kubernetes (for PVC and environment variables)\n*   MinIO (implied by environment variables)\n*   Potentially other libraries within the custom components (e.g., pandas, numpy for data handling, scikit-learn for metrics)."
  },
  {
    "repo": "references_kfp_files",
    "file": "stackdemos_kfappx_components_training_component.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `stackdemos-kfappx-components-training-component` that performs the following steps:\n\nThis pipeline includes two utility functions that define reusable container operations: `training_op` and `http_download_op`.\n\n1.  **`training_op` Function:**\n    *   **Purpose:** This function serves as a template for creating Kubeflow `ContainerOp` components that execute Python training scripts. It encapsulates common logic for defining training steps.\n    *   **Inputs:**\n        *   `script` (str): The path to the Python script to be executed.\n        *   `image` (str, optional): The Docker image to use for the container. If not provided and running in an IPython environment, it attempts to retrieve the `TRAINING_IMAGE` global variable.\n        *   `arguments` (list, optional): A list of command-line arguments to pass to the script.\n        *   `file_outputs` (dict, optional): A dictionary defining file outputs for the component.\n    *   **Functionality:**\n        *   Determines the Docker image to use, prioritizing the explicit `image` parameter, then `TRAINING_IMAGE` from IPython, and raising an error if neither is found.\n        *   Constructs a `ContainerOp` with a sanitized name derived from the script name.\n        *   Sets the command to `/usr/local/bin/python` followed by the `script` path.\n        *   Passes any specified `arguments` and defines `file_outputs`.\n    *   **Tools/Libraries:** `kfp.dsl.ContainerOp`, `urllib.parse`, `os`, `re`.\n\n2.  **`http_download_op` Function:**\n    *   **Purpose:** This function defines a Kubeflow `ContainerOp` for securely downloading artifacts from a URL with an MD5 checksum pre-check.\n    *   **Inputs:**\n        *   `url` (str): The URL of the artifact to download.\n        *   `download_to` (str): The local path where the artifact should be saved.\n        *   `md5sum` (str): The expected MD5 checksum of the artifact.\n    *   **Functionality:**\n        *   Creates a `ContainerOp` named `download-artifact`.\n        *   Uses the `appropriate/curl` Docker image.\n        *   Executes a shell script (`sh -c`) that performs the following logic:\n            *   Checks if the MD5 sum of the `download_to` file (if it exists) matches the provided `md5sum`.\n            *   If they match, it prints \"Skipping due to [download_to] has been already downloaded\".\n            *   Otherwise, it uses `curl` to download the file from the `url` to the `download_to` path, creating directories as needed (`--create-dirs`).\n    *   **Tools/Libraries:** `kfp.dsl.ContainerOp`, `curl` (within the Docker image).\n\nThe control flow is defined by how these functions are called within a larger Kubeflow pipeline; they are building blocks, not a complete pipeline themselves. Dependencies and specific sequencing (`after` or `parallelFor`) would be dictated by the pipeline definition that utilizes these functions."
  },
  {
    "repo": "references_kfp_files",
    "file": "sbakiu_kubeflow-spark_kubeflow_pipeline.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `Spark Operator job pipeline` that performs the following steps:\n\n**Pipeline Description:** \"Spark Operator job pipeline\"\n\n**Overall Structure:**\nThe pipeline orchestrates the deployment and monitoring of a Spark application using the Kubeflow Spark Operator.\n\n**Components:**\n\n1.  **`spark_job_op`**:\n    *   **Function:** Applies a Kubernetes SparkApplication manifest to create a Spark job.\n    *   **Inputs:**\n        *   `object`: A JSON string representation of the Spark job definition. This definition is dynamically generated by calling `get_spark_job_definition()` which reads `spark-job-python.yaml` and adds a unique epoch timestamp to the job name.\n    *   **Outputs:**\n        *   `name`: The name of the created Spark application.\n    *   **Tools/Libraries:** Uses `kfp.components.load_component_from_file(\"k8s-apply-component.yaml\")`.\n    *   **Caching:** Caching is disabled (`max_cache_staleness = \"P0D\"`) to ensure the Spark job is always applied.\n\n2.  **`spark_application_status_op`**:\n    *   **Function:** This is a **graph component** (recursive function) named `graph_component_spark_app_status` that continuously checks the status of the Spark application until it reaches a \"COMPLETED\" state. It introduces a 5-second delay between checks.\n    *   **Inputs:**\n        *   `input_application_name`: The name of the Spark application to monitor (obtained from `spark_job_op.outputs[\"name\"]`).\n    *   **Outputs:**\n        *   `applicationstate`: The current state of the Spark application.\n        *   `name`: The name of the application being monitored (passed through recursively).\n    *   **Tools/Libraries:** Uses `kfp.components.load_component_from_file(\"k8s-get-component.yaml\")` to query Kubernetes for the Spark application status.\n    *   **Caching:** Caching is disabled (`max_cache_staleness = \"P0D\"`) for the status check to ensure real-time updates.\n\n3.  **`print_message`**:\n    *   **Function:** Prints a message indicating that the Spark job has completed.\n    *   **Inputs:** A string message, which includes the `spark_job_name` (derived from `spark_job_op.outputs[\"name\"]`).\n    *   **Tools/Libraries:** Uses an `alpine:3.6` image with a simple `echo` command.\n    *   **Caching:** Caching is disabled (`max_cache_staleness = \"P0D\"`) to ensure the completion message is always printed.\n\n**Control Flow:**\n\n*   `spark_job_op` is the initial step, responsible for submitting the Spark application.\n*   `spark_application_status_op` runs **after** `spark_job_op`, ensuring that status monitoring only begins once the Spark job has been submitted. This component is a recursive graph component that will repeatedly check the Spark application's status until it's \"COMPLETED\".\n*   `print_message` runs **after** `spark_application_status_op`, ensuring the completion message is displayed only after the Spark application has successfully finished.\n\n**External Files/Dependencies:**\n*   `spark-job-python.yaml`: Defines the Spark job manifest.\n*   `k8s-apply-component.yaml`: Defines a Kubeflow component for applying Kubernetes resources.\n*   `k8s-get-component.yaml`: Defines a Kubeflow component for getting Kubernetes resources.\n\nThe pipeline should be compiled to `spark_job_pipeline.yaml`."
  },
  {
    "repo": "references_kfp_files",
    "file": "ajinkya933_Kubeflow_mnist_REST_API.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `MNIST Pipeline` that performs GPU-accelerated training of an MNIST model.\n\nThe pipeline consists of **two** main components:\n\n1.  **`create_volume` (dsl.VolumeOp)**:\n    *   **Function:** Creates a persistent volume for data storage within the pipeline.\n    *   **Inputs:**\n        *   `name`: 'create_volume'\n        *   `resource_name`: 'data-volume'\n        *   `size`: '1Gi'\n        *   `modes`: `dsl.VOLUME_MODE_RWM` (ReadWriteMany)\n    *   **Outputs:** A volume object (`vop.volume`).\n\n2.  **`train_op` (Custom Component based on `train` function)**:\n    *   **Function:** This component handles the entire MNIST training process. It includes:\n        *   **Data Import:** Loads the Fashion MNIST dataset using Keras.\n        *   **Data Preprocessing:** Normalizes image pixel values (scales to 0-1).\n        *   **Model Creation:** Defines a Keras Sequential model with a Flatten layer, a Dense ReLU layer (128 units), and a final Dense layer (10 units).\n        *   **Model Compilation:** Configures the model with 'adam' optimizer, `SparseCategoricalCrossentropy` loss (from logits), and 'accuracy' metric.\n        *   **Training:** Fits the model to the training data for 10 epochs.\n        *   **Evaluation:** Evaluates the trained model on test data and prints the test accuracy.\n        *   **Model Saving:** Saves the trained Keras model to a specified file path (`mnist_model.h5`).\n        *   **Test Data Saving:** Saves the `test_images` and `test_labels` as a pickle file.\n    *   **Inputs:**\n        *   `data_path` (DSL parameter): Specifies the mount path for the volume, defaulting to '/mnt'.\n        *   `model_file` (DSL parameter): Specifies the name for the saved model file, defaulting to 'mnist_model.h5'.\n    *   **Outputs:** None explicitly defined, but it saves model and test data artifacts to the mounted volume.\n    *   **Libraries/Tools:** `tensorflow` (specifically `keras`), `pickle`.\n    *   **Base Image:** `tensorflow/tensorflow:latest-gpu-py3`.\n\n**Control Flow:**\n\n*   The `create_volume` component runs first to provision the storage.\n*   The `train_op` component depends on `create_volume`. It attaches the volume created by `create_volume` at the specified `data_path` within its container. The output volume from `create_volume` (`vop.volume`) is passed to `train_op` via `add_pvolumes({data_path: vop.volume})`.\n\nThe pipeline is designed to utilize GPU resources for training."
  },
  {
    "repo": "references_kfp_files",
    "file": "kubeflow_pipelines_samples_v2_collected_artifacts.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `collecting_artifacts` that performs the following steps to demonstrate various artifact collection scenarios, including nested parallel loops and subdag integration.\n\nThe pipeline utilizes the following 10 components:\n\n1.  **`split_ids`**: Takes a comma-separated string `model_ids` as input and returns a `list` of strings, splitting the input by commas. This component is used to parse model IDs.\n2.  **`create_file`**: Takes a `content` string and an `Output[Artifact]` named `file` as inputs. It writes the `content` to the specified artifact path.\n3.  **`read_files`**: Takes a `List[Artifact]` named `files` as input and reads the content of each file, printing it. It returns a string 'files read'.\n4.  **`read_single_file`**: Takes a single `Artifact` named `file` as input, reads its content, prints it, and returns the file's URI.\n5.  **`split_chars`**: Takes a comma-separated string `model_chars` as input and returns a `list` of strings, splitting the input by commas. This component is used to parse model characters.\n6.  **`create_dataset`**: Takes a `content` string and an `Output[Dataset]` named `data` as inputs. It writes the `content` to the specified dataset path.\n7.  **`read_datasets`**: Takes a `List[Dataset]` named `data` as input and reads the content of each dataset, printing it. It returns a string 'files read'.\n8.  **`read_single_dataset_generate_model`**: Takes a `Dataset` named `data`, a string `id`, and an `Output[Model]` named `results` as inputs. It reads content from the dataset, appends the `id`, writes this combined string to the model output, and sets `model` and `model_name` metadata for the model.\n9.  **`read_models`**: Takes a `List[Model]` named `models` as input and reads the content and metadata of each model, printing them. It returns a string 'models read'.\n10. **`single_node_dag`**: This is a sub-pipeline. It takes a string `char` as input. Internally, it calls `create_dataset` with the `char` as content, disables caching for this operation, and returns the `data` output of `create_dataset`.\n\nThe pipeline's overall structure and control flow are as follows:\n\n*   **Inputs**:\n    *   `model_ids` (string, default: empty string)\n    *   `model_chars` (string, default: empty string)\n*   **Outputs**:\n    *   A `List[Model]`\n*   **Control Flow**:\n    1.  **`ids_split_op`**: Calls `split_ids` with `model_ids`.\n    2.  **`create_files_task` (ParallelFor loop)**: Iterates over the output of `ids_split_op` (each element named `id_`).\n        *   Inside the loop, `create_file` is called with `content=id_`. The output artifact is named `my_file_`.\n    3.  **`read_files_op`**: Calls `read_files` after `create_files_task` completes, providing all collected `my_file_` artifacts from the `create_files_task` loop using `dsl.Collected(create_files_task.outputs[\"my_file_\"])`.\n    4.  **`nested_loop_task` (ParallelFor loop)**: Iterates over the output of `ids_split_op` (each element named `id_2`).\n        *   **`inner_create_file_op`**: Inside this loop, `create_file` is called with `content=id_2`. The output artifact is named `my_file_inner_`.\n        *   **`inner_read_file_op`**: Calls `read_single_file` with the `my_file_inner_` output from the current iteration of `inner_create_file_op`.\n    5.  **`chars_split_op`**: Calls `split_chars` with `model_chars`.\n    6.  **`nested_dataset_loop_task` (ParallelFor loop)**: Iterates over the output of `ids_split_op` (each element named `id_3`).\n        *   **`inner_subdag_op` (ParallelFor loop)**: Iterates over the output of `chars_split_op` (each element named `char_`).\n            *   Inside this nested loop, the `single_node_dag` sub-pipeline is called with `char=char_`. The output dataset is named `dataset_from_subdag_`. The subdag is uncached (`.set_caching_options(False)`).\n        *   **`create_model_from_dataset_op`**: Calls `read_single_dataset_generate_model` after `inner_subdag_op` completes for the current `id_3` iteration. It uses `dsl.Collected(inner_subdag_op.outputs[\"dataset_from_subdag_\"])` as the `data` input (note: this requires a specific collection behavior for `Dataset` list to single `Dataset`), and `id_3` as the `id` input. The output model is named `model_from_dataset_`.\n    7.  **`read_models_op`**: Calls `read_models` after `nested_dataset_loop_task` completes, providing all collected `model_from_dataset_` models from the `nested_dataset_loop_task` loop using `dsl.Collected(nested_dataset_loop_task.outputs[\"model_from_dataset_\"])`.\n*   **Final Output**: The pipeline returns the collected models from `read_models_op`.\n\nThe pipeline heavily utilizes `kfp.dsl.Collected` for passing collected artifacts and datasets between parallel iterations and to downstream tasks, demonstrating complex data dependencies. No external libraries beyond `kfp` and standard Python types are explicitly used within the component logic shown."
  },
  {
    "repo": "references_kfp_files",
    "file": "xxxtrillionarie_KubeFlow_MLOps_Pipelines_samples_test_cache_v2_compatible_test.py",
    "structured_prompt": "As an MLOps expert and prompt engineer, I'll craft a detailed prompt for LLM to regenerate the Kubeflow pipeline.\n\n---\n\nGenerate a Kubeflow Pipeline named `two_step_pipeline` that performs a simple two-step machine learning workflow: data preprocessing followed by model training.\n\nThe pipeline consists of **two components**:\n\n1.  **`preprocess`**:\n    *   **Function**: This component is responsible for data preprocessing.\n    *   **Inputs**:\n        *   `uri` (str): An input parameter representing a URI.\n        *   `some_int` (int): An input parameter of integer type.\n    *   **Outputs**:\n        *   `output_dataset_one` (Dataset artifact): Represents the preprocessed dataset. It has a display name of 'output_dataset_one'.\n        *   `output_parameter_one` (int parameter): An output parameter that echoes the `some_int` input.\n\n2.  **`train_op`**:\n    *   **Function**: This component performs the model training.\n    *   **Inputs**:\n        *   `dataset` (Dataset artifact): This input artifact receives the `output_dataset_one` from the `preprocess` component.\n        *   `num_steps` (int): An input parameter, which receives the `output_parameter_one` from the `preprocess` component.\n    *   **Outputs**:\n        *   `model` (Model artifact): Represents the trained model. It has a display name of 'model'.\n\n**Control Flow**:\nThe `train_op` component **depends on** the `preprocess` component. The `train_op` must execute `after` the `preprocess` component has successfully completed and provided its outputs. The `dataset` input to `train_op` must be the `output_dataset_one` artifact from `preprocess`, and the `num_steps` input to `train_op` must be the `output_parameter_one` parameter from `preprocess`.\n\n**Pipeline Inputs**:\nThe `two_step_pipeline` itself takes the following inputs:\n*   `uri` (str): A string representing a URI, with a default value.\n*   `some_int` (int): An integer parameter, with a default value.\n\n**Tools/Libraries**:\nThe components are basic container executions; no specific external ML libraries (like scikit-learn or TensorFlow) are explicitly mentioned within the component definitions provided, but they would typically reside within the container images used by the components. The pipeline itself uses the `kfp` library for defining Kubeflow pipelines."
  },
  {
    "repo": "references_kfp_files",
    "file": "xxxtrillionarie_KubeFlow_MLOps_Pipelines_samples_test_legacy_exit_handler_test.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `download_and_print` that performs the following steps:\n\nThis pipeline consists of a single component.\n\n**Component 1: `download_and_print`**\n*   **Function:** This component is designed to download some content (implied by the name \"download\") and then print it.\n*   **Inputs:** No explicit inputs are visible in this snippet.\n*   **Outputs:** No explicit outputs are visible in this snippet.\n*   **Dependencies:** This is the initial and only component in the pipeline.\n\n**Control Flow:**\nThe pipeline executes the `download_and_print` component directly. There are no complex control flow constructs like parallel execution, conditional logic, or loops.\n\n**Libraries/Tools:**\nThe pipeline utilizes the `kfp.deprecated` library for defining and running Kubeflow pipelines. The component itself `download_and_print` is imported from a local module `.legacy_exit_handler`. The specific tools or libraries used within the `download_and_print` component (e.g., for downloading or printing) are not detailed in this snippet, but it implies standard Python capabilities."
  },
  {
    "repo": "references_kfp_files",
    "file": "kubeflow_pipelines_samples_v2_hello_world.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `hello-world` that performs a simple text printing operation.\n\nThis pipeline consists of a single component.\n\nThe pipeline accepts one input parameter:\n- `text`: A string with a default value of 'hi there'. This parameter is described as \"small pipeline parameter string\".\n\nThe pipeline's sole component is `hello_world`.\n- **Component Function:** This component takes a string input, prints it to standard output, and then returns the same string.\n- **Component Inputs:** It receives `text` as an input, which is directly passed from the pipeline's `text` parameter.\n- **Component Outputs:** It returns a string, which is the same as its input.\n\n**Control Flow:**\nThe pipeline executes the `hello_world` component directly, passing the pipeline's `text` input as an argument to the component's `text` parameter. There are no dependencies, parallel execution, or conditional logic.\n\n**Tools/Libraries:**\nThe pipeline uses the `kfp` library for defining pipelines and components. The component specifically uses standard Python `print()` function."
  },
  {
    "repo": "references_kfp_files",
    "file": "rakesh283343_kubeflow-sample-pipelines_core_exit_handler_exit_handler.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `Exit Handler` that performs the following steps:\n\nThis pipeline has a total of **three components**, including one used as an exit handler.\n\n1.  **`GCS - Download` (gcs_download_op)**:\n    *   **Function**: Downloads content from a Google Cloud Storage (GCS) URL.\n    *   **Input**: `url` (a string, defaults to 'gs://ml-pipeline/shakespeare/shakespeare1.txt').\n    *   **Output**: `data` (the content downloaded to `/tmp/results.txt`).\n    *   **Image**: `google/cloud-sdk:279.0.0`\n    *   **Command**: `sh -c`\n    *   **Arguments**: `gsutil cat $0 | tee $1` with `$0` being the input URL and `$1` being `/tmp/results.txt`.\n\n2.  **`echo` (echo_op)**:\n    *   **Function**: Prints a given text string.\n    *   **Input**: `text` (a string).\n    *   **Image**: `library/bash:4.4.23`\n    *   **Command**: `sh -c`\n    *   **Arguments**: `echo \"$0\"` with `$0` being the input text.\n\n3.  **`echo` (echo_op) - Used as Exit Handler**:\n    *   **Function**: Prints the static string \"exit!\". This component is configured as the pipeline's exit handler.\n\n**Control Flow**:\n\n*   The pipeline uses a **`dsl.ExitHandler`** construct. The component `echo_op('exit!')` named `exit_task` is set as the exit handler, meaning it will execute after the main pipeline graph completes, regardless of success or failure.\n*   Within the `ExitHandler` block, the following sequential execution occurs:\n    *   `download_task` (`GCS - Download`) executes first, taking the `url` pipeline input.\n    *   `echo_task` (`echo`) executes after `download_task` completes, using the `data` output from `download_task` as its input.\n\n**Libraries Used**: `kfp` and `kfp.dsl`."
  },
  {
    "repo": "references_kfp_files",
    "file": "e2fyi_kfx_kfx_dsl__transformers_test.py",
    "structured_prompt": "The provided Python code does *not* define a Kubeflow Pipeline using `@dsl.pipeline` or `@component` decorators. Instead, it contains unit tests for a custom `ContainerOpTransform` class, which appears to be part of a library (`kfx.dsl._transformers`) that extends or modifies `kfp.dsl.ContainerOp` behavior.\n\nTherefore, I cannot generate a prompt for a Kubeflow Pipeline from this code. The code focuses on testing transformation functionalities for `ContainerOp` instances, not on defining a pipeline flow.\n\nTo give you an example of the prompt I *would* generate if a pipeline was present:\n\n---\n\n**Example of a valid prompt for a *hypothetical* pipeline, if the code defined one:**\n\nGenerate a Kubeflow Pipeline named `ImageProcessingPipeline` that performs a series of image transformations.\n\nThis pipeline consists of three components:\n1.  **`download_image`**: This component takes a `url` (string) as input and outputs an `image_path` (string) representing the local path to the downloaded image. It should use an image like `ubuntu:latest` and a simple `wget` command to download the file.\n2.  **`resize_image`**: This component takes an `input_image_path` (string) and a `target_size` (string, e.g., \"640x480\") as inputs. It outputs an `output_image_path` (string) for the resized image. This component should leverage `Pillow` or `OpenCV` to perform the resizing. It should also demonstrate setting resource requests and limits (e.g., `cpu=\"500m\"`, `memory=\"1G\"`).\n3.  **`apply_filter`**: This component takes an `input_image_path` (string) and a `filter_type` (string, e.g., \"grayscale\", \"sepia\") as inputs. It outputs a `filtered_image_path` (string). This component should also use `Pillow` or `OpenCV` for image processing and demonstrate setting environment variables (e.g., `DEBUG_MODE=true`).\n\n**Control Flow:**\n*   The `download_image` component runs first, taking a pipeline parameter `image_url` as its input `url`.\n*   The `resize_image` component runs `after` `download_image`, using `download_image.outputs['image_path']` as its `input_image_path`. It also takes a pipeline parameter `desired_size` for its `target_size`.\n*   The `apply_filter` component runs `after` `resize_image`, using `resize_image.outputs['output_image_path']` as its `input_image_path`. It takes a pipeline parameter `chosen_filter` for its `filter_type`.\n\n**Pipeline Inputs:**\n*   `image_url`: type `str`, default `https://example.com/some_image.jpg`\n*   `desired_size`: type `str`, default `800x600`\n*   `chosen_filter`: type `str`, default `grayscale`\n\n**Libraries/Tools:** The `resize_image` and `apply_filter` components should internally use a Python environment with `Pillow` (PIL) installed. The `download_image` component will use standard shell commands (e.g., `wget`).\n\n---"
  },
  {
    "repo": "references_kfp_files",
    "file": "fybrik_kfp-components_samples_house_price_estimates_pipeline-tekton.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `Fybrik housing price estimate pipeline` that performs the following steps to estimate house prices:\n\nThe pipeline consists of four main components and takes two string parameters: `test_dataset_id` and `train_dataset_id`.\n\n**Pipeline Logic:**\n\n1.  **Retrieve Current Namespace:** Before any components run, the pipeline first determines the current Kubernetes namespace it is executing within. This value is then used as an input to subsequent components.\n2.  **Generate Run-Specific Names:** It also generates unique names for the current run and for storing results, incorporating the `dsl.RUN_ID_PLACEHOLDER`.\n\n**Components:**\n\n*   **Component 1: `getDataEndpoints`**\n    *   **Function:** This component is responsible for retrieving data endpoints for both training and testing datasets. It likely interacts with a data catalog or a similar service to get access paths for the specified datasets.\n    *   **Inputs:** `train_dataset_id`, `test_dataset_id`, `namespace` (the determined current namespace), `run_name` (the generated run ID), and `result_name` (the generated result submission name).\n    *   **Outputs:** `train_endpoint`, `test_endpoint`, and `result_endpoint`, and `result_catalogid`.\n    *   **Dependency:** This is the initial component, having no upstream dependencies.\n    *   **Source:** Loaded from `../../get_data_endpoints/component.yaml`.\n\n*   **Component 2: `visualizeTable`**\n    *   **Function:** This component visualizes the training data table. This likely involves generating charts, statistics, or a tabular view of the data.\n    *   **Inputs:** `train_endpoint` (from `getDataEndpointsStep.outputs['train_endpoint']`), `train_dataset_id`, and `namespace`.\n    *   **Dependency:** Executes *after* `getDataEndpointsStep` has successfully completed.\n    *   **Source:** Loaded from `./visualize_table/component.yaml`.\n\n*   **Component 3: `trainModel`**\n    *   **Function:** This is the core machine learning component. It trains a model using the provided training data and evaluates it against the test data. It also prepares results for submission. The model training likely involves libraries such as scikit-learn or similar ML frameworks, but no specific libraries are explicitly visible from the prompt's context.\n    *   **Inputs:** `train_endpoint_path` (from `getDataEndpointsStep.outputs['train_endpoint']`), `test_endpoint_path` (from `getDataEndpointsStep.outputs['test_endpoint']`), `result_name`, `result_endpoint_path` (from `getDataEndpointsStep.outputs['result_endpoint']`), `train_dataset_id`, `test_dataset_id`, and `namespace`.\n    *   **Dependency:** Executes *after* `visualizeTableStep` has successfully completed.\n    *   **Source:** Loaded from `./train_model/component.yaml`.\n\n*   **Component 4: `submitResult`**\n    *   **Function:** This final component submits the results of the model training, likely registering them back into a catalog or storage system.\n    *   **Inputs:** `result_catalogid` (from `getDataEndpointsStep.outputs['result_catalogid']`).\n    *   **Dependency:** Executes *after* `trainModelStep` has successfully completed.\n    *   **Source:** Loaded from `./submit_result/component.yaml`.\n\n**Control Flow:**\n\nThe pipeline executes sequentially, with explicit `after()` dependencies ensuring that each step only begins once its preceding dependency is complete:\n1.  `getDataEndpointsStep`\n2.  `visualizeTableStep` depends on `getDataEndpointsStep`\n3.  `trainModelStep` depends on `visualizeTableStep`\n4.  `submitResultStep` depends on `trainModelStep`"
  },
  {
    "repo": "references_kfp_files",
    "file": "jonrossclaytor_kubeflow-mlb_pipelines_baseball-pipeline-single.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `Sequential pipeline` that performs a sequential machine learning workflow for a specific `pitch_type`.\n\nThe pipeline consists of nine components:\n\n1.  **`Collect Stats`**: This initial component is responsible for collecting statistical data. It has no explicit inputs or outputs beyond its internal operations, which likely involve data collection from external sources. Its container image is `gcr.io/ross-kubeflow/collect-stats:latest`.\n2.  **`Feature Engineering`**: This component performs feature engineering. It depends on `Collect Stats` and implicitly takes its processed output as input. It generates transformed features as its output. Its container image is `gcr.io/ross-kubeflow/feature-eng:latest`.\n3.  **`Split Train Test Val`**: This component splits the data into training, testing, and validation sets. It takes `pitch_type` as an argument and implicitly uses the output from `Feature Engineering` as its input. Its container image is `gcr.io/ross-kubeflow/train-test-val:latest`.\n4.  **`Tune Hyperparameters`**: This component tunes the hyperparameters of a machine learning model. It takes `pitch_type` as an argument and depends on `Split Train Test Val`. Its container image is `gcr.io/ross-kubeflow/tune-hp:latest`.\n5.  **`Train XGBoost`**: This component trains an XGBoost model. It takes `pitch_type` as an argument and depends on `Tune Hyperparameters`. Its container image is `gcr.io/ross-kubeflow/train-xgboost:latest`.\n6.  **`Host Model`**: This component hosts the trained XGBoost model. It takes `pitch_type` as an argument and depends on `Train XGBoost`. Its container image is `gcr.io/ross-kubeflow/host-xgboost:latest`.\n7.  **`Find Threshold`**: This component finds a suitable threshold for the model's predictions. It takes `pitch_type` as an argument and depends on `Host Model`. Its container image is `gcr.io/ross-kubeflow/find-threshold:latest`.\n8.  **`Evaluate Models`**: This component evaluates the performance of the model. It takes `pitch_type` as an argument and has an additional `dummy1` parameter which is not explicitly used in the arguments. It depends on `Find Threshold`. This component produces a file output named `data` from the path `/root/dummy.txt`. Its container image is `gcr.io/ross-kubeflow/evaluate-model:latest`.\n\n**Control Flow:**\n\nThe pipeline executes in a strictly sequential manner, with each step depending on the successful completion of the previous one. All components use `gcp.use_gcp_secret('user-gcp-sa')` for authentication.\n\nThe sequence of execution is as follows:\n\n- `Collect Stats` runs first.\n- `Feature Engineering` runs after `Collect Stats`.\n- `Split Train Test Val` runs after `Feature Engineering`.\n- `Tune Hyperparameters` runs after `Split Train Test Val`.\n- `Train XGBoost` runs after `Tune Hyperparameters`.\n- `Host Model` runs after `Train XGBoost`.\n- `Find Threshold` runs after `Host Model`.\n- `Evaluate Models` runs after `Find Threshold`.\n\nThe pipeline is parameterized by a `pitch_type` string input. The example provided shows a hardcoded `pitch_type` of `'FT'` for the relevant components.\n\nNo specific external libraries like sklearn or Snowflake are explicitly mentioned in the component definitions themselves, but the component names suggest machine learning tasks, implying the use of such libraries within the container images. The pipeline itself uses `kfp` and `kfp.dsl` for definition and `kfp.gcp` for GCP-specific configurations."
  },
  {
    "repo": "references_kfp_files",
    "file": "getindata_kedro-kubeflow_tests_test_one_pod_pipeline_generator.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `OnePodKedroPipeline` that performs a single operation.\n\nThis pipeline consists of **one component**:\n\n1.  **`pipeline` component**: This component encapsulates a Kedro pipeline execution.\n    *   **Function**: It executes a pre-defined Kedro pipeline, likely using `kedro run`.\n    *   **Inputs**:\n        *   `param1`: A float parameter with a default value of `0.3`.\n        *   `param2`: An integer parameter with a default value of `42`.\n        *   `param3`: A date parameter with a default value of `2022-02-24`.\n    *   **Configuration**:\n        *   The container image for this component is `unittest-image`.\n        *   The image pull policy is `Never`.\n        *   It injects pipeline parameters (`param1`, `param2`, `param3`) as arguments to the underlying Kedro command.\n\n**Control Flow**: The pipeline executes the `pipeline` component directly. There are no explicit dependencies or parallel operations within the Kubeflow DSL as it's designed to run a single, monolithic Kedro pipeline within one pod.\n\n**Tools/Libraries**: This pipeline leverages `kedro-kubeflow` to wrap a Kedro pipeline, indicating that the core logic is implemented in Kedro. It uses `kfp.dsl` for pipeline definition."
  },
  {
    "repo": "references_kfp_files",
    "file": "muhammadyaseen_kubeflow-on-linode_kfp-examples_01_markdown-visualization-pipeline_md_pipeline.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `md-pipeline` that performs the following:\n\nThe pipeline consists of four distinct components, each focused on generating and visualizing Markdown content:\n\n1.  **`write_simple_markdown_table`**:\n    *   **Function:** Generates a simple Markdown table string and writes it to an output Markdown artifact.\n    *   **Outputs:** `markdown_artifact` (type: `Markdown`).\n    *   **Tools/Libraries:** Standard Python file I/O.\n\n2.  **`write_simple_markdown_heading`**:\n    *   **Function:** Generates a Markdown heading and some basic text, then writes it to an output Markdown artifact.\n    *   **Outputs:** `markdown_artifact` (type: `Markdown`).\n    *   **Tools/Libraries:** Standard Python file I/O.\n\n3.  **`vertex_ai_markdown_example`**:\n    *   **Function:** Creates a more elaborate Markdown table string, simulating a Vertex AI visualization example, and writes it to an output Markdown artifact.\n    *   **Outputs:** `md_artifact` (type: `Markdown`).\n    *   **Tools/Libraries:** Standard Python file I/O.\n\n4.  **`write_pandas_dataframe_as_markdown`**:\n    *   **Function:** Creates a Pandas DataFrame, converts it into a Markdown table string using `df.to_markdown()`, and writes this string to an output Markdown artifact.\n    *   **Outputs:** `df_as_md` (type: `Markdown`).\n    *   **Tools/Libraries:** `pandas` (explicitly installed via `packages_to_install`).\n\n**Control Flow:**\nAll four components (`write_simple_markdown_table`, `write_simple_markdown_heading`, `vertex_ai_markdown_example`, and `write_pandas_dataframe_as_markdown`) run in parallel with no explicit dependencies on each other. They are instantiated directly within the pipeline definition without any `.after()` calls or data passing between them."
  },
  {
    "repo": "references_kfp_files",
    "file": "s102401002_kubeflowPipeline0722_src_diabetes_prediction.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `DiabetesPredictionPipeline` that performs a machine learning workflow for diabetes prediction.\n\nThe pipeline consists of **three** components:\n\n1.  **`load_data`**:\n    *   **Functionality**: This component is responsible for downloading raw diabetes datasets from specified URLs, consolidating them, cleaning the data by dropping rows with \"No Info\" for diabetes, dropping rows with insufficient valid data, and transforming categorical features (gender) and handling missing numerical values by imputation (mean).\n    *   **Inputs**: None.\n    *   **Outputs**:\n        *   `data_output`: An `Artifact` representing the preprocessed DataFrame, saved as a CSV file.\n    *   **Tools/Libraries**: Uses `pandas` (version 2.2.2) for data manipulation.\n    *   **Base Image**: `python:3.9`.\n\n2.  **`prepare_data`**:\n    *   **Functionality**: This component takes the preprocessed data, separates features (X) and target (Y), and then splits the data into training, testing, and validation sets.\n    *   **Inputs**:\n        *   `data_input`: An `Input[Artifact]` representing the preprocessed DataFrame from the `load_data` component.\n    *   **Outputs**:\n        *   `X_train_output`: An `Artifact` containing the training features.\n        *   `X_test_output`: An `Artifact` containing the testing features.\n        *   `Y_train_output`: An `Artifact` containing the training labels.\n        *   `Y_test_output`: An `Artifact` containing the testing labels.\n        *   `X_val_output`: An `Artifact` containing the validation features.\n        *   `Y_val_output`: An `Artifact` containing the validation labels.\n    *   **Tools/Libraries**: Uses `pandas` (version 2.2.2) and `scikit-learn` (version 1.5.1) for data splitting.\n    *   **Base Image**: `python:3.9`.\n\n3.  **`train_model`**:\n    *   **Functionality**: This component trains a machine learning model (e.g., RandomForestClassifier) using the training data, evaluates its performance on the validation set, and saves the trained model.\n    *   **Inputs**:\n        *   `X_train`: An `Input[Artifact]` containing training features.\n        *   `Y_train`: An `Input[Artifact]` containing training labels.\n        *   `X_val`: An `Input[Artifact]` containing validation features.\n        *   `Y_val`: An `Input[Artifact]` containing validation labels.\n    *   **Outputs**:\n        *   `model_output`: An `Output[Model]` representing the trained machine learning model.\n        *   `accuracy_output`: An `Output[Artifact]` containing the accuracy score.\n    *   **Tools/Libraries**: Uses `pandas` (version 2.2.2) and `scikit-learn` (version 1.5.1) for model training and evaluation.\n    *   **Base Image**: `python:3.9`.\n\n**Control Flow**:\n\n*   `load_data` runs first.\n*   `prepare_data` runs *after* `load_data` and uses its output (`data_output`) as input (`data_input`).\n*   `train_model` runs *after* `prepare_data` and uses its outputs (`X_train_output`, `Y_train_output`, `X_val_output`, `Y_val_output`) as inputs.\n\nThe pipeline should be defined using the `@dsl.pipeline` decorator from `kfp.dsl`. The components should use `@dsl.component` decorators, specifying `base_image` and `packages_to_install` as indicated above. Type hints for inputs and outputs (e.g., `Input[Artifact]`, `Output[Model]`) should be correctly applied."
  },
  {
    "repo": "references_kfp_files",
    "file": "s102401002_kubeflowPipeline_kubeflowPipeline_xgboost.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `xgboost_pipeline` that performs the following machine learning workflow:\n\nThe pipeline consists of three components: `load_data`, `prepare_data`, and `train_xgboost_model`.\n\n1.  **`load_data` Component:**\n    *   **Function:** Downloads and preprocesses diabetes-related data from two specified URLs, combines them, cleans the data by dropping 'No Info' entries, renaming columns to a standard format, handling missing values by replacing 'No Info' with the mean for numerical columns, and mapping 'gender' categories to numerical values.\n    *   **Inputs:** None explicitly defined as parameters.\n    *   **Outputs:**\n        *   `data_output`: An `Artifact` containing the processed DataFrame as a CSV file.\n    *   **Tools/Libraries:** `pandas` (version 2.2.2).\n    *   **Base Image:** `python:3.9`.\n\n2.  **`prepare_data` Component:**\n    *   **Function:** Splits the preprocessed data into training and testing sets for features (X) and target (y).\n    *   **Inputs:**\n        *   `data_input`: An `Input[Artifact]` containing the preprocessed data from the `load_data` component.\n    *   **Outputs:**\n        *   `x_train_output`: An `Output[Artifact]` for the training features (X_train) as a CSV file.\n        *   `x_test_output`: An `Output[Artifact]` for the testing features (X_test) as a CSV file.\n        *   `y_train_output`: An `Output[Artifact]` for the training target (y_train) as a CSV file.\n        *   `y_test_output`: An `Output[Artifact]` for the testing target (y_test) as a CSV file.\n    *   **Tools/Libraries:** `pandas` (version 2.2.2), `scikit-learn` (version 1.5.1).\n    *   **Base Image:** `python:3.9`.\n\n3.  **`train_xgboost_model` Component:**\n    *   **Function:** (Implicit, as the provided code snippet is truncated) This component is expected to train an XGBoost model using the training data and evaluate it with the test data.\n    *   **Inputs:**\n        *   `x_train_input`: An `Input[Artifact]` for X_train.\n        *   `x_test_input`: An `Input[Artifact]` for X_test.\n        *   `y_train_input`: An `Input[Artifact]` for y_train.\n        *   `y_test_input`: An `Input[Artifact]` for y_test.\n    *   **Outputs:** (Implicit) Likely a trained model artifact or evaluation metrics.\n    *   **Tools/Libraries:** `pandas`, `xgboost`, `scikit-learn`.\n    *   **Base Image:** `python:3.9`.\n\n**Control Flow:**\n*   `load_data` runs first.\n*   `prepare_data` depends on the `data_output` from `load_data`.\n*   `train_xgboost_model` depends on all four outputs (`x_train_output`, `x_test_output`, `y_train_output`, `y_test_output`) from `prepare_data`.\n\nThe pipeline should be defined using the `@dsl.pipeline` decorator and the components using `@dsl.component`. Type hints (`Input`, `Output`, `Artifact`) should be used for component inputs and outputs."
  },
  {
    "repo": "references_kfp_files",
    "file": "redhat-ai-services_kubeflow-pipelines-examples_pipelines_05_metrics_pipeline.py",
    "structured_prompt": "Generate a Kubeflow Pipeline named `metrics pipeline` that performs the following:\n\nThe pipeline consists of a single component:\n1.  **`produce_metrics`**:\n    *   **Function**: This component is responsible for generating and saving evaluation metrics in a format understood by Kubeflow Pipelines. It calculates dummy `accuracy` and `mse` values and then writes them to a JSON file specified by `mlpipeline_metrics_path`.\n    *   **Inputs**:\n        *   `mlpipeline_metrics_path`: An `dsl.OutputPath(\"Metrics\")` representing the file path where the metrics JSON should be written.\n    *   **Outputs**: It produces a metrics file accessible via `mlpipeline_metrics_path`.\n    *   **Libraries/Tools Used Internally**: `json` for serializing the metrics dictionary.\n    *   **Base Image**: `image-registry.openshift-image-registry.svc:5000/openshift/python:latest`\n\n**Control Flow**:\nThe `metrics pipeline` simply executes the `produce_metrics` component. There are no dependencies or complex control flow constructs (e.g., `after`, `parallelFor`) in this pipeline, as it's a single-step process."
  }
]