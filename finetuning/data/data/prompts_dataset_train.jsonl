{"prompt": "Generate a Kubeflow Pipeline named `upload_model_pipeline` that performs the following:\n\nThis pipeline consists of **one** component: `upload_model`.\n\n**Component 1: `upload_model`**\n*   **Function:** Uploads a scikit-learn model to Google Cloud AI Platform.\n*   **Inputs:**\n    *   `model`: An `Input[Model]` representing the model artifact to be uploaded.\n*   **Outputs:** None explicitly defined, as it performs an action (model upload).\n*   **Base Image:** `gcr.io/deeplearning-platform-release/tf2-cpu.2-6:latest`\n*   **Packages to Install:** `google-cloud-aiplatform`\n*   **Internal Logic:**\n    *   Initializes the `google.cloud.aiplatform` client with `project=\"gsd-ai-mx-ferneutron\"` and `location=\"us-central1\"`.\n    *   Uses `aiplatform.Model.upload_scikit_learn_model_file` to upload the model.\n    *   The `model_file_path` is taken from `model.path`.\n    *   The `display_name` for the uploaded model is `\"IrisModelv3\"`.\n    *   The `project` for the upload is `\"gsd-ai-mx-ferneutron\"`.\n*   **Tools/Libraries Used:** `google-cloud-aiplatform`.\n\n**Control Flow:**\nThe pipeline executes the `upload_model` component. There are no dependencies or complex control flow constructs (like `after`, `parallelFor`, or conditional execution) as it's a single-component pipeline.", "completion": "from kfp.dsl import Input, Model, component\n\n\n@component(\n    base_image=\"gcr.io/deeplearning-platform-release/tf2-cpu.2-6:latest\",\n    packages_to_install=[\"google-cloud-aiplatform\"],\n)\ndef upload_model(\n    model: Input[Model],\n):\n    from pathlib import Path\n\n    from google.cloud import aiplatform\n\n    aiplatform.init(project=\"gsd-ai-mx-ferneutron\", location=\"us-central1\")\n\n    aiplatform.Model.upload_scikit_learn_model_file(\n        model_file_path=model.path,\n        display_name=\"IrisModelv3\",\n        project=\"gsd-ai-mx-ferneutron\",\n    )"}
{"prompt": "Generate a Kubeflow Pipeline named `data_passing_pipeline` that performs a comprehensive demonstration of data passing mechanisms in KFP.\n\nThe pipeline consists of 15 components, categorized into \"Produce\" and \"Consume\" functions, showcasing various input/output types and consumption methods.\n\n**Pipeline Parameters:**\n* `anything_param`: Default value \"anything_param\"\n* `something_param`: Type \"Something\", Default value \"something_param\"\n* `string_param`: Type `str`, Default value \"string_param\"\n\n**Components and their functions:**\n\n**Produce Components (4):**\n1.  **`produce_anything`**:\n    *   **Function**: Writes the string \"produce_anything\" to an output file.\n    *   **Outputs**: `data_path` (untyped output path).\n2.  **`produce_something`**:\n    *   **Function**: Writes the string \"produce_something\" to an output file.\n    *   **Outputs**: `data_path` (output path of type \"Something\").\n3.  **`produce_something2`**:\n    *   **Function**: Returns the string \"produce_something2\" directly.\n    *   **Outputs**: Untyped string output (implicitly type \"Something\" due to annotation).\n4.  **`produce_string`**:\n    *   **Function**: Returns the string \"produce_string\" directly.\n    *   **Outputs**: String output of type `str`.\n\n**Consume Components - As Value (3):**\n1.  **`consume_anything_as_value`**:\n    *   **Function**: Prints the input data directly.\n    *   **Inputs**: `data` (untyped).\n2.  **`consume_something_as_value`**:\n    *   **Function**: Prints the input data directly.\n    *   **Inputs**: `data` (of type \"Something\").\n3.  **`consume_string_as_value`**:\n    *   **Function**: Prints the input data directly.\n    *   **Inputs**: `data` (of type `str`).\n\n**Consume Components - As File (3):**\n1.  **`consume_anything_as_file`**:\n    *   **Function**: Reads and prints the content of the input file.\n    *   **Inputs**: `data_path` (untyped input path).\n2.  **`consume_something_as_file`**:\n    *   **Function**: Reads and prints the content of the input file.\n    *   **Inputs**: `data_path` (input path of type \"Something\").\n3.  **`consume_string_as_file`**:\n    *   **Function**: Reads and prints the content of the input file.\n    *   **Inputs**: `data_path` (input path of type `str`).\n\n**Control Flow and Data Passing:**\n\nThe pipeline demonstrates 6 end-to-end data passing scenarios, covering constant values, pipeline parameters, and upstream component outputs, consumed both as values and as files.\n\n1.  **Passing constant value consumed as value:**\n    *   A task `consume_anything_as_value_from_const` is created by calling `consume_anything_as_value` with the string \"constant_value\".\n2.  **Passing constant value consumed as file:**\n    *   A task `consume_anything_as_file_from_const` is created by calling `consume_anything_as_file` with the string \"constant_value\".\n3.  **Passing pipeline parameter consumed as value:**\n    *   A task `consume_anything_as_value_from_param` is created by calling `consume_anything_as_value` with the `anything_param` pipeline parameter.\n    *   A task `consume_something_as_value_from_param` is created by calling `consume_something_as_value` with the `something_param` pipeline parameter.\n    *   A task `consume_string_as_value_from_param` is created by calling `consume_string_as_value` with the `string_param` pipeline parameter.\n4.  **Passing pipeline parameter consumed as file:**\n    *   A task `consume_anything_as_file_from_param` is created by calling `consume_anything_as_file` with the `anything_param` pipeline parameter.\n    *   A task `consume_something_as_file_from_param` is created by calling `consume_something_as_file` with the `something_param` pipeline parameter.\n    *   A task `consume_string_as_file_from_param` is created by calling `consume_string_as_file` with the `string_param` pipeline parameter.\n5.  **Passing task output consumed as value:**\n    *   A task `consume_anything_as_value_from_produce_anything` is created by calling `consume_anything_as_value` with the output of `produce_anything`.\n    *   A task `consume_something_as_value_from_produce_something` is created by calling `consume_something_as_value` with the output of `produce_something`.\n    *   A task `consume_something_as_value_from_produce_something2` is created by calling `consume_something_as_value` with the output of `produce_something2`.\n    *   A task `consume_string_as_value_from_produce_string` is created by calling `consume_string_as_value` with the output of `produce_string`.\n6.  **Passing task output consumed as file:**\n    *   A task `consume_anything_as_file_from_produce_anything` is created by calling `consume_anything_as_file` with the output of `produce_anything`.\n    *   A task `consume_something_as_file_from_produce_something` is created by calling `consume_something_as_file` with the output of `produce_something`.\n    *   A task `consume_string_as_file_from_produce_string` is created by calling `consume_string_as_file` with the output of `produce_string`.\n\n**Libraries/Tools Used:**\n*   `kfp.deprecated` for Kubeflow Pipelines definition.\n*   `kfp.deprecated.components` for creating components from Python functions.", "completion": "# This pipeline demonstrates and verifies all ways of data passing supported by KFP.\n\n# %% [markdown]\n# KFP has simple data passing model. There are three different kinds of arguments and tw different ways to consume an input argument. All combinations are supported.\n\n# Any input can be consumed:\n# * As value (`inputValue` placeholder)\n# * As file (`inputPath` placeholder).\n\n# Input argument can come from:\n# * Constant value\n# * Pipeline parameter\n# * Upstream component output\n\n# Combining these options there are 6 end-to-end data passing cases, each of which works regardless of type:\n\n# 1. Pass constant value which gets consumed as value. (Common)\n# 2. Pass constant value which gets consumed as file. (Often used in test pipelines)\n# 3. Pass pipeline parameter that gets consumed as value. (Common)\n# 4. Pass pipeline parameter that gets consumed as file. (Rare. Sometimes used with JSON parameters)\n# 5. Pass task output that gets consumed as value. (Common)\n# 6. Pass task output that gets consumed as file. (Common)\n\n# The only restriction on types is that when both upstream output and downstream input have types, the types must match.\n\n# %%\nimport kfp.deprecated as kfp\nfrom kfp.deprecated.components import create_component_from_func, InputPath, OutputPath\n\n\n# Components\n# Produce\n@create_component_from_func\ndef produce_anything(data_path: OutputPath()):\n    with open(data_path, \"w\") as f:\n        f.write(\"produce_anything\")\n\n\n@create_component_from_func\ndef produce_something(data_path: OutputPath(\"Something\")):\n    with open(data_path, \"w\") as f:\n        f.write(\"produce_something\")\n\n\n@create_component_from_func\ndef produce_something2() -> 'Something':\n    return \"produce_something2\"\n\n\n@create_component_from_func\ndef produce_string() -> str:\n    return \"produce_string\"\n\n\n# Consume as value\n@create_component_from_func\ndef consume_anything_as_value(data):\n    print(\"consume_anything_as_value: \" + data)\n\n\n@create_component_from_func\ndef consume_something_as_value(data: \"Something\"):\n    print(\"consume_something_as_value: \" + data)\n\n\n@create_component_from_func\ndef consume_string_as_value(data: str):\n    print(\"consume_string_as_value: \" + data)\n\n\n# Consume as file\n@create_component_from_func\ndef consume_anything_as_file(data_path: InputPath()):\n    with open(data_path) as f:\n        print(\"consume_anything_as_file: \" + f.read())\n\n\n@create_component_from_func\ndef consume_something_as_file(data_path: InputPath('Something')):\n    with open(data_path) as f:\n        print(\"consume_something_as_file: \" + f.read())\n\n\n@create_component_from_func\ndef consume_string_as_file(data_path: InputPath(str)):\n    with open(data_path) as f:\n        print(\"consume_string_as_file: \" + f.read())\n\n\n# Pipeline\n@kfp.dsl.pipeline(name='data_passing_pipeline')\ndef data_passing_pipeline(\n    anything_param=\"anything_param\",\n    something_param: \"Something\" = \"something_param\",\n    string_param: str = \"string_param\",\n):\n    produced_anything = produce_anything().output\n    produced_something = produce_something().output\n    produced_string = produce_string().output\n\n    # Pass constant value; consume as value\n    consume_anything_as_value(\"constant\")\n    consume_something_as_value(\"constant\")\n    consume_string_as_value(\"constant\")\n\n    # Pass constant value; consume as file\n    consume_anything_as_file(\"constant\")\n    consume_something_as_file(\"constant\")\n    consume_string_as_file(\"constant\")\n\n    # Pass pipeline parameter; consume as value\n    consume_anything_as_value(anything_param)\n    consume_anything_as_value(something_param)\n    consume_anything_as_value(string_param)\n    consume_something_as_value(anything_param)\n    consume_something_as_value(something_param)\n    consume_string_as_value(anything_param)\n    consume_string_as_value(string_param)\n\n    # Pass pipeline parameter; consume as file\n    consume_anything_as_file(anything_param)\n    consume_anything_as_file(something_param)\n    consume_anything_as_file(string_param)\n    consume_something_as_file(anything_param)\n    consume_something_as_file(something_param)\n    consume_string_as_file(anything_param)\n    consume_string_as_file(string_param)\n\n    # Pass task output; consume as value\n    consume_anything_as_value(produced_anything)\n    consume_anything_as_value(produced_something)\n    consume_anything_as_value(produced_string)\n    consume_something_as_value(produced_anything)\n    consume_something_as_value(produced_something)\n    consume_string_as_value(produced_anything)\n    consume_string_as_value(produced_string)\n\n    # Pass task output; consume as file\n    consume_anything_as_file(produced_anything)\n    consume_anything_as_file(produced_something)\n    consume_anything_as_file(produced_string)\n    consume_something_as_file(produced_anything)\n    consume_something_as_file(produced_something)\n    consume_string_as_file(produced_anything)\n    consume_string_as_file(produced_string)\n\n\nif __name__ == \"__main__\":\n    kfp_endpoint = None\n    kfp.Client(host=kfp_endpoint).create_run_from_pipeline_func(\n        data_passing_pipeline, arguments={})"}
{"prompt": "Generate a Kubeflow Pipeline named `fail-pipeline` that performs the following steps:\n\nThis pipeline consists of a single component.\n\nThe pipeline defines one component called `fail_op`, which is created from the Python function `fail`.\n- **`fail` function**: This function simulates a failure by exiting the Python process with an exit code of 1 (`sys.exit(1)`). Its purpose is to demonstrate a failing operation within a pipeline.\n- **Base Image**: The `fail_op` component uses the `alpine:latest` Docker image as its base.\n\n**Control Flow**:\n- The pipeline simply executes the `fail_op` component as `fail_task`. There are no dependencies or complex control flow structures. The pipeline is designed to always fail when this task is executed.\n\n**Tools/Libraries**:\n- The pipeline primarily uses the `kfp.deprecated` library for defining components and the pipeline structure.\n- The `sys` module is used within the `fail` function for exiting the process.", "completion": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Fail pipeline.\"\"\"\n\nfrom kfp.deprecated import components, dsl\n\n\ndef fail():\n    '''Fails'''\n    import sys\n    sys.exit(1)\n\n\nfail_op = components.create_component_from_func(\n    fail, base_image='alpine:latest')\n\n\n@dsl.pipeline(name='fail-pipeline')\ndef fail_pipeline():\n    fail_task = fail_op()"}
{"prompt": "Generate a Kubeflow Pipeline named `metrics_visualization_pipeline` that performs the following steps:\n\nThe pipeline consists of five components:\n1.  **`wine-classification`**: This component is responsible for performing wine classification.\n2.  **`iris-sgdclassifier`**: This component handles iris classification using an SGD Classifier.\n3.  **`digit-classification`**: This component performs digit classification.\n4.  **`html-visualization`**: This component generates an HTML visualization.\n5.  **`markdown-visualization`**: This component generates a Markdown visualization.\n\n**Control Flow and Dependencies:**\n\n*   The `html-visualization` component depends on the successful completion of `wine-classification`, `iris-sgdclassifier`, and `digit-classification`. It takes outputs (likely metrics or results) from these three classification components as inputs to generate the HTML visualization.\n*   Similarly, the `markdown-visualization` component also depends on the successful completion of `wine-classification`, `iris-sgdclassifier`, and `digit-classification`. It uses outputs from these classification components to generate the Markdown visualization.\n\n**Inputs and Outputs (where visible):**\n\n*   **`wine-classification`**:\n    *   **Outputs**: An artifact named `metrics` with specific metadata, including `display_name: 'metrics'` and `confidenceMetrics`. The `confidenceMetrics` contain a list of dictionaries, each with `confidenceThreshold`, `falsePositiveRate`, and `recall` values. These metrics suggest the component is outputting performance evaluation results for a classification model.\n*   **`iris-sgdclassifier`**:\n    *   **Outputs**: Similar to `wine-classification`, it outputs an artifact named `metrics` with `display_name: 'metrics'` and `confidenceMetrics` containing a list of dictionaries with `confidenceThreshold`, `falsePositiveRate`, and `recall`.\n*   **`digit-classification`**:\n    *   **Outputs**: Also outputs an artifact named `metrics` with `display_name: 'metrics'` and `confidenceMetrics` containing a list of dictionaries with `confidenceThreshold`, `falsePositiveRate`, and `recall`.\n*   **`html-visualization`**:\n    *   **Inputs**: Takes the `metrics` outputs from `wine-classification`, `iris-sgdclassifier`, and `digit-classification`.\n*   **`markdown-visualization`**:\n    *   **Inputs**: Takes the `metrics` outputs from `wine-classification`, `iris-sgdclassifier`, and `digit-classification`.\n\nThe pipeline aims to perform multiple classification tasks and then visualize their performance metrics using both HTML and Markdown formats.", "completion": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import annotations\nimport unittest\nimport unittest.mock as mock\nimport kfp.deprecated as kfp\nimport kfp_server_api\n\nfrom .metrics_visualization_v2 import metrics_visualization_pipeline\nfrom kfp.samples.test.utils import KfpTask, run_pipeline_func, TestCase\nfrom ml_metadata.proto import Execution\n\n\ndef verify(t: unittest.TestCase, run: kfp_server_api.ApiRun,\n           tasks: dict[str, KfpTask], **kwargs):\n    t.assertEqual(run.status, 'Succeeded')\n    task_names = [*tasks.keys()]\n    t.assertCountEqual(task_names, [\n        'wine-classification', 'iris-sgdclassifier', 'digit-classification',\n        'html-visualization', 'markdown-visualization'\n    ], 'task names')\n\n    wine_classification = tasks['wine-classification']\n    iris_sgdclassifier = tasks['iris-sgdclassifier']\n    digit_classification = tasks['digit-classification']\n    html_visualization = tasks['html-visualization']\n    markdown_visualization = tasks['markdown-visualization']\n\n    t.assertEqual(\n        {\n            'name': 'wine-classification',\n            'inputs': {},\n            'outputs': {\n                'artifacts': [{\n                    'metadata': {\n                        'display_name': 'metrics',\n                        'confidenceMetrics': {\n                            'list': [{\n                                'confidenceThreshold': 2.0,\n                                'falsePositiveRate': 0.0,\n                                'recall': 0.0\n                            }, {\n                                'confidenceThreshold': 1.0,\n                                'falsePositiveRate': 0.0,\n                                'recall': 0.33962264150943394\n                            }, {\n                                'confidenceThreshold': 0.9,\n                                'falsePositiveRate': 0.0,\n                                'recall': 0.6037735849056604\n                            }, {\n                                'confidenceThreshold': 0.8,\n                                'falsePositiveRate': 0.0,\n                                'recall': 0.8490566037735849\n                            }, {\n                                'confidenceThreshold': 0.6,\n                                'falsePositiveRate': 0.0,\n                                'recall': 0.8867924528301887\n                            }, {\n                                'confidenceThreshold': 0.5,\n                                'falsePositiveRate': 0.0125,\n                                'recall': 0.9245283018867925\n                            }, {\n                                'confidenceThreshold': 0.4,\n                                'falsePositiveRate': 0.075,\n                                'recall': 0.9622641509433962\n                            }, {\n                                'confidenceThreshold': 0.3,\n                                'falsePositiveRate': 0.0875,\n                                'recall': 1.0\n                            }, {\n                                'confidenceThreshold': 0.2,\n                                'falsePositiveRate': 0.2375,\n                                'recall': 1.0\n                            }, {\n                                'confidenceThreshold': 0.1,\n                                'falsePositiveRate': 0.475,\n                                'recall': 1.0\n                            }, {\n                                'confidenceThreshold': 0.0,\n                                'falsePositiveRate': 1.0,\n                                'recall': 1.0\n                            }]\n                        }\n                    },\n                    'name': 'metrics',\n                    'type': 'system.ClassificationMetrics'\n                }],\n            },\n            'type': 'system.ContainerExecution',\n            'state': Execution.State.COMPLETE,\n        }, wine_classification.get_dict())\n    t.assertEqual(\n        {\n            'inputs': {\n                'parameters': {\n                    'test_samples_fraction': 0.3\n                }\n            },\n            'name': 'iris-sgdclassifier',\n            'outputs': {\n                'artifacts': [{\n                    'metadata': {\n                        'display_name': 'metrics',\n                        'confusionMatrix': {\n                            'struct': {\n                                'annotationSpecs': [{\n                                    'displayName': 'Setosa'\n                                }, {\n                                    'displayName': 'Versicolour'\n                                }, {\n                                    'displayName': 'Virginica'\n                                }],\n                                'rows': [\n                                    {  # these numbers can be random during execution\n                                        'row': [mock.ANY, mock.ANY, mock.ANY]\n                                    },\n                                    {\n                                        'row': [mock.ANY, mock.ANY, mock.ANY]\n                                    },\n                                    {\n                                        'row': [mock.ANY, mock.ANY, mock.ANY]\n                                    }\n                                ]\n                            }\n                        }\n                    },\n                    'name': 'metrics',\n                    'type': 'system.ClassificationMetrics'\n                }],\n            },\n            'type': 'system.ContainerExecution',\n            'state': Execution.State.COMPLETE,\n        },\n        iris_sgdclassifier.get_dict())\n    rows = iris_sgdclassifier.get_dict()['outputs']['artifacts'][0]['metadata'][\n        'confusionMatrix']['struct']['rows']\n    for i, row in enumerate(rows):\n        for j, item in enumerate(row['row']):\n            t.assertIsInstance(\n                item, float,\n                f'value of confusion matrix row {i}, col {j} is not a number')\n\n    t.assertEqual(\n        {\n            'name': 'digit-classification',\n            'inputs': {},\n            'outputs': {\n                'artifacts': [{\n                    'metadata': {\n                        'display_name': 'metrics',\n                        'accuracy': 92.0,\n                    },\n                    'name': 'metrics',\n                    'type': 'system.Metrics'\n                }],\n            },\n            'type': 'system.ContainerExecution',\n            'state': Execution.State.COMPLETE,\n        }, digit_classification.get_dict())\n\n    t.assertEqual(\n        {\n            'name': 'html-visualization',\n            'inputs': {},\n            'outputs': {\n                'artifacts': [{\n                    'metadata': {\n                        'display_name': 'html_artifact'\n                    },\n                    'name': 'html_artifact',\n                    'type': 'system.HTML'\n                }],\n            },\n            'state': Execution.State.COMPLETE,\n            'type': 'system.ContainerExecution'\n        }, html_visualization.get_dict())\n\n    t.assertEqual(\n        {\n            'name': 'markdown-visualization',\n            'inputs': {},\n            'outputs': {\n                'artifacts': [{\n                    'metadata': {\n                        'display_name': 'markdown_artifact'\n                    },\n                    'name': 'markdown_artifact',\n                    'type': 'system.Markdown'\n                }],\n            },\n            'state': Execution.State.COMPLETE,\n            'type': 'system.ContainerExecution'\n        }, markdown_visualization.get_dict())\n\n\nrun_pipeline_func([\n    TestCase(\n        pipeline_func=metrics_visualization_pipeline,\n        verify_func=verify,\n        mode=kfp.dsl.PipelineExecutionMode.V2_ENGINE),\n])"}
{"prompt": "Generate a Kubeflow Pipeline named `test_metrics_visualization_v2` that performs machine learning tasks and visualizes metrics.\n\nThe pipeline consists of two components:\n1.  **`digit_classification`**: This component trains a Logistic Regression model on the Iris dataset, evaluates its accuracy using cross-validation and a test set, and logs the final accuracy as a metric.\n    *   **Functionality**: Loads the Iris dataset, performs data splitting and cross-validation, trains a `LogisticRegression` model, calculates and logs the accuracy metric.\n    *   **Libraries Used**: `sklearn` (specifically `model_selection`, `linear_model`, `datasets`, `metrics`).\n    *   **Outputs**: `metrics` (of type `kfp.dsl.Metrics`), which contains the logged accuracy.\n\n2.  **`wine_classification`**: This component trains a RandomForestClassifier on the Wine dataset, focusing on a binary classification problem for a specific label, and generates ROC curve data.\n    *   **Functionality**: Loads the Wine dataset, transforms the target for binary classification, splits data, trains a `RandomForestClassifier`, and generates predicted probabilities and classes for ROC curve calculation.\n    *   **Libraries Used**: `sklearn` (specifically `ensemble`, `metrics`, `datasets`, `model_selection`).\n    *   **Outputs**: `metrics` (of type `kfp.dsl.ClassificationMetrics`), intended to visualize classification-specific metrics like ROC curve data. Note that the provided code snippet is truncated and only shows the `fpr`, `tpr`, and `thresholds` calculation.\n\n**Control Flow**:\nThe pipeline runs the `digit_classification` and `wine_classification` components in parallel, as there are no explicit dependencies defined between them. Each component runs independently and outputs its respective metrics.", "completion": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport os\n\nfrom kfp import dsl\nfrom kfp.dsl import (\n    component,\n    Output,\n    ClassificationMetrics,\n    Metrics,\n    HTML,\n    Markdown\n)\n\n# In tests, we install a KFP package from the PR under test. Users should not\n# normally need to specify `kfp_package_path` in their component definitions.\n_KFP_PACKAGE_PATH = os.getenv('KFP_PACKAGE_PATH')\n\n\n@component(\n    packages_to_install=['sklearn'],\n    base_image='python:3.9',\n    kfp_package_path=_KFP_PACKAGE_PATH,\n)\ndef digit_classification(metrics: Output[Metrics]):\n    from sklearn import model_selection\n    from sklearn.linear_model import LogisticRegression\n    from sklearn import datasets\n    from sklearn.metrics import accuracy_score\n\n    # Load digits dataset\n    iris = datasets.load_iris()\n\n    # # Create feature matrix\n    X = iris.data\n\n    # Create target vector\n    y = iris.target\n\n    #test size\n    test_size = 0.33\n\n    seed = 7\n    #cross-validation settings\n    kfold = model_selection.KFold(n_splits=10, random_state=seed, shuffle=True)\n\n    #Model instance\n    model = LogisticRegression()\n    scoring = 'accuracy'\n    results = model_selection.cross_val_score(model, X, y, cv=kfold, scoring=scoring)\n\n    #split data\n    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=test_size, random_state=seed)\n    #fit model\n    model.fit(X_train, y_train)\n\n    #accuracy on test set\n    result = model.score(X_test, y_test)\n    metrics.log_metric('accuracy', (result*100.0))\n\n@component(\n    packages_to_install=['sklearn'],\n    base_image='python:3.9',\n    kfp_package_path=_KFP_PACKAGE_PATH,\n)\ndef wine_classification(metrics: Output[ClassificationMetrics]):\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.metrics import roc_curve\n    from sklearn.datasets import load_wine\n    from sklearn.model_selection import train_test_split, cross_val_predict\n\n    X, y = load_wine(return_X_y=True)\n    # Binary classification problem for label 1.\n    y = y == 1\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n    rfc = RandomForestClassifier(n_estimators=10, random_state=42)\n    rfc.fit(X_train, y_train)\n    y_scores = cross_val_predict(rfc, X_train, y_train, cv=3, method='predict_proba')\n    y_predict = cross_val_predict(rfc, X_train, y_train, cv=3, method='predict')\n    fpr, tpr, thresholds = roc_curve(y_true=y_train, y_score=y_scores[:,1], pos_label=True)\n    metrics.log_roc_curve(fpr, tpr, thresholds)\n\n@component(\n    packages_to_install=['sklearn'],\n    base_image='python:3.9',\n    kfp_package_path=_KFP_PACKAGE_PATH,\n)\ndef iris_sgdclassifier(test_samples_fraction: float, metrics: Output[ClassificationMetrics]):\n    from sklearn import datasets, model_selection\n    from sklearn.linear_model import SGDClassifier\n    from sklearn.metrics import confusion_matrix\n\n    iris_dataset = datasets.load_iris()\n    train_x, test_x, train_y, test_y = model_selection.train_test_split(\n        iris_dataset['data'], iris_dataset['target'], test_size=test_samples_fraction)\n\n\n    classifier = SGDClassifier()\n    classifier.fit(train_x, train_y)\n    predictions = model_selection.cross_val_predict(classifier, train_x, train_y, cv=3)\n    metrics.log_confusion_matrix(\n        ['Setosa', 'Versicolour', 'Virginica'],\n        confusion_matrix(train_y, predictions).tolist() # .tolist() to convert np array to list.\n    )\n\n@component(\n    kfp_package_path=_KFP_PACKAGE_PATH,\n)\ndef html_visualization(html_artifact: Output[HTML]):\n    html_content = '<!DOCTYPE html><html><body><h1>Hello world</h1></body></html>'\n    with open(html_artifact.path, 'w') as f:\n        f.write(html_content)\n\n@component(\n    kfp_package_path=_KFP_PACKAGE_PATH,\n)\ndef markdown_visualization(markdown_artifact: Output[Markdown]):\n    markdown_content = '## Hello world \\n\\n Markdown content'\n    with open(markdown_artifact.path, 'w') as f:\n        f.write(markdown_content)\n\n@dsl.pipeline(\n    name='metrics-visualization-pipeline')\ndef metrics_visualization_pipeline():\n    wine_classification_op = wine_classification()\n    iris_sgdclassifier_op = iris_sgdclassifier(test_samples_fraction=0.3)\n    digit_classification_op = digit_classification()\n    html_visualization_op = html_visualization()\n    markdown_visualization_op = markdown_visualization()"}
{"prompt": "Generate a Kubeflow Pipeline named `First Pipeline` that performs classification using Decision Tree and Logistic Regression.\n\nThis pipeline consists of **four** components:\n\n1.  **`download_data`**: This component is loaded from the `download_data/download_data.yaml` file. Its function is to download the necessary dataset for the classification problem. It has **no explicit inputs** and produces **one output** (likely the path or reference to the downloaded data).\n2.  **`decision_tree`**: This component is loaded from the `decision_tree/decision_tree.yaml` file. Its function is to train and evaluate a Decision Tree model. It takes **one input**, which is the output from the `download_data` component (the downloaded data). It produces **one output**, representing the accuracy or performance metric of the Decision Tree model (a float).\n3.  **`logistic_regression`**: This component is loaded from the `logistic_regression/logistic_regression.yaml` file. Its function is to train and evaluate a Logistic Regression model. It takes **one input**, which is also the output from the `download_data` component (the downloaded data). It produces **one output**, representing the accuracy or performance metric of the Logistic Regression model (a float).\n4.  **`show_results`**: This is a custom Python function component, defined using `@func_to_container_op`. Its function is to display the final results. It takes **two inputs**: `decision_tree` (a float representing the Decision Tree accuracy) and `logistic_regression` (a float representing the Logistic Regression accuracy). It has **no explicit outputs** but prints the accuracy scores to standard output.\n\nThe control flow of the pipeline is as follows:\n\n*   The `download_data` component runs first.\n*   Once `download_data` completes, its output is used as input for both the `decision_tree` and `logistic_regression` components, which run **in parallel**.\n*   After both `decision_tree` and `logistic_regression` components have completed, their respective outputs (accuracies) are passed as inputs to the `show_results` component, which runs last.\n\nThe pipeline utilizes the `kfp` library for defining and orchestrating the components. It loads component definitions from YAML files for the data download and model training steps, and uses a Python function for result display. No specific external machine learning libraries (like sklearn) or data platforms (like Snowflake) are explicitly mentioned within the pipeline definition itself, but they are likely used within the `.yaml` components.", "completion": "import kfp\nfrom kfp import dsl\nfrom kfp.components import func_to_container_op\n\n@func_to_container_op\ndef show_results(decision_tree : float, logistic_regression : float) -> None:\n    # Given the outputs from decision_tree and logistic regression components\n    # the results are shown.\n\n    print(f\"Decision tree (accuracy): {decision_tree}\")\n    print(f\"Logistic regression (accuracy): {logistic_regression}\")\n\n\n@dsl.pipeline(name='First Pipeline', description='Applies Decision Tree and Logistic Regression for classification problem.')\ndef first_pipeline():\n\n    # Loads the yaml manifest for each component\n    download = kfp.components.load_component_from_file('download_data/download_data.yaml')\n    decision_tree = kfp.components.load_component_from_file('decision_tree/decision_tree.yaml')\n    logistic_regression = kfp.components.load_component_from_file('logistic_regression/logistic_regression.yaml')\n\n    # Run download_data task\n    download_task = download()\n\n    # Run tasks \"decison_tree\" and \"logistic_regression\" given\n    # the output generated by \"download_task\".\n    decision_tree_task = decision_tree(download_task.output)\n    logistic_regression_task = logistic_regression(download_task.output)\n\n    # Given the outputs from \"decision_tree\" and \"logistic_regression\"\n    # the component \"show_results\" is called to print the results.\n    show_results(decision_tree_task.output, logistic_regression_task.output)\n\n\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(first_pipeline, 'FirstPipeline.yaml')\n    # kfp.Client().create_run_from_pipeline_func(basic_pipeline, arguments={})"}
{"prompt": "Generate a Kubeflow Pipeline named `parameter_value_missing` that performs the following:\n\nThe pipeline has **one** component:\n*   **`echo`**: This component takes a single string input named `text` and simply echoes it to standard output. It uses the `alpine` container image and the `echo` command.\n\nThe pipeline definition `pipeline` takes one input parameter:\n*   `parameter` (type: `str`): This parameter is intended to be provided at pipeline submission time.\n\nThe control flow is as follows:\n*   The `echo_op` component is called, with its `text` input directly mapped to the pipeline's `parameter` input.", "completion": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom kfp import dsl, components\n\necho = components.load_component_from_text(\n    \"\"\"\nname: Echo\ninputs:\n- {name: text, type: String}\nimplementation:\n  container:\n    image: alpine\n    command:\n    - echo\n    - {inputValue: text}\n\"\"\"\n)\n\n\n@dsl.pipeline(name='parameter_value_missing')\ndef pipeline(\n    parameter:\n    str  # parameter should be specified when submitting, but we are missing it in the test\n):\n    echo_op = echo(text=parameter)"}
{"prompt": "I need the content of `pipeline/pipeline.py` to generate the prompt. The provided `run_pipeline` function *uses* that file, but it doesn't *define* the Kubeflow Pipeline itself.\n\nPlease provide the content of `pipeline/pipeline.py`, which should contain the `@dsl.pipeline` and `@component` definitions. Once I have that, I can generate the complete and accurate prompt.", "completion": "import kfp_server_api\nimport kfp\nfrom kfp.compiler.compiler import Compiler\nimport os\nfrom importlib import invalidate_caches\n\n\ndef run_pipeline(\n        dataset_name: str = \"tweet_eval\",\n        dataset_subset: str = \"emotion\",\n        model_names: str = \"google/electra-small-discriminator\",\n) -> kfp_server_api.ApiRun:\n    \"\"\"Runs the pipeline\n\n    Args:\n        dataset_name: Name of the dataset\n        dataset_subset: Subset of the dataset\n        model_name: Name of the model\n\n    Returns:\n        None\n    \"\"\"\n\n    # Connect to KFP, this command is used to connect to the KFP UI:\n    # kubectl port-forward -n kubeflow svc/ml-pipeline-ui 8080:80\n    client = kfp.Client(host=\"http://127.0.0.1:8080/pipeline\")\n\n    models = model_names.split(\",\")\n    models = [f'\\\"{model}\\\"' for model in models]\n    model_names = \",\".join(models)\n\n    # ParallerFor takes static arguments, it is not possible to give it as pipeline parameter so we need to override the\n    # pipeline function code to pass the arguments.\n\n    # Open the pipeline code\n    with open(\"pipeline/pipeline.py\", \"r\") as f:\n        pipeline_yaml = f.read()\n\n    # Replace the template with the actual arguments\n    pipeline_yaml = pipeline_yaml.replace('\"?model_names?\"', model_names)\n\n    # Save the pipeline code\n    with open(\"pipeline/pipeline_modified.py\", \"w\") as f:\n        f.write(pipeline_yaml)\n\n    # Invalidate the cache to make sure the modified pipeline is loaded\n    invalidate_caches()\n\n    # Import the pipeline function\n    from pipeline.pipeline_modified import classification_training_pipeline\n\n    # Compile the pipeline\n    Compiler(mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE).compile(pipeline_func=classification_training_pipeline, package_path=\"pipeline.yaml\")\n\n    # Create an experiment\n    experiment = client.create_experiment(name=\"text-classification\", description=\"Text classification pipeline\")\n\n    # Run the pipeline\n    job = client.run_pipeline(\n        experiment_id=experiment.id,\n        job_name=\"text-classification\",\n        pipeline_package_path=\"pipeline.yaml\",\n        enable_caching=False,\n        params={\n            \"dataset_name\": dataset_name,\n            \"dataset_subset\": dataset_subset\n        },\n    )\n\n    # Delete the modified pipeline code\n    os.remove(\"pipeline/pipeline_modified.py\")\n\n    components = filter(lambda x: x.endswith(\".yaml\"), os.listdir())\n\n    for file in components:\n        os.remove(file)\n\n    return job"}
{"prompt": "Generate a Kubeflow Pipeline named `pipeline-2` that performs the following:\n\nThis pipeline currently contains no components and thus has no specific functionality or control flow defined. It serves as a basic skeleton for a Kubeflow pipeline.", "completion": "import sys\nsys.path.append(\"../..\")\n\nimport kfp\n\n@kfp.dsl.pipeline(name=\"pipeline-2\")\ndef pipeline():\n    pass"}
{"prompt": "Generate a Kubeflow Pipeline named `parameter_value_missing` that performs the following steps:\n\nThis pipeline consists of a single component.\n\n**Components:**\n\n1.  **`echo_op`**:\n    *   **Function:** This component is an `Echo` component. It takes a string input and prints it to standard output.\n    *   **Inputs:** It takes one input:\n        *   `text`: A string value.\n    *   **Dependencies:** This component depends on the `parameter` input provided to the pipeline.\n\n**Control Flow:**\n\nThe `echo_op` component directly uses the `parameter` input provided to the main pipeline function.\n\n**Inputs to the Pipeline:**\n\n*   `parameter`: A string. This parameter is expected to be provided when the pipeline is submitted.", "completion": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom kfp.deprecated import dsl, components\n\necho = components.load_component_from_text(\n    \"\"\"\nname: Echo\ninputs:\n- {name: text, type: String}\nimplementation:\n  container:\n    image: alpine\n    command:\n    - echo\n    - {inputValue: text}\n\"\"\"\n)\n\n\n@dsl.pipeline(name='parameter_value_missing')\ndef pipeline(\n    parameter:\n    str  # parameter should be specified when submitting, but we are missing it in the test\n):\n    echo_op = echo(text=parameter)"}
{"prompt": "Generate a Kubeflow Pipeline named `Env Vars Pipeline` that performs the following steps:\n\nThis pipeline consists of **two** parallel components:\n\n1.  **`print_envvar` (secret_print_task)**:\n    *   **Function:** This component takes an environment variable name (`env_var`) as input and prints its value.\n    *   **Inputs:** `env_var` (string)\n    *   **Outputs:** Prints the value of the specified environment variable to standard output.\n    *   **Configuration:**\n        *   The component is configured to **disable caching**.\n        *   It mounts a Kubernetes secret named `my-secret`.\n        *   Specifically, the key `my-secret-data` from `my-secret` is exposed as an environment variable named `my-secret-env-var` within the component's execution environment.\n        *   The `env_var` input to this component is set to `\"my-secret-env-var\"`.\n    *   **Base Image:** `image-registry.openshift-image-registry.svc:5000/openshift/python:latest`\n\n2.  **`print_envvar` (cm_print_task)**:\n    *   **Function:** This component also takes an environment variable name (`env_var`) as input and prints its value.\n    *   **Inputs:** `env_var` (string)\n    *   **Outputs:** Prints the value of the specified environment variable to standard output.\n    *   **Configuration:**\n        *   The component is configured to **disable caching**.\n        *   It mounts a Kubernetes ConfigMap named `my-configmap`.\n        *   Specifically, the key `my-configmap-data` from `my-configmap` is exposed as an environment variable named `my-cm-env-var` within the component's execution environment.\n        *   The `env_var` input to this component is set to `\"my-cm-env-var\"`.\n    *   **Base Image:** `image-registry.openshift-image-registry.svc:5000/openshift/python:latest`\n\n**Control Flow:**\nBoth `secret_print_task` and `cm_print_task` run in **parallel** as they do not have explicit dependencies on each other.\n\n**Tools/Libraries:**\nThe pipeline uses `kfp` for defining the pipeline and components, and `kubernetes` for interacting with Kubernetes resources like Secrets and ConfigMaps to expose them as environment variables. The `dotenv` library is used for loading environment variables locally for pipeline compilation and execution setup.", "completion": "\"\"\"Example of a pipeline to demonstrate accessing secrets/config maps in a pipeline.\"\"\"\n\nimport os\n\nimport kfp.compiler\nfrom dotenv import load_dotenv\nfrom kfp import dsl\nfrom kfp import kubernetes\n\nload_dotenv(override=True)\n\nkubeflow_endpoint = os.environ[\"KUBEFLOW_ENDPOINT\"]\nbearer_token = os.environ[\"BEARER_TOKEN\"]\n\n\n@dsl.component(base_image=\"image-registry.openshift-image-registry.svc:5000/openshift/python:latest\")\ndef print_envvar(env_var: str):\n    import os\n\n    var_value = os.environ[env_var]\n    print(f\"my var value: {var_value}\")\n\n\n@kfp.dsl.pipeline(\n    name=\"Env Vars Pipeline\",\n)\ndef env_vars_pipeline():\n    \"\"\"Pipeline to add values.\n\n    Pipeline to take the value of a, add 4 to it and then\n    perform a second task to take the put of the first task and add b.\n    \"\"\"\n    secret_print_task = print_envvar(env_var=\"my-secret-env-var\")\n    secret_print_task.set_caching_options(False)\n    kubernetes.use_secret_as_env(\n        secret_print_task, secret_name=\"my-secret\", secret_key_to_env={\"my-secret-data\": \"my-secret-env-var\"}\n    )\n\n    cm_print_task = print_envvar(env_var=\"my-cm-env-var\")\n    cm_print_task.set_caching_options(False)\n    kubernetes.use_config_map_as_env(\n        cm_print_task, config_map_name=\"my-configmap\", config_map_key_to_env={\"my-configmap-data\": \"my-cm-env-var\"}\n    )\n\n\nif __name__ == \"__main__\":\n    client = kfp.Client(\n        host=kubeflow_endpoint,\n        existing_token=bearer_token,\n    )\n    client.create_run_from_pipeline_func(env_vars_pipeline, arguments={}, experiment_name=\"secrets-configmap-example\")"}
{"prompt": "Generate a Kubeflow Pipeline named `SF_Crime_Prediction_Training_Pipeline` that performs the following steps:\n\nThe pipeline consists of **two components**:\n\n1.  **`get_train_test_split` Component:**\n    *   **Function:** This component reads a DataFrame, splits it into training and testing sets (X and Y), and selects the split that minimizes data divergence (using Kolmogorov-Smirnov statistic) across specified continuous columns over `n_tries` attempts.\n    *   **Inputs:**\n        *   `df_path`: An `Input[Dataset]` representing the input DataFrame.\n        *   `label_column`: A string indicating the name of the target variable column.\n        *   `test_size`: A float specifying the proportion of the dataset to include in the test split.\n        *   `n_tries`: An integer representing the number of random splits to attempt to find the best one.\n    *   **Outputs:**\n        *   `output_x_train`: An `Output[Dataset]` containing the features for the training set.\n        *   `output_x_test`: An `Output[Dataset]` containing the features for the test set.\n        *   `output_y_train`: An `Output[Artifact]` containing the labels for the training set (serialized using pickle).\n        *   `output_y_test`: An `Output[Artifact]` containing the labels for the test set (serialized using pickle).\n        *   `divergence_output_dict`: An `Output[Artifact]` containing a dictionary of Kolmogorov-Smirnov statistics for continuous columns from the best split (serialized using pickle).\n    *   **Libraries/Tools:** `numpy`, `scikit-learn` (specifically `train_test_split`), `pandas`, `scipy`.\n    *   **Base Image:** `python:3.9`\n    *   **Output Component File:** `new_yamls/split_train_test.yaml`\n\n2.  **`prepare_data_for_training` Component:**\n    *   **Function:** This component is intended to perform data preparation steps for training, such as handling different data types (float, categorical, binary columns). *(Note: The provided code snippet for this component is incomplete, so its exact full functionality cannot be fully described. However, based on its name and input types, it processes the split datasets.)*\n    *   **Inputs:**\n        *   `x_train_input_path`: An `Input[Dataset]` representing the training features.\n        *   `x_test_input_path`: An `Input[Dataset]` representing the test features.\n        *   `float_cols`: A list of strings identifying float columns.\n        *   `cat_cols`: A list of strings identifying categorical columns.\n        *   `binary_cols`: A list of strings identifying binary columns.\n        *   *(Implicit/Inferred Outputs: Given the name, it's likely to output processed versions of x_train and x_test.)*\n    *   **Libraries/Tools:** `scikit-learn`, `pandas`, `numpy`.\n    *   **Base Image:** `python:3.9`\n    *   **Output Component File:** `new_yamls/prep_data_for_training.yaml`\n\n**Control Flow:**\n\n*   The `prepare_data_for_training` component should execute *after* `get_train_test_split` has successfully completed.\n*   The `prepare_data_for_training` component will use `x_train_input_path` and `x_test_input_path` as inputs, which should be the `output_x_train` and `output_x_test` from the `get_train_test_split` component, respectively.\n\n**General Notes:**\n\n*   The pipeline uses Kubeflow Pipelines V2 DSL (`kfp.v2.dsl`).\n*   The components specify `base_image` and `packages_to_install`.\n*   The pipeline integrates with custom `variables` and `common_fns` modules (though their content is not provided, their presence indicates potential shared configurations or utilities).\n*   The pipeline uses standard Python libraries for data manipulation and machine learning.", "completion": "import argparse\nfrom variables import *\nfrom common_fns import *\nfrom datetime import datetime, timedelta\nimport pytz\nimport kfp\nfrom kfp.dsl.structures import yaml\nfrom kfp.client import Client\nimport os\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import component\nfrom kfp.v2.dsl import (\n    Input,\n    Output,\n    Artifact,\n    Dataset,\n    Model,\n    Metrics\n)\n\n\n@component(\n    base_image='python:3.9',\n    packages_to_install=['numpy', 'scikit-learn', 'pandas', 'scipy'],\n    output_component_file='new_yamls/split_train_test.yaml')\ndef get_train_test_split(df_path: Input[Dataset], label_column: str, test_size: float, n_tries: int,\n                        output_x_train: Output[Dataset], output_x_test: Output[Dataset],\n                        output_y_train: Output[Artifact], output_y_test: Output[Artifact],\n                        divergence_output_dict: Output[Artifact]):\n    import numpy as np\n    from sklearn.model_selection import train_test_split\n    import pickle\n    import pandas as pd\n    import scipy\n\n    def get_kolmogorov_smiron(train_df, test_df, col_name):\n        train_df[col_name] = train_df.groupby('analysis_neighborhood', group_keys = False)[col_name].apply(lambda x: x.fillna(x.mean()))\n        test_df[col_name] = test_df.groupby('analysis_neighborhood', group_keys = False)[col_name].apply(lambda x: x.fillna(x.mean()))\n        statistic = scipy.stats.ks_2samp(train_df[col_name], test_df[col_name]).statistic\n        return statistic\n\n    df = pd.read_csv(df_path.path)\n    Y = df.pop(label_column)\n    cont_cols = ['diff', 'till_game_time', 'latitude', 'longitude']\n    results = []\n    for random_state in range(n_tries):\n        x_train, x_test, y_train, y_test = train_test_split(df, Y, test_size = test_size, random_state = random_state)\n        distances = dict(map(lambda a: (a, get_kolmogorov_smiron(x_train, x_test, a)), cont_cols))\n        results.append((random_state, distances))\n\n    ks_best = min(results, key=lambda x: np.mean(list(x[1].values())))\n    best_seed = ks_best[0]\n    ks_dict = ks_best[1]\n    x_train, x_test, y_train, y_test = train_test_split(df, Y, test_size = test_size, random_state = best_seed)\n\n    y_train = np.array(y_train)\n    y_test = np.array(y_test)\n    x_train.to_csv(output_x_train.path, index=False)\n    x_test.to_csv(output_x_test.path, index=False)\n    with open(output_y_train.path, 'wb') as file:\n        pickle.dump(y_train, file)\n    with open(output_y_test.path, 'wb') as file:\n        pickle.dump(y_test, file)\n    with open(divergence_output_dict.path, 'wb') as file:\n        pickle.dump(ks_dict, file)\n\n\n@component(\n    base_image='python:3.9',\n    packages_to_install=['scikit-learn', 'pandas', 'numpy'],\n    output_component_file='new_yamls/prep_data_for_training.yaml')\ndef prepare_data_for_training(x_train_input_path: Input[Dataset], x_test_input_path: Input[Dataset],\n                              float_cols: list, cat_cols: list, binary_cols: list,\n                              x_train_output_path: Output[Dataset], x_test_output_path: Output[Dataset],\n                              cat_label_en_dict_out: Output[Artifact]):\n\n    import pandas as pd\n    import pickle\n    import numpy as np\n    from sklearn.preprocessing import LabelEncoder\n\n    req_columns = ['incident_id', 'report_datetime', 'incident_datetime', 'latitude',\n                   'longitude', 'datetime', 'incident_date', 'year',\n                   'incident_time', 'gameday', 'gametime', 'data_type']\n    cat_lab_encoder_dict = {}\n    x_train = pd.read_csv(x_train_input_path.path)\n    x_test = pd.read_csv(x_test_input_path.path)\n\n    X_TRAIN = pd.DataFrame(x_train[binary_cols + float_cols + req_columns])\n    X_TEST = pd.DataFrame(x_test[binary_cols + float_cols + req_columns])\n\n\n    for col in cat_cols:\n        x_train[col] = x_train[col].astype(str)\n        x_test[col] = x_test[col].astype(str)\n        label_enc = LabelEncoder()\n        unique_vals = np.append(x_train[col].unique(), 'UNK')\n        x_test.loc[~x_test[col].isin(unique_vals), col] = 'UNK'\n        label_enc.fit(unique_vals)\n        X_TRAIN[col] = label_enc.transform(x_train[col])\n        X_TEST[col] = label_enc.transform(x_test[col])\n        cat_lab_encoder_dict[col] = label_enc\n\n    with open(cat_label_en_dict_out.path, 'wb') as file:\n        pickle.dump(cat_lab_encoder_dict, file)\n\n    X_TRAIN.to_csv(x_train_output_path.path, index=False)\n    X_TEST.to_csv(x_test_output_path.path, index=False)\n\n@component(\n    base_image='python:3.9',\n    packages_to_install=['xgboost', 'pandas', 'scikit-learn', 'numpy', 'google-cloud-storage'],\n    output_component_file='new_yamls/training.yaml')\ndef train_model(x_train_input_path: Input[Dataset], y_train_input_path: Input[Artifact],\n                y_test_input_path: Input[Artifact], y_test_output_path: Output[Artifact],\n                y_train_output_path: Output[Artifact], model_output: Output[Artifact],\n                label_encoder_path: Output[Artifact], model_dict_path: Output[Artifact],\n                output_bucket: str, output_blob_name: str, projectid: str):\n    from xgboost import XGBClassifier\n    import pandas as pd\n    import pickle\n    from sklearn.preprocessing import LabelEncoder\n    import numpy as np\n    from datetime import datetime\n    from google.cloud import storage\n\n    model_dict = {}\n    req_columns = ['incident_id', 'report_datetime', 'incident_datetime', 'latitude',\n                   'longitude', 'datetime', 'incident_date', 'year',\n                   'incident_time', 'gameday', 'gametime', 'data_type']\n    now = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    client = storage.Client(project=projectid)\n    bucket = client.get_bucket(output_bucket)\n    output_blob_name = f'models/{output_blob_name}_{now}'\n    blob = bucket.blob(f'{output_blob_name}/model.pkl')\n    gcs_folder_path = f'gs://{output_bucket}/{output_blob_name}'\n\n    x_train = pd.read_csv(x_train_input_path.path)\n    with open(y_train_input_path.path, 'rb') as file:\n        y_train = pickle.load(file)\n\n    with open(y_test_input_path.path, 'rb') as file:\n        y_test = pickle.load(file)\n\n    label_enc_target = LabelEncoder()\n    y_train = np.asarray(label_enc_target.fit_transform(y_train))\n    y_test = np.asarray(label_enc_target.transform(y_test))\n\n    with open(y_train_output_path.path, 'wb') as file:\n        pickle.dump(y_train, file)\n\n    with open(y_test_output_path.path, 'wb') as file:\n        pickle.dump(y_test, file)\n\n    with open(label_encoder_path.path, 'wb') as file:\n        pickle.dump(label_enc_target, file)\n\n    model = XGBClassifier(learning_rate = 0.03, tree_method='hist', n_estimators = 3,\n                          objective='multi:softmax')\n    clf = model.fit(x_train.drop(req_columns, axis = 1), y_train)\n\n    model_dict['create_time'] = now\n    model_dict['uri'] = gcs_folder_path\n\n    clf.save_model(model_output.path)\n\n    with open(model_dict_path.path, 'wb') as file:\n        pickle.dump(model_dict, file)\n\n    blob.upload_from_filename(model_output.path)\n\n@component(\n    base_image='python:3.9',\n    packages_to_install=['xgboost', 'pandas', 'scikit-learn', 'numpy'],\n    output_component_file='new_yamls/predict.yaml')\n    # base_image='python:3.10.2')\ndef predict(x_train_input_path: Input[Dataset], x_test_input_path: Input[Dataset],\n            y_train_input_path: Input[Dataset], y_test_input_path: Input[Artifact],\n            model_path: Input[Artifact], float_cols: list, bq_data_path: Output[Dataset],\n            eval_metrics: Output[Artifact], stats_dict_output: Output[Artifact]):\n    import pandas as pd\n    import pickle\n    import numpy as np\n    from xgboost import XGBClassifier\n\n    model = XGBClassifier()\n    model.load_model(model_path.path)\n    req_columns = ['incident_id', 'report_datetime', 'incident_datetime', 'latitude',\n                   'longitude', 'datetime', 'incident_date', 'year',\n                   'incident_time', 'gameday', 'gametime', 'data_type']\n\n    metrics = {}\n    x_train = pd.read_csv(x_train_input_path.path)\n    x_test = pd.read_csv(x_test_input_path.path)\n    with open(y_train_input_path.path, 'rb') as file:\n        y_train = pickle.load(file)\n    with open(y_test_input_path.path, 'rb') as file:\n        y_test = pickle.load(file)\n\n    y_pred_train = model.predict(x_train.drop(req_columns, axis = 1))\n    y_pred_test = model.predict(x_test.drop(req_columns, axis = 1))\n\n    stats_dict = x_train[float_cols].describe().to_dict()\n\n    x_train['incident_category'] = y_train\n    x_train['Prediction'] = y_pred_train\n    x_test['incident_category'] = y_test\n    x_test['Prediction'] = y_pred_test\n\n    train_accuracy = np.mean(y_pred_train == y_train)\n    test_accuracy = np.mean(y_pred_test == y_test)\n    metrics['train_accuracy'] = train_accuracy\n    metrics['test_accuracy'] = test_accuracy\n\n    bq_data = pd.concat([x_train, x_test], axis = 0)\n    bq_data = bq_data.query('data_type == \"ingest\"')\n    if bq_data.shape[0] > 0:\n        bq_data['data_type'] = 'non_ingest'\n    bq_data.to_csv(bq_data_path.path, index=False)\n    with open(eval_metrics.path, 'wb') as file:\n        pickle.dump(metrics, file)\n    with open(stats_dict_output.path, 'wb') as file:\n        pickle.dump(stats_dict, file)\n\n@component(\n    base_image='python:3.9',\n    packages_to_install= ['google-cloud-aiplatform', 'xgboost', 'scikit-learn',\n                          'google-cloud-storage', 'pandas', 'pyarrow'],\n    output_component_file= 'new_yamls/upload_model_registry.yaml')\n    # base_image='python:3.10.2')\ndef upload_model_registry(train_df_path: Input[Artifact], model_path: Input[Artifact], eval_metrics: Input[Artifact],\n                          ks_dict_path: Input[Artifact], model_dict_path: Input[Artifact], projectid: str,\n                          region: str, accuracy_threshold:float, model_name: str, bucket_name: str,\n                          stats_dict_input:Input[Artifact], eval_output: Output[Metrics],\n                          output_train_name: str):\n    from google.cloud import aiplatform, storage\n    import pickle\n    import os\n    from xgboost import XGBClassifier\n    import pandas as pd\n    aiplatform.init(project=projectid, location=region)\n\n    with open(eval_metrics.path, 'rb') as file:\n        accuracy = pickle.load(file)\n        test_accuracy = accuracy['test_accuracy']\n        train_accuracy = accuracy['train_accuracy']\n\n    eval_output.log_metric('train accuracy', train_accuracy)\n    eval_output.log_metric('test accuracy', test_accuracy)\n    with open(model_dict_path.path, 'rb') as file:\n        model_dict = pickle.load(file)\n\n    train_df = pd.read_csv(train_df_path.path)\n    train_df.to_parquet(output_train_name, compression='gzip')\n    client = storage.Client(project=projectid)\n    uri = model_dict['uri']\n    print (uri)\n    uri_folder = '/'.join(uri.split('/')[-2:])\n    print ('uri folder: ', uri_folder)\n    bucket = client.get_bucket(bucket_name)\n    train_df_blob = bucket.blob(f'{uri_folder}/{output_train_name}')\n    drift_blob = bucket.blob(f'{uri_folder}/data_drift_dict.pkl')\n    metrics_blob = bucket.blob(f'{uri_folder}/metrics_dict.pkl')\n    stats_dict_blob = bucket.blob(f'{uri_folder}/stats_dict.pkl')\n    model_param_blob = bucket.blob(f'{uri_folder}/model_params.pkl')\n    container_image = 'us-docker.pkg.dev/vertex-ai/prediction/xgboost-cpu.1-7:latest'\n\n    if test_accuracy > accuracy_threshold:\n        print ('Model Performance satisfactory: Saving')\n        model = XGBClassifier()\n        model.load_model(model_path.path)\n        params = model.get_params()\n        present_models = aiplatform.Model.list(order_by= 'update_time', location= region,)\n        print (present_models)\n        if len(present_models) > 0:\n            previous_model = present_models[-1].resource_name\n            uploaded_model = aiplatform.Model.upload(\n                                    display_name = model_name,\n                                    artifact_uri = uri,\n                                    serving_container_image_uri= container_image,\n                                    parent_model = previous_model,\n                                    is_default_version = True)\n        else:\n            uploaded_model = aiplatform.Model.upload(\n                                    display_name = model_name,\n                                    artifact_uri = uri,\n                                    serving_container_image_uri= container_image)\n        with open('model_params.pkl', 'wb') as file:\n            pickle.dump(params, file)\n        drift_blob.upload_from_filename(ks_dict_path.path)\n        metrics_blob.upload_from_filename(eval_metrics.path)\n        stats_dict_blob.upload_from_filename(stats_dict_input.path)\n        model_param_blob.upload_from_filename('model_params.pkl')\n        train_df_blob.upload_from_filename(output_train_name)\n\n\n    else:\n        print ('Model Performance not satisfactory: Not saving')\n\n@component(\n    base_image='python:3.9',\n    packages_to_install= ['google-cloud-bigquery'],\n    output_component_file= 'new_yamls/delete_bq_data.yaml'\n)\ndef delete_bq_data(df: Input[Dataset], projectid: str, datasetid: str, tableid: str):\n    from google.cloud import bigquery as bq\n\n    client = bq.Client(project=projectid)\n    q = f'DELETE from {datasetid}.{tableid} where data_type = \"ingest\"'\n    job = client.query(q)\n    job.result()\n\n@dsl.pipeline(\n    name='SF Training Pipeline',\n    description= 'Postprocessing data, Training the model and uploading to registry')\n\ndef sf_training_pipeline(projectid:str = projectid, datasetid:str = datasetid, tablename:str = tablename,\n                        label_name:str = label_name, test_size:float =test_size, best_split_tries:int =best_split_tries,\n                        float_cols:list = float_cols, cat_cols:list = cat_cols, binary_cols:list = binary_cols,\n                        bucket_name:str = bucket_name, accuracy_threshold:float = accuracy_threshold,\n                        model_name:str = model_name):\n    get_data_task = read_training_data(url = '', method='training', projectid=projectid, datasetid=datasetid,\n                                       tablename=tablename)\n    split_data_task = get_train_test_split(df_path = get_data_task.outputs['output_csv'], label_column = label_name,\n                                           test_size = test_size, n_tries=best_split_tries)\n    post_proc_train_task = post_processing_after_split(input_df_path= split_data_task.outputs['output_x_train'])\n\n    post_proc_test_task = post_processing_after_split(input_df_path= split_data_task.outputs['output_x_test'])\n\n    data_prep_for_train_task = prepare_data_for_training(x_train_input_path= post_proc_train_task.outputs['output_df_path'],\n                              x_test_input_path= post_proc_test_task.outputs['output_df_path'],\n                              float_cols= float_cols,\n                              cat_cols= cat_cols,\n                              binary_cols= binary_cols)\n\n    upload_cat_lab_enc_task = upload_artifact(artifact_path= data_prep_for_train_task.outputs['cat_label_en_dict_out'],\n                                              projectid= projectid, bucket_name= bucket_name,\n                                              output_path= 'cat_lab_encoder.pkl')\n\n    train_task = train_model(x_train_input_path= data_prep_for_train_task.outputs['x_train_output_path'],\n                             y_train_input_path= split_data_task.outputs['output_y_train'],\n                             y_test_input_path = split_data_task.outputs['output_y_test'],\n                             output_bucket= bucket_name, output_blob_name= 'models',\n                             projectid= projectid)\n\n    upload_target_label_enc = upload_artifact(artifact_path= train_task.outputs['label_encoder_path'],\n                                              projectid= projectid, bucket_name= bucket_name,\n                                              output_path= 'target_lab_encoder.pkl')\n\n    predict_task = predict(x_train_input_path= data_prep_for_train_task.outputs['x_train_output_path'],\n                           y_train_input_path= train_task.outputs['y_train_output_path'],\n                           model_path= train_task.outputs['model_output'],\n                           x_test_input_path= data_prep_for_train_task.outputs['x_test_output_path'],\n                           y_test_input_path= train_task.outputs['y_test_output_path'],\n                           float_cols = float_cols)\n\n    categ_decode = decode_cols(df_path= predict_task.outputs['bq_data_path'], projectid='ml-deployments',\n                               bucket_name= bucket_name, cat_enc_path= 'cat_lab_encoder.pkl',\n                               label_enc_path= 'target_lab_encoder.pkl', target_col = label_name,\n                               cols_to_decode= cat_cols)\n\n    upload_to_registry_task = upload_model_registry(train_df_path= data_prep_for_train_task.outputs['x_train_output_path'],\n                          model_path= train_task.outputs['model_output'], \n                          stats_dict_input = predict_task.outputs['stats_dict_output'],\n                          eval_metrics= predict_task.outputs['eval_metrics'],\n                          model_dict_path= train_task.outputs['model_dict_path'], projectid= projectid,\n                          region= 'us-central1', accuracy_threshold= accuracy_threshold, model_name= model_name,\n                          ks_dict_path= split_data_task.outputs['divergence_output_dict'],\n                          bucket_name=bucket_name, output_train_name= 'train_df.parquet.gzip')\n\n    delete_duplicate_task= delete_bq_data(df = categ_decode.outputs['output_df'], projectid= projectid, \n                                          datasetid= datasetid, tableid= tablename)\n\n    upload_task = upload_to_bq(df_path = categ_decode.outputs['output_df'], projectid= projectid,\n                                datasetid= datasetid, tableid= tablename, location='US',\n                                schema = schema)\n\n    \nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='Kubeflow pipeline to run ingestion script for SF Crime data.')\n    parser.add_argument('type', help='Enter \"yaml\" if file needs to be saved or \"run\" if pipline needs to be run directly',\n                       type=str)\n    parser.add_argument('--test_size', type=float, help='Value for size of the test set, default=0.3')\n    parser.add_argument('--best_split_tries', type=int, help='Number of attempts at splitting the data to get the most similar distributions between train and test, default=5')\n    parser.add_argument('--accuracy_threshold', type=float, help='Minimum test set accuracy required to push the new model to registry, default=0.3')\n    args = parser.parse_args()\n    if args.type == 'yaml':\n        if not os.path.exists('new_yamls'):\n            os.mkdir('new_yamls')\n        kfp.compiler.Compiler().compile(\n            pipeline_func=sf_training_pipeline,\n            package_path='new_yamls/training_pipeline.yaml')\n    elif args.type == 'run':\n        print ('Here')\n        args = {key: value for (key,value) in vars(args).items() if (value != None and key != 'type')}\n        print (args)\n        client = Client(host= host_url)\n        experiments = client.list_experiments().experiments\n        experiment_names = list(map(lambda x: x.display_name, experiments))\n        if training_experiment_name not in experiment_names:\n            client.create_experiment(name=training_experiment_name)\n        client.create_run_from_pipeline_func(\n        pipeline_func=sf_training_pipeline,\n        experiment_name=training_experiment_name,\n        arguments=args)"}
{"prompt": "Generate a Kubeflow Pipeline named `add_pipeline` that performs a series of additions.\n\nThis pipeline consists of two components.\n\n**Component 1: `add`**\n- **Function:** Calculates the sum of two floating-point numbers.\n- **Inputs:**\n    - `a`: A float.\n    - `b`: A float.\n- **Output:** A float representing the sum.\n- **Base Image:** `image-registry.openshift-image-registry.svc:5000/openshift/python:latest`\n\n**Pipeline `add_pipeline` Details:**\n- **Description:** Takes an initial value `a`, adds 4 to it, and then takes the output of that first addition and adds a second value `b`.\n- **Inputs:**\n    - `a`: A float with a default value of `1.0`.\n    - `b`: A float with a default value of `7.0`.\n\n**Control Flow:**\n1. **`first_add_task`:** This task calls the `add` component.\n    - It takes the pipeline input `a` as its first argument.\n    - It uses a literal value `4.0` as its second argument `b`.\n2. **`second_add_task`:** This task also calls the `add` component.\n    - It takes the *output* of `first_add_task` as its first argument `a`.\n    - It takes the pipeline input `b` as its second argument.\n\nNo external tools or libraries beyond `kfp` for pipeline definition are explicitly used within the component logic itself.", "completion": "\"\"\"Example of a pipeline built from inline functions with kfp and compiled to yaml.\"\"\"\n\nimport kfp.compiler\nfrom kfp import dsl\n\n\n@dsl.component(base_image=\"image-registry.openshift-image-registry.svc:5000/openshift/python:latest\")\ndef add(a: float, b: float) -> float:\n    \"\"\"Calculate the sum of the two arguments.\"\"\"\n    return a + b\n\n\n@dsl.pipeline()\ndef add_pipeline(a: float = 1.0, b: float = 7.0):\n    \"\"\"Pipeline to add values.\n\n    Pipeline to take the value of a, add 4 to it and then\n    perform a second task to take the put of the first task and add b.\n    \"\"\"\n    first_add_task = add(a=a, b=4.0)\n    second_add_task = add(a=first_add_task.output, b=b)  # noqa: F841\n\n\nif __name__ == \"__main__\":\n    kfp.compiler.Compiler().compile(add_pipeline, package_path=__file__.replace(\".py\", \".yaml\"))"}
{"prompt": "You provided Python code that defines helper functions `processing_op` and `http_download_op` to create `ContainerOp` instances. However, this code **does not define a Kubeflow Pipeline** using `@dsl.pipeline` or any `@component` decorators. It only provides building blocks.\n\nTherefore, I cannot generate a prompt that describes a Kubeflow Pipeline from the given code. The prompt structure I would use requires a complete pipeline definition.\n\nIf you can provide a Python file that defines a Kubeflow Pipeline using `@dsl.pipeline` and `@component` decorators, I would be happy to generate the appropriate prompt.", "completion": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef processing_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('GOLANG_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=[script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )"}
{"prompt": "Generate a Kubeflow Pipeline named `SpamDetectorPipeline` that performs a machine learning workflow to classify SMS messages as spam or ham.\n\nThe pipeline consists of **six** components, each with a specific function:\n\n1.  **`extract_data`**:\n    *   **Function**: Downloads the SMS Spam Collection dataset from a UCI archive URL, extracts the data from a zip file, and saves it as a CSV file.\n    *   **Outputs**: A `dsl.OutputPath(str)` representing the path to the extracted CSV file (e.g., `/mnt/data/smsspamcollection.csv`).\n    *   **Tools/Libraries**: `requests`, `zipfile`, `io`, `pandas`.\n\n2.  **`preprocess_data`**:\n    *   **Function**: Reads the raw SMS data, adds two new features: `length` (length of the message) and `punct` (count of punctuation marks in the message). Saves the preprocessed data to a new CSV file.\n    *   **Inputs**: A `dsl.InputPath(str)` pointing to the raw CSV file generated by `extract_data`.\n    *   **Tools/Libraries**: `pandas`, `string`.\n\n3.  **`eda` (Exploratory Data Analysis)**:\n    *   **Function**: Performs basic exploratory data analysis on the preprocessed data. This includes checking for missing values, unique labels, and the distribution of 'ham' and 'spam' messages. It also generates and saves histograms to visualize the 'length' and 'punct' features for both categories, providing insights into their distributions.\n    *   **Inputs**: Reads the preprocessed data from a specific path (e.g., `/mnt/data/preprocessed_smsspamcollection.csv`).\n    *   **Tools/Libraries**: `pandas`, `matplotlib.pyplot`, `numpy`.\n\n4.  **`train_model`**:\n    *   **Function**: Trains a machine learning model to classify spam messages. It reads the preprocessed data, splits it into training and testing sets, creates a scikit-learn pipeline using `TfidfVectorizer` for text feature extraction and `RandomForestClassifier` for classification. The trained model is then saved using `joblib`.\n    *   **Inputs**: Reads the preprocessed data from a specific path (e.g., `/mnt/data/preprocessed_smsspamcollection.csv`).\n    *   **Outputs**: A `dsl.OutputPath(str)` representing the path to the saved model file (e.g., `/mnt/data/spam_detector_model.joblib`).\n    *   **Tools/Libraries**: `pandas`, `sklearn.model_selection`, `sklearn.pipeline`, `sklearn.ensemble`, `sklearn.feature_extraction.text`, `joblib`.\n\n5.  **`evaluate_model`**:\n    *   **Function**: Evaluates the performance of the trained model. It loads the saved model, re-reads the preprocessed data, splits it into training and testing sets, makes predictions on the test set, and calculates various classification metrics (e.g., accuracy, confusion matrix, classification report). It also generates and saves a confusion matrix plot.\n    *   **Inputs**: A `dsl.InputPath(str)` pointing to the trained model file from `train_model`. It also reads the preprocessed data from a specific path (e.g., `/mnt/data/preprocessed_smsspamcollection.csv`).\n    *   **Tools/Libraries**: `pandas`, `sklearn.model_selection`, `sklearn.metrics`, `matplotlib.pyplot`, `joblib`.\n\n6.  **`prediction_example`**:\n    *   **Function**: Demonstrates the trained model's capability by making predictions on new, unseen SMS messages. It loads the trained model, defines example messages, and uses the model to predict whether each message is 'ham' or 'spam'.\n    *   **Inputs**: A `dsl.InputPath(str)` pointing to the trained model file from `train_model`.\n    *   **Tools/Libraries**: `pandas`, `sklearn.pipeline`, `sklearn.feature_extraction.text`, `joblib`, `typing.List`.\n\n**Control Flow:**\n\n*   `preprocess_data` depends on `extract_data` (its input `file_path` is the output of `extract_data`).\n*   `eda` depends on `preprocess_data` (it reads the output file from `preprocess_data`).\n*   `train_model` depends on `preprocess_data` (it reads the output file from `preprocess_data`).\n*   `evaluate_model` depends on `train_model` (its input is the output model file from `train_model`).\n*   `prediction_example` depends on `train_model` (its input is the output model file from `train_model`).\n*   `eda`, `train_model` can run in parallel after `preprocess_data` completes.\n*   `evaluate_model` and `prediction_example` can run in parallel after `train_model` completes.", "completion": "import kfp.dsl as dsl\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn import metrics\nimport string\nfrom typing import List\nimport joblib\n\n\n# Component 1: Extract data\n@dsl.component(\n    base_image='python:3.8',  # Specifying the base image\n    packages_to_install=['requests', 'pandas']\n)\ndef extract_data() -> dsl.OutputPath(str):\n    import requests\n    import zipfile\n    import io\n    import pandas as pd\n\n    url = 'https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip'\n\n    # Step 1: Download the zip file\n    response = requests.get(url)\n    response.raise_for_status()  # Check if the request was successful\n\n    # Step 2: Extract the contents of the zip file\n    with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n        with z.open('SMSSpamCollection') as f:\n            # Step 3: Read the contents into a DataFrame\n            df = pd.read_csv(f, sep='\\t', names=[\"label\", \"message\"], header=None)\n\n    # Step 4: Save the DataFrame to a CSV file\n    output_path = '/mnt/data/smsspamcollection.csv'\n    df.to_csv(output_path, index=False)\n\n    return output_path\n\n\n# Component 2: Data Preprocessing\n@dsl.component(\n    base_image='python:3.8',\n    packages_to_install=['pandas']\n)\ndef preprocess_data(file_path: dsl.InputPath(str)) -> None:\n    import pandas as pd\n    import string\n    \n    df = pd.read_csv(file_path)\n    \n    # Add 'length' and 'punct' features\n    df['length'] = df['message'].apply(len)\n    df['punct'] = df['message'].apply(lambda x: sum([1 for char in x if char in string.punctuation]))\n    \n    # Save the preprocessed data\n    df.to_csv('/mnt/data/preprocessed_smsspamcollection.csv', index=False)\n\n\n# Component 3: Exploratory Data Analysis (EDA)\n@dsl.component(\n    base_image='python:3.8',\n    packages_to_install=['pandas', 'matplotlib', 'numpy']\n)\ndef eda() -> None:\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    import numpy as np\n\n    df = pd.read_csv('/mnt/data/preprocessed_smsspamcollection.csv')\n\n    print('Missing values: ')\n    print(df.isnull().sum(),'\\n')\n\n    print('Categories: ',df['label'].unique(),'\\n')\n\n    print('Rate of each category: ')\n    print(df['label'].value_counts())\n\n    plt.xscale('log')\n    bins = 1.15**(np.arange(0,50))\n    plt.hist(df[df['label']=='ham']['length'], bins=bins,alpha=0.8)\n    plt.hist(df[df['label']=='spam']['length'], bins=bins,alpha=0.8)\n    plt.legend(('ham','spam'))\n    plt.title('Inference: usually spams are longer in text compared to ham')\n    plt.xlabel('Text length')\n    plt.ylabel('Category rate')\n    plt.savefig('/mnt/data/length_histogram.png')\n    plt.clf()\n\n    plt.xscale('log')\n    bins = 1.15**(np.arange(0,50))\n    plt.hist(df[df['label']=='ham']['punct'], bins=bins,alpha=0.8)\n    plt.hist(df[df['label']=='spam']['punct'], bins=bins,alpha=0.8)\n    plt.legend(('ham','spam'))\n    plt.title('Inference: a small tendency of spams towards more punctutations (not a firm inference)')\n    plt.xlabel('Text length')\n    plt.ylabel('Category rate')\n    plt.savefig('/mnt/data/punct_histogram.png')\n\n\n# Component 4: Train Model\n@dsl.component(\n    base_image='python:3.8',\n    packages_to_install=['pandas', 'scikit-learn', 'joblib']\n)\ndef train_model() -> None:\n    import pandas as pd\n    from sklearn.model_selection import train_test_split\n    from sklearn.pipeline import Pipeline\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.feature_extraction.text import TfidfVectorizer\n    from sklearn import metrics\n    import joblib\n\n    df = pd.read_csv('/mnt/data/preprocessed_smsspamcollection.csv')\n\n    X = df['message']\n    y = df['label']\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)\n\n    text_clf = Pipeline([('tfidf', TfidfVectorizer()), ('clf', RandomForestClassifier())])\n    text_clf.fit(X_train, y_train)\n\n    predictions = text_clf.predict(X_test)\n\n    df_conf_mat = pd.DataFrame(metrics.confusion_matrix(y_test, predictions), index=['ham', 'spam'], columns=['ham', 'spam'])\n    print(df_conf_mat, '\\n')\n\n    clf_report = metrics.classification_report(y_test, predictions)\n    print(clf_report, '\\n')\n\n    acc = metrics.accuracy_score(y_test, predictions)\n    print('Model accuracy: ', acc * 100)\n\n    # Save the model to a file\n    joblib.dump(text_clf, '/mnt/data/text_clf.joblib')\n\n\n# Component 5: Test Model\n@dsl.component(\n    base_image='python:3.8',\n    packages_to_install=['scikit-learn', 'joblib']\n)\ndef test_model( sample_messages: List[str]) -> List[str]:\n    import joblib\n    model = joblib.load('/mnt/data/text_clf.joblib')\n    predictions = model.predict(sample_messages)\n    return predictions\n\n\n# Define the pipeline\n@dsl.pipeline(\n    name='SMS Spam Detection Pipeline',\n    description='A pipeline for detecting spam messages from SMS data'\n)\ndef sms_spam_detection_pipeline():\n    # Step 1: Extract data\n    extracted_data = extract_data()\n    print(extracted_data.output)\n    # Step 2: Preprocess data\n    preprocess_data(\n        file_path=extracted_data.output,\n    )\n\n    # Step 3: Perform EDA\n    eda()\n\n    # Step 4: Train model\n    train_model()\n\n    # Step 5: Test model\n    test_samples = ['Hi, how you doing?', 'Congratulations! You have won a $1000 prize! Text 1 to 1423.']\n    test_output = test_model(sample_messages=test_samples)\n    print(test_output)\n\n# Compile the pipeline\nif __name__ == '__main__':\n    import kfp.compiler as compiler\n    compiler.Compiler().compile(sms_spam_detection_pipeline, 'sms_spam_detection_pipeline.yaml')"}
{"prompt": "Generate a Kubeflow Pipeline named `PVC Pipeline` that performs arithmetic operations and demonstrates mounting a Persistent Volume Claim (PVC) to a task.\n\nThe pipeline consists of **one** reusable component named `add`.\n\n**Component Details:**\n\n*   **`add` component:**\n    *   **Function:** Calculates the sum of two floating-point numbers.\n    *   **Inputs:**\n        *   `a` (float): The first number.\n        *   `b` (float): The second number.\n    *   **Output:** (float) The sum of `a` and `b`.\n    *   **Base Image:** `image-registry.openshift-image-registry.svc:5000/openshift/python:latest`\n\n**Pipeline Flow (`add_pipeline`):**\n\nThe pipeline takes two float parameters: `a` (defaulting to `1.0`) and `b` (defaulting to `7.0`).\n\n1.  **`first_add_task`:**\n    *   Calls the `add` component.\n    *   Inputs: `a` (from pipeline parameter `a`) and `b` (fixed value `4.0`).\n    *   **Control Flow:** Immediately after this task is defined, a Persistent Volume Claim (PVC) named `\"my-data\"` is mounted to this task at the path `\"/opt/data\"`. This is achieved using `kubernetes.mount_pvc`.\n\n2.  **`second_add_task`:**\n    *   Calls the `add` component.\n    *   Inputs: `a` (the output of `first_add_task`) and `b` (from pipeline parameter `b`).\n    *   **Control Flow:** This task implicitly depends on `first_add_task` as it uses its output.\n\n**Tools/Libraries Used:**\n\n*   `kfp.dsl` for pipeline and component definition.\n*   `kfp.kubernetes` for PVC mounting functionality.", "completion": "\"\"\"Example of a pipeline to demonstrate mounting a pvc to a task in a pipeline.\"\"\"\n\nimport os\n\nimport kfp.compiler\nfrom dotenv import load_dotenv\nfrom kfp import dsl\nfrom kfp import kubernetes\n\nload_dotenv(override=True)\n\nkubeflow_endpoint = os.environ[\"KUBEFLOW_ENDPOINT\"]\nbearer_token = os.environ[\"BEARER_TOKEN\"]\n\n\n@dsl.component(base_image=\"image-registry.openshift-image-registry.svc:5000/openshift/python:latest\")\ndef add(a: float, b: float) -> float:\n    \"\"\"Calculate the sum of the two arguments.\"\"\"\n    return a + b\n\n\n@kfp.dsl.pipeline(\n    name=\"PVC Pipeline\",\n)\ndef add_pipeline(a: float = 1.0, b: float = 7.0):\n    \"\"\"Pipeline to add values.\n\n    Pipeline to take the value of a, add 4 to it and then\n    perform a second task to take the put of the first task and add b.\n    \"\"\"\n    first_add_task = add(a=a, b=4.0)\n    kubernetes.mount_pvc(first_add_task, pvc_name=\"my-data\", mount_path=\"/opt/data\")\n\n    second_add_task = add(a=first_add_task.output, b=b)  # noqa: F841\n\n\nif __name__ == \"__main__\":\n    client = kfp.Client(\n        host=kubeflow_endpoint,\n        existing_token=bearer_token,\n    )\n    arguments = {\"a\": 7.0, \"b\": 8.0}\n    client.create_run_from_pipeline_func(add_pipeline, arguments=arguments, experiment_name=\"pvc-example\")"}
{"prompt": "Generate a Kubeflow Pipeline named `A Simple CI pipeline` that performs data preprocessing.\n\nThe pipeline consists of a single component:\n1.  **`tfidfvec_op` (vectorizing):** This component takes `data_path` (defaulting to \"gs://kubeflow_pipelines_sentiment/data/test.csv\") and `vectorizer_gcs_location` (defaulting to \"gs://kubeflow_pipelines_sentiment/assets/x.pkl\") as inputs. Its function is to perform TF-IDF vectorization on the input data. It implicitly produces an output named `local_output` (though not explicitly used in a subsequent step, it's defined as an output). This component utilizes Google Cloud Platform (GCP) secrets by applying `gcp.use_gcp_secret('user-gcp-sa')`.\n\nThe pipeline's control flow is straightforward, executing the single `tfidfvec_op` component. There are no explicit dependencies or conditional logic present in the provided active code.\n\nThe pipeline implicitly uses `kfp` and `kfp.dsl` for pipeline definition, and `kfp.gcp` for GCP integration. It relies on a `ComponentStore` to load components, specifically `tfidf-vectoriser`, indicating it's a pre-built or custom component. The specific libraries used within the `tfidf-vectoriser` component (e.g., `sklearn` for TF-IDF) are not directly visible in this pipeline definition but would reside within the component's implementation.", "completion": "import kfp\nimport kfp.dsl as dsl\nimport kfp.gcp as gcp\nimport os\n\n\ndname = os.path.dirname( os.path.abspath(__file__))\n\n# Initialize component store\ncomponent_store = kfp.components.ComponentStore(local_search_paths=[dname+'/../../component'])\n\n# Create component factories\ntfidfvec_op = component_store.load_component('tfidf-vectoriser')\n\n# Define pipeline\n@dsl.pipeline(\n  name='A Simple CI pipeline',\n  description=''\n)\ndef preprocessing_pl(\n  data_path: str=\"gs://kubeflow_pipelines_sentiment/data/test.csv\",\n  vectorizer_gcs_location: str=\"gs://kubeflow_pipelines_sentiment/assets/x.pkl\"\n):\n  tfidfvec_step = tfidfvec_op(data_path=data_path, vectorizer_gcs_location=vectorizer_gcs_location).apply(gcp.use_gcp_secret('user-gcp-sa')) # We do not define output local file, that is auto-generated.\n  tfidfvec_step.set_display_name('vectorizing')\n  tfidfvec_step.outputs['local_output']\n\n\n  '''with kfp.dsl.Condition(sum_value != 0):\n    divide_step = divide_op(x_value=sum_value, y_value=z_value)\n    divide_step.set_display_name('Divide sum by z')\n    add_step2 = add_op(\n      x_value=divide_step.outputs['quotient'],\n      y_value=divide_step.outputs['remainder'])\n    add_step2.set_display_name('Add quotient and remainder')\n  '''\n\n\n'''\npipeline_func = preprocessing_pl\npipeline_filename = pipeline_func.__name__ + '.zip'\n\n\nimport kfp.compiler as compiler\ncompiler.Compiler().compile(pipeline_func, pipeline_filename)\n'''\n# dsl-compile --py pipeline.py --output pipeline_build.tar.gz"}
{"prompt": "Generate a Kubeflow Pipeline named `SF_Crime_Data_Ingestion_and_Preprocessing` that performs data ingestion and preprocessing for crime prediction.\n\nThe pipeline consists of **two** components:\n\n1.  **`preprocess_data`**:\n    *   **Function**: This component takes raw crime data, preprocesses it by dropping duplicates, filtering out irrelevant categories ('Case Closure'), handling missing values, standardizing date/time formats, calculating time differences between incident and report, extracting date/time features (year, month, hour, weekend flag), and filtering data based on a `max_date_training` for training data.\n    *   **Inputs**:\n        *   `df_path` (Input[Dataset]): The path to the raw input DataFrame.\n        *   `to_keep` (list): A list of columns to retain in the DataFrame.\n        *   `data` (str): A string indicating the data type, either 'serving' or 'training', which affects date/time parsing and filtering logic.\n    *   **Outputs**:\n        *   `df_output_path` (Output[Dataset]): The path where the preprocessed DataFrame will be saved.\n    *   **Libraries**: Uses `pandas`, `numpy`, `datetime`, and `pytz`.\n    *   **Base Image**: `python:3.9`\n\n2.  **`map_label`**:\n    *   **Function**: This component takes a preprocessed DataFrame and applies a mapping to specific incident categories, specifically consolidating different 'Human Trafficking' labels into a single 'Human Trafficking' category.\n    *   **Inputs**:\n        *   `df_path` (Input[Dataset]): The path to the input DataFrame requiring label mapping.\n    *   **Outputs**:\n        *   `mapped_df_path` (Output[Dataset]): The path where the DataFrame with mapped labels will be saved.\n    *   **Libraries**: Uses `pandas`.\n    *   **Base Image**: `python:3.9`\n\n**Control Flow**:\nThe pipeline's control flow is sequential: the `map_label` component should execute **after** the `preprocess_data` component. The `map_label` component will receive the output `Dataset` from `preprocess_data` as its input.\n\n**Additional Details**:\n*   The components are defined using `@component` from `kfp.v2.dsl`.\n*   The pipeline should import necessary modules like `kfp`, `kfp.dsl`, `kfp.v2`, `kfp.v2.dsl`, `Input`, `Output`, `Dataset`, and `Artifact`.\n*   Ensure that the Kubeflow SDK v2 compatible syntax is used for defining components and the pipeline.\n*   The components should generate their respective YAML files in a directory named `new_yamls`.", "completion": "import argparse\nfrom variables import *\nfrom common_fns import *\nfrom datetime import datetime, timedelta\nimport pytz\nimport kfp\nfrom kfp.dsl.structures import yaml\nfrom kfp.client import Client\nimport os\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import component\nfrom kfp.v2.dsl import (\n    Input,\n    Output,\n    Artifact,\n    Dataset,\n    Model,\n    Metrics\n)\n\npwd = os.getcwd()\nif not os.path.exists(os.path.join(pwd, 'new_yamls')):\n    os.mkdir(os.path.join(pwd, 'new_yamls'))\n\n\n@component(\n    base_image='python:3.9',\n    packages_to_install=['pandas', 'numpy'],\n    output_component_file='new_yamls/preprocess_data.yaml'\n)\ndef preprocess_data(df_path: Input[Dataset], to_keep: list, data:str,\n                    df_output_path: Output[Dataset]):\n    import pandas as pd\n    from datetime import datetime, timedelta\n    import pytz\n    import numpy as np\n\n    def get_year_month(input):\n        return input.year, input.month, input.hour\n\n    def get_date_time(input):\n        return input.date(), input.time()\n    df = pd.read_csv(df_path.path)\n    df.drop_duplicates('incident_id', inplace = True)\n    df = df[df['incident_category'] != 'Case Closure']\n    if 'filed_online' not in df.columns:\n        df['filed_online'] = False\n    df['filed_online'].fillna(False, inplace = True)\n    df = df[to_keep]\n    df = df.dropna().reset_index(drop=True)\n    df = df[df['police_district'] != 'Out of SF']\n    if data == 'serving':\n        time_format = None\n    else:\n        time_format = '%Y/%m/%d %I:%M:%S %p'\n    for col in ['incident_datetime', 'report_datetime']:\n        df[col] = pd.to_datetime(df[col], format = time_format)\n    today = datetime.now(tz= pytz.timezone('US/Pacific'))\n    max_date_training = (today - timedelta(days=3)).date()\n    df['diff'] = (df['report_datetime'] - df['incident_datetime']).apply(lambda x: x.total_seconds()/3600)\n    df = df[df['diff'] >= 0]\n    df['diff'] = df['diff'].apply(lambda x: np.log(min(x, 365*24) + 1))\n    df['incident_date'], df['incident_time'] = zip(*df['incident_datetime'].apply(lambda x: get_date_time(x)))\n    if data == 'training':\n        df = df[df['incident_date'] <= max_date_training]\n    df['year'], df['month'], df['hour'] = zip(*df['incident_datetime'].apply(lambda x: get_year_month(x)))\n    print (df['year'].unique())\n    df['is_weekend'] = df['incident_day_of_week'].apply(lambda x: int(x in ['Saturday', 'Sunday']))\n    # df = df[df['year'].isin([2018,2023])]\n    df.to_csv(df_output_path.path, index=False)\n\n\n@component(\n    base_image='python:3.9',\n    packages_to_install=['pandas'],\n    output_component_file='new_yamls/mapping_labels.yaml'\n)\ndef map_label(df_path: Input[Dataset], mapped_df_path: Output[Dataset]):\n    import pandas as pd\n\n    df = pd.read_csv(df_path.path)\n    label_mapping = {'Human Trafficking (B), Involuntary Servitude': 'Human Trafficking',\n                     'Human Trafficking, Commercial Sex Acts':'Human Trafficking',\n                     'Human Trafficking (A), Commercial Sex Acts':'Human Trafficking',\n                     'Weapons Offense':'Weapons Offence',\n                     'Motor Vehicle Theft?':'Motor Vehicle Theft',\n                     'Suspicious': 'Suspicious Occ',\n                     'Other Offenses': 'Other',\n                     'Other Miscellaneous': 'Other',\n                     'Drug Offense': 'Drug Violation'}\n    df['incident_category'] = df['incident_category'].replace(label_mapping)\n    all_crimes = list(df['incident_category'].unique())\n    crimes_dict = {'Other': 'other'}\n    violent_crimes = ['Assault', 'Rape', 'Arson', 'Suicide', 'Weapons Offence', 'Homicide',\n                        'Sex Offense', 'Human Trafficking']\n    non_violent_crimes = [\"Lost Property\",\"Non-Criminal\",\"Warrant\",\"Suspicious Occ\",\"Missing Person\",\n                          \"Disorderly Conduct\",\"Larceny Theft\",\"Fire Report\",\"Fraud\",\"Malicious Mischief\",\"Robbery\",\n                          \"Burglary\",\"Vandalism\",\"Traffic Collision\",\"Drug Violation\",\"Motor Vehicle Theft\",\n                          \"Traffic Violation Arrest\",\"Recovered Vehicle\",\"Miscellaneous Investigation\",\"Weapons Carrying Etc\",\n                          \"Forgery And Counterfeiting\",\"Embezzlement\",\"Stolen Property\",\"Vehicle Misplaced\",\n                          \"Offences Against The Family And Children\",\"Prostitution\",\"Vehicle Impounded\",\n                          \"Courtesy Report\",\"Liquor Laws\",\"Gambling\",\"Civil Sidewalks\"]\n\n    for crime in all_crimes:\n        if crime in violent_crimes:\n            crimes_dict[crime] = 'violent'\n        elif crime in non_violent_crimes:\n            crimes_dict[crime] = 'non violent'\n        else:\n            crimes_dict[crime] = 'other'\n\n    df['incident_category'] = df['incident_category'].map(crimes_dict)\n    df.to_csv(mapped_df_path.path, index=False)\n\n@component(\n    base_image='python:3.9',\n    packages_to_install=['nfl-data-py', 'pandas', 'numpy', 'astral'],\n    output_component_file='new_yamls/merging_crime_nfl_data.yaml')\ndef merge_with_nfl(sf_dataset: Input[Dataset], merged_csv: Output[Dataset]):\n    import nfl_data_py as nfl\n    import pandas as pd\n    import numpy as np\n    from astral import LocationInfo\n    from astral.sun import sun\n    from astral.location import Location\n    from datetime import date\n\n    def get_diff_in_time(input):\n        game_time = input['datetime']\n        incident_time = input['incident_datetime']\n        if input['is_game_day'] == 0:\n            return 0\n        else:\n            if game_time > incident_time:\n                return (game_time-incident_time).total_seconds()//60\n            return (-1 * ((incident_time-game_time).total_seconds()//60))\n\n    def game_date_time(datetime):\n        date = pd.to_datetime(datetime['gameday'], format= '%Y-%m-%d').date()\n        time = pd.to_datetime(datetime['gametime'], format= '%H:%M').time()\n        return time, date\n\n    def get_sunrise_sunset(input, city, sf):\n        incident_dt = input['incident_datetime']\n        incident_time = input['incident_time']\n        s = sun(city.observer, date=incident_dt, tzinfo=sf.timezone)\n        sr = s['sunrise'].replace(tzinfo=None).time()\n        ss = s['sunset'].replace(tzinfo=None).time()\n        if incident_time <= sr or incident_time >= ss:\n            return 1\n        return 0\n\n    lat, lon = [37.7, -122.452]\n    city = LocationInfo(\"San Francisco\", \"United States\", \"US/Pacific\", lat, lon)\n    sf = Location(city)\n    df = pd.read_csv(sf_dataset.path)\n    all_years = df['year'].unique().tolist()\n\n    nfl_data = nfl.import_schedules(all_years).query('home_team == \"SF\"')[['gameday', 'gametime']]\n    nfl_data['datetime'] = pd.to_datetime(nfl_data['gameday'].astype(str) + ' ' + nfl_data['gametime'].astype(str), format='%Y-%m-%d %H:%M')\n    nfl_data['gameday'] = pd.to_datetime(nfl_data['gameday']).apply(lambda x: x.date())\n    print ('data type of date in nfl data: ', type(nfl_data['gameday'].tolist()[0]))\n    print ('nfl shape: ', nfl_data.shape)\n    print ('data type of date in orig data: ', type(df['incident_date'].tolist()[0]))\n    df['incident_datetime'] = pd.to_datetime(df['incident_datetime'])\n    df['incident_date'] = pd.to_datetime(df['incident_date']).apply(lambda x: x.date())\n    df['incident_time'] = pd.to_datetime(df['incident_time']).apply(lambda x: x.time())\n    df = df.merge(nfl_data, how = 'left', right_on='gameday', left_on='incident_date')\n    print ('number of game dates that are not null: ', df.query('gameday==gameday').shape)\n    df.loc[pd.notnull(df['gameday']), 'is_game_day'] = 1\n    df.loc[pd.isnull(df['gameday']), 'is_game_day'] = 0\n    print (df['is_game_day'].value_counts())\n    df['till_game_time'] = df.apply(lambda x: get_diff_in_time(x), axis = 1)\n    df['is_between_sr_ss'] = df.apply(lambda x: get_sunrise_sunset(x, city, sf), axis = 1)\n    df.to_csv(merged_csv.path, index=False)\n\n@component(\n    base_image='python:3.9',\n    packages_to_install=['pandas'],\n    output_component_file='new_yamls/add_dummy_cols.yaml'\n)\ndef add_dummy_cols(df_path: Input[Dataset], output_df: Output[Dataset]):\n    import pandas as pd\n\n    df = pd.read_csv(df_path.path)\n    dummy_cols = ['latitude_buckets', 'longitude_buckets', 'Prediction']\n    for col in dummy_cols:\n        df[col] = ''\n\n    df.to_csv(output_df.path, index=False)\n    \n@dsl.pipeline(\n    name='SF Data Pipeline',\n    description= 'The entire pipeline from reading sf and NFL data to pre-processing it and uploading to BQ')\ndef sf_ingest_pipeline(url: str, projectid:str = projectid, datasetid:str =datasetid, tablename:str =tablename):\n\n    get_data_task = read_training_data(url = url, method='ingesting', projectid=projectid, datasetid=datasetid,\n                                       tablename=tablename)\n    preprocess_data_task = preprocess_data(df_path= get_data_task.outputs['output_csv'], data = 'training',\n                                           to_keep = ['incident_id', 'police_district', 'incident_datetime',\n                                      'latitude', 'longitude', 'filed_online', 'incident_day_of_week',\n                                      'report_datetime', 'analysis_neighborhood', 'supervisor_district',\n                                      'incident_category', 'data_type'])\n    map_label_task = map_label(df_path= preprocess_data_task.outputs['df_output_path'])\n    merge_nfl_task = merge_with_nfl(sf_dataset = map_label_task.outputs['mapped_df_path'])\n    insert_dummy_cols_task = add_dummy_cols(df_path= merge_nfl_task.outputs['merged_csv'])\n    upload_task = upload_to_bq(df_path = insert_dummy_cols_task.outputs['output_df'], projectid= projectid,\n                                datasetid= datasetid, tableid= tablename, location='US',\n                                schema = schema)\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='Kubeflow pipeline to run ingestion script for SF Crime data.')\n    parser.add_argument('type', help='Enter \"yaml\" if file needs to be saved or \"run\" if pipline needs to be run directly',\n                       type= str)\n    args = parser.parse_args()\n    \n    if args.type == 'yaml':\n        kfp.compiler.Compiler().compile(\n            pipeline_func=sf_ingest_pipeline,\n            package_path='new_yamls/ingestion_pipeline.yaml')\n    elif args.type == 'run':\n        client = Client(host= host_url)\n        today = datetime.now(tz= pytz.timezone('US/Pacific'))\n        max_date_training = (today - timedelta(days=3)).date().strftime('%Y%m%d')\n        url = f'https://data.sfgov.org/api/views/wg3w-h783/rows.csv?date={max_date_training}&accessType=DOWNLOAD'\n        experiments = client.list_experiments().experiments\n        experiment_names = list(map(lambda x: x.display_name, experiments))\n        if ingest_experiment_name not in experiment_names:\n            client.create_experiment(name=ingest_experiment_name)\n        client.create_run_from_pipeline_func(\n        pipeline_func=sf_ingest_pipeline,\n        experiment_name=ingest_experiment_name,\n        arguments={\n            'url': url,\n        })"}
{"prompt": "Generate a Kubeflow Pipeline named `Parallel pipeline` that performs the following steps:\n\nThis pipeline downloads two text files from GCS in parallel and then prints their concatenated content.\n\nIt consists of **three components**:\n\n1.  **`GCS - Download`**: This component downloads a file from a specified GCS URL.\n    *   It takes one input: `url` (string), which is the GCS path to the file.\n    *   It produces one output: `data` (file path), which points to the downloaded file at `/tmp/results.txt`.\n    *   It uses the `google/cloud-sdk:279.0.0` image and executes a `gsutil cat` command to download the file.\n\n2.  **`GCS - Download`** (second instance): This is another instance of the same GCS download component, running in parallel to the first.\n    *   It takes one input: `url` (string), which is the GCS path to the file.\n    *   It produces one output: `data` (file path), which points to the downloaded file at `/tmp/results.txt`.\n    *   It uses the `google/cloud-sdk:279.0.0` image and executes a `gsutil cat` command to download the file.\n\n3.  **`echo`**: This component takes two text inputs and prints them.\n    *   It takes two inputs: `text1` (string) and `text2` (string).\n    *   It does not produce any visible outputs.\n    *   It uses the `library/bash:4.4.23` image and executes a `echo` command to print the inputs.\n\n**Control Flow**:\n\n*   The pipeline defines two input parameters:\n    *   `url1` (string, default: `gs://ml-pipeline/sample-data/shakespeare/shakespeare1.txt`)\n    *   `url2` (string, default: `gs://ml-pipeline/sample-data/shakespeare/shakespeare2.txt`)\n*   The first two `GCS - Download` components (`download1_task` and `download2_task`) run **in parallel**.\n    *   `download1_task` takes `url1` as its input.\n    *   `download2_task` takes `url2` as its input.\n*   The `echo` component (`echo_task`) runs **after** both `download1_task` and `download2_task` have completed.\n    *   Its first input (`text1`) is the `data` output from `download1_task`.\n    *   Its second input (`text2`) is the `data` output from `download2_task`.\n\nThe pipeline's overall description is \"Download two messages in parallel and prints the concatenated result.\"", "completion": "#!/usr/bin/env python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport kfp\nfrom kfp import dsl\n\ndef gcs_download_op(url):\n    return dsl.ContainerOp(\n        name='GCS - Download',\n        image='google/cloud-sdk:279.0.0',\n        command=['sh', '-c'],\n        arguments=['gsutil cat $0 | tee $1', url, '/tmp/results.txt'],\n        file_outputs={\n            'data': '/tmp/results.txt',\n        }\n    )\n\n\ndef echo2_op(text1, text2):\n    return dsl.ContainerOp(\n        name='echo',\n        image='library/bash:4.4.23',\n        command=['sh', '-c'],\n        arguments=['echo \"Text 1: $0\"; echo \"Text 2: $1\"', text1, text2]\n    )\n\n\n@dsl.pipeline(\n  name='Parallel pipeline',\n  description='Download two messages in parallel and prints the concatenated result.'\n)\ndef download_and_join(\n    url1='gs://ml-pipeline/sample-data/shakespeare/shakespeare1.txt',\n    url2='gs://ml-pipeline/sample-data/shakespeare/shakespeare2.txt'\n):\n    \"\"\"A three-step pipeline with first two running in parallel.\"\"\"\n\n    download1_task = gcs_download_op(url1)\n    download2_task = gcs_download_op(url2)\n\n    echo_task = echo2_op(download1_task.output, download2_task.output)\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(download_and_join, __file__ + '.yaml')"}
{"prompt": "Generate a Kubeflow Pipeline named `CLV Batch Predict` that performs Customer Lifetime Value (CLV) batch prediction using BigQuery for feature engineering and AutoML Tables for batch inference.\n\nThe pipeline consists of four components:\n\n1.  **`Load sales transactions`**: This component, implemented by `load_sales_transactions_op` (which wraps the `load_sales_transactions` function), is responsible for loading sales transactions data.\n    *   **Inputs:** `project_id`, `source_gcs_path`, `source_bq_table`, `dataset_location`, `dataset_name`, and `table_id` (mapped to `transactions_table_name`).\n    *   **Outputs:** This component likely outputs the loaded sales transaction data (though not explicitly shown in the provided snippet).\n\n2.  **`Generate the feature engineering query`**: This component, implemented by `prepare_feature_engineering_query_op` (which wraps the `prepare_feature_engineering_query` function), prepares the BigQuery SQL query for feature engineering.\n    *   **Inputs:** `project_id`, `threshold_date`, `predict_end`, `max_monetary`, `query_template_uri`, `transactions_table_name`, `features_table_name`, and `bq_dataset_name`.\n    *   **Outputs:** It outputs the generated BigQuery query string.\n\n3.  **`Engineer features`**: This component, implemented by `engineer_features_op` (loaded from `bigquery/query` from the component store), executes the BigQuery query to engineer features.\n    *   **Inputs:** It takes the query string from the `Generate the feature engineering query` component.\n    *   **Outputs:** It outputs the engineered features, likely into a BigQuery table.\n    *   **Dependencies:** This component depends on the successful completion and output of `Generate the feature engineering query`.\n\n4.  **`Batch predict`**: This component, implemented by `batch_predict_op` (loaded from `aml-batch-predict` from the component store), performs batch prediction using the engineered features and an AutoML Tables model.\n    *   **Inputs:** It takes `aml_model_id`, `destination_prefix`, `aml_compute_region`, and the engineered features (presumably from the `Engineer features` component).\n    *   **Outputs:** It outputs the prediction results.\n    *   **Dependencies:** This component depends on the successful completion and output of `Engineer features`.\n\n**Control Flow:**\n\n*   `Load sales transactions` runs first, independently.\n*   `Generate the feature engineering query` runs concurrently with `Load sales transactions`.\n*   `Engineer features` runs *after* `Generate the feature engineering query` completes, using its output as input.\n*   `Batch predict` runs *after* `Engineer features` completes, using its output as input.\n\n**Pipeline Parameters:**\n\nThe pipeline accepts the following parameters:\n*   `project_id` (string)\n*   `source_gcs_path` (string)\n*   `staging_gcs_path` (string)\n*   `source_bq_table` (string)\n*   `bq_dataset_name` (string)\n*   `threshold_date` (string)\n*   `predict_end` (string)\n*   `max_monetary` (float)\n*   `aml_model_id` (string)\n*   `destination_prefix` (string)\n*   `features_table_name` (string, with a default value from `argument_defaults`)\n*   `transactions_table_name` (string, with a default value from `argument_defaults`)\n*   `dataset_location` (string, with a default value from `argument_defaults`)\n*   `aml_compute_region` (string, with a default value from `argument_defaults`)\n*   `query_template_uri` (string, with a default value from `argument_defaults`)\n\n**Tools/Libraries:**\n\n*   **Kubeflow Pipelines (KFP)**: Used for defining the pipeline (`@dsl.pipeline`, `@component`), creating lightweight components (`kfp.components.func_to_container_op`), and loading pre-built components from a component store (`kfp.components.ComponentStore`).\n*   **Google Cloud Platform (GCP)**: Integration with BigQuery for data querying and AutoML Tables for model prediction.\n*   **`pathlib` and `yaml`**: Used for loading pipeline settings from `settings.yaml`.\n*   **Helper components**: `load_sales_transactions` and `prepare_feature_engineering_query` are custom Python functions wrapped as lightweight components.\n*   **`bigquery/query` (from component store)**: A pre-built BigQuery component.\n*   **`aml-batch-predict` (from component store)**: A pre-built AutoML Tables batch prediction component.", "completion": "# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#            http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Batch Predict Pipeline.\"\"\"\n\nfrom helper_components import load_sales_transactions\nfrom helper_components import prepare_feature_engineering_query\n\nimport kfp\nfrom kfp import gcp\n\nimport pathlib\nimport yaml\n\n# Load pipeline settings\ncompiler_settings = yaml.safe_load(\n    pathlib.Path('settings.yaml').read_text())['compiler_settings']\nargument_defaults = yaml.safe_load(\n    pathlib.Path('settings.yaml').read_text())['argument_defaults']\n\n# Initialize component store\ncomponent_store = kfp.components.ComponentStore(\n    compiler_settings['local_search_paths'],\n    compiler_settings['url_search_prefixes'])\n\n# Create component factories\nload_sales_transactions_op = kfp.components.func_to_container_op(\n    load_sales_transactions,\n    base_image=compiler_settings['lightweight_components_base_image'])\nprepare_feature_engineering_query_op = kfp.components.func_to_container_op(\n    prepare_feature_engineering_query,\n    base_image=compiler_settings['lightweight_components_base_image'])\nengineer_features_op = component_store.load_component('bigquery/query')\nbatch_predict_op = component_store.load_component('aml-batch-predict')\n\n\n# Define the batch predict pipeline\n@kfp.dsl.pipeline(\n    name='CLV Batch Predict',\n    description='CLV Batch Predict Pipeline using BigQuery for feature engineering and Automl Tables for batch inference'\n)\ndef clv_batch_predict(\n    project_id,\n    source_gcs_path,\n    staging_gcs_path,\n    source_bq_table,\n    bq_dataset_name,\n    threshold_date,\n    predict_end,\n    max_monetary,\n    aml_model_id,\n    destination_prefix,\n    features_table_name=argument_defaults['features_table_name'],\n    transactions_table_name=argument_defaults['transactions_table_name'],\n    dataset_location=argument_defaults['dataset_location'],\n    aml_compute_region=argument_defaults['aml_compute_region'],\n    query_template_uri=argument_defaults['query_template_uri']):\n  \"\"\"Prepares and scores sales transactions dataset.\"\"\"\n\n  # Load sales transactions\n  load_transactions = load_sales_transactions_op(\n      project_id=project_id,\n      source_gcs_path=source_gcs_path,\n      source_bq_table=source_bq_table,\n      dataset_location=dataset_location,\n      dataset_name=bq_dataset_name,\n      table_id=transactions_table_name)\n\n  # Generate the feature engineering query\n  prepare_query = prepare_feature_engineering_query_op(\n      project_id=project_id,\n      source_table_id=load_transactions.output,\n      destination_dataset=bq_dataset_name,\n      features_table_name=features_table_name,\n      threshold_date=threshold_date,\n      predict_end=predict_end,\n      max_monetary=max_monetary,\n      query_template_uri=query_template_uri)\n\n  # Run the feature engineering query on BigQuery.\n  engineer_features = engineer_features_op(\n      query=prepare_query.outputs['query'],\n      project_id=project_id,\n      dataset_id=prepare_query.outputs['dataset_name'],\n      table_id=prepare_query.outputs['table_name'],\n      output_gcs_path=staging_gcs_path,\n      dataset_location=dataset_location,\n      job_config='')\n\n  # Run the batch predict task on features in Big Query\n  source_data_uri = 'bq://{}.{}.{}'.format(\n      project_id, prepare_query.outputs['dataset_name'],\n      prepare_query.outputs['table_name'])\n\n  predict_batch = batch_predict_op(\n      project_id=project_id,\n      region=aml_compute_region,\n      model_id=aml_model_id,\n      datasource=source_data_uri,\n      destination_prefix=destination_prefix)\n\n  predict_batch.after(engineer_features)\n\n  # Configure the pipeline to use a service account secret\n  if compiler_settings['use_sa_secret']:\n    steps = [load_transactions, prepare_query, engineer_features, predict_batch]\n    for step in steps:\n      step.apply(gcp.use_gcp_secret('user-gcp-sa'))"}
{"prompt": "This prompt cannot be generated as the provided Python code does not define a Kubeflow Pipeline using `@dsl.pipeline` and `@component` decorators. It only contains helper functions for creating `ContainerOp` instances. \n\nTo generate the prompt, I would need a Python file that defines a Kubeflow pipeline structure with components and their dependencies using the `kfp.dsl` decorators.", "completion": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef training_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('TRAINING_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=['/usr/local/bin/python', script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )"}
{"prompt": "Generate a Kubeflow Pipeline named `Paul_Sud_Demo_Pipeline` that performs a bioinformatics quality control and plotting workflow.\n\nThe pipeline consists of two components:\n1.  **`trim`**: This component performs quality trimming on a FASTQ file using Trimmomatic.\n    *   **Inputs**:\n        *   `fastq` (str): The path to the input FASTQ file.\n        *   `leading` (int): Quality threshold for leading bases.\n        *   `trailing` (int): Quality threshold for trailing bases.\n        *   `minlen` (int): Minimum read length after trimming.\n        *   `sliding_window` (str): Sliding window parameters (e.g., \"4:15\").\n    *   **Outputs**:\n        *   `trimmed_fastq` (OutputPath(\"TrimmedFastq\")): The path to the trimmed FASTQ file.\n    *   **Function**: It executes the `Trimmomatic-0.38.jar` tool to trim low-quality bases from the input FASTQ file. It uses a temporary `trimmed.fastq.gz` file and then renames it to the Kubeflow-provided output path.\n    *   **Base Image**: `quay.io/encode-dcc/demo-pipeline:template`\n\n2.  **`plot`**: This component generates quality score plots for both the original and trimmed FASTQ files.\n    *   **Inputs**:\n        *   `fastq` (str): The path to the original FASTQ file.\n        *   `trimmed_fastq` (InputPath(\"TrimmedFastq\")): The path to the trimmed FASTQ file, received from the `trim` component.\n        *   `bar_color` (str): Color for bars in the plot.\n        *   `flier_color` (str): Color for fliers (outliers) in the plot.\n        *   `plot_color` (str): General plot color.\n    *   **Outputs**:\n        *   `plot` (OutputPath(\"Plot\")): The path to the generated quality score PNG plot.\n    *   **Function**: It copies the `trimmed_fastq` to a temporary `trimmed.fastq.gz` file and then executes a Python script `/software/demo-pipeline/src/plot_fastq_scores.py` to generate quality score plots. It then renames the generated PNG file to the Kubeflow-provided output path.\n    *   **Base Image**: `quay.io/encode-dcc/demo-pipeline:template`\n\n**Control Flow**:\nThe `plot` component is dependent on the `trim` component. The `trimmed_fastq` output of the `trim` component is passed as the `trimmed_fastq` input to the `plot` component.\n\n**Pipeline Parameters**:\nThe pipeline should expose the following parameters to the user:\n*   `fastq_file` (str): The input FASTQ file.\n*   `leading_trim` (int): Leading trim quality. Default to `3`.\n*   `trailing_trim` (int): Trailing trim quality. Default to `3`.\n*   `min_length` (int): Minimum read length. Default to `36`.\n*   `sliding_window_params` (str): Sliding window parameters. Default to `\"4:15\"`.\n*   `bar_chart_color` (str): Bar color for plots. Default to `\"blue\"`.\n*   `flier_chart_color` (str): Flier color for plots. Default to `\"red\"`.\n*   `plot_line_color` (str): Plot line color. Default to `\"green\"`.\n\nThe pipeline should instantiate the `trim` and `plot` components and link them accordingly, passing the pipeline parameters to the respective component inputs.", "completion": "from kfp import dsl\nfrom kfp.components import InputPath, OutputPath, func_to_container_op\n\n\ndef trim(\n    fastq: str,\n    leading: int,\n    trailing: int,\n    minlen: int,\n    sliding_window: str,\n    trimmed_fastq: OutputPath(\"TrimmedFastq\"),\n) -> None:\n    \"\"\"\n    In the function signature, `InputPath(\"Fastq\")` is used to indicate a path to some\n    fastq, and at runtime the actual path of the data gets passed. The `Fastq` argument\n    is typechecked by the DSL compiler to make sure that types of data being passed from\n    component to component match up.\n\n    Because the trimmed fastq output path is controlled by Kubeflow, it does not have a\n    .gz extension, and as such Trimmomatic thinks it's OK to not gzip the output file\n    if we just pass that path directly. Instead, we pass a dummy filename to Trimmomatic\n    and then just write the contents of that to the output path.\n    \"\"\"\n    import os\n    import subprocess\n\n    subprocess.run(\n        [\n            \"java\",\n            \"-jar\",\n            \"/software/Trimmomatic-0.38/trimmomatic-0.38.jar\",\n            \"SE\",\n            \"-phred33\",\n            fastq,\n            \"trimmed.fastq.gz\",\n            \"LEADING:{}\".format(leading),\n            \"TRAILING:{}\".format(trailing),\n            \"SLIDINGWINDOW:{}\".format(sliding_window),\n            \"MINLEN:{}\".format(minlen),\n        ],\n        stderr=subprocess.STDOUT\n    )\n\n    os.rename(\"trimmed.fastq.gz\", trimmed_fastq)\n\n\ndef plot(\n    fastq: str,\n    trimmed_fastq: InputPath(\"TrimmedFastq\"),\n    bar_color: str,\n    flier_color: str,\n    plot_color: str,\n    plot: OutputPath(\"Plot\")\n) -> None:\n    \"\"\"\n    This is a rather odd Pythonception. Don't do this in production! One of the nice\n    things about the Kubeflow Pipelines SDK is that you can define your components\n    in Python. Calling the script as a subprocess here is a legacy of the original WDL\n    pipeline, WDL forces you to wrap all code with shell scripts. With KFP the compiler\n    autogenerates CLI wrappers for you.\n\n    Like in the trim task, tools often make unfortunate assumptions about the names of\n    files. Here we pull a similar trick to get the plot to not complain due to the\n    unexpected input filename.\n    \"\"\"\n    import glob\n    import os\n    from shutil import copyfile\n    import subprocess\n\n    copyfile(trimmed_fastq, \"trimmed.fastq.gz\")\n\n    subprocess.run(\n        [\n            \"python3\",\n            \"/software/demo-pipeline/src/plot_fastq_scores.py\",\n            \"--untrimmed\",\n            fastq,\n            \"--trimmed\",\n            \"trimmed.fastq.gz\",\n            \"--bar-color\",\n            bar_color,\n            \"--flier-color\",\n            flier_color,\n            \"--plot-color\",\n            plot_color,\n        ],\n        stderr=subprocess.STDOUT\n    )\n\n    os.rename(glob.glob(\"*quality_scores.png\")[0], plot)\n\n\n\ntrim_op = func_to_container_op(trim, base_image=\"quay.io/encode-dcc/demo-pipeline:template\")\nplot_op = func_to_container_op(plot, base_image=\"quay.io/encode-dcc/demo-pipeline:template\")\n\n\n@dsl.pipeline(\n    name='ENCODE Demo Pipeline',\n    description='A Kubeflow Pipelines implementation of the ENCODE DCC demo pipeline'\n)\ndef demo_pipeline(\n    fastqs=[\"/mnt/data/file1.fastq.gz\", \"/mnt/data/file2.fastq.gz\"],\n    leading: int = 5,\n    trailing: int = 5,\n    minlen: int = 80,\n    sliding_window: str = \"4:25\",\n    bar_color: str = \"white\",\n    flier_color: str = \"grey\",\n    plot_color: str = \"darkgrid\",\n):\n    \"\"\"\n    func_to_container_op simply converts the function into a factory that produces ops\n    when called. add_pvolumes is a method of the op itself, so we need to apply it here\n    when the op is actually generated, NOT above where the trim_op factory is created.\n    \"\"\"\n    with dsl.ParallelFor(fastqs) as fastq:\n        trim_task = trim_op(\n            fastq=fastq,\n            leading=leading,\n            trailing=trailing,\n            minlen=minlen,\n            sliding_window=sliding_window,\n        ).add_pvolumes({\"/mnt/data\": dsl.PipelineVolume(pvc=\"test-data-pv-claim\")})\n\n        _ = plot_op(\n            fastq=fastq,\n            trimmed_fastq=trim_task.outputs[\"trimmed_fastq\"],\n            bar_color=bar_color,\n            flier_color=flier_color,\n            plot_color=plot_color\n        ).add_pvolumes({\"/mnt/data\": dsl.PipelineVolume(pvc=\"test-data-pv-claim\")})"}
{"prompt": "Generate a Kubeflow Pipeline named `GoLang_Compile_Demo` that performs the following steps:\n\nThis pipeline consists of **three** components:\n\n1.  **`download-code`**:\n    *   **Function:** Downloads a GoLang source code file from a specified URL.\n    *   **Inputs:**\n        *   `url`: A string URL pointing to the GoLang source code (e.g., `https://raw.githubusercontent.com/kubeflow/pipelines/master/samples/core/container_build/testdata/main.go`).\n        *   `download_to`: A string path where the file should be downloaded within the container (e.g., `/go/src/github.com/go-kubeflow/pipelines/cmd/main.go`).\n        *   `md5sum`: A string representing the expected MD5 checksum of the downloaded file for integrity verification (e.g., `380dd7283626d708f579512353a25d2c`).\n    *   **Outputs:** None explicitly defined, but the effect is a downloaded file at `download_to`.\n    *   **Tools/Libraries:** Uses `curl` for downloading and `md5sum` for verification. The image used is `appropriate/curl`.\n\n2.  **`compile-golang`**:\n    *   **Function:** Compiles the downloaded GoLang source code into an executable.\n    *   **Inputs:**\n        *   `script`: A string path to the GoLang source file to be compiled (e.g., `/go/src/github.com/go-kubeflow/pipelines/cmd/main.go`).\n        *   `arguments`: A list of strings for additional arguments to the compilation command (e.g., `['build', '-o', '/go/bin/go-program', '/go/src/github.com/go-kubeflow/pipelines/cmd/main.go']`).\n    *   **Outputs:** None explicitly defined, but the effect is a compiled executable.\n    *   **Tools/Libraries:** Uses the GoLang compiler. The image used is a GoLang base image, typically defined in the environment (e.g., `GOLANG_IMAGE`).\n    *   **Dependencies:** This component must run `after` the `download-code` component, ensuring the source code is available.\n\n3.  **`run-golang`**:\n    *   **Function:** Executes the compiled GoLang program.\n    *   **Inputs:**\n        *   `script`: A string path to the compiled GoLang executable (e.g., `/go/bin/go-program`).\n        *   `arguments`: A list of strings for additional arguments to the executable (e.g., `[]`).\n    *   **Outputs:** None explicitly defined, but the effect is the execution of the program.\n    *   **Tools/Libraries:** Executes the compiled GoLang binary. The image used is the same GoLang base image as `compile-golang`.\n    *   **Dependencies:** This component must run `after` the `compile-golang` component, ensuring the executable is ready.\n\n**Control Flow:**\nThe pipeline executes sequentially: `download-code` -> `compile-golang` -> `run-golang`. Each subsequent component is dependent on the successful completion of its predecessor.", "completion": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef processing_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('GOLANG_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=[script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )"}
{"prompt": "Generate a Kubeflow Pipeline named `Two-step-pipeline` that performs a data preprocessing and a model training step.\n\nThe pipeline consists of two components:\n1.  **`preprocess`**:\n    *   **Functionality**: Simulates a data preprocessing step.\n    *   **Inputs**:\n        *   `uri` (str): A URI string.\n        *   `some_int` (int): An integer parameter.\n    *   **Outputs**:\n        *   `output_dataset_one` (Dataset): An output dataset artifact.\n        *   `output_parameter_one` (int): An output parameter.\n    *   **Implementation Details**: This component logs the input `uri` and `some_int`, and outputs `some_int` as a parameter and `output_dataset_one` as an artifact with a dummy value. It uses the `kfp.dsl.OutputPath` and `kfp.dsl.Output` types for artifact and parameter outputs, respectively.\n\n2.  **`train_op`**:\n    *   **Functionality**: Simulates a model training step.\n    *   **Inputs**:\n        *   `dataset` (Dataset): An input dataset artifact.\n        *   `num_steps` (int): An integer parameter representing the number of training steps.\n    *   **Outputs**:\n        *   `model` (Model): An output model artifact.\n    *   **Implementation Details**: This component logs the `num_steps` and the path of the input `dataset`. It produces a dummy `model` artifact. It uses the `kfp.dsl.InputPath` and `kfp.dsl.OutputPath` types for artifact inputs and outputs, respectively.\n\n**Control Flow**:\n*   The `preprocess` component runs first.\n*   The `train_op` component depends on the successful completion of `preprocess`. It uses the `output_dataset_one` artifact from `preprocess` as its `dataset` input and the `output_parameter_one` from `preprocess` as its `num_steps` input.\n\n**Pipeline Inputs**:\n*   `uri` (str): Defaults to 'gs://my-bucket/some-data'.\n*   `some_int` (int): Defaults to 10.\n\n**Decorators**: The pipeline uses `@dsl.pipeline` and `@dsl.component`.\n**Libraries**: The pipeline utilizes `kfp.dsl` for component and pipeline definition.", "completion": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Two step v2-compatible pipeline.\"\"\"\n\n# %%\n\nfrom __future__ import annotations\n\nimport random\nimport string\nimport unittest\nimport functools\n\nimport kfp\nimport kfp_server_api\n\nfrom .two_step import two_step_pipeline\nfrom .util import run_pipeline_func, TestCase, KfpMlmdClient, KfpTask\nfrom ml_metadata.proto import Execution\n\n\ndef verify_tasks(\n    t: unittest.TestCase, tasks: dict[str, KfpTask], task_state, uri: str,\n    some_int: int\n):\n    task_names = [*tasks.keys()]\n    t.assertEqual(task_names, ['train-op', 'preprocess'], 'task names')\n\n    preprocess = tasks['preprocess']\n    train = tasks['train-op']\n\n    t.assertEqual({\n        'name': 'preprocess',\n        'inputs': {\n            'artifacts': [],\n            'parameters': {\n                'some_int': some_int,\n                'uri': uri\n            }\n        },\n        'outputs': {\n            'artifacts': [{\n                'metadata': {\n                    'display_name': 'output_dataset_one',\n                },\n                'name': 'output_dataset_one',\n                'type': 'system.Dataset'\n            }],\n            'parameters': {\n                'output_parameter_one': some_int\n            }\n        },\n        'type': 'system.ContainerExecution',\n        'state': task_state,\n    }, preprocess.get_dict())\n    t.assertEqual({\n        'name': 'train-op',\n        'inputs': {\n            'artifacts': [{\n                'metadata': {\n                    'display_name': 'output_dataset_one',\n                },\n                'name': 'dataset',\n                'type': 'system.Dataset',\n            }],\n            'parameters': {\n                'num_steps': some_int\n            }\n        },\n        'outputs': {\n            'artifacts': [{\n                'metadata': {\n                    'display_name': 'model',\n                },\n                'name': 'model',\n                'type': 'system.Model',\n            }],\n            'parameters': {}\n        },\n        'type': 'system.ContainerExecution',\n        'state': task_state,\n    }, train.get_dict())\n\n\ndef verify(\n    run: kfp_server_api.ApiRun, mlmd_connection_config, uri: str, some_int,\n    state: int, **kwargs\n):\n    t = unittest.TestCase()\n    t.maxDiff = None  # we always want to see full diff\n    t.assertEqual(run.status, 'Succeeded')\n    client = KfpMlmdClient(mlmd_connection_config=mlmd_connection_config)\n    tasks = client.get_tasks(run_id=run.id)\n    verify_tasks(t, tasks, state, uri, some_int)\n\n\nif __name__ == '__main__':\n    letters = string.ascii_lowercase\n    random_uri = 'http://' + ''.join(random.choice(letters) for i in range(5))\n    random_int = random.randint(0, 10000)\n    run_pipeline_func([\n        TestCase(\n            pipeline_func=two_step_pipeline,\n            arguments={\n                'uri': f'{random_uri}',\n                'some_int': f'{random_int}'\n            },\n            verify_func=functools.partial(\n                verify,\n                uri=random_uri,\n                some_int=random_int,\n                state=Execution.State.COMPLETE,\n            ),\n            mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE,\n            enable_caching=True\n        ),\n    ]),\n    run_pipeline_func([\n        TestCase(\n            pipeline_func=two_step_pipeline,\n            arguments={\n                'uri': f'{random_uri}',\n                'some_int': f'{random_int}'\n            },\n            verify_func=functools.partial(\n                verify,\n                uri=random_uri,\n                some_int=random_int,\n                state=Execution.State.CACHED\n            ),\n            mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE,\n            enable_caching=True\n        ),\n    ])\n\n# %%"}
{"prompt": "Generate a Kubeflow Pipeline named `optuna pipeline` that performs hyperparameter optimization and sends notifications.\n\nThe pipeline consists of **two** main components:\n1.  **`optuna-worker`**: This component executes the core hyperparameter optimization logic using the Optuna library.\n    *   **Inputs**:\n        *   `pipeline_name` (string, default: \"optuna-pipeline\"): Name of the current pipeline.\n        *   `bucket_name` (string, default: \"\"): GCS bucket name for storing artifacts.\n        *   `job_id` (string, default: \"{{JOB_ID}}\"): Unique identifier for the current job run.\n        *   `n_trials` (integer, default: 100): Number of optimization trials to run.\n        *   `n_jobs` (integer, default: 5): Number of parallel jobs for Optuna.\n        *   `training_pipeline_name` (string, default: \"head-pose-pipeline\"): Name of the training pipeline to be optimized.\n        *   `dataset` (string, default: \"\"): Identifier for the dataset being used.\n        *   `epochs` (integer, default: 5): Number of training epochs per trial.\n    *   **Function**: Orchestrates hyperparameter optimization studies using Optuna, likely by calling a separate training pipeline for each trial.\n    *   **Resource Configuration**:\n        *   Runs on a node pool with the label `\"cloud.google.com/gke-nodepool\": \"cpu-pool\"`.\n        *   Includes a `cloudsqlproxy` sidecar container (image: `gcr.io/cloudsql-docker/gce-proxy:1.14`) to connect to a Cloud SQL instance. The sidecar command connects to a MySQL instance specified by environment variables `GCP_PROJECT`, `GCP_REGION`, and `DB_NAME`.\n        *   Configured to retry up to 2 times in case of failure.\n\n2.  **`slack-notification`**: This component sends a notification to Slack.\n    *   **Inputs**:\n        *   `pipeline_name` (string): Name of the pipeline.\n        *   `job_id` (string): Identifier of the job.\n        *   `message` (string, default: \"Status: {{workflow.status}}\"): The message to be sent, including the workflow status.\n    *   **Function**: Provides status updates or alerts to a Slack channel.\n    *   **Resource Configuration**:\n        *   Runs on a node pool with the label `\"cloud.google.com/gke-nodepool\": \"main-pool\"`.\n\n**Control Flow**:\nThe pipeline uses an `dsl.ExitHandler`. The `slack-notification` component is executed as the `exit_op`, meaning it will run **after** the `optuna-worker` component completes, regardless of success or failure. The `optuna-worker` component is the main execution step within the pipeline's context.\n\nThe pipeline is compiled into `optuna-pipeline.yaml`.", "completion": "import kfp\nfrom kfp import dsl\nimport os\n\n# Initialize component store\ncomponent_store = kfp.components.ComponentStore(\n    local_search_paths=[\"pipelines/optuna-pipeline\", \"components\"])\n\n# Create component factories\noptuna_op = component_store.load_component(\"optuna-worker\")\nslack_notification_op = component_store.load_component(\"slack-notification\")\n\nsidecar = kfp.dsl.Sidecar(\n    name=\"cloudsqlproxy\",\n    image=\"gcr.io/cloudsql-docker/gce-proxy:1.14\",\n    command=[\n        \"/cloud_sql_proxy\",\n        f\"-instances={os.environ.get('GCP_PROJECT')}:{os.environ.get('GCP_REGION')}:{os.environ.get('DB_NAME')}=tcp:3306\",\n    ],\n)\n\n# Define pipeline\n@dsl.pipeline(\n    name=\"optuna pipeline\",\n    description=\"optuna pipeline\"\n)\ndef pipeline(\n    pipeline_name: str = \"optuna-pipeline\",\n    bucket_name: str = \"\",\n    job_id: str = \"{{JOB_ID}}\",\n    n_trials: int = 100,\n    n_jobs: int = 5,\n    training_pipeline_name: str = \"head-pose-pipeline\",\n    dataset: str = \"\",\n    epochs: int = 5\n):\n    with dsl.ExitHandler(\n        exit_op=slack_notification_op(\n            pipeline_name=pipeline_name,\n            job_id=job_id,\n            message=\"Status: {{workflow.status}}\"\n        ).add_node_selector_constraint(\"cloud.google.com/gke-nodepool\", \"main-pool\")\n    ):\n        optuna_op(\n            pipeline_name=pipeline_name,\n            bucket_name=bucket_name,\n            job_id=job_id,\n            n_trials=n_trials,\n            n_jobs=n_jobs,\n            training_pipeline_name=training_pipeline_name,\n            dataset=dataset,\n            epochs=epochs\n        ).set_display_name(\"optuna-worker\")\\\n            .add_node_selector_constraint(\"cloud.google.com/gke-nodepool\", \"cpu-pool\")\\\n            .add_sidecar(sidecar)\\\n            .set_retry(num_retries=2)\n\n\nif __name__ == \"__main__\":\n    kfp.compiler.Compiler().compile(pipeline, \"optuna-pipeline.yaml\")"}
{"prompt": "Generate a Kubeflow Pipeline named `sample-pipeline` that performs the following steps:\n\nThe pipeline consists of a single component:\n1.  **`_save_message_to_file`**: This component takes two string inputs: `message` and `gcs_filepath`. Its function is to save the provided `message` content to the specified file path in Google Cloud Storage (`gcs_filepath`). It uses `python:3.10` as its base image and requires the `cloudpathlib==0.10.0` package to interact with GCS.\n\nThe pipeline's control flow is straightforward:\n-   The `_save_message_to_file` component is executed directly with the `message` and `gcs_filepath` parameters passed to the pipeline.\n\nThe pipeline definition includes `kfp` and `kfp.v2.dsl` for pipeline and component creation. The `cloudpathlib` library is used within the component for GCS interactions.", "completion": "# Copyright 2022 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Sample Kubeflow pipeline.\"\"\"\n\n\nimport kfp\nfrom kfp.v2 import dsl\n\n\n@dsl.component(base_image=\"python:3.10\", packages_to_install=[\"cloudpathlib==0.10.0\"])\ndef _save_message_to_file(message: str, gcs_filepath: str) -> None:\n    \"\"\"Saves a given message to a given file in GCS.\"\"\"\n    import cloudpathlib as cpl\n\n    with cpl.CloudPath(gcs_filepath).open(\"w\") as fp:\n        fp.write(message)\n\n\n@kfp.dsl.pipeline(name=\"sample-pipeline\")\ndef pipeline(message: str, gcs_filepath: str) -> None:\n    \"\"\"Sample Kubeflow pipeline definition.\"\"\"\n    _save_message_to_file(message, gcs_filepath)"}
{"prompt": "Generate a Kubeflow Pipeline named `train-plot-pipeline` that performs the following steps:\n\nThe pipeline will consist of two components:\n\n1.  **`traintest_op`**: This component is loaded from an external URL (`https://raw.githubusercontent.com/pavan-kumar-99/ml-ops/master/components/test-train.yaml`). Its function is to split a dataset into training and testing sets.\n\n2.  **`TrainPlot`**: This component is a Python function-based component that uses `sklearn` for model training and evaluation, and `matplotlib` for plotting.\n    *   **Functionality**:\n        *   It initializes and trains a `LinearRegression` model using `X_train` and `y_train`.\n        *   It makes predictions on `X_test` to get `y_pred`.\n        *   It generates two scatter plots using `matplotlib.pyplot`:\n            *   \"Salary vs Experience (Training set)\" showing `X_train` vs `y_train` and the regression line.\n            *   \"Salary vs Experience (Test set)\" showing `X_test` vs `y_test` and the regression line (using the training set's regression line for comparison).\n        *   It calculates several regression metrics: Mean Squared Error (MSE), Mean Absolute Error (MAE), and Explained Variance Score (EVS) using `sklearn.metrics`.\n        *   It prints `X_train`, a `params` variable (which is not explicitly defined in the provided snippet's `TrainPlot` but implied to be an input or global), and the calculated metrics.\n    *   **Inputs/Outputs**: The provided code snippet for `TrainPlot` implicitly expects `X_train`, `y_train`, `X_test`, and `y_test` to be available in its scope. These would typically be outputs from the preceding `traintest_op` component. The component itself does not explicitly define formal KFP inputs/outputs in the provided `create_component_from_func` decorator usage.\n    *   **Tools/Libraries**: `sklearn` (for `LinearRegression`, `f1_score`, `precision_score`, `recall_score`, `confusion_matrix`, `mean_squared_error`, `mean_absolute_error`, `explained_variance_score`), `matplotlib.pyplot`.\n\n**Control Flow**:\n\n*   The `TrainPlot` component should run `after` the `traintest_op` component has successfully completed, implicitly receiving the training and testing data splits as inputs from `traintest_op`.\n\nThe pipeline should be defined using the `@dsl.pipeline` decorator, and the Python components should be defined using `kfp.components.create_component_from_func` or by loading external component definitions.", "completion": "import kfp\nimport kfp.dsl as dsl\nimport kfp.components as components\nfrom kfp.components import create_component_from_func\n\ntraintest_op = components.load_component_from_url(\n    'https://raw.githubusercontent.com/pavan-kumar-99/ml-ops/master/components/test-train.yaml'\n)\n\n@components.create_component_from_func\ndef TrainPlot():\n    from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, mean_squared_error, mean_absolute_error, explained_variance_score\n    from sklearn.linear_model import LinearRegression\n    regressor = LinearRegression()\n    regressor.fit(X_train, y_train)\n\n    y_pred = regressor.predict(X_test)\n\n    plt.scatter(X_train, y_train, color = 'red')\n    plt.plot(X_train, regressor.predict(X_train), color = 'blue')\n    plt.title('Salary vs Experience (Training set)')\n    plt.xlabel('Years of Experience')\n    plt.ylabel('Salary')\n    plt.show()\n\n    plt.plot(y_pred, y_test)\n\n    plt.scatter(X_test, y_test, color = 'red')\n    plt.plot(X_train, regressor.predict(X_train), color = 'blue')\n    plt.title('Salary vs Experience (Test set)')\n    plt.xlabel('Years of Experience')\n    plt.ylabel('Salary')\n    plt.show()\n\n    mse = mean_squared_error(y_test, y_pred)\n    mae = mean_absolute_error(y_test, y_pred)\n    evs = explained_variance_score(y_test, y_pred) \n\n\n    metrics = {\"mse\":mse,\n            \"mae\":mae,\n            \"evs\":evs\n    }\n\n    print(X_train)\n    print(params)\n    print(metrics)\n   \nif __name__ == '__main__':\n    create_component_from_func(func=TrainPlot,base_image='python:slim',output_component_file='./components/train-plot.yaml')"}
{"prompt": "Generate a Kubeflow Pipeline named `echo-pipeline` that performs the following steps:\n\nThe pipeline has a single component.\n\n**Components:**\n\n1.  **`echo` component:**\n    *   **Function:** This component is designed to return a simple string, \"hello, world\".\n    *   **Outputs:** It returns a string value.\n\n**Pipeline Parameters:**\n\nThe pipeline accepts the following input parameters:\n\n*   `no_default_param`: An integer (int) with no default value.\n*   `int_param`: An integer (int) with a default value of `1`.\n*   `float_param`: A floating-point number (float) with a default value of `1.5`.\n*   `str_param`: A string (str) with a default value of `\"string_value\"`.\n*   `bool_param`: A boolean (bool) with a default value of `True`.\n*   `list_param`: A list of integers (`List[int]`) with a default value of `[1, 2, 3]`.\n*   `dict_param`: A dictionary with string keys and integer values (`Dict[str, int]`) with a default value of `{\"key\": 4}`.\n\n**Control Flow:**\n\n*   The pipeline simply calls the `echo` component. There are no dependencies or complex control flow mechanisms (like `after`, `parallelFor`, or conditional execution).\n\n**Libraries/Tools:**\n\n*   The pipeline utilizes the `kfp` library for defining components and pipelines.\n*   Standard Python `typing` module for type hints (e.g., `List`, `Dict`).", "completion": "import os\nfrom typing import Dict, List\n\nimport pytest\nfrom kfp import compiler as compiler_v1\nfrom kfp import dsl as dsl_v1\nfrom kfp.v2 import compiler, dsl\n\nfrom kfp_toolbox.pipeline_parser import Parameter, parse_pipeline_package\n\n\nclass TestParsePipelinePackage:\n    def test_v1(self, tmp_path):\n        @dsl.component()\n        def echo() -> str:\n            return \"hello, world\"\n\n        @dsl.pipeline(name=\"echo-pipeline\")\n        def echo_pipeline(\n            no_default_param: int,\n            int_param: int = 1,\n            float_param: float = 1.5,\n            str_param: str = \"string_value\",\n            bool_param: bool = True,\n            list_param: List[int] = [1, 2, 3],\n            dict_param: Dict[str, int] = {\"key\": 4},\n        ):\n            echo()\n\n        pipeline_path = os.fspath(tmp_path / \"pipeline.yaml\")\n        compiler_v1.Compiler(mode=dsl_v1.PipelineExecutionMode.V2_COMPATIBLE).compile(\n            pipeline_func=echo_pipeline, package_path=pipeline_path\n        )\n        pipeline = parse_pipeline_package(pipeline_path)\n\n        no_default_param = Parameter(name=\"no_default_param\", type=int)\n        int_param = Parameter(name=\"int_param\", type=int, default=1)\n        float_param = Parameter(name=\"float_param\", type=float, default=1.5)\n        str_param = Parameter(name=\"str_param\", type=str, default=\"string_value\")\n        bool_param = Parameter(name=\"bool_param\", type=str, default=\"True\")\n        list_param = Parameter(name=\"list_param\", type=str, default=\"[1, 2, 3]\")\n        dict_param = Parameter(name=\"dict_param\", type=str, default='{\"key\": 4}')\n\n        assert pipeline.name == \"echo-pipeline\"\n        assert len(pipeline.parameters) == 7\n        assert no_default_param in pipeline.parameters\n        assert int_param in pipeline.parameters\n        assert float_param in pipeline.parameters\n        assert str_param in pipeline.parameters\n        assert bool_param in pipeline.parameters\n        assert list_param in pipeline.parameters\n        assert dict_param in pipeline.parameters\n\n    def test(self, tmp_path):\n        @dsl.component()\n        def echo() -> str:\n            return \"hello, world\"\n\n        @dsl.pipeline(name=\"echo-pipeline\")\n        def echo_pipeline(\n            no_default_param: int,\n            int_param: int = 1,\n            float_param: float = 1.5,\n            str_param: str = \"string_value\",\n            bool_param: bool = True,\n            list_param: List[int] = [1, 2, 3],\n            dict_param: Dict[str, int] = {\"key\": 4},\n        ):\n            echo()\n\n        pipeline_path = os.fspath(tmp_path / \"pipeline.json\")\n        compiler.Compiler().compile(\n            pipeline_func=echo_pipeline, package_path=pipeline_path\n        )\n        pipeline = parse_pipeline_package(pipeline_path)\n\n        no_default_param = Parameter(name=\"no_default_param\", type=int)\n        int_param = Parameter(name=\"int_param\", type=int, default=1)\n        float_param = Parameter(name=\"float_param\", type=float, default=1.5)\n        str_param = Parameter(name=\"str_param\", type=str, default=\"string_value\")\n        bool_param = Parameter(name=\"bool_param\", type=str, default=\"True\")\n        list_param = Parameter(name=\"list_param\", type=str, default=\"[1, 2, 3]\")\n        dict_param = Parameter(name=\"dict_param\", type=str, default='{\"key\": 4}')\n\n        assert pipeline.name == \"echo-pipeline\"\n        assert len(pipeline.parameters) == 7\n        assert no_default_param in pipeline.parameters\n        assert int_param in pipeline.parameters\n        assert float_param in pipeline.parameters\n        assert str_param in pipeline.parameters\n        assert bool_param in pipeline.parameters\n        assert list_param in pipeline.parameters\n        assert dict_param in pipeline.parameters\n\n    def test_falsy_default_values_v1(self, tmp_path):\n        @dsl.component()\n        def echo() -> str:\n            return \"hello, world\"\n\n        @dsl.pipeline(name=\"echo-pipeline\")\n        def echo_pipeline(\n            int_param: int = 0,\n            float_param: float = 0.0,\n            str_param: str = \"\",\n        ):\n            echo()\n\n        pipeline_path = os.fspath(tmp_path / \"pipeline.yaml\")\n        compiler_v1.Compiler(mode=dsl_v1.PipelineExecutionMode.V2_COMPATIBLE).compile(\n            pipeline_func=echo_pipeline, package_path=pipeline_path\n        )\n        pipeline = parse_pipeline_package(pipeline_path)\n\n        int_param = Parameter(name=\"int_param\", type=int, default=0)\n        float_param = Parameter(name=\"float_param\", type=float, default=0.0)\n        str_param = Parameter(name=\"str_param\", type=str, default=\"\")\n\n        assert pipeline.name == \"echo-pipeline\"\n        assert len(pipeline.parameters) == 3\n        assert int_param in pipeline.parameters\n        assert float_param in pipeline.parameters\n        assert str_param in pipeline.parameters\n\n    def test_falsy_default_values(self, tmp_path):\n        @dsl.component()\n        def echo() -> str:\n            return \"hello, world\"\n\n        @dsl.pipeline(name=\"echo-pipeline\")\n        def echo_pipeline(\n            int_param: int = 0,\n            float_param: float = 0.0,\n            str_param: str = \"\",\n        ):\n            echo()\n\n        pipeline_path = os.fspath(tmp_path / \"pipeline.json\")\n        compiler.Compiler().compile(\n            pipeline_func=echo_pipeline, package_path=pipeline_path\n        )\n        pipeline = parse_pipeline_package(pipeline_path)\n\n        int_param = Parameter(name=\"int_param\", type=int, default=0)\n        float_param = Parameter(name=\"float_param\", type=float, default=0.0)\n        str_param = Parameter(name=\"str_param\", type=str, default=\"\")\n\n        assert pipeline.name == \"echo-pipeline\"\n        assert len(pipeline.parameters) == 3\n        assert int_param in pipeline.parameters\n        assert float_param in pipeline.parameters\n        assert str_param in pipeline.parameters\n\n    def test_no_parameters_v1(self, tmp_path):\n        @dsl.component()\n        def echo() -> str:\n            return \"hello, world\"\n\n        @dsl.pipeline(name=\"echo-pipeline\")\n        def echo_pipeline():\n            echo()\n\n        pipeline_path = os.fspath(tmp_path / \"pipeline.yaml\")\n        compiler_v1.Compiler(mode=dsl_v1.PipelineExecutionMode.V2_COMPATIBLE).compile(\n            pipeline_func=echo_pipeline, package_path=pipeline_path\n        )\n\n        pipeline = parse_pipeline_package(pipeline_path)\n        assert pipeline.name == \"echo-pipeline\"\n        assert len(pipeline.parameters) == 0\n\n    def test_no_parameters(self, tmp_path):\n        @dsl.component()\n        def echo() -> str:\n            return \"hello, world\"\n\n        @dsl.pipeline(name=\"echo-pipeline\")\n        def echo_pipeline():\n            echo()\n\n        pipeline_path = os.fspath(tmp_path / \"pipeline.json\")\n        compiler.Compiler().compile(\n            pipeline_func=echo_pipeline, package_path=pipeline_path\n        )\n\n        pipeline = parse_pipeline_package(pipeline_path)\n        assert pipeline.name == \"echo-pipeline\"\n        assert len(pipeline.parameters) == 0\n\n    def test_empty_file(self, tmp_path):\n        pipeline = \"\"\n        pipeline_path = os.fspath(tmp_path / \"pipeline.json\")\n        with open(pipeline_path, \"w\") as f:\n            f.write(pipeline)\n\n        with pytest.raises(ValueError) as exc_info:\n            parse_pipeline_package(pipeline_path)\n\n        assert str(exc_info.value) == f\"invalid schema: {pipeline_path}\"\n\n    def test_invalid_schema(self, tmp_path):\n        pipeline = \"\"\"\n            {\n                \"invalid_schema\": {}\n            }\n        \"\"\"\n        pipeline_path = os.fspath(tmp_path / \"pipeline.json\")\n        with open(pipeline_path, \"w\") as f:\n            f.write(pipeline)\n\n        with pytest.raises(ValueError) as exc_info:\n            parse_pipeline_package(pipeline_path)\n\n        assert str(exc_info.value) == f\"invalid schema: {pipeline_path}\""}
{"prompt": "Generate a Kubeflow Pipeline named `California Housing prediction Pipeline` that performs a machine learning workflow for California Housing predictions using an SGDRegressor.\n\nThe pipeline consists of **four components**:\n\n1.  **`Download and preprocess data`**:\n    *   **Function**: Downloads the California Housing dataset, preprocesses it, and splits it into training and testing sets.\n    *   **Inputs**:\n        *   `output_path`: A string specifying the base output directory for generated files.\n        *   `run_date`: A string representing the current timestamp, used for organizing output files.\n        *   `test_size`: A float specifying the proportion of the dataset to be used as test data.\n    *   **Outputs**:\n        *   `x_train`: Path to the preprocessed training features (NumPy array).\n        *   `x_test`: Path to the preprocessed testing features (NumPy array).\n        *   `y_train`: Path to the preprocessed training labels (NumPy array).\n        *   `y_test`: Path to the preprocessed testing labels (NumPy array).\n    *   **Tool/Library**: Uses a custom Docker image `dzieciolfilipit/kf_ch_preprocess_data:1.1`.\n\n2.  **`Train SGD Regressor`**:\n    *   **Function**: Trains an SGD Regressor model using the preprocessed training data.\n    *   **Inputs**:\n        *   `x_train`: Path to the training features from the `preprocess` component.\n        *   `y_train`: Path to the training labels from the `preprocess` component.\n        *   `output_path`: A string specifying the base output directory.\n        *   `run_date`: A string representing the current timestamp.\n    *   **Outputs**:\n        *   `model_path`: Path to the trained SGD Regressor model (pickle file).\n    *   **Tool/Library**: Uses a custom Docker image `dzieciolfilipit/kf_ch_train_model:1.1` and likely relies on `scikit-learn` internally for SGDRegressor.\n\n3.  **`Test model and get MSE metric`**:\n    *   **Function**: Evaluates the trained model on the test dataset and calculates the Mean Squared Error (MSE).\n    *   **Inputs**:\n        *   `x_test`: Path to the testing features from the `preprocess` component.\n        *   `y_test`: Path to the testing labels from the `preprocess` component.\n        *   `model_path`: Path to the trained model from the `train` component.\n        *   `output_path`: A string specifying the base output directory.\n        *   `run_date`: A string representing the current timestamp.\n    *   **Outputs**:\n        *   `mean_squared_error`: Path to a text file containing the calculated MSE value.\n    *   **Tool/Library**: Uses a custom Docker image `dzieciolfilipit/kf_ch_test_model:1.1` and likely relies on `scikit-learn` internally for MSE calculation.\n\n4.  **`Deploy model for inference`**:\n    *   **Function**: Deploys the trained model for inference, potentially conditionally based on the calculated MSE.\n    *   **Inputs**:\n        *   `model_path`: Path to the trained model from the `train` component.\n        *   `mse_path`: Path to the Mean Squared Error metric from the `test` component.\n    *   **Outputs**: None explicitly defined in the provided snippet.\n    *   **Tool/Library**: Uses a custom Docker image `dzieciolfilipit/kf_ch_deploy_model:1.1`.\n\n**Control Flow**:\n\n*   The `Download and preprocess data` component runs first.\n*   The `Train SGD Regressor` component depends on and runs **after** `Download and preprocess data`. It consumes `x_train` and `y_train` produced by the preprocessing step.\n*   The `Test model and get MSE metric` component depends on and runs **after** `Train SGD Regressor`. It consumes `x_test` and `y_test` from preprocessing and the `model_path` from training.\n*   The `Deploy model for inference` component depends on and runs **after** both `Train SGD Regressor` (for `model_path`) and `Test model and get MSE metric` (for `mse_path`).\n\n**Pipeline Parameters**:\n\nThe pipeline accepts the following input parameters:\n*   `test_size`: A float, likely for the data split.\n*   `output_path`: A string, for output file storage.\n*   `deployment_threshhold_mse`: A float, likely a threshold for conditional deployment.\n\nThe pipeline uses `datetime.now().strftime(\"%Y%m%d%H%M%S\")` to generate a `run_date` string internally for organizing outputs.", "completion": "import kfp\nfrom kfp import dsl\n\nimport argparse\n\nfrom datetime import datetime\n\n# Define components: preprocess, train, test, deploy\n\ndef preprocess_op(output_path, run_date, test_size):\n\n    return dsl.ContainerOp(\n        name = \"Download and preprocess data\",\n        image = \"dzieciolfilipit/kf_ch_preprocess_data:1.1\",\n        arguments = [\n            '--output_path', output_path,\n            '--run_date', run_date,\n            '--test_size', test_size\n        ],\n        file_outputs = {\n              'x_train': '{}{}/x_train.npy'.format(output_path, run_date)\n            , 'x_test' : '{}{}/x_test.npy'.format(output_path, run_date)\n            , 'y_train': '{}{}/y_train.npy'.format(output_path, run_date)\n            , 'y_test' : '{}{}/y_test.npy'.format(output_path, run_date)\n        }\n    )\n\ndef train_op(x_train, y_train, output_path, run_date):\n\n    return dsl.ContainerOp(\n        name = \"Train SGD Regressor\",\n        image = \"dzieciolfilipit/kf_ch_train_model:1.1\",\n        arguments = [\n            '--x_train', x_train,\n            '--y_train', y_train,\n            '--output_path', output_path,\n            '--run_date', run_date\n        ], \n        file_outputs = {\n            'model_path': '{}{}/model.pkl'.format(output_path, run_date)\n        }\n    )\n\ndef test_op(x_test, y_test, model_path, output_path, run_date):\n\n    return dsl.ContainerOp(\n        name = \"Test model and get MSE metric\",\n        image = \"dzieciolfilipit/kf_ch_test_model:1.1\",\n        arguments = [\n            '--x_test', x_test,\n            '--y_test', y_test,\n            '--model_path', model_path,\n            '--output_path', output_path,\n            '--run_date', run_date\n        ],\n        file_outputs = {\n            'mean_squared_error': '{}{}/output.txt'.format(output_path, run_date)\n        }\n    )\n\ndef deploy_model_op(model_path, mse_path):\n\n    return dsl.ContainerOp(\n        name = \"Deploy model for inference\",\n        image = \"dzieciolfilipit/kf_ch_deploy_model:1.1\",\n        arguments = [\n            '--model_path', model_path,\n            '--mse_path', mse_path\n        ]\n    )\n\n\n# Describe Pipeline\n@dsl.pipeline(\n    name = \"California Housing prediction Pipeline\",\n    description = \"Sample Pipeline for California Housing predictions using SGDRegressor regression model\"\n)\n# Define Pipeline steps\ndef pipeline(test_size, output_path, deployment_threshhold_mse):\n\n    # Use current date without separators\n    # Sample date: 20220529160603\n    run_date = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n\n    _preprocess_op = preprocess_op(\n        dsl.InputArgumentPath(output_path),\n        dsl.InputArgumentPath(run_date),\n        dsl.InputArgumentPath(test_size)\n    )\n\n    _train_op = train_op(\n        dsl.InputArgumentPath(_preprocess_op.outputs['x_train']),\n        dsl.InputArgumentPath(_preprocess_op.outputs['y_train']),\n        dsl.InputArgumentPath(output_path),\n        dsl.InputArgumentPath(run_date)\n    ).after(_preprocess_op)\n\n    _test_op = test_op(\n        dsl.InputArgumentPath(_preprocess_op.outputs['x_test']),\n        dsl.InputArgumentPath(_preprocess_op.outputs['y_test']),\n        dsl.InputArgumentPath(_train_op.outputs['model_path']),\n        dsl.InputArgumentPath(output_path),\n        dsl.InputArgumentPath(run_date)\n    ).after(_test_op)\n\n    # Define Production deployment condition, retrain otherwise\n    with dsl.Condition(_test_op.outputs['mean_squared_error'] < deployment_threshhold_mse):\n\n        deploy_model_op(\n            dsl.InputArgumentPath(_train_op.outputs['model_path']),\n            dsl.InputArgumentPath(_test_op.outputs['mean_squared_error'])\n        ).after(_test_op)\n\n    with dsl.Condition(_test_op.outputs['mean_squared_error'] >= deployment_threshhold_mse):\n\n        client = kfp.Client(host='http://ml-pipeline-ui:80')\n        client.create_run_from_pipeline_func(\n            pipeline, \n            arguments = {\n                'test_size': args.test_size,\n                'output_path': args.output_path,\n                'deployment_threshhold_mse': args.deployment_threshhold_mse\n        }\n    )\n\n\n\n# Add arguments to main run\nparser = argparse.ArgumentParser()\n\nparser.add_argument('--test_size', type=float)\nparser.add_argument('--output_path')\nparser.add_argument('--deployment_threshhold_mse', type=float)\n\nargs = parser.parse_args()\n\n# Create Kubeflow Client object and run Pipeline Function\nclient = kfp.Client(host='http://ml-pipeline-ui:80')\nclient.create_run_from_pipeline_func(\n    pipeline, \n    arguments = {\n        'test_size': args.test_size,\n        'output_path': args.output_path,\n        'deployment_threshhold_mse': args.deployment_threshhold_mse\n    }\n)"}
{"prompt": "Generate a Kubeflow Pipeline named `First Pipeline` that performs classification using Decision Tree and Logistic Regression.\n\nThis pipeline consists of **four** components:\n1.  **`download`**: This component is loaded from the YAML manifest `download_data/download_data.yaml`. Its function is to download the necessary dataset. It takes no explicit inputs and produces a single output which is passed to subsequent components.\n2.  **`decision_tree`**: This component is loaded from the YAML manifest `decision_tree/decision_tree.yaml`. Its function is to train and evaluate a Decision Tree model. It takes the output of the `download` component as its input and produces a single output representing the accuracy (a float value) of the Decision Tree model.\n3.  **`logistic_regression`**: This component is loaded from the YAML manifest `logistic_regression/logistic_regression.yaml`. Its function is to train and evaluate a Logistic Regression model. It takes the output of the `download` component as its input and produces a single output representing the accuracy (a float value) of the Logistic Regression model.\n4.  **`show_results`**: This is a custom Python-based component (defined using `@func_to_container_op`). Its function is to display the accuracy results from both the Decision Tree and Logistic Regression models. It takes two float inputs: `decision_tree` (the accuracy from the `decision_tree` component) and `logistic_regression` (the accuracy from the `logistic_regression` component). It prints these values to standard output.\n\nThe control flow of the pipeline is as follows:\n- The `download` component runs first.\n- Once the `download` component completes, the `decision_tree` and `logistic_regression` components run **in parallel**, both taking the output of `download` as their input.\n- After both `decision_tree` and `logistic_regression` components have completed, the `show_results` component runs, taking the respective accuracy outputs from `decision_tree` and `logistic_regression` as its inputs.\n\nThe pipeline utilizes the `kfp` library for defining and compiling the pipeline. It loads external component definitions from YAML files.", "completion": "import kfp\nfrom kfp import dsl\nfrom kfp.components import func_to_container_op\n\n@func_to_container_op\ndef show_results(decision_tree : float, logistic_regression : float) -> None:\n    # Given the outputs from decision_tree and logistic regression components\n    # the results are shown.\n\n    print(f\"Decision tree (accuracy): {decision_tree}\")\n    print(f\"Logistic regression (accuracy): {logistic_regression}\")\n\n\n@dsl.pipeline(name='First Pipeline', description='Applies Decision Tree and Logistic Regression for classification problem.')\ndef first_pipeline():\n\n    # Loads the yaml manifest for each component\n    download = kfp.components.load_component_from_file('download_data/download_data.yaml')\n    decision_tree = kfp.components.load_component_from_file('decision_tree/decision_tree.yaml')\n    logistic_regression = kfp.components.load_component_from_file('logistic_regression/logistic_regression.yaml')\n\n    # Run download_data task\n    download_task = download()\n\n    # Run tasks \"decison_tree\" and \"logistic_regression\" given\n    # the output generated by \"download_task\".\n    decision_tree_task = decision_tree(download_task.output)\n    logistic_regression_task = logistic_regression(download_task.output)\n\n    # Given the outputs from \"decision_tree\" and \"logistic_regression\"\n    # the component \"show_results\" is called to print the results.\n    show_results(decision_tree_task.output, logistic_regression_task.output)\n\n\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(first_pipeline, 'FirstPipeline.yaml')\n    # kfp.Client().create_run_from_pipeline_func(basic_pipeline, arguments={})"}
{"prompt": "Generate a Kubeflow Pipeline named `pipeline-with-after` that performs the following steps:\n\nThis pipeline consists of **three** components, all of which use the same reusable `Print Text` component.\n\nHere's the detailed breakdown of each component and the pipeline's control flow:\n\n**Reusable Component Definition:**\nFirst, define a reusable Kubeflow component named `Print Text`.\n*   **Function:** This component takes a single string input named `text` and prints it to standard output.\n*   **Implementation:** It uses an `alpine` container and executes a shell command `echo \"$0\"` where `$0` is replaced by the input `text`.\n\n**Pipeline Components and Control Flow:**\n\n1.  **`task1`**\n    *   **Function:** This task uses the `Print Text` component to print the string '1st task'.\n    *   **Inputs:** `text='1st task'`\n    *   **Dependencies:** None (it runs first).\n\n2.  **`task2`**\n    *   **Function:** This task uses the `Print Text` component to print the string '2nd task'.\n    *   **Inputs:** `text='2nd task'`\n    *   **Dependencies:** It must run **after** `task1` completes successfully.\n\n3.  **`task3`**\n    *   **Function:** This task uses the `Print Text` component to print the string '3rd task'.\n    *   **Inputs:** `text='3rd task'`\n    *   **Dependencies:** It must run **after** both `task1` and `task2` complete successfully.\n\nThe pipeline uses the `@dsl.pipeline` and `@component` decorators from the `kfp` library. The `compiler.Compiler()` is used to compile the pipeline function `my_pipeline` into a Kubeflow pipeline package, specifying `dummy_root` as the pipeline root and saving the output to a JSON file.", "completion": "# Copyright 2020 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom kfp import components\nfrom kfp import dsl\nfrom kfp import compiler\n\ncomponent_op = components.load_component_from_text(\"\"\"\nname: Print Text\ninputs:\n- {name: text, type: String}\nimplementation:\n  container:\n    image: alpine\n    command:\n    - sh\n    - -c\n    - |\n      set -e -x\n      echo \"$0\"\n    - {inputValue: text}\n\"\"\")\n\n\n@dsl.pipeline(name='pipeline-with-after')\ndef my_pipeline():\n    task1 = component_op(text='1st task')\n    task2 = component_op(text='2nd task').after(task1)\n    task3 = component_op(text='3rd task').after(task1, task2)\n\n\nif __name__ == '__main__':\n    compiler.Compiler().compile(\n        pipeline_func=my_pipeline,\n        pipeline_root='dummy_root',\n        output_path=__file__ + '.json')"}
{"prompt": "Generate a Kubeflow Pipeline named `data_passing_pipeline` that demonstrates various data passing mechanisms.\n\nThe pipeline comprises **13 components**:\n\n**Producer Components:**\n1.  **`produce_anything`**: This component writes the string \"produce_anything\" to an `OutputPath()` named `data_path`.\n2.  **`produce_something`**: This component writes the string \"produce_something\" to an `OutputPath(\"Something\")` named `data_path`.\n3.  **`produce_something2`**: This component returns the string \"produce_something2\" as an output of type `Something`.\n4.  **`produce_string`**: This component returns the string \"produce_string\" as an output of type `str`.\n\n**Consumer Components (Consume as Value):**\n1.  **`consume_anything_as_value`**: This component takes an input `data` of unspecified type and prints its value.\n2.  **`consume_something_as_value`**: This component takes an input `data` of type `Something` and prints its value.\n3.  **`consume_string_as_value`**: This component takes an input `data` of type `str` and prints its value.\n\n**Consumer Components (Consume as File):**\n1.  **`consume_anything_as_file`**: This component takes an `InputPath()` named `data_path`, reads its content, and prints it.\n2.  **`consume_something_as_file`**: This component takes an `InputPath('Something')` named `data_path`, reads its content, and prints it.\n3.  **`consume_string_as_file`**: This component takes an `InputPath(str)` named `data_path`, reads its content, and prints it.\n\n**Pipeline Structure and Data Flow:**\n\nThe pipeline accepts three parameters:\n*   `anything_param` (default: \"anything_param\")\n*   `something_param` of type `Something` (default: \"something_param\")\n*   `string_param` of type `str` (default: \"string_param\")\n\nThe pipeline performs the following steps:\n\n**1. Producer Task Instantiation:**\n*   A task `produced_anything` is created from the `produce_anything` component. Its output is captured.\n*   A task `produced_something` is created from the `produce_something` component. Its output is captured.\n*   The code snippet provided is incomplete for `produce_something2` and `produce_string`. Assuming they are also instantiated and their outputs captured for subsequent consumption.\n\n**2. Data Passing Demonstrations (Incomplete from provided code, but based on the problem description):**\n\nThe prompt should complete the logic for all 6 data passing scenarios mentioned in the original problem description. For example, for the \"Consume as Value\" and \"Consume as File\" components, the prompts would describe how they are called with:\n\n*   Constant values\n*   Pipeline parameters\n*   Outputs from upstream producer components\n\n**Control Flow:**\nAll tasks are expected to run in parallel by default, with data dependencies dictating the execution order (e.g., a consumer task will only run after its required producer task has completed). There are no explicit `after` or `parallelFor` constructs mentioned in the provided snippet.\n\n**Libraries/Tools:**\nThe pipeline uses `kfp` for pipeline definition and component creation, and standard Python file I/O operations.", "completion": "# This pipeline demonstrates and verifies all ways of data passing supported by KFP.\n\n# %% [markdown]\n# KFP has simple data passing model. There are three different kinds of arguments and tw different ways to consume an input argument. All combinations are supported.\n\n# Any input can be consumed:\n# * As value (`inputValue` placeholder)\n# * As file (`inputPath` placeholder).\n\n# Input argument can come from:\n# * Constant value\n# * Pipeline parameter\n# * Upstream component output\n\n# Combining these options there are 6 end-to-end data passing cases, each of which works regardless of type:\n\n# 1. Pass constant value which gets consumed as value. (Common)\n# 2. Pass constant value which gets consumed as file. (Often used in test pipelines)\n# 3. Pass pipeline parameter that gets consumed as value. (Common)\n# 4. Pass pipeline parameter that gets consumed as file. (Rare. Sometimes used with JSON parameters)\n# 5. Pass task output that gets consumed as value. (Common)\n# 6. Pass task output that gets consumed as file. (Common)\n\n# The only restriction on types is that when both upstream output and downstream input have types, the types must match.\n\n# %%\nimport kfp\nfrom kfp.components import create_component_from_func, InputPath, OutputPath\n\n\n# Components\n# Produce\n@create_component_from_func\ndef produce_anything(data_path: OutputPath()):\n    with open(data_path, \"w\") as f:\n        f.write(\"produce_anything\")\n\n\n@create_component_from_func\ndef produce_something(data_path: OutputPath(\"Something\")):\n    with open(data_path, \"w\") as f:\n        f.write(\"produce_something\")\n\n\n@create_component_from_func\ndef produce_something2() -> 'Something':\n    return \"produce_something2\"\n\n\n@create_component_from_func\ndef produce_string() -> str:\n    return \"produce_string\"\n\n\n# Consume as value\n@create_component_from_func\ndef consume_anything_as_value(data):\n    print(\"consume_anything_as_value: \" + data)\n\n\n@create_component_from_func\ndef consume_something_as_value(data: \"Something\"):\n    print(\"consume_something_as_value: \" + data)\n\n\n@create_component_from_func\ndef consume_string_as_value(data: str):\n    print(\"consume_string_as_value: \" + data)\n\n\n# Consume as file\n@create_component_from_func\ndef consume_anything_as_file(data_path: InputPath()):\n    with open(data_path) as f:\n        print(\"consume_anything_as_file: \" + f.read())\n\n\n@create_component_from_func\ndef consume_something_as_file(data_path: InputPath('Something')):\n    with open(data_path) as f:\n        print(\"consume_something_as_file: \" + f.read())\n\n\n@create_component_from_func\ndef consume_string_as_file(data_path: InputPath(str)):\n    with open(data_path) as f:\n        print(\"consume_string_as_file: \" + f.read())\n\n\n# Pipeline\n@kfp.dsl.pipeline(name='data_passing_pipeline')\ndef data_passing_pipeline(\n    anything_param=\"anything_param\",\n    something_param: \"Something\" = \"something_param\",\n    string_param: str = \"string_param\",\n):\n    produced_anything = produce_anything().output\n    produced_something = produce_something().output\n    produced_string = produce_string().output\n\n    # Pass constant value; consume as value\n    consume_anything_as_value(\"constant\")\n    consume_something_as_value(\"constant\")\n    consume_string_as_value(\"constant\")\n\n    # Pass constant value; consume as file\n    consume_anything_as_file(\"constant\")\n    consume_something_as_file(\"constant\")\n    consume_string_as_file(\"constant\")\n\n    # Pass pipeline parameter; consume as value\n    consume_anything_as_value(anything_param)\n    consume_anything_as_value(something_param)\n    consume_anything_as_value(string_param)\n    consume_something_as_value(anything_param)\n    consume_something_as_value(something_param)\n    consume_string_as_value(anything_param)\n    consume_string_as_value(string_param)\n\n    # Pass pipeline parameter; consume as file\n    consume_anything_as_file(anything_param)\n    consume_anything_as_file(something_param)\n    consume_anything_as_file(string_param)\n    consume_something_as_file(anything_param)\n    consume_something_as_file(something_param)\n    consume_string_as_file(anything_param)\n    consume_string_as_file(string_param)\n\n    # Pass task output; consume as value\n    consume_anything_as_value(produced_anything)\n    consume_anything_as_value(produced_something)\n    consume_anything_as_value(produced_string)\n    consume_something_as_value(produced_anything)\n    consume_something_as_value(produced_something)\n    consume_string_as_value(produced_anything)\n    consume_string_as_value(produced_string)\n\n    # Pass task output; consume as file\n    consume_anything_as_file(produced_anything)\n    consume_anything_as_file(produced_something)\n    consume_anything_as_file(produced_string)\n    consume_something_as_file(produced_anything)\n    consume_something_as_file(produced_something)\n    consume_string_as_file(produced_anything)\n    consume_string_as_file(produced_string)\n\n\nif __name__ == \"__main__\":\n    kfp_endpoint = None\n    kfp.Client(host=kfp_endpoint).create_run_from_pipeline_func(\n        data_passing_pipeline, arguments={})"}
{"prompt": "Generate a Kubeflow Pipeline named `loop_consume_upstream` that performs the following steps:\n\nThis pipeline consists of **four** components: `split_input`, `create_file`, `read_file`, and `print_input`.\n\n**Component Breakdown:**\n\n1.  **`split_input`**:\n    *   **Function:** Takes a comma-separated string `input` and returns a `list` of strings by splitting the input.\n    *   **Inputs:** `input` (string).\n    *   **Outputs:** A `list` of strings.\n\n2.  **`create_file`**:\n    *   **Function:** Creates a file at the specified `Output[Artifact]` path and writes the `content` string into it.\n    *   **Inputs:** `file` (Output[Artifact]), `content` (string).\n    *   **Outputs:** Writes to the provided `Output[Artifact]`.\n\n3.  **`read_file`**:\n    *   **Function:** Reads the content of an `Input[Artifact]` file, prints its content to standard output, and then returns the file's path as a string.\n    *   **Inputs:** `file` (Input[Artifact]).\n    *   **Outputs:** A string (the file path).\n\n4.  **`print_input`**:\n    *   **Function:** Iterates through a given `list` of strings and prints each item prefixed with \"Input item: \".\n    *   **Inputs:** `input` (list).\n    *   **Outputs:** None (prints to stdout).\n\n**Pipeline Control Flow:**\n\n1.  The pipeline starts by invoking the `split_input` component, providing the string `'component1,component2,component3'` as its `input`. This operation is named `model_ids_split_op`.\n2.  `model_ids_split_op` has caching disabled and its display name is set to `'same display mame'`.\n3.  A `dsl.ParallelFor` loop is initiated, iterating over the `output` of `model_ids_split_op`. Each item from this list is assigned to the loop variable `model_id`.\n4.  Inside the `ParallelFor` loop:\n    *   The `create_file` component is called, providing the current `model_id` from the loop as its `content`. This operation is named `create_file_op`.\n    *   `create_file_op` has caching disabled and its display name is set to `'same display name'`.\n    *   The `read_file` component is then called, taking the `file` output from the `create_file_op` as its input. This demonstrates consuming an output from an upstream component within the same loop iteration. This operation is named `read_file_op`.\n    *   `read_file_op` has caching disabled and its display name is set to `'same display name'`.\n5.  After the `ParallelFor` loop completes (i.e., `print_input_op` runs after the entire parallel execution), the `print_input` component is called. It takes the original `output` from the `model_ids_split_op` (which was the input to the `ParallelFor` loop) as its `input`. This operation is named `print_input_op`.\n6.  `print_input_op` has caching disabled and its display name is set to `'same display name'`.\n\n**Libraries Used:**\n\n*   `kfp` (Kubeflow Pipelines SDK)\n*   `kfp.dsl` (for defining components, pipelines, and types like `Artifact`, `Input`, `Output`, `ParallelFor`).", "completion": "# This pipeline tests the ability to consume outputs from upstream components in\n# a loop context as well as having the inputs resolve when set_display_name is\n# used within the pipeline.\nfrom kfp import Client\nfrom kfp import dsl\nfrom kfp.dsl import Artifact\nfrom kfp.dsl import Input\nfrom kfp.dsl import Output\n\n\n@dsl.component\ndef split_input(input: str) -> list:\n    return input.split(',')\n\n\n@dsl.component\ndef create_file(file: Output[Artifact], content: str):\n    with open(file.path, 'w') as f:\n        f.write(content)\n\n\n@dsl.component\ndef read_file(file: Input[Artifact]) -> str:\n    with open(file.path, 'r') as f:\n        print(f.read())\n    return file.path\n\n\n@dsl.component\ndef print_input(input: list):\n    for item in input:\n        print(f'Input item: {item}')\n\n\n@dsl.pipeline()\ndef loop_consume_upstream():\n    model_ids_split_op = split_input(input='component1,component2,component3')\n    model_ids_split_op.set_caching_options(False)\n    model_ids_split_op.set_display_name('same display mame')\n    with dsl.ParallelFor(model_ids_split_op.output) as model_id:\n        create_file_op = create_file(content=model_id)\n        create_file_op.set_caching_options(False)\n        create_file_op.set_display_name('same display name')\n        # Consume the output from a op in the loop iteration DAG context\n        read_file_op = read_file(file=create_file_op.outputs['file'])\n        read_file_op.set_caching_options(False)\n        read_file_op.set_display_name('same display name')\n\n    print_input_op = print_input(input=model_ids_split_op.output)\n    print_input_op.set_caching_options(False)\n    print_input_op.set_display_name('same display name')\n\n\nif __name__ == '__main__':\n    client = Client()\n    run = client.create_run_from_pipeline_func(loop_consume_upstream)"}
{"prompt": "Generate a Kubeflow Pipeline named `stackdemos_secrets-tetst-01_training_pipeline` that performs the following steps:\n\nThis pipeline consists of **three** components.\n\n1.  **`training_op`**: This component is a generic container operation designed to execute Python training scripts.\n    *   **Inputs**:\n        *   `script`: (Required) The path to the Python script to be executed.\n        *   `image`: (Optional) The Docker image to use for the container. If not provided and running in an IPython environment, it attempts to use a global `TRAINING_IMAGE` variable.\n        *   `arguments`: (Optional, list) A list of command-line arguments to pass to the script.\n        *   `file_outputs`: (Optional, dictionary) A dictionary specifying file paths within the container that should be captured as output artifacts.\n    *   **Function**: It creates a `ContainerOp` with a sanitized name derived from the script name, uses the specified image, and executes the Python script with provided arguments.\n    *   **Tools/Libraries**: Uses `os` and `re` for path and string manipulation.\n\n2.  **`http_download_op`**: This component downloads a file from a URL and performs an MD5 checksum verification.\n    *   **Inputs**:\n        *   `url`: (Required) The URL of the file to download.\n        *   `download_to`: (Required) The local path where the file should be downloaded.\n        *   `md5sum`: (Required) The expected MD5 checksum of the downloaded file.\n    *   **Function**: It uses a `curl` command within an `appropriate/curl` Docker image to download the file. Before downloading, it checks if a file with the same MD5 sum already exists at `download_to` and skips the download if it does. If not, it downloads the file and creates necessary directories.\n    *   **Tools/Libraries**: Uses `curl` and `md5sum` (via `awk`).\n\n3.  **`_is_ipython`**: This is a helper function, not a direct Kubeflow component.\n    *   **Function**: It checks if the current execution environment is an IPython notebook.\n    *   **Tools/Libraries**: Uses `IPython`.\n\n**Control Flow**:\nThe provided code defines the individual components (`training_op`, `http_download_op`) and a helper function (`_is_ipython`). The pipeline definition itself (i.e., how these components are orchestrated with `after`, `parallelFor`, etc.) is not present in this snippet. The prompt assumes these are callable functions that would be used within an `@dsl.pipeline` decorated function to construct the workflow. The pipeline's name is inferred from the file path.", "completion": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef training_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('TRAINING_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=['/usr/local/bin/python', script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )"}
{"prompt": "Generate a Kubeflow Pipeline named `metrics_visualization_v1_pipeline` that performs metrics visualization.\n\nThe pipeline consists of five components, all executed in parallel:\n\n1.  **`table-visualization`**: This component likely generates a table for visualization. Its output is an artifact named `mlpipeline_ui_metadata`.\n\n2.  **`markdown-visualization`**: This component likely generates Markdown content for visualization. Its output is an artifact named `mlpipeline_ui_metadata`.\n\n3.  **`roc-visualization`**: This component is responsible for visualizing a Receiver Operating Characteristic (ROC) curve. Its output is an artifact named `mlpipeline_ui_metadata`.\n\n4.  **`html-visualization`**: This component generates HTML content for visualization. Its output is an artifact named `mlpipeline_ui_metadata`.\n\n5.  **`confusion-visualization`**: This component visualizes a confusion matrix. Its output is an artifact named `mlpipeline_ui_metadata`.\n\nAll five components appear to operate independently and in parallel, with no explicit dependencies or control flow specified between them beyond their individual execution. The common output artifact name `mlpipeline_ui_metadata` suggests that each component contributes specific UI metadata for display in the Kubeflow Pipelines UI.", "completion": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp_server_api\nimport unittest\nfrom pprint import pprint\nfrom .metrics_visualization_v1 import metrics_visualization_v1_pipeline\nfrom .util import KfpMlmdClient, run_pipeline_func, TestCase\n\nimport kfp\nimport kfp_server_api\n\n\ndef verify(\n    run: kfp_server_api.ApiRun, mlmd_connection_config, argo_workflow_name: str,\n    **kwargs\n):\n    t = unittest.TestCase()\n    t.maxDiff = None  # we always want to see full diff\n    t.assertEqual(run.status, 'Succeeded')\n    client = KfpMlmdClient(mlmd_connection_config=mlmd_connection_config)\n    tasks = client.get_tasks(argo_workflow_name=argo_workflow_name)\n    pprint(tasks)\n\n    table_visualization = tasks['table-visualization']\n    markdown_visualization = tasks['markdown-visualization']\n    roc_visualization = tasks['roc-visualization']\n    html_visualization = tasks['html-visualization']\n    confusion_visualization = tasks['confusion-visualization']\n\n    t.assertEqual(table_visualization.get_dict()['outputs']['artifacts'][0]['name'],\n                  'mlpipeline_ui_metadata')\n    t.assertEqual(markdown_visualization.get_dict()['outputs']['artifacts'][0]['name'],\n                  'mlpipeline_ui_metadata')\n    t.assertEqual(roc_visualization.get_dict()['outputs']['artifacts'][0]['name'],\n                  'mlpipeline_ui_metadata')\n    t.assertEqual(html_visualization.get_dict()['outputs']['artifacts'][0]['name'],\n                  'mlpipeline_ui_metadata')\n    t.assertEqual(confusion_visualization.get_dict()['outputs']['artifacts'][0]['name'],\n                  'mlpipeline_ui_metadata')\n\n\nrun_pipeline_func([TestCase(pipeline_func=metrics_visualization_v1_pipeline,\n                            mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE\n                            )])"}
{"prompt": "Generate a Kubeflow Pipeline named `fail-pipeline` that performs the following:\n\nThis pipeline consists of a single component:\n1.  **`fail`**: This component's function is to simulate a failure by immediately exiting with a non-zero status code (specifically, `sys.exit(1)`). It does not take any inputs or produce any outputs.\n\nThe control flow is straightforward:\n- The `fail_task` is an instance of the `fail` component and is executed directly within the pipeline.\n\nNo specific external tools or libraries beyond standard Python `sys` module for exiting are used within the component logic visible in the definition.", "completion": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Fail pipeline.\"\"\"\n\nfrom kfp import dsl\n\n\n@dsl.component\ndef fail():\n    '''Fails'''\n    import sys\n    sys.exit(1)\n\n\n@dsl.pipeline(name='fail-pipeline')\ndef fail_pipeline():\n    fail_task = fail()"}
{"prompt": "Generate a Kubeflow Pipeline named `Artifact Pipeline` that performs the following steps:\n\nThis pipeline consists of **two components**:\n\n1.  **`create_artifact`**:\n    *   **Function**: This component creates a Python `list` (represented as a string \"1, 2, 3, 4\") and serializes it using `pickle`.\n    *   **Outputs**: It produces one output: `my_artifact` of type `dsl.Output[dsl.Artifact]`, which stores the pickled data.\n    *   **Tools/Libraries**: Uses the `pickle` library for serialization.\n    *   **Base Image**: `image-registry.openshift-image-registry.svc:5000/openshift/python:latest`.\n\n2.  **`consume_artifact`**:\n    *   **Function**: This component reads and deserializes the artifact created by the `create_artifact` component, then prints its content.\n    *   **Inputs**: It consumes one input: `my_artifact` of type `dsl.Input[dsl.Artifact]`.\n    *   **Tools/Libraries**: Uses the `pickle` library for deserialization.\n    *   **Base Image**: `image-registry.openshift-image-registry.svc:5000/openshift/python:latest`.\n\n**Control Flow**:\n*   The `create_artifact` component runs first.\n*   The `consume_artifact` component runs *after* `create_artifact` and depends on its `my_artifact` output. Specifically, `consume_artifact` takes `create_artifact_task.outputs[\"my_artifact\"]` as its input.", "completion": "\"\"\"Example of a pipeline to demonstrate accessing secrets/config maps in a pipeline.\"\"\"\n\nimport os\n\nimport kfp.compiler\nfrom dotenv import load_dotenv\nfrom kfp import dsl\n\nload_dotenv(override=True)\n\nkubeflow_endpoint = os.environ[\"KUBEFLOW_ENDPOINT\"]\nbearer_token = os.environ[\"BEARER_TOKEN\"]\n\n\n@dsl.component(base_image=\"image-registry.openshift-image-registry.svc:5000/openshift/python:latest\")\ndef create_artifact(my_artifact: dsl.Output[dsl.Artifact]):\n    import pickle\n\n    artifact = \"1, 2, 3, 4\"\n\n    with open(my_artifact.path, \"bw\") as f:\n        pickle.dump(artifact, f)\n\n\n@dsl.component(base_image=\"image-registry.openshift-image-registry.svc:5000/openshift/python:latest\")\ndef consume_artifact(my_artifact: dsl.Input[dsl.Artifact]):\n    import pickle\n\n    with open(my_artifact.path, \"br\") as f:\n        artifact = pickle.load(f)  # noqa: S301\n\n    print(artifact)\n\n\n@kfp.dsl.pipeline(\n    name=\"Artifact Pipeline\",\n)\ndef artifact_pipeline():\n    create_artifact_task = create_artifact()\n    consume_artifact_task = consume_artifact(  # noqa: F841\n        my_artifact=create_artifact_task.outputs[\"my_artifact\"]\n    )\n\n\nif __name__ == \"__main__\":\n    client = kfp.Client(\n        host=kubeflow_endpoint,\n        existing_token=bearer_token,\n    )\n    client.create_run_from_pipeline_func(artifact_pipeline, arguments={}, experiment_name=\"artifact-example\")"}
{"prompt": "Generate a Kubeflow Pipeline named `Santander Customer Transaction Prediction Release Pipeline` that performs the release of a trained classification model for Santander customer transactions.\n\nThe pipeline has a dynamic number of components, with a base of **2 or 3 components**, depending on the `platform` variable:\n\n**Pipeline Parameters:**\n* `output`: String, used for path configuration.\n* `project`: String, typically representing a GCP project ID or Kubernetes cluster name.\n\n**Control Flow and Component Details:**\n\n1.  **Platform Conditional Logic (`if platform != 'GCP'`):**\n    *   **If `platform` is NOT 'GCP' (e.g., On-premise Kubernetes):**\n        *   **`create_pvc` (dsl.VolumeOp):**\n            *   **Function:** Creates a Persistent Volume Claim (PVC) named \"pipeline-pvc\" with `RWM` (ReadWriteMany) mode and \"1Gi\" size.\n            *   **Outputs:** `name` (of the created PVC).\n        *   **`checkout` (dsl.ContainerOp):**\n            *   **Function:** Clones the `https://github.com/kubeflow/pipelines.git` repository into a directory specified by the `output` pipeline parameter (e.g., `/pipelines`).\n            *   **Image:** `alpine/git:latest`\n            *   **Command:** `[\"git\", \"clone\", \"https://github.com/kubeflow/pipelines.git\", str(output) + \"/pipelines\"]`\n            *   **Dependencies:** Runs `after` `create_pvc`.\n            *   **Volume Mounts:** Mounts the PVC created by `create_pvc` at `local-storage` path, corresponding to the `output` parameter.\n\n2.  **`kubeflow_deploy_op` (Loaded Component):**\n    *   **Function:** Deploys the machine learning model to a serving infrastructure (likely TensorFlow Serving).\n    *   **Component Definition Source:** Loaded from `pipeline_steps/serving/deployer/component.yaml`.\n    *   **Dynamic Inputs based on `platform`:**\n        *   **If `platform` is 'GCP':**\n            *   `model_dir`: Set to `gs://kubeflow-pipelines-demo/tfx/0b22081a-ed94-11e9-81fb-42010a800160/santander-customer-transaction-prediction-95qxr-268134926/data/export/export`\n            *   `server_name`: Set to `kfdemo-service`\n        *   **If `platform` is NOT 'GCP':**\n            *   `cluster_name`: Set to the `project` pipeline parameter.\n            *   `model_dir`: Set to `gs://kubeflow-pipelines-demo/tfx/0b22081a-ed94-11e9-81fb-42010a800160/santander-customer-transaction-prediction-95qxr-268134926/data/export/export`\n            *   `pvc_name`: Set to the `name` output of the `create_pvc` component.\n            *   `server_name`: Set to `kfdemo-service`\n\n3.  **`webapp` (dsl.ContainerOp):**\n    *   **Function:** Launches a web application.\n    *   **Image:** `us.gcr.io/kf-pipelines/ml-pipeline-webapp-launcher:v0.3`\n    *   **Arguments:** `[\"--model_name\", \"santanderapp\"]`\n    *   **Dependencies:** Runs `after` `kubeflow_deploy_op`.\n\n**Post-Component Execution (Loop for `deploy` and `webapp`):**\n\n*   For both the `deploy` and `webapp` components:\n    *   **If `platform` is 'GCP':** Applies `gcp.use_gcp_secret('user-gcp-sa')` to enable GCP service account authentication.\n    *   **If `platform` is NOT 'GCP':** Applies `onprem.mount_pvc(vop.outputs[\"name\"], 'local-storage', output)` to mount the previously created PVC.\n\n**Libraries/Frameworks Used:**\n*   Kubeflow Pipelines (kfp, dsl, components)\n*   Google Cloud Platform (gcp) utilities for Kubeflow\n*   On-premise (onprem) utilities for Kubeflow\n*   Git (alpine/git image)\n*   A custom component defined in `component.yaml` for model deployment.\n*   Potentially TensorFlow Serving (implied by `tf_server_name` and model export paths).", "completion": "# !/usr/bin/env python3\n\nimport kfp\nfrom kfp import components\nfrom kfp import dsl\nfrom kfp import gcp\nfrom kfp import onprem\n\nplatform = 'GCP'\n\nkubeflow_deploy_op = components.load_component_from_file('pipeline_steps/serving/deployer/component.yaml')\n\n\n@dsl.pipeline(\n    name='Santander Customer Transaction Prediction Release Pipeline',\n    description='Example pipeline that releases the trained classification model for Santander customer transaction.'\n)\ndef santander_transaction_classification(\n        output,\n        project,\n):\n    tf_server_name = 'kfdemo-service'\n\n    if platform != 'GCP':\n        vop = dsl.VolumeOp(\n            name=\"create_pvc\",\n            resource_name=\"pipeline-pvc\",\n            modes=dsl.VOLUME_MODE_RWM,\n            size=\"1Gi\"\n        )\n\n        checkout = dsl.ContainerOp(\n            name=\"checkout\",\n            image=\"alpine/git:latest\",\n            command=[\"git\", \"clone\", \"https://github.com/kubeflow/pipelines.git\", str(output) + \"/pipelines\"],\n        ).apply(onprem.mount_pvc(vop.outputs[\"name\"], 'local-storage', output))\n        checkout.after(vop)\n\n    if platform == 'GCP':\n        deploy = kubeflow_deploy_op(\n            model_dir=str(\n                'gs://kubeflow-pipelines-demo/tfx/0b22081a-ed94-11e9-81fb-42010a800160/santander-customer-transaction-prediction-95qxr-268134926/data') + '/export/export',\n            server_name=tf_server_name\n        )\n    else:\n        deploy = kubeflow_deploy_op(\n            cluster_name=project,\n            model_dir=str(\n                'gs://kubeflow-pipelines-demo/tfx/0b22081a-ed94-11e9-81fb-42010a800160/santander-customer-transaction-prediction-95qxr-268134926/data') + '/export/export',\n            pvc_name=vop.outputs[\"name\"],\n            server_name=tf_server_name\n        )\n\n    webapp = dsl.ContainerOp(\n        name='webapp',\n        image='us.gcr.io/kf-pipelines/ml-pipeline-webapp-launcher:v0.3',\n        arguments=[\"--model_name\", 'santanderapp']\n\n    )\n    webapp.after(deploy)\n\n    steps = [deploy, webapp]\n\n    for step in steps:\n        if platform == 'GCP':\n            step.apply(gcp.use_gcp_secret('user-gcp-sa'))\n        else:\n            step.apply(onprem.mount_pvc(vop.outputs[\"name\"], 'local-storage', output))\n\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(santander_transaction_classification, __file__ + '.zip')"}
{"prompt": "Generate a Kubeflow Pipeline named `continuous_training_pipeline` that performs a continuous training scenario for a machine learning model, specifically addressing the training of new models starting from a production model if one exists.\n\nThe pipeline takes the following parameters:\n- `model_dir_uri` (str): GCS URI of a directory where models and model pointers should be stored.\n- `training_start_date` (str, default: '2019-02-01'): Start date for the training dataset.\n- `training_end_date` (str, default: '2019-03-01'): End date for the training dataset.\n- `testing_start_date` (str, default: '2019-01-01'): Start date for the testing dataset.\n- `testing_end_date` (str, default: '2019-02-01'): End date for the testing dataset.\n\nThe pipeline consists of the following components, connected sequentially to form two main branches for training and evaluation:\n\n**Data Preparation Branch:**\n\n1.  **`training_data`**: Uses `chicago_taxi_dataset_op` to fetch training data.\n    -   **Inputs**: `where` (based on `training_start_date` and `training_end_date`).\n    -   **Outputs**: Training dataset (CSV).\n\n2.  **`testing_data`**: Uses `chicago_taxi_dataset_op` to fetch testing data.\n    -   **Inputs**: `where` (based on `testing_start_date` and `testing_end_date`).\n    -   **Outputs**: Testing dataset (CSV).\n\n3.  **`training_data_for_xgboost`**: Uses `pandas_transform_csv_op` to transform the training data.\n    -   **Inputs**: `table` (from `training_data.output`), `transform_code` (Python code for data transformation, including feature selection and one-hot encoding).\n    -   **Outputs**: Transformed training data.\n\n4.  **`testing_data_for_xgboost`**: Uses `pandas_transform_csv_op` to transform the testing data.\n    -   **Inputs**: `table` (from `testing_data.output`), `transform_code` (same Python code as for `training_data_for_xgboost`).\n    -   **Outputs**: Transformed testing data.\n\n5.  **`test_data_no_header`**: Uses `drop_header_op` to remove the header from the transformed testing data.\n    -   **Inputs**: `table` (from `testing_data_for_xgboost.transformed_table`).\n    -   **Outputs**: Testing data without header.\n\n**Model Training and Evaluation Branch:**\n\nThis branch is designed to first check for an existing production model and warm-start training if one is found.\n\n1.  **`download_prod_model_pointer`**: Uses `download_from_gcs_op` to attempt downloading the production model pointer.\n    -   **Inputs**: `uri` (constructed from `model_dir_uri` and a fixed filename `prod_model.txt`).\n    -   **Outputs**: Production model pointer content.\n    -   **Control Flow**: This component is optional and its failure is handled.\n\n2.  **`download_prod_model`**: Uses `download_from_gcs_op` to download the actual production model if the pointer was successfully downloaded.\n    -   **Inputs**: `uri` (from `download_prod_model_pointer.output`).\n    -   **Outputs**: Production model artifact.\n    -   **Control Flow**: `after(download_prod_model_pointer)`. This component is optional.\n\n3.  **`train_model`**: Uses `xgboost_train_on_csv_op` to train an XGBoost model.\n    -   **Inputs**:\n        -   `training_data` (from `training_data_for_xgboost.transformed_table`).\n        -   `label_column` ('target').\n        -   `model_artifact` (optional, from `download_prod_model.output` if available, for warm-starting).\n    -   **Outputs**: Trained XGBoost model.\n\n4.  **`predict_test_data`**: Uses `xgboost_predict_on_csv_op` to make predictions on the testing data.\n    -   **Inputs**:\n        -   `model` (from `train_model.model`).\n        -   `data` (from `test_data_no_header.output`).\n    -   **Outputs**: Predictions (CSV).\n\n5.  **`calculate_metrics`**: Uses `calculate_regression_metrics_from_csv_op` to calculate regression metrics.\n    -   **Inputs**:\n        -   `predictions` (from `predict_test_data.predictions`).\n        -   `targets` (from `testing_data_for_xgboost.transformed_table` with `target` column selected).\n    -   **Outputs**: Regression metrics (e.g., `mean_absolute_error`).\n\n6.  **`upload_model_to_unique_uri`**: Uses `upload_to_gcs_unique_op` to upload the newly trained model to a unique GCS URI.\n    -   **Inputs**: `source` (from `train_model.model`), `uri_prefix` (constructed from `model_dir_uri` and 'models/').\n    -   **Outputs**: Unique GCS URI of the uploaded model.\n\n7.  **`upload_model_to_prod`**: Uses `upload_to_gcs_op` to set the newly trained model as the production model by updating the `prod_model.txt` pointer.\n    -   **Inputs**:\n        -   `source` (from `upload_model_to_unique_uri.output`).\n        -   `uri` (constructed from `model_dir_uri` and 'prod_model.txt').\n    -   **Control Flow**: `after(calculate_metrics)`. This step implies that the model is only promoted to production after successful evaluation.\n\n**External Component Libraries Used:**\n\nThe pipeline leverages several external Kubeflow components loaded from URLs, including:\n-   `chicago_taxi_dataset_op` for dataset retrieval.\n-   `xgboost_train_on_csv_op` and `xgboost_predict_on_csv_op` for XGBoost model operations.\n-   `pandas_transform_csv_op` for data transformation using Pandas.\n-   `drop_header_op` for table manipulation.\n-   `calculate_regression_metrics_from_csv_op` for metric calculation.\n-   `download_from_gcs_op`, `upload_to_gcs_op`, and `upload_to_gcs_unique_op` for Google Cloud Storage interactions.\n\nThe pipeline ensures that training can be warm-started from an existing production model, and a newly trained model is promoted to production only after its performance is evaluated.", "completion": "# This sample demonstrates a common training scenario.\n# New models are being trained strarting from the production model (if it exists).\n# This sample produces two runs:\n# 1. The trainer will train the model from scratch and set as prod after testing it\n# 2. Exact same configuration, but the pipeline will discover the existing prod model (published by the 1st run) and warm-start the training from it.\n\n\n# GCS URI of a directory where the models and the model pointers should be be stored.\nmodel_dir_uri='gs://<bucket>/<path>'\nkfp_endpoint=None\n\n\nimport kfp\nfrom kfp import components\n\n\nchicago_taxi_dataset_op = components.load_component_from_url('https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/datasets/Chicago_Taxi_Trips/component.yaml')\nxgboost_train_on_csv_op = components.load_component_from_url('https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/XGBoost/Train/component.yaml')\nxgboost_predict_on_csv_op = components.load_component_from_url('https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/XGBoost/Predict/component.yaml')\n\npandas_transform_csv_op = components.load_component_from_url('https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/pandas/Transform_DataFrame/in_CSV_format/component.yaml')\ndrop_header_op = kfp.components.load_component_from_url('https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/tables/Remove_header/component.yaml')\ncalculate_regression_metrics_from_csv_op = kfp.components.load_component_from_url('https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/ml_metrics/Calculate_regression_metrics/from_CSV/component.yaml')\n\ndownload_from_gcs_op = components.load_component_from_url('https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/google-cloud/storage/download/component.yaml')\nupload_to_gcs_op = components.load_component_from_url('https://raw.githubusercontent.com/Ark-kun/pipeline_components/6210648f30b2b3a8c01cc10be338da98300efb6b/components/google-cloud/storage/upload_to_explicit_uri/component.yaml')\nupload_to_gcs_unique_op = components.load_component_from_url('https://raw.githubusercontent.com/Ark-kun/pipeline_components/6210648f30b2b3a8c01cc10be338da98300efb6b/components/google-cloud/storage/upload_to_unique_uri/component.yaml')\n\n\ndef continuous_training_pipeline(\n    model_dir_uri,\n    training_start_date: str = '2019-02-01',\n    training_end_date: str = '2019-03-01',\n    testing_start_date: str = '2019-01-01',\n    testing_end_date: str = '2019-02-01',\n):\n    # Preparing the training and testing data\n    training_data = chicago_taxi_dataset_op(\n        where='trip_start_timestamp >= \"{}\" AND trip_start_timestamp < \"{}\"'.format(str(training_start_date), str(training_end_date)),\n        select='tips,trip_seconds,trip_miles,pickup_community_area,dropoff_community_area,fare,tolls,extras,trip_total',\n        limit=10000,\n    ).set_display_name('Training data').output\n\n    testing_data = chicago_taxi_dataset_op(\n        where='trip_start_timestamp >= \"{}\" AND trip_start_timestamp < \"{}\"'.format(str(testing_start_date), str(testing_end_date)),\n        select='tips,trip_seconds,trip_miles,pickup_community_area,dropoff_community_area,fare,tolls,extras,trip_total',\n        limit=10000,\n    ).set_display_name('Testing data').output\n\n    # Preparing the true values for the testing data\n    true_values_table = pandas_transform_csv_op(\n        table=testing_data,\n        transform_code='''df = df[[\"tips\"]]''',\n    ).set_display_name('True values').output\n\n    true_values = drop_header_op(true_values_table).output\n\n    # Getting the active prod model\n    prod_model_pointer_uri = str(model_dir_uri) + 'prod'\n    get_prod_model_uri_task = download_from_gcs_op(\n        gcs_path=prod_model_pointer_uri,\n        default_data='',\n    ).set_display_name('Get prod model')\n    # Disabling cache reuse to always get new data\n    get_prod_model_uri_task.execution_options.caching_strategy.max_cache_staleness = 'P0D'\n    prod_model_uri = get_prod_model_uri_task.output\n\n    # Training new model from scratch\n    with kfp.dsl.Condition(prod_model_uri == \"\"):\n        # Training\n        model = xgboost_train_on_csv_op(\n            training_data=training_data,\n            label_column=0,\n            objective='reg:squarederror',\n            num_iterations=400,\n        ).outputs['model']\n\n        # Predicting\n        predictions = xgboost_predict_on_csv_op(\n            data=testing_data,\n            model=model,\n            label_column=0,\n        ).output\n\n        # Calculating the regression metrics\n        metrics_task = calculate_regression_metrics_from_csv_op(\n            true_values=true_values,\n            predicted_values=predictions,\n        )\n\n        # Checking the metrics\n        with kfp.dsl.Condition(metrics_task.outputs['mean_squared_error'] < 2.0):\n            # Uploading the model\n            model_uri = upload_to_gcs_unique_op(\n                data=model,\n                gcs_path_prefix=model_dir_uri,\n            ).set_display_name('Upload model').output\n\n            # Setting the model as prod\n            upload_to_gcs_op(\n                data=model_uri,\n                gcs_path=prod_model_pointer_uri,\n            ).set_display_name('Set prod model')\n\n    # Training new model starting from the prod model\n    with kfp.dsl.Condition(prod_model_uri != \"\"):\n        # Downloading the model\n        prod_model = download_from_gcs_op(prod_model_uri).output\n\n        # Training\n        model = xgboost_train_on_csv_op(\n            training_data=training_data,\n            starting_model=prod_model,\n            label_column=0,\n            objective='reg:squarederror',\n            num_iterations=100,\n        ).outputs['model']\n\n        # Predicting\n        predictions = xgboost_predict_on_csv_op(\n            data=testing_data,\n            model=model,\n            label_column=0,\n        ).output\n\n        # Calculating the regression metrics\n        metrics_task = calculate_regression_metrics_from_csv_op(\n            true_values=true_values,\n            predicted_values=predictions,\n        )\n\n        # Checking the metrics\n        with kfp.dsl.Condition(metrics_task.outputs['mean_squared_error'] < 2.0):\n            # Uploading the model\n            model_uri = upload_to_gcs_unique_op(\n                data=model,\n                gcs_path_prefix=model_dir_uri,\n            ).set_display_name('Upload model').output\n\n            # Setting the model as prod\n            upload_to_gcs_op(\n                data=model_uri,\n                gcs_path=prod_model_pointer_uri,\n            ).set_display_name('Set prod model')\n\n\nif __name__ == '__main__':\n    # Running the first time. The trainer will train the model from scratch and set as prod after testing it\n    pipelin_run = kfp.Client(host=kfp_endpoint).create_run_from_pipeline_func(\n        continuous_training_pipeline,\n        arguments=dict(\n            model_dir_uri=model_dir_uri,\n            training_start_date='2019-02-01',\n            training_end_date='2019-03-01',\n        ),\n    )\n    pipelin_run.wait_for_run_completion()\n\n    # Running the second time. The trainer should warm-start the training from the prod model and set the new model as prod after testing it\n    kfp.Client(host=kfp_endpoint).create_run_from_pipeline_func(\n        continuous_training_pipeline,\n        arguments=dict(\n            model_dir_uri=model_dir_uri,\n            training_start_date='2019-02-01',\n            training_end_date='2019-03-01',\n        ),\n    )"}
{"prompt": "Generate a Kubeflow Pipeline named `Metadata Example Pipeline` that performs a demonstration of Kubeflow UI visualizations.\n\nThis pipeline consists of **one component**:\n\n1.  **`confusion_matrix_viz`**:\n    *   **Functionality**: This component simulates the generation of a confusion matrix. It creates a hardcoded confusion matrix in CSV format and writes it to an output path. Crucially, it also generates `mlpipeline-ui-metadata.json` which contains the necessary metadata for Kubeflow UI to render a confusion matrix visualization based on the generated CSV.\n    *   **Inputs**: None.\n    *   **Outputs**:\n        *   `mlpipeline_ui_metadata_path` (type: `dsl.OutputPath`): The path where the UI metadata JSON file will be stored. This file is essential for rendering the visualization in the Kubeflow UI.\n        *   `confusion_matrix_path` (type: `dsl.OutputPath`): The path where the confusion matrix data (CSV string) will be stored.\n    *   **Libraries/Tools Used**: `json` for creating the UI metadata.\n    *   **Base Image**: `image-registry.openshift-image-registry.svc:5000/openshift/python:latest`.\n\n**Control Flow**:\nThe `visualization_pipeline` directly invokes the `confusion_matrix_viz` component. There are no explicit dependencies or complex control flow structures (like `parallelFor` or `after`) as it's a single-step pipeline.", "completion": "\"\"\"Example of a pipeline to demonstrate visualizations from a pipeline.\n\nThe visualization doesn't work for this.  PR's are welcome to help\nget a visualization functioning.\n\nThis pipeline example is currently broken.\n\"\"\"\n\nimport os\n\nimport kfp.compiler\nfrom dotenv import load_dotenv\nfrom kfp import dsl\n\nload_dotenv(override=True)\n\nkubeflow_endpoint = os.environ[\"KUBEFLOW_ENDPOINT\"]\nbearer_token = os.environ[\"BEARER_TOKEN\"]\n\n\n@dsl.component(base_image=\"image-registry.openshift-image-registry.svc:5000/openshift/python:latest\")\ndef confusion_matrix_viz(\n    mlpipeline_ui_metadata_path: dsl.OutputPath(),\n    confusion_matrix_path: dsl.OutputPath(),\n):\n    import json\n\n    cf = \"\"\"\n    16,0,0\n    0,11,0\n    0,0,18\n    \"\"\"\n\n    with open(confusion_matrix_path, \"w\") as text_file:\n        text_file.write(cf)\n\n    metadata = {\n        \"outputs\": [\n            {\n                \"type\": \"confusion_matrix\",\n                \"format\": \"csv\",\n                \"schema\": [\n                    {\"name\": \"target\", \"type\": \"CATEGORY\"},\n                    {\"name\": \"predicted\", \"type\": \"CATEGORY\"},\n                    {\"name\": \"count\", \"type\": \"NUMBER\"},\n                ],\n                \"source\": cf,\n                \"storage\": \"inline\",\n                \"labels\": [\"one\", \"two\", \"three\"],\n            }\n        ]\n    }\n\n    with open(mlpipeline_ui_metadata_path, \"w\") as metadata_file:\n        json.dump(metadata, metadata_file)\n\n    return cf\n\n\n@kfp.dsl.pipeline(\n    name=\"Metadata Example Pipeline\",\n    description=\"A pipeline that is built from inline functions\",\n)\ndef visualization_pipeline():\n    \"\"\"Pipeline to add values.\n\n    Pipeline to take the value of a, add 4 to it and then\n    perform a second task to take the put of the first task and add b.\n    \"\"\"\n    confusion_matrix_task = confusion_matrix_viz()  # noqa: F841\n\n\nif __name__ == \"__main__\":\n    client = kfp.Client(\n        host=kubeflow_endpoint,\n        existing_token=bearer_token,\n    )\n    client.create_run_from_pipeline_func(visualization_pipeline, arguments={}, experiment_name=\"visualization-example\")"}
{"prompt": "Generate a Kubeflow Pipeline named `IrisClassificationPipeline` that performs a machine learning workflow for classifying the Iris dataset.\n\nThe pipeline consists of **four** components:\n\n1.  **`load_data`**:\n    *   **Function**: Loads the Iris dataset using `sklearn.datasets.load_iris`, converts it into a pandas DataFrame, and saves it to a CSV file.\n    *   **Outputs**:\n        *   `output_csv`: A `Dataset` artifact containing the raw Iris data.\n    *   **Libraries**: `pandas`, `scikit-learn`.\n\n2.  **`preprocess_data`**:\n    *   **Function**: Loads the raw dataset, handles any missing values (by dropping rows), standardizes features using `StandardScaler`, and splits the data into training and testing sets (80% train, 20% test) using `train_test_split` with `random_state=42`. It includes debug print statements and assertions for data validation.\n    *   **Inputs**:\n        *   `input_csv`: A `Dataset` artifact representing the raw Iris data from `load_data`.\n    *   **Outputs**:\n        *   `output_train`: A `Dataset` artifact containing the preprocessed training features (X_train).\n        *   `output_test`: A `Dataset` artifact containing the preprocessed testing features (X_test).\n        *   `output_ytrain`: A `Dataset` artifact containing the training labels (y_train).\n        *   `output_ytest`: A `Dataset` artifact containing the testing labels (y_test).\n    *   **Libraries**: `pandas`, `scikit-learn`.\n\n3.  **`train_model`**:\n    *   **Function**: Trains a machine learning model using the preprocessed training data. The specific model type is not specified but it should accept features and labels for training.\n    *   **Inputs**:\n        *   `input_train`: A `Dataset` artifact containing the training features (X_train) from `preprocess_data`.\n        *   `input_ytrain`: A `Dataset` artifact containing the training labels (y_train) from `preprocess_data`.\n    *   **Outputs**:\n        *   `output_model`: A `Model` artifact representing the trained machine learning model.\n    *   **Libraries**: `pandas`, `scikit-learn`.\n\n4.  **`evaluate_model`**:\n    *   **Function**: Evaluates the trained model using the preprocessed testing data and outputs evaluation metrics.\n    *   **Inputs**:\n        *   `input_model`: A `Model` artifact representing the trained model from `train_model`.\n        *   `input_test`: A `Dataset` artifact containing the testing features (X_test) from `preprocess_data`.\n        *   `input_ytest`: A `Dataset` artifact containing the testing labels (y_test) from `preprocess_data`.\n    *   **Outputs**:\n        *   `output_metrics`: A `ClassificationMetrics` artifact containing the evaluation metrics (e.g., accuracy, precision, recall, F1-score).\n    *   **Libraries**: `pandas`, `scikit-learn`.\n\n**Control Flow**:\n*   `load_data` is the initial step and runs first.\n*   `preprocess_data` runs *after* `load_data` completes and uses its output (`output_csv`) as its input.\n*   `train_model` runs *after* `preprocess_data` completes and uses its outputs (`output_train`, `output_ytrain`) as its inputs.\n*   `evaluate_model` runs *after* both `preprocess_data` and `train_model` complete. It depends on the trained model from `train_model` and the test data (`output_test`, `output_ytest`) from `preprocess_data`.\n\nEach component should use `python:3.9` as its `base_image` and install necessary libraries (`pandas`, `scikit-learn`) using `subprocess.run([\"pip\", \"install\", ...])` within the component's execution.", "completion": "from typing import Dict, List\nfrom kfp import dsl\nfrom kfp import compiler\nimport kfp\nfrom kfp.dsl import Input, Output, Dataset, Model, component\n\n# Step 1: Load Dataset\n@dsl.component(base_image=\"python:3.9\")\ndef load_data(output_csv: Output[Dataset]):\n    import subprocess\n    subprocess.run([\"pip\", \"install\", \"pandas\", \"scikit-learn\"], check=True)\n\n    from sklearn.datasets import load_iris\n    import pandas as pd\n    iris = load_iris()\n    df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n    df['target'] = iris.target\n\n    # Save the dataset to the output artifact path\n    df.to_csv(output_csv.path, index=False)\n\n# Step 2: Preprocess Data\n@dsl.component(base_image=\"python:3.9\")\ndef preprocess_data(input_csv: Input[Dataset], output_train: Output[Dataset], output_test: Output[Dataset], \n                    output_ytrain: Output[Dataset], output_ytest: Output[Dataset]):\n    import subprocess\n    subprocess.run([\"pip\", \"install\", \"pandas\", \"scikit-learn\"], check=True)\n\n    import pandas as pd\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.model_selection import train_test_split\n\n    # Load dataset\n    df = pd.read_csv(input_csv.path)\n\n    # Debug: Check for NaN values\n    print(\"Initial dataset shape:\", df.shape)\n    print(\"Missing values before preprocessing:\\n\", df.isnull().sum())\n\n    # Handle missing values\n    if df.isnull().values.any():\n        print(\"Missing values detected. Handling them...\")\n        df = df.dropna()  # Drop rows with any NaN values\n    \n    # Validate that there are no NaNs in the target column\n    assert not df['target'].isnull().any(), \"Target column contains NaN values after handling missing values.\"\n\n    features = df.drop(columns=['target'])\n    target = df['target']\n\n    # Standardize features\n    scaler = StandardScaler()\n    scaled_features = scaler.fit_transform(features)\n\n    # Train-test split\n    X_train, X_test, y_train, y_test = train_test_split(scaled_features, target, test_size=0.2, random_state=42)\n\n    # Debug: Validate splits\n    print(\"Shapes after train-test split:\")\n    print(\"X_train:\", X_train.shape, \"X_test:\", X_test.shape) \n    print(\"y_train:\", y_train.shape, \"y_test:\", y_test.shape)\n    print(\"Missing values in y_train:\", y_train.isnull().sum())\n\n    # Ensure no NaNs in the split data\n    assert not y_train.isnull().any(), \"y_train contains NaN values.\"\n    assert not y_test.isnull().any(), \"y_test contains NaN values.\"\n\n    # Create DataFrames for train and test sets\n    X_train_df = pd.DataFrame(X_train, columns=features.columns)\n    print(\"X_train_df:\", X_train_df) \n\n    y_train_df = pd.DataFrame(y_train) \n    print(\"y_train_df: \", y_train_df)  \n\n    X_test_df = pd.DataFrame(X_test, columns=features.columns)\n    print(\"X_test_df:\", X_test_df) \n\n    y_test_df = pd.DataFrame(y_test) \n    print(\"y_test_df: \", y_test_df) \n\n    # Save processed train and test data\n    X_train_df.to_csv(output_train.path, index=False)  \n    X_test_df.to_csv(output_test.path, index=False)\n\n    y_train_df.to_csv(output_ytrain.path, index=False)  \n    y_test_df.to_csv(output_ytest.path, index=False) \n\n# Step 3: Train Model\n@dsl.component(base_image=\"python:3.9\")\ndef train_model(train_data: Input[Dataset], ytrain_data: Input[Dataset], model_output: Output[Model]):\n\n    import subprocess\n    subprocess.run([\"pip\", \"install\", \"pandas\", \"scikit-learn\", \"joblib\"], check=True)\n\n    import pandas as pd\n    from sklearn.linear_model import LogisticRegression\n    from joblib import dump\n\n    # Load training data\n    train_df = pd.read_csv(train_data.path)\n    print(\"Shape of train_df:\", train_df.shape)\n    print(\"train_df:\", train_df)\n    X_train = train_df \n\n    y_train = pd.read_csv(ytrain_data.path)\n    print(\"Shape of ytrain_df:\", y_train.shape)\n    print(\"y_train_df:\", y_train)\n\n    # Debug: Validate splits\n    print(\"Shapes of X_train and y_train: \")\n    print(\"X_train:\", X_train.shape)\n    print(\"y_train:\", y_train.shape) \n    print(\"Missing values in X_train:\", X_train.isnull().sum())\n    print(\"Missing values in y_train:\", y_train.isnull().sum()) \n\n    # Ensure no NaN values\n    assert not X_train.isnull().values.any(), \"X_train contains NaN values.\"\n    assert not y_train.isnull().values.any(), \"y_train contains NaN values.\" \n\n    # Train model\n    model = LogisticRegression()\n    model.fit(X_train, y_train)\n\n    # Save model\n    dump(model, model_output.path)\n\n# Step 4: Evaluate Model\n@dsl.component(base_image=\"python:3.9\")\ndef evaluate_model(test_data: Input[Dataset], ytest_data: Input[Dataset], model: Input[Model], metrics_output: Output[Dataset]):\n    import subprocess\n    subprocess.run([\"pip\", \"install\", \"pandas\", \"scikit-learn\", \"matplotlib\", \"joblib\"], check=True)\n\n    import pandas as pd\n    from sklearn.metrics import classification_report, confusion_matrix\n    import matplotlib.pyplot as plt\n    from joblib import load\n\n    # Load test data\n    X_test = pd.read_csv(test_data.path)\n\n    y_test = pd.read_csv(ytest_data.path)  \n\n    # Load model\n    model = load(model.path)\n\n    # Predict\n    y_pred = model.predict(X_test)\n\n    # Generate metrics\n    report = classification_report(y_test, y_pred, output_dict=True)\n    cm = confusion_matrix(y_test, y_pred)\n\n    # Save metrics to a file\n    metrics_path = metrics_output.path\n    with open(metrics_path, 'w') as f:\n        f.write(str(report))\n\n    # Plot confusion matrix\n    plt.figure(figsize=(8, 6))\n    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n    plt.title('Confusion Matrix')\n    plt.colorbar()\n    plt.xlabel('Predicted Label')\n    plt.ylabel('True Label')\n    plt.savefig(metrics_path.replace('.txt', '.png'))\n\n# Define the pipeline\n@dsl.pipeline(name=\"ml-pipeline\")\ndef ml_pipeline():\n    # Step 1: Load Dataset\n    load_op = load_data()\n\n    # Step 2: Preprocess Data\n    preprocess_op = preprocess_data(input_csv=load_op.outputs[\"output_csv\"])\n\n    # Step 3: Train Model\n    train_op = train_model(train_data=preprocess_op.outputs[\"output_train\"], ytrain_data=preprocess_op.outputs[\"output_ytrain\"])\n\n    # Step 4: Evaluate Model\n    evaluate_op = evaluate_model(test_data=preprocess_op.outputs[\"output_test\"], ytest_data=preprocess_op.outputs[\"output_ytest\"], model=train_op.outputs[\"model_output\"]) \n\n# Compile the pipeline\nif __name__ == \"__main__\":\n    compiler.Compiler().compile(pipeline_func=ml_pipeline, package_path=\"kubeflow_pipeline.yaml\")"}
{"prompt": "Generate a Kubeflow Pipeline named `download_and_print` that performs the following steps:\n\nThis pipeline consists of a single component.\n\nThe primary function of this pipeline is to demonstrate the use of an exit handler with a legacy Kubeflow Pipelines V1 execution mode.\n\n**Components:**\n\n1.  **`download_and_print`**: This is the main component and also serves as the pipeline's entry point. Its specific function is to download some content (implied by the name `download_and_print` which suggests fetching data) and then print it.\n    *   **Inputs**: The code snippet does not explicitly show any `dsl.Input` or `dsl.Parameter` for this component within the provided file. It's likely its inputs are defined internally within the `legacy_exit_handler.py` file or it operates without external inputs.\n    *   **Outputs**: The code snippet does not explicitly show any `dsl.Output` for this component. The \"print\" aspect suggests side effects (e.g., logging to stdout).\n\n**Control Flow:**\n\n*   The pipeline has a simple linear flow: the `download_and_print` component executes.\n*   The pipeline is configured to run in `kfp.dsl.PipelineExecutionMode.V1_LEGACY` mode.\n*   This pipeline is designed to include an implicit or explicit exit handler, which is a mechanism to execute a cleanup or finalization task regardless of whether the main pipeline run succeeds or fails. While the exit handler itself is not directly defined in this snippet, the context implies its presence and functionality, likely within the `legacy_exit_handler.py` module.\n\n**Libraries/Tools:**\n\n*   `kfp`: Used for defining the Kubeflow Pipeline and specifying its execution mode.\n*   The `download_and_print` component likely utilizes standard Python libraries for file downloading and printing. No external data science or machine learning libraries (e.g., sklearn, pandas) are explicitly indicated by this snippet.", "completion": "#!/usr/bin/env python3\n# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp\nfrom .legacy_exit_handler import download_and_print\nfrom .util import run_pipeline_func, TestCase\n\n\nrun_pipeline_func([\n    # This sample is expected to not work on v2 compatible mode.\n    TestCase(\n        pipeline_func=download_and_print,\n        mode=kfp.dsl.PipelineExecutionMode.V1_LEGACY\n    )\n])"}
{"prompt": "Generate a Kubeflow Pipeline named `MetricsVisualizationPipeline` that performs a series of machine learning classification tasks and visualizes their results.\n\nThe pipeline consists of two components:\n1.  **`digit_classification`**:\n    *   **Function:** This component trains and evaluates a Logistic Regression model on the Iris dataset. It performs cross-validation and calculates the accuracy on a test set.\n    *   **Outputs:** It logs a single metric named `accuracy` (a float value representing percentage accuracy) to an `Output[Metrics]` artifact.\n    *   **Libraries/Tools:** It uses `sklearn` for dataset loading, model selection, and metric calculation.\n    *   **Base Image:** `python:3.9`\n\n2.  **`wine_classification`**:\n    *   **Function:** This component trains and evaluates a RandomForestClassifier on the Wine dataset. It transforms the problem into a binary classification task and calculates ROC curve data.\n    *   **Outputs:** It logs classification metrics (specifically ROC curve data) to an `Output[ClassificationMetrics]` artifact.\n    *   **Libraries/Tools:** It uses `sklearn` for dataset loading, model training, and metric calculation.\n    *   **Base Image:** `python:3.9`\n\n**Control Flow:**\nBoth `digit_classification` and `wine_classification` components run in parallel within the pipeline. There are no explicit dependencies between them.", "completion": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport os\n\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import (\n    component,\n    Output,\n    ClassificationMetrics,\n    Metrics,\n    HTML,\n    Markdown\n)\n\n# In tests, we install a KFP package from the PR under test. Users should not\n# normally need to specify `kfp_package_path` in their component definitions.\n_KFP_PACKAGE_PATH = os.getenv('KFP_PACKAGE_PATH')\n\n\n@component(\n    packages_to_install=['sklearn'],\n    base_image='python:3.9',\n    kfp_package_path=_KFP_PACKAGE_PATH,\n)\ndef digit_classification(metrics: Output[Metrics]):\n    from sklearn import model_selection\n    from sklearn.linear_model import LogisticRegression\n    from sklearn import datasets\n    from sklearn.metrics import accuracy_score\n\n    # Load digits dataset\n    iris = datasets.load_iris()\n\n    # # Create feature matrix\n    X = iris.data\n\n    # Create target vector\n    y = iris.target\n\n    #test size\n    test_size = 0.33\n\n    seed = 7\n    #cross-validation settings\n    kfold = model_selection.KFold(n_splits=10, random_state=seed, shuffle=True)\n\n    #Model instance\n    model = LogisticRegression()\n    scoring = 'accuracy'\n    results = model_selection.cross_val_score(model, X, y, cv=kfold, scoring=scoring)\n\n    #split data\n    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=test_size, random_state=seed)\n    #fit model\n    model.fit(X_train, y_train)\n\n    #accuracy on test set\n    result = model.score(X_test, y_test)\n    metrics.log_metric('accuracy', (result*100.0))\n\n@component(\n    packages_to_install=['sklearn'],\n    base_image='python:3.9',\n    kfp_package_path=_KFP_PACKAGE_PATH,\n)\ndef wine_classification(metrics: Output[ClassificationMetrics]):\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.metrics import roc_curve\n    from sklearn.datasets import load_wine\n    from sklearn.model_selection import train_test_split, cross_val_predict\n\n    X, y = load_wine(return_X_y=True)\n    # Binary classification problem for label 1.\n    y = y == 1\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n    rfc = RandomForestClassifier(n_estimators=10, random_state=42)\n    rfc.fit(X_train, y_train)\n    y_scores = cross_val_predict(rfc, X_train, y_train, cv=3, method='predict_proba')\n    y_predict = cross_val_predict(rfc, X_train, y_train, cv=3, method='predict')\n    fpr, tpr, thresholds = roc_curve(y_true=y_train, y_score=y_scores[:,1], pos_label=True)\n    metrics.log_roc_curve(fpr, tpr, thresholds)\n\n@component(\n    packages_to_install=['sklearn'],\n    base_image='python:3.9',\n    kfp_package_path=_KFP_PACKAGE_PATH,\n)\ndef iris_sgdclassifier(test_samples_fraction: float, metrics: Output[ClassificationMetrics]):\n    from sklearn import datasets, model_selection\n    from sklearn.linear_model import SGDClassifier\n    from sklearn.metrics import confusion_matrix\n\n    iris_dataset = datasets.load_iris()\n    train_x, test_x, train_y, test_y = model_selection.train_test_split(\n        iris_dataset['data'], iris_dataset['target'], test_size=test_samples_fraction)\n\n\n    classifier = SGDClassifier()\n    classifier.fit(train_x, train_y)\n    predictions = model_selection.cross_val_predict(classifier, train_x, train_y, cv=3)\n    metrics.log_confusion_matrix(\n        ['Setosa', 'Versicolour', 'Virginica'],\n        confusion_matrix(train_y, predictions).tolist() # .tolist() to convert np array to list.\n    )\n\n@component(\n    kfp_package_path=_KFP_PACKAGE_PATH,\n)\ndef html_visualization(html_artifact: Output[HTML]):\n    html_content = '<!DOCTYPE html><html><body><h1>Hello world</h1></body></html>'\n    with open(html_artifact.path, 'w') as f:\n        f.write(html_content)\n\n@component(\n    kfp_package_path=_KFP_PACKAGE_PATH,\n)\ndef markdown_visualization(markdown_artifact: Output[Markdown]):\n    markdown_content = '## Hello world \\n\\n Markdown content'\n    with open(markdown_artifact.path, 'w') as f:\n        f.write(markdown_content)\n\n@dsl.pipeline(\n    name='metrics-visualization-pipeline')\ndef metrics_visualization_pipeline():\n    wine_classification_op = wine_classification()\n    iris_sgdclassifier_op = iris_sgdclassifier(test_samples_fraction=0.3)\n    digit_classification_op = digit_classification()\n    html_visualization_op = html_visualization()\n    markdown_visualization_op = markdown_visualization()"}
{"prompt": "I apologize, but the provided Python code (`stackdemos_ml-kbf_components_training_component.py`) defines *Kubeflow component functions* (`training_op`, `http_download_op`) but **does not define a Kubeflow Pipeline** using `@dsl.pipeline`.\n\nTo generate a prompt for regenerating a Kubeflow Pipeline, I need to see the code that actually defines the pipeline structure, including:\n- `@dsl.pipeline` decorator and its `name`\n- `@component` decorated functions (if any, though the provided code uses `ContainerOp` directly)\n- How the components are called and linked together (e.g., `component_a.after(component_b)`)\n- Any pipeline inputs or outputs\n\n**If you can provide the Kubeflow Pipeline definition code, I will be happy to generate the requested prompt.**\n\n---\n\n**Example of what I'd need to see (conceptual):**\n\n```python\nfrom kfp import dsl\nfrom kfp.dsl import ContainerOp # Assuming training_op and http_download_op are available\n\n# Assume training_op and http_download_op are defined as in your original snippet\n\n@dsl.pipeline(\n    name='MyAwesomePipeline',\n    description='A pipeline to download data and train a model.'\n)\ndef my_awesome_pipeline(\n    data_url: str = 'http://example.com/data.csv',\n    data_md5: str = 'abc123def456',\n    model_script: str = 'train.py',\n    training_image: str = 'my-training-image:latest'\n):\n    download_task = http_download_op(\n        url=data_url,\n        download_to='/data/input.csv',\n        md5sum=data_md5\n    )\n\n    train_task = training_op(\n        script=model_script,\n        image=training_image,\n        arguments=[\n            '--input-path', download_task.output, # Example: passing output\n            '--output-model', '/model/output.pkl'\n        ],\n        file_outputs={'trained_model': '/model/output.pkl'}\n    )\n\n    # ... other tasks\n```", "completion": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef training_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('TRAINING_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=['/usr/local/bin/python', script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )"}
{"prompt": "Generate a Kubeflow Pipeline named `HypertensionPredictionPipeline` that performs a machine learning workflow for predicting hypertension.\n\nThis pipeline consists of **three** components:\n\n1.  **`load_data`**:\n    *   **Function:** Loads the `hypertension_data.csv` dataset from a specified URL, performs data cleaning, removes unnecessary columns (`cp`, `thal`), handles missing values by dropping rows with \"N/A\" or NaN in critical columns, and filters out `restecg` values equal to 2.\n    *   **Outputs:**\n        *   `data_output`: An `Artifact` containing the cleaned DataFrame.\n    *   **Tools/Libraries:** `pandas==2.2.2`.\n\n2.  **`prepare_data`**:\n    *   **Function:** Reads the cleaned data, splits it into features (X) and target (Y) arrays, and then further splits the data into training, testing, and validation sets using `train_test_split`. The initial split is 80% train, 20% test. The 20% test set is then split again into 50% test and 50% validation.\n    *   **Inputs:**\n        *   `data_input`: An `Input[Artifact]` representing the cleaned data from the `load_data` component.\n    *   **Outputs:**\n        *   `X_train_output`: An `Artifact` containing the training features.\n        *   `X_test_output`: An `Artifact` containing the testing features.\n        *   `Y_train_output`: An `Artifact` containing the training targets.\n        *   `Y_test_output`: An `Artifact` containing the testing targets.\n        *   `X_val_output`: An `Artifact` containing the validation features.\n        *   `Y_val_output`: An `Artifact` containing the validation targets.\n    *   **Tools/Libraries:** `pandas==2.2.2`, `scikit-learn==1.5.1`.\n    *   **Dependencies:** This component depends on the successful completion of the `load_data` component.\n\n3.  **`train_model`**: (This component is not fully provided in the snippet, but based on typical ML workflows, it would follow `prepare_data`.)\n    *   **Function:** (Assumed) Trains a machine learning model (e.g., Logistic Regression, RandomForest) using the training data (`X_train`, `Y_train`).\n    *   **Inputs:**\n        *   `X_train_input`: An `Input[Artifact]` for training features.\n        *   `Y_train_input`: An `Input[Artifact]` for training targets.\n    *   **Outputs:**\n        *   `model_output`: A `Model` artifact representing the trained model.\n    *   **Tools/Libraries:** (Assumed) `scikit-learn` and potentially other ML libraries.\n    *   **Dependencies:** This component depends on the successful completion of the `prepare_data` component.\n\n**Control Flow:**\nThe pipeline execution flow is sequential:\n*   `load_data` runs first.\n*   `prepare_data` runs after `load_data` completes, taking its output as input.\n*   (Assumed) `train_model` runs after `prepare_data` completes, taking its training data outputs as input.\n\nAll components should use `python:3.9` as their base image.", "completion": "from typing import NewType\n\nimport kfp\nimport kfp.compiler\n\nfrom kfp import dsl\nfrom kfp.dsl import OutputPath, InputPath, Input, Output, Artifact, Model, Dataset \n\nfrom pandas import DataFrame\n# from kfp.components import func_to_container_op\n\nDF = NewType('DF', DataFrame)\n\n@dsl.component(\n    base_image='python:3.9',\n    packages_to_install=['pandas==2.2.2']\n)\ndef load_data(data_output: Output[Artifact]):\n    import pandas as pd\n\n    # \u8b80\u53d6\u8207\u7a0b\u5f0f\u78bc\u4f4d\u65bc\u540c\u4e00\u500b\u8cc7\u6599\u593e\u4e2d\u7684 hypertension.csv\n    df_data = pd.read_csv('https://raw.githubusercontent.com/s102401002/kubeflowPipeline0722/main/hypertension_data.csv')\n\n    # \u79fb\u9664\u4e0d\u9700\u8981\u7684\u6b04\u4f4d\n    df_data = df_data.drop(columns=['cp','thal'])\n\n\n    # \u88dc\u9f4a\u8cc7\u6599\n    df_data = df_data[(df_data['age'] != 'N/A') & (~df_data['age'].isna())]\n    df_data = df_data[(df_data['sex'] != 'N/A') & (~df_data['sex'].isna())]\n    df_data = df_data[(df_data['trestbps'] != 'N/A') & (~df_data['trestbps'].isna())] #\u975c\u6b62\u8840\u58d3\n    df_data = df_data[(df_data['chol'] != 'N/A') & (~df_data['chol'].isna())] #\u8840\u6e05\u81bd\u56fa\u9187\n    df_data = df_data[(df_data['fbs'] != 'N/A') & (~df_data['fbs'].isna())] #\u662f\u5426\u60a3\u8005\u7684\u7a7a\u8179\u8840\u7cd6 > 120\n    df_data = df_data[(df_data['restecg'] != 'N/A') & (~df_data['restecg'].isna())] #ST-T \u6ce2\u662f\u5426\u7570\u5e38\n    df_data = df_data[df_data['restecg'] != 2]\n    df_data = df_data[(df_data['thalach'] != 'N/A') & (~df_data['thalach'].isna())] #\u6700\u5927\u5fc3\u7387\n    df_data = df_data[(df_data['exang'] != 'N/A') & (~df_data['exang'].isna())] #\u662f\u5426\u904b\u52d5\u8a98\u767c\u5fc3\u7d5e\u75db\n    df_data = df_data[(df_data['oldpeak'] != 'N/A') & (~df_data['oldpeak'].isna())] #\u904b\u52d5\u76f8\u5c0d\u65bc\u4f11\u606f\u5f15\u8d77\u7684 ST \u3002\n    df_data = df_data[(df_data['slope'] != 'N/A') & (~df_data['slope'].isna())] #\u904b\u52d5\u5cf0\u503c ST \u6bb5\u659c\u7387\uff1a0\uff1a\u4e0a\u5761 1\uff1a\u5e73\u5766 2\uff1a\u4e0b\u5761\n    df_data = df_data[(df_data['ca'] != 'N/A') & (~df_data['ca'].isna())]#\u87a2\u5149\u6aa2\u67e5\u8457\u8272\u7684\u4e3b\u8981\u8840\u7ba1\u6578\u91cf (0\u20134)\n    df_data = df_data[(df_data['target'] != 'N/A') & (~df_data['target'].isna())]\n    df_data.to_csv(data_output.path)\n\n@dsl.component(\n    base_image='python:3.9',\n    packages_to_install=['pandas==2.2.2', 'scikit-learn==1.5.1']\n)\ndef prepare_data(\n    data_input: Input[Artifact], \n    X_train_output: Output[Artifact], X_test_output: Output[Artifact],\n    Y_train_output: Output[Artifact], Y_test_output: Output[Artifact],\n    X_val_output: Output[Artifact], Y_val_output: Output[Artifact]\n):\n    import pandas as pd\n    from sklearn.model_selection import train_test_split\n\n    df_data = pd.read_csv(data_input.path)\n\n    X = df_data.drop(labels=['target'], axis=1)\n    Y = df_data[['target']]\n    \n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n    X_test, X_val, Y_test, Y_val = train_test_split(X_test, Y_test, test_size=0.5, random_state=42)\n    X_train.to_csv(X_train_output.path, index=False)\n    X_test.to_csv(X_test_output.path, index=False)\n    Y_train.to_csv(Y_train_output.path, index=False)\n    Y_test.to_csv(Y_test_output.path, index=False)\n    X_val.to_csv(X_val_output.path, index=False)\n    Y_val.to_csv(Y_val_output.path, index=False)\n\n\n@dsl.component(\n    base_image='python:3.9',\n    packages_to_install=['pandas==2.2.2', 'scikit-learn==1.5.1', 'joblib==1.4.2']\n)\ndef train_model_LogisticRegression(\n    X_train: Input[Artifact], \n    Y_train: Input[Artifact], \n    X_val: Input[Artifact], \n    Y_val: Input[Artifact],\n    X_test: Input[Artifact], \n    Y_test: Input[Artifact], \n    model: Output[Artifact],\n    file: Output[Artifact]\n):\n    import pandas as pd\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.metrics import accuracy_score\n    import joblib\n    import json\n    \n    X_train = pd.read_csv(X_train.path)\n    Y_train = pd.read_csv(Y_train.path)\n    X_val = pd.read_csv(X_val.path)\n    Y_val = pd.read_csv(Y_val.path)\n    X_test = pd.read_csv(X_test.path)\n    Y_test = pd.read_csv(Y_test.path)\n\n    # Logistic Regression\n    print('Logistic Regression')\n    lr_model = LogisticRegression(random_state=0, max_iter=10000)\n    lr_model.fit(X_train, Y_train.values.ravel())\n    print('Training accuracy:', lr_model.score(X_train, Y_train))\n    lr_accuracy = lr_model.score(X_test, Y_test)\n    print('Test accuracy:', lr_accuracy)\n    \n    # Save the model\n    joblib.dump(lr_model, model.path)\n    # Save the accuracy\n    jsonFile = open(file.path,'w')\n    data = {}\n    data['accuracy'] = lr_accuracy\n    data['model_path'] = model.path\n    json.dump(data, jsonFile, indent=2)\n@dsl.component(\n    base_image='python:3.9',\n    packages_to_install=['pandas==2.2.2', 'scikit-learn==1.5.1', 'joblib==1.4.2', 'xgboost==2.0.3']\n)\ndef train_model_xgboost(\n    X_train: Input[Artifact], \n    Y_train: Input[Artifact],\n    X_val: Input[Artifact], \n    Y_val: Input[Artifact],\n    X_test: Input[Artifact], \n    Y_test: Input[Artifact], \n    model: Output[Artifact],\n    file: Output[Artifact]\n):\n    import pandas as pd\n    import xgboost as xgb\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.metrics import accuracy_score\n    import joblib\n    import json\n    \n    X_train = pd.read_csv(X_train.path)\n    Y_train = pd.read_csv(Y_train.path)\n    X_val = pd.read_csv(X_val.path)\n    Y_val = pd.read_csv(Y_val.path)\n    X_test = pd.read_csv(X_test.path)\n    Y_test = pd.read_csv(Y_test.path)\n\n    dtrain = xgb.DMatrix(X_train, label=Y_train)\n    dval = xgb.DMatrix(X_val, label=Y_val)\n    dtest = xgb.DMatrix(X_test, label=Y_test)\n\n    scale_pos_weight = len(Y_train[Y_train == 0]) / len(Y_train[Y_train == 1])\n    param = {\n        'max_depth': 3,\n        'eta': 0.3,\n        'objective': 'binary:logistic',\n        'eval_metric': 'logloss',\n        'scale_pos_weight': scale_pos_weight\n    }\n    evallist = [(dtrain, 'train'), (dval, 'eval')]\n    num_round = 1000\n    xgb_model = xgb.train(param, dtrain, num_round, evallist, early_stopping_rounds=10)\n    preds = xgb_model.predict(dtest)\n    predictions = [round(value) for value in preds]\n    global xgb_accuracy \n    xgb_accuracy = accuracy_score(Y_test, predictions)\n    print('XGBoost Test accuracy:', xgb_accuracy)\n    \n    # Save the model\n    joblib.dump(xgb_model, model.path)\n\n     # Save the accuracy\n    jsonFile = open(file.path,'w')\n    data = {}\n    data['accuracy'] = xgb_accuracy\n    data['model_path'] = model.path\n    json.dump(data, jsonFile, indent=2)\n    # with open(accuracy.path, 'w') as f:\n    #     f.write(str(xgb_accuracy))\n\n# @dsl.component(\n#     base_image='python:3.9',\n#     packages_to_install=['pandas==2.2.2', 'scikit-learn==1.5.1', 'joblib==1.4.2']\n# )\n# def train_model_svm(\n#     X_train: Input[Artifact], \n#     Y_train: Input[Artifact],\n#     X_val: Input[Artifact], \n#     Y_val: Input[Artifact],\n#     X_test: Input[Artifact], \n#     Y_test: Input[Artifact], \n#     model: Output[Artifact],\n#     accuracy: Output[Artifact]\n# ):\n#     import pandas as pd\n#     from sklearn.metrics import accuracy_score\n#     from sklearn import svm\n#     import joblib\n    \n#     X_train = pd.read_csv(X_train.path)\n#     Y_train = pd.read_csv(Y_train.path)\n#     X_val = pd.read_csv(X_val.path)\n#     Y_val = pd.read_csv(Y_val.path)\n#     X_test = pd.read_csv(X_test.path)\n#     Y_test = pd.read_csv(Y_test.path)\n\n#     clf=svm.SVC(kernel='poly',gamma='auto',C=100)\n#     clf.fit(X_train,Y_train.values.ravel())\n#     clf.predict(X_test)\n#     svm_accuracy = clf.score(X_test, Y_test)\n#     # Save the model\n#     joblib.dump(model, clf.path)\n\n#      # Save the accuracy\n#     with open(accuracy.path, 'w') as f:\n#         f.write(str(svm_accuracy))\n\n@dsl.component(\n    base_image='python:3.9',\n    packages_to_install=['pandas==2.2.2', 'scikit-learn==1.5.1', 'joblib==1.4.2']\n)\ndef train_model_RandomForest(\n    X_train: Input[Artifact], \n    Y_train: Input[Artifact],\n    X_val: Input[Artifact], \n    Y_val: Input[Artifact],\n    X_test: Input[Artifact], \n    Y_test: Input[Artifact], \n    model: Output[Artifact],\n    file: Output[Artifact]\n):\n    import pandas as pd\n    from sklearn.metrics import accuracy_score\n    from sklearn.ensemble import RandomForestClassifier\n    import joblib\n    import json\n    \n    X_train = pd.read_csv(X_train.path)\n    Y_train = pd.read_csv(Y_train.path)\n    X_val = pd.read_csv(X_val.path)\n    Y_val = pd.read_csv(Y_val.path)\n    X_test = pd.read_csv(X_test.path)\n    Y_test = pd.read_csv(Y_test.path)\n\n    rfc=RandomForestClassifier(n_estimators=5)\n    rfc.fit(X_train,Y_train.values.ravel())    \n    y_predict=rfc.predict(X_test)\n    rfc.predict(X_test)\n    rf_accuracy = rfc.score(X_test,Y_test)\n    # Save the model\n    joblib.dump(rfc, model.path)\n\n     # Save the accuracy\n    jsonFile = open(file.path,'w')\n    data = {}\n    data['accuracy'] = rf_accuracy\n    data['model_path'] = model.path\n    json.dump(data, jsonFile, indent=2)\n\n@dsl.component(\n    base_image='python:3.9',\n    packages_to_install=['pandas==2.2.2', 'scikit-learn==1.5.1', 'joblib==1.4.2']\n)\ndef train_model_KNN(\n    X_train: Input[Artifact], \n    Y_train: Input[Artifact],\n    X_val: Input[Artifact], \n    Y_val: Input[Artifact],\n    X_test: Input[Artifact], \n    Y_test: Input[Artifact], \n    model: Output[Artifact],\n    file: Output[Artifact]\n):\n    import pandas as pd\n    from sklearn.neighbors import KNeighborsClassifier\n    import joblib\n    import json\n\n    X_train = pd.read_csv(X_train.path)\n    Y_train = pd.read_csv(Y_train.path)\n    X_val = pd.read_csv(X_val.path)\n    Y_val = pd.read_csv(Y_val.path)\n    X_test = pd.read_csv(X_test.path)\n    Y_test = pd.read_csv(Y_test.path)\n\n    knc = KNeighborsClassifier(n_neighbors=3)\n    knc.fit(X_train,Y_train.values.ravel())\n    knn_accuracy = knc.score(X_test,Y_test)\n    # Save the model\n    joblib.dump(knc, model.path)\n\n     # Save the accuracy\n    jsonFile = open(file.path,'w')\n    data = {}\n    data['accuracy'] = knn_accuracy\n    data['model_path'] = model.path\n    json.dump(data, jsonFile, indent=2)\n\n@dsl.component(\n    base_image='python:3.9',\n    packages_to_install=['joblib==1.4.2', 'scikit-learn==1.5.1', 'xgboost==2.0.3']# \n)\ndef choose_model(\n    LogisticRegression_model: Input[Artifact],\n    XGBoost_model: Input[Artifact],\n    # SVM_model: Input[Artifact],\n    RandomForest_model: Input[Artifact],\n    KNN_model: Input[Artifact],\n    lr_file: Input[Artifact],\n    xgb_file: Input[Artifact],\n    # svm_accuracy: Input[Artifact],\n    rf_file: Input[Artifact],\n    knn_file: Input[Artifact],\n    final_model: Output[Model],\n    result: Output[Artifact]\n) -> None:\n    import joblib\n    import json\n\n    # Define a dictionary to store model artifacts and their corresponding JSON files\n    models = {\n        'LogisticRegression': lr_file,\n        'XGBoost': xgb_file,\n        'RandomForest': rf_file,\n        'KNN': knn_file\n    }\n\n    accuracy = {}\n    model_paths = {}\n\n    # Read accuracies and model paths\n    for model_name, json_file in models.items():\n        with open(json_file.path, 'r') as f:\n            data = json.load(f)\n        accuracy[model_name] = data['accuracy']\n        model_paths[model_name] = data['model_path']\n\n    # Find the best model\n    best_model_name = max(accuracy, key=accuracy.get)\n    best_model = joblib.load(model_paths[best_model_name])\n    \n    # Save the best model\n    joblib.dump(best_model, final_model.path)\n\n    # Prepare result string\n    result_string = f'Best Model is {best_model_name} : {accuracy[best_model_name]}'\n    result_string += f'\\nAccuracy:\\n'\n    for model_name, acc in accuracy.items():\n        result_string += f'{model_name:17} : {acc}\\n'\n    print(result_string)\n\n\n    # Write the result to a file\n    with open(result.path, 'w') as f:\n        f.write(result_string)\n@dsl.pipeline(\n    name='Hypertension Prediction Pipeline',\n    description='Using Kubeflow pipeline to train and evaluate a Hypertension prediction model'\n)\ndef hypertension_prediction_pipeline():\n    # Load data\n    load_data_task = load_data()\n\n    # Prepare data\n    prepare_data_task = prepare_data(data_input=load_data_task.outputs['data_output'])\n    \n    # Train models\n    train_model_LogisticRegression_task = train_model_LogisticRegression(\n        X_train=prepare_data_task.outputs['X_train_output'], \n        Y_train=prepare_data_task.outputs['Y_train_output'],\n        X_val=prepare_data_task.outputs['X_val_output'],\n        Y_val=prepare_data_task.outputs['Y_val_output'],\n        X_test=prepare_data_task.outputs['X_test_output'], \n        Y_test=prepare_data_task.outputs['Y_test_output']\n    )\n\n    train_model_xgboost_task = train_model_xgboost(\n        X_train=prepare_data_task.outputs['X_train_output'], \n        Y_train=prepare_data_task.outputs['Y_train_output'],\n        X_val=prepare_data_task.outputs['X_val_output'],\n        Y_val=prepare_data_task.outputs['Y_val_output'],\n        X_test=prepare_data_task.outputs['X_test_output'], \n        Y_test=prepare_data_task.outputs['Y_test_output']\n    )\n    \n    # train_model_svm_task = train_model_svm(\n    #     X_train=prepare_data_task.outputs['X_train_output'], \n    #     Y_train=prepare_data_task.outputs['Y_train_output'],\n    #     X_val=prepare_data_task.outputs['X_val_output'],\n    #     Y_val=prepare_data_task.outputs['Y_val_output'],\n    #     X_test=prepare_data_task.outputs['X_test_output'], \n    #     Y_test=prepare_data_task.outputs['Y_test_output']\n    # )\n\n    train_model_RandomForest_task = train_model_RandomForest(\n        X_train=prepare_data_task.outputs['X_train_output'], \n        Y_train=prepare_data_task.outputs['Y_train_output'],\n        X_val=prepare_data_task.outputs['X_val_output'],\n        Y_val=prepare_data_task.outputs['Y_val_output'],\n        X_test=prepare_data_task.outputs['X_test_output'], \n        Y_test=prepare_data_task.outputs['Y_test_output']\n    )\n\n    train_model_KNN_task = train_model_KNN(\n        X_train=prepare_data_task.outputs['X_train_output'], \n        Y_train=prepare_data_task.outputs['Y_train_output'],\n        X_val=prepare_data_task.outputs['X_val_output'],\n        Y_val=prepare_data_task.outputs['Y_val_output'],\n        X_test=prepare_data_task.outputs['X_test_output'], \n        Y_test=prepare_data_task.outputs['Y_test_output']\n    )\n\n    choose_model_task = choose_model(\n        LogisticRegression_model=train_model_LogisticRegression_task.outputs['model'],\n        XGBoost_model=train_model_xgboost_task.outputs['model'],\n        # SVM_model=train_model_svm_task.outputs['model'],\n        RandomForest_model=train_model_RandomForest_task.outputs['model'],\n        KNN_model=train_model_KNN_task.outputs['model'],\n        lr_file=train_model_LogisticRegression_task.outputs['file'],\n        xgb_file=train_model_xgboost_task.outputs['file'],\n        # svm_file=train_model_svm_task.outputs['file'],\n        rf_file=train_model_RandomForest_task.outputs['file'],\n        knn_file=train_model_KNN_task.outputs['file']\n    )\n    \n    # The pipeline doesn't need to return anything explicitly now\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(hypertension_prediction_pipeline, '../Hypertension_prediction_pipeline.yaml')"}
{"prompt": "Generate a Kubeflow Pipeline named `stackdemos-jj-test2-components-golang-component` that performs the following:\n\nThis pipeline does not use the `@dsl.pipeline` decorator directly but defines two utility functions, `processing_op` and `http_download_op`, which are intended to be used within a Kubeflow pipeline definition. These functions create `ContainerOp` instances.\n\n**Components (as functions returning `ContainerOp`):**\n\n1.  **`processing_op`**:\n    *   **Function:** This is a templated function designed to encapsulate similar container operations. It creates a `ContainerOp` with a dynamically generated name based on the script name, uses a specified Docker image (which can be derived from an IPython environment variable `GOLANG_IMAGE`), executes a given script as its command, and accepts a list of arguments and file outputs.\n    *   **Inputs:**\n        *   `script` (string): The path to the script to be executed.\n        *   `image` (string, optional): The Docker image to use. If not provided and running in IPython, it attempts to get it from `IPython.get_ipython().user_ns.get('GOLANG_IMAGE')`.\n        *   `arguments` (list of strings, optional): Command-line arguments to pass to the script.\n        *   `file_outputs` (dict, optional): A dictionary mapping output names to file paths.\n    *   **Outputs:** Returns a `kfp.dsl.ContainerOp` instance.\n    *   **Logic:**\n        *   It checks if `image` is provided; if not, and it's running in an IPython environment, it tries to retrieve the image from the `GOLANG_IMAGE` user namespace variable.\n        *   If `image` is still missing, it raises a `ValueError`.\n        *   The `ContainerOp`'s name is derived from the `script` name by converting it to lowercase and replacing non-alphanumeric characters with hyphens.\n\n2.  **`http_download_op`**:\n    *   **Function:** This component performs an HTTP download of a specified URL to a target path, including an MD5 sum pre-check to avoid re-downloading.\n    *   **Inputs:**\n        *   `url` (string): The URL of the file to download.\n        *   `download_to` (string): The local path where the file should be downloaded.\n        *   `md5sum` (string): The expected MD5 checksum of the file.\n    *   **Outputs:** Returns a `kfp.dsl.ContainerOp` instance.\n    *   **Logic:**\n        *   Uses the `appropriate/curl` Docker image.\n        *   Executes a shell script (`sh -c`) that first checks if the existing file at `download_to` has the expected `md5sum`.\n        *   If the MD5 sums match, it prints a \"Skipping\" message.\n        *   Otherwise, it uses `curl` to download the file, creating directories as needed, and saving it to `download_to`.\n\n**Control Flow:**\nThese are utility functions, not a complete pipeline definition. Therefore, there are no explicit dependencies or control flow within these functions themselves. They are designed to be called sequentially or in parallel from a higher-level `@dsl.pipeline` function, where the actual `after()` or `parallelFor` constructs would be defined.\n\n**Tools/Libraries Used:**\n*   `kfp.dsl.ContainerOp` for defining container operations.\n*   `urllib.parse` for URL parsing (though not directly used in the provided `ContainerOp` definitions, `urlparse` is imported).\n*   `os` for operating system-dependent functionality (specifically `os.path.splitext`).\n*   `re` for regular expressions (specifically `re.sub` for naming).\n*   `IPython` for checking if running in a notebook environment and accessing user namespaces.\n*   `curl` command-line tool (within `http_download_op`).", "completion": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef processing_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('GOLANG_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=[script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )"}
{"prompt": "Generate a Kubeflow Pipeline named `demo` that performs the following steps:\n\nThis pipeline consists of **two components**.\n\n1.  **Component `echo`**:\n    *   **Function**: Takes a string `text` as input, prints it to standard output, and returns the same string.\n    *   **Inputs**: `text` (string).\n    *   **Outputs**: A string output.\n    *   **Implementation**: This is a Python function decorated with `@kfp.dsl.components.func_to_container_op`.\n\n2.  **Pipeline `demo`**:\n    *   **Function**: Orchestrates the execution of the `echo` component.\n    *   **Inputs**: `text` (string).\n    *   **Control Flow**:\n        *   An instance of the `echo` component, named `op1`, is created and executed with the pipeline input `text`.\n        *   Another instance of the `echo` component, named `op2`, is created and executed with a formatted string `\"%s-%s\" % text` as its input.\n        *   A global `ContainerOpTransform` is applied to all components in the pipeline:\n            *   It sets resource limits: CPU to \"500m\", memory to \"1G\" (request) and \"4G\" (limit).\n            *   It sets the image pull policy to \"Always\".\n            *   It sets an environment variable `ENV` to \"production\".\n            *   It sets an environment variable `AWS_ACCESS_KEY` from a Kubernetes secret named \"aws\" with key \"access_key\".\n            *   It sets an annotation `iam.amazonaws.com/role` to \"some-arn\".\n    *   **Tools/Libraries**: `kfp` (Kubeflow Pipelines SDK), `kubernetes.client` (for Kubernetes resource specifications).", "completion": "\"\"\"Transform functions that modify containerOp.\"\"\"\nfrom fnmatch import fnmatch\nfrom typing import Callable, Dict, List, Tuple, Union\n\nimport kfp.dsl\nimport kubernetes.client as k8s\n\nTransformFunc = Callable[[kfp.dsl.ContainerOp], kfp.dsl.ContainerOp]\n\n\nclass ContainerOpTransform:\n    \"\"\"Helper class to manipulate some common internal properties of ContainerOp.\n\n    Please refer to documentation for full set of transforms available.\n\n    ::\n\n        import kfp.components\n        import kfp.dsl\n        import kfx.dsl\n\n        transforms = (\n            kfx.dsl.ContainerOpTransform()\n            .set_resources(cpu=\"500m\", memory=(\"1G\", \"4G\"))\n            .set_image_pull_policy(\"Always\")\n            .set_env_vars({\"ENV\": \"production\"})\n            .set_env_var_from_secret(\"AWS_ACCESS_KEY\", secret_name=\"aws\", secret_key=\"access_key\")\n            .set_annotations({\"iam.amazonaws.com/role\": \"some-arn\"})\n        )\n\n\n        @kfp.dsl.components.func_to_container_op\n        def echo(text: str) -> str:\n            print(text)\n            return text\n\n\n        @kfp.dsl.pipeline(name=\"demo\")\n        def pipeline(text: str):\n            op1 = echo(text)\n            op2 = echo(\"%s-%s\" % text)\n\n            # u can apply the transform on op1 only\n            # op1.apply(transforms)\n\n            # or apply on all ops in the pipeline\n            kfp.dsl.get_pipeline_conf().add_op_transformer(transforms)\n\n    \"\"\"\n\n    def __init__(self, transforms: List[TransformFunc] = None):\n        \"\"\"Creates a new instance of ContainerOpTransform object.\n\n        Args:\n            transforms (List[TransformFunc], optional): Optional list of custom transform functions. Defaults to None.\n        \"\"\"\n        self._transforms: List[TransformFunc] = transforms or []\n\n    def __call__(self, op: kfp.dsl.ContainerOp) -> kfp.dsl.ContainerOp:\n        \"\"\"In-place transform of the provided ContainerOp.\n\n        Args:\n            op (kfp.dsl.ContainerOp): ContainerOp obj.\n\n        Returns:\n            kfp.dsl.ContainerOp: ContainerOp obj.\n        \"\"\"\n        [  # pylint: disable=expression-not-assigned\n            transform(op) for transform in self._transforms\n        ]\n        return op\n\n    def set_annotations(self, annotations: Dict[str, str]) -> \"ContainerOpTransform\":\n        \"\"\"Update the transform function to set the provided annotations to the ContainerOp.\n\n        Args:\n            annotations (Dict[str, str]): dict of annotation keys and values.\n        \"\"\"\n\n        def set_annotations_transform(op: kfp.dsl.ContainerOp) -> kfp.dsl.ContainerOp:\n            [  # pylint: disable=expression-not-assigned\n                op.add_pod_annotation(name, value)\n                for name, value in annotations.items()\n            ]\n            return op\n\n        self._transforms.append(set_annotations_transform)\n        return self\n\n    def set_labels(self, labels: Dict[str, str]) -> \"ContainerOpTransform\":\n        \"\"\"Update the transform function to set the provided labels to the ContainerOp.\n\n        Args:\n            labels (Dict[str, str]): dict of labels keys and values.\n        \"\"\"\n\n        def set_labels_transform(op: kfp.dsl.ContainerOp) -> kfp.dsl.ContainerOp:\n            [  # pylint: disable=expression-not-assigned\n                op.add_pod_label(name, value) for name, value in labels.items()\n            ]\n            return op\n\n        self._transforms.append(set_labels_transform)\n        return self\n\n    def add_env_vars(self, env_vars: Dict[str, str]) -> \"ContainerOpTransform\":\n        \"\"\"Update the transform function to set the provided env vars to the ContainerOp.\n\n        Args:\n            env_vars (Dict[str, str]): dict of env vars keys and values.\n        \"\"\"\n\n        def set_env_vars_transform(op: kfp.dsl.ContainerOp) -> kfp.dsl.ContainerOp:\n            [  # pylint: disable=expression-not-assigned\n                op.add_env_variable(k8s.V1EnvVar(name, value))\n                for name, value in env_vars.items()\n            ]\n            return op\n\n        self._transforms.append(set_env_vars_transform)\n        return self\n\n    def add_env_var(self, name: str, value: str) -> \"ContainerOpTransform\":\n        \"\"\"Update the transform function to set the provided env var to the ContainerOp.\n\n        Args:\n            name (str): name of the env var.\n            value (str): value of the env var.\n\n        Returns:\n            ContainerOpTransform: updated ContainerOpTransform object.\n        \"\"\"\n        self._transforms.append(\n            lambda op: op.container.add_env_variable(k8s.V1EnvVar(name, value))\n        )\n        return self\n\n    def add_env_var_from_secret(\n        self, name: str, secret_name: str, secret_key: str\n    ) -> \"ContainerOpTransform\":\n        \"\"\"Update the transform function to set the provided env var from a k8s secret.\n\n        Args:\n            name (str): name of the env var.\n            secret_name (str): name of the k8s secret.\n            secret_key (str): key to retrieve from the k8s secret.\n\n        Returns:\n            ContainerOpTransform: updated ContainerOpTransform object.\n        \"\"\"\n        self._transforms.append(\n            lambda op: op.container.add_env_variable(\n                k8s.V1EnvVar(\n                    name,\n                    value_from=k8s.V1EnvVarSource(\n                        secret_key_ref=k8s.V1SecretKeySelector(\n                            key=secret_key, name=secret_name\n                        )\n                    ),\n                )\n            )\n        )\n        return self\n\n    def add_env_var_from_configmap(self, configmap_name: str) -> \"ContainerOpTransform\":\n        \"\"\"Update the transform function to set env vars from a configmap.\n\n        Args:\n            configmap_name (str): name of the configmap.\n\n        Returns:\n            ContainerOpTransform: updated ContainerOpTransform object.\n        \"\"\"\n        self._transforms.append(\n            lambda op: op.container.add_env_from(\n                k8s.V1EnvFromSource(\n                    config_map_ref=k8s.V1ConfigMapEnvSource(name=configmap_name)\n                )\n            )\n        )\n        return self\n\n    def set_cpu_resources(\n        self, request: Union[int, str], limit: Union[int, str] = None\n    ) -> \"ContainerOpTransform\":\n        \"\"\"Update the transform function to set the cpu resources for the main container.\n\n        Args:\n            request (Union[int, str]): How much cpu to request.\n            limit (Union[int, str], optional): Max cpu load before throttling. Defaults to None.\n\n        Returns:\n            ContainerOpTransform: updated ContainerOpTransform object.\n        \"\"\"\n\n        self._transforms.append(\n            lambda op: op.container.set_cpu_request(str(request)).set_cpu_limit(\n                str(limit or request)\n            )\n        )\n\n        return self\n\n    def set_memory_resources(\n        self, request: Union[int, str], limit: Union[int, str] = None\n    ) -> \"ContainerOpTransform\":\n        \"\"\"Update the transform function to set the memory resources for the main container.\n\n        Args:\n            request (Union[int, str]): How much memory to request.\n            limit (Union[int, str], optional): Max memory before killing the pod. Defaults to None.\n\n        Returns:\n            ContainerOpTransform: updated ContainerOpTransform object.\n        \"\"\"\n        self._transforms.append(\n            lambda op: op.container.set_memory_request(str(request)).set_memory_limit(\n                str(limit or request)\n            )\n        )\n\n        return self\n\n    def set_gpu_limit(\n        self, value: Union[int, str], vendor: str = \"nvidia\"\n    ) -> \"ContainerOpTransform\":\n        \"\"\"Update the transform function to set the cpu limit for the main container.\n\n        Args:\n            value (Union[int, str]): GPU limit for the main container.\n            vendor (str, optional): Either \"nvidia\" or \"amd\". Defaults to \"nvidia\".\n\n        Returns:\n            ContainerOpTransform: updated ContainerOpTransform object.\n        \"\"\"\n        self._transforms.append(\n            lambda op: op.container.set_gpu_limit(str(value), vendor)\n        )\n        return self\n\n    def set_image_pull_policy(self, policy: str) -> \"ContainerOpTransform\":\n        \"\"\"Update the transform function to set the image pull policy for the main container.\n\n        Args:\n            policy (str): One of \"Always\", \"Never\", \"IfNotPresent\".\n\n        Returns:\n            ContainerOpTransform: updated ContainerOpTransform object.\n        \"\"\"\n        self._transforms.append(lambda op: op.container.set_image_pull_policy(policy))\n        return self\n\n    def set_sidecar_image_pull_policy(\n        self, policy: str, sidecar_name: str = \"*\"\n    ) -> \"ContainerOpTransform\":\n        \"\"\"Update the transform function to set the image pull policy for the sidecars.\n\n        Args:\n            policy (str): One of \"Always\", \"Never\", \"IfNotPresent\".\n            sidecar_name (str, optional): Glob pattern for sidecar name. Defaults to \"*\".\n\n        Returns:\n            ContainerOpTransform: updated ContainerOpTransform object.\n        \"\"\"\n\n        def set_sidecar_transform(op: kfp.dsl.ContainerOp) -> kfp.dsl.ContainerOp:\n            [  # pylint: disable=expression-not-assigned\n                sidecar.set_image_pull_policy(policy)\n                for sidecar in op.sidecars\n                if fnmatch(sidecar.name, sidecar_name)\n            ]\n\n        self._transforms.append(set_sidecar_transform)\n        return self\n\n    def set_resources(\n        self,\n        cpu: Union[int, str, Tuple[Union[int, str], Union[int, str]]] = None,\n        memory: Union[str, Tuple[str, str]] = None,\n    ) -> \"ContainerOpTransform\":\n        \"\"\"Update the transform function to set the cpu and memory resources for the main container.\n\n        If a int or str is provided, the resource request will equal the limit (i.e. QoS is Guaranteed).\n        Otherwise, a tuple should be provided specifying the request and the limit.\n\n        Args:\n            cpu (Union[int, str, Tuple[Union[int, str], Union[int, str]]], optional): A str or tuple representing the cpu request and limit.\n            memory (Union[str, Tuple[str, str]], optional): A str or tuple representing the memory request and limit.\n\n        Returns:\n            ContainerOpTransform: updated ContainerOpTransform object.\n        \"\"\"\n        if isinstance(cpu, (tuple, list)):\n            cpu_request, cpu_limit = cpu\n        else:\n            cpu_request = cpu  # type: ignore\n            cpu_limit = cpu  # type: ignore\n\n        if isinstance(memory, (tuple, list)):\n            memory_request, memory_limit = memory\n        else:\n            memory_request = memory  # type: ignore\n            memory_limit = memory  # type: ignore\n\n        def set_resources_transform(op: kfp.dsl.ContainerOp) -> kfp.dsl.ContainerOp:\n\n            if cpu_request:\n                op.container.set_cpu_request(str(cpu_request))\n            if cpu_limit:\n                op.container.set_cpu_limit(str(cpu_limit))\n            if memory_request:\n                op.container.set_memory_request(memory_request)\n            if memory_limit:\n                op.container.set_memory_limit(memory_limit)\n            return op\n\n        self._transforms.append(set_resources_transform)\n        return self\n\n    def set_sidecar_resources(\n        self,\n        cpu: Union[int, str, Tuple[Union[int, str], Union[int, str]]] = None,\n        memory: Union[str, Tuple[str, str]] = None,\n        sidecar_name: str = \"*\",\n    ) -> \"ContainerOpTransform\":\n        \"\"\"Update the transform function to set the cpu and memory resources for the sidecars.\n\n        If a int or str is provided, the resource request will equal the limit (i.e. QoS is Guaranteed).\n        Otherwise, a tuple should be provided specifying the request and the limit.\n\n        Args:\n            cpu (Union[int, str, Tuple[Union[int, str], Union[int, str]]], optional): A str or tuple representing the cpu request and limit.\n            memory (Union[str, Tuple[str, str]], optional): A str or tuple representing the memory request and limit.\n            sidecar_name (str, optional): Glob pattern matching the sidecar name. Defaults to \"*\".\n\n        Returns:\n            ContainerOpTransform: updated ContainerOpTransform object.\n        \"\"\"\n        if isinstance(cpu, (tuple, list)):\n            cpu_request, cpu_limit = cpu\n        else:\n            cpu_request = cpu  # type: ignore\n            cpu_limit = cpu  # type: ignore\n\n        if isinstance(memory, (tuple, list)):\n            memory_request, memory_limit = memory\n        else:\n            memory_request = memory  # type: ignore\n            memory_limit = memory  # type: ignore\n\n        def set_sidecar_resources_transform(\n            op: kfp.dsl.ContainerOp,\n        ) -> kfp.dsl.ContainerOp:\n            for sidecar in op.sidecars:\n                if fnmatch(sidecar.name, sidecar_name):\n                    if cpu_request:\n                        sidecar.set_cpu_request(str(cpu_request))\n                    if cpu_limit:\n                        sidecar.set_cpu_limit(str(cpu_limit))\n                    if memory_request:\n                        sidecar.set_memory_request(memory_request)\n                    if memory_limit:\n                        sidecar.set_memory_limit(memory_limit)\n            return op\n\n        self._transforms.append(set_sidecar_resources_transform)\n        return self"}
{"prompt": "Generate a Kubeflow Pipeline named `container-pipeline` that performs an addition operation.\n\nThis pipeline has 1 component:\n1.  **`add`**: This component is designed to perform an addition. It utilizes a custom container image `quay.io/rhiap/kubeflow-example:latest` and executes a Python script located at `components/add.py` within that container. This component doesn't explicitly expose inputs or outputs in the provided DSL definition, but its purpose is implied to perform an addition.\n\nThe pipeline accepts two floating-point parameters:\n-   `a`: Default value is `1.0`.\n-   `b`: Default value is `7.0`.\n\nThe control flow is sequential: the `add` component is the sole task in the pipeline.\n\nThis pipeline uses the `kfp.dsl.ContainerOp` function for defining container-based components. It also implicitly uses Python for the component's internal logic as indicated by the command arguments.", "completion": "\"\"\"Example of a pipeline to demonstrate running code built into the container image.\n\nThis pipeline uses the kfp.dsl.ContainerOp() function which throws some warnings.\nWould be nice to find a better way to run code build into the container image.\n\nThis pipeline example is currently broken.\n\"\"\"\n\nimport os\n\nimport kfp.compiler\nfrom dotenv import load_dotenv\n\nload_dotenv(override=True)\n\nkubeflow_endpoint = os.environ[\"KUBEFLOW_ENDPOINT\"]\nbearer_token = os.environ[\"BEARER_TOKEN\"]\n\n\n@kfp.dsl.pipeline(\n    name=\"container-pipeline\",\n)\ndef add_pipeline(a: float = 1.0, b: float = 7.0):\n    \"\"\"Pipeline to add values.\n\n    Pipeline to take the value of a, add 4 to it and then\n    perform a second task to take the put of the first task and add b.\n    \"\"\"\n    add_op = kfp.dsl.ContainerOp(  # noqa: F841\n        name=\"add\",\n        image=\"quay.io/rhiap/kubeflow-example:latest\",\n        command=[\"sh\", \"-c\"],\n        arguments=[\"python components/add.py\"],\n    )\n\n\nif __name__ == \"__main__\":\n    client = kfp.Client(\n        host=kubeflow_endpoint,\n        existing_token=bearer_token,\n    )\n    arguments = {\"a\": 7.0, \"b\": 8.0}\n    client.create_run_from_pipeline_func(add_pipeline, arguments=arguments, experiment_name=\"submitted-example\")"}
{"prompt": "Generate a Kubeflow Pipeline named `First Pipeline` that performs machine learning classification.\n\nThe pipeline consists of **four** components:\n1.  **`download`**: This component is loaded from the YAML manifest file `download_data/download_data.yaml`. Its function is to download the necessary data for the machine learning tasks. It takes no explicit inputs and produces an output, which is the downloaded data.\n2.  **`decision_tree`**: This component is loaded from the YAML manifest file `decision_tree/decision_tree.yaml`. Its function is to train a Decision Tree model. It takes the output from the `download` component as input and is expected to output a floating-point value representing the model's accuracy.\n3.  **`logistic_regression`**: This component is loaded from the YAML manifest file `logistic_regression/logistic_regression.yaml`. Its function is to train a Logistic Regression model. It also takes the output from the `download` component as input and is expected to output a floating-point value representing the model's accuracy.\n4.  **`show_results`**: This is a Python function-based component decorated with `@func_to_container_op`. Its function is to display the accuracy results from both the Decision Tree and Logistic Regression models. It takes two floating-point inputs: `decision_tree` (accuracy from the Decision Tree model) and `logistic_regression` (accuracy from the Logistic Regression model). It prints these values to the console and has no explicit output.\n\nThe control flow of the pipeline is as follows:\n- The `download` component runs first.\n- Once the `download` component completes, both the `decision_tree` and `logistic_regression` components run **in parallel**, taking the output of `download` as their respective inputs.\n- After both `decision_tree` and `logistic_regression` components have finished executing, the `show_results` component runs, taking the accuracy outputs from `decision_tree` and `logistic_regression` as its inputs.\n\nThe pipeline utilizes the `kfp` library for defining and compiling the pipeline. It does not explicitly use other external machine learning libraries like scikit-learn or data platforms like Snowflake within the pipeline definition itself, but these might be implicitly used within the components defined by the YAML files. The pipeline is compiled to `FirstPipeline.yaml`.", "completion": "import kfp\nfrom kfp import dsl\nfrom kfp.components import func_to_container_op\n\n@func_to_container_op\ndef show_results(decision_tree : float, logistic_regression : float) -> None:\n    # Given the outputs from decision_tree and logistic regression components\n    # the results are shown.\n\n    print(f\"Decision tree (accuracy): {decision_tree}\")\n    print(f\"Logistic regression (accuracy): {logistic_regression}\")\n\n\n@dsl.pipeline(name='First Pipeline', description='Applies Decision Tree and Logistic Regression for classification problem.')\ndef first_pipeline():\n\n    # Loads the yaml manifest for each component\n    download = kfp.components.load_component_from_file('download_data/download_data.yaml')\n    decision_tree = kfp.components.load_component_from_file('decision_tree/decision_tree.yaml')\n    logistic_regression = kfp.components.load_component_from_file('logistic_regression/logistic_regression.yaml')\n\n    # Run download_data task\n    download_task = download()\n\n    # Run tasks \"decison_tree\" and \"logistic_regression\" given\n    # the output generated by \"download_task\".\n    decision_tree_task = decision_tree(download_task.output)\n    logistic_regression_task = logistic_regression(download_task.output)\n\n    # Given the outputs from \"decision_tree\" and \"logistic_regression\"\n    # the component \"show_results\" is called to print the results.\n    show_results(decision_tree_task.output, logistic_regression_task.output)\n\n\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(first_pipeline, 'FirstPipeline.yaml')\n    # kfp.Client().create_run_from_pipeline_func(basic_pipeline, arguments={})"}
{"prompt": "Generate a Kubeflow Pipeline named `CLV Training` that performs Customer Lifetime Value (CLV) prediction using BigQuery for feature engineering and AutoML Tables for model training.\n\nThe pipeline consists of 7 components:\n\n1.  **`load_sales_transactions`**: This component loads sales transaction data.\n    *   **Inputs**: `project_id` (project ID), `source_gcs_path` (GCS path to source data), `transactions_table_name` (name of the transactions table), `bq_dataset_name` (BigQuery dataset name), `dataset_location` (BigQuery dataset location).\n    *   **Outputs**: `sales_transactions_bq_table_id` (BigQuery table ID for sales transactions).\n    *   **Function**: Ingests sales transaction data from GCS into a BigQuery table.\n\n2.  **`prepare_feature_engineering_query`**: This component prepares the SQL query for feature engineering.\n    *   **Inputs**: `project_id`, `sales_transactions_bq_table_id` (from `load_sales_transactions`), `threshold_date` (date for data partitioning), `predict_end` (prediction end date), `max_monetary` (maximum monetary value), `features_table_name` (name for the engineered features table), `bq_dataset_name`, `dataset_location`.\n    *   **Outputs**: `feature_engineering_query` (the SQL query string), `feature_engineering_table_id` (BigQuery table ID for features).\n    *   **Function**: Constructs a BigQuery SQL query to extract and engineer features relevant for CLV prediction.\n\n3.  **`engineer_features`**: This component executes the feature engineering query using BigQuery.\n    *   **Inputs**: `query` (the SQL query from `prepare_feature_engineering_query`), `project_id`, `output_dataset_id` (BigQuery dataset for output), `output_table_id` (BigQuery table for output), `dataset_location`.\n    *   **Outputs**: None directly visible from the snippet, but it creates the BigQuery table specified by `output_table_id`.\n    *   **Function**: Runs the generated SQL query in BigQuery to create the features table.\n\n4.  **`import_dataset`**: This component imports the engineered features dataset into AutoML Tables.\n    *   **Inputs**: `bq_source` (BigQuery table ID of the engineered features table), `dataset_display_name` (display name for the AutoML dataset), `project_id`, `region` (AutoML compute region).\n    *   **Outputs**: `dataset_id` (AutoML dataset ID).\n    *   **Function**: Creates and imports a dataset in AutoML Tables from the BigQuery features table.\n\n5.  **`train_model`**: This component trains a machine learning model using AutoML Tables.\n    *   **Inputs**: `dataset_id` (AutoML dataset ID from `import_dataset`), `model_display_name` (display name for the AutoML model), `target_column_name` (the column to predict), `excluded_features` (features to exclude from training), `optimization_objective` (optimization objective for training), `budget_milli_node_hours` (training budget), `project_id`, `region`.\n    *   **Outputs**: `model_id` (AutoML model ID), `model_evaluation_metrics` (evaluation metrics of the trained model).\n    *   **Function**: Initiates and monitors a model training job in AutoML Tables.\n\n6.  **`deploy_model`**: This component deploys the trained model from AutoML Tables.\n    *   **Inputs**: `model_id` (AutoML model ID from `train_model`), `project_id`, `region`.\n    *   **Outputs**: `endpoint_id` (AutoML endpoint ID).\n    *   **Function**: Deploys the trained AutoML model to an endpoint for predictions.\n\n7.  **`log_metrics`**: This component logs the model evaluation metrics.\n    *   **Inputs**: `metrics` (model evaluation metrics from `train_model`), `model_name` (name of the model).\n    *   **Outputs**: None directly visible.\n    *   **Function**: Records the performance metrics of the trained model, likely for tracking and analysis.\n\n**Control Flow:**\n\nThe pipeline execution flow is sequential with explicit dependencies:\n*   `load_sales_transactions` runs first.\n*   `prepare_feature_engineering_query` depends on the output of `load_sales_transactions`.\n*   `engineer_features` depends on the output of `prepare_feature_engineering_query`.\n*   `import_dataset` depends on the successful completion of `engineer_features`.\n*   `train_model` depends on the output of `import_dataset`.\n*   `deploy_model` depends on the output of `train_model`.\n*   `log_metrics` depends on the output of `train_model`.\n\n**Tools/Libraries Used:**\n\n*   **Kubeflow Pipelines (kfp)**: For defining and orchestrating the pipeline.\n*   **BigQuery**: For data loading and feature engineering.\n*   **AutoML Tables**: For dataset import, model training, and model deployment.\n*   **Google Cloud Storage (GCS)**: For source data.\n*   **Python `pathlib` and `yaml`**: For loading pipeline settings.\n*   **`helper_components`**: Custom Python functions (`load_sales_transactions`, `prepare_feature_engineering_query`) defined as lightweight components.\n*   **`kfp.gcp`**: For GCP-specific functionality (though not explicitly shown in the component definitions, it's imported).\n\nThe pipeline should accept the following parameters: `project_id`, `source_gcs_path`, `staging_gcs_path`, `source_bq_table`, `bq_dataset_name`, `threshold_date`, `predict_end`, `max_monetary`, `features_table_name` (with a default), `transactions_table_name` (with a default), `dataset_location` (with a default), `aml_dataset_name` (with a default), `aml_model_name` (with a default), `aml_compute_region` (with a default), `train_budget` (with a default), `target_column_name` (with a default), `features_to_exclude` (with a default), `optimization_objective` (with a default), and `primary_metric` (with a default). The specific default values for these parameters should be loaded from a `settings.yaml` file.", "completion": "# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#            http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"CLV training and deployment pipeline.\"\"\"\n\nfrom helper_components import load_sales_transactions\nfrom helper_components import prepare_feature_engineering_query\n\nimport kfp\nfrom kfp import gcp\n\nimport pathlib\nimport yaml\n\n# Load pipeline settings\ncompiler_settings = yaml.safe_load(\n    pathlib.Path('settings.yaml').read_text())['compiler_settings']\nargument_defaults = yaml.safe_load(\n    pathlib.Path('settings.yaml').read_text())['argument_defaults']\n\n# Initialize component store\ncomponent_store = kfp.components.ComponentStore(\n    compiler_settings['local_search_paths'],\n    compiler_settings['url_search_prefixes'])\n\n# Create component factories\nload_sales_transactions_op = kfp.components.func_to_container_op(\n    load_sales_transactions,\n    base_image=compiler_settings['lightweight_components_base_image'])\nprepare_feature_engineering_query_op = kfp.components.func_to_container_op(\n    prepare_feature_engineering_query,\n    base_image=compiler_settings['lightweight_components_base_image'])\nengineer_features_op = component_store.load_component('bigquery/query')\nimport_dataset_op = component_store.load_component('aml-import-dataset')\ntrain_model_op = component_store.load_component('aml-train-model')\ndeploy_model_op = component_store.load_component('aml-deploy-model')\nlog_metrics_op = component_store.load_component('aml-log-metrics')\n\n\n# Pipeline definition\n@kfp.dsl.pipeline(\n    name='CLV Training',\n    description='CLV Training Pipeline using BigQuery for feature engineering and Automl Tables for model training'\n)\ndef clv_train(\n    project_id,\n    source_gcs_path,\n    staging_gcs_path,\n    source_bq_table,\n    bq_dataset_name,\n    threshold_date,\n    predict_end,\n    max_monetary,\n    features_table_name=argument_defaults['features_table_name'],\n    transactions_table_name=argument_defaults['transactions_table_name'],\n    dataset_location=argument_defaults['dataset_location'],\n    aml_dataset_name=argument_defaults['aml_dataset_name'],\n    aml_model_name=argument_defaults['aml_model_name'],\n    aml_compute_region=argument_defaults['aml_compute_region'],\n    train_budget=argument_defaults['train_budget'],\n    target_column_name=argument_defaults['target_column_name'],\n    features_to_exclude=argument_defaults['features_to_exclude'],\n    optimization_objective=argument_defaults['optimization_objective'],\n    primary_metric=argument_defaults['primary_metric'],\n    deployment_threshold=argument_defaults['deployment_threshold'],\n    skip_deployment=argument_defaults['skip_deployment'],\n    query_template_uri=argument_defaults['query_template_uri']):\n  \"\"\"Trains and optionally deploys a CLV Model.\"\"\"\n\n  # Load sales transactions\n  load_transactions = load_sales_transactions_op(\n      project_id=project_id,\n      source_gcs_path=source_gcs_path,\n      source_bq_table=source_bq_table,\n      dataset_location=dataset_location,\n      dataset_name=bq_dataset_name,\n      table_id=transactions_table_name)\n\n  # Generate the feature engineering query\n  prepare_query = prepare_feature_engineering_query_op(\n      project_id=project_id,\n      source_table_id=load_transactions.output,\n      destination_dataset=bq_dataset_name,\n      features_table_name=features_table_name,\n      threshold_date=threshold_date,\n      predict_end=predict_end,\n      max_monetary=max_monetary,\n      query_template_uri=query_template_uri)\n\n  # Run the feature engineering query on BigQuery.\n  engineer_features = engineer_features_op(\n      query=prepare_query.outputs['query'],\n      project_id=project_id,\n      dataset_id=prepare_query.outputs['dataset_name'],\n      table_id=prepare_query.outputs['table_name'],\n      output_gcs_path=staging_gcs_path,\n      dataset_location=dataset_location,\n      job_config='')\n\n  source_data_uri = 'bq://{}.{}.{}'.format(\n      project_id, prepare_query.outputs['dataset_name'],\n      prepare_query.outputs['table_name'])\n\n  # Import BQ table with features into AML dataset\n  import_dataset = import_dataset_op(\n      project_id=project_id,\n      region=aml_compute_region,\n      dataset_name=aml_dataset_name,\n      description='',\n      source_data_uri=source_data_uri,\n      target_column_name=target_column_name,\n      weight_column_name='',\n      ml_use_column_name='')\n  import_dataset.after(engineer_features)\n\n  # Train the model\n  train_model = train_model_op(\n      project_id=project_id,\n      region=aml_compute_region,\n      dataset_id=import_dataset.outputs['output_dataset_id'],\n      model_name=aml_model_name,\n      train_budget=train_budget,\n      optimization_objective=optimization_objective,\n      target_name=target_column_name,\n      features_to_exclude=features_to_exclude)\n\n  # Log evaluation metrics\n  log_metrics = log_metrics_op(\n      model_full_id=train_model.outputs['output_model_full_id'],\n      primary_metric=primary_metric)\n\n  # Deploy the model if configured and the primary metric below the threshold\n  with kfp.dsl.Condition(skip_deployment != True):\n    with kfp.dsl.Condition(log_metrics.outputs['output_primary_metric_value'] <\n                           deployment_threshold):\n      deploy_model = deploy_model_op(\n          train_model.outputs['output_model_full_id'])\n\n  # Configure the pipeline to use a service account secret\n  if compiler_settings['use_sa_secret']:\n    steps = [\n        load_transactions, prepare_query, engineer_features, import_dataset,\n        train_model, log_metrics, deploy_model\n    ]\n    for step in steps:\n      step.apply(gcp.use_gcp_secret('user-gcp-sa'))"}
{"prompt": "Generate a Kubeflow Pipeline named `echo-pipeline` that performs the following steps:\n\nThe pipeline consists of a single component:\n1.  **`echo`**: This component is defined by the `@dsl.component()` decorator and is decorated with `@spec`.\n    *   **Function**: Returns the string \"hello, world\".\n    *   **Resource Specifications**:\n        *   `name`: \"Echo Component\"\n        *   `cpu`: \"2\"\n        *   `memory`: \"16G\"\n        *   `gpu`: \"1\"\n        *   `accelerator`: \"NVIDIA_TESLA_T4\"\n        *   `caching`: `True` (enabled)\n    *   **Outputs**: A string.\n\n**Control Flow**: The `echo` component runs directly within the pipeline.\n\n**Tools/Libraries Used**: `kfp`, `yaml`. The `@spec` decorator from `kfp_toolbox.decorators` is used to define resource requirements and metadata for the component.", "completion": "import os\n\nimport yaml\nfrom kfp import compiler as compiler_v1\nfrom kfp import dsl as dsl_v1\nfrom kfp.v2 import compiler, dsl\n\nfrom kfp_toolbox.decorators import (\n    caching,\n    container_spec,\n    display_name,\n    override_docstring,\n    spec,\n)\n\n\nclass TestOverrideDocstring:\n    def test_no_decorators(self):\n        @dsl.component()\n        def echo() -> str:\n            \"\"\"Say hello\n\n            This component just says hello.\n\n            Returns:\n                str: hello\n            \"\"\"\n            return \"hello, world\"\n\n        assert echo.__doc__ == \"Echo\\nSay hello\"\n\n    def test_no_arguments(self):\n        @override_docstring()\n        @dsl.component()\n        def echo() -> str:\n            \"\"\"Say hello\n\n            This component just says hello.\n\n            Returns:\n                str: hello\n            \"\"\"\n            return \"hello, world\"\n\n        assert echo.__doc__ == (\n            \"Say hello\\n\\n            This component just says hello.\\n\\n\"\n            \"            Returns:\\n                str: hello\\n            \"\n        )\n\n    def test_specific_docs(self):\n        @override_docstring(\"Just say hello component\")\n        @dsl.component()\n        def echo() -> str:\n            \"\"\"Say hello\n\n            This component just says hello.\n\n            Returns:\n                str: hello\n            \"\"\"\n            return \"hello, world\"\n\n        assert echo.__doc__ == \"Just say hello component\"\n\n\nclass TestSpec:\n    def test_as_decorator_v1(self, tmp_path):\n        @spec(\n            name=\"Echo Component\",\n            cpu=\"2\",\n            memory=\"16G\",\n            gpu=\"1\",\n            accelerator=\"NVIDIA_TESLA_T4\",\n            caching=True,\n        )\n        @dsl.component()\n        def echo() -> str:\n            return \"hello, world\"\n\n        @dsl.pipeline(name=\"echo-pipeline\")\n        def echo_pipeline():\n            echo()\n\n        pipeline_path = os.fspath(tmp_path / \"pipeline.yaml\")\n        compiler_v1.Compiler(mode=dsl_v1.PipelineExecutionMode.V2_COMPATIBLE).compile(\n            pipeline_func=echo_pipeline, package_path=pipeline_path\n        )\n\n        with open(pipeline_path, \"r\") as f:\n            pipeline_spec = yaml.safe_load(f)\n\n        task = next(\n            t for t in pipeline_spec[\"spec\"][\"templates\"] if t[\"name\"] == \"echo\"\n        )\n        resource_limit = task[\"container\"][\"resources\"][\"limits\"]\n        node_selector = task[\"nodeSelector\"]\n        annotations = task[\"metadata\"][\"annotations\"]\n        labels = task[\"metadata\"][\"labels\"]\n\n        assert resource_limit[\"cpu\"] == \"2\"\n        assert resource_limit[\"memory\"] == \"16G\"\n        assert resource_limit[\"nvidia.com/gpu\"] == \"1\"\n        assert node_selector[\"cloud.google.com/gke-accelerator\"] == \"NVIDIA_TESLA_T4\"\n        assert (\n            annotations[\"pipelines.kubeflow.org/task_display_name\"] == \"Echo Component\"\n        )\n        assert labels[\"pipelines.kubeflow.org/enable_caching\"] == \"true\"\n\n    def test_as_decorator(self, tmp_path):\n        @spec(\n            name=\"Echo Component\",\n            cpu=\"2\",\n            memory=\"16G\",\n            gpu=\"1\",\n            accelerator=\"NVIDIA_TESLA_T4\",\n            caching=True,\n        )\n        @dsl.component()\n        def echo() -> str:\n            return \"hello, world\"\n\n        @dsl.pipeline(name=\"echo-pipeline\")\n        def echo_pipeline():\n            echo()\n\n        pipeline_path = os.fspath(tmp_path / \"pipeline.json\")\n        compiler.Compiler().compile(\n            pipeline_func=echo_pipeline, package_path=pipeline_path\n        )\n\n        with open(pipeline_path, \"r\") as f:\n            pipeline_spec = yaml.safe_load(f)\n\n        container = pipeline_spec[\"pipelineSpec\"][\"deploymentSpec\"][\"executors\"][\n            \"exec-echo\"\n        ][\"container\"]\n        task = pipeline_spec[\"pipelineSpec\"][\"root\"][\"dag\"][\"tasks\"][\"echo\"]\n\n        assert container[\"resources\"][\"cpuLimit\"] == 2.0\n        assert container[\"resources\"][\"memoryLimit\"] == 16.0\n        assert container[\"resources\"][\"accelerator\"][\"count\"] == \"1\"\n        assert container[\"resources\"][\"accelerator\"][\"type\"] == \"NVIDIA_TESLA_T4\"\n        assert task[\"taskInfo\"][\"name\"] == \"Echo Component\"\n        assert task[\"cachingOptions\"][\"enableCache\"] is True\n\n    def test_as_function_v1(self, tmp_path):\n        @dsl.component()\n        def echo() -> str:\n            return \"hello, world\"\n\n        @dsl.pipeline(name=\"echo-pipeline\")\n        def echo_pipeline():\n            spec(\n                name=\"Echo Component\",\n                cpu=\"2\",\n                memory=\"16G\",\n                gpu=\"1\",\n                accelerator=\"NVIDIA_TESLA_T4\",\n                caching=True,\n            )(echo)()\n\n        pipeline_path = os.fspath(tmp_path / \"pipeline.yaml\")\n        compiler_v1.Compiler(mode=dsl_v1.PipelineExecutionMode.V2_COMPATIBLE).compile(\n            pipeline_func=echo_pipeline, package_path=pipeline_path\n        )\n\n        with open(pipeline_path, \"r\") as f:\n            pipeline_spec = yaml.safe_load(f)\n\n        task = next(\n            t for t in pipeline_spec[\"spec\"][\"templates\"] if t[\"name\"] == \"echo\"\n        )\n        resource_limit = task[\"container\"][\"resources\"][\"limits\"]\n        node_selector = task[\"nodeSelector\"]\n        annotations = task[\"metadata\"][\"annotations\"]\n        labels = task[\"metadata\"][\"labels\"]\n\n        assert resource_limit[\"cpu\"] == \"2\"\n        assert resource_limit[\"memory\"] == \"16G\"\n        assert resource_limit[\"nvidia.com/gpu\"] == \"1\"\n        assert node_selector[\"cloud.google.com/gke-accelerator\"] == \"NVIDIA_TESLA_T4\"\n        assert (\n            annotations[\"pipelines.kubeflow.org/task_display_name\"] == \"Echo Component\"\n        )\n        assert labels[\"pipelines.kubeflow.org/enable_caching\"] == \"true\"\n\n    def test_as_function(self, tmp_path):\n        @dsl.component()\n        def echo() -> str:\n            return \"hello, world\"\n\n        @dsl.pipeline(name=\"echo-pipeline\")\n        def echo_pipeline():\n            spec(\n                name=\"Echo Component\",\n                cpu=\"2\",\n                memory=\"16G\",\n                gpu=\"1\",\n                accelerator=\"NVIDIA_TESLA_T4\",\n                caching=True,\n            )(echo)()\n\n        pipeline_path = os.fspath(tmp_path / \"pipeline.json\")\n        compiler.Compiler().compile(\n            pipeline_func=echo_pipeline, package_path=pipeline_path\n        )\n\n        with open(pipeline_path, \"r\") as f:\n            pipeline_spec = yaml.safe_load(f)\n\n        container = pipeline_spec[\"pipelineSpec\"][\"deploymentSpec\"][\"executors\"][\n            \"exec-echo\"\n        ][\"container\"]\n        task = pipeline_spec[\"pipelineSpec\"][\"root\"][\"dag\"][\"tasks\"][\"echo\"]\n\n        assert container[\"resources\"][\"cpuLimit\"] == 2.0\n        assert container[\"resources\"][\"memoryLimit\"] == 16.0\n        assert container[\"resources\"][\"accelerator\"][\"count\"] == \"1\"\n        assert container[\"resources\"][\"accelerator\"][\"type\"] == \"NVIDIA_TESLA_T4\"\n        assert task[\"taskInfo\"][\"name\"] == \"Echo Component\"\n        assert task[\"cachingOptions\"][\"enableCache\"] is True\n\n    def test_disable_caching_v1(self, tmp_path):\n        @spec(caching=False)\n        @dsl.component()\n        def echo() -> str:\n            return \"hello, world\"\n\n        @dsl.pipeline(name=\"echo-pipeline\")\n        def echo_pipeline():\n            echo()\n\n        pipeline_path = os.fspath(tmp_path / \"pipeline.yaml\")\n        compiler_v1.Compiler(mode=dsl_v1.PipelineExecutionMode.V2_COMPATIBLE).compile(\n            pipeline_func=echo_pipeline, package_path=pipeline_path\n        )\n\n        with open(pipeline_path, \"r\") as f:\n            pipeline_spec = yaml.safe_load(f)\n\n        task = next(\n            t for t in pipeline_spec[\"spec\"][\"templates\"] if t[\"name\"] == \"echo\"\n        )\n        labels = task[\"metadata\"][\"labels\"]\n\n        assert labels[\"pipelines.kubeflow.org/enable_caching\"] == \"false\"\n\n    def test_disable_caching(self, tmp_path):\n        @spec(caching=False)\n        @dsl.component()\n        def echo() -> str:\n            return \"hello, world\"\n\n        @dsl.pipeline(name=\"echo-pipeline\")\n        def echo_pipeline():\n            echo()\n\n        pipeline_path = os.fspath(tmp_path / \"pipeline.json\")\n        compiler.Compiler().compile(\n            pipeline_func=echo_pipeline, package_path=pipeline_path\n        )\n\n        with open(pipeline_path, \"r\") as f:\n            pipeline_spec = yaml.safe_load(f)\n\n        task = pipeline_spec[\"pipelineSpec\"][\"root\"][\"dag\"][\"tasks\"][\"echo\"]\n\n        assert \"enableCache\" not in task[\"cachingOptions\"]\n\n    def test_multiple_decorators_v1(self, tmp_path):\n        default_spec = spec(cpu=\"2\", memory=\"16G\")\n\n        @spec(cpu=\"1\")\n        @default_spec\n        @dsl.component()\n        def echo() -> str:\n            return \"hello, world\"\n\n        @dsl.pipeline(name=\"echo-pipeline\")\n        def echo_pipeline():\n            echo()\n\n        pipeline_path = os.fspath(tmp_path / \"pipeline.yaml\")\n        compiler_v1.Compiler(mode=dsl_v1.PipelineExecutionMode.V2_COMPATIBLE).compile(\n            pipeline_func=echo_pipeline, package_path=pipeline_path\n        )\n\n        with open(pipeline_path, \"r\") as f:\n            pipeline_spec = yaml.safe_load(f)\n\n        task = next(\n            t for t in pipeline_spec[\"spec\"][\"templates\"] if t[\"name\"] == \"echo\"\n        )\n        resource_limit = task[\"container\"][\"resources\"][\"limits\"]\n\n        assert resource_limit[\"cpu\"] == \"1\"\n        assert resource_limit[\"memory\"] == \"16G\"\n\n    def test_multiple_decorators(self, tmp_path):\n        default_spec = spec(cpu=\"2\", memory=\"16G\")\n\n        @spec(cpu=\"1\")\n        @default_spec\n        @dsl.component()\n        def echo() -> str:\n            return \"hello, world\"\n\n        @dsl.pipeline(name=\"echo-pipeline\")\n        def echo_pipeline():\n            echo()\n\n        pipeline_path = os.fspath(tmp_path / \"pipeline.json\")\n        compiler.Compiler().compile(\n            pipeline_func=echo_pipeline, package_path=pipeline_path\n        )\n\n        with open(pipeline_path, \"r\") as f:\n            pipeline_spec = yaml.safe_load(f)\n\n        container = pipeline_spec[\"pipelineSpec\"][\"deploymentSpec\"][\"executors\"][\n            \"exec-echo\"\n        ][\"container\"]\n\n        assert container[\"resources\"][\"cpuLimit\"] == 1.0\n        assert container[\"resources\"][\"memoryLimit\"] == 16.0\n\n\nclass TestContainerSpec:\n    def test_as_decorator_v1(self, tmp_path):\n        @container_spec(\n            cpu=\"2\",\n            memory=\"16G\",\n            gpu=\"1\",\n            accelerator=\"NVIDIA_TESLA_T4\",\n        )\n        @dsl.component()\n        def echo() -> str:\n            return \"hello, world\"\n\n        @dsl.pipeline(name=\"echo-pipeline\")\n        def echo_pipeline():\n            echo()\n\n        pipeline_path = os.fspath(tmp_path / \"pipeline.yaml\")\n        compiler_v1.Compiler(mode=dsl_v1.PipelineExecutionMode.V2_COMPATIBLE).compile(\n            pipeline_func=echo_pipeline, package_path=pipeline_path\n        )\n\n        with open(pipeline_path, \"r\") as f:\n            pipeline_spec = yaml.safe_load(f)\n\n        task = next(\n            t for t in pipeline_spec[\"spec\"][\"templates\"] if t[\"name\"] == \"echo\"\n        )\n        resource_limit = task[\"container\"][\"resources\"][\"limits\"]\n        node_selector = task[\"nodeSelector\"]\n\n        assert resource_limit[\"cpu\"] == \"2\"\n        assert resource_limit[\"memory\"] == \"16G\"\n        assert resource_limit[\"nvidia.com/gpu\"] == \"1\"\n        assert node_selector[\"cloud.google.com/gke-accelerator\"] == \"NVIDIA_TESLA_T4\"\n\n    def test_as_decorator(self, tmp_path):\n        @container_spec(\n            cpu=\"2\",\n            memory=\"16G\",\n            gpu=\"1\",\n            accelerator=\"NVIDIA_TESLA_T4\",\n        )\n        @dsl.component()\n        def echo() -> str:\n            return \"hello, world\"\n\n        @dsl.pipeline(name=\"echo-pipeline\")\n        def echo_pipeline():\n            echo()\n\n        pipeline_path = os.fspath(tmp_path / \"pipeline.json\")\n        compiler.Compiler().compile(\n            pipeline_func=echo_pipeline, package_path=pipeline_path\n        )\n\n        with open(pipeline_path, \"r\") as f:\n            pipeline_spec = yaml.safe_load(f)\n\n        container = pipeline_spec[\"pipelineSpec\"][\"deploymentSpec\"][\"executors\"][\n            \"exec-echo\"\n        ][\"container\"]\n\n        assert container[\"resources\"][\"cpuLimit\"] == 2.0\n        assert container[\"resources\"][\"memoryLimit\"] == 16.0\n        assert container[\"resources\"][\"accelerator\"][\"count\"] == \"1\"\n        assert container[\"resources\"][\"accelerator\"][\"type\"] == \"NVIDIA_TESLA_T4\"\n\n    def test_as_function_v1(self, tmp_path):\n        @dsl.component()\n        def echo() -> str:\n            return \"hello, world\"\n\n        @dsl.pipeline(name=\"echo-pipeline\")\n        def echo_pipeline():\n            container_spec(\n                cpu=\"2\",\n                memory=\"16G\",\n                gpu=\"1\",\n                accelerator=\"NVIDIA_TESLA_T4\",\n            )(echo)()\n\n        pipeline_path = os.fspath(tmp_path / \"pipeline.yaml\")\n        compiler_v1.Compiler(mode=dsl_v1.PipelineExecutionMode.V2_COMPATIBLE).compile(\n            pipeline_func=echo_pipeline, package_path=pipeline_path\n        )\n\n        with open(pipeline_path, \"r\") as f:\n            pipeline_spec = yaml.safe_load(f)\n\n        task = next(\n            t for t in pipeline_spec[\"spec\"][\"templates\"] if t[\"name\"] == \"echo\"\n        )\n        resource_limit = task[\"container\"][\"resources\"][\"limits\"]\n        node_selector = task[\"nodeSelector\"]\n\n        assert resource_limit[\"cpu\"] == \"2\"\n        assert resource_limit[\"memory\"] == \"16G\"\n        assert resource_limit[\"nvidia.com/gpu\"] == \"1\"\n        assert node_selector[\"cloud.google.com/gke-accelerator\"] == \"NVIDIA_TESLA_T4\"\n\n    def test_as_function(self, tmp_path):\n        @dsl.component()\n        def echo() -> str:\n            return \"hello, world\"\n\n        @dsl.pipeline(name=\"echo-pipeline\")\n        def echo_pipeline():\n            container_spec(\n                cpu=\"2\",\n                memory=\"16G\",\n                gpu=\"1\",\n                accelerator=\"NVIDIA_TESLA_T4\",\n            )(echo)()\n\n        pipeline_path = os.fspath(tmp_path / \"pipeline.json\")\n        compiler.Compiler().compile(\n            pipeline_func=echo_pipeline, package_path=pipeline_path\n        )\n\n        with open(pipeline_path, \"r\") as f:\n            pipeline_spec = yaml.safe_load(f)\n\n        container = pipeline_spec[\"pipelineSpec\"][\"deploymentSpec\"][\"executors\"][\n            \"exec-echo\"\n        ][\"container\"]\n\n        assert container[\"resources\"][\"cpuLimit\"] == 2.0\n        assert container[\"resources\"][\"memoryLimit\"] == 16.0\n        assert container[\"resources\"][\"accelerator\"][\"count\"] == \"1\"\n        assert container[\"resources\"][\"accelerator\"][\"type\"] == \"NVIDIA_TESLA_T4\"\n\n    def test_multiple_decorators_v1(self, tmp_path):\n        default_spec = container_spec(cpu=\"2\", memory=\"16G\")\n\n        @container_spec(cpu=\"1\")\n        @default_spec\n        @dsl.component()\n        def echo() -> str:\n            return \"hello, world\"\n\n        @dsl.pipeline(name=\"echo-pipeline\")\n        def echo_pipeline():\n            echo()\n\n        pipeline_path = os.fspath(tmp_path / \"pipeline.yaml\")\n        compiler_v1.Compiler(mode=dsl_v1.PipelineExecutionMode.V2_COMPATIBLE).compile(\n            pipeline_func=echo_pipeline, package_path=pipeline_path\n        )\n\n        with open(pipeline_path, \"r\") as f:\n            pipeline_spec = yaml.safe_load(f)\n\n        task = next(\n            t for t in pipeline_spec[\"spec\"][\"templates\"] if t[\"name\"] == \"echo\"\n        )\n        resource_limit = task[\"container\"][\"resources\"][\"limits\"]\n\n        assert resource_limit[\"cpu\"] == \"1\"\n        assert resource_limit[\"memory\"] == \"16G\"\n\n    def test_multiple_decorators(self, tmp_path):\n        default_spec = container_spec(cpu=\"2\", memory=\"16G\")\n\n        @container_spec(cpu=\"1\")\n        @default_spec\n        @dsl.component()\n        def echo() -> str:\n            return \"hello, world\"\n\n        @dsl.pipeline(name=\"echo-pipeline\")\n        def echo_pipeline():\n            echo()\n\n        pipeline_path = os.fspath(tmp_path / \"pipeline.json\")\n        compiler.Compiler().compile(\n            pipeline_func=echo_pipeline, package_path=pipeline_path\n        )\n\n        with open(pipeline_path, \"r\") as f:\n            pipeline_spec = yaml.safe_load(f)\n\n        container = pipeline_spec[\"pipelineSpec\"][\"deploymentSpec\"][\"executors\"][\n            \"exec-echo\"\n        ][\"container\"]\n\n        assert container[\"resources\"][\"cpuLimit\"] == 1.0\n        assert container[\"resources\"][\"memoryLimit\"] == 16.0\n\n\nclass TestDisplayName:\n    def test_v1(self, tmp_path):\n        @display_name(\"Echo Component\")\n        @dsl.component()\n        def echo() -> str:\n            return \"hello, world\"\n\n        @dsl.pipeline(name=\"echo-pipeline\")\n        def echo_pipeline():\n            echo()\n\n        pipeline_path = os.fspath(tmp_path / \"pipeline.yaml\")\n        compiler_v1.Compiler(mode=dsl_v1.PipelineExecutionMode.V2_COMPATIBLE).compile(\n            pipeline_func=echo_pipeline, package_path=pipeline_path\n        )\n\n        with open(pipeline_path, \"r\") as f:\n            pipeline_spec = yaml.safe_load(f)\n\n        task = next(\n            t for t in pipeline_spec[\"spec\"][\"templates\"] if t[\"name\"] == \"echo\"\n        )\n        annotations = task[\"metadata\"][\"annotations\"]\n        assert (\n            annotations[\"pipelines.kubeflow.org/task_display_name\"] == \"Echo Component\"\n        )\n\n    def test(self, tmp_path):\n        @display_name(\"Echo Component\")\n        @dsl.component()\n        def echo() -> str:\n            return \"hello, world\"\n\n        @dsl.pipeline(name=\"echo-pipeline\")\n        def echo_pipeline():\n            echo()\n\n        pipeline_path = os.fspath(tmp_path / \"pipeline.json\")\n        compiler.Compiler().compile(\n            pipeline_func=echo_pipeline, package_path=pipeline_path\n        )\n\n        with open(pipeline_path, \"r\") as f:\n            pipeline_spec = yaml.safe_load(f)\n\n        task = pipeline_spec[\"pipelineSpec\"][\"root\"][\"dag\"][\"tasks\"][\"echo\"]\n        assert task[\"taskInfo\"][\"name\"] == \"Echo Component\"\n\n\nclass TestCaching:\n    def test_no_caching_v1(self, tmp_path):\n        @dsl.component()\n        def echo() -> str:\n            return \"hello, world\"\n\n        @dsl.pipeline(name=\"echo-pipeline\")\n        def echo_pipeline():\n            echo()\n\n        pipeline_path = os.fspath(tmp_path / \"pipeline.yaml\")\n        compiler_v1.Compiler(mode=dsl_v1.PipelineExecutionMode.V2_COMPATIBLE).compile(\n            pipeline_func=echo_pipeline, package_path=pipeline_path\n        )\n\n        with open(pipeline_path, \"r\") as f:\n            pipeline_spec = yaml.safe_load(f)\n\n        task = next(\n            t for t in pipeline_spec[\"spec\"][\"templates\"] if t[\"name\"] == \"echo\"\n        )\n        labels = task[\"metadata\"][\"labels\"]\n\n        assert labels[\"pipelines.kubeflow.org/enable_caching\"] == \"true\"\n\n    def test_no_caching(self, tmp_path):\n        @dsl.component()\n        def echo() -> str:\n            return \"hello, world\"\n\n        @dsl.pipeline(name=\"echo-pipeline\")\n        def echo_pipeline():\n            echo()\n\n        pipeline_path = os.fspath(tmp_path / \"pipeline.json\")\n        compiler.Compiler().compile(\n            pipeline_func=echo_pipeline, package_path=pipeline_path\n        )\n\n        with open(pipeline_path, \"r\") as f:\n            pipeline_spec = yaml.safe_load(f)\n\n        task = pipeline_spec[\"pipelineSpec\"][\"root\"][\"dag\"][\"tasks\"][\"echo\"]\n\n        assert task[\"cachingOptions\"][\"enableCache\"] is True\n\n    def test_enable_caching_v1(self, tmp_path):\n        @caching(True)\n        @dsl.component()\n        def echo() -> str:\n            return \"hello, world\"\n\n        @dsl.pipeline(name=\"echo-pipeline\")\n        def echo_pipeline():\n            echo()\n\n        pipeline_path = os.fspath(tmp_path / \"pipeline.yaml\")\n        compiler_v1.Compiler(mode=dsl_v1.PipelineExecutionMode.V2_COMPATIBLE).compile(\n            pipeline_func=echo_pipeline, package_path=pipeline_path\n        )\n\n        with open(pipeline_path, \"r\") as f:\n            pipeline_spec = yaml.safe_load(f)\n\n        task = next(\n            t for t in pipeline_spec[\"spec\"][\"templates\"] if t[\"name\"] == \"echo\"\n        )\n        labels = task[\"metadata\"][\"labels\"]\n\n        assert labels[\"pipelines.kubeflow.org/enable_caching\"] == \"true\"\n\n    def test_enable_caching(self, tmp_path):\n        @caching(True)\n        @dsl.component()\n        def echo() -> str:\n            return \"hello, world\"\n\n        @dsl.pipeline(name=\"echo-pipeline\")\n        def echo_pipeline():\n            echo()\n\n        pipeline_path = os.fspath(tmp_path / \"pipeline.json\")\n        compiler.Compiler().compile(\n            pipeline_func=echo_pipeline, package_path=pipeline_path\n        )\n\n        with open(pipeline_path, \"r\") as f:\n            pipeline_spec = yaml.safe_load(f)\n\n        task = pipeline_spec[\"pipelineSpec\"][\"root\"][\"dag\"][\"tasks\"][\"echo\"]\n\n        assert task[\"cachingOptions\"][\"enableCache\"] is True\n\n    def test_disable_caching_v1(self, tmp_path):\n        @caching(False)\n        @dsl.component()\n        def echo() -> str:\n            return \"hello, world\"\n\n        @dsl.pipeline(name=\"echo-pipeline\")\n        def echo_pipeline():\n            echo()\n\n        pipeline_path = os.fspath(tmp_path / \"pipeline.yaml\")\n        compiler_v1.Compiler(mode=dsl_v1.PipelineExecutionMode.V2_COMPATIBLE).compile(\n            pipeline_func=echo_pipeline, package_path=pipeline_path\n        )\n\n        with open(pipeline_path, \"r\") as f:\n            pipeline_spec = yaml.safe_load(f)\n\n        task = next(\n            t for t in pipeline_spec[\"spec\"][\"templates\"] if t[\"name\"] == \"echo\"\n        )\n        labels = task[\"metadata\"][\"labels\"]\n\n        assert labels[\"pipelines.kubeflow.org/enable_caching\"] == \"false\"\n\n    def test_disable_caching(self, tmp_path):\n        @caching(False)\n        @dsl.component()\n        def echo() -> str:\n            return \"hello, world\"\n\n        @dsl.pipeline(name=\"echo-pipeline\")\n        def echo_pipeline():\n            echo()\n\n        pipeline_path = os.fspath(tmp_path / \"pipeline.json\")\n        compiler.Compiler().compile(\n            pipeline_func=echo_pipeline, package_path=pipeline_path\n        )\n\n        with open(pipeline_path, \"r\") as f:\n            pipeline_spec = yaml.safe_load(f)\n\n        task = pipeline_spec[\"pipelineSpec\"][\"root\"][\"dag\"][\"tasks\"][\"echo\"]\n\n        assert \"enableCache\" not in task[\"cachingOptions\"]"}
{"prompt": "Generate a Kubeflow Pipeline named `VolumeOp Basic` that performs operations demonstrating basic `VolumeOp` usage.\n\nThe pipeline consists of two components:\n1.  **`create-pvc` (VolumeOp):** This component is responsible for creating a Persistent Volume Claim (PVC).\n    *   It is named `create-pvc`.\n    *   Its resource name is `my-pvc`.\n    *   It uses the `RWO` (ReadWriteOnce) access mode.\n    *   Its size is dynamically determined by the pipeline input `size`.\n    *   It exposes its created volume as an output.\n\n2.  **`cop` (ContainerOp):** This component is a containerized operation that writes data to the created volume.\n    *   It is named `cop`.\n    *   It uses the `library/bash:4.4.23` Docker image.\n    *   It executes the command `sh -c \"echo foo > /mnt/file1\"`, which writes the string \"foo\" to a file named `file1` within the `/mnt` directory.\n    *   **Control Flow:** This component explicitly depends on the `create-pvc` component and mounts its created volume at the `/mnt` path within its container.\n\nThe pipeline takes one input:\n*   `size`: A string representing the desired size for the PVC (e.g., \"1Gi\", \"500Mi\").", "completion": "# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp\nimport kfp.dsl as dsl\n\n\n@dsl.pipeline(\n    name=\"VolumeOp Basic\",\n    description=\"A Basic Example on VolumeOp Usage.\"\n)\ndef volumeop_basic(size):\n    vop = dsl.VolumeOp(\n        name=\"create-pvc\",\n        resource_name=\"my-pvc\",\n        modes=dsl.VOLUME_MODE_RWO,\n        size=size\n    )\n\n    cop = dsl.ContainerOp(\n        name=\"cop\",\n        image=\"library/bash:4.4.23\",\n        command=[\"sh\", \"-c\"],\n        arguments=[\"echo foo > /mnt/file1\"],\n        pvolumes={\"/mnt\": vop.volume}\n    )\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(volumeop_basic, __file__ + '.yaml')"}
{"prompt": "Generate a Kubeflow Pipeline named `A Simple CI pipeline` that performs basic arithmetic operations to demonstrate CI using CloudBuild.\n\nThe pipeline takes three integer inputs: `x_value` (default 1), `y_value` (default 1), and `z_value` (default 1).\n\nIt consists of **three** components:\n\n1.  **`my_add`**: This component is named 'Add x and y' and takes `x_value` and `y_value` as inputs. It produces an output named `sum`.\n2.  **`my_divide`**: This component is named 'Divide sum by z' and takes the `sum` output from the first `add_op` and `z_value` as inputs. This component is executed only if the `sum` from the first `add_op` is not equal to zero. It produces two outputs: `quotient` and `remainder`.\n3.  **`my_add` (second instance)**: This component is named 'Add quotient and remainder' and takes the `quotient` and `remainder` outputs from the `my_divide` component as inputs. This component is dependent on and executed after the `my_divide` component.\n\nThe control flow is as follows:\n*   The first `my_add` component (`add_step`) executes initially with `x_value` and `y_value` from the pipeline inputs.\n*   A `kfp.dsl.Condition` checks if the `sum` output of `add_step` is not equal to zero.\n*   If the condition is met, the `my_divide` component (`divide_step`) executes, taking `sum` from `add_step` and `z_value` as inputs.\n*   Immediately following `divide_step` (and nested within the same condition block), the second `my_add` component (`add_step2`) executes, using the `quotient` and `remainder` outputs from `divide_step`.\n\nThe pipeline utilizes pre-defined components loaded from a `ComponentStore` with `local_search_paths=['components']`. The specific components used are `my_add` and `my_divide`.", "completion": "# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Pipeline workflow definition.\"\"\"\n\nimport kfp\nimport kfp.dsl as dsl\n\n# Initialize component store\ncomponent_store = kfp.components.ComponentStore(\n  local_search_paths=['components'])\n\n# Create component factories\nadd_op = component_store.load_component('my_add')\ndivide_op = component_store.load_component('my_divide')\n\n# Define pipeline\n@dsl.pipeline(\n  name='A Simple CI pipeline',\n  description='Basic sample to show how to do CI with KFP using CloudBuild'\n)\ndef pipeline(\n  x_value: int=1,\n  y_value: int=1,\n  z_value: int=1,\n):\n\n  add_step = add_op(x_value=x_value, y_value=y_value)\n  add_step.set_display_name('Add x and y')\n  add_result = add_step.outputs\n  sum_value = add_result['sum']\n  with kfp.dsl.Condition(sum_value != 0):\n    divide_step = divide_op(x_value=sum_value, y_value=z_value)\n    divide_step.set_display_name('Divide sum by z')\n    add_step2 = add_op(\n      x_value=divide_step.outputs['quotient'],\n      y_value=divide_step.outputs['remainder'])\n    add_step2.set_display_name('Add quotient and remainder')"}
{"prompt": "Generate a Kubeflow Pipeline named `GoLang_pipeline` that performs the following steps:\n\n**Pipeline Description:**\n\nThis pipeline consists of **two** distinct components designed to download an artifact and then process it using a GoLang-based container.\n\n**Components:**\n\n1.  **`download-artifact` (http_download_op):**\n    *   **Function:** Downloads an artifact from a specified URL using `curl`.\n    *   **Inputs:**\n        *   `url`: The URL of the artifact to download.\n        *   `download_to`: The local path where the artifact should be saved.\n        *   `md5sum`: The expected MD5 checksum of the artifact for integrity verification.\n    *   **Logic:** It first checks if a file with the same name and matching MD5 sum already exists at `download_to`. If so, it skips the download. Otherwise, it uses `curl` to download the file to the specified path.\n    *   **Image:** Uses the `appropriate/curl` Docker image.\n\n2.  **`[script-name]` (processing_op):**\n    *   **Function:** Executes a GoLang script. The name of this component is dynamically derived from the `script` parameter by converting it to lowercase and replacing non-alphanumeric characters with hyphens.\n    *   **Inputs:**\n        *   `script`: The path to the GoLang script to be executed. This script is passed as the `command` to the container.\n        *   `image`: The Docker image to use for running the GoLang script. This can optionally be retrieved from a global `GOLANG_IMAGE` variable if running in a Jupyter Notebook. It is a mandatory input otherwise.\n        *   `arguments`: A list of arguments to pass to the GoLang script.\n        *   `file_outputs`: A dictionary defining expected file outputs from the component.\n    *   **Outputs:** The specific outputs are determined by the `file_outputs` parameter.\n    *   **Logic:** It creates a `ContainerOp` that runs the specified `script` using the provided `image` and `arguments`.\n\n**Control Flow:**\n\nThere is no explicit dependency or control flow defined between these two components in the provided code snippet. The `processing_op` and `http_download_op` are standalone functions that define Kubeflow components. When constructing the pipeline, an `after` dependency would typically be used if the `processing_op` needed the output of the `http_download_op`.\n\n**Tools/Libraries:**\n\n*   **Python Libraries:** `kfp.dsl`, `urllib.parse`, `os`, `re`, `IPython` (optional, for notebook environment detection).\n*   **External Tools (within containers):** `curl`, `awk` (used in `http_download_op`).\n*   **GoLang:** The `processing_op` is designed to run GoLang scripts, implying a GoLang runtime/compiler would be present in the specified `image`.", "completion": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef processing_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('GOLANG_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=[script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )"}
{"prompt": "Generate a Kubeflow Pipeline named `Kubeflow Pipeline Test` that performs a machine learning workflow for an XGBoost training scenario.\n\nThis pipeline consists of **three components**:\n\n1.  **`create_pvc` (VolumeOp)**:\n    *   **Function**: Creates a Persistent Volume Claim (PVC) named `my-pvc` with `1Gi` storage in `ReadWriteOnce` mode. This volume will be shared by subsequent components.\n    *   **Inputs**: None (implicit configuration parameters).\n    *   **Outputs**: A `dsl.PipelineVolume` object representing the created PVC.\n\n2.  **`preprocess` (ContainerOp)**:\n    *   **Function**: Executes a preprocessing step for the machine learning pipeline. It uses a custom image `gcr.io/compose-flask/hub:v6`.\n    *   **Inputs**:\n        *   `project` (parameter): Passed from the pipeline's `project` input.\n        *   `mode`: Hardcoded to `cloud`.\n    *   **Outputs**:\n        *   `output`: A file containing the preprocessed output, located at `/tmp/output.txt` within the container.\n    *   **Dependencies**: Requires the `create_pvc` volume operation to complete successfully and mounts the created volume at `/data`.\n\n3.  **`build` (ContainerOp)**:\n    *   **Function**: Executes a build step, likely for model building or further data preparation. It uses a custom image `gcr.io/compose-flask/build:v6`.\n    *   **Inputs**:\n        *   `project` (parameter): Passed from the pipeline's `project` input.\n        *   `mode`: Hardcoded to `cloud`.\n        *   `preprocessed`: Receives the `output` from the `preprocess` component.\n        *   `preprocessed2`: Receives the `output` from the `preprocess` component (note: `preprocess.output` is an alias for `preprocess.outputs['output']`).\n    *   **Outputs**:\n        *   `output`: A file containing the build output, located at `/tmp/output.txt` within the container.\n    *   **Dependencies**: Requires both the `create_pvc` volume operation and the `preprocess` component to complete successfully. It also mounts the `create_pvc` volume at `/data`.\n\n**Pipeline Inputs**:\n\n*   `output` (parameter): A generic output parameter, not explicitly used by the components in the provided snippet.\n*   `project` (parameter): A string specifying the GCP project.\n*   `region` (parameter, default: `us-central1`): A string specifying the GCP region.\n*   `train_data` (parameter, default: `gs://ml-pipeline-playground/sfpd/train.csv`): GCS path to the training data.\n*   `eval_data` (parameter, default: `gs://ml-pipeline-playground/sfpd/eval.csv`): GCS path to the evaluation data.\n*   `schema` (parameter, default: `gs://ml-pipeline-playground/sfpd/schema.json`): GCS path to the data schema.\n*   `target` (parameter, default: `resolution`): The name of the target column.\n*   `rounds` (parameter, default: `200`): Number of training rounds for XGBoost.\n*   `workers` (parameter, default: `2`): Number of workers for training.\n*   `true_label` (parameter, default: `ACTION`): The true label for the target.\n\n**Control Flow**:\n\n1.  `create_pvc` runs first.\n2.  `preprocess` starts *after* `create_pvc` has completed.\n3.  `build` starts *after* both `create_pvc` and `preprocess` have completed.\n\nThe pipeline utilizes custom container images `gcr.io/compose-flask/hub:v6` and `gcr.io/compose-flask/build:v6` for its `ContainerOp` steps. It also demonstrates the use of `dsl.VolumeOp` for shared persistent storage.", "completion": "import kfp.dsl as dsl\nimport kfp.gcp as gcp\nimport kfp.onprem as onprem\n\nfrom string import Template\nimport json\n\n@dsl.pipeline(\n  name='Kubeflow Pipeline Test',\n  description='Kubeflow Pipeline Test'\n)\ndef xgb_train_pipeline(\n    output,\n    project,\n    region='us-central1',\n    train_data='gs://ml-pipeline-playground/sfpd/train.csv',\n    eval_data='gs://ml-pipeline-playground/sfpd/eval.csv',\n    schema='gs://ml-pipeline-playground/sfpd/schema.json',\n    target='resolution',\n    rounds=200,\n    workers=2,\n    true_label='ACTION'):\n    #vol_common = dsl.PipelineVolume()\n    vol_common = dsl.VolumeOp(\n        name=\"create_pvc\",\n        resource_name=\"my-pvc\",\n        modes=dsl.VOLUME_MODE_RWO,\n        size=\"1Gi\"\n    )\n\n    preprocess = dsl.ContainerOp(\n        name='preprocess',\n        image='gcr.io/compose-flask/hub:v6',\n        arguments=[\n            '--project', project,\n            '--mode', 'cloud',\n        ],\n        file_outputs={'output': '/tmp/output.txt'},\n        pvolumes={\"/data\": vol_common.volume}\n    )\n    preprocess.after(vol_common)\n    build = dsl.ContainerOp(\n        name='build',\n        image='gcr.io/compose-flask/build:v6',\n        arguments=[\n            '--project', project,\n            '--mode', 'cloud',\n            '--preprocessed', preprocess.outputs['output'],\n            '--preprocessed2', preprocess.output\n        ],\n        file_outputs={'output': '/tmp/output.txt'},\n        pvolumes={\"/data\": vol_common.volume}\n    )\n    build.after(vol_common)\n    build.after(preprocess)"}
{"prompt": "Generate a Kubeflow Pipeline named `First Pipeline` that performs data extraction, preprocessing, and trains two different classification models.\n\nThe pipeline consists of **four components**:\n1.  **`extract_data`**: This component is responsible for extracting the initial data. It takes no explicit inputs and produces an output that serves as the input for the next stage.\n2.  **`preprocess_data`**: This component performs preprocessing on the extracted data. It takes the output from the `extract_data` component as its input. Notably, caching is disabled for this component (`max_cache_staleness = \"P0D\"`), ensuring it always re-executes. Its output is the preprocessed data, which is then used by the subsequent model training components.\n3.  **`logistic_regression`**: This component trains a logistic regression model. It takes the preprocessed data from the `preprocess_data` component as its input.\n4.  **`random_forest_classifier`**: This component trains a random forest classifier model. It also takes the preprocessed data from the `preprocess_data` component as its input.\n\n**Control Flow:**\n- The `extract_data` component runs first.\n- The `preprocess_data` component runs **after** `extract_data` and consumes its output.\n- Both `logistic_regression` and `random_forest_classifier` components run **in parallel** after `preprocess_data`, and both consume the output of `preprocess_data`.\n\n**Tools/Libraries:**\n- The pipeline is built using the `kfp` (Kubeflow Pipelines) SDK.\n- Each component (`extract_data`, `preprocess_data`, `logistic_regression`, `random_forest_classifier`) is loaded from a YAML manifest file, indicating external component definitions.\n- The pipeline description is \"Applies extraction of data and pre processing it.\"", "completion": "# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Fri Oct  8 17:11:47 2021\n\n@author: Nikhil\n\"\"\"\n\nimport kfp\nfrom kfp import dsl\n\n\n@dsl.pipeline(name='First Pipeline',\n              description='Applies extraction of data and pre processing it.')\ndef first_pipeline():\n    # Loads the yaml manifest for components\n    extract_data = kfp.components.load_component_from_file('components/extract_data/extract_data.yaml')\n    preprocess_data = kfp.components.load_component_from_file('components/preprocess/process_data_classifier.yaml')\n    logistic_regression = kfp.components.load_component_from_file('components/logistic_regression/logistic_regression.yaml')\n    random_forest_classifier = kfp.components.load_component_from_file('components/random_forest/random_forest_classifier.yaml')\n    # Run task\n    extract_task = extract_data()\n    preprocess_task = preprocess_data(extract_task.output)\n    preprocess_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"    # disable caching\n    logistic_regression_task = linear_regression(preprocess_task.output)\n    random_forest_classifier_task = random_forest_classifier(preprocess_task.output)\n\n\n# submit the pipeline for execution.\n# keep in mind to change the host url according to your host.\nif __name__ == '__main__':\n    pipeline = kfp.Client(\n        host='https://4ccec0259a834495-dot-us-central1.pipelines.googleusercontent.com').create_run_from_pipeline_func(\n        first_pipeline, arguments={})"}
{"prompt": "Generate a Kubeflow Pipeline named `DiabetesPredictionPipeline` that performs a machine learning workflow for diabetes prediction.\n\nThe pipeline consists of three components:\n1.  **`load_data`**:\n    *   **Function**: This component is responsible for loading data from a remote URL, performing initial data cleaning and preprocessing, and converting the processed data into Parquet format.\n    *   **Inputs**: None.\n    *   **Outputs**: `load_data_output` (a Parquet file containing the cleaned and preprocessed dataset).\n    *   **Tools/Libraries**: `pandas`, `numpy`, `requests`, `pyarrow`.\n    *   **Details**: It fetches a CSV from a GitHub URL, drops rows with 'No Info' in 'diabetes', selects relevant columns ('gender', 'age', 'bmi', 'HbA1c_level', 'blood_glucose_level', 'diabetes'), handles 'No Info' in numerical columns by converting to NaN and then filling NaNs with column means, and maps 'gender' categories ('Male', 'Female', 'Other') to numerical values, filtering out 'Other'. The final processed DataFrame is saved as a Parquet file.\n\n2.  **`prepare_data`**:\n    *   **Function**: This component takes the preprocessed data, splits it into features (X) and target (y), and then further splits these into training and testing sets.\n    *   **Inputs**: `data` (an `InputPath` representing the Parquet file output from `load_data`).\n    *   **Outputs**: `x_train_output`, `x_test_output`, `y_train_output`, `y_test_output` (all are `OutputPath` for Parquet files containing the respective data splits).\n    *   **Tools/Libraries**: `pandas`, `scikit-learn` (`train_test_split`), `pyarrow`.\n    *   **Details**: It reads the input Parquet file, separates 'diabetes' as the target variable `y` and the rest as features `x`. It then performs a 80/20 train-test split with `random_state=42`. All four resulting dataframes are saved as separate Parquet files.\n\n3.  **`train_model`**:\n    *   **Function**: This component trains a Logistic Regression model using the provided training data.\n    *   **Inputs**: `x_train` (an `InputPath` representing the training features from `prepare_data`), `y_train` (an `InputPath` representing the training target from `prepare_data`).\n    *   **Outputs**: `train_model_output` (an `OutputPath` for the serialized trained model).\n    *   **Tools/Libraries**: `pandas`, `scikit-learn` (`LogisticRegression`), `joblib`, `pyarrow`.\n    *   **Details**: It reads the `x_train` and `y_train` Parquet files, trains a `LogisticRegression` model, and then saves the trained model using `joblib`.\n\n**Control Flow**:\n*   The `prepare_data` component depends on the successful completion of `load_data` and consumes its output as input.\n*   The `train_model` component depends on the successful completion of `prepare_data` and consumes its `x_train_output` and `y_train_output` as inputs.\n\nAll components should use `python:3.9` as their base image.\nThe Kubeflow DSL decorators `@dsl.pipeline` and `@dsl.component` should be used.\nThe outputs should be saved as Parquet files where indicated.", "completion": "import kfp\n\nfrom kfp import dsl\nimport kfp.compiler\nfrom kfp.dsl import OutputPath, InputPath\n# from kfp.components import func_to_container_op\n\n@dsl.component(\n    base_image='python:3.9',\n    packages_to_install=['pandas==2.2.2', 'numpy==2.0.0', 'requests==2.32.3', 'pyarrow==15.0.0']\n)\ndef load_data(load_data_output: OutputPath()):\n    import pandas as pd\n    import numpy as np\n    import io\n    import requests\n    import pyarrow as pa\n    import pyarrow.parquet as pq\n    \n    url = \"https://raw.githubusercontent.com/daniel88516/diabetes-data/main/10k.csv\"\n    s = requests.get(url).content\n    df_data = pd.read_csv(io.StringIO(s.decode('utf-8')))\n    df_data.drop(df_data[df_data['diabetes'] == 'No Info'].index, inplace=True)\n    df_data = df_data[['gender','age', 'bmi', 'HbA1c_level', 'blood_glucose_level', 'diabetes']]\n    df_data.dropna(thresh=4, inplace=True)\n    \n    gender_map = {'Male':0 , 'Female':1  , 'Other':2}\n    df_data['gender'] = df_data['gender'].map(gender_map)\n    df_data = df_data[df_data['gender'] != 2]\n    df_data['age'] = pd.to_numeric(df_data['age'].replace('No Info', np.nan), errors='coerce')\n    df_data['bmi'] = pd.to_numeric(df_data['bmi'].replace('No Info', np.nan), errors='coerce')\n    df_data['HbA1c_level'] = pd.to_numeric(df_data['HbA1c_level'].replace('No Info', np.nan), errors='coerce')\n    df_data['blood_glucose_level'] = pd.to_numeric(df_data['blood_glucose_level'].replace('No Info', np.nan), errors='coerce')\n    \n    df_data = df_data.fillna(df_data.mean())\n    \n    # \u8f49\u63db\u70ba Parquet \u683c\u5f0f\n    table = pa.Table.from_pandas(df_data)\n    pq.write_table(table, load_data_output)\n\n@dsl.component(\n    base_image='python:3.9',\n    packages_to_install=['pandas==2.2.2', 'scikit-learn==1.5.1', 'pyarrow==15.0.0']\n)\ndef prepare_data(data: InputPath(), x_train_output: OutputPath(), x_test_output: OutputPath(), y_train_output: OutputPath(), y_test_output: OutputPath()):\n    import pandas as pd\n    import pyarrow.parquet as pq\n    from sklearn.model_selection import train_test_split\n    \n    df_data = pq.read_table(data).to_pandas()\n    x = df_data.drop(labels=['diabetes'], axis=1)\n    y = df_data[['diabetes']]\n    \n    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n    \n    pq.write_table(pa.Table.from_pandas(x_train), x_train_output)\n    pq.write_table(pa.Table.from_pandas(x_test), x_test_output)\n    pq.write_table(pa.Table.from_pandas(y_train), y_train_output)\n    pq.write_table(pa.Table.from_pandas(y_test), y_test_output)\n\n@dsl.component(\n    base_image='python:3.9',\n    packages_to_install=['pandas==2.2.2', 'scikit-learn==1.5.1', 'joblib==1.4.2', 'pyarrow==15.0.0']\n)\ndef train_model(x_train: InputPath(), y_train: InputPath(), train_model_output: OutputPath()):\n    import pandas as pd\n    import pyarrow.parquet as pq\n    from sklearn.linear_model import LogisticRegression\n    import joblib\n    \n    x_train = pq.read_table(x_train).to_pandas()\n    y_train = pq.read_table(y_train).to_pandas()\n    \n    model = LogisticRegression(random_state=0, max_iter=100) # 100 times for test p.s. it is 10000 times in beginning\n    model.fit(x_train, y_train.values.ravel())\n    \n    joblib.dump(model, train_model_output)\n\n@dsl.component(\n    base_image='python:3.9',\n    packages_to_install=['pandas==2.2.2', 'joblib==1.4.2', 'pyarrow==15.0.0']\n)\ndef evaluate_model(model_path: InputPath(), x_test: InputPath(), y_test: InputPath()) -> str:\n    import pandas as pd\n    import pyarrow.parquet as pq\n    import joblib\n    \n    model = joblib.load(filename=model_path)\n    x_test_df = pq.read_table(x_test).to_pandas()\n    y_test_df = pq.read_table(y_test).to_pandas()\n    \n    accuracy = model.score(x_test_df, y_test_df)\n    \n    return f'Test accuracy: {accuracy}'\n\n@dsl.pipeline(\n    name='Diabetes Prediction Pipeline',\n    description='Using kubeflow pipeline to train and evaluate a diabetes prediction model'\n)\ndef diabetes_prediction_pipeline() -> str:\n    load_data_task = load_data()\n    prepare_data_task = prepare_data(data=load_data_task.outputs['load_data_output'])\n    \n    train_model_task = train_model(\n        x_train = prepare_data_task.outputs['x_train_output'], \n        y_train = prepare_data_task.outputs['y_train_output']\n    )\n    \n    evaluate_model_task = evaluate_model(\n        model_path = train_model_task.outputs['train_model_output'], \n        x_test = prepare_data_task.outputs['x_test_output'], \n        y_test = prepare_data_task.outputs['y_test_output']\n    )\n    \n    return evaluate_model_task.output\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(diabetes_prediction_pipeline, 'diabetes_prediction_pipeline_parquet.yaml')"}
{"prompt": "Generate a Kubeflow Pipeline named `My pipeline` that performs the following steps:\n\nThe pipeline consists of three main components:\n1.  **`multiply`**: This component takes an `input_file` and a `multiplier` to perform a multiplication operation. It produces two outputs: `output_file` (the result of the multiplication) and `output_uri_in_file` (a URI pointing to the output file). It uses the Docker image `hiruna72/multiplier` and mounts a volume at `/data`.\n2.  **`concatenate`**: This component takes two input files, `input_file1` and `input_file2`, and concatenates them. It produces two outputs: `output_file` (the concatenated result) and `output_uri_in_file` (a URI pointing to the output file). It uses the Docker image `hiruna72/concatenate` and mounts a volume at `/data`.\n\nThe pipeline takes one parameter: `rok_url`.\n\nThe control flow is as follows:\n- The pipeline begins by defining a `pvc_op` which creates a Persistent Volume Claim (PVC) with a size of 1GB and a name `my-pvc`. This PVC is then used as a volume across subsequent tasks.\n- A **`multiply`** task named `multiply-op1` is executed.\n    - It takes an `input_file` which is a `kfp.dsl.PipelineParam` named `rok_url`.\n    - It uses a fixed `multiplier` value of `2`.\n    - It directs its `output_uri` to `/data/1.txt` and its `output_uri_in_file` to `/data/1.txt.url`.\n    - It uses the `pvc_op` as its volume.\n- A **`multiply`** task named `multiply-op2` is executed.\n    - It takes an `input_file` which is the `output_file` from `multiply-op1` (i.e., `/data/1.txt`).\n    - It uses a fixed `multiplier` value of `3`.\n    - It directs its `output_uri` to `/data/2.txt` and its `output_uri_in_file` to `/data/2.txt.url`.\n    - It uses the `pvc_op` as its volume.\n    - This task runs `after` `multiply-op1`.\n- A **`concatenate`** task named `concat-op` is executed.\n    - It takes `input_file1` as the `output_file` from `multiply-op1` (i.e., `/data/1.txt`).\n    - It takes `input_fi1e2` as the `output_file` from `multiply-op2` (i.e., `/data/2.txt`).\n    - It directs its `output_uri` to `/data/3.txt` and its `output_uri_in_file` to `/data/3.txt.url`.\n    - It uses the `pvc_op` as its volume.\n    - This task runs `after` both `multiply-op1` and `multiply-op2`.\n\nThe pipeline should be defined using the `@kfp.dsl.pipeline` decorator with the specified name and an empty description.\nThe component functions (`multiply` and `concatenate`) are defined as `kfp.dsl.ContainerOp` and use the specified Docker images, arguments, commands, file outputs, and pvolumes.", "completion": "import kfp\n# import os\n# component_root=\"/home/jovyan\"\n# Load the component by calling load_component_from_file or load_component_from_url\n# To load the component, the pipeline author only needs to have access to the component.yaml file.\n# The Kubernetes cluster executing the pipeline needs access to the container image specified in the component.\n# dummy_op = kfp.components.load_component_from_file(os.path.join(component_root, 'component.yaml')) \n# dummy_op = kfp.components.load_component_from_url('http://....../component.yaml')\n\n# dummy_op is now a \"factory function\" that accepts the arguments for the component's inputs\n# and produces a task object (e.g. ContainerOp instance).\n# Inspect the dummy_op function in Jupyter Notebook by typing \"dummy_op(\" and pressing Shift+Tab\n# You can also get help by writing help(dummy_op) or dummy_op? or dummy_op??\n# The signature of the dummy_op function corresponds to the inputs section of the component.\n# Some tweaks are performed to make the signature valid and pythonic:\n# 1) All inputs with default values will come after the inputs without default values\n# 2) The input names are converted to pythonic names (spaces and symbols replaced\n#    with underscores and letters lowercased).\n\ndef multiply(input_file, multiplier,\n               output_uri, output_uri_in_file,\n               volume,\n               step_name='multiply',\n              mount_output_to='/data'):\n    return kfp.dsl.ContainerOp(\n        name=step_name,\n        image='hiruna72/multiplier@sha256:3016c55dcb8015ef9b457dce839206b5704afacd71a42a688132569d97684f99',\n        arguments=[\n            '--input1-path', input_file,\n            '--param1', multiplier,\n            '--output1-path', output_uri,\n            '--output1-path-file', output_uri_in_file,\n        ],\n        command=['python3', '/pipelines/component/src/multiplier.py'],\n        file_outputs={\n            'output_file': output_uri,\n            'output_uri_in_file': output_uri_in_file,\n        },\n        pvolumes={mount_output_to: volume}\n    )\n\ndef concatenate(input_file1, input_fi1e2,\n               output_uri, output_uri_in_file,\n               volume,\n               step_name='concat',\n              mount_output_to='/data'):\n    return kfp.dsl.ContainerOp(\n        name=step_name,\n        image='hiruna72/concatenate@sha256:2119c2f95d5b65eb02cfca29dbbe6d8d9c1e61d900498ae45381ed9e28b0e48c',\n        arguments=[\n            '--input1-path', input_file1,\n            '--input2-path', input_fi1e2,\n            '--output1-path', output_uri,\n            '--output1-path-file', output_uri_in_file,\n        ],\n        command=['python3', '/pipelines/component/src/concat.py'],\n        file_outputs={\n            'output_file': output_uri,\n            'output_uri_in_file': output_uri_in_file,\n        },\n        pvolumes={mount_output_to: volume}\n    )\n\n\n# Define a pipeline and create a task from a component:\n@kfp.dsl.pipeline(name='My pipeline', description='')\ndef my_pipeline(\n        rok_url,\n        pvc_size='1Gi'):\n    \n    vop = kfp.dsl.VolumeOp(\n        name='create-volume',\n        resource_name='helloPipeline_data',\n        annotations={\"rok/origin\": rok_url},\n        size=pvc_size\n    )\n    \n    compo1 = multiply(\n        input_file='/data/input_compo1.txt',\n        multiplier=7,\n        output_uri='/data/output_compo1.txt',\n        output_uri_in_file='/data/output_compo1_uri.txt',\n        volume=vop.volume\n    )\n    \n    compo2 = multiply(\n        input_file='/data/input_compo2.txt',\n        multiplier=7,\n        output_uri='/data/output_compo2.txt',\n        output_uri_in_file='/data/output_compo2_uri.txt',\n        volume=vop.volume\n    )\n    \n#     compo3 = concatenate(\n#         input_file1='/data/input_compo1.txt',\n#         input_file2='/data/input_compo2.txt',\n#         output_uri='/data/output_compo3.txt',\n#         output_uri_in_file='/data/output_compo3_uri.txt',\n#         volume=vop.volume\n#     )\n    \n    compo3 = concatenate(\n        input_file1=compo1.outputs['output_uri_in_file'],\n        input_fi1e2=compo2.outputs['output_uri_in_file'],\n        output_uri='/data/output_compo3.txt',\n        output_uri_in_file='/data/output_compo3_uri.txt',\n        volume=vop.volume\n    )\n    \nif __name__ == '__main__':\n    import kfp.compiler as compiler\n    compiler.Compiler().compile(my_pipeline, 'small_pipeline.tar.gz')"}
{"prompt": "Generate a Kubeflow Pipeline named `lightweight_python_functions_v2_pipeline` that performs a data processing and machine learning workflow.\n\nThe pipeline consists of three components:\n1.  **`preprocess`**: This component serves as a dummy preprocessing step.\n    *   **Inputs**:\n        *   `message` (string): An input string parameter.\n        *   `empty_message` (string, optional): An input string parameter with an empty string default value.\n    *   **Outputs**:\n        *   `output_dataset_one` (Output[Dataset]): A metadata-rich handle for an output `Dataset` artifact. The component writes the `message` content to this dataset's path.\n        *   `output_dataset_two_path` (OutputPath('Dataset')): A locally accessible filepath for another output `Dataset` artifact. The component writes the `message` content to this path.\n        *   `output_parameter_path` (OutputPath(str)): A locally accessible filepath for an output string parameter. The component writes the `message` content to this path.\n        *   `output_bool_parameter_path` (OutputPath(bool)): A locally accessible filepath for an output boolean parameter. The component writes the string representation of `True` to this path.\n        *   `output_dict_parameter_path` (OutputPath(Dict[str, int])): A locally accessible filepath for an output dictionary parameter. The component writes a JSON string `{'A': 1, 'B': 2}` to this path.\n        *   `output_list_parameter_path` (OutputPath(List[str])): A locally accessible filepath for an output list parameter. The component writes a JSON string `['a', 'b', 'c']` to this path.\n    *   **Functionality**: It writes the input `message` to two different `Dataset` output paths and an output string parameter path. It also writes a boolean, a dictionary, and a list as output parameters. It uses the `json` library for serializing dictionary and list outputs.\n\n2.  **`train`**: This component performs a dummy training step.\n    *   **Inputs**:\n        *   `input_dataset_one` (Input[Dataset]): An input `Dataset` artifact, which is the `output_dataset_one` from the `preprocess` component.\n        *   `input_dataset_two_path` (InputPath('Dataset')): A locally accessible filepath for an input `Dataset` artifact, which is the `output_dataset_two_path` from the `preprocess` component.\n        *   `input_parameter` (str): An input string parameter, which is the `output_parameter_path` from the `preprocess` component.\n        *   `input_bool_parameter` (bool): An input boolean parameter, which is the `output_bool_parameter_path` from the `preprocess` component.\n        *   `input_dict_parameter` (Dict[str, int]): An input dictionary parameter, which is the `output_dict_parameter_path` from the `preprocess` component.\n        *   `input_list_parameter` (List[str]): An input list parameter, which is the `output_list_parameter_path` from the `preprocess` component.\n    *   **Outputs**:\n        *   `output_model` (Output[Model]): A metadata-rich handle for an output `Model` artifact. The component writes a dummy \"trained model\" string to this model's path.\n    *   **Functionality**: It reads data from the provided input datasets and parameters, printing their content to standard output. It then writes a dummy \"trained model\" string to the `output_model` path. It uses the `json` library for deserializing dictionary and list inputs.\n\nThe control flow of the pipeline is sequential:\n*   The `preprocess` component runs first, initialized with the `message` parameter set to \"Hello world\".\n*   The `train` component runs after `preprocess` completes, taking all its inputs from the corresponding outputs of the `preprocess` component.\n\nThe pipeline uses the `kfp.v2.dsl` module for defining components and the pipeline structure, and `kfp.v2.compiler` for compilation. It also utilizes standard Python libraries for file I/O and the `json` library for handling dictionary and list parameters.", "completion": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Sample pipeline for passing data in KFP v2.\"\"\"\nfrom typing import Dict, List\nimport os\n\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import Input, InputPath, Output, OutputPath, Dataset, Model, component\nimport kfp.v2.compiler as compiler\n\n# In tests, we install a KFP package from the PR under test. Users should not\n# normally need to specify `kfp_package_path` in their component definitions.\n_KFP_PACKAGE_PATH = os.getenv('KFP_PACKAGE_PATH')\n\n\n@component(kfp_package_path=_KFP_PACKAGE_PATH)\ndef preprocess(\n    # An input parameter of type string.\n    message: str,\n    # Use Output[T] to get a metadata-rich handle to the output artifact\n    # of type `Dataset`.\n    output_dataset_one: Output[Dataset],\n    # A locally accessible filepath for another output artifact of type\n    # `Dataset`.\n    output_dataset_two_path: OutputPath('Dataset'),\n    # A locally accessible filepath for an output parameter of type string.\n    output_parameter_path: OutputPath(str),\n    # A locally accessible filepath for an output parameter of type bool.\n    output_bool_parameter_path: OutputPath(bool),\n    # A locally accessible filepath for an output parameter of type dict.\n    output_dict_parameter_path: OutputPath(Dict[str, int]),\n    # A locally accessible filepath for an output parameter of type list.\n    output_list_parameter_path: OutputPath(List[str]),\n    # An input message that defaults to empty.\n    empty_message: str = \"\",\n):\n    \"\"\"Dummy preprocessing step\"\"\"\n\n    # Use Dataset.path to access a local file path for writing.\n    # One can also use Dataset.uri to access the actual URI file path.\n    with open(output_dataset_one.path, 'w') as f:\n        f.write(message)\n\n    # OutputPath is used to just pass the local file path of the output artifact\n    # to the function.\n    with open(output_dataset_two_path, 'w') as f:\n        f.write(message)\n\n    with open(output_parameter_path, 'w') as f:\n        f.write(message)\n\n    with open(output_bool_parameter_path, 'w') as f:\n        f.write(\n            str(True)\n        )  # use either `str()` or `json.dumps()` for bool values.\n\n    import json\n    with open(output_dict_parameter_path, 'w') as f:\n        f.write(json.dumps({'A': 1, 'B': 2}))\n\n    with open(output_list_parameter_path, 'w') as f:\n        f.write(json.dumps(['a', 'b', 'c']))\n\n\n@component(kfp_package_path=_KFP_PACKAGE_PATH)\ndef train(\n    # Use InputPath to get a locally accessible path for the input artifact\n    # of type `Dataset`.\n    dataset_one_path: InputPath('Dataset'),\n    # Use Input[T] to get a metadata-rich handle to the input artifact\n    # of type `Dataset`.\n    dataset_two: Input[Dataset],\n    # An input parameter of type string.\n    message: str,\n    # Use Output[T] to get a metadata-rich handle to the output artifact\n    # of type `Dataset`.\n    model: Output[Model],\n    # An input parameter of type bool.\n    input_bool: bool,\n    # An input parameter of type dict.\n    input_dict: Dict[str, int],\n    # An input parameter of type List[str].\n    input_list: List[str],\n    # An input parameter of type int with a default value.\n    num_steps: int = 100,\n):\n    \"\"\"Dummy Training step\"\"\"\n    with open(dataset_one_path, 'r') as input_file:\n        dataset_one_contents = input_file.read()\n\n    with open(dataset_two.path, 'r') as input_file:\n        dataset_two_contents = input_file.read()\n\n    line = (\n        f'dataset_one_contents: {dataset_one_contents} || '\n        f'dataset_two_contents: {dataset_two_contents} || '\n        f'message: {message} || '\n        f'input_bool: {input_bool}, type {type(input_bool)} || '\n        f'input_dict: {input_dict}, type {type(input_dict)} || '\n        f'input_list: {input_list}, type {type(input_list)} \\n'\n    )\n\n    with open(model.path, 'w') as output_file:\n        for i in range(num_steps):\n            output_file.write('Step {}\\n{}\\n=====\\n'.format(i, line))\n\n    # Use model.get() to get a Model artifact, which has a .metadata dictionary\n    # to store arbitrary metadata for the output artifact.\n    model.metadata['accuracy'] = 0.9\n\n\n@dsl.pipeline(name='my-test-pipeline-beta')\ndef pipeline(message: str = 'message'):\n    preprocess_task = preprocess(message=message)\n    train_task = train(\n        dataset_one_path=preprocess_task.outputs['output_dataset_one'],\n        dataset_two=preprocess_task.outputs['output_dataset_two_path'],\n        message=preprocess_task.outputs['output_parameter_path'],\n        input_bool=preprocess_task.outputs['output_bool_parameter_path'],\n        input_dict=preprocess_task.outputs['output_dict_parameter_path'],\n        input_list=preprocess_task.outputs['output_list_parameter_path'],\n    )\n\n\nif __name__ == '__main__':\n    compiler.Compiler().compile(\n        pipeline_func=pipeline, package_path=__file__.replace('.py', '.json')\n    )"}
{"prompt": "Generate a Kubeflow Pipeline named `SF_Crime_Prediction_Batch_Serving` that performs batch inference for crime prediction.\n\nThe pipeline consists of **6 components**:\n\n1.  **`get_data`**:\n    *   **Function**: Collects batch serving data from the San Francisco OpenData Socrata API. It queries for incidents from two days prior to the current date.\n    *   **Inputs**: None.\n    *   **Outputs**:\n        *   `serving_data_path` (Output[Dataset]): Path to the CSV file containing the raw batch serving data.\n    *   **Tools/Libraries**: `sodapy`, `pandas`.\n    *   **Base Image**: `python:3.9`.\n\n2.  **`transform_cat_label_data`**:\n    *   **Function**: Transforms categorical features and labels in the serving data using pre-trained `OrdinalEncoder` and `LabelEncoder` models. It handles unknown categorical values by mapping them to 'UNK'. It downloads the encoders from Google Cloud Storage.\n    *   **Inputs**:\n        *   `df_path` (Input[Dataset]): Path to the raw serving data CSV file from the `get_data` component.\n        *   `cat_encoder_path` (str): Local path to save the downloaded categorical encoder.\n        *   `label_encoder_path` (str): Local path to save the downloaded label encoder.\n        *   `projectid` (str): Google Cloud Project ID.\n        *   `bucket_name` (str): Google Cloud Storage bucket name where encoders are stored.\n        *   `blob_path_cat_enc` (str): GCS blob path for the categorical encoder.\n        *   `blob_path_lab_enc` (str): GCS blob path for the label encoder.\n    *   **Outputs**:\n        *   `df_output_path` (Output[Dataset]): Path to the CSV file containing the transformed DataFrame.\n        *   `label_output_path` (Output[Artifact]): Path to the pickled transformed labels.\n    *   **Tools/Libraries**: `pandas`, `google-cloud-storage`, `scikit-learn`.\n    *   **Base Image**: `python:3.9`.\n\n3.  **`predict_crime_category`**:\n    *   **Function**: Loads a pre-trained XGBoost model and uses it to predict crime categories on the transformed serving data. It loads the model from Google Cloud Storage.\n    *   **Inputs**:\n        *   `df_path` (Input[Dataset]): Path to the transformed DataFrame from `transform_cat_label_data`.\n        *   `model_path` (str): Local path to save the downloaded model.\n        *   `projectid` (str): Google Cloud Project ID.\n        *   `bucket_name` (str): Google Cloud Storage bucket name where the model is stored.\n        *   `blob_path_model` (str): GCS blob path for the XGBoost model.\n    *   **Outputs**:\n        *   `predictions_path` (Output[Dataset]): Path to the CSV file containing the predictions.\n    *   **Tools/Libraries**: `pandas`, `xgboost`, `google-cloud-storage`.\n    *   **Base Image**: `python:3.9`.\n\n4.  **`evaluate_predictions`**:\n    *   **Function**: Evaluates the model's predictions against the true labels, calculating metrics such as accuracy, precision, recall, and F1-score.\n    *   **Inputs**:\n        *   `predictions_path` (Input[Dataset]): Path to the predictions CSV from `predict_crime_category`.\n        *   `true_labels_path` (Input[Artifact]): Path to the true labels (pickled) from `transform_cat_label_data`.\n    *   **Outputs**:\n        *   `metrics_output` (Output[Metrics]): Kubeflow Metrics object containing evaluation results.\n    *   **Tools/Libraries**: `pandas`, `sklearn`.\n    *   **Base Image**: `python:3.9`.\n\n5.  **`upload_predictions`**:\n    *   **Function**: Uploads the raw serving data, transformed data, and predictions to Google Cloud Storage.\n    *   **Inputs**:\n        *   `raw_data_path` (Input[Dataset]): Path to the raw serving data from `get_data`.\n        *   `transformed_data_path` (Input[Dataset]): Path to the transformed data from `transform_cat_label_data`.\n        *   `predictions_path` (Input[Dataset]): Path to the predictions from `predict_crime_category`.\n        *   `projectid` (str): Google Cloud Project ID.\n        *   `bucket_name` (str): Google Cloud Storage bucket name for uploads.\n    *   **Outputs**: None.\n    *   **Tools/Libraries**: `pandas`, `google-cloud-storage`.\n    *   **Base Image**: `python:3.9`.\n\n6.  **`send_slack_alert`**:\n    *   **Function**: Sends a Slack alert with the evaluation metrics.\n    *   **Inputs**:\n        *   `metrics_json` (str): JSON string of the evaluation metrics from `evaluate_predictions`.\n        *   `webhook_url` (str): Slack webhook URL for sending alerts.\n    *   **Outputs**: None.\n    *   **Tools/Libraries**: `requests`.\n    *   **Base Image**: `python:3.9`.\n\n**Control Flow and Dependencies**:\n\n*   The pipeline starts with `get_data`.\n*   `transform_cat_label_data` depends on the output of `get_data` (raw serving data).\n*   `predict_crime_category` depends on the transformed data output from `transform_cat_label_data`.\n*   `evaluate_predictions` depends on the predictions from `predict_crime_category` and the transformed labels from `transform_cat_label_data`.\n*   `upload_predictions` depends on the raw data from `get_data`, the transformed data from `transform_cat_label_data`, and the predictions from `predict_crime_category`.\n*   `send_slack_alert` depends on the metrics output from `evaluate_predictions`.\n\n**Pipeline Parameters**:\n\nThe pipeline will accept the following parameters:\n*   `projectid` (str): GCP Project ID.\n*   `bucket_name` (str): GCS bucket name.\n*   `blob_path_cat_enc` (str): GCS blob path for the categorical encoder.\n*   `blob_path_lab_enc` (str): GCS blob path for the label encoder.\n*   `blob_path_model` (str): GCS blob path for the trained model.\n*   `webhook_url` (str): Slack webhook URL.\n*   `cat_encoder_path` (str): Local path to store the downloaded categorical encoder.\n*   `label_encoder_path` (str): Local path to store the downloaded label encoder.\n*   `model_path` (str): Local path to store the downloaded model.\n\nThe components should utilize the `kfp.v2.dsl` decorators (`@component`, `Input`, `Output`, `Dataset`, `Artifact`, `Model`, `Metrics`). The base image for all components should be `python:3.9`. All component definitions should be output to a YAML file named `new_yamls/collect_batch_data.yaml`.", "completion": "import os\nimport argparse\nfrom variables import *\nfrom common_fns import *\nfrom data_ingest_fns import preprocess_data, map_label, merge_with_nfl, add_dummy_cols\nfrom kfp.dsl.structures import yaml\nfrom kfp.client import Client\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import component\nfrom kfp.v2.dsl import (\n    Input,\n    Output,\n    Artifact,\n    Dataset,\n    Model,\n    Metrics\n)\n\n@component(\n    base_image='python:3.9',\n    packages_to_install=['sodapy', 'pandas'],\n    output_component_file='new_yamls/collect_batch_data.yaml'\n)\ndef get_data(serving_data_path: Output[Dataset]):\n    from datetime import datetime, timedelta\n    from sodapy import Socrata\n    import pandas as pd\n    import pytz\n\n    client = Socrata('data.sfgov.org', None)\n    police_dataset_id = 'wg3w-h783'\n    date_req = (datetime.now(pytz.timezone('US/Pacific')) - timedelta(days=2)).strftime(\"%Y-%m-%dT00:00:00\")\n    results = client.get(police_dataset_id, query= f'select * where incident_date = \"{date_req}\"')\n    if len(results) > 0:\n        df = pd.DataFrame(results).drop('point', axis = 1).drop_duplicates('incident_id')\n        # df['filed_online'] = 1\n        df.to_csv(serving_data_path.path, index=False)\n\n    else:\n        print ('No Serving Data')\n\n\n@component(\n    base_image='python:3.9',\n    packages_to_install=['pandas', 'google-cloud-storage', 'scikit-learn'],\n    output_component_file = 'new_yamls/collect_batch_data.yaml'\n)\ndef transform_cat_label_data(df_path: Input[Dataset], cat_encoder_path: str,\n                             label_encoder_path: str, projectid: str,\n                             bucket_name: str, blob_path_cat_enc: str,\n                             blob_path_lab_enc: str, df_output_path: Output[Dataset],\n                             label_output_path: Output[Artifact]):\n    import pandas as pd\n    import pickle\n    from google.cloud import storage\n\n    client = storage.Client(project=projectid)\n    bucket = client.get_bucket(bucket_name)\n    blob_cat = bucket.blob(blob_path_cat_enc)\n    blob_lab_enc = bucket.blob(blob_path_lab_enc)\n    blob_cat.download_to_filename(cat_encoder_path)\n    blob_lab_enc.download_to_filename(label_encoder_path)\n\n    with open(label_encoder_path, 'rb') as file:\n        lab_enc = pickle.load(file)\n\n    with open(cat_encoder_path, 'rb') as file:\n        cat_enc = pickle.load(file)\n\n    df = pd.read_csv(df_path.path)\n    labels = df.pop('incident_category')\n    for col in cat_enc.keys():\n        df[col] = df[col].astype(str)\n        new_values_index = df.loc[~df[col].isin(cat_enc[col].classes_)].index\n        print (new_values_index)\n        if len(new_values_index) > 0:\n            print (f'New values found in {col} column: {df.loc[new_values_index, col].values.tolist()}')\n        df.loc[new_values_index, col] = 'UNK'\n        df[col] = cat_enc[col].transform(df[col])\n    labels = lab_enc.transform(labels)\n    df.to_csv(df_output_path.path)\n    with open(label_output_path.path, 'wb') as file:\n        pickle.dump(labels, file)\n\n@component(\n    base_image='python:3.9',\n    packages_to_install=['scikit-learn', 'pandas', 'google-cloud-storage',\n                         'google-cloud-aiplatform', 'xgboost', 'pyarrow'],\n    output_component_file='new_yamls/predict_serving.yaml'\n)\ndef serving_predict(serving_data_path: Input[Dataset], labels_path: Input[Artifact],\n                    model_name: str, req_columns: list, output_df_path: Output[Dataset],\n                    projectid: str, cat_cols: list, float_cols: list,\n                    binary_cols: list):\n    from xgboost import XGBClassifier\n    import pandas as pd\n    from google.cloud import storage, aiplatform\n    import pickle\n    import os\n\n    models = aiplatform.Model.list(\n    filter=f'display_name={model_name}',\n    order_by=\"update_time\",\n    location='us-central1')\n    model = XGBClassifier()\n    if len(models) > 0:\n        serving_data = pd.read_csv(serving_data_path.path)[binary_cols+float_cols+cat_cols+req_columns]\n        with open(labels_path.path, 'rb') as file:\n            y_labels = pickle.load(file)\n        latest_model = models[-1]\n        model_path = latest_model.to_dict()['artifactUri']\n        path_split = model_path.split('/')\n        bucket_name = path_split[2]\n        blob_name = os.path.join('/'.join(path_split[3:]), 'model.pkl')\n        client = storage.Client(project=projectid)\n        bucket = client.get_bucket(bucket_name)\n        blob = bucket.blob(blob_name)\n        blob.download_to_filename('model.pkl')\n        model.load_model('model.pkl')\n        y_pred = model.predict(serving_data.drop(req_columns, axis = 1).values)\n        serving_data['Prediction'] = y_pred\n        serving_data['incident_category'] = y_labels\n        serving_data['data_type'] = 'non_ingest'\n        serving_data.to_csv(output_df_path.path, index=False)\n\n@dsl.pipeline(\n    name='SF Crime predict pipeline',\n    description= 'Reading data, using the same transformations and predict')\ndef sf_pipeline_predict(projectid:str =projectid, bucket_name:str =bucket_name, model_name:str =model_name,\n                       float_cols:list =float_cols, cat_cols:list =cat_cols, binary_cols:list =binary_cols,\n                       label_name:str =label_name, datasetid:str = datasetid, tableid:str = tablename, schema:dict =schema):\n    get_serving_data_task = get_data()\n    serving_preprocess_task = preprocess_data(df_path= get_serving_data_task.outputs['serving_data_path'], data='serving',\n                                           to_keep = ['incident_id', 'police_district', 'incident_datetime',\n                                      'latitude', 'longitude', 'filed_online', 'incident_day_of_week',\n                                      'report_datetime', 'analysis_neighborhood', 'supervisor_district',\n                                      'incident_category'])\n    map_serving_label_task = map_label(df_path= serving_preprocess_task.outputs['df_output_path'])\n    serving_merge_nfl_task = merge_with_nfl(sf_dataset = map_serving_label_task.outputs['mapped_df_path'])\n    insert_dummy_cols_task = add_dummy_cols(df_path= serving_merge_nfl_task.outputs['merged_csv'])\n    post_proc_serve_task = post_processing_after_split(input_df_path= insert_dummy_cols_task.outputs['output_df'])\n    transform_cat_label_task = transform_cat_label_data(df_path=post_proc_serve_task.outputs['output_df_path'],\n                                                        cat_encoder_path='cat_lab_encoder.pkl',\n                                                        label_encoder_path='target_lab_encoder.pkl',\n                                                        projectid=projectid,\n                                                        bucket_name=bucket_name,\n                                                        blob_path_cat_enc='cat_lab_encoder.pkl',\n                                                        blob_path_lab_enc='target_lab_encoder.pkl')\n    predict_serving_task = serving_predict(serving_data_path=transform_cat_label_task.outputs['df_output_path'],\n            labels_path = transform_cat_label_task.outputs['label_output_path'], model_name=model_name, \n            req_columns = ['incident_id', 'report_datetime','incident_datetime', 'latitude', 'year', 'longitude', 'datetime', \n                           'incident_date', 'incident_time', 'gameday', 'gametime'],\n            projectid=projectid, float_cols= float_cols, cat_cols= cat_cols, binary_cols= binary_cols)\n\n    \n    decode_serve_columns_task = decode_cols(df_path= predict_serving_task.outputs['output_df_path'], \n                                            projectid= projectid, bucket_name= bucket_name,\n                target_col= label_name, cat_enc_path= 'cat_lab_encoder.pkl', label_enc_path= 'target_lab_encoder.pkl', \n                                            cols_to_decode= ['police_district', 'incident_day_of_week',\n                                            'analysis_neighborhood', 'supervisor_district', 'month', 'hour',\n                                            'latitude_buckets', 'longitude_buckets'])\n\n    upload_serving_task = upload_to_bq(df_path = decode_serve_columns_task.outputs['output_df'], projectid= projectid,\n                                datasetid= datasetid, tableid= tableid, location='US',\n                                schema = schema)\n    \nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='Kubeflow pipeline to run inference on new data.')\n    parser.add_argument('type', help='Enter \"yaml\" if file needs to be saved or \"run\" if pipline needs to be run directly',\n                       type=str)\n    args = parser.parse_args()\n    if args.type == 'yaml':\n        if not os.path.exists('new_yamls'):\n            os.mkdir('new_yamls')\n        kfp.compiler.Compiler().compile(\n            pipeline_func=sf_training_pipeline,\n            package_path='new_yamls/serving_pipeline.yaml')\n    elif args.type == 'run':\n        client = Client(host= host_url)\n        experiments = client.list_experiments().experiments\n        experiment_names = list(map(lambda x: x.display_name, experiments))\n        if serving_experiment_name not in experiment_names:\n            client.create_experiment(name=serving_experiment_name)\n        print ('HERE!!')\n        client.create_run_from_pipeline_func(\n        pipeline_func=sf_pipeline_predict,\n        experiment_name=serving_experiment_name)"}
{"prompt": "Generate a Kubeflow Pipeline named `stackdemos-ghsumm2-components-training-component` that defines two reusable container operation factory functions:\n\n**1. `training_op`:**\n   - **Purpose:** Encapsulates the execution of Python training scripts within a container.\n   - **Inputs:**\n     - `script` (str): The path to the Python script to be executed.\n     - `image` (str, optional): The Docker image to use for the container. If not provided and running in an IPython environment, it attempts to retrieve a global `TRAINING_IMAGE` variable. Raises a `ValueError` if `image` is still missing.\n     - `arguments` (list, optional): A list of command-line arguments to pass to the script.\n     - `file_outputs` (dict, optional): A dictionary defining named file outputs for the component.\n   - **Functionality:**\n     - Dynamically generates the component name by cleaning the script name (e.g., `my_script.py` becomes `my-script`).\n     - Sets the container `command` to execute the Python script using `/usr/local/bin/python`.\n     - Passes the provided `arguments` to the script.\n     - Maps `file_outputs` to the component's outputs.\n   - **Libraries/Tools:** `re`, `os` for name manipulation.\n\n**2. `http_download_op`:**\n   - **Purpose:** Downloads a file from a given URL using `curl` and performs an MD5 checksum pre-check.\n   - **Inputs:**\n     - `url` (str): The URL of the file to download.\n     - `download_to` (str): The destination path where the file should be saved.\n     - `md5sum` (str): The expected MD5 checksum of the file.\n   - **Functionality:**\n     - Sets the component name to `download-artifact`.\n     - Uses the `appropriate/curl` Docker image.\n     - Executes a shell command (`sh -c`) that:\n       - Checks if the calculated MD5 sum of `download_to` matches the provided `md5sum`.\n       - If they match, it prints a \"Skipping\" message.\n       - Otherwise, it uses `curl` with options `-#Lv --create-dirs -o` to download the file from `url` to `download_to`.\n   - **Libraries/Tools:** `curl`, `awk` (implicitly via shell command).\n\nThe pipeline does not define a top-level `@dsl.pipeline` function but provides these two utility functions to be used as building blocks in other Kubeflow Pipelines.", "completion": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef training_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('TRAINING_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=['/usr/local/bin/python', script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )"}
{"prompt": "Generate a Kubeflow Pipeline named `DistributedTrainingPipeline` that performs a machine learning workflow involving data loading, parameter parsing, distributed training with Katib, and Spark-based processing.\n\nThe pipeline consists of 7 components:\n\n1.  **`load_file_from_nas_to_minio`**:\n    *   **Function**: Loads CSV files (X_train, X_test, Y_train, Y_test) from specified input paths into Kubeflow Pipeline `Dataset` artifacts. It uses the `pandas` library for data reading and writing.\n    *   **Inputs**:\n        *   `x_train_input_path` (str)\n        *   `x_test_input_path` (str)\n        *   `y_train_input_path` (str)\n        *   `y_test_input_path` (str)\n    *   **Outputs**:\n        *   `x_train_output` (Output[Dataset])\n        *   `x_test_output` (Output[Dataset])\n        *   `y_train_output` (Output[Dataset])\n        *   `y_test_output` (Output[Dataset])\n    *   **Base Image**: `python:3.10-slim`\n    *   **Packages**: `pandas`\n\n2.  **`parse_input_json`**:\n    *   **Function**: Parses a JSON file containing input parameters for different machine learning methods (xgboost, random_forest, knn, lr). It extracts relevant metrics and logs them to separate Kubeflow Pipeline `Metrics` artifacts based on the `method` field. It uses the `json` library.\n    *   **Inputs**:\n        *   `json_file_path` (str)\n    *   **Outputs**:\n        *   `xgboost_input_metrics` (Output[Metrics])\n        *   `random_forest_input_metrics` (Output[Metrics])\n        *   `knn_input_metrics` (Output[Metrics])\n        *   `lr_input_metrics` (Output[Metrics])\n    *   **Base Image**: `python:3.10-slim`\n\n3.  **`run_xgboost_katib_experiment`**:\n    *   **Function**: Orchestrates a Katib experiment for XGBoost model training. It likely uses the input parameters to configure the Katib experiment (e.g., hyperparameter search space, objective metrics) and outputs the best parameters found by Katib. It uses `kubeflow-katib` and `kubernetes.client`.\n    *   **Inputs**:\n        *   `input_params_metrics` (Input[Metrics])\n    *   **Outputs**:\n        *   `best_params_metrics` (Output[Metrics])\n    *   **Base Image**: `python:3.10-slim`\n    *   **Packages**: `kubeflow-katib==0.17.0`\n\n4.  **`run_random_forest_katib_experiment`**:\n    *   **Function**: Orchestrates a Katib experiment for Random Forest model training, similar to the XGBoost component.\n    *   **Inputs**:\n        *   `input_params_metrics` (Input[Metrics])\n    *   **Outputs**:\n        *   `best_params_metrics` (Output[Metrics])\n    *   **Base Image**: `python:3.10-slim`\n    *   **Packages**: `kubeflow-katib==0.17.0`\n\n5.  **`run_knn_katib_experiment`**:\n    *   **Function**: Orchestrates a Katib experiment for K-Nearest Neighbors (KNN) model training, similar to the other Katib components.\n    *   **Inputs**:\n        *   `input_params_metrics` (Input[Metrics])\n    *   **Outputs**:\n        *   `best_params_metrics` (Output[Metrics])\n    *   **Base Image**: `python:3.10-slim`\n    *   **Packages**: `kubeflow-katib==0.17.0`\n\n6.  **`run_lr_katib_experiment`**:\n    *   **Function**: Orchestrates a Katib experiment for Logistic Regression (LR) model training, similar to the other Katib components.\n    *   **Inputs**:\n        *   `input_params_metrics` (Input[Metrics])\n    *   **Outputs**:\n        *   `best_params_metrics` (Output[Metrics])\n    *   **Base Image**: `python:3.10-slim`\n    *   **Packages**: `kubeflow-katib==0.17.0`\n\n7.  **`run_spark_job_with_best_params`**:\n    *   **Function**: This component is a custom container that executes a Spark job defined by a YAML manifest. It configures the Spark job to run on a Spark operator cluster within Kubernetes. It accepts the best parameters obtained from a Katib experiment and uses them to configure the Spark job. It uses the `kubernetes` and `kfp.kubernetes` libraries to manage Kubernetes resources. The component's `container` field defines a specific command execution using `sh -c` and references a custom image.\n    *   **Inputs**:\n        *   `best_params_metrics` (Input[Metrics])\n        *   `x_train` (Input[Dataset])\n        *   `x_test` (Input[Dataset])\n        *   `y_train` (Input[Dataset])\n        *   `y_test` (Input[Dataset])\n    *   **Outputs**: None explicitly defined in the provided snippet.\n    *   **Container Spec**:\n        *   Image: `spark:spark-py-operator`\n        *   Command: `sh -c`\n        *   Args: `python /app/main.py --x_train_path ${inputPath:x_train} --y_train_path ${inputPath:y_train} --x_test_path ${inputPath:x_test} --y_test_path ${inputPath:y_test}`. This command dynamically passes the input dataset paths to the Spark application.\n    *   **Kubernetes Configuration**: Sets a `timeout` for the Spark job, defines a `spark_driver_annotations` for `spark-operator.io/restart-policy` and `spark-operator.io/disable-monitoring-metrics`. It also defines `tolerations` for `dedicated` nodes.\n\n**Control Flow and Dependencies**:\n\n1.  The `load_file_from_nas_to_minio` component runs first, taking input paths and producing Dataset artifacts.\n2.  The `parse_input_json` component runs in parallel to `load_file_from_nas_to_minio`, taking a JSON file path and producing separate `Metrics` artifacts for each ML method's input parameters.\n3.  Four Katib experiment components (`run_xgboost_katib_experiment`, `run_random_forest_katib_experiment`, `run_knn_katib_experiment`, `run_lr_katib_experiment`) run in parallel. Each of these components `after` `parse_input_json` and takes the respective `input_params_metrics` output from `parse_input_json` as its input.\n4.  The `run_spark_job_with_best_params` component is designed to run `after` one of the Katib experiments (specifically, `run_xgboost_katib_experiment` is used in the provided `dsl.Condition` example, but it implies similar logic for others). It takes the `best_params_metrics` from a Katib experiment and the dataset outputs from `load_file_from_nas_to_minio` as inputs.\n5.  There is a `dsl.Condition` named `check-xgboost-params-if-exists` that checks if the `xgboost_input_metrics` output from `parse_input_json` exists and its value is not empty. If this condition is met, the `run_spark_job_with_best_params` component is executed, using the `best_params_metrics` from `run_xgboost_katib_experiment` and the dataset outputs.\n\n**Tools and Libraries Used**:\n*   `kfp` (Kubeflow Pipelines SDK)\n*   `pandas`\n*   `json`\n*   `yaml` (implicitly used by `get_spark_job_definition`)\n*   `kubeflow-katib`\n*   `kubernetes` (client for Kubernetes API interaction)\n*   Spark Operator (implied by `run_spark_job_with_best_params` and the `get_spark_job_definition` helper function).", "completion": "from kfp import dsl, compiler, kubernetes, components\nfrom kfp.dsl import Input, Output, Metrics, Dataset, Model, Artifact, component, ContainerSpec,  OutputPath, InputPath\n\nimport json\n\n\ndef get_spark_job_definition():\n    import yaml\n    import time\n    # Read manifest file\n    with open('spark-job-python-10kprocess.yaml', \"r\") as stream:\n        spark_job_manifest = yaml.safe_load(stream)\n\n    # Add epoch time in the job name\n    epoch = int(time.time())\n    spark_job_manifest[\"metadata\"][\"name\"] = spark_job_manifest[\"metadata\"][\"name\"].format(epoch=epoch)\n    return spark_job_manifest\n\n\n@component(\n    base_image='python:3.10-slim', \n    packages_to_install=['pandas']\n)\ndef load_file_from_nas_to_minio(\n    x_train_input_path: str, \n    x_test_input_path: str, \n    y_train_input_path: str, \n    y_test_input_path: str, \n    x_train_output: Output[Dataset], \n    x_test_output: Output[Dataset], \n    y_train_output: Output[Dataset], \n    y_test_output: Output[Dataset]\n):\n    import pandas as pd\n\n    df = pd.read_csv(x_train_input_path)\n    df.to_csv(x_train_output.path, index=False)\n\n    df = pd.read_csv(x_test_input_path)\n    df.to_csv(x_test_output.path, index=False)\n\n    df = pd.read_csv(y_train_input_path)\n    df.to_csv(y_train_output.path, index=False)\n\n    df = pd.read_csv(y_test_input_path)\n    df.to_csv(y_test_output.path, index=False)\n\n@component(base_image='python:3.10-slim')\ndef parse_input_json(\n    json_file_path: str, \n    xgboost_input_metrics: Output[Metrics], \n    random_forest_input_metrics: Output[Metrics], \n    knn_input_metrics: Output[Metrics],\n    lr_input_metrics: Output[Metrics]\n):\n    import json\n\n    def log_metric(metrics: Metrics, input_dict: dict):\n        for key in input_dict:\n            if key == \"method\":\n                continue\n            else:\n                metrics.log_metric(key, input_dict.get(key))\n\n    with open(file=json_file_path, mode='r', encoding='utf8') as file:\n        input_dict_arr: list[dict] = json.load(file)\n    \n    for input_dict in input_dict_arr:\n        if input_dict[\"method\"] == \"xgboost\":\n            log_metric(xgboost_input_metrics, input_dict)\n        elif input_dict[\"method\"] == \"random_forest\":\n            log_metric(random_forest_input_metrics, input_dict)\n        elif input_dict[\"method\"] == \"knn\":\n            log_metric(knn_input_metrics, input_dict)\n        elif input_dict[\"method\"] == \"lr\":\n            log_metric(lr_input_metrics, input_dict)\n        else:\n            continue\n\n#xgboost_katib\n@dsl.component(\n    base_image='python:3.10-slim', \n    packages_to_install=[\n        'kubeflow-katib==0.17.0'\n    ]\n)\ndef run_xgboost_katib_experiment(\n    input_params_metrics: Input[Metrics], \n    best_params_metrics: Output[Metrics]\n):\n    from kubeflow.katib import KatibClient\n    from kubernetes.client import V1ObjectMeta\n    from kubeflow.katib import V1beta1Experiment\n    from kubeflow.katib import V1beta1AlgorithmSpec\n    from kubeflow.katib import V1beta1ObjectiveSpec\n    from kubeflow.katib import V1beta1FeasibleSpace\n    from kubeflow.katib import V1beta1ExperimentSpec\n    from kubeflow.katib import V1beta1ObjectiveSpec\n    from kubeflow.katib import V1beta1ParameterSpec\n    from kubeflow.katib import V1beta1TrialTemplate\n    from kubeflow.katib import V1beta1TrialParameterSpec\n\n    from datetime import datetime, timezone, timedelta\n\n    dt_str = datetime.now(timezone(timedelta(hours=8))).strftime(\"%-Y-%m-%d-%H-%M-%S\")\n\n    experiment_name = \"xgboost-\" + dt_str.replace(\"_\", \"-\")\n    experiment_namespace = input_params_metrics.metadata.get(\"experiment_namespace\")\n\n    if experiment_name is None or experiment_namespace is None:\n        raise ValueError(\"Both experiment_name and experiment namespace needs to be a string!\")\n\n    metadata = V1ObjectMeta(\n        name=experiment_name, \n        namespace=experiment_namespace\n    )\n\n    algorithm_spec = V1beta1AlgorithmSpec(\n        algorithm_name=\"random\"\n    )\n\n    objective_spec = V1beta1ObjectiveSpec(\n        type=\"maximize\",\n        goal= 0.99,\n        objective_metric_name=\"accuracy\",\n    )\n\n    learning_rate_min = input_params_metrics.metadata.get(\"learning_rate_min\")\n    learning_rate_max = input_params_metrics.metadata.get(\"learning_rate_max\")\n    learning_rate_step = input_params_metrics.metadata.get(\"learning_rate_step\")\n\n    if learning_rate_min is None or learning_rate_max is None or learning_rate_step is None:\n        raise ValueError(\"All learning_rate_min, learning_rate_max and learning_rate_step cannot be null!\")\n\n    try:\n        learning_rate_min = float(learning_rate_min)\n        learning_rate_max = float(learning_rate_max)\n        learning_rate_step = float(learning_rate_step)\n    except ValueError:\n        raise ValueError(\"All learning_rate_min, learning_rate_max and learning_rate_step needs to be a float!\")\n\n    n_estimators_min = input_params_metrics.metadata.get(\"n_estimators_min\")\n    n_estimators_max = input_params_metrics.metadata.get(\"n_estimators_max\")\n    n_estimators_step = input_params_metrics.metadata.get(\"n_estimators_step\")\n\n    if n_estimators_min is None or n_estimators_max is None or n_estimators_step is None:\n        raise ValueError(\"All n_estimators_min, n_estimators_max and n_estimators_step cannot be null!\")\n\n    try:\n        n_estimators_min = int(n_estimators_min)\n        n_estimators_max = int(n_estimators_max)\n        n_estimators_step = int(n_estimators_step)\n    except ValueError:\n        raise ValueError(\"All n_estimators_min, n_estimators_max and n_estimators_step needs to be a float!\")\n\n    parameters = [\n        V1beta1ParameterSpec(\n            name=\"lr\",\n            parameter_type=\"double\",\n            feasible_space=V1beta1FeasibleSpace(\n                min=str(learning_rate_min),\n                max=str(learning_rate_max), \n                step=str(learning_rate_step)\n            ),\n        ), \n        V1beta1ParameterSpec(\n            name=\"ne\",\n            parameter_type=\"int\",\n            feasible_space=V1beta1FeasibleSpace(\n                min=str(n_estimators_min),\n                max=str(n_estimators_max), \n                step=str(n_estimators_step)\n            ),\n        )\n    ]\n\n    docker_image_name = input_params_metrics.metadata.get(\"docker_image_name\")\n    if docker_image_name is None:\n        raise ValueError(\"Docker image name cannot be null!\")\n\n    random_state = input_params_metrics.metadata.get(\"random_state\")\n    if random_state is None:\n        random_state = 42\n    else:\n        try:\n            random_state = int(random_state)\n        except ValueError:\n            raise ValueError(\"Random state needs to be an int!\")\n        \n    x_train_path = input_params_metrics.metadata.get(\"x_train_path\")\n    x_test_path = input_params_metrics.metadata.get(\"x_test_path\")\n    y_train_path = input_params_metrics.metadata.get(\"y_train_path\")\n    y_test_path = input_params_metrics.metadata.get(\"y_test_path\")\n\n    train_container = {\n        \"name\": \"training-container\",\n        \"image\": f\"docker.io/{docker_image_name}\",\n        \"command\": [\n            \"python3\",\n            \"/opt/xgboost/train.py\",\n            \"--lr=${trialParameters.learningRate}\",\n            \"--ne=${trialParameters.nEstimators}\",\n            f\"--rs={random_state}\",\n            f\"--esp=100000\",\n            f\"--booster=gbtree\",\n            f\"--x_train_path={x_train_path}\",\n            f\"--x_test_path={x_test_path}\",\n            f\"--y_train_path={y_train_path}\",\n            f\"--y_test_path={y_test_path}\",\n            f\"--save_model=false\",\n            f\"--model_folder_path=models\"\n        ]\n    }\n\n    template_spec = {\n        \"containers\": [\n            train_container\n        ],\n        \"restartPolicy\": \"Never\"\n    }\n\n    volumes = []\n    volumeMounts = []\n\n    datasets_from_pvc = input_params_metrics.metadata.get(\"datasets_from_pvc\")\n    datasets_pvc_name = input_params_metrics.metadata.get(\"datasets_pvc_name\")\n    datasets_pvc_mount_path = input_params_metrics.metadata.get(\"datasets_pvc_mount_path\")\n\n    datasets_from_pvc = bool(datasets_from_pvc)\n    \n    if datasets_from_pvc is True:\n        if datasets_pvc_name is None or datasets_pvc_mount_path is None:\n            raise ValueError(\"Both datasets_pvc_name and datasets_pvc_mount_path cannot be null\")\n\n        volumes.append({\n            \"name\": \"datasets\", \n            \"persistentVolumeClaim\": {\n                \"claimName\": datasets_pvc_name\n            }\n        })\n        volumeMounts.append({\n            \"name\": \"datasets\", \n            \"mountPath\": datasets_pvc_mount_path\n        })\n\n    '''\n    if save_model is True:\n        volumes.append({\n            \"name\": \"models\", \n            \"persistentVolumeClaim\": {\n                \"claimName\": models_pvc_name\n            }\n        })\n        volumeMounts.append({\n            \"name\": \"models\", \n            \"mountPath\": \"/opt/xgboost/models\"\n        })\n\n    if datasets_from_pvc is True or save_model is True:\n        train_container[\"volumeMounts\"] = volumeMounts\n        template_spec[\"volumes\"] = volumes\n    '''\n    if datasets_from_pvc is True:\n        train_container[\"volumeMounts\"] = volumeMounts\n        template_spec[\"volumes\"] = volumes\n\n    trial_spec={\n        \"apiVersion\": \"batch/v1\",\n        \"kind\": \"Job\",\n        \"spec\": {\n            \"template\": {\n                \"metadata\": {\n                    \"annotations\": {\n                        \"sidecar.istio.io/inject\": \"false\"\n                    }\n                },\n                \"spec\": template_spec\n            }\n        }\n    }\n\n    trial_template=V1beta1TrialTemplate(\n        primary_container_name=\"training-container\",\n        trial_parameters=[\n            V1beta1TrialParameterSpec(\n                name=\"learningRate\",\n                description=\"Learning rate for the training model\",\n                reference=\"lr\"\n            ), \n            V1beta1TrialParameterSpec(\n                name=\"nEstimators\",\n                description=\"N estimators for the training model\",\n                reference=\"ne\"\n            )\n        ],\n        trial_spec=trial_spec,\n        retain=True\n    )\n\n    max_trial_counts = input_params_metrics.metadata.get(\"max_trial_counts\")\n    max_failed_trial_counts = input_params_metrics.metadata.get(\"max_failed_trial_counts\")\n    parallel_trial_counts = input_params_metrics.metadata.get(\"parallel_trial_counts\")\n\n    if max_failed_trial_counts is None or max_failed_trial_counts is None or parallel_trial_counts is None:\n        raise ValueError(\"All max_trial_counts, max_failed_trial_counts and parallel_trial_counts cannot be null!\")\n    \n    try:\n        max_trial_counts = int(max_trial_counts)\n        max_failed_trial_counts = int(max_failed_trial_counts)\n        parallel_trial_counts = int(parallel_trial_counts)\n    except ValueError:\n        raise ValueError(\"All max_trial_counts, max_failed_trial_counts and needs to be an int!\")\n\n    experiment = V1beta1Experiment(\n        api_version=\"kubeflow.org/v1beta1\",\n        kind=\"Experiment\",\n        metadata=metadata,\n        spec=V1beta1ExperimentSpec(\n            max_trial_count=max_trial_counts,\n            parallel_trial_count=parallel_trial_counts,\n            max_failed_trial_count=max_failed_trial_counts,\n            algorithm=algorithm_spec,\n            objective=objective_spec,\n            parameters=parameters,\n            trial_template=trial_template,\n        )\n    )\n\n    client_namespace = input_params_metrics.metadata.get(\"client_namespace\")\n    if client_namespace is None:\n        raise ValueError(\"Client namespace cannot be null!\")\n\n    client = KatibClient(namespace=client_namespace)\n    client.create_experiment(experiment=experiment)\n    client.wait_for_experiment_condition(name=experiment_name, namespace=experiment_namespace, timeout=3600)\n\n    result = client.get_optimal_hyperparameters(name=experiment_name, namespace=experiment_namespace).to_dict()\n\n    best_params_list = result[\"parameter_assignments\"]\n\n    for params in best_params_list:\n        name = params[\"name\"]\n        value = params[\"value\"]\n\n        if name == \"lr\":\n            value = float(value)\n        elif name == \"ne\":\n            value = int(value)\n            \n        best_params_metrics.log_metric(metric=name, value=value)\n\n'''\n==================================\nThis is for seperating the code.\nDon't remove it.\nThx!\n\nRandom forest katib below.\n==================================\n'''\n\n#random_forest_katib\n@dsl.component(\n    base_image='python:3.10-slim', \n    packages_to_install=[\n        'kubeflow-katib==0.17.0'\n    ]\n)\ndef run_random_forest_katib_experiment(\n    input_params_metrics: Input[Metrics], \n    best_params_metrics: Output[Metrics]\n):\n    from kubeflow.katib import KatibClient\n    from kubernetes.client import V1ObjectMeta\n    from kubeflow.katib import V1beta1Experiment\n    from kubeflow.katib import V1beta1AlgorithmSpec\n    from kubeflow.katib import V1beta1ObjectiveSpec\n    from kubeflow.katib import V1beta1FeasibleSpace\n    from kubeflow.katib import V1beta1ExperimentSpec\n    from kubeflow.katib import V1beta1ObjectiveSpec\n    from kubeflow.katib import V1beta1ParameterSpec\n    from kubeflow.katib import V1beta1TrialTemplate\n    from kubeflow.katib import V1beta1TrialParameterSpec\n\n    from datetime import datetime, timezone, timedelta\n\n    dt_str = datetime.now(timezone(timedelta(hours=8))).strftime(\"%-Y-%m-%d-%H-%M-%S\")\n\n    experiment_name = \"random-forest-\" + dt_str.replace(\"_\", \"-\")\n    experiment_namespace = input_params_metrics.metadata.get(\"experiment_namespace\")\n\n    if experiment_name is None or experiment_namespace is None:\n        raise ValueError(\"Both experiment_name and experiment namespace needs to be a string!\")\n\n    metadata = V1ObjectMeta(\n        name=experiment_name, \n        namespace=experiment_namespace\n    )\n\n    algorithm_spec = V1beta1AlgorithmSpec(\n        algorithm_name=\"random\"\n    )\n\n    objective_spec = V1beta1ObjectiveSpec(\n        type=\"maximize\",\n        goal= 0.99,\n        objective_metric_name=\"accuracy\",\n    )\n\n    n_estimators_min = input_params_metrics.metadata.get(\"n_estimators_min\")\n    n_estimators_max = input_params_metrics.metadata.get(\"n_estimators_max\")\n    n_estimators_step = input_params_metrics.metadata.get(\"n_estimators_step\")\n\n    if n_estimators_min is None or n_estimators_max is None or n_estimators_step is None:\n        raise ValueError(\"All n_estimators_min, n_estimators_max and n_estimators_step cannot be null!\")\n\n    try:\n        n_estimators_min = int(n_estimators_min)\n        n_estimators_max = int(n_estimators_max)\n        n_estimators_step = int(n_estimators_step)\n    except ValueError:\n        raise ValueError(\"All n_estimators_min, n_estimators_max and n_estimators_step needs to be a float!\")\n\n    parameters = [\n        V1beta1ParameterSpec(\n            name=\"ne\",\n            parameter_type=\"int\",\n            feasible_space=V1beta1FeasibleSpace(\n                min=str(n_estimators_min),\n                max=str(n_estimators_max), \n                step=str(n_estimators_step)\n            ),\n        )\n    ]\n\n    docker_image_name = input_params_metrics.metadata.get(\"docker_image_name\")\n    if docker_image_name is None:\n        raise ValueError(\"Docker image name cannot be null!\")\n\n    random_state = input_params_metrics.metadata.get(\"random_state\")\n    if random_state is None:\n        random_state = 42\n    else:\n        try:\n            random_state = int(random_state)\n        except ValueError:\n            raise ValueError(\"Random state needs to be an int!\")\n        \n    x_train_path = input_params_metrics.metadata.get(\"x_train_path\")\n    x_test_path = input_params_metrics.metadata.get(\"x_test_path\")\n    y_train_path = input_params_metrics.metadata.get(\"y_train_path\")\n    y_test_path = input_params_metrics.metadata.get(\"y_test_path\")\n\n    train_container = {\n        \"name\": \"training-container\",\n        \"image\": f\"docker.io/{docker_image_name}\",\n        \"command\": [\n            \"python3\",\n            \"/opt/rfc/train.py\",\n            \"--ne=${trialParameters.nEstimators}\",\n            f\"--rs={random_state}\",\n            f\"--x_train_path={x_train_path}\",\n            f\"--x_test_path={x_test_path}\",\n            f\"--y_train_path={y_train_path}\",\n            f\"--y_test_path={y_test_path}\",\n            f\"--save_model=false\",\n            f\"--model_folder_path=models\"\n        ]\n    }\n\n    template_spec = {\n        \"containers\": [\n            train_container\n        ],\n        \"restartPolicy\": \"Never\"\n    }\n\n    volumes = []\n    volumeMounts = []\n\n    datasets_from_pvc = input_params_metrics.metadata.get(\"datasets_from_pvc\")\n    datasets_pvc_name = input_params_metrics.metadata.get(\"datasets_pvc_name\")\n    datasets_pvc_mount_path = input_params_metrics.metadata.get(\"datasets_pvc_mount_path\")\n\n    datasets_from_pvc = bool(datasets_from_pvc)\n    \n    if datasets_from_pvc is True:\n        if datasets_pvc_name is None or datasets_pvc_mount_path is None:\n            raise ValueError(\"Both datasets_pvc_name and datasets_pvc_mount_path cannot be null\")\n\n        volumes.append({\n            \"name\": \"datasets\", \n            \"persistentVolumeClaim\": {\n                \"claimName\": datasets_pvc_name\n            }\n        })\n        volumeMounts.append({\n            \"name\": \"datasets\", \n            \"mountPath\": datasets_pvc_mount_path\n        })\n\n    '''\n    if save_model is True:\n        volumes.append({\n            \"name\": \"models\", \n            \"persistentVolumeClaim\": {\n                \"claimName\": models_pvc_name\n            }\n        })\n        volumeMounts.append({\n            \"name\": \"models\", \n            \"mountPath\": \"/opt/rfc/models\"\n        })\n\n    if datasets_from_pvc is True or save_model is True:\n        train_container[\"volumeMounts\"] = volumeMounts\n        template_spec[\"volumes\"] = volumes\n    '''\n    if datasets_from_pvc is True:\n        train_container[\"volumeMounts\"] = volumeMounts\n        template_spec[\"volumes\"] = volumes\n\n    trial_spec={\n        \"apiVersion\": \"batch/v1\",\n        \"kind\": \"Job\",\n        \"spec\": {\n            \"template\": {\n                \"metadata\": {\n                    \"annotations\": {\n                        \"sidecar.istio.io/inject\": \"false\"\n                    }\n                },\n                \"spec\": template_spec\n            }\n        }\n    }\n\n    trial_template=V1beta1TrialTemplate(\n        primary_container_name=\"training-container\",\n        trial_parameters=[\n            V1beta1TrialParameterSpec(\n                name=\"nEstimators\",\n                description=\"N estimators for the training model\",\n                reference=\"ne\"\n            )\n        ],\n        trial_spec=trial_spec,\n        retain=True\n    )\n\n    max_trial_counts = input_params_metrics.metadata.get(\"max_trial_counts\")\n    max_failed_trial_counts = input_params_metrics.metadata.get(\"max_failed_trial_counts\")\n    parallel_trial_counts = input_params_metrics.metadata.get(\"parallel_trial_counts\")\n\n    if max_failed_trial_counts is None or max_failed_trial_counts is None or parallel_trial_counts is None:\n        raise ValueError(\"All max_trial_counts, max_failed_trial_counts and parallel_trial_counts cannot be null!\")\n    \n    try:\n        max_trial_counts = int(max_trial_counts)\n        max_failed_trial_counts = int(max_failed_trial_counts)\n        parallel_trial_counts = int(parallel_trial_counts)\n    except ValueError:\n        raise ValueError(\"All max_trial_counts, max_failed_trial_counts and needs to be an int!\")\n\n    experiment = V1beta1Experiment(\n        api_version=\"kubeflow.org/v1beta1\",\n        kind=\"Experiment\",\n        metadata=metadata,\n        spec=V1beta1ExperimentSpec(\n            max_trial_count=max_trial_counts,\n            parallel_trial_count=parallel_trial_counts,\n            max_failed_trial_count=max_failed_trial_counts,\n            algorithm=algorithm_spec,\n            objective=objective_spec,\n            parameters=parameters,\n            trial_template=trial_template,\n        )\n    )\n\n    client_namespace = input_params_metrics.metadata.get(\"client_namespace\")\n    if client_namespace is None:\n        raise ValueError(\"Client namespace cannot be null!\")\n\n    client = KatibClient(namespace=client_namespace)\n    client.create_experiment(experiment=experiment)\n    client.wait_for_experiment_condition(name=experiment_name, namespace=experiment_namespace, timeout=3600)\n\n    result = client.get_optimal_hyperparameters(name=experiment_name, namespace=experiment_namespace).to_dict()\n\n    best_params_list = result[\"parameter_assignments\"]\n\n    for params in best_params_list:\n        name = params[\"name\"]\n        value = params[\"value\"]\n\n        if name == \"ne\":\n            value = int(value)\n            \n        best_params_metrics.log_metric(metric=name, value=value)\n\n'''\n==================================\nThis is for seperating the code.\nDon't remove it.\nThx!\n\nKNN katib below.\n==================================\n'''\n\n#knn_katib\n@dsl.component(\n    base_image='python:3.10-slim', \n    packages_to_install=[\n        'kubeflow-katib==0.17.0'\n    ]\n)\ndef run_knn_katib_experiment(\n    input_params_metrics: Input[Metrics], \n    best_params_metrics: Output[Metrics]\n):\n    from kubeflow.katib import KatibClient\n    from kubernetes.client import V1ObjectMeta\n    from kubeflow.katib import V1beta1Experiment\n    from kubeflow.katib import V1beta1AlgorithmSpec\n    from kubeflow.katib import V1beta1ObjectiveSpec\n    from kubeflow.katib import V1beta1FeasibleSpace\n    from kubeflow.katib import V1beta1ExperimentSpec\n    from kubeflow.katib import V1beta1ObjectiveSpec\n    from kubeflow.katib import V1beta1ParameterSpec\n    from kubeflow.katib import V1beta1TrialTemplate\n    from kubeflow.katib import V1beta1TrialParameterSpec\n\n    from datetime import datetime, timezone, timedelta\n\n    dt_str = datetime.now(timezone(timedelta(hours=8))).strftime(\"%-Y-%m-%d-%H-%M-%S\")\n\n    experiment_name = \"knn-\" + dt_str.replace(\"_\", \"-\")\n    experiment_namespace = input_params_metrics.metadata.get(\"experiment_namespace\")\n\n    if experiment_name is None or experiment_namespace is None:\n        raise ValueError(\"Both experiment_name and experiment namespace needs to be a string!\")\n\n    metadata = V1ObjectMeta(\n        name=experiment_name, \n        namespace=experiment_namespace\n    )\n\n    algorithm_spec = V1beta1AlgorithmSpec(\n        algorithm_name=\"random\"\n    )\n\n    objective_spec = V1beta1ObjectiveSpec(\n        type=\"maximize\",\n        goal= 0.99,\n        objective_metric_name=\"accuracy\",\n    )\n\n    n_neighbors_min = input_params_metrics.metadata.get(\"n_neighbors_min\")\n    n_neighbors_max = input_params_metrics.metadata.get(\"n_neighbors_max\")\n    n_neighbors_step = input_params_metrics.metadata.get(\"n_neighbors_step\")\n\n    if n_neighbors_min is None or n_neighbors_max is None or n_neighbors_step is None:\n        raise ValueError(\"All n_neighbors_min, n_neighbors_max and n_neighbors_step cannot be null!\")\n\n    try:\n        n_neighbors_min = int(n_neighbors_min)\n        n_neighbors_max = int(n_neighbors_max)\n        n_neighbors_step = int(n_neighbors_step)\n    except ValueError:\n        raise ValueError(\"All n_neighbors_min, n_neighbors_max and n_neighbors_step needs to be a int!\")\n    \n    if n_neighbors_min % 2 != 1 or n_neighbors_max % 2 != 1 or n_neighbors_step % 2 != 0:\n        raise ValueError(\"N neighbors needs to be an odd number!\")\n\n    parameters = [\n        V1beta1ParameterSpec(\n            name=\"nn\",\n            parameter_type=\"int\",\n            feasible_space=V1beta1FeasibleSpace(\n                min=str(n_neighbors_min),\n                max=str(n_neighbors_max), \n                step=str(n_neighbors_step)\n            )\n        )\n    ]\n\n    docker_image_name = input_params_metrics.metadata.get(\"docker_image_name\")\n    if docker_image_name is None:\n        raise ValueError(\"Docker image name cannot be null!\")\n\n    random_state = input_params_metrics.metadata.get(\"random_state\")\n    if random_state is None:\n        random_state = 42\n    else:\n        try:\n            random_state = int(random_state)\n        except ValueError:\n            raise ValueError(\"Random state needs to be an int!\")\n        \n    x_train_path = input_params_metrics.metadata.get(\"x_train_path\")\n    x_test_path = input_params_metrics.metadata.get(\"x_test_path\")\n    y_train_path = input_params_metrics.metadata.get(\"y_train_path\")\n    y_test_path = input_params_metrics.metadata.get(\"y_test_path\")\n\n    train_container = {\n        \"name\": \"training-container\",\n        \"image\": f\"docker.io/{docker_image_name}\",\n        \"command\": [\n            \"python3\",\n            \"/opt/knn/train.py\",\n            \"--nn=${trialParameters.nNeighbors}\",\n            f\"--rs={random_state}\",\n            f\"--x_train_path={x_train_path}\",\n            f\"--x_test_path={x_test_path}\",\n            f\"--y_train_path={y_train_path}\",\n            f\"--y_test_path={y_test_path}\",\n            f\"--save_model=false\",\n            f\"--model_folder_path=models\"\n        ]\n    }\n\n    template_spec = {\n        \"containers\": [\n            train_container\n        ],\n        \"restartPolicy\": \"Never\"\n    }\n\n    volumes = []\n    volumeMounts = []\n\n    datasets_from_pvc = input_params_metrics.metadata.get(\"datasets_from_pvc\")\n    datasets_pvc_name = input_params_metrics.metadata.get(\"datasets_pvc_name\")\n    datasets_pvc_mount_path = input_params_metrics.metadata.get(\"datasets_pvc_mount_path\")\n\n    datasets_from_pvc = bool(datasets_from_pvc)\n    \n    if datasets_from_pvc is True:\n        if datasets_pvc_name is None or datasets_pvc_mount_path is None:\n            raise ValueError(\"Both datasets_pvc_name and datasets_pvc_mount_path cannot be null\")\n\n        volumes.append({\n            \"name\": \"datasets\", \n            \"persistentVolumeClaim\": {\n                \"claimName\": datasets_pvc_name\n            }\n        })\n        volumeMounts.append({\n            \"name\": \"datasets\", \n            \"mountPath\": datasets_pvc_mount_path\n        })\n\n    '''\n    if save_model is True:\n        volumes.append({\n            \"name\": \"models\", \n            \"persistentVolumeClaim\": {\n                \"claimName\": models_pvc_name\n            }\n        })\n        volumeMounts.append({\n            \"name\": \"models\", \n            \"mountPath\": \"/opt/rfc/models\"\n        })\n\n    if datasets_from_pvc is True or save_model is True:\n        train_container[\"volumeMounts\"] = volumeMounts\n        template_spec[\"volumes\"] = volumes\n    '''\n    if datasets_from_pvc is True:\n        train_container[\"volumeMounts\"] = volumeMounts\n        template_spec[\"volumes\"] = volumes\n\n    trial_spec={\n        \"apiVersion\": \"batch/v1\",\n        \"kind\": \"Job\",\n        \"spec\": {\n            \"template\": {\n                \"metadata\": {\n                    \"annotations\": {\n                        \"sidecar.istio.io/inject\": \"false\"\n                    }\n                },\n                \"spec\": template_spec\n            }\n        }\n    }\n\n    trial_template=V1beta1TrialTemplate(\n        primary_container_name=\"training-container\",\n        trial_parameters=[\n            V1beta1TrialParameterSpec(\n                name=\"nNeighbors\",\n                description=\"N neighbors for the training model\",\n                reference=\"nn\"\n            )\n        ],\n        trial_spec=trial_spec,\n        retain=True\n    )\n\n    max_trial_counts = input_params_metrics.metadata.get(\"max_trial_counts\")\n    max_failed_trial_counts = input_params_metrics.metadata.get(\"max_failed_trial_counts\")\n    parallel_trial_counts = input_params_metrics.metadata.get(\"parallel_trial_counts\")\n\n    if max_failed_trial_counts is None or max_failed_trial_counts is None or parallel_trial_counts is None:\n        raise ValueError(\"All max_trial_counts, max_failed_trial_counts and parallel_trial_counts cannot be null!\")\n    \n    try:\n        max_trial_counts = int(max_trial_counts)\n        max_failed_trial_counts = int(max_failed_trial_counts)\n        parallel_trial_counts = int(parallel_trial_counts)\n    except ValueError:\n        raise ValueError(\"All max_trial_counts, max_failed_trial_counts and needs to be an int!\")\n\n    experiment = V1beta1Experiment(\n        api_version=\"kubeflow.org/v1beta1\",\n        kind=\"Experiment\",\n        metadata=metadata,\n        spec=V1beta1ExperimentSpec(\n            max_trial_count=max_trial_counts,\n            parallel_trial_count=parallel_trial_counts,\n            max_failed_trial_count=max_failed_trial_counts,\n            algorithm=algorithm_spec,\n            objective=objective_spec,\n            parameters=parameters,\n            trial_template=trial_template,\n        )\n    )\n\n    client_namespace = input_params_metrics.metadata.get(\"client_namespace\")\n    if client_namespace is None:\n        raise ValueError(\"Client namespace cannot be null!\")\n\n    client = KatibClient(namespace=client_namespace)\n    client.create_experiment(experiment=experiment)\n    client.wait_for_experiment_condition(name=experiment_name, namespace=experiment_namespace, timeout=3600)\n\n    result = client.get_optimal_hyperparameters(name=experiment_name, namespace=experiment_namespace).to_dict()\n\n    best_params_list = result[\"parameter_assignments\"]\n\n    for params in best_params_list:\n        name = params[\"name\"]\n        value = params[\"value\"]\n\n        if name == \"nn\":\n            value = int(value)\n            \n        best_params_metrics.log_metric(metric=name, value=value)\n\n'''\n==================================\nThis is for seperating the code.\nDon't remove it.\nThx!\n\nLogistic Regression katib below.\n==================================\n'''\n\n#Logistic Regression_katib\n@dsl.component(\n    base_image='python:3.10-slim', \n    packages_to_install=[\n        'kubeflow-katib==0.17.0'\n    ]\n)\ndef run_lr_katib_experiment(\n    input_params_metrics: Input[Metrics], \n    best_params_metrics: Output[Metrics]\n):\n    from kubeflow.katib import KatibClient\n    from kubernetes.client import V1ObjectMeta\n    from kubeflow.katib import V1beta1Experiment\n    from kubeflow.katib import V1beta1AlgorithmSpec\n    from kubeflow.katib import V1beta1ObjectiveSpec\n    from kubeflow.katib import V1beta1FeasibleSpace\n    from kubeflow.katib import V1beta1ExperimentSpec\n    from kubeflow.katib import V1beta1ObjectiveSpec\n    from kubeflow.katib import V1beta1ParameterSpec\n    from kubeflow.katib import V1beta1TrialTemplate\n    from kubeflow.katib import V1beta1TrialParameterSpec\n\n    from datetime import datetime, timezone, timedelta\n\n    dt_str = datetime.now(timezone(timedelta(hours=8))).strftime(\"%-Y-%m-%d-%H-%M-%S\")\n\n    experiment_name = \"lr-\" + dt_str.replace(\"_\", \"-\")\n    experiment_namespace = input_params_metrics.metadata.get(\"experiment_namespace\")\n\n    if experiment_name is None or experiment_namespace is None:\n        raise ValueError(\"Both experiment_name and experiment namespace needs to be a string!\")\n\n    metadata = V1ObjectMeta(\n        name=experiment_name, \n        namespace=experiment_namespace\n    )\n\n    algorithm_spec = V1beta1AlgorithmSpec(\n        algorithm_name=\"random\"\n    )\n\n    objective_spec = V1beta1ObjectiveSpec(\n        type=\"maximize\",\n        goal= 0.99,\n        objective_metric_name=\"accuracy\",\n    )\n\n    iterators_min = input_params_metrics.metadata.get(\"iterators_min\")\n    iterators_max = input_params_metrics.metadata.get(\"iterators_max\")\n    iterators_step = input_params_metrics.metadata.get(\"iterators_step\")\n\n    if iterators_min is None or iterators_max is None or iterators_step is None:\n        raise ValueError(\"All iterators_min, iterators_max and iterators_step cannot be null!\")\n\n    try:\n        iterators_min = int(iterators_min)\n        iterators_max = int(iterators_max)\n        iterators_step = int(iterators_step)\n    except ValueError:\n        raise ValueError(\"All iterators_min, iterators_max and iterators_step needs to be a int!\")\n\n    parameters = [\n        V1beta1ParameterSpec(\n            name=\"it\",\n            parameter_type=\"int\",\n            feasible_space=V1beta1FeasibleSpace(\n                min=str(iterators_min),\n                max=str(iterators_max), \n                step=str(iterators_step)\n            )\n        )\n    ]\n\n    docker_image_name = input_params_metrics.metadata.get(\"docker_image_name\")\n    if docker_image_name is None:\n        raise ValueError(\"Docker image name cannot be null!\")\n\n    random_state = input_params_metrics.metadata.get(\"random_state\")\n    if random_state is None:\n        random_state = 42\n    else:\n        try:\n            random_state = int(random_state)\n        except ValueError:\n            raise ValueError(\"Random state needs to be an int!\")\n        \n    x_train_path = input_params_metrics.metadata.get(\"x_train_path\")\n    x_test_path = input_params_metrics.metadata.get(\"x_test_path\")\n    y_train_path = input_params_metrics.metadata.get(\"y_train_path\")\n    y_test_path = input_params_metrics.metadata.get(\"y_test_path\")\n\n    train_container = {\n        \"name\": \"training-container\",\n        \"image\": f\"docker.io/{docker_image_name}\",\n        \"command\": [\n            \"python3\",\n            \"/opt/lr/train.py\",\n            \"--it=${trialParameters.iterators}\",\n            f\"--rs={random_state}\",\n            f\"--x_train_path={x_train_path}\",\n            f\"--x_test_path={x_test_path}\",\n            f\"--y_train_path={y_train_path}\",\n            f\"--y_test_path={y_test_path}\",\n            f\"--save_model=false\",\n            f\"--model_folder_path=models\"\n        ]\n    }\n\n    template_spec = {\n        \"containers\": [\n            train_container\n        ],\n        \"restartPolicy\": \"Never\"\n    }\n\n    volumes = []\n    volumeMounts = []\n\n    datasets_from_pvc = input_params_metrics.metadata.get(\"datasets_from_pvc\")\n    datasets_pvc_name = input_params_metrics.metadata.get(\"datasets_pvc_name\")\n    datasets_pvc_mount_path = input_params_metrics.metadata.get(\"datasets_pvc_mount_path\")\n\n    datasets_from_pvc = bool(datasets_from_pvc)\n    \n    if datasets_from_pvc is True:\n        if datasets_pvc_name is None or datasets_pvc_mount_path is None:\n            raise ValueError(\"Both datasets_pvc_name and datasets_pvc_mount_path cannot be null\")\n\n        volumes.append({\n            \"name\": \"datasets\", \n            \"persistentVolumeClaim\": {\n                \"claimName\": datasets_pvc_name\n            }\n        })\n        volumeMounts.append({\n            \"name\": \"datasets\", \n            \"mountPath\": datasets_pvc_mount_path\n        })\n\n    '''\n    if save_model is True:\n        volumes.append({\n            \"name\": \"models\", \n            \"persistentVolumeClaim\": {\n                \"claimName\": models_pvc_name\n            }\n        })\n        volumeMounts.append({\n            \"name\": \"models\", \n            \"mountPath\": \"/opt/lr/models\"\n        })\n    '''\n\n    if datasets_from_pvc is True:\n        train_container[\"volumeMounts\"] = volumeMounts\n        template_spec[\"volumes\"] = volumes\n\n    trial_spec={\n        \"apiVersion\": \"batch/v1\",\n        \"kind\": \"Job\",\n        \"spec\": {\n            \"template\": {\n                \"metadata\": {\n                    \"annotations\": {\n                        \"sidecar.istio.io/inject\": \"false\"\n                    }\n                },\n                \"spec\": template_spec\n            }\n        }\n    }\n\n    trial_template=V1beta1TrialTemplate(\n        primary_container_name=\"training-container\",\n        trial_parameters=[\n            V1beta1TrialParameterSpec(\n                name=\"iterators\",\n                description=\"iterators for the training model\",\n                reference=\"it\"\n            )\n        ],\n        trial_spec=trial_spec,\n        retain=True\n    )\n\n    max_trial_counts = input_params_metrics.metadata.get(\"max_trial_counts\")\n    max_failed_trial_counts = input_params_metrics.metadata.get(\"max_failed_trial_counts\")\n    parallel_trial_counts = input_params_metrics.metadata.get(\"parallel_trial_counts\")\n\n    if max_failed_trial_counts is None or max_failed_trial_counts is None or parallel_trial_counts is None:\n        raise ValueError(\"All max_trial_counts, max_failed_trial_counts and parallel_trial_counts cannot be null!\")\n    \n    try:\n        max_trial_counts = int(max_trial_counts)\n        max_failed_trial_counts = int(max_failed_trial_counts)\n        parallel_trial_counts = int(parallel_trial_counts)\n    except ValueError:\n        raise ValueError(\"All max_trial_counts, max_failed_trial_counts and needs to be an int!\")\n\n    experiment = V1beta1Experiment(\n        api_version=\"kubeflow.org/v1beta1\",\n        kind=\"Experiment\",\n        metadata=metadata,\n        spec=V1beta1ExperimentSpec(\n            max_trial_count=max_trial_counts,\n            parallel_trial_count=parallel_trial_counts,\n            max_failed_trial_count=max_failed_trial_counts,\n            algorithm=algorithm_spec,\n            objective=objective_spec,\n            parameters=parameters,\n            trial_template=trial_template,\n        )\n    )\n\n    client_namespace = input_params_metrics.metadata.get(\"client_namespace\")\n    if client_namespace is None:\n        raise ValueError(\"Client namespace cannot be null!\")\n\n    client = KatibClient(namespace=client_namespace)\n    client.create_experiment(experiment=experiment)\n    client.wait_for_experiment_condition(name=experiment_name, namespace=experiment_namespace, timeout=3600)\n\n    result = client.get_optimal_hyperparameters(name=experiment_name, namespace=experiment_namespace).to_dict()\n\n    best_params_list = result[\"parameter_assignments\"]\n\n    for params in best_params_list:\n        name = params[\"name\"]\n        value = params[\"value\"]\n\n        if name == \"it\":\n            value = int(value)\n            \n        best_params_metrics.log_metric(metric=name, value=value)\n\n@dsl.component(\n    base_image='python:3.10-slim', \n    packages_to_install=['pandas', 'xgboost', 'scikit-learn', 'joblib']\n)\ndef run_xgboost_train(\n    best_params_metrics: Input[Metrics], \n    x_train: Input[Dataset], \n    x_test: Input[Dataset], \n    y_train: Input[Dataset], \n    y_test: Input[Dataset], \n    model: Output[Model], \n    file: Output[Artifact]\n):\n    import pandas as pd\n    import xgboost as xgb\n    from xgboost import XGBClassifier\n    import joblib\n    import json\n\n    from sklearn.metrics import accuracy_score\n\n    learning_rate = best_params_metrics.metadata.get(\"lr\")\n    n_estimators = best_params_metrics.metadata.get(\"ne\")\n\n    x_train_df = pd.read_csv(x_train.path)\n    y_train_df = pd.read_csv(y_train.path)\n    x_test_df = pd.read_csv(x_test.path)\n    y_test_df = pd.read_csv(y_test.path)\n\n    dtrain = xgb.DMatrix(x_train_df.values, label=y_train_df.values)\n    dtest = xgb.DMatrix(x_test_df.values, label=y_test_df.values)\n  \n    xgb_model = XGBClassifier(\n        n_estimators=n_estimators, \n        learning_rate= learning_rate\n    )\n    \n    xgb_model.fit(x_train_df, y_train_df.values.ravel())\n    \n    preds = xgb_model.predict(dtest)\n\n    predictions = [round(value) for value in preds]\n    xgb_accuracy = accuracy_score(y_test_df.values, predictions)\n    print('XGBoost Test accuracy:', xgb_accuracy)\n    \n    # Save the model\n    joblib.dump(xgb_model, model.path)\n\n     # Save the accuracy\n    data = {}\n    data['accuracy'] = xgb_accuracy\n    data['model_path'] = model.path\n\n    with open(file=file.path, mode='w', encoding='utf8') as file:\n        json.dump(data, file, indent=4)\n\n@dsl.component(\n    base_image='python:3.10-slim', \n    packages_to_install=['pandas', 'scikit-learn', 'joblib']\n)\ndef run_random_forest_train(\n    best_params_metrics: Input[Metrics], \n    x_train: Input[Dataset], \n    x_test: Input[Dataset], \n    y_train: Input[Dataset], \n    y_test: Input[Dataset], \n    model: Output[Model], \n    file: Output[Artifact]\n):\n    import pandas as pd\n    import joblib\n    import json\n\n    from sklearn.metrics import accuracy_score\n    from sklearn.ensemble import RandomForestClassifier\n\n    n_estimators = best_params_metrics.metadata.get(\"ne\")\n\n    x_train_df = pd.read_csv(x_train.path)\n    y_train_df = pd.read_csv(y_train.path)\n    x_test_df = pd.read_csv(x_test.path)\n    y_test_df = pd.read_csv(y_test.path)\n\n    rfc = RandomForestClassifier(n_estimators=n_estimators)\n    rfc.fit(x_train_df.values, y_train_df.values.ravel())\n\n    rfc.predict(x_test_df.values)\n    rfc_accuracy = rfc.score(x_test_df.values, y_test_df.values)\n\n    # Save the model\n    joblib.dump(rfc, model.path)\n\n    data = {}\n    data['accuracy'] = rfc_accuracy\n    data['model_path'] = model.path\n\n    with open(file=file.path, mode='w', encoding='utf8') as file:\n        json.dump(data, file, indent=4)\n\n@dsl.component(\n    base_image='python:3.10-slim', \n    packages_to_install=['pandas', 'scikit-learn', 'joblib']\n)\ndef run_knn_train(\n    best_params_metrics: Input[Metrics], \n    x_train: Input[Dataset], \n    x_test: Input[Dataset], \n    y_train: Input[Dataset], \n    y_test: Input[Dataset], \n    model: Output[Model], \n    file: Output[Artifact]\n):\n    import pandas as pd\n    import joblib\n    import json\n\n    from sklearn.metrics import accuracy_score\n    from sklearn.neighbors import KNeighborsClassifier\n\n    n_neighbors = best_params_metrics.metadata.get(\"nn\")\n\n    x_train_df = pd.read_csv(x_train.path)\n    y_train_df = pd.read_csv(y_train.path)\n    x_test_df = pd.read_csv(x_test.path)\n    y_test_df = pd.read_csv(y_test.path)\n\n    knn_model = KNeighborsClassifier(\n        n_neighbors=n_neighbors\n    )\n    knn_model.fit(x_train_df.values, y_train_df.values.ravel())\n\n    y_pred = knn_model.predict(x_test_df.values)\n    accuracy = accuracy_score(y_test_df.values, y_pred)\n\n    # Save the model\n    joblib.dump(knn_model, model.path)\n\n    data = {}\n    data['accuracy'] = accuracy\n    data['model_path'] = model.path\n\n    with open(file=file.path, mode='w', encoding='utf8') as file:\n        json.dump(data, file, indent=4)\n\n@dsl.component(\n    base_image='python:3.10-slim', \n    packages_to_install=['pandas', 'scikit-learn', 'joblib']\n)\ndef run_lr_train(\n    best_params_metrics: Input[Metrics], \n    x_train: Input[Dataset], \n    x_test: Input[Dataset], \n    y_train: Input[Dataset], \n    y_test: Input[Dataset], \n    model: Output[Model], \n    file: Output[Artifact]\n):\n    import pandas as pd\n    import joblib\n    import json\n\n    from sklearn.metrics import accuracy_score\n    from sklearn.linear_model import LogisticRegression\n\n    iterators = best_params_metrics.metadata.get(\"it\")\n\n    x_train_df = pd.read_csv(x_train.path)\n    y_train_df = pd.read_csv(y_train.path)\n    x_test_df = pd.read_csv(x_test.path)\n    y_test_df = pd.read_csv(y_test.path)\n\n    lr_model = LogisticRegression(\n        random_state=0, \n        max_iter=iterators\n    )\n    lr_model.fit(x_train_df.values, y_train_df.values.ravel())\n\n    y_pred = lr_model.predict(x_test_df.values)\n    accuracy = accuracy_score(y_test_df.values, y_pred)\n\n    # Save the model\n    joblib.dump(lr_model, model.path)\n\n    data = {}\n    data['accuracy'] = accuracy\n    data['model_path'] = model.path\n\n    with open(file=file.path, mode='w', encoding='utf8') as file:\n        json.dump(data, file, indent=4)\n\n@dsl.component(\n    base_image='python:3.10-slim',\n    packages_to_install=['joblib==1.4.2', 'scikit-learn==1.5.1', 'xgboost==2.0.3']# \n)\ndef choose_model(\n    LogisticRegression_model: Input[Model],\n    XGBoost_model: Input[Model],\n    RandomForest_model: Input[Model],\n    KNN_model: Input[Model],\n    lr_file: Input[Artifact],\n    xgb_file: Input[Artifact],\n    rf_file: Input[Artifact],\n    knn_file: Input[Artifact],\n    final_model: Output[Model],\n    result: Output[Artifact]\n) -> None:\n    import joblib\n    import json\n\n    # Define a dictionary to store model artifacts and their corresponding JSON files\n    models = {\n        'LogisticRegression': lr_file,\n        'XGBoost': xgb_file,\n        'RandomForest': rf_file,\n        'KNN': knn_file\n    }\n\n    accuracy = {}\n    model_paths = {}\n\n    # Read accuracies and model paths\n    for model_name, json_file in models.items():\n        with open(json_file.path, 'r') as f:\n            data = json.load(f)\n        accuracy[model_name] = data['accuracy']\n        model_paths[model_name] = data['model_path']\n\n    # Find the best model\n    best_model_name = max(accuracy, key=accuracy.get)\n    best_model = joblib.load(model_paths[best_model_name])\n    \n    # Save the best model\n    joblib.dump(best_model, final_model.path)\n\n    # Prepare result string\n    result_string = f'Best Model is {best_model_name} : {accuracy[best_model_name]}'\n    result_string += f'\\nAccuracy:\\n'\n    for model_name, acc in accuracy.items():\n        result_string += f'{model_name:17} : {acc}\\n'\n    print(result_string)\n\n    # Write the result to a file\n    data = {}\n    data['accuracy'] = accuracy[best_model_name]\n    data['model_path'] = final_model.path\n\n    with open(file=result.path, mode='w', encoding='utf8') as file:\n        json.dump(data, file, indent=4)\n\n@dsl.component(\n    base_image='python:3.10-slim',\n    packages_to_install=['joblib==1.4.2', 'scikit-learn==1.5.1', 'xgboost==2.0.3']\n)\ndef change_model(\n    old_model_path: str, \n    old_model_file_path: str, \n    new_model: Input[Model],\n    new_model_file: Input[Artifact], \n):\n    import joblib\n    import json\n    import os\n\n    with open(new_model_file.path, 'r') as f:\n        data_new = json.load(f)\n    new_model_accuracy = data_new['accuracy']\n\n    new_model_model = joblib.load(new_model.path)\n\n    if not (os.path.exists(old_model_path) and os.path.exists(old_model_file_path)):\n        joblib.dump(new_model_model, old_model_path)\n        with open(old_model_file_path, 'w', encoding='utf-8') as file:\n            json.dump(data_new, file, indent=4)\n        result_message = f\"New model saved to NAS. Accuracy: {new_model_accuracy}\"\n        return None\n    \n    try:\n        with open(old_model_file_path, 'r') as f:\n            data_old = json.load(f)\n        old_model_accuracy = data_old['accuracy']\n\n        if new_model_accuracy > old_model_accuracy:\n            joblib.dump(new_model_model, old_model_path)\n            with open(old_model_file_path, 'w', encoding='utf-8') as file:\n                json.dump(data_new, file, indent=4)\n            result_message = f\"Model updated. New accuracy: {new_model_accuracy}, Old accuracy: {old_model_accuracy}\"\n        else:\n            result_message = f\"Existing model retained. Existing accuracy: {old_model_accuracy}, New accuracy: {new_model_accuracy}\"\n    except (json.JSONDecodeError, UnicodeDecodeError):\n        joblib.dump(new_model_model, old_model_path)\n        with open(old_model_file_path, 'w', encoding='utf-8') as file:\n            json.dump(data_new, file, indent=4)\n        result_message = f\"New model saved to NAS. Accuracy: {new_model_accuracy}\"\n\n    print(result_message)\n\n@dsl.pipeline(\n    name=\"compose\", \n    description=\"Compose of kubeflow, katib and spark\"\n)\ndef compose_pipeline(\n    params_pvc_name: str = \"params-pvc\", \n    params_json_file_path: str = \"/mnt/params/params_heart_disease.json\", \n    models_pvc_name: str = \"models-pvc\"\n):\n    sparkapplication_dict = get_spark_job_definition()\n\n    k8s_apply_op = components.load_component_from_file(\"k8s-apply-component.yaml\")\n    apply_sparkapplication_task = k8s_apply_op(object=json.dumps(sparkapplication_dict))\n    apply_sparkapplication_task.set_caching_options(enable_caching=False)\n\n    check_sparkapplication_status_op = components.load_component_from_file(\"checkSparkapplication.yaml\")\n    check_sparkapplication_status_task = check_sparkapplication_status_op(\n        name=sparkapplication_dict[\"metadata\"][\"name\"],\n        namespace=sparkapplication_dict[\"metadata\"][\"namespace\"]\n    ).after(apply_sparkapplication_task)\n    check_sparkapplication_status_task.set_caching_options(enable_caching=False)\n\n    load_datasets_task = load_file_from_nas_to_minio(\n        x_train_input_path=\"/mnt/datasets/heart_disease/x_train.csv\", \n        x_test_input_path=\"/mnt/datasets/heart_disease/x_test.csv\", \n        y_train_input_path=\"/mnt/datasets/heart_disease/y_train.csv\", \n        y_test_input_path=\"/mnt/datasets/heart_disease/y_test.csv\", \n    ).after(check_sparkapplication_status_task)\n    load_datasets_task.set_caching_options(enable_caching=False)\n\n    kubernetes.mount_pvc(\n        task=load_datasets_task, \n        pvc_name=\"datasets-pvc\", \n        mount_path=\"/mnt/datasets\"\n    )\n\n    parse_input_json_task = parse_input_json(\n        json_file_path=params_json_file_path\n    ).after(load_datasets_task)\n    parse_input_json_task.set_caching_options(enable_caching=False)\n\n    kubernetes.mount_pvc(\n        task=parse_input_json_task, \n        pvc_name=params_pvc_name, \n        mount_path=\"/mnt/params\"\n    )\n\n    xgboost_katib_experiment_task = run_xgboost_katib_experiment(\n        input_params_metrics=parse_input_json_task.outputs[\"xgboost_input_metrics\"]\n    )\n\n    random_forest_katib_experiment_task = run_random_forest_katib_experiment(\n        input_params_metrics=parse_input_json_task.outputs[\"random_forest_input_metrics\"]\n    )\n\n    knn_katib_experiment_task = run_knn_katib_experiment(\n        input_params_metrics=parse_input_json_task.outputs[\"knn_input_metrics\"]\n    )\n\n    lr_katib_experiment_task = run_lr_katib_experiment(\n        input_params_metrics=parse_input_json_task.outputs[\"lr_input_metrics\"]\n    )\n\n    xgboost_train_task = run_xgboost_train(\n        best_params_metrics=xgboost_katib_experiment_task.outputs['best_params_metrics'], \n        x_train=load_datasets_task.outputs['x_train_output'], \n        x_test=load_datasets_task.outputs['x_test_output'], \n        y_train=load_datasets_task.outputs['y_train_output'], \n        y_test=load_datasets_task.outputs['y_test_output']\n    )\n\n    random_forest_train_task = run_random_forest_train(\n        best_params_metrics=random_forest_katib_experiment_task.outputs['best_params_metrics'], \n        x_train=load_datasets_task.outputs['x_train_output'], \n        x_test=load_datasets_task.outputs['x_test_output'], \n        y_train=load_datasets_task.outputs['y_train_output'], \n        y_test=load_datasets_task.outputs['y_test_output']\n    )\n\n    knn_train_task = run_knn_train(\n        best_params_metrics=knn_katib_experiment_task.outputs['best_params_metrics'], \n        x_train=load_datasets_task.outputs['x_train_output'], \n        x_test=load_datasets_task.outputs['x_test_output'], \n        y_train=load_datasets_task.outputs['y_train_output'], \n        y_test=load_datasets_task.outputs['y_test_output']\n    )\n\n    lr_train_task = run_lr_train(\n        best_params_metrics=lr_katib_experiment_task.outputs['best_params_metrics'], \n        x_train=load_datasets_task.outputs['x_train_output'], \n        x_test=load_datasets_task.outputs['x_test_output'], \n        y_train=load_datasets_task.outputs['y_train_output'], \n        y_test=load_datasets_task.outputs['y_test_output']\n    )\n\n    choose_model_task = choose_model(\n        LogisticRegression_model=lr_train_task.outputs['model'],\n        XGBoost_model=xgboost_train_task.outputs['model'],\n        RandomForest_model=random_forest_train_task.outputs['model'],\n        KNN_model=knn_train_task.outputs['model'],\n        lr_file=lr_train_task.outputs['file'],\n        xgb_file=xgboost_train_task.outputs['file'],\n        rf_file=random_forest_train_task.outputs['file'],\n        knn_file=knn_train_task.outputs['file']\n    )\n\n    change_model_task = change_model(\n        old_model_path=\"/mnt/models/heart_disease_model.pkl\", \n        old_model_file_path=\"/mnt/models/heart_disease_model.json\", \n        new_model=choose_model_task.outputs[\"final_model\"], \n        new_model_file=choose_model_task.outputs[\"result\"]\n    )\n\n    kubernetes.mount_pvc(\n        task=change_model_task, \n        pvc_name=models_pvc_name, \n        mount_path=\"/mnt/models\"\n    )\n\nif __name__ == \"__main__\":\n    compiler.Compiler().compile(compose_pipeline, \"../compose_pipeline.yaml\")"}
{"prompt": "Generate a Kubeflow Pipeline named `Retry random failures` that performs the following:\n\nThe pipeline consists of **two** components, both of which are instances of a reusable `random_failure_op` function. This function creates a `ContainerOp` that simulates random failures.\n\n**Component 1: `op1`**\n*   **Function:** `op1` is an instance of `random_failure_op`. It is configured to randomly exit with one of the codes `0, 1, 2, 3`.\n*   **Retry Mechanism:** This component has a retry policy applied, allowing it to **retry up to 10 times** if it fails.\n\n**Component 2: `op2`**\n*   **Function:** `op2` is also an instance of `random_failure_op`. It is configured to randomly exit with one of the codes `0, 1`.\n*   **Retry Mechanism:** This component has a retry policy applied, allowing it to **retry up to 5 times** if it fails.\n\n**Control Flow:**\n*   The two components, `op1` and `op2`, run **in parallel** as there are no explicit dependencies defined between them. Each component's execution is independent, with its own retry logic.\n\n**Libraries/Tools:**\n*   The pipeline uses the `kfp` (Kubeflow Pipelines) SDK, specifically `kfp.dsl` for defining the pipeline and components.\n*   The `random_failure_op` uses a `python:alpine3.6` Docker image and executes a Python script to simulate random exits using the `random` and `sys` modules.\n\n**Pipeline Description:**\nThe pipeline's description is \"The pipeline includes two steps which fail randomly. It shows how to use ContainerOp(...).set_retry(...)\".", "completion": "#!/usr/bin/env python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport kfp\nfrom kfp import dsl\n\n\ndef random_failure_op(exit_codes):\n    \"\"\"A component that fails randomly.\"\"\"\n    return dsl.ContainerOp(\n        name='random_failure',\n        image='python:alpine3.6',\n        command=['python', '-c'],\n        arguments=['import random; import sys; exit_code = int(random.choice(sys.argv[1].split(\",\"))); print(exit_code); sys.exit(exit_code)', exit_codes]\n    )\n\n\n@dsl.pipeline(\n    name='Retry random failures',\n    description='The pipeline includes two steps which fail randomly. It shows how to use ContainerOp(...).set_retry(...).'\n)\ndef retry_sample_pipeline():\n    op1 = random_failure_op('0,1,2,3').set_retry(10)\n    op2 = random_failure_op('0,1').set_retry(5)\n\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(retry_sample_pipeline, __file__ + '.yaml')"}
{"prompt": "Generate a Kubeflow Pipeline named `End2end Resnet50 Classification` that performs an end-to-end Kubeflow demonstration including TensorRT Inference Server, TF-AMP, and TensorRT.\n\nThe pipeline accepts the following parameters:\n- `num_iter` (default: 20)\n- `batch_size` (default: 1024)\n- `use_tf_amp` (default: 1)\n- `use_auto_loss_scaling` (default: 1)\n- `trtserver_name` (default: 'trtserver')\n- `model_name` (default: 'resnet_graphdef')\n- `model_version` (default: '1')\n- `webapp_prefix` (default: 'webapp')\n- `webapp_port` (default: '80')\n- `storage_bucket` (default: 'gs://dongm-kubeflow')\n- `ckpt_dir` (default: 'ckpt')\n- `mount_dir` (default: '/mnt/vol')\n- `model_dir` (default: 'model_repository')\n- `raw_data_dir` (default: 'raw_data')\n- `processed_data_dir` (default: 'processed_data')\n\nThe pipeline consists of at least one component:\n1.  **`preprocessing`**: This component is a `ContainerOp` that downloads raw data.\n    -   **Image**: `gcr.io/nvidia-sa-org/gcp-joc-end2end-demo-preprocessing`\n    -   **Command**: `python`\n    -   **Arguments**: `download.py --data_dir {mount_dir}/{raw_data_dir}`\n    -   **Resources**: Requires 1 GPU.\n    -   **Volumes**: Mounts a Persistent Volume Claim named `my-rw-pvc` to `/mnt/vol`.", "completion": "# Copyright 2018 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport kfp.dsl as dsl\nimport kfp.gcp as gcp\nfrom kubernetes import client as k8s_client\n\n\n@dsl.pipeline(\n  name='End2end Resnet50 Classification',\n  description='END to END kubeflow demo with TensorRT Inference Server, TF-AMP and TensorRT'\n)\ndef end2end_demo(  # pylint: disable=unused-argument\n    num_iter: dsl.PipelineParam = dsl.PipelineParam(name='num_iter', value=20),\n    batch_size: dsl.PipelineParam = dsl.PipelineParam(name='batch_size', value=1024),\n    use_tf_amp: dsl.PipelineParam = dsl.PipelineParam(name='use_tf_amp', value=1),\n    use_auto_loss_scaling: dsl.PipelineParam = dsl.PipelineParam(name='use_auto_loss_scaling', value=1),\n    trtserver_name: dsl.PipelineParam = dsl.PipelineParam(name='trtserver_name', value='trtserver'),\n    model_name: dsl.PipelineParam = dsl.PipelineParam(name='model_name', value='resnet_graphdef'),\n    model_version: dsl.PipelineParam = dsl.PipelineParam(name='model_version', value='1'),\n    webapp_prefix: dsl.PipelineParam = dsl.PipelineParam(name='webapp_prefix', value='webapp'),\n    webapp_port: dsl.PipelineParam = dsl.PipelineParam(name='webapp_port', value='80'),\n    storage_bucket: dsl.PipelineParam = dsl.PipelineParam(name='storage_bucket', value='gs://dongm-kubeflow'),\n    ckpt_dir: dsl.PipelineParam = dsl.PipelineParam(name='ckpt_dir', value='ckpt'),\n    mount_dir: dsl.PipelineParam = dsl.PipelineParam(name='mount_dir', value='/mnt/vol'),\n    model_dir: dsl.PipelineParam = dsl.PipelineParam(name='model_dir', value='model_repository'),\n    raw_data_dir: dsl.PipelineParam = dsl.PipelineParam(name='raw_data_dir', value='raw_data'),\n    processed_data_dir: dsl.PipelineParam = dsl.PipelineParam(name='processed_data_dir', value='processed_data')\n):\n  project_name = 'nvidia-sa-org'\n\n  preprocessing = dsl.ContainerOp(\n    name='preprocessing',\n    image='gcr.io/' + project_name + '/gcp-joc-end2end-demo-preprocessing',\n    command=['python'],\n    arguments=[\n      'download.py',\n      '--data_dir', '%s/%s' % (mount_dir, raw_data_dir)\n    ],\n    file_outputs={}\n  ).set_gpu_limit(1).add_volume(k8s_client.V1Volume(name='my-rw-pv',\n                                                    persistent_volume_claim=k8s_client.V1PersistentVolumeClaimVolumeSource(\n                                                      claim_name='my-rw-pvc'))\n                                ).add_volume_mount(k8s_client.V1VolumeMount(mount_path='/mnt/vol', name='my-rw-pv'))\n\n  training = dsl.ContainerOp(\n    name='training',\n    image='gcr.io/' + project_name + '/gcp-joc-end2end-demo-training',\n    command=['python'],\n    arguments=[\n      'main.py',\n      '--num_iter', '%s' % (num_iter),\n      '--iter_unit', 'epoch',\n      '--data_dir', '%s/%s/cifar-10-batches-bin' % (mount_dir, raw_data_dir),\n      '--batch_size', '%s' % (batch_size),\n      '--results_dir', '%s/%s' % (mount_dir, ckpt_dir),\n      '--use_tf_amp', '%s' % (use_tf_amp),\n      '--use_auto_loss_scaling', '%s' % (use_auto_loss_scaling),\n      '--storage_bucket_dir', '%s' % (storage_bucket),\n      '--model_dir', '%s/%s' % (mount_dir, model_name)\n    ],\n    file_outputs={}\n  ).set_gpu_limit(1).add_volume(k8s_client.V1Volume(name='my-rw-pv',\n                                                    persistent_volume_claim=k8s_client.V1PersistentVolumeClaimVolumeSource(\n                                                      claim_name='my-rwm-pvc'))\n                                ).add_volume_mount(k8s_client.V1VolumeMount(mount_path='/mnt/vol', name='my-rw-pv'))\n\n  serve = dsl.ContainerOp(\n    name='serve',\n    image='gcr.io/' + project_name + '/ml-pipeline-kubeflow-trtisserve',\n    arguments=[\"--trtserver_name\", trtserver_name,\n               '--model_version', model_version,\n               '--orig_model_path', '%s/%s' % (mount_dir, model_name),\n               \"--model_path\", '%s/%s' % (storage_bucket, model_dir)\n               ]\n  ).add_volume(k8s_client.V1Volume(name='my-rw-pv',\n                                   persistent_volume_claim=k8s_client.V1PersistentVolumeClaimVolumeSource(\n                                     claim_name='my-rwm-pvc'))\n               ).add_volume_mount(k8s_client.V1VolumeMount(mount_path='/mnt/vol', name='my-rw-pv')).apply(gcp.use_gcp_secret('user-nvidia-sa'))\n\n  webapp = dsl.ContainerOp(\n    name='webapp',\n    image='gcr.io/' + project_name + '/ml-pipeline-trtis-webapp-launcher',\n    arguments=[\"--workflow_name\", '%s' % ('{{workflow.name}}',),\n               \"--trtserver_name\", trtserver_name,\n               \"--model_name\", model_name,\n               \"--model_version\", model_version,\n               \"--webapp_prefix\", webapp_prefix,\n               \"--webapp_port\", webapp_port\n               ]\n  )\n\n  training.after(preprocessing)\n  serve.after(training)\n  webapp.after(serve)\n\n\nif __name__ == '__main__':\n  import kfp.compiler as compiler\n\n  compiler.Compiler().compile(end2end_demo, __file__ + '.tar.gz')"}
{"prompt": "Generate a Kubeflow Pipeline named `hello-world-pipeline` that performs the following:\n\nThis pipeline consists of **one component**:\n1.  **`hello_op`**: This component takes a `message` as an input parameter. Its function is to output the provided message.\n    *   **Inputs**:\n        *   `message`: A string with a default value of \"hello world\".\n    *   **Resource Configuration**: This component is configured to use a preemptible node pool on GCP.\n    *   **Error Handling**: It has a retry policy configured to retry up to 2 times in case of failure.\n\nThe pipeline definition includes the following input parameters:\n*   `job_id`: A string with a default value of \"xxxx\".\n*   `message`: A string with a default value of \"hello world\".\n\nThere are no explicit dependencies or complex control flow (like parallelFor, or `after` conditions) between multiple components, as there is only one component in this pipeline. The `hello_op` component is executed directly.\n\nThe pipeline is compiled using `kfp.v2.compiler.Compiler` into a JSON package named \"hello-world-pipeline.json\".", "completion": "import kfp\nfrom kfp.v2 import dsl\nfrom kfp import gcp\n\nPIPELINE_NAME = \"hello-world-pipeline\"\n\n# Initialize component store\ncomponent_store = kfp.components.ComponentStore(\n    local_search_paths=[\"pipelines/hello-world-pipeline\", \"components\"])\n\n# Create component factories\nhello_op = component_store.load_component(\"hello\")\n\n\n# Define pipeline\n@dsl.pipeline(\n    name=\"hello-world-pipeline\",\n    description=\"Output messages\"\n)\ndef pipeline(\n    job_id: str = \"xxxx\",\n    message: str = \"hello world\",\n):\n    hello_op(\n        message=message,\n    ).apply(gcp.use_preemptible_nodepool()) \\\n        .set_retry(num_retries=2)\n\n\nif __name__ == \"__main__\":\n    kfp.v2.compiler.Compiler().compile(\n        pipeline_func=pipeline, \n        package_path=\"hello-world-pipeline.json\"\n    )"}
{"prompt": "Generate a Kubeflow Pipeline named `Kensert_CNN_test` that performs a CNN workflow.\n\nThe pipeline consists of one component at this point:\n1.  **`preprocessing`**: This component uses the `pharmbio/pipelines-kensert-preprocess:test` Docker image.\n    *   **Functionality**: It's responsible for the preprocessing step of the CNN workflow. It takes `model_type` and `checkpoint_preprocess` as arguments.\n    *   **Inputs**:\n        *   `model_type` (PipelineParam, default value: \"Inception_v3\"): Specifies the type of model to be used.\n        *   `checkpoint_preprocess` (PipelineParam, default value: \"false\"): A boolean flag indicating whether to use a checkpoint for preprocessing.\n        *   `workspace_name` (PipelineParam, default value: \"kensert_CNN\"): Passed as an environment variable `WORKFLOW_NAME` to the container.\n    *   **Outputs**: It outputs a file named `labels` which corresponds to `/home/output/bbbc014_labels.npy` within the container.\n    *   **Configuration**: The container has its `image_pull_policy` set to \"Always\".\n\n**Pipeline Parameters**:\nThe pipeline accepts the following input parameters:\n*   `model_type` (default: \"Inception_v3\")\n*   `artifact_bucket` (default: \"kensert_CNN\")\n*   `checkpoint_preprocess` (default: \"false\")\n*   `checkpoint_training` (default: \"false\")\n*   `checkpoint_evaluation` (default: \"false\")\n*   `workspace_name` (default: \"kensert_CNN\")\n*   `model_repo` (default: \"\")\n\n**Utility Functions**:\n*   `mount_pvc`: A utility function that can be applied to a `ContainerOp` to mount a `PersistentVolumeClaim`. It takes `pvc_name`, `volume_name`, `volume_mount_path`, and optional `volume_sub_path` as arguments.\n*   `set_resources`: A utility function to set resource requests and limits (memory, CPU, GPUs) for a container. It takes `memory_req`, `memory_lim`, `cpu_req`, `cpu_lim`, `gpus`, and the `container` object as arguments.\n\n**Control Flow**:\nThe `preprocessing` component is the initial and currently only step in the pipeline. There are no explicit dependencies or parallel executions defined yet, but the prompt should anticipate the addition of `training` and `evaluation` steps as hinted by the unused pipeline parameters.", "completion": "import kfp.dsl as dsl\nimport kfp.gcp as gcp\nfrom kubernetes.client.models import V1SecretKeySelector, V1EnvVar\nfrom kubernetes import client as k8s_client\nfrom json import loads\n\ndef mount_pvc(pvc_name='pipeline-claim', volume_name='pipeline', volume_mount_path='/mnt/pipeline', volume_sub_path=None):\n    \"\"\"\n        Modifier function to apply to a Container Op to simplify volume, volume mount addition and\n        enable better reuse of volumes, volume claims across container ops.\n        Usage:\n            train = train_op(...)\n            train.apply(mount_pvc('claim-name', 'pipeline', '/mnt/pipeline'))\n    \"\"\"\n    def _mount_pvc(task):\n        from kubernetes import client as k8s_client\n        local_pvc = k8s_client.V1PersistentVolumeClaimVolumeSource(claim_name=pvc_name)\n        return (\n            task\n                .add_volume(\n                    k8s_client.V1Volume(name=volume_name, persistent_volume_claim=local_pvc)\n                )\n                .add_volume_mount(\n                    k8s_client.V1VolumeMount(mount_path=volume_mount_path, name=volume_name, sub_path=volume_sub_path)\n                )\n        )\n    return _mount_pvc\n\n\ndef set_resources(memory_req=None, memory_lim=None, cpu_req=None, cpu_lim=None, gpus=\"0\", container:dsl.ContainerOp.container=None):\n    if container:\n        container.set_memory_request(memory_req)\n        container.set_memory_limit(memory_lim)\n        container.set_cpu_request(cpu_req)\n        container.set_cpu_limit(cpu_lim)\n        if int(gpus) > 0:\n            container.set_gpu_limit(gpus)\n        return container\n    else:\n        return None\n\n@dsl.pipeline(\n  name='Kensert_CNN_test',\n  description='Testing a CNN workflow in kfpipelines'\n)\ndef cnn_workflow(\n    ### inputs to pipeline\n    model_type: dsl.PipelineParam=dsl.PipelineParam(name=\"model_type\", value=\"Inception_v3\"),\n    artifact_bucket: dsl.PipelineParam=dsl.PipelineParam(name=\"artifact_bucket\", value=\"kensert_CNN\"),\n    checkpoint_preprocess: dsl.PipelineParam=dsl.PipelineParam(name=\"checkpoint_preprocess\", value=\"false\"),\n    checkpoint_training: dsl.PipelineParam=dsl.PipelineParam(name=\"checkpoint_training\", value=\"false\"),\n    checkpoint_evaluation: dsl.PipelineParam=dsl.PipelineParam(name=\"checkpoint_evaluation\", value=\"false\"),\n    workspace_name: dsl.PipelineParam=dsl.PipelineParam(name=\"workspace_name\", value=\"kensert_CNN\"),\n    model_repo: dsl.PipelineParam=dsl.PipelineParam(name=\"model_repo\", value=\"\"),\n):\n\n    ### preprocessing step\n    preprocessing = dsl.ContainerOp(\n        name=\"preprocessing\",\n        image=\"pharmbio/pipelines-kensert-preprocess:test\",\n        arguments=[ \"--model-type\" , model_type, \"--checkpoint\", checkpoint_preprocess],\n        # NOTE: arguments from pipeline not utilized in training script yet\n        container_kwargs={\"image_pull_policy\": \"Always\", \"env\":[V1EnvVar(name=\"WORKFLOW_NAME\",value=workspace_name)]},\n        file_outputs={\"labels\":\"/home/output/bbbc014_labels.npy\"}\n    )\n    ### add resource definitions, pvc mounts etc.\n    preprocessing.apply(mount_pvc(pvc_name=\"external-images-kubeflow-pvc\",volume_name=\"external-images-kubeflow-pv\",volume_mount_path=\"/mnt/data\"))\n    preprocessing.apply(mount_pvc(pvc_name=\"kubeflow-workdir-pvc\",volume_name=\"kubeflow-workdir-pv\",volume_mount_path=\"/mnt/data/workdir\",volume_sub_path=workspace_name))\n    set_resources(memory_req=\"4Gi\", memory_lim=\"8Gi\", cpu_req=\"2\", cpu_lim=\"4\", container=preprocessing.container)\n\n    ### training step\n    training = dsl.ContainerOp(\n        name=\"training\",\n        image=\"pharmbio/pipelines-kensert-training:test\",\n        arguments=[ \"--model-type\" , model_type,  \"--checkpoint\", checkpoint_training],\n        container_kwargs={\"image_pull_policy\": \"Always\", \"env\":[V1EnvVar(name=\"WORKFLOW_NAME\",value=workspace_name)]}\n    )\n    ### set order (after preprocessing), add pvc and resource definitions\n    training.apply(mount_pvc(pvc_name=\"kubeflow-workdir-pvc\",volume_name=\"kubeflow-workdir-pv\",volume_mount_path=\"/mnt/data/workdir\",volume_sub_path=workspace_name))\n    set_resources(memory_req=\"4Gi\", memory_lim=\"8Gi\", cpu_req=\"2\", cpu_lim=\"4\", gpus=\"1\", container=training.container)\n    training.after(preprocessing)\n\n     ### evaluation step\n    evaluation = dsl.ContainerOp(\n        name=\"evaluation\",\n        image=\"pharmbio/pipelines-kensert-evaluation:test\",\n        # NOTE: arguments from pipeline not utilized in training script yet\n        arguments=[ \"--model-type\" , model_type,  \"--checkpoint\", checkpoint_evaluation],\n        container_kwargs={\"image_pull_policy\": \"Always\", \"env\":[V1EnvVar(name=\"WORKFLOW_NAME\",value=workspace_name)]}\n        #file_outputs={\"prediction_accuracy\":\"/home/output/kensert_CNN/predictions_bbbc014\"}\n    )\n    ### set order (after training), add pvc and resource definitions\n    evaluation.apply(mount_pvc(pvc_name=\"kubeflow-workdir-pvc\",volume_name=\"kubeflow-workdir-pv\",volume_mount_path=\"/mnt/data/workdir\",volume_sub_path=workspace_name))\n    set_resources(memory_req=\"1Gi\", memory_lim=\"4Gi\", cpu_req=\"1\", cpu_lim=\"2\", gpus=\"1\", container=evaluation.container)\n    evaluation.after(training)\n\n    ### model building step\n    model_building = dsl.ContainerOp(\n        name=\"model_building\",\n        image=\"pharmbio/pipelines-kensert-building:test\",\n        arguments=[ ],\n        container_kwargs={\"image_pull_policy\": \"Always\", \"env\":[V1EnvVar(name=\"WORKFLOW_NAME\",value=workspace_name), V1EnvVar(name=\"MODEL_REPO\",value=model_repo)]}\n    )\n    ### set order (after evaluation), add secrets, socket (hostpath pv mount), pvc and resource definitions\n    model_building.apply(mount_pvc(pvc_name=\"kubeflow-workdir-pvc\",volume_name=\"kubeflow-workdir-pv\",volume_mount_path=\"/home/models\",volume_sub_path=(str(workspace_name) + \"/models\")))\n    docker_socket_volume = k8s_client.V1Volume(name='dockervol-pv', host_path=k8s_client.V1HostPathVolumeSource(path='/var/run/docker.sock'))\n    docker_credentials_volume = k8s_client.V1Volume(name=\"dockercreds\", secret=k8s_client.V1SecretVolumeSource(secret_name=\"dockercreds\"))\n    model_building.add_pvolumes({\"/var/run/docker.sock\":docker_socket_volume, \"/root/.docker/\":docker_credentials_volume})\n    set_resources(memory_req=\"1Gi\", memory_lim=\"2Gi\", cpu_req=\"1\", cpu_lim=\"2\", container=model_building.container)\n    model_building.after(evaluation)\n\n### build pipeline when executing python script\nif __name__ == '__main__':\n    import kfp.compiler as compiler\n    compiler.Compiler().compile(cnn_workflow, __file__ + '.tar.gz')"}
{"prompt": "Generate a Kubeflow Pipeline named `pipeline-1` that performs the following:\n\nThis pipeline has no components or specific control flow defined in the provided code snippet. The pipeline is currently empty.", "completion": "import sys\nsys.path.append(\"../..\")\n\nimport kfp\n\n\n@kfp.dsl.pipeline(name=\"pipeline-1\")\ndef pipeline():\n    pass"}
{"prompt": "Generate a Kubeflow Pipeline named `one-step-pipeline-with-if-placeholder-supply-both` that performs the following:\n\n**Pipeline Name:** `one-step-pipeline-with-if-placeholder-supply-both`\n\n**Number of Components:** 1\n\n**Components:**\n\n1.  **`component_op`**:\n    *   **Function:** This component is designed to echo a required input and conditionally echo two optional inputs based on their presence. If `optional_input_2` is not provided, it defaults to 'default value'.\n    *   **Inputs:**\n        *   `required_input` (String): A mandatory input.\n        *   `optional_input_1` (String, optional): An optional input.\n        *   `optional_input_2` (String, optional): An optional input.\n    *   **Implementation Details:**\n        *   Uses the `registry.k8s.io/busybox` image.\n        *   The command starts with `echo --arg0` followed by the value of `required_input`.\n        *   It includes a conditional `if` statement: if `optional_input_1` is present, it adds `--arg1` and its value to the command.\n        *   It includes another conditional `if` statement: if `optional_input_2` is present, it adds `--arg2` and its value. Otherwise (`else`), it adds `--arg2` and the string 'default value'.\n    *   **Usage within the pipeline:**\n        *   It is instantiated as `component` and receives the following inputs:\n            *   `required_input`: Connected to the pipeline parameter `input0` (default 'input0').\n            *   `optional_input_1`: Connected to the pipeline parameter `input1` (default 'input1').\n            *   `optional_input_2`: Connected to the pipeline parameter `input2` (default 'input2').\n\n**Control Flow:**\n*   The pipeline has three string parameters: `input0`, `input1`, and `input2`, all with default values.\n*   The `component_op` is executed once, receiving all three pipeline parameters as its corresponding inputs.\n\n---\n\nAdditionally, regenerate a Kubeflow Pipeline named `one-step-pipeline-with-if-placeholder-supply-none` that performs the following:\n\n**Pipeline Name:** `one-step-pipeline-with-if-placeholder-supply-none`\n\n**Number of Components:** 1\n\n**Components:**\n\n1.  **`component_op`**:\n    *   **Function:** This component is designed to echo a required input and conditionally echo two optional inputs based on their presence. If `optional_input_2` is not provided, it defaults to 'default value'.\n    *   **Inputs:**\n        *   `required_input` (String): A mandatory input.\n        *   `optional_input_1` (String, optional): An optional input.\n        *   `optional_input_2` (String, optional): An optional input.\n    *   **Implementation Details:**\n        *   Uses the `registry.k8s.io/busybox` image.\n        *   The command starts with `echo --arg0` followed by the value of `required_input`.\n        *   It includes a conditional `if` statement: if `optional_input_1` is present, it adds `--arg1` and its value to the command.\n        *   It includes another conditional `if` statement: if `optional_input_2` is present, it adds `--arg2` and its value. Otherwise (`else`), it adds `--arg2` and the string 'default value'.\n    *   **Usage within the pipeline:**\n        *   It is instantiated as `component` and receives only the `required_input`:\n            *   `required_input`: Connected to the pipeline parameter `input0` (default 'input0').\n        *   `optional_input_1` and `optional_input_2` are *not* provided in this pipeline definition.\n\n**Control Flow:**\n*   The pipeline has one string parameter: `input0` with a default value.\n*   The `component_op` is executed once, receiving only the `input0` pipeline parameter as its `required_input`. The optional inputs are intentionally omitted at compile-time.", "completion": "# Copyright 2020,2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom kfp import dsl, components\n\ncomponent_op = components.load_component_from_text('''\nname: Component with optional inputs\ninputs:\n- {name: required_input, type: String, optional: false}\n- {name: optional_input_1, type: String, optional: true}\n- {name: optional_input_2, type: String, optional: true}\nimplementation:\n  container:\n    image: registry.k8s.io/busybox\n    command:\n    - echo\n    args:\n    - --arg0\n    - {inputValue: required_input}\n    - if:\n        cond:\n          isPresent: optional_input_1\n        then:\n          - --arg1\n          - {inputValue: optional_input_1}\n    - if:\n        cond:\n          isPresent: optional_input_2\n        then:\n          - --arg2\n          - {inputValue: optional_input_2}\n        else:\n          - --arg2\n          - 'default value'\n''')\n\n\n@dsl.pipeline(name='one-step-pipeline-with-if-placeholder-supply-both')\ndef pipeline_both(input0: str = 'input0',\n                  input1: str = 'input1',\n                  input2: str = 'input2'):\n    # supply both optional_input_1 and optional_input_2\n    component = component_op(\n        required_input=input0, optional_input_1=input1, optional_input_2=input2)\n\n\n@dsl.pipeline(name='one-step-pipeline-with-if-placeholder-supply-none')\ndef pipeline_none(input0: str = 'input0'):\n    # supply neither optional_input_1 nor optional_input_2\n    # Note, KFP only supports compile-time optional arguments, e.g. it's not\n    # supported to write a pipeline that supplies both inputs and pass None\n    # at runtime -- in that case, the input arguments will be interpreted as\n    # the raw text \"None\".\n    component = component_op(required_input=input0)"}
{"prompt": "Generate a Kubeflow Pipeline named `Pipeline_Manager` that performs a series of operations based on a dynamically loaded DAG configuration.\n\nThis pipeline consists of **multiple** components, with the exact number and names of components being determined at runtime by loading a YAML configuration file.\n\nHere's a breakdown of its functionality and structure:\n\n**1. Configuration Loading:**\n   - The pipeline uses a `load_dag_configuration` utility function to parse a YAML DAG file.\n   - This function takes `dag_path` as an input and returns three pieces of information: a list of `components`, a list of `dependencies`, and the `initial_input_media` file path.\n\n**2. Dynamic Component Creation:**\n   - A `create_component` utility function dynamically defines Kubeflow container components using `@dsl.container_component`.\n   - Each dynamically created component function is named after the `component_name` (with hyphens replaced by underscores).\n   - Each component takes `input_path` and `output_path` as string arguments.\n   - The components execute a Docker image in the format `{username}/{component_name}:latest`.\n   - The container command is `python main.py`, with arguments `-i` (for `input_path`) and `-o` (for `output_path`).\n\n**3. Component Setup and Execution:**\n   - The `setup_component` utility function is responsible for retrieving and configuring the dynamically created component functions.\n   - It takes `component_name`, `input_path`, `output_dir`, and `pvc_name` as arguments.\n   - Each component step in the pipeline will:\n     - Mount a Kubernetes Persistent Volume Claim (PVC) named `pvc_name` to the path `/data`.\n     - The `input_path` for a component will typically be the output path of a preceding component or the `initial_input_media`.\n     - The `output_path` for a component is constructed using `output_dir`.\n\n**4. Control Flow and Dependencies:**\n   - The pipeline's control flow is determined by the `dependencies` loaded from the DAG configuration.\n   - The pipeline dynamically constructs the execution graph, ensuring that components run in the correct order based on their defined dependencies.\n   - Specifically, if component `B` depends on component `A`, then `B` will run `after` `A`.\n\n**5. Inputs and Outputs:**\n   - **Pipeline Input:**\n     - `dag_path`: A string representing the path to the YAML DAG configuration file.\n     - `pvc_name`: A string specifying the name of the Persistent Volume Claim to be used.\n     - `username`: A string representing the Docker username used for component images.\n   - **Component Inputs:** Each dynamic component will receive `input_path` and `output_path` strings.\n   - **Component Outputs:** Each component is expected to save its output file in the specified `output_path` within the mounted PVC. The naming convention for output files is `output_name + \".tar.gz\"`, where `output_name` is derived from the base name of `output_path_dir`.\n\n**6. Libraries and Tools:**\n   - The pipeline heavily relies on `kfp` for Kubeflow Pipeline definition.\n   - `kfp.kubernetes.mount_pvc` is used for PVC mounting.\n   - `yaml` is used for loading DAG configurations.\n   - `os`, `time`, `subprocess`, `logging`, `argparse`, and `dotenv` are used for general Python operations, logging, and environment management within the utility functions.\n\n**Note:** The `get_output_filepath` function is commented out and not used, indicating a design decision to pass explicit output paths rather than relying on directory listing.", "completion": "import os\nimport time\nimport yaml\nimport subprocess\nimport logging\nimport argparse\nfrom dotenv import load_dotenv\n\nfrom kfp import dsl\nfrom kfp.kubernetes import mount_pvc\n\nfrom kube.pvc_manager import *\nfrom kube.pipeline_run import *\n\n\n# Configure logging to display information based on your needs\nlogging.basicConfig(level=logging.INFO, format='[%(asctime)s] - %(message)s', datefmt='%H:%M:%S')\n\n\ndef load_dag_configuration(dag_path):\n    \"\"\"\n    Load the yaml dag configuration file, to extract the required information\n\n    :param dag_path: The file path to the YAML configuration file\n    :return: A tuple containing lists of components, dependencies, and the initial input media file path\n    \"\"\"\n    with open(dag_path, 'r') as file:\n        data = yaml.safe_load(file)\n        return data['System']['components'], data['System']['dependencies'], data['System']['input_media']\n\n\n# With download_from_pvc method defined in pvc_manager.py, it might be possible to search for the output file path\n# saved by the previous component (independently of its name) in the PVC and download it to the local machine, then\n# use it as the input for the next component.\n# -----\n# This method is not used, because it would imply to create an updated pod after each component by overwriting the\n# previous one, which is not an optimal approach.\n# -----\n# instead we consider that the component will save their output file as: output_name + \".tar.gz\"\n# where output_name = os.path.basename(output_path_dir)\n\"\"\"\ndef get_output_filepath(output_dir: str):\n    files = os.listdir(output_dir)\n    if not files:\n        logging.error(f\"No files found in directory: {output_dir}\")\n    file_path = os.path.join(output_dir, files[0])\n    return file_path\n\"\"\"\n\n\ndef create_component(username: str, component_name: str):\n    \"\"\"\n    Dynamically create a reusable container component for the Kubeflow Pipeline steps\n\n    :param username: Docker username to prefix to the Docker image name\n    :param component_name: Name of the component, used to generate the function name and specify the Docker image\n    \"\"\"\n    comp_name = component_name.replace('-', '_')\n\n    # Default kfp.container_component configuration\n    component_code = f\"\"\"\n@dsl.container_component\ndef {comp_name}(input_path: str, output_path: str):\n    return dsl.ContainerSpec(\n        image=f'{username}/{component_name}:latest',\n        command=['python', 'main.py'],\n        args=[\n            '-i', input_path, '-o', output_path\n        ]\n    )\n    \"\"\"\n    # Execute the generated code to define the component function dynamically\n    exec(component_code, globals())\n\n\ndef setup_component(component_name: str, input_path: str, output_dir: str, pvc_name: str):\n    \"\"\"\n    Set up a component for the pipeline with various configurations (including input and output paths, and PVC mounting),\n    by retrieving the dynamically created component function.\n    If needed, the method can be extended to include more configurations based on the Kubeflow pipeline requirements.\n\n    :param name: Name of the component to be included in the pipeline\n    :param input_path: Path to the input file for the component\n    :param output_dir: Output directory for the component's results\n    :param pvc_name: Name of the Persistent Volume Claim (PVC) to be mounted\n    :return: Configured component operation for the pipeline\n    \"\"\"\n    comp_func = globals().get(component_name.replace('-', '_'))\n    component_op = comp_func(input_path=input_path, output_path=output_dir)\n    component_op = mount_pvc(component_op, pvc_name=pvc_name, mount_path='/mnt/data')\n    # Caching can be enabled or disabled here for a specific component, if needed.\n    # component_op.set_caching_options(False)\n    return component_op\n\n\ndef generate_pipeline(username: str, dag_components: list, dag_dependencies: list, init_input: str):\n    \"\"\"\n    Dynamically generate a Kubeflow Pipeline based on the DAG configuration. This involves creating container\n    components for each step in the pipeline and setting up their execution order based on dependencies.\n\n    :param username: Docker username for Docker image naming\n    :param dag_components: List of components defined in the DAG configuration file\n    :param dag_dependencies: List of dependencies defined in the DAG configuration file\n    :param init_input: Name of the initial input media file path\n    :return: The Kubeflow Pipeline function\n    \"\"\"\n    for component in dag_components:\n        create_component(username, component)\n    create_component(username, 'save-media')\n\n    @dsl.pipeline(\n        name=\"Kubeflow Autopipe\",\n        description=\"Automatically generated pipeline based on the provided configuration file\"\n    )\n    def dynamic_pipeline(pvc_name: str):\n        base_mount = \"/mnt/data\"\n        component_op = {}\n        input_path = init_input\n\n        # Set up the save_media component as first component\n        output_dir = f\"{base_mount}/\"\n        component_op['save-media'] = setup_component('save-media', input_path, output_dir, pvc_name)\n\n        # Set up the other components based on dependencies\n        for dependency in dag_dependencies:\n            this_component, next_component, _ = dependency\n\n            if this_component not in component_op:\n                output_dir_this = f\"{base_mount}/{this_component}\"\n                input_path = f\"{base_mount}/{init_input}\"\n                component_op[this_component] = setup_component(this_component, input_path, output_dir_this, pvc_name)\n                component_op[this_component].after(component_op['save-media'])\n\n            if next_component not in component_op:\n                output_dir_next = f\"{base_mount}/{next_component}\"\n                input_path = f\"{base_mount}/{this_component}.tar.gz\"\n                component_op[next_component] = setup_component(next_component, input_path, output_dir_next, pvc_name)\n                component_op[next_component].after(component_op[this_component])\n\n    return dynamic_pipeline\n\n\ndef main(input_file: str):\n    # Define the local path to store the outputs saved into the pvc\n    local_path = 'output'\n    # Save need data from the configuration file\n    dag_components, dag_dependencies, media = load_dag_configuration(input_file)\n\n    # Save register_username defined in the .env file\n    load_dotenv()\n    register_username = os.getenv('register_USERNAME')\n\n    # Create the PVC for the pipeline\n    pvc_name = create_pvc()\n\n    # Save the name of the file to be used as input for the pipeline\n    input_filename = os.path.basename(media)\n\n    # Generate the pipeline function\n    pipeline_func = generate_pipeline(username=register_username, dag_components=dag_components, dag_dependencies=dag_dependencies, init_input=input_filename)\n    pipeline_filename = 'pipeline.yaml'\n    # Execute the pipeline\n    pipeline_run(pvc_name, pipeline_func, pipeline_filename)\n    time.sleep(5)\n\n    # Download the output file from the PVC to the local machine\n    download_from_pvc(pvc_name, local_path)\n    # Delete the PVC after the pipeline execution\n    delete_pvc(pvc_name)\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"-i\", \"--input\", required=True, help=\"path to application_dag.yaml configuration file\")\n    args = vars(parser.parse_args())\n\n    main(args['input'])"}
{"prompt": "Generate a Kubeflow Pipeline named `data_passing_pipeline` that performs a data preprocessing and training workflow.\n\nThe pipeline consists of **two components**:\n1.  **`preprocess`**: This component performs a dummy preprocessing step.\n    *   **Inputs**:\n        *   `message` (string): An input parameter.\n        *   `empty_message` (string, optional): An input message that defaults to empty.\n    *   **Outputs**:\n        *   `output_dataset_one` (Output[Dataset]): A metadata-rich handle to an output artifact of type `Dataset`.\n        *   `output_dataset_two_path` (OutputPath('Dataset')): A locally accessible filepath for another output artifact of type `Dataset`.\n        *   `output_parameter_path` (OutputPath(str)): A locally accessible filepath for an output parameter of type string.\n        *   `output_bool_parameter_path` (OutputPath(bool)): A locally accessible filepath for an output parameter of type boolean.\n        *   `output_dict_parameter_path` (OutputPath(Dict[str, int])): A locally accessible filepath for an output parameter of type dictionary.\n        *   `output_list_parameter_path` (OutputPath(List[str])): A locally accessible filepath for an output parameter of type list.\n    *   **Functionality**: It writes the input `message` to `output_dataset_one` and `output_dataset_two_path`. It also writes the `message` to `output_parameter_path`, `True` to `output_bool_parameter_path`, `{'A': 1, 'B': 2}` (as JSON) to `output_dict_parameter_path`, and `['a', 'b', 'c']` (as JSON) to `output_list_parameter_path`. It uses the `json` library for serializing dictionary and list outputs.\n2.  **`train`**: This component performs a dummy training step.\n    *   **Inputs**:\n        *   `dataset_one` (Input[Dataset]): An input artifact of type `Dataset`.\n        *   `dataset_two_path` (InputPath('Dataset')): A locally accessible path for another input artifact of type `Dataset`.\n        *   `message` (Input[str]): An input parameter of type string.\n        *   `boolean` (Input[bool]): An input parameter of type boolean.\n        *   `dictionary` (Input[Dict[str, int]]): An input parameter of type dictionary.\n        *   `a_list` (Input[List[str]]): An input parameter of type list.\n    *   **Outputs**: None explicitly defined in the provided snippet, but it implies consumption of inputs.\n    *   **Functionality**: It reads content from `dataset_one.path` and `dataset_two_path`. It also prints the `message`, `boolean`, `dictionary`, and `a_list` inputs. It uses the `json` library for deserializing dictionary and list inputs.\n\n**Control Flow**:\nThe pipeline executes sequentially. The `train` component depends on the outputs of the `preprocess` component.\n*   The `dataset_one` input of `train` receives the `output_dataset_one` from `preprocess`.\n*   The `dataset_two_path` input of `train` receives the `output_dataset_two_path` from `preprocess`.\n*   The `message` input of `train` receives the `output_parameter_path` from `preprocess`.\n*   The `boolean` input of `train` receives the `output_bool_parameter_path` from `preprocess`.\n*   The `dictionary` input of `train` receives the `output_dict_parameter_path` from `preprocess`.\n*   The `a_list` input of `train` receives the `output_list_parameter_path` from `preprocess`.\n\n**Pipeline Inputs**:\nThe `data_passing_pipeline` takes a single input parameter:\n*   `message` (str): Defaults to \"Hello world!\".\n\n**Tools/Libraries**: The components utilize the `json` standard library for serializing/deserializing dictionary and list data. The pipeline definition itself uses the `kfp` library, specifically `kfp.dsl` for `@dsl.pipeline`, `@component`, `Input`, `InputPath`, `Output`, and `OutputPath` decorators/types.", "completion": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Sample pipeline for passing data in KFP v2.\"\"\"\nfrom typing import Dict, List\nimport os\n\nfrom kfp import dsl\nfrom kfp.dsl import Input, InputPath, Output, OutputPath, Dataset, Model, component\nimport kfp.compiler as compiler\n\n# In tests, we install a KFP package from the PR under test. Users should not\n# normally need to specify `kfp_package_path` in their component definitions.\n_KFP_PACKAGE_PATH = os.getenv('KFP_PACKAGE_PATH')\n\n\n@component(kfp_package_path=_KFP_PACKAGE_PATH)\ndef preprocess(\n    # An input parameter of type string.\n    message: str,\n    # Use Output[T] to get a metadata-rich handle to the output artifact\n    # of type `Dataset`.\n    output_dataset_one: Output[Dataset],\n    # A locally accessible filepath for another output artifact of type\n    # `Dataset`.\n    output_dataset_two_path: OutputPath('Dataset'),\n    # A locally accessible filepath for an output parameter of type string.\n    output_parameter_path: OutputPath(str),\n    # A locally accessible filepath for an output parameter of type bool.\n    output_bool_parameter_path: OutputPath(bool),\n    # A locally accessible filepath for an output parameter of type dict.\n    output_dict_parameter_path: OutputPath(Dict[str, int]),\n    # A locally accessible filepath for an output parameter of type list.\n    output_list_parameter_path: OutputPath(List[str]),\n    # An input message that defaults to empty.\n    empty_message: str = \"\",\n):\n    \"\"\"Dummy preprocessing step\"\"\"\n\n    # Use Dataset.path to access a local file path for writing.\n    # One can also use Dataset.uri to access the actual URI file path.\n    with open(output_dataset_one.path, 'w') as f:\n        f.write(message)\n\n    # OutputPath is used to just pass the local file path of the output artifact\n    # to the function.\n    with open(output_dataset_two_path, 'w') as f:\n        f.write(message)\n\n    with open(output_parameter_path, 'w') as f:\n        f.write(message)\n\n    with open(output_bool_parameter_path, 'w') as f:\n        f.write(\n            str(True)\n        )  # use either `str()` or `json.dumps()` for bool values.\n\n    import json\n    with open(output_dict_parameter_path, 'w') as f:\n        f.write(json.dumps({'A': 1, 'B': 2}))\n\n    with open(output_list_parameter_path, 'w') as f:\n        f.write(json.dumps(['a', 'b', 'c']))\n\n\n@component(kfp_package_path=_KFP_PACKAGE_PATH)\ndef train(\n    # Use InputPath to get a locally accessible path for the input artifact\n    # of type `Dataset`.\n    dataset_one_path: InputPath('Dataset'),\n    # Use Input[T] to get a metadata-rich handle to the input artifact\n    # of type `Dataset`.\n    dataset_two: Input[Dataset],\n    # An input parameter of type string.\n    message: str,\n    # Use Output[T] to get a metadata-rich handle to the output artifact\n    # of type `Dataset`.\n    model: Output[Model],\n    # An input parameter of type bool.\n    input_bool: bool,\n    # An input parameter of type dict.\n    input_dict: Dict[str, int],\n    # An input parameter of type List[str].\n    input_list: List[str],\n    # An input parameter of type int with a default value.\n    num_steps: int = 100,\n):\n    \"\"\"Dummy Training step\"\"\"\n    with open(dataset_one_path, 'r') as input_file:\n        dataset_one_contents = input_file.read()\n\n    with open(dataset_two.path, 'r') as input_file:\n        dataset_two_contents = input_file.read()\n\n    line = (\n        f'dataset_one_contents: {dataset_one_contents} || '\n        f'dataset_two_contents: {dataset_two_contents} || '\n        f'message: {message} || '\n        f'input_bool: {input_bool}, type {type(input_bool)} || '\n        f'input_dict: {input_dict}, type {type(input_dict)} || '\n        f'input_list: {input_list}, type {type(input_list)} \\n'\n    )\n\n    with open(model.path, 'w') as output_file:\n        for i in range(num_steps):\n            output_file.write('Step {}\\n{}\\n=====\\n'.format(i, line))\n\n    # model is an instance of Model artifact, which has a .metadata dictionary\n    # to store arbitrary metadata for the output artifact.\n    model.metadata['accuracy'] = 0.9\n\n\n@dsl.pipeline(name='my-test-pipeline-beta')\ndef pipeline(message: str = 'message'):\n    preprocess_task = preprocess(message=message)\n    train_task = train(\n        dataset_one_path=preprocess_task.outputs['output_dataset_one'],\n        dataset_two=preprocess_task.outputs['output_dataset_two_path'],\n        message=preprocess_task.outputs['output_parameter_path'],\n        input_bool=preprocess_task.outputs['output_bool_parameter_path'],\n        input_dict=preprocess_task.outputs['output_dict_parameter_path'],\n        input_list=preprocess_task.outputs['output_list_parameter_path'],\n    )\n\n\nif __name__ == '__main__':\n    compiler.Compiler().compile(\n        pipeline_func=pipeline, package_path=__file__.replace('.py', '.yaml')\n    )"}
{"prompt": "Generate a Kubeflow Pipeline named `stackdemos-jjtest-ml-components-training-component` that performs the following steps:\n\nThe pipeline consists of two primary component definitions: `training_op` and `http_download_op`.\n\n1.  **`training_op` Component**:\n    *   **Function**: Encapsulates a container operation for running a Python training script. It dynamically sets the container image.\n    *   **Inputs**:\n        *   `script` (Python string): Path to the Python script to be executed.\n        *   `image` (Python string, optional): Docker image to use for the container. If not provided and running in an IPython environment, it attempts to retrieve `TRAINING_IMAGE` from the IPython user namespace.\n        *   `arguments` (Python list, optional): A list of command-line arguments to pass to the script.\n        *   `file_outputs` (Python dictionary, optional): Specifies output files to be captured by Kubeflow.\n    *   **Output**: Returns a `kfp.dsl.ContainerOp` instance.\n    *   **Logic**:\n        *   Checks if `image` is provided. If not, and running in IPython, tries to get `TRAINING_IMAGE` from `IPython.get_ipython().user_ns`.\n        *   Raises a `ValueError` if `image` is still missing.\n        *   The component's name is derived from the `script` parameter, with non-alphanumeric characters replaced by hyphens.\n        *   The container command is `/usr/local/bin/python` followed by the `script` path.\n\n2.  **`http_download_op` Component**:\n    *   **Function**: Downloads a file from a given URL using `curl` and performs an MD5 checksum pre-check to avoid re-downloading if the file already exists and matches the checksum.\n    *   **Inputs**:\n        *   `url` (Python string): The URL of the file to download.\n        *   `download_to` (Python string): The local path where the file should be saved.\n        *   `md5sum` (Python string): The expected MD5 checksum of the file.\n    *   **Output**: Returns a `kfp.dsl.ContainerOp` instance.\n    *   **Logic**:\n        *   The component is named `download-artifact`.\n        *   It uses the `appropriate/curl` Docker image.\n        *   The command executed is `sh -c`.\n        *   The arguments contain a shell script that first checks if the `md5sum` of the existing `download_to` file matches the provided `md5sum`. If they match, it prints a \"Skipping\" message. Otherwise, it downloads the file using `curl -#Lv --create-dirs -o {download_to} {url}`.\n\n**Control Flow**:\nThe provided Python file defines component factory functions, not a complete pipeline structure using `@dsl.pipeline`. Therefore, there are no explicit dependencies, parallel execution, or `after` conditions defined at this level. These functions are designed to be called within a larger Kubeflow pipeline definition to instantiate the actual components.", "completion": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef training_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('TRAINING_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=['/usr/local/bin/python', script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )"}
{"prompt": "Here's the prompt you requested, designed for an LLM to regenerate the Kubeflow Pipeline:\n\nGenerate a Kubeflow Pipeline named `fail-pipeline` that performs the following:\n\nThis pipeline consists of **one** component:\n- **Component `fail`**: This component is designed to fail during its execution. Its primary function is to demonstrate a failing pipeline scenario. It does not take any explicit inputs or produce any explicit outputs that are passed between components.\n\nThe control flow of the pipeline is straightforward:\n- The `fail` component executes directly without any dependencies on other tasks.\n\nNo specific external tools or libraries like sklearn or Snowflake are explicitly used or demonstrated within this pipeline's component logic; its purpose is to show a basic failure.", "completion": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Fail pipeline.\"\"\"\n\nfrom __future__ import annotations\nimport unittest\nimport kfp.deprecated as kfp\nimport kfp_server_api\nfrom ml_metadata.proto import Execution\nfrom .fail import fail_pipeline\nfrom .fail_v2 import fail_pipeline as fail_v2_pipeline\nfrom kfp.samples.test.utils import TaskInputs, TaskOutputs, run_pipeline_func, TestCase, KfpTask\n\n\ndef verify(run, **kwargs):\n    assert run.status == 'Failed'\n\n\ndef verify_v2(t: unittest.TestCase, run: kfp_server_api.ApiRun,\n              tasks: dict[str, KfpTask], **kwargs):\n    t.assertEqual(run.status, 'Failed')\n    t.assertEqual(\n        {\n            'fail':\n                KfpTask(\n                    name='fail',\n                    type='system.ContainerExecution',\n                    # TODO(Bobgy): fix v2 engine to properly publish FAILED state.\n                    state=Execution.State.RUNNING,\n                    inputs=TaskInputs(parameters={}, artifacts=[]),\n                    outputs=TaskOutputs(parameters={}, artifacts=[]),\n                )\n        },\n        tasks,\n    )\n\n\nrun_pipeline_func([\n    TestCase(\n        pipeline_func=fail_v2_pipeline,\n        verify_func=verify_v2,\n        mode=kfp.dsl.PipelineExecutionMode.V2_ENGINE),\n    TestCase(\n        pipeline_func=fail_pipeline,\n        verify_func=verify,\n        mode=kfp.dsl.PipelineExecutionMode.V1_LEGACY),\n])"}
{"prompt": "Generate a Kubeflow Pipeline named `My first pipeline` that performs the following:\n\nThis pipeline consists of a single component.\n\nThe component is named `echo` and its function is to print \"hello world\" to standard output. It uses the `library/bash:4.4.23` Docker image and executes the command `sh -c 'echo \"hello world\"'`.\n\nThe control flow is straightforward; there are no complex dependencies or parallel executions as it's a single-component pipeline.", "completion": "# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp\nfrom kfp import dsl\n\ndef echo_op():\n    return dsl.ContainerOp(\n        name='echo',\n        image='library/bash:4.4.23',\n        command=['sh', '-c'],\n        arguments=['echo \"hello world\"']\n    )\n\n@dsl.pipeline(\n    name='My first pipeline',\n    description='A hello world pipeline.'\n)\ndef hello_world_pipeline():\n    echo_task = echo_op()\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(hello_world_pipeline, __file__ + '.yaml')"}
{"prompt": "This is not a Kubeflow pipeline definition. It's a Python file containing helper functions (`_is_ipython`, `training_op`, `http_download_op`) that could be used *within* a Kubeflow pipeline to define components.\n\nTo provide a prompt for an LLM, I need an actual Kubeflow pipeline decorated with `@dsl.pipeline` and `@component`.\n\n**Please provide the actual Kubeflow pipeline code, including the `@dsl.pipeline` and `@component` decorators, for me to generate the prompt.**", "completion": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef training_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('TRAINING_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=['/usr/local/bin/python', script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )"}
{"prompt": "Generate a Kubeflow Pipeline named `retrain` that performs model retraining based on new data availability.\n\nThis pipeline consists of two main components:\n1.  **`check_cnt`**: This component, implemented as a Python function, is responsible for counting the number of newly added images in a specified Google Cloud Storage bucket.\n    *   **Inputs**:\n        *   `bucket_name` (str): The name of the GCS bucket to check.\n        *   `data_type` (str): A string indicating the type of data (its specific usage is not shown in the provided snippet but is part of the function signature).\n    *   **Outputs**:\n        *   A `NamedTuple` with two fields:\n            *   `count` (int): The calculated number of images.\n            *   `type` (str): The `data_type` input passed through.\n    *   **Functionality**: Connects to Google Cloud Storage (`google.cloud.storage`) to list blobs within a specified prefix (`\"originals\"`) in the `\"images-original\"` bucket and returns the count of these blobs, excluding the root directory.\n\n2.  **`retrain_op`**: This component is a `ContainerOp` that executes a retraining script.\n    *   **Image**: `us-west1-docker.pkg.dev/viai/retrain:v1.0`\n    *   **Arguments**:\n        *   `--data-dir`: The directory containing training data (represented by the `data_dir` variable).\n        *   `--torch-export-dir`: The directory where the trained model should be exported (represented by the `model_export_dir` variable).\n    *   **Functionality**: Executes a Python script `retrain.py` within the container image to perform the model retraining process.\n\nThe pipeline flow is as follows:\n*   The `check_cnt` component is executed first.\n*   A **conditional step** is introduced: The `retrain_op` component is only executed if the `count` output from the `check_cnt` component is greater than 100.\n*   **Parameters**: The pipeline accepts the following parameters:\n    *   `name` (default: \"retrain\")\n    *   `training_image` (default: \"gcr.io/aiffel-gn-3/retrain:latest\")\n    *   `training_namespace` (default: \"kubeflow\")\n    *   `model_export_dir` (default: \"gs://model-cpt/\")\n*   **GCP Integration**: If the `platform` variable is set to \"GCP\" (which is assumed outside the pipeline definition), the `retrain_op` component will apply a GCP secret named `user-gcp-sa` for authentication.\n\n**Libraries used**: `kfp`, `kfp.dsl`, `kfp.gcp`, `google.cloud.storage`.", "completion": "from typing import NamedTuple\nimport kfp\nimport kfp.dsl as dsl\nimport kfp.gcp as gcp\nfrom kfp import components as comp\nfrom typing import NamedTuple\n\n\ndef check_cnt(\n    bucket_name: str, data_type: str\n) -> NamedTuple(\"output\", [(\"count\", int), (\"type\", str)]):\n    \"\"\"\n        count the number of newly added images\n    \"\"\"\n    from google.cloud import storage\n\n    client = storage.Client()\n    blob = client.list_blobs(\"images-original\", prefix=\"originals\")\n    cnt = -1  # root directory \uc81c\uc678\n    for b in blob:\n        cnt += 1\n\n\ndef retrain_op():\n    return dsl.ContainerOp(\n        name=\"retrain\",\n        image=\"us-west1-docker.pkg.dev/viai/retrain:v1.0\",\n        arguments=[\n            \"/opt/retrain.py\",\n            \"--data-dir\",\n            data_dir,\n            \"--torch-export-dir\",\n            model_export_dir,\n        ],\n    )\n\n\n@dsl.pipeline(name=\"retrain\", description=\"retrain demo\")\ndef retrain_pipeline(\n    name=\"retrain\",\n    training_image=\"gcr.io/aiffel-gn-3/retrain:latest\",\n    training_namespace=\"kubeflow\",\n    model_export_dir=\"gs://model-cpt/\",\n):\n    comp.func_to_container_op(check_cnt)\n    check_cnt = check_op()\n    with dsl.Condition(check_cnt.output > 100):\n        retrain = retrain_op()\n\n\nsteps = [retrain]\nfor step in steps:\n    if platform == \"GCP\":\n        step.apply(gcp.use_gcp_secret(\"user-gcp-sa\"))\nif __name__ == \"__main__\":\n    import kfp.compiler as compiler\n\n    compiler.Compiler().compile(retrain_pipeline, __file__ + \".tar.gz\")"}
{"prompt": "Generate a Kubeflow Pipeline named `Pipeline transformer` that performs the following:\n\nThis pipeline demonstrates how to apply a common transformation to all operations within the pipeline using a pipeline transformer.\n\nIt consists of **two** components:\n\n1.  **`Print`**: This component, instantiated twice as `op1` and `op2`, prints a specified message.\n    *   The first instance (`op1`) prints \"hey, what are you up to?\".\n    *   The second instance (`op2`) prints \"train my model.\".\n\n**Control Flow and Transformations**:\n\n*   Both `op1` and `op2` run independently.\n*   Before the pipeline execution, a pipeline-level transformer function named `add_annotation` is applied to all operations within the pipeline.\n*   The `add_annotation` function adds a pod annotation with the name `hobby` and value `football` to each operation it transforms.\n\n**Inputs/Outputs**:\n\n*   Each `Print` component takes a string message as input.\n*   There are no explicit outputs passed between components.\n\n**Libraries/Tools**:\n\n*   `kfp` (Kubeflow Pipelines SDK) for defining the pipeline and components.\n*   `alpine:3.6` Docker image is used for the `Print` component.", "completion": "#!/usr/bin/env python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport kfp\nfrom kfp import dsl\n\ndef print_op(msg):\n  \"\"\"Print a message.\"\"\"\n  return dsl.ContainerOp(\n      name='Print',\n      image='alpine:3.6',\n      command=['echo', msg],\n  )\n\ndef add_annotation(op):\n  op.add_pod_annotation(name='hobby', value='football')\n  return op\n\n@dsl.pipeline(\n    name='Pipeline transformer',\n    description='The pipeline shows how to apply functions to all ops in the pipeline by pipeline transformers'\n)\ndef transform_pipeline():\n  op1 = print_op('hey, what are you up to?')\n  op2 = print_op('train my model.')\n  dsl.get_pipeline_conf().add_op_transformer(add_annotation)\n\nif __name__ == '__main__':\n  kfp.compiler.Compiler().compile(transform_pipeline, __file__ + '.yaml')"}
{"prompt": "Generate a Kubeflow Pipeline named `stackdemos_kfappx_components_golang_component` that performs the following:\n\nThe pipeline consists of two primary utility functions designed to generate `ContainerOp` instances:\n\n1.  **`processing_op`**:\n    *   **Function**: This function serves as a template to create a `ContainerOp` for executing a given script within a specified container image. It encapsulates common parameters for containerized processing steps.\n    *   **Inputs**:\n        *   `script` (string): The path to the script to be executed within the container.\n        *   `image` (string, optional): The Docker image to use for the container. If not provided and running in an IPython environment, it attempts to retrieve the image from `IPython.get_ipython().user_ns.get('GOLANG_IMAGE')`. A `ValueError` is raised if `image` is not provided and cannot be inferred.\n        *   `arguments` (list of strings, optional): A list of command-line arguments to pass to the script. Defaults to an empty list.\n        *   `file_outputs` (dictionary, optional): A dictionary mapping output names to file paths that will be captured as component outputs. Defaults to an empty dictionary.\n    *   **Outputs**: Returns a `kfp.dsl.ContainerOp` instance.\n    *   **Logic**:\n        *   Dynamically sets the component name by cleaning the script name (converting to lowercase and replacing non-alphanumeric characters with hyphens).\n        *   Uses `script` as the primary command to execute.\n        *   Passes `arguments` and `file_outputs` directly to the `ContainerOp`.\n    *   **Dependencies/Tools**: Utilizes `kfp.dsl.ContainerOp`, `os`, `re`, and `IPython` (conditionally for `_is_ipython` helper function).\n\n2.  **`http_download_op`**:\n    *   **Function**: This function creates a `ContainerOp` to download a file from a given URL using `curl`, with an optional MD5 checksum pre-check to avoid re-downloading if the file already exists and matches the checksum.\n    *   **Inputs**:\n        *   `url` (string): The URL of the file to download.\n        *   `download_to` (string): The local path where the file should be downloaded.\n        *   `md5sum` (string): The expected MD5 checksum of the file. If the file at `download_to` already exists and its MD5 matches this `md5sum`, the download will be skipped.\n    *   **Outputs**: Returns a `kfp.dsl.ContainerOp` instance.\n    *   **Logic**:\n        *   Sets the component name to 'download-artifact'.\n        *   Uses the `appropriate/curl` Docker image.\n        *   Executes a shell script (`sh -c`) that first checks if the `md5sum` of the existing `download_to` file matches the provided `md5sum`. If they match, it prints a skipping message. Otherwise, it uses `curl` to download the file, creating directories as needed, and outputs verbose information.\n    *   **Dependencies/Tools**: Utilizes `kfp.dsl.ContainerOp` and `urllib.parse` (though `urlparse` is imported, it's not directly used in the provided function logic for this specific op), and relies on standard Unix tools like `test`, `awk`, and `curl` within the container image.\n\nThe pipeline itself is defined by these two reusable component functions, which would typically be called within a `@dsl.pipeline` decorated function to compose a complete workflow.", "completion": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef processing_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('GOLANG_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=[script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )"}
{"prompt": "Generate a Kubeflow Pipeline named `Katib Launcher` that performs hyperparameter tuning.\n\nThe pipeline consists of a single component:\n1.  **`StudyJob-Launcher`**: This component launches a Kubeflow Katib StudyJob.\n    *   **Inputs**:\n        *   `name` (string): The name for the Katib StudyJob.\n        *   `namespace` (string): The Kubernetes namespace where the StudyJob will be created.\n        *   `optimizationtype` (string): The type of optimization (e.g., `maximize`, `minimize`).\n        *   `objectivevaluename` (string): The name of the metric to optimize.\n        *   `optimizationgoal` (float): The target value for the objective metric.\n        *   `requestcount` (int): The number of trials to request.\n        *   `metricsnames` (string): Comma-separated list of metrics to collect.\n        *   `parameterconfigs` (string): JSON string representing the hyperparameter configuration.\n        *   `nasConfig` (string): JSON string representing Neural Architecture Search configuration (if applicable).\n        *   `workertemplatepath` (string): Path to the worker template YAML file.\n        *   `mcollectortemplatepath` (string): Path to the metrics collector template YAML file.\n        *   `suggestionspec` (string): JSON string representing the Katib Suggestion specification.\n        *   `studyjob_timeout_minutes` (int): Timeout for the StudyJob in minutes.\n        *   `delete` (boolean, default: `True`): Whether to delete the StudyJob after completion.\n        *   `output_file` (string, default: `/output.txt`): The path for the output file containing hyperparameter results.\n    *   **Outputs**:\n        *   `hyperparameter` (string): The content of the `output_file`, which typically contains the best hyperparameters found.\n    *   **Image**: `liuhougangxa/ml-pipeline-kubeflow-studyjob:latest`\n    *   **Purpose**: Orchestrates the execution of a Katib hyperparameter tuning experiment, managing its lifecycle and extracting the results.\n\nThe control flow is sequential, as there is only one component. The pipeline leverages the Kubeflow Katib tool for hyperparameter optimization.", "completion": "# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp.dsl as dsl\n\ndef kubeflow_studyjob_launcher_op(name, namespace, optimizationtype, objectivevaluename, optimizationgoal, requestcount, metricsnames,\n                                  parameterconfigs, nasConfig, workertemplatepath, mcollectortemplatepath, suggestionspec,\n                                  studyjob_timeout_minutes, delete=True, output_file='/output.txt', step_name='StudyJob-Launcher'):\n    return dsl.ContainerOp(\n        name = step_name,\n        image = 'liuhougangxa/ml-pipeline-kubeflow-studyjob:latest',\n        arguments = [\n            '--name', name,\n            '--namespace', namespace,\n            \"--optimizationtype\", optimizationtype,\n            \"--objectivevaluename\", objectivevaluename,\n            \"--optimizationgoal\", optimizationgoal,\n            \"--requestcount\", requestcount,\n            \"--metricsnames\", metricsnames,\n            \"--parameterconfigs\", parameterconfigs,\n            \"--nasConfig\", nasConfig,\n            \"--workertemplatepath\", workertemplatepath,\n            \"--mcollectortemplatepath\", mcollectortemplatepath,\n            \"--suggestionspec\", suggestionspec,\n            \"--outputfile\", output_file,\n            \"--deleteAfterDone\", delete,\n            '--studyjobtimeoutminutes', studyjob_timeout_minutes,\n        ],\n        file_outputs = {'hyperparameter': output_file}\n    )"}
{"prompt": "Generate a Kubeflow Pipeline named `Pipeline` that performs an end-to-end machine learning workflow including data preprocessing, model training, model testing, and model deployment.\n\nThe pipeline consists of four components:\n\n1.  **`Preprocess Data`**:\n    *   **Function**: Preprocesses raw data into training and testing sets.\n    *   **Inputs**: None.\n    *   **Outputs**:\n        *   `x_train` (file path: `/app/x_train.npy`)\n        *   `x_test` (file path: `/app/x_test.npy`)\n        *   `y_train` (file path: `/app/y_train.npy`)\n        *   `y_test` (file path: `/app/y_test.npy`)\n    *   **Image**: `tomerlev/pipeline_preprocessing:latest`\n\n2.  **`Train Model`**:\n    *   **Function**: Trains a machine learning model using the preprocessed training data.\n    *   **Inputs**:\n        *   `x_train` (from `Preprocess Data`)\n        *   `y_train` (from `Preprocess Data`)\n    *   **Outputs**:\n        *   `model` (file path: `/app/model.pkl`)\n    *   **Image**: `tomerlev/pipeline_train:latest`\n\n3.  **`Test Model`**:\n    *   **Function**: Evaluates the trained model using the preprocessed test data.\n    *   **Inputs**:\n        *   `x_test` (from `Preprocess Data`)\n        *   `y_test` (from `Preprocess Data`)\n        *   `model` (from `Train Model`)\n    *   **Outputs**:\n        *   `mean_squared_error` (file path: `/app/output.txt`)\n    *   **Image**: `tomerlev/pipeline_test:latest`\n\n4.  **`Deploy Model`**:\n    *   **Function**: Deploys the trained model.\n    *   **Inputs**:\n        *   `model` (from `Train Model`)\n    *   **Outputs**: None.\n    *   **Image**: `tomerlev/pipeline_deploy_model:latest`\n\n**Control Flow**:\n*   `Train Model` runs after `Preprocess Data` completes.\n*   `Test Model` runs after `Train Model` completes.\n*   `Deploy Model` runs after `Test Model` completes.\n\nThe pipeline should be compiled to `./pipeline.yaml` and then uploaded to a Kubeflow Pipelines instance with the name \"pipeline\".", "completion": "import kfp\nfrom kfp import dsl\nfrom kfp import compiler\n\ndef preprocess_op():\n\n    # It would be much cleaner to have our component accept the file paths as command-line arguments.\n    return dsl.ContainerOp(\n        name='Preprocess Data',\n        image='tomerlev/pipeline_preprocessing:latest',\n        arguments=[],\n        file_outputs={\n            'x_train': '/app/x_train.npy',\n            'x_test': '/app/x_test.npy',\n            'y_train': '/app/y_train.npy',\n            'y_test': '/app/y_test.npy',\n        }\n    )\n\ndef train_op(x_train, y_train):\n\n    return dsl.ContainerOp(\n        name='Train Model',\n        image='tomerlev/pipeline_train:latest',\n        arguments=[\n            '--x_train', x_train,\n            '--y_train', y_train\n        ],\n        file_outputs={\n            'model': '/app/model.pkl'\n        }\n    )\n\ndef test_op(x_test, y_test, model):\n\n    return dsl.ContainerOp(\n        name='Test Model',\n        image='tomerlev/pipeline_test:latest',\n        arguments=[\n            '--x_test', x_test,\n            '--y_test', y_test,\n            '--model', model\n        ],\n        file_outputs={\n            'mean_squared_error': '/app/output.txt'\n        }\n    )\n\ndef deploy_model_op(model):\n\n    return dsl.ContainerOp(\n        name='Deploy Model',\n        image='tomerlev/pipeline_deploy_model:latest',\n        arguments=[\n            '--model', model\n        ]\n    )\n\n@dsl.pipeline(\n   name='Pipeline',\n   description='An example pipeline that trains and logs a model.'\n)\ndef pipeline():\n    _preprocess_op = preprocess_op()\n\n    _train_op = train_op(\n        dsl.InputArgumentPath(_preprocess_op.outputs['x_train']),\n        dsl.InputArgumentPath(_preprocess_op.outputs['y_train'])\n    ).after(_preprocess_op)\n\n    _test_op = test_op(\n        dsl.InputArgumentPath(_preprocess_op.outputs['x_test']),\n        dsl.InputArgumentPath(_preprocess_op.outputs['y_test']),\n        dsl.InputArgumentPath(_train_op.outputs['model'])\n    ).after(_train_op)\n\n    deploy_model_op(\n        dsl.InputArgumentPath(_train_op.outputs['model'])\n    ).after(_test_op)\n\n\ncompiler.Compiler().compile(pipeline, './pipeline.yaml')\n\nclient = kfp.Client()\nclient.upload_pipeline('./pipeline.yaml', \"pipeline\")"}
{"prompt": "Generate a Kubeflow Pipeline named `KCDemoPipeline` that performs the following steps:\n\nThe pipeline consists of three components:\n1.  **`download-artifact`**: This component is a `ContainerOp` that downloads a file from a specified URL to a local path.\n    *   **Function**: Downloads an artifact using `curl` and performs an MD5 checksum pre-check to avoid re-downloading if the file already exists and matches the checksum.\n    *   **Inputs**:\n        *   `url`: The URL of the file to download.\n        *   `download_to`: The local path where the file should be saved.\n        *   `md5sum`: The expected MD5 checksum of the downloaded file for validation.\n    *   **Tool/Library**: Uses the `appropriate/curl` Docker image and `sh` for execution.\n\n2.  **`_is_ipython`**: This is a helper function that checks if the code is running within an IPython environment (e.g., Jupyter Notebook).\n    *   **Function**: Returns `True` if `IPython` is imported and available, `False` otherwise.\n    *   **Note**: This is a utility function used internally by `training_op` and is not a standalone pipeline component.\n\n3.  **`training_op`**: This component is a `ContainerOp` designed for running a Python training script.\n    *   **Function**: Executes a Python script within a specified Docker image, passing arguments and defining file outputs. It dynamically determines the image if running in an IPython environment and `TRAINING_IMAGE` is set.\n    *   **Inputs**:\n        *   `script`: The path to the Python script to execute.\n        *   `image`: The Docker image to use for the container. If not provided, it attempts to get it from `IPython.get_ipython().user_ns.get('TRAINING_IMAGE')` if in an IPython environment.\n        *   `arguments`: A list of command-line arguments to pass to the script.\n        *   `file_outputs`: A dictionary mapping output names to file paths that should be captured as component outputs.\n    *   **Naming Convention**: The component name is derived from the `script` parameter by converting it to lowercase and replacing non-alphanumeric characters with hyphens.\n    *   **Dependencies**: Internally uses the `_is_ipython` helper function.\n\n**Control Flow**:\nThe components (`download-artifact` and `training_op`) are independent functions that can be called to create `ContainerOp` instances. The `_is_ipython` function is a helper used internally by `training_op`. There are no explicit dependencies or sequential/parallel execution defined within this prompt, as the provided code only defines the *components themselves*, not a pipeline orchestrating them. A future pipeline would orchestrate these components using `.after()` or by passing outputs as inputs.", "completion": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef training_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('TRAINING_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=['/usr/local/bin/python', script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )"}
{"prompt": "Generate a Kubeflow Pipeline named `simple-training-pipeline` that performs an image classification training process.\n\nThe pipeline comprises three components:\n1.  **`training`**: This component is responsible for the core machine learning training. It takes the following inputs: `pipeline_name`, `bucket_name`, `job_id`, `global_batch_size`, `epochs`, `learning_rate`, `dataset`, `model_type`, `image_size`, and `num_classes`. It should be named \"training\" in the pipeline graph. This component utilizes GCP's preemptible nodepool and a TPU with 8 v3 cores running TensorFlow 2.8.0. It should be configured to retry up to 2 times upon failure.\n2.  **`tb-observer`**: This component is used to observe and visualize training logs, likely generating a TensorBoard instance. It takes `pipeline_name`, `bucket_name`, `job_id`, and `tblog_dir` as inputs. It should be named \"tboard\" in the pipeline graph. This component also utilizes GCP's preemptible nodepool.\n3.  **`slack-notification`**: This component sends notifications to Slack. It takes `pipeline_name`, `job_id`, and `message` as inputs. The message should dynamically include the workflow status (`\"Status: {{workflow.status}}\"`).\n\n**Control Flow:**\nThe pipeline uses a `dsl.ExitHandler` to ensure that the `slack-notification` component is always executed at the end of the pipeline, regardless of whether the other steps succeed or fail.\nInside the `ExitHandler`, the `training` component runs first. Upon its completion (success or failure), the `slack-notification` component is triggered as the exit handler.\nThe `tb-observer` component runs in parallel with or after the `training` component, as there is no explicit `after` dependency specified, but it logically depends on training logs being generated.\n\n**Inputs and Outputs:**\nThe pipeline takes the following parameters:\n*   `pipeline_name` (str, default: \"simple-training-pipeline\")\n*   `bucket_name` (str, default: \"kfp-project\")\n*   `job_id` (str, default: \"{{JOB_ID}}\")\n*   `model_type` (str, default: \"resnet\")\n*   `global_batch_size` (int, default: 1024)\n*   `epochs` (int, default: 30)\n*   `lr` (float, default: 0.001)\n*   `image_size` (int, default: 64)\n*   `num_classes` (int, default: 100)\n*   `dataset` (str, default: \"gs://kfp-project/datasets/mnist\")\n\nThe components are assumed to be loaded from a `kfp.components.ComponentStore` with `local_search_paths=[\"pipelines/simple-training-pipeline\", \"components\"]`.\n\n**Tools and Libraries:**\nThe pipeline heavily leverages Kubeflow Pipelines (KFP) for orchestration and Google Cloud Platform (GCP) services (specifically preemptible nodepools and TPUs) for execution. It also implicitly uses TensorFlow (version 2.8.0) within the TPU environment for the training component.", "completion": "import kfp\nfrom kfp import dsl\nfrom kfp import gcp\n\n# Initialize component store\ncomponent_store = kfp.components.ComponentStore(\n    local_search_paths=[\"pipelines/simple-training-pipeline\", \"components\"])\n\n# Create component factories\ntrain_op = component_store.load_component(\"training\")\ntensorboard_op = component_store.load_component(\"tb-observer\")\nslack_notification_op = component_store.load_component(\"slack-notification\")\n\n\n# Define pipeline\n@dsl.pipeline(\n    name=\"simple-training-pipeline\",\n    description=\"training pipeline for image classification\"\n)\ndef pipeline(\n    pipeline_name: str = \"simple-training-pipeline\",\n    bucket_name: str = \"kfp-project\",\n    job_id: str = \"{{JOB_ID}}\",\n    model_type: str = \"resnet\",\n    global_batch_size: int = 1024,\n    epochs: int = 30,\n    lr: float = 0.001,\n    image_size: int = 64,\n    num_classes: int = 100,\n    dataset: str = \"gs://kfp-project/datasets/mnist\",\n):\n    with dsl.ExitHandler(\n        exit_op=slack_notification_op(\n            pipeline_name=pipeline_name,\n            job_id=job_id,\n            message=\"Status: {{workflow.status}}\"\n        )\n    ):\n        train_op(\n            pipeline_name=pipeline_name,\n            bucket_name=bucket_name,\n            job_id=job_id,\n            global_batch_size=global_batch_size,\n            epochs=epochs,\n            learning_rate=lr,\n            dataset=dataset,\n            model_type=model_type,\n            image_size=image_size,\n            num_classes=num_classes,\n        ).set_display_name(\"training\")\\\n            .apply(gcp.use_preemptible_nodepool())\\\n            .apply(gcp.use_tpu(\n                tpu_cores=8,\n                tpu_resource=\"v3\",\n                tf_version=\"2.8.0\"))\\\n            .set_retry(num_retries=2)\n\n        tensorboard_op(\n            pipeline_name=pipeline_name,\n            bucket_name=bucket_name,\n            job_id=job_id,\n            tblog_dir=\"training/logs\"\n        ).set_display_name(\"tboard\")\\\n            .apply(gcp.use_preemptible_nodepool())\n\n\nif __name__ == \"__main__\":\n    kfp.compiler.Compiler().compile(pipeline, \"simple-training-pipeline.yaml\")"}
{"prompt": "Generate a Kubeflow Pipeline named `resnet_cifar10_pipeline` that performs an end-to-end training and serving workflow using ResNet and CIFAR-10.\n\nThe pipeline consists of four components: `PreprocessOp`, `TrainOp`, `InferenceServerLauncherOp`, and `WebappLauncherOp`.\n\n1.  **`PreprocessOp`**:\n    *   **Function**: Preprocesses raw data.\n    *   **Inputs**: `name` (string), `input_dir` (string), `output_dir` (string).\n    *   **Outputs**: A file output named `output` from `/output.txt`.\n    *   **Image**: `zdou001/only_tests:preprocess-image`.\n    *   **Arguments**: `--input_dir` mapped to the `input_dir` parameter and `--output_dir` mapped to the `output_dir` parameter.\n\n2.  **`TrainOp`**:\n    *   **Function**: Trains a ResNet model.\n    *   **Inputs**: `name` (string), `input_dir` (string), `output_dir` (string), `model_name` (string), `model_version` (integer), `epochs` (integer).\n    *   **Outputs**: A file output named `output` from `/output.txt`.\n    *   **Image**: `zdou001/only_tests:train-image`.\n    *   **Arguments**: `--input_dir`, `--output_dir`, `--model_name`, `--model_version`, and `--epochs` mapped to their respective parameters.\n\n3.  **`InferenceServerLauncherOp`**:\n    *   **Function**: Launches an inference server (Triton Inference Server).\n    *   **Inputs**: `name` (string), `input_dir` (string), `trtserver_name` (string).\n    *   **Outputs**: A file output named `output` from `/output.txt`.\n    *   **Image**: `zdou001/only_tests:inference-server-launcher-image`.\n    *   **Arguments**: `--trtserver_name` and `--model_path` (mapped to `input_dir`) mapped to their respective parameters.\n\n4.  **`WebappLauncherOp`**:\n    *   **Function**: Launches a web application for inference.\n    *   **Inputs**: `name` (string), `trtserver_name` (string), `model_name` (string), `model_version` (integer), `webapp_prefix` (string), `webapp_port` (integer).\n    *   **Outputs**: No explicit file outputs.\n    *   **Image**: `zdou001/only_tests:webapp-launcher-image`.\n    *   **Arguments**: `--workflow_name` (using Kubeflow's built-in `{{workflow.name}}`), `--trtserver_name`, `--model_name`, `--model_version`, `--webapp_prefix`, and `--webapp_port` mapped to their respective parameters.\n\nThe pipeline definition `resnet_pipeline` takes the following parameters:\n*   `raw_data_dir` (default: `/mnt/workspace/raw_data`)\n*   `processed_data_dir` (default: `/mnt/workspace/processed_data`)\n*   `model_dir` (default: `/mnt/workspace/saved_model`)\n*   `epochs` (default: `50`)\n*   `trtserver_name` (default: `trtis`)\n*   `model_name` (default: `resnet_graphdef`)\n*   `model_version` (default: `1`)\n*   `webapp_prefix` (default: `webapp`)\n*   `webapp_port` (default: `80`)\n\nThe pipeline should define a persistent volume named `nvidia-workspace` and mount it to `/mnt/workspace` for all components.\n\n**Control Flow:**\n1.  The `PreprocessOp` named `preprocess` should run first, taking `raw_data_dir` as `input_dir` and `processed_data_dir` as `output_dir`.\n2.  The `TrainOp` named `train` should run after `preprocess` completes. It takes the output of `preprocess` as its `input_dir`, `model_dir` as `output_dir`, and the pipeline parameters `model_name`, `model_version`, and `epochs`.\n3.  The `InferenceServerLauncherOp` named `trtserver` should run after `train` completes. It takes the output of `train` as its `input_dir` and the pipeline parameter `trtserver_name`.\n4.  The `WebappLauncherOp` named `webapp` should run after `trtserver` completes. It takes the pipeline parameters `trtserver_name`, `model_name`, `model_version`, `webapp_prefix`, and `webapp_port`.\n\nEach component must have its image specified as shown above. The `datetime` and `os` libraries are imported but not explicitly used in the provided pipeline structure; `kubernetes` client (aliased as `k8s_client`) is also imported, indicating potential use for volume-related configurations. Ensure the definition of the persistent volume and its application to each task is correct.", "completion": "# Copyright (c) 2019, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport kfp.dsl as dsl\nimport datetime\nimport os\nfrom kubernetes import client as k8s_client\n\n\n# Modify image='<image>' in each op to match IMAGE in the build.sh of its corresponding component\n\ndef PreprocessOp(name, input_dir, output_dir):\n    return dsl.ContainerOp(\n        name=name,\n        image='zdou001/only_tests:preprocess-image',\n        arguments=[\n            '--input_dir', input_dir,\n            '--output_dir', output_dir,\n        ],\n        file_outputs={'output': '/output.txt'}\n    )\n\n\ndef TrainOp(name, input_dir, output_dir, model_name, model_version, epochs):\n    return dsl.ContainerOp(\n        name=name,\n        image='zdou001/only_tests:train-image',\n        arguments=[\n            '--input_dir', input_dir,\n            '--output_dir', output_dir,\n            '--model_name', model_name,\n            '--model_version', model_version,\n            '--epochs', epochs\n        ],\n        file_outputs={'output': '/output.txt'}\n    )\n\n\ndef InferenceServerLauncherOp(name, input_dir, trtserver_name):\n    return dsl.ContainerOp(\n        name=name,\n        image='zdou001/only_tests:inference-server-launcher-image',\n        arguments=[\n            '--trtserver_name', trtserver_name,\n            '--model_path', input_dir,\n        ],\n        file_outputs={'output': '/output.txt'}\n    )\n\n\ndef WebappLauncherOp(name, trtserver_name, model_name, model_version, webapp_prefix, webapp_port):\n    return dsl.ContainerOp(\n        name=name,\n        image='zdou001/only_tests:webapp-launcher-image',\n        arguments=[\n            '--workflow_name', '{{workflow.name}}',\n            '--trtserver_name', trtserver_name,\n            '--model_name', model_name,\n            '--model_version', str(model_version),\n            '--webapp_prefix', webapp_prefix,\n            '--webapp_port', str(webapp_port)\n        ],\n        file_outputs={}\n    )\n\n\n@dsl.pipeline(\n    name='resnet_cifar10_pipeline',\n    description='Demonstrate an end-to-end training & serving pipeline using ResNet and CIFAR-10'\n)\ndef resnet_pipeline(\n    raw_data_dir='/mnt/workspace/raw_data',\n    processed_data_dir='/mnt/workspace/processed_data',\n    model_dir='/mnt/workspace/saved_model',\n    epochs=50,\n    trtserver_name='trtis',\n    model_name='resnet_graphdef',\n    model_version=1,\n    webapp_prefix='webapp',\n    webapp_port=80\n):\n\n    persistent_volume_name = 'nvidia-workspace'\n    persistent_volume_path = '/mnt/workspace'\n\n    op_dict = {}\n\n    op_dict['preprocess'] = PreprocessOp(\n        'preprocess', raw_data_dir, processed_data_dir)\n\n    op_dict['train'] = TrainOp(\n        'train', op_dict['preprocess'].output, model_dir, model_name, model_version, epochs)\n\n    op_dict['deploy_inference_server'] = InferenceServerLauncherOp(\n        'deploy_inference_server', op_dict['train'].output, trtserver_name)\n\n    op_dict['deploy_webapp'] = WebappLauncherOp(\n        'deploy_webapp', op_dict['deploy_inference_server'].output, model_name, model_version, webapp_prefix, webapp_port)\n\n    for _, container_op in op_dict.items():\n        container_op.add_volume(k8s_client.V1Volume(\n            host_path=k8s_client.V1HostPathVolumeSource(\n                path=persistent_volume_path),\n            name=persistent_volume_name))\n        container_op.add_volume_mount(k8s_client.V1VolumeMount(\n            mount_path=persistent_volume_path,\n            name=persistent_volume_name))\n\n\nif __name__ == '__main__':\n    import kfp.compiler as compiler\n    compiler.Compiler().compile(resnet_pipeline, __file__ + '.tar.gz')"}
{"prompt": "Generate a Kubeflow Pipeline named `metrics-visualization-v1-pipeline` that performs a series of visualization tasks.\n\nThe pipeline consists of five components, all running in parallel:\n\n1.  **`confusion_visualization`**: This component, imported from `../core/visualization/confusion_matrix`, is responsible for generating a confusion matrix visualization. It does not take any explicit inputs visible in the pipeline definition.\n\n2.  **`html_visualization`**: This component, imported from `../core/visualization/html`, generates an HTML visualization. It takes one string input, initialized with an empty string `\"\"`.\n\n3.  **`markdown_visualization`**: This component, imported from `../core/visualization/markdown`, generates a Markdown visualization. It does not take any explicit inputs visible in the pipeline definition.\n\n4.  **`roc_visualization`**: This component, imported from `../core/visualization/roc`, generates a Receiver Operating Characteristic (ROC) curve visualization. It does not take any explicit inputs visible in the pipeline definition.\n\n5.  **`table_visualization`**: This component, imported from `../core/visualization/table`, generates a table visualization. It does not take any explicit inputs visible in the pipeline definition.\n\nThere are no explicit dependencies or control flow mechanisms like `after` or `parallelFor` defined between these components; they are all instantiated directly within the pipeline function, implying they can run in parallel.\n\nThe pipeline primarily utilizes internal `kfp.deprecated.dsl` features and custom visualization components imported from relative paths (`../core/visualization/`). No external libraries like `sklearn` or external services like `Snowflake` are explicitly used within the pipeline definition itself.", "completion": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp.deprecated.dsl as dsl\nfrom ..core.visualization.confusion_matrix import confusion_visualization\nfrom ..core.visualization.html import html_visualization\nfrom ..core.visualization.markdown import markdown_visualization\nfrom ..core.visualization.roc import roc_visualization\nfrom ..core.visualization.table import table_visualization\n\n# Note: This test is to verify that visualization metrics on V1 is runnable by KFP.\n# However, this pipeline is only runnable on V1 mode, but not V2 compatible mode.\n\n@dsl.pipeline(\n    name='metrics-visualization-v1-pipeline')\ndef metrics_visualization_v1_pipeline():\n    confusion_visualization_task = confusion_visualization()\n    html_visualization_task = html_visualization(\"\")\n    markdown_visualization_task = markdown_visualization()\n    roc_visualization_task = roc_visualization()\n    table_visualization_task = table_visualization()"}
{"prompt": "Generate a Kubeflow Pipeline named `GolangComponent` that performs the following steps:\n\nThis pipeline consists of two reusable utility components: `processing_op` and `http_download_op`. These components are designed to be flexible and are not directly chained within a single, fixed pipeline definition in the provided code. Instead, they serve as building blocks that can be used independently or combined in various ways to construct pipelines.\n\nHere's a breakdown of each component:\n\n**Component 1: `processing_op`**\n*   **Function:** This component is a generic template for running a containerized script. It encapsulates the common logic for creating `ContainerOp` instances in Kubeflow.\n*   **Inputs:**\n    *   `script` (str): The path to the script to be executed within the container.\n    *   `image` (str, optional): The Docker image to use for the container. If not provided and running in an IPython environment, it attempts to fetch `GOLANG_IMAGE` from the IPython user namespace.\n    *   `arguments` (list, optional): A list of arguments to pass to the script. Defaults to an empty list.\n    *   `file_outputs` (dict, optional): A dictionary defining file outputs for the component. Defaults to an empty dictionary.\n*   **Outputs:** None explicitly defined as file outputs, but the component returns a `ContainerOp` object which can have outputs configured via `file_outputs`.\n*   **Tools/Libraries:** `os`, `re`, `IPython` (conditionally for `_is_ipython` check).\n*   **Control Flow:** It performs a check to determine if it's running in an IPython environment to dynamically set the `image` if not provided. It raises a `ValueError` if `image` is still missing.\n\n**Component 2: `http_download_op`**\n*   **Function:** This component is designed to download a file from a specified URL using `curl` and includes an MD5 checksum pre-check to avoid re-downloading if the file already exists and matches the checksum.\n*   **Inputs:**\n    *   `url` (str): The URL of the file to download.\n    *   `download_to` (str): The local path where the file should be saved.\n    *   `md5sum` (str): The expected MD5 checksum of the file.\n*   **Outputs:** None explicitly defined as file outputs. The operation's primary \"output\" is the downloaded file on the container's filesystem.\n*   **Tools/Libraries:** `appropriate/curl` Docker image is used, executing `sh -c` commands involving `curl` and `awk`.\n*   **Control Flow:** The `command` arguments include a shell script that first checks if the `md5sum` of the existing file at `download_to` matches the provided `md5sum`. If they match, it prints a message and skips the download; otherwise, it proceeds with the `curl` download.\n\n**Overall Pipeline Structure:**\n\nThe provided Python file defines **reusable Kubeflow `ContainerOp` factory functions** rather than a single, complete `@dsl.pipeline` decorated function. To generate a pipeline that *uses* these, one would typically define a separate `@dsl.pipeline` that calls these functions to construct its steps. For example:\n\n```python\nfrom kfp import dsl\n\n@dsl.pipeline(name='ExamplePipelineUsingUtilities')\ndef example_pipeline(\n    download_url: str = 'http://example.com/data.zip',\n    target_path: str = '/data/downloaded.zip',\n    expected_md5: str = 'abcdef1234567890abcdef1234567890',\n    processing_script: str = 'my_processing_script.sh',\n    processing_image: str = 'my_custom_image'\n):\n    # Step 1: Download data using http_download_op\n    download_task = http_download_op(\n        url=download_url,\n        download_to=target_path,\n        md5sum=expected_md5\n    )\n\n    # Step 2: Process the downloaded data using processing_op\n    # This step implicitly depends on the download_task completing\n    process_task = processing_op(\n        script=processing_script,\n        image=processing_image,\n        arguments=[f'--input={download_task.output}'] # Example of passing an output if download_op had one\n    ).after(download_task) # Explicit dependency\n\n    # Note: The original http_download_op doesn't have a direct .output for the file path,\n    # so a more robust approach might involve volume sharing or passing the path directly.\n    # The example above is illustrative of how dependencies would work.\n```", "completion": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef processing_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('GOLANG_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=[script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )"}
{"prompt": "Generate a Kubeflow Pipeline named `ModelEvaluationPipeline` that performs the following:\n\nThe pipeline consists of a single component:\n1.  **`choose_best_model`**: This component takes two trained models (a Decision Tree and a Random Forest) and a test dataset as input. It evaluates both models on the test dataset, logs their respective accuracy scores, and then selects and saves the model with the higher accuracy as the `best_model`.\n\n**Inputs/Outputs:**\n*   **`choose_best_model` component:**\n    *   **Inputs:**\n        *   `test_dataset`: An `Input[Dataset]` containing the data for evaluation.\n        *   `decision_tree_model`: An `Input[Model]` representing the trained Decision Tree model.\n        *   `random_forest_model`: An `Input[Model]` representing the trained Random Forest model.\n    *   **Outputs:**\n        *   `metrics`: An `Output[Metrics]` object to log the accuracy scores of both models.\n        *   `best_model`: An `Output[Model]` representing the selected best performing model.\n\n**Control Flow:**\n*   The pipeline executes the `choose_best_model` component directly. There are no dependencies or advanced control flow mechanisms like `parallelFor` or `after` specified, as it's a single-component pipeline.\n\n**Tools/Libraries Used:**\n*   The `choose_best_model` component uses `pandas` for data handling, `joblib` for loading and saving models, and `sklearn.metrics.accuracy_score` for model evaluation.\n*   The base image for the component is `gcr.io/deeplearning-platform-release/tf2-cpu.2-6:latest`.\n*   The component explicitly installs `pandas==1.3.5` and `joblib==1.1.0`.", "completion": "from kfp.dsl import Dataset, Input, Metrics, Model, Output, component\n\n\n@component(\n    base_image=\"gcr.io/deeplearning-platform-release/tf2-cpu.2-6:latest\",\n    packages_to_install=[\n        \"pandas==1.3.5\",\n        \"joblib==1.1.0\",\n    ],\n)\ndef choose_best_model(\n    test_dataset: Input[Dataset],\n    decision_tree_model: Input[Model],\n    random_forest_model: Input[Model],\n    metrics: Output[Metrics],\n    best_model: Output[Model],\n):\n    import joblib\n    import pandas as pd\n    from sklearn.metrics import accuracy_score\n\n    test_data = pd.read_csv(test_dataset.path)\n\n    dt = joblib.load(decision_tree_model.path)\n    rf = joblib.load(random_forest_model.path)\n\n    dt_pred = dt.predict(test_data.drop(\"Species\", axis=1))\n    rf_pred = rf.predict(test_data.drop(\"Species\", axis=1))\n\n    dt_accuracy = accuracy_score(test_data[\"Species\"], dt_pred)\n    rf_accuracy = accuracy_score(test_data[\"Species\"], rf_pred)\n\n    metrics.log_metric(\"Decision Tree (Accuracy)\", (dt_accuracy))\n    metrics.log_metric(\"Random Forest (Accuracy)\", (rf_accuracy))\n\n    if dt_accuracy > rf_accuracy:\n        joblib.dump(dt, best_model.path)\n    else:\n        joblib.dump(rf, best_model.path)"}
{"prompt": "Generate a Kubeflow Pipeline named `GolangComponentTest` that performs the following steps:\n\nThe pipeline consists of 3 components:\n1.  **`download-golang-compiler`**: This component downloads a Go compiler.\n    *   It uses the `http_download_op` function.\n    *   **Inputs**:\n        *   `url`: A string representing the URL to download the Go compiler from.\n        *   `download_to`: A string specifying the path where the compiler should be downloaded.\n        *   `md5sum`: A string representing the MD5 checksum for integrity verification.\n    *   **Function**: Downloads the Go compiler executable from the provided URL to the specified path and verifies its integrity using the MD5 checksum.\n    *   **Tool/Library**: `curl` for downloading.\n\n2.  **`download-helloworld-source`**: This component downloads a \"Hello World\" Go source file.\n    *   It uses the `http_download_op` function.\n    *   **Inputs**:\n        *   `url`: A string representing the URL to download the Go source file from.\n        *   `download_to`: A string specifying the path where the source file should be downloaded.\n        *   `md5sum`: A string representing the MD5 checksum for integrity verification.\n    *   **Function**: Downloads the \"Hello World\" Go source file from the provided URL to the specified path and verifies its integrity using the MD5 checksum.\n    *   **Tool/Library**: `curl` for downloading.\n\n3.  **`golang-component`**: This component compiles and runs the \"Hello World\" Go program.\n    *   It uses the `processing_op` function.\n    *   **Inputs**:\n        *   `script`: A string representing the path to the Go program to be executed (`/usr/local/go/bin/go run /app/hello.go`).\n        *   `image`: A string specifying the Docker image to use for this component (e.g., `golang:1.16`).\n    *   **Function**: Executes the Go program using the Go compiler downloaded in the first step.\n    *   **Dependencies**: This component depends on both `download-golang-compiler` and `download-helloworld-source` completing successfully.\n\n**Control Flow**:\n*   `download-golang-compiler` and `download-helloworld-source` run in parallel.\n*   `golang-component` runs after both `download-golang-compiler` and `download-helloworld-source` have completed.", "completion": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef processing_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('GOLANG_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=[script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )"}
{"prompt": "Generate a Kubeflow Pipeline named `Fybrik housing price estimate pipeline` that performs the following steps to provide data policy governed access to cataloged data, analyze it, train a model, and write/catalog the results.\n\nThe pipeline takes two input parameters:\n- `test_dataset_id` (string)\n- `train_dataset_id` (string)\n\nIt consists of four main components and utilizes a helper function to determine the current Kubernetes namespace.\n\n**Components Breakdown:**\n\n1.  **`getDataEndpoints`**:\n    *   **Function:** Retrieves data endpoints for the training and testing datasets. It seems to handle the initial data access and potentially sets up where results will be stored and cataloged.\n    *   **Inputs:** `train_dataset_id`, `test_dataset_id`, `namespace` (determined dynamically by `get_current_namespace`), `run_name` (derived from `dsl.RUN_ID_PLACEHOLDER`), and `result_name` (derived from `run_name`).\n    *   **Outputs:** `train_endpoint`, `test_endpoint`, and `result_catalogid`.\n    *   **Component Definition:** Loaded from `../../get_data_endpoints/component.yaml`.\n\n2.  **`visualizeTable`**:\n    *   **Function:** Visualizes the training data.\n    *   **Inputs:** `train_endpoint` (obtained from `getDataEndpointsStep.outputs['train_endpoint']`), `train_dataset_id`, and `namespace`.\n    *   **Dependencies:** Runs *after* `getDataEndpointsStep`.\n    *   **Component Definition:** Loaded from `./visualize_table/component.yaml`.\n\n3.  **`trainModel`**:\n    *   **Function:** Trains a machine learning model using the provided training and testing data endpoints. It also specifies where to output the results.\n    *   **Inputs:** `train_endpoint_path` (obtained from `getDataEndpointsStep.outputs['train_endpoint']`), `test_endpoint_path` (obtained from `getDataEndpointsStep.outputs['test_endpoint']`), `result_name`, `result_endpoint_path` (obtained from `getDataEndpointsStep.outputs['result_endpoint']`), `train_dataset_id`, `test_dataset_id`, and `namespace`.\n    *   **Dependencies:** Runs *after* `visualizeTableStep`.\n    *   **Component Definition:** Loaded from `./train_model/component.yaml`.\n\n4.  **`submitResult`**:\n    *   **Function:** Submits and catalogs the results of the model training.\n    *   **Inputs:** `result_catalogid` (obtained from `getDataEndpointsStep.outputs['result_catalogid']`).\n    *   **Dependencies:** Runs *after* `trainModelStep`.\n    *   **Component Definition:** Loaded from `./submit_result/component.yaml`.\n\n**Control Flow:**\nThe pipeline executes sequentially: `getDataEndpoints` -> `visualizeTable` -> `trainModel` -> `submitResult`, with each step explicitly waiting for the completion of the previous one using `.after()` dependencies.\n\n**Helper Function:**\nA local Python function `get_current_namespace()` is used to dynamically determine the Kubernetes namespace where the pipeline is running. This function uses the `kubernetes` library and checks `kubeflow` specific paths. The result of this function is passed as an argument to the `getDataEndpoints` component.\n\n**Internal Variables:**\n- `namespace`: Stores the current Kubernetes namespace.\n- `run_name`: A string derived from `dsl.RUN_ID_PLACEHOLDER` and prefixed with \"run-\", ensuring it's lowercase and starts with a letter.\n- `result_name`: A string derived from `run_name` and prefixed with \"submission-\".", "completion": "import kfp.dsl as dsl\nimport kfp.components as components\n\n@dsl.pipeline(\n    name = \"Fybrik housing price estimate pipeline\",\n    description = \"Pipeline that provides data policy governed access to cataloged data, analyses data, trains model, and writes the results and catalogs them\"\n)\n\ndef get_current_namespace():\n    import kubernetes\n    import os\n\n    ns_path = \"/var/run/secrets/kubernetes.io/serviceaccount/namespace\"\n    if os.path.exists(ns_path):\n        with open(ns_path) as f:\n            return f.read().strip()\n    try:\n        _, active_context = kubernetes.config.list_kube_config_contexts()\n        return active_context[\"context\"][\"namespace\"]\n    except KeyError:\n        return \"kubeflow\"\n\ndef houseprice_pipeline(\n    test_dataset_id: str,\n    train_dataset_id: str\n):\n    # Get the namespace in which kfp is running\n    namespace = get_current_namespace()\n\n    # Get the ID of the run.  Make sure it's lower case and starts with a letter for our use\n    run_name = \"run-\" + dsl.RUN_ID_PLACEHOLDER.lower()\n\n    # Where to store parameters passed between workflow steps\n    result_name = \"submission-\" + str(run_name)\n\n    getDataEndpointsOp = components.load_component_from_file('../../get_data_endpoints/component.yaml')\n    getDataEndpointsStep = getDataEndpointsOp(train_dataset_id=train_dataset_id, test_dataset_id=test_dataset_id, namespace=namespace, run_name=run_name, result_name=result_name)\n    \n    visualizeTableOp = components.load_component_from_file('./visualize_table/component.yaml')\n    visualizeTableStep = visualizeTableOp(train_endpoint='%s'% getDataEndpointsStep.outputs['train_endpoint'], train_dataset_id=train_dataset_id, namespace=namespace)\n    visualizeTableStep.after(getDataEndpointsStep)\n\n    trainModelOp = components.load_component_from_file('./train_model/component.yaml')\n    trainModelStep = trainModelOp(train_endpoint_path='%s' % getDataEndpointsStep.outputs['train_endpoint'],\n                                test_endpoint_path='%s' % getDataEndpointsStep.outputs['test_endpoint'],\n                                result_name=result_name,\n                                result_endpoint_path='%s' % getDataEndpointsStep.outputs['result_endpoint'],\n                                train_dataset_id=train_dataset_id,\n                                test_dataset_id=test_dataset_id,\n                                namespace=namespace)\n    trainModelStep.after(visualizeTableStep)\n\n    sumbitResultOp = components.load_component_from_file('./submit_result/component.yaml')\n    submitResultStep = sumbitResultOp(getDataEndpointsStep.outputs['result_catalogid'])\n    submitResultStep.after(trainModelStep)\n \nif __name__ == '__main__':\n\n    # Set environment values to ensure persistent volumes used to pass parameters are allocated successfully \n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--test_dataset_id', type=str)\n    parser.add_argument('--train_dataset_id', type=str)\n \n    args = parser.parse_args()\n    import kfp.compiler as compiler\n    compiler.Compiler().compile(pipeline_func=houseprice_pipeline, package_path=__file__.replace('.py', '.yaml'))"}
{"prompt": "Generate a Kubeflow Pipeline named `TrainEvalPipeline` that performs a machine learning model training, evaluation, and selection workflow.\n\nThe pipeline consists of **three components**:\n\n1.  **`train_eval_baseline_model`**: This component is a containerized step responsible for training and evaluating various machine learning models (e.g., baseline, logistic regression, gradient boosting, decision tree, including resampled versions).\n    *   **Inputs**:\n        *   `model_name` (str): The name of the model to train and evaluate.\n        *   `script` (str): The Python script to execute within the container.\n        *   `ip` (str): IP address for data storage/retrieval.\n        *   `port` (str): Port for data storage/retrieval.\n        *   `bucket_name` (str): The name of the S3-compatible bucket where data/models are stored.\n        *   `object_name` (str): The name of the object (e.g., file path) within the bucket.\n    *   **Function**: It executes a Python script within a `myaseende/my-scikit:latest` Docker image to perform model training and evaluation. It's expected to output evaluation metrics that will be consumed by subsequent steps.\n    *   **Tools/Libraries**: Uses a custom Docker image `myaseende/my-scikit:latest`.\n\n2.  **`show_best_model_info`**: This component retrieves and processes evaluation metrics for a specific model from an S3-compatible storage.\n    *   **Inputs**:\n        *   `model_name` (str): The name of the model whose information is to be retrieved.\n        *   `ip` (str): IP address for MinIO client connection.\n        *   `port` (str): Port for MinIO client connection.\n        *   `bucket_name` (str): The name of the S3-compatible bucket.\n        *   `object_name` (str): The object name (e.g., file path) within the bucket where metrics are stored.\n    *   **Output**: A `float` representing the evaluation metric for the specified model.\n    *   **Function**: Connects to a MinIO (S3-compatible) server, downloads a specified object (expected to contain metrics), and extracts a single float metric. It handles potential `S3Error` during MinIO operations.\n    *   **Tools/Libraries**: `pandas`, `minio==7.1.14`.\n\n3.  **`find_best_model_on_full_data`**: This component analyzes the evaluation metrics from various models to determine the best-performing one.\n    *   **Inputs**:\n        *   `baseline_metric` (float): Evaluation metric for the baseline model.\n        *   `lr_metric` (float): Evaluation metric for the Logistic Regression model.\n        *   `lr_resampled_metric` (float): Evaluation metric for the resampled Logistic Regression model.\n        *   `gbt_metric` (float): Evaluation metric for the Gradient Boosting Trees model.\n        *   `gbt_resampled_metric` (float): Evaluation metric for the resampled Gradient Boosting Trees model.\n        *   `dtree_metric` (float): Evaluation metric for the Decision Tree model.\n        *   `dtree_resampled_metric` (float): Evaluation metric for the resampled Decision Tree model.\n    *   **Output**: `experiment_summary` (Output[Markdown]): A Markdown-formatted summary detailing all model metrics and identifying the best model.\n    *   **Function**: Compares the provided metrics for seven different models, identifies the model with the highest metric, and generates a markdown table summarizing all results, including the best model.\n    *   **Tools/Libraries**: `pandas`.\n\nThe **control flow** of the pipeline is as follows:\n\n*   The `train_eval_baseline_model` component will be executed multiple times using a `parallelFor` loop. Each iteration will train and evaluate a specific model type. The loop will iterate over a list of dictionaries, where each dictionary defines the `name`, `script`, and `object_name` for a particular model to be trained. The `ip`, `port`, and `bucket_name` will be constant across all training tasks.\n*   For each model trained in the `parallelFor` loop, an instance of the `show_best_model_info` component will be executed **after** its corresponding `train_eval_baseline_model` task completes. This `show_best_model_info` component will retrieve the evaluation metric for that specific model.\n*   Once all individual `show_best_model_info` tasks have completed and their respective metrics are available, the `find_best_model_on_full_data` component will be executed. It will receive the outputs (metrics) from all instances of `show_best_model_info` as its inputs.\n\nThe pipeline accepts the following **parameters**:\n*   `ip_addr` (str): IP address for MinIO/data access, with a default value of \"172.17.0.1\".\n*   `port_num` (str): Port number for MinIO/data access, with a default value of \"9000\".\n*   `bucket_name` (str): The name of the S3 bucket, with a default value of \"kubeflow-pipelines\".\n*   `script_path` (str): The path to the training script, with a default value of \"src/main.py\".\n\nThe pipeline will use the `@dsl.pipeline` and `@component` decorators from `kfp.dsl`.", "completion": "import json\nfrom typing import Dict\nimport kfp\nfrom kfp.dsl import Output, Dataset, Input, Markdown\n\n\n@kfp.dsl.container_component\ndef train_eval_baseline_model(\n    model_name: str,\n    script: str,\n    ip: str,\n    port: str,\n    bucket_name: str,\n    object_name: str\n):\n    \"\"\"\n    This component loads our main training container image and runs the code to train the models\n    \"\"\"\n\n    return kfp.dsl.ContainerSpec(\n        image='myaseende/my-scikit:latest', \n        command=['python'], \n        args=[\n            script,\n            model_name,\n            ip,\n            port,\n            bucket_name,\n            object_name\n        ]\n    )\n\n@kfp.dsl.component(packages_to_install=['pandas'])\ndef find_best_model_on_full_data(\n    baseline_metric: float,\n    lr_metric: float,\n    lr_resampled_metric: float,\n    gbt_metric: float,\n    gbt_resampled_metric: float,\n    dtree_metric: float,\n    dtree_resampled_metric: float,\n    experiment_summary: Output[Markdown]\n):\n    \"\"\"\n    Given the evaluation scores, find out which model did the best\n    \"\"\"\n\n    import logging\n    import pandas as pd\n\n    logger = logging.getLogger('kfp_logger')\n    logger.setLevel(logging.INFO)\n    \n    # TODO: there should be a better way to do this\n    model_names = [\n        'baseline',\n        'lr',\n        'lr_resampled',\n        'gbt',\n        'gbt_resampled',\n        'dtree',\n        'dtree_resampled'\n    ]\n\n    model_metrics = [\n        baseline_metric,\n        lr_metric,\n        lr_resampled_metric,\n        gbt_metric,\n        gbt_resampled_metric,\n        dtree_metric,\n        dtree_resampled_metric\n    ]\n    \n    models_and_metrics = dict(\n        zip(\n            model_names,\n            model_metrics\n        )\n    )\n\n    best_model_name = max(models_and_metrics, key=lambda key: models_and_metrics[key])\n    best_model_metric = models_and_metrics[best_model_name]\n\n    # write experiments summary as a markdown table\n\n    logger.info('Writing experiment summary')\n    \n    with open(experiment_summary.path, 'w') as f:\n        \n        experiment_summary_df = pd.DataFrame({\n            # We replace the underscore so that Mardown is rendered properly\n            \"Models\": list(map(lambda name: name.replace(\"_\", \" \"), model_names)),\n            \"Metrics\": model_metrics,\n        })\n\n        summaries_table_md = experiment_summary_df.to_markdown()\n        summaries_table_md += \"\\n\\n\"\n        summaries_table_md += f\"The best model is **{best_model_name}** with evaluation metric value: **{best_model_metric}.**\"\n        \n        f.write(summaries_table_md)\n    \n\n@kfp.dsl.component(packages_to_install=['pandas', 'minio==7.1.14'])\ndef show_best_model_info(\n    model_name: str,\n    ip: str,\n    port: str,\n    bucket_name: str, \n    object_name: str,\n) -> float:\n    \"\"\"\n    Retrieve the saved training metrics\n    \"\"\"\n\n    import logging\n    import io\n\n    from minio import Minio\n    from minio.error import S3Error\n    import pandas as pd\n\n    try:\n\n        logger = logging.getLogger('kfp_logger')\n        logger.setLevel(logging.INFO)\n\n        # Create client with access and secret key\n        client = Minio(\n            f'{ip}:{port}',\n            'minio',\n            'minio123',\n            secure=False\n        )\n\n        logger.info(f'Attempting to read data from MinIO.')\n\n        # read the ranks and hparams info df from MinIO\n        response = client.get_object(bucket_name, object_name)\n\n        pandas_df = pd.read_csv(io.BytesIO(response.data))\n\n        logger.info('Columns: ' + ', '.join(pandas_df.columns))  \n        \n        metric = pandas_df.loc[0, 'mean_test_balanced_accuracy'].item()\n        logger.info(f\"Metric (mean_test_balanced_accuracy) on {model_name}: {metric}\")\n\n        return metric\n    \n    except Exception as err:\n    \n        logger.error(f'Error occurred: {err}.')\n    \n    # output the main metric (this can be used for comparison)\n    return 0.0\n\n@kfp.dsl.pipeline(\n   name='base-train-eval-pipeline',\n   description='Pipeline that will train a baseline model and run eval'\n)\ndef model_train_eval_pipeline(\n    ip: str,\n    port: str,\n    bucket_name: str,\n    object_name: str\n):\n\n    # Note: the `.set_caching_options(False)` call is used so that we don't reuse results from previous runs.\n    # This was required during developmenet because sometimes I change the container and push it to DockerHub, but \n    # Kubeflow wasn't pulling the new container and was instead using old results.\n\n    ############################\n    # Baseline model\n    ############################\n\n    # This will train, eval, and save the model as well as hparams\n    base_line_train_eval_task = train_eval_baseline_model(\n        model_name='baseline',\n        script='baseline_model.py',\n        ip=ip,\n        port=port,\n        bucket_name=bucket_name,\n        object_name=object_name\n    )\n\n    base_line_train_eval_task.set_caching_options(False)\n\n    base_line_metric = show_best_model_info(\n        model_name='baseline',\n        ip=ip,\n        port=port,\n        bucket_name=bucket_name,\n        object_name=\"baseline_model_ranks.pandas_df\"\n    ).after(base_line_train_eval_task)\n\n    base_line_metric.set_caching_options(False)\n\n\n    ############################\n    # Full models\n    ############################\n    \n    # 1. Logistic regression + with resampling\n    lr_train_eval_task = train_eval_baseline_model(\n        model_name='logistic_regression',\n        script='full_models.py',\n        ip=ip,\n        port=port,\n        bucket_name=bucket_name,\n        object_name=object_name\n    )\n    lr_train_eval_task.set_caching_options(False)\n\n    lr_metric = show_best_model_info(\n        model_name='logistic_regression',\n        ip=ip,\n        port=port,\n        bucket_name=bucket_name,\n        object_name=\"logistic_regression_ranks.pandas_df\"\n    ).after(lr_train_eval_task)\n\n    lr_resampled_metric = show_best_model_info(\n        model_name='logistic_regression_resampled',\n        ip=ip,\n        port=port,\n        bucket_name=bucket_name,\n        object_name=\"logistic_regression_ranks_resampled.pandas_df\"\n    ).after(lr_train_eval_task)\n\n    lr_metric.set_caching_options(False)\n    lr_resampled_metric.set_caching_options(False)\n\n    # 2. Gradient Boosted Trees regression + with resampling\n    gbt_train_eval_task = train_eval_baseline_model(\n        model_name='gbt',\n        script='full_models.py',\n        ip=ip,\n        port=port,\n        bucket_name=bucket_name,\n        object_name=object_name\n    )\n    gbt_train_eval_task.set_caching_options(False)\n\n    gbt_metric = show_best_model_info(\n        model_name='gbt',\n        ip=ip,\n        port=port,\n        bucket_name=bucket_name,\n        object_name=\"gbt_ranks.pandas_df\"\n    ).after(gbt_train_eval_task)\n    \n    gbt_resampeld_metric = show_best_model_info(\n        model_name='gbt_resampled',\n        ip=ip,\n        port=port,\n        bucket_name=bucket_name,\n        object_name=\"gbt_ranks_resampled.pandas_df\"\n    ).after(gbt_train_eval_task)\n\n    gbt_metric.set_caching_options(False)\n    gbt_resampeld_metric.set_caching_options(False)\n\n    # Decision Tree regression + with resampling\n    dtree_train_eval_task = train_eval_baseline_model(\n        model_name='decision_tree',\n        script='full_models.py',\n        ip=ip,\n        port=port,\n        bucket_name=bucket_name,\n        object_name=object_name\n    )\n    dtree_train_eval_task.set_caching_options(False)\n\n    dtree_metric = show_best_model_info(\n        model_name='decision_tree',\n        ip=ip,\n        port=port,\n        bucket_name=bucket_name,\n        object_name=\"decision_tree_ranks.pandas_df\"\n    ).after(dtree_train_eval_task)\n\n    dtree_resampled_metric = show_best_model_info(\n        model_name='decision_tree_resampled',\n        ip=ip,\n        port=port,\n        bucket_name=bucket_name,\n        object_name=\"decision_tree_ranks_resampled.pandas_df\"\n    ).after(dtree_train_eval_task)\n\n    dtree_metric.set_caching_options(False)\n    dtree_resampled_metric.set_caching_options(False)\n\n    # ############################\n    # # Compare models\n    # ############################\n    # TODO: For some reason, collecting the results into a dictionary like this doesn't seem to work\n\n    # model_and_metric_dict={\n    #     \"baseline\":         base_line_metric.output,\n    #     \"lr\":               lr_metric.output,\n    #     \"lr_resampled\":     lr_resampled_metric.output,\n    #     \"gbt\":              gbt_metric.output,\n    #     \"gbt_resampled\":    gbt_resampeld_metric.output,\n    #     \"dtree\":            dtree_metric.output,\n    #     \"dtree_resampled\":  dtree_resampled_metric.output\n    # }\n    \n    # TODO: There has to be a better way of doing this, but for now we go with this (honestly) ugly solution\n    find_best_model_on_full_data(\n        baseline_metric=base_line_metric.output,\n        lr_metric=lr_metric.output,\n        lr_resampled_metric=lr_resampled_metric.output,\n        gbt_metric=gbt_metric.output,\n        gbt_resampled_metric=gbt_resampeld_metric.output,\n        dtree_metric=dtree_metric.output,\n        dtree_resampled_metric=dtree_resampled_metric.output\n    )\n\n\nif __name__ == \"__main__\":\n\n    kfp.compiler.Compiler().compile(\n        pipeline_func=model_train_eval_pipeline,\n        package_path=\"model_train_eval_pipeline.yaml\",\n        pipeline_parameters={\n            'bucket_name': 'car-tier-prediction-data',\n            'object_name': 'data-with-features-cleaned.csv',\n            'port': '9000'\n        }\n    )"}
{"prompt": "Generate a Kubeflow Pipeline named `test-run-local-pipeline` that performs the following steps:\n\nThis pipeline consists of a single component.\n\n**Components:**\n\n1.  **`hello`**: This component takes a string input named `name` and returns a greeting string in the format \"hello {name}\".\n\n**Control Flow:**\n\nThe pipeline starts by executing the `hello` component, passing the `name` parameter received by the pipeline as its input.\n\n**Inputs:**\n\nThe pipeline accepts one input parameter:\n*   `name`: A string that will be used by the `hello` component.\n\n**Tools/Libraries:**\n\nThe pipeline utilizes standard Python functionalities and the `kfp.v2.dsl` module for Kubeflow pipeline definitions.", "completion": "import unittest\nfrom typing import Union\n\nfrom kfp.v2 import dsl\n\nfrom kfp_local import LocalClient\n\n\n@dsl.component\ndef hello(name: str) -> str:\n    return f\"hello {name}\"\n\n\n@dsl.component\ndef local_loader(src: str, dst: dsl.OutputPath(str)):\n    import os\n    import shutil\n\n    if os.path.exists(src):\n        shutil.copyfile(src, dst)\n\n\n@dsl.component\ndef flip_coin() -> str:\n    import random\n\n    return \"head\" if random.randint(0, 1) == 0 else \"tail\"\n\n\n@dsl.component\ndef component_with_inputpath(src: dsl.InputPath()) -> str:\n    with open(src, \"r\") as f:\n        return f.read()\n\n\n@dsl.component\ndef component_return_artifact(content: str) -> dsl.Artifact:\n    return content\n\n\n@dsl.component\ndef component_consume_artifact(artifact: dsl.Input[dsl.Artifact]) -> str:\n    with open(artifact.path, \"r\") as f:\n        return f.read()\n\n\nclass LocalRunnerTest(unittest.TestCase):\n    def setUp(self):\n        self.local_client = LocalClient()\n        import tempfile\n\n        with tempfile.NamedTemporaryFile(\"w\", delete=False) as f:\n            self.temp_file_path = f.name\n            f.write(\"hello world\")\n\n    def get_param_output(self, result, task_name: str, output_name: str = None) -> str:\n        with open(result.get_output_file(task_name, output_name), \"r\") as f:\n            return f.read()\n\n    def get_param_return(self, result, task_name: str, output_name: str = None):\n        with open(result.get_output_file(task_name, \"output_metadata.json\"), \"r\") as f:\n            from google.protobuf import json_format\n            from kfp.pipeline_spec import pipeline_spec_pb2\n\n            metadata = pipeline_spec_pb2.ExecutorOutput()\n            json_format.Parse(f.read(), metadata)\n            output = metadata.parameters[output_name or \"Output\"]\n            value_type = output.WhichOneof(\"value\")\n            if value_type == \"string_value\":\n                output_value = output.string_value\n            elif value_type == \"int_value\":\n                output_value = output.int_value\n            elif value_type == \"double_value\":\n                output_value = output.double_value\n            else:\n                output_value = None\n            return output_value\n\n    def assert_func_param_return_equal(\n        self,\n        content: Union[str, int, float, bool, dict, list],\n        result,\n        task_name: str,\n        output_name: str = None,\n    ):\n        output_value = self.get_param_return(result, task_name, output_name)\n\n        if type(output_value) == str and type(content) != str:\n            import json\n\n            content = json.dumps(content)\n\n        self.assertEqual(content, output_value)\n\n    def test_run_local(self):\n        @dsl.pipeline(name=\"test-run-local-pipeline\")\n        def _pipeline(name: str):\n            hello(name)\n\n        result = self.local_client.create_run_from_pipeline_func(\n            _pipeline,\n            {\"name\": \"world\"},\n            execution_mode=LocalClient.ExecutionMode(\"local\"),\n        )\n        self.assertTrue(result.success)\n        self.assert_func_param_return_equal(\"hello world\", result, \"hello\")\n\n    def test_local_file(self):\n        @dsl.pipeline(name=\"test-local-file-pipeline\")\n        def _pipeline(file_path: str):\n            local_loader(file_path)\n\n        run_result = self.local_client.create_run_from_pipeline_func(\n            _pipeline,\n            {\"file_path\": self.temp_file_path},\n            execution_mode=LocalClient.ExecutionMode(\"local\"),\n        )\n        self.assertEqual(\n            \"hello world\", self.get_param_output(run_result, \"local-loader\", \"dst\")\n        )\n\n    def test_condition(self):\n        @dsl.pipeline(name=\"test-condition\")\n        def _pipeline():\n            _flip = flip_coin()\n            with dsl.Condition(_flip.output == \"head\"):\n                hello(\"head\")\n\n            with dsl.Condition(_flip.output == \"tail\"):\n                hello(\"tail\")\n\n        run_result = self.local_client.create_run_from_pipeline_func(\n            _pipeline, {}, execution_mode=LocalClient.ExecutionMode(\"local\")\n        )\n        self.assertTrue(run_result.success)\n        coin_result = self.get_param_return(run_result, \"flip-coin\")\n        if coin_result == \"head\":\n            self.assert_func_param_return_equal(\"hello head\", run_result, \"hello\")\n        else:\n            self.assert_func_param_return_equal(\"hello tail\", run_result, \"hello-2\")\n\n    def test_loop(self):\n        @dsl.pipeline(name=\"for-pipeline\")\n        def _pipeline():\n            with dsl.ParallelFor([\"hello\", \"world\"]) as item:\n                hello(item)\n\n        run_result = self.local_client.create_run_from_pipeline_func(\n            _pipeline, {}, execution_mode=LocalClient.ExecutionMode(\"local\")\n        )\n        self.assertTrue(run_result.success)\n        self.assertEqual(\n            \"hello world\", self.get_param_return(result=run_result, task_name=\"hello\")\n        )\n\n    def test_nest_loop(self):\n        @dsl.pipeline(name=\"test-nest-loop\")\n        def _pipeline():\n            loop_parameter = [[1, 2], [3, 4]]\n            with dsl.ParallelFor(loop_parameter) as item:\n                with dsl.ParallelFor(item) as item_a:\n                    hello(item)\n                    hello(item_a)\n\n        run_result = self.local_client.create_run_from_pipeline_func(\n            _pipeline, {}, execution_mode=LocalClient.ExecutionMode(\"local\")\n        )\n        self.assertTrue(run_result.success)\n        self.assertEqual(\"hello 4\", self.get_param_return(run_result, \"hello-2\"))\n\n    def test_exit_handler(self):\n        @dsl.pipeline(\"test-exit-handler\")\n        def _pipeline():\n            with dsl.ExitHandler(exit_op=hello(\"exit\")):\n                hello(\"inner\")\n\n        run_result = self.local_client.create_run_from_pipeline_func(\n            _pipeline, {}, execution_mode=LocalClient.ExecutionMode(\"local\")\n        )\n\n        self.assertTrue(run_result.success)\n        self.assertEqual(\"hello exit\", self.get_param_return(run_result, \"hello\"))\n        self.assertEqual(\"hello inner\", self.get_param_return(run_result, \"hello-2\"))\n\n    @unittest.skip(\"docker is not installed in CI environment.\")\n    def test_connect_artifact(self):\n        @dsl.pipeline(name=\"test-connect-artifact\")\n        def _pipeline():\n            input_component = component_return_artifact(\"hello world\")\n            component_with_inputpath(input_component.output)\n\n        run_result = self.local_client.create_run_from_pipeline_func(\n            _pipeline,\n            {},\n            execution_mode=LocalClient.ExecutionMode(\"docker\"),\n        )\n        self.assertTrue(run_result.success)\n        self.assertEqual(\n            \"hello world\",\n            self.get_param_return(run_result, \"component-with-inputpath\"),\n        )\n\n    @unittest.skip(\"docker is not installed in CI environment.\")\n    def test_output_artifact(self):\n        @dsl.pipeline(name=\"test-output-artifact\")\n        def _pipeline():\n            component1 = component_return_artifact(\"artifact content\")\n            component_consume_artifact(component1.output)\n\n        run_result = self.local_client.create_run_from_pipeline_func(\n            _pipeline, {}, execution_mode=LocalClient.ExecutionMode(\"docker\")\n        )\n        self.assertTrue(run_result.success)\n        self.assertEqual(\n            \"artifact content\",\n            self.get_param_return(run_result, \"component-consume-artifact\"),\n        )\n\n    @unittest.skip(\"docker is not installed in CI environment.\")\n    def test_execution_mode_exclude_op(self):\n        @dsl.component(base_image=\"image_not_exist\")\n        def cat_on_image_not_exist(name: str) -> str:\n            return name\n\n        @dsl.pipeline(name=\"test-execution-mode-exclude-op\")\n        def _pipeline():\n            cat_on_image_not_exist(\"exclude tasks\")\n\n        run_result = self.local_client.create_run_from_pipeline_func(\n            _pipeline,\n            {},\n            execution_mode=LocalClient.ExecutionMode(mode=\"docker\"),\n        )\n        self.assertFalse(run_result.success)\n\n        run_result = self.local_client.create_run_from_pipeline_func(\n            _pipeline,\n            {},\n            execution_mode=LocalClient.ExecutionMode(\n                mode=\"docker\", tasks_to_exclude=[\"cat-on-image-not-exist\"]\n            ),\n        )\n        self.assertTrue(run_result.success)\n        self.assertEqual(\n            \"exclude tasks\",\n            self.get_param_return(run_result, \"cat-on-image-not-exist\"),\n        )\n\n    @unittest.skip(\"docker is not installed in CI environment.\")\n    def test_docker_options(self):\n        @dsl.component\n        def check_option(env_name: str) -> str:\n            import os\n\n            return os.environ[env_name]\n\n        @dsl.pipeline(name=\"test-docker-options\")\n        def _pipeline():\n            check_option(\"foo\")\n\n        run_result = self.local_client.create_run_from_pipeline_func(\n            _pipeline,\n            {},\n            execution_mode=LocalClient.ExecutionMode(\n                mode=\"docker\", docker_options=[\"-e\", \"foo=bar\"]\n            ),\n        )\n        assert run_result.success\n        self.assertEqual(\"bar\", self.get_param_return(run_result, \"check-option\"))"}
{"prompt": "Generate a Kubeflow Pipeline named `my-pipeline` that performs the following steps:\n\nThis pipeline accepts one input parameter:\n- `loopidy_doop`: A list of dictionaries, defaulting to `[{'a': 1, 'b': 2}, {'a': 10, 'b': 20}]`. This parameter is used for parallel iteration.\n\nThe pipeline consists of three components:\n\n1.  **`my-out-cop0`**:\n    *   **Function**: Generates a JSON list of integers from 20 to 30 (inclusive) and writes it to a file.\n    *   **Image**: `python:alpine3.6`\n    *   **Output**: `out` (JSON list of integers)\n\n2.  **`my-in-cop1`**:\n    *   **Function**: Prints a string including a value from the current item in the parallel loop.\n    *   **Image**: `library/bash:4.4.23`\n    *   **Dependency**: This component runs *after* `my-out-cop0` has completed.\n    *   **Control Flow**: This component is executed within a `ParallelFor` loop that iterates over the `loopidy_doop` pipeline parameter. For each item in `loopidy_doop`, a new instance of `my-in-cop1` is run, accessing `item.a` from the current loop item.\n\n3.  **`my-out-cop2`**:\n    *   **Function**: Prints a string including the output generated by `my-out-cop0`.\n    *   **Image**: `library/bash:4.4.23`\n    *   **Input**: Consumes the `out` output from `my-out-cop0`.\n\nThe pipeline should be compiled to a YAML file.", "completion": "# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp\nfrom kfp import dsl\n\n\n@dsl.pipeline(name='my-pipeline')\ndef pipeline(loopidy_doop=[{'a': 1, 'b': 2}, {'a': 10, 'b': 20}]):\n    op0 = dsl.ContainerOp(\n        name=\"my-out-cop0\",\n        image='python:alpine3.6',\n        command=[\"sh\", \"-c\"],\n        arguments=[\n            'python -c \"import json; import sys; json.dump([i for i in range(20, 31)], open(\\'/tmp/out.json\\', \\'w\\'))\"'],\n        file_outputs={'out': '/tmp/out.json'},\n    )\n\n    with dsl.ParallelFor(loopidy_doop) as item:\n        op1 = dsl.ContainerOp(\n            name=\"my-in-cop1\",\n            image=\"library/bash:4.4.23\",\n            command=[\"sh\", \"-c\"],\n            arguments=[\"echo no output global op1, item.a: %s\" % item.a],\n        ).after(op0)\n\n    op_out = dsl.ContainerOp(\n        name=\"my-out-cop2\",\n        image=\"library/bash:4.4.23\",\n        command=[\"sh\", \"-c\"],\n        arguments=[\"echo no output global op2, outp: %s\" % op0.output],\n    )\n\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(pipeline, __file__ + '.yaml')"}
{"prompt": "Generate a Kubeflow Pipeline named `stackdemos_kf-flatcar2_components_training_component` that performs the following steps:\n\nThe pipeline should define two reusable container operation templates: `training_op` and `http_download_op`.\n\nThe `training_op` component:\n- **Functionality**: Serves as a template for various training-related container operations. It executes a Python script within a specified Docker image.\n- **Inputs**:\n    - `script` (str): The path to the Python script to be executed.\n    - `image` (str, optional): The Docker image to use for the container. If not provided, it attempts to retrieve `TRAINING_IMAGE` from the IPython user namespace if running in a notebook.\n    - `arguments` (list, optional): A list of arguments to pass to the Python script.\n    - `file_outputs` (dict, optional): A dictionary defining the file outputs of the component.\n- **Output**: A `kfp.dsl.ContainerOp` instance.\n- **Logic**:\n    - If `image` is not provided and running in an IPython environment, it attempts to retrieve `TRAINING_IMAGE` from `IPython.get_ipython().user_ns`.\n    - If `image` is still not provided, it raises a `ValueError`.\n    - The `ContainerOp`'s `name` is derived from the `script` parameter by converting it to lowercase, removing non-alphanumeric characters, and splitting the extension.\n    - The `command` is `['/usr/local/bin/python', script]`.\n    - The `arguments` and `file_outputs` are passed directly to the `ContainerOp`.\n- **Tools/Libraries**: Uses `os` and `re` for name manipulation. Checks for `IPython` for notebook environment detection.\n\nThe `http_download_op` component:\n- **Functionality**: Downloads a file from a given URL and performs an MD5 checksum pre-check to avoid re-downloading if the file already exists and matches the checksum.\n- **Inputs**:\n    - `url` (str): The URL of the file to download.\n    - `download_to` (str): The local path where the file should be saved.\n    - `md5sum` (str): The expected MD5 checksum of the file.\n- **Output**: A `kfp.dsl.ContainerOp` instance.\n- **Logic**:\n    - The `ContainerOp`'s `name` is fixed as 'download-artifact'.\n    - The `image` is `appropriate/curl`.\n    - The `command` is `['sh', '-c']`.\n    - The `arguments` contain a shell script that first checks if the `md5sum` of the existing file at `download_to` matches the provided `md5sum`. If it matches, it prints a message skipping the download. Otherwise, it uses `curl -#Lv --create-dirs -o {download_to} {url}` to download the file.\n- **Tools/Libraries**: Uses `curl` for downloading and `awk` for MD5 checksum extraction.\n\nNo explicit pipeline structure (e.g., dependencies, parallel execution) is defined, as these are reusable component definitions. The pipeline itself acts as a collection of these definitions.", "completion": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef training_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('TRAINING_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=['/usr/local/bin/python', script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )"}
{"prompt": "Generate a Kubeflow Pipeline named `two-step-with-uri-placeholders` that performs a two-step data transfer operation using Google Cloud Storage (GCS).\n\nThe pipeline consists of **two components**:\n\n1.  **`write-to-gcs`**:\n    *   **Function**: This component writes a given string message to a GCS file.\n    *   **Inputs**:\n        *   `msg` (String): The content to be written.\n    *   **Outputs**:\n        *   `artifact` (Artifact): The GCS file path where the message was written.\n    *   **Implementation**: It uses the `google/cloud-sdk:slim` Docker image and executes a shell command to `echo` the `msg` into a GCS location specified by the output URI placeholder.\n\n2.  **`read-from-gcs`**:\n    *   **Function**: This component reads content from a specified GCS file and prints it to standard output.\n    *   **Inputs**:\n        *   `artifact` (Artifact): The GCS file path to read from.\n    *   **Outputs**: None explicitly defined, but the content is printed.\n    *   **Implementation**: It uses the `google/cloud-sdk:slim` Docker image and executes a shell command to `gsutil cat` the content from the GCS location specified by the input URI placeholder.\n\n**Control Flow and Dependencies**:\n*   The `write-to-gcs` component runs first, taking the `msg` pipeline parameter as its input.\n*   The `read-from-gcs` component runs **after** `write-to-gcs` and is directly dependent on its output.\n*   The `artifact` output (GCS file path) from `write-to-gcs` is passed as the `artifact` input to `read-from-gcs`.\n\n**Pipeline Parameters**:\n*   The pipeline accepts a single string parameter named `msg`, with a default value of `'Hello world!'`.\n\n**Tools/Libraries**:\n*   The pipeline heavily leverages `gsutil` commands from the `google/cloud-sdk:slim` Docker image for GCS operations.", "completion": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Two step v2-compatible pipeline with URI placeholders.\"\"\"\nfrom kfp import components, dsl\n\nwrite_to_gcs_op = components.load_component_from_text(\"\"\"\nname: write-to-gcs\ninputs:\n- {name: msg, type: String, description: 'Content to be written to GCS'}\noutputs:\n- {name: artifact, type: Artifact, description: 'GCS file path'}\nimplementation:\n  container:\n    image: google/cloud-sdk:slim\n    command:\n    - sh\n    - -c\n    - |\n      set -e -x\n      echo \"$0\" | gsutil cp - \"$1\"\n    - {inputValue: msg}\n    - {outputUri: artifact}\n\"\"\")\n\nread_from_gcs_op = components.load_component_from_text(\"\"\"\nname: read-from-gcs\ninputs:\n- {name: artifact, type: Artifact, description: 'GCS file path'}\nimplementation:\n  container:\n    image: google/cloud-sdk:slim\n    command:\n    - sh\n    - -c\n    - |\n      set -e -x\n      gsutil cat \"$0\"\n    - {inputUri: artifact}\n\"\"\")\n\n\n@dsl.pipeline(name='two-step-with-uri-placeholders')\ndef two_step_with_uri_placeholder(msg: str = 'Hello world!'):\n    write_to_gcs = write_to_gcs_op(msg=msg)\n    read_from_gcs = read_from_gcs_op(artifact=write_to_gcs.outputs['artifact'])"}
{"prompt": "Generate a Kubeflow Pipeline named `Pipeline` that performs a machine learning workflow for training with a Reddit response dataset.\n\nThe pipeline has five components:\n1.  **`tokenize`**: This component is a `ContainerOp` that tokenizes the input data.\n    *   **Image**: Uses a custom image from ECR (`<ID>.dkr.ecr.eu-central-1.amazonaws.com/<registry>/tokenize:1.0.0`).\n    *   **Command**: `python -m src.steps.tokenize.pipeline_step`.\n    *   **Outputs**: Produces two file outputs: `tokenize_location` (path to tokenized data) and `labels_location` (path to labels data).\n    *   **Security**: Applies AWS secrets for access key and secret key.\n    *   **Pull Policy**: Sets image pull policy to `Always`.\n\n2.  **`vectorize`**: This component is a `ContainerOp` that performs TF-IDF vectorization.\n    *   **Image**: Uses the same custom image as `tokenize` (`<ID>.dkr.ecr.eu-central-1.amazonaws.com/<registry>/tokenize:1.0.0`).\n    *   **Command**: `python -m src.steps.tfidftransformer.pipeline_step`.\n    *   **Inputs**: Takes `tokenize_location` as input from the `tokenize` component.\n    *   **Outputs**: Produces two file outputs: `tfidftransformer_location` (path to vectorizer model) and `tfidfvectors_location` (path to vectorized data).\n    *   **Security**: Applies AWS secrets for access key and secret key.\n    *   **Pull Policy**: Sets image pull policy to `Always`.\n\n3.  **`logical regression`**: This component is a `ContainerOp` that trains a Logistic Regression classifier.\n    *   **Image**: Uses the same custom image as `tokenize` and `vectorize` (`<ID>.dkr.ecr.eu-central-1.amazonaws.com/<registry>/tokenize:1.0.0`).\n    *   **Command**: `python -m src.steps.lrclassifier.pipeline_step`.\n    *   **Inputs**: Takes `labels_location` from the `tokenize` component and `tfidfvectors_location` from the `vectorize` component.\n    *   **Outputs**: Produces one file output: `lr_model_location` (path to the trained LR model).\n    *   **Security**: Applies AWS secrets for access key and secret key.\n    *   **Pull Policy**: Sets image pull policy to `Always`.\n\n4.  **`Build tokenize Serving`**: This component is a `ContainerOp` responsible for building a serving image for the tokenizer.\n    *   **Image**: Uses a Kaniko executor image from ECR (`<ID>.dkr.ecr.eu-central-1.amazonaws.com/<registry>/kaniko-executor:1.0.0`).\n    *   **Arguments**:\n        *   `--dockerfile=Dockerfile`\n        *   `--build-arg=TOKENIZE_MODEL=<tokenize_location output from tokenize component>`\n        *   `--context=dir:///workspace`\n        *   `--destination=<ID>.dkr.ecr.eu-central-1.amazonaws.com/<registry>/tokenizeserving:1.0.0`\n    *   **Security**: Applies AWS secrets for access key and secret key.\n    *   **Pull Policy**: Sets image pull policy to `Always`.\n\n5.  **`Build vectorize Serving`**: This component is a `ContainerOp` responsible for building a serving image for the vectorizer.\n    *   **Image**: Uses the same Kaniko executor image as `Build tokenize Serving` (`<ID>.dkr.ecr.eu-central-1.amazonaws.com/<registry>/kaniko-executor:1.0.0`).\n    *   **Arguments**:\n        *   `--dockerfile=Dockerfile`\n        *   `--build-arg=VECTORIZE_MODEL=<tfidftransformer_location output from vectorize component>`\n        *   `--context=dir:///workspace`\n        *   `--destination=<ID>.dkr.ecr.eu-central-1.amazonaws.com/<registry>/vectorizeserving:1.0.0`\n    *   **Security**: Applies AWS secrets for access key and secret key.\n    *   **Pull Policy**: Sets image pull policy to `Always`.\n\n**Control Flow and Dependencies**:\n*   `vectorize_training_step` depends on `tokenize_training_step` as it consumes its output.\n*   `lr_training_step` depends on both `tokenize_training_step` and `vectorize_training_step` as it consumes their outputs.\n*   `tokenize_build_step` runs `after` `lr_training_step`.\n*   `vectorize_build_step` runs `after` `lr_training_step`.\n\n**Pipeline Input Parameters**:\n*   `input_data`: Default value `reddit_train.csv`.\n\n**General Pipeline Configuration**:\n*   The pipeline description is 'A pipeline for training with Reddit response dataset'.\n*   All components apply an AWS secret named 'aws-secret' for 'AWS_ACCESS_KEY_ID' and 'AWS_SECRET_ACCESS_KEY'.\n*   All components set the image pull policy to 'Always'.\n*   The pipeline leverages custom Docker images stored in an ECR registry (`<ID>.dkr.ecr.eu-central-1.amazonaws.com/<registry>`) with a consistent tag (`1.0.0`).\n*   The build steps use `kaniko-executor` for building Docker images.", "completion": "import kfp.dsl as dsl\nimport kfp.aws as aws\nimport logging\nimport kfp.compiler as compiler\n\nlogging.basicConfig(level=logging.INFO)\n\nTAG = \"1.0.0\"\nREGISTRY = \"<ID>.dkr.ecr.eu-central-1.amazonaws.com/<registry>\"\n\n\n@dsl.pipeline(\n    name='Pipeline',\n    description='A pipeline for training with Reddit response dataset'\n)\ndef kf_pipeline(\n        input_data='reddit_train.csv',\n):\n    \"\"\"\n    Pipeline\n    \"\"\"\n\n    tokenize_training_step = dsl.ContainerOp(\n        name='tokenize',\n        image=f\"{REGISTRY}/tokenize:{TAG}\",\n        command=\"python\",\n        arguments=[\n            \"-m\",\n            \"src.steps.tokenize.pipeline_step\"\n        ],\n        file_outputs={\"tokenize_location\": \"/tokenized_location.txt\",\n                      \"labels_location\": \"/labels_location.txt\"},\n        pvolumes={}\n    ).apply(aws.use_aws_secret('aws-secret', 'AWS_ACCESS_KEY_ID', 'AWS_SECRET_ACCESS_KEY')) \\\n        .set_image_pull_policy('Always')\n\n    vectorize_training_step = dsl.ContainerOp(\n        name='vectorize',\n        image=f\"{REGISTRY}/tokenize:{TAG}\",\n        command=\"python\",\n        arguments=[\n            \"-m\",\n            \"src.steps.tfidftransformer.pipeline_step\",\n            \"--input-data\", tokenize_training_step.outputs['tokenize_location']\n        ],\n        file_outputs={\"tfidftransformer_location\": \"/vectorizer_location.txt\",\n                      \"tfidfvectors_location\": \"/vectors_location.txt\"},\n        pvolumes={}\n    ).apply(aws.use_aws_secret('aws-secret', 'AWS_ACCESS_KEY_ID', 'AWS_SECRET_ACCESS_KEY')) \\\n        .set_image_pull_policy('Always')\n\n    lr_training_step = dsl.ContainerOp(\n        name='logical regression',\n        image=f\"{REGISTRY}/tokenize:{TAG}\",\n        command=\"python\",\n        arguments=[\n            \"-m\",\n            \"src.steps.lrclassifier.pipeline_step\",\n            \"--labels-data\", tokenize_training_step.outputs['labels_location'],\n            \"--vectors-data\", vectorize_training_step.outputs['tfidfvectors_location']\n        ],\n        file_outputs={\"lr_model_location\": \"/lr_model_location.txt\"},\n        pvolumes={}\n    ).apply(aws.use_aws_secret('aws-secret', 'AWS_ACCESS_KEY_ID', 'AWS_SECRET_ACCESS_KEY')) \\\n        .set_image_pull_policy('Always')\n\n    tokenize_build_step = dsl.ContainerOp(\n        name='Build tokenize Serving',\n        image=f\"{REGISTRY}/kaniko-executor:{TAG}\",\n        arguments=[\n            \"--dockerfile=Dockerfile\",\n            f\"--build-arg=TOKENIZE_MODEL={tokenize_training_step.outputs['tokenize_location']}\",\n            \"--context=dir:///workspace\",\n            f\"--destination={REGISTRY}/tokenizeserving:{TAG}\"\n        ],\n        pvolumes={}\n    ).apply(aws.use_aws_secret('aws-secret', 'AWS_ACCESS_KEY_ID', 'AWS_SECRET_ACCESS_KEY')) \\\n        .set_image_pull_policy('Always') \\\n        .after(lr_training_step)\n\n    vectorize_build_step = dsl.ContainerOp(\n        name='Build vectorize Serving',\n        image=f\"{REGISTRY}/kaniko-executor:{TAG}\",\n        arguments=[\n            \"--dockerfile=Dockerfile\",\n            f\"--build-arg=VECTORIZE_MODEL={vectorize_training_step.outputs['tfidftransformer_location']}\",\n            \"--context=dir:///workspace\",\n            f\"--destination={REGISTRY}/vecotrizeserving:{TAG}\"\n        ],\n        pvolumes={}\n    ).apply(aws.use_aws_secret('aws-secret', 'AWS_ACCESS_KEY_ID', 'AWS_SECRET_ACCESS_KEY')) \\\n        .set_image_pull_policy('Always') \\\n        .after(tokenize_build_step)\n\n    lr_model_build_step = dsl.ContainerOp(\n        name='Build LR Serving',\n        image=f\"{REGISTRY}/kaniko-executor:{TAG}\",\n        arguments=[\n            \"--dockerfile=Dockerfile\",\n            f\"--build-arg=LR_MODEL={lr_training_step.outputs['lr_model_location']}\",\n            \"--context=dir:///workspace\",\n            f\"--destination={REGISTRY}/lrserving:{TAG}\"\n        ],\n        pvolumes={}\n    ).apply(aws.use_aws_secret('aws-secret', 'AWS_ACCESS_KEY_ID', 'AWS_SECRET_ACCESS_KEY')) \\\n        .set_image_pull_policy('Always') \\\n        .after(vectorize_build_step)\n\n\nif __name__ == '__main__':\n\n    pipeline_file_path = __file__ + '.yaml'\n    compiler.Compiler().compile(kf_pipeline, pipeline_file_path)"}
{"prompt": "Generate a Kubeflow Pipeline named `DiabetesPredictionPipeline` that performs an end-to-end machine learning workflow for diabetes prediction.\n\nThe pipeline consists of three components:\n1.  **`load_data`**: This component is responsible for downloading, cleaning, and preprocessing the raw diabetes dataset.\n    *   **Function**: Downloads CSV files from specified URLs, consolidates them into a single DataFrame, renames columns to a standard format, removes rows with 'No Info' in the 'diabetes' column, selects relevant features (`gender`, `age`, `bmi`, `HbA1c_level`, `blood_glucose_level`, `diabetes`), drops rows with less than 4 non-NA values, maps 'gender' values (Male:0, Female:1), filters out 'Other' gender, and replaces 'No Info' values in numerical columns with their respective means.\n    *   **Inputs**: None.\n    *   **Outputs**: `data_output` (Artifact), which contains the preprocessed DataFrame in CSV format.\n    *   **Tools/Libraries**: Uses `pandas`.\n    *   **Base Image**: `python:3.9` with `pandas==2.2.2` installed.\n\n2.  **`prepare_data`**: This component splits the preprocessed data into training and testing sets.\n    *   **Function**: Reads the preprocessed DataFrame, separates features (X) from the target variable (y), and then splits them into training and testing sets using a 80/20 ratio.\n    *   **Inputs**: `data_input` (Input[Artifact]), which expects the preprocessed DataFrame from `load_data`.\n    *   **Outputs**:\n        *   `x_train_output` (Output[Artifact])\n        *   `x_test_output` (Output[Artifact])\n        *   `y_train_output` (Output[Artifact])\n        *   `y_test_output` (Output[Artifact])\n        Each output contains the respective DataFrame in CSV format.\n    *   **Tools/Libraries**: Uses `pandas` and `scikit-learn`.\n    *   **Base Image**: `python:3.9` with `pandas==2.2.2` and `scikit-learn==1.5.1` installed.\n\n3.  **(Implicit - based on common ML pipelines, you'd expect a model training/evaluation component)**: (You can add this if you'd like the LLM to infer a next step, otherwise, explicitly state it's a two-component pipeline based on the provided code.)\n\n**Control Flow**:\n*   The `load_data` component runs first.\n*   The `prepare_data` component runs `after` `load_data` and depends on its `data_output` as its `data_input`.", "completion": "# test\n\nfrom typing import NewType\n\nimport kfp\nimport kfp.compiler\n\nfrom kfp import dsl\nfrom kfp.dsl import OutputPath, InputPath, Input, Output, Artifact\n\nfrom pandas import DataFrame\n# from kfp.components import func_to_container_op\n\nDF = NewType('DF', DataFrame)\n\n@dsl.component(\n    base_image='python:3.9',\n    packages_to_install=['pandas==2.2.2']\n)\ndef load_data(data_output: Output[Artifact]):\n    import pandas as pd\n    \n    urls = [\n        \"https://raw.githubusercontent.com/daniel88516/diabetes-data/main/10k.csv\",\n        \"https://raw.githubusercontent.com/s102401002/kubeflowPipeline/main/data1.csv\"\n    ]\n    \n    standard_name_mapping = {\n        'gender': ['gender', 'gen', 'Gender', 'sex', 'Sex'],\n        'age': ['age', 'Age', 'AGE'],\n        'bmi': ['bmi', 'BMI', 'Bmi'],\n        'HbA1c_level': ['HbA1c_level', 'HbA1c', 'hba1c'],\n        'blood_glucose_level': ['blood_glucose_level', 'glucose', 'BloodGlucose'],\n        'diabetes': ['diabetes', 'Diabetes']\n    }\n\n    datas = [] # download all the csv in urls as a array\n    for url in urls:\n        df = pd.read_csv(url)\n        for standard_name, variants in standard_name_mapping.items():\n            for variant in variants:\n                if variant in df.columns:\n                    df.rename(columns={variant: standard_name}, inplace=True) # inplace=True: changing directly instead of creating a new column\n                    break\n        \n        datas.append(df)\n\n    df_data = pd.concat(datas, ignore_index=True)\n    \n    df_data = df_data.drop(df_data[df_data['diabetes'] == 'No Info'].index)\n    df_data = df_data[['gender','age', 'bmi', 'HbA1c_level', 'blood_glucose_level', 'diabetes']]\n    df_data = df_data.dropna(thresh=4)\n    \n    gender_map = {'Male': 0 , 'Female': 1  , 'Other': 2}\n    df_data['gender'] = df_data['gender'].map(gender_map)\n    df_data = df_data[df_data['gender'] != 2]\n    df_data['age'] = df_data['age'].replace('No Info', df_data['age'].mean())\n    df_data['bmi'] = df_data['bmi'].replace('No Info', df_data['bmi'].mean())\n    df_data['HbA1c_level'] = df_data['HbA1c_level'].replace('No Info', df_data['HbA1c_level'].mean())\n    df_data['blood_glucose_level'] = df_data['blood_glucose_level'].replace('No Info', df_data['blood_glucose_level'].mean())\n\n    df_data.to_csv(data_output.path)\n\n@dsl.component(\n    base_image='python:3.9',\n    packages_to_install=['pandas==2.2.2', 'scikit-learn==1.5.1']\n)\ndef prepare_data(\n    data_input: Input[Artifact], \n    x_train_output: Output[Artifact], x_test_output: Output[Artifact],\n    y_train_output: Output[Artifact], y_test_output: Output[Artifact]\n):\n    import pandas as pd\n    from sklearn.model_selection import train_test_split\n\n    df_data = pd.read_csv(data_input.path)\n\n    x = df_data.drop(labels=['diabetes'], axis=1)\n    y = df_data[['diabetes']]\n    \n    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n    \n    x_train_df = pd.DataFrame(x_train)\n    x_test_df = pd.DataFrame(x_test)\n    y_train_df = pd.DataFrame(y_train)\n    y_test_df = pd.DataFrame(y_test)\n\n    x_train_df.to_csv(x_train_output.path, index=False)\n    x_test_df.to_csv(x_test_output.path, index=False)\n    y_train_df.to_csv(y_train_output.path, index=False)\n    y_test_df.to_csv(y_test_output.path, index=False)\n\n@dsl.component(\n    base_image='python:3.9',\n    packages_to_install=['pandas==2.2.2', 'scikit-learn==1.5.1', 'joblib==1.4.2', 'xgboost==2.0.3']\n)\ndef train_model(x_train: Input[Artifact], y_train: Input[Artifact], train_model_output: Output[Artifact]):\n    import pandas as pd\n    from xgboost import XGBClassifier\n    import joblib\n    \n    x_train = pd.read_csv(x_train.path)\n    y_train = pd.read_csv(y_train.path)\n    \n    model = XGBClassifier(n_estimators=1000, learning_rate= 0.01)\n    model.fit(x_train, y_train.values.ravel())\n    \n    joblib.dump(model, train_model_output.path)\n\n@dsl.component(\n    base_image='python:3.9',\n    packages_to_install=['pandas==2.2.2', 'scikit-learn==1.5.1', 'joblib==1.4.2', 'xgboost==2.0.3']\n)\ndef evaluate_model(model_path: Input[Artifact], x_test: Input[Artifact], y_test: Input[Artifact]) -> str:\n    import pandas as pd\n    from sklearn.metrics import accuracy_score\n    import joblib\n\n    model = joblib.load(filename=model_path.path)\n\n    x_test_df = pd.read_csv(x_test.path)\n    y_test_df = pd.read_csv(y_test.path)\n    \n    y_pred = model.predict(x_test_df)\n    accuracy = accuracy_score(y_test_df, y_pred)\n    \n    return f'Test accuracy: {accuracy}'\n\n@dsl.pipeline(\n    name='Diabetes Prediction Pipeline',\n    description='Using kubeflow pipeline to train and evaluate a diabetes prediction model'\n)\ndef diabetes_prediction_pipeline() -> str:\n    load_data_task = load_data()\n\n    prepare_data_task = prepare_data(data_input=load_data_task.outputs['data_output'])\n    \n    train_model_task = train_model(\n        x_train = prepare_data_task.outputs['x_train_output'], \n        y_train = prepare_data_task.outputs['y_train_output']\n    )\n    \n    evaluate_model_task = evaluate_model(\n        model_path = train_model_task.outputs['train_model_output'], \n        x_test = prepare_data_task.outputs['x_test_output'], \n        y_test = prepare_data_task.outputs['y_test_output']\n    )\n    \n    return evaluate_model_task.output\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(diabetes_prediction_pipeline, 'diabetes_prediction_pipeline_xgboost.yaml')"}
{"prompt": "Generate a Kubeflow Pipeline named `timestamp-pipeline` that performs the following:\n\n**Pipeline Name:** `timestamp-pipeline`\n\n**Number of Components:** 1\n\n**Components:**\n\n1.  **`timestamp` component:**\n    *   **Function:** This component is responsible for generating a timestamp string.\n    *   **Inputs:**\n        *   `format` (str, default: `\"%Y%m%d%H%M%S\"`): Specifies the desired format for the timestamp string.\n        *   `prefix` (str, default: `\"\"`): A string to prepend to the timestamp.\n        *   `suffix` (str, default: `\"\"`): A string to append to the timestamp.\n        *   `separator` (str, default: `\"-\"`): A character or string to separate parts of the timestamp.\n        *   `tz_offset` (int, default: `0`): An integer representing the timezone offset in hours.\n    *   **Outputs:**\n        *   `time_string` (Artifact/String): The generated timestamp string.\n\n**Control Flow:**\n\n*   The `timestamp_pipeline` directly calls the `timestamp` component. There are no complex dependencies, parallel executions, or conditional logic involved. The output of the `timestamp` component (`time_string`) is then returned by the pipeline.\n\n**Tools/Libraries Used:**\n\n*   `kfp.v2.dsl`: Used for defining the Kubeflow Pipeline and its components.\n*   The `timestamp` component is imported from a local `components` module, suggesting it's a custom-defined Kubeflow component.", "completion": "import os\nimport warnings\nfrom typing import Any, Mapping, Optional, Union\n\nfrom kfp.v2 import dsl\n\nfrom . import pipeline_jobs, pipeline_parser\nfrom .components import timestamp\nfrom .pipeline_parser import Parameter, ParameterValue, Pipeline  # noqa: F401\n\n\n@dsl.pipeline(name=\"timestamp-pipeline\")\ndef timestamp_pipeline(\n    format: str = \"%Y%m%d%H%M%S\",\n    prefix: str = \"\",\n    suffix: str = \"\",\n    separator: str = \"-\",\n    tz_offset: int = 0,\n):\n    time_string = timestamp(\n        format=format,\n        prefix=prefix,\n        suffix=suffix,\n        separator=separator,\n        tz_offset=tz_offset,\n    )\n    return time_string\n\n\ndef load_pipeline_from_file(filepath: Union[str, os.PathLike]) -> Pipeline:\n    \"\"\"Load a pipeline object from the pipeline file.\n\n    Load a :class:`Pipeline` object from a pre-compiled file that represents\n    the pipeline.\n\n    Args:\n        filepath (Union[str, os.PathLike]): The path of the pre-compiled file that\n            represents the pipeline.\n\n    Raises:\n        ValueError: If the :attr:`filepath` file has an invalid schema.\n\n    Returns:\n        Pipeline: An object that represents the pipeline.\n\n    .. deprecated:: 0.6.0\n        Use :func:`.pipeline_parser.parse_pipeline_package` instead.\n\n    \"\"\"\n\n    warnings.warn(\n        \"`load_pipeline_from_file is deprecated.\"\n        \" Use `pipeline_parser.parse_pipeline_package` instead.\",\n        DeprecationWarning,\n    )\n\n    return pipeline_parser.parse_pipeline_package(filepath=filepath)\n\n\ndef submit_pipeline_job(\n    pipeline_file: Union[str, os.PathLike],\n    endpoint: Optional[str] = None,\n    iap_client_id: Optional[str] = None,\n    api_namespace: str = \"kubeflow\",\n    other_client_id: Optional[str] = None,\n    other_client_secret: Optional[str] = None,\n    arguments: Optional[Mapping[str, Any]] = None,\n    run_name: Optional[str] = None,\n    experiment_name: Optional[str] = None,\n    namespace: Optional[str] = None,\n    pipeline_root: Optional[str] = None,\n    enable_caching: Optional[bool] = None,\n    service_account: Optional[str] = None,\n    encryption_spec_key_name: Optional[str] = None,\n    labels: Optional[Mapping[str, str]] = None,\n    project: Optional[str] = None,\n    location: Optional[str] = None,\n    network: Optional[str] = None,\n):\n    \"\"\"Submit a pipeline job.\n\n    Submit a pipeline job to the appropriate environment. If an :attr:`endpoint` is\n    specified, it is considered an instance of Kubeflow Pipelines. Otherwise, attempt\n    to submit to Vertex AI Pipelines.\n\n    Args:\n        pipeline_file (Union[str, os.PathLike]): Path of the pipeline package file.\n        endpoint (Optional[str], optional): Endpoint of the KFP API service to connect.\n            Used only for Kubeflow Pipelines. Defaults to None.\n        iap_client_id (Optional[str], optional): The client ID used by Identity-Aware\n            Proxy. Used only for Kubeflow Pipelines. Defaults to None.\n        api_namespace (str, optional): Kubernetes namespace to connect to the KFP API.\n            Used only for Kubeflow Pipelines. Defaults to \"kubeflow\".\n        other_client_id (Optional[str], optional): The client ID used to obtain\n            the auth codes and refresh tokens. Used only for Kubeflow Pipelines.\n            Defaults to None.\n        other_client_secret (Optional[str], optional): The client secret used to obtain\n            the auth codes and refresh tokens. Used only for Kubeflow Pipelines.\n            Defaults to None.\n        arguments (Optional[Mapping[str, Any]], optional): Arguments to the pipeline\n            function provided as a dict. Defaults to None.\n        run_name (Optional[str], optional): Name of the run to be shown in the UI.\n            Defaults to None.\n        experiment_name (Optional[str], optional): Name of the experiment to add the\n            run to. Defaults to None.\n        namespace (Optional[str], optional): Kubernetes namespace where the pipeline\n            runs are created. Used only for Kubeflow Pipelines. Defaults to None.\n        pipeline_root (Optional[str], optional): The root path of the pipeline outputs.\n            Defaults to None.\n        enable_caching (Optional[bool], optional): Whether or not to enable caching for\n            the run. Defaults to None.\n        service_account (Optional[str], optional): Specifies which Kubernetes service\n            account this run uses. Defaults to None.\n        encryption_spec_key_name (Optional[str], optional): The Cloud KMS resource\n            identifier of the customer managed encryption key used to protect the job.\n            Used only for Vertex AI Pipelines. Defaults to None.\n        labels (Optional[Mapping[str, str]], optional): The user defined metadata to\n            organize PipelineJob. Used only for Vertex AI Pipelines. Defaults to None.\n        project (Optional[str], optional): The project that you want to run this\n            PipelineJob in. Used only for Vertex AI Pipelines. Defaults to None.\n        location (Optional[str], optional): Location to create PipelineJob. Used only\n            for Vertex AI Pipelines. Defaults to None.\n        network (Optional[str], optional): The full name of the Compute Engine network\n            to which the job should be peered. Used only for Vertex AI Pipelines.\n            Defaults to None.\n\n    .. deprecated:: 0.6.0\n        Use :func:`.pipeline_jobs.submit_pipeline_job` instead.\n\n    \"\"\"\n\n    warnings.warn(\n        \"`pipelines.submit_pipeline_job` is deprecated.\"\n        \" Use `pipeline_jobs.submit_pipeline_job` instead.\",\n        DeprecationWarning,\n    )\n\n    return pipeline_jobs.submit_pipeline_job(\n        pipeline_file=pipeline_file,\n        endpoint=endpoint,\n        iap_client_id=iap_client_id,\n        api_namespace=api_namespace,\n        other_client_id=other_client_id,\n        other_client_secret=other_client_secret,\n        arguments=arguments,\n        run_name=run_name,\n        experiment_name=experiment_name,\n        namespace=namespace,\n        pipeline_root=pipeline_root,\n        enable_caching=enable_caching,\n        service_account=service_account,\n        encryption_spec_key_name=encryption_spec_key_name,\n        labels=labels,\n        project=project,\n        location=location,\n        network=network,\n    )"}
{"prompt": "Generate a Kubeflow Pipeline named `End2end Resnet50 Classification` that performs an end-to-end Kubeflow demonstration using TensorRT Inference Server, TF-AMP, and TensorRT.\n\nThe pipeline should accept the following parameters with their default values:\n- `trtserver_name`: `trtserver` (string)\n- `model_name`: `resnet_graphdef` (string)\n- `model_version`: `1` (string)\n- `num_iter`: `10` (integer)\n- `batch_size`: `128` (integer)\n- `webapp_prefix`: `webapp` (string)\n- `webapp_port`: `80` (string)\n- `storage_bucket`: `gs://dongm-kubeflow` (string)\n- `ckpt_dir`: `ckpt` (string)\n- `mount_dir`: `/mnt/vol` (string)\n- `model_dir`: `test_model_repository` (string)\n- `raw_data_dir`: `raw_data` (string)\n- `processed_data_dir`: `processed_data` (string)\n\nThe pipeline consists of at least two components (the provided code snippet is incomplete, but we can infer the first two):\n\n1.  **`preprocessing`**:\n    *   **Function**: Preprocesses raw data.\n    *   **Image**: `gcr.io/nvidia-sa-org/gcp-end2end-demo-preprocessing`\n    *   **Command**: `python`\n    *   **Arguments**:\n        *   `/scripts/preprocess.py`\n        *   `--input_dir`: Constructed using `mount_dir` and `raw_data_dir` (e.g., `/mnt/vol/raw_data`).\n        *   `--output_dir`: Constructed using `mount_dir` and `processed_data_dir` (e.g., `/mnt/vol/processed_data`).\n    *   **Outputs**: No file outputs are explicitly defined (`file_outputs={}`).\n    *   **Resource Requirements**: Requests 1 GPU.\n    *   **Volume Mounts**: Attaches a `PersistentVolumeClaim` named `my-rw-pvc` to `/mnt/vol` using a `V1Volume` and `V1VolumeMount`.\n\n2.  **`training`**:\n    *   **Function**: Performs model training.\n    *   **Image**: `gcr.io/nvidia-sa-org/gcp-end2end-demo-training`\n    *   **Command**: `python`\n    *   **Arguments**: The snippet is incomplete, but based on typical MLOps pipelines, it would likely take inputs from the `preprocessing` step and potentially other pipeline parameters like `num_iter`, `batch_size`, `ckpt_dir`, etc.\n    *   **Dependencies**: This component is expected to run *after* the `preprocessing` component has completed successfully.\n\nThe pipeline should leverage `kfp.dsl` for pipeline definition and `kubernetes.client` for volume management. The `project_name` variable should be set to `nvidia-sa-org`.", "completion": "# Copyright 2018 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport kfp.dsl as dsl\nimport kfp.gcp as gcp\nfrom kubernetes import client as k8s_client\n\n@dsl.pipeline(\n  name='End2end Resnet50 Classification',\n  description='END to END kubeflow demo with TensorRT Inference Server, TF-AMP and TensorRT'\n)\ndef end2end_demo(  # pylint: disable=unused-argument\n    trtserver_name: dsl.PipelineParam = dsl.PipelineParam(name='trtserver_name', value='trtserver'),\n    model_name: dsl.PipelineParam = dsl.PipelineParam(name='model_name', value='resnet_graphdef'),\n    model_version: dsl.PipelineParam = dsl.PipelineParam(name='model_version', value='1'),\n    num_iter: dsl.PipelineParam = dsl.PipelineParam(name='num_iter', value=10),\n    batch_size: dsl.PipelineParam = dsl.PipelineParam(name='batch_size', value=128),\n    webapp_prefix: dsl.PipelineParam = dsl.PipelineParam(name='webapp_prefix', value='webapp'),\n    webapp_port: dsl.PipelineParam = dsl.PipelineParam(name='webapp_port', value='80'),\n    storage_bucket: dsl.PipelineParam = dsl.PipelineParam(name='storage_bucket', value='gs://dongm-kubeflow'),\n    ckpt_dir: dsl.PipelineParam = dsl.PipelineParam(name='ckpt_dir', value='ckpt'),\n    mount_dir: dsl.PipelineParam = dsl.PipelineParam(name='mount_dir', value='/mnt/vol'),\n    model_dir: dsl.PipelineParam = dsl.PipelineParam(name='model_dir', value='test_model_repository'),\n    raw_data_dir: dsl.PipelineParam = dsl.PipelineParam(name='raw_data_dir', value='raw_data'),\n    processed_data_dir: dsl.PipelineParam = dsl.PipelineParam(name='processed_data_dir', value='processed_data')\n):\n  project_name = 'nvidia-sa-org'\n\n  preprocessing = dsl.ContainerOp(\n    name='preprocessing',\n    image='gcr.io/' + project_name + '/gcp-end2end-demo-preprocessing',\n    command=['python'],\n    arguments=[\n      '/scripts/preprocess.py',\n      '--input_dir', '%s/%s' % (mount_dir, raw_data_dir),\n      '--output_dir', '%s/%s' % (mount_dir, processed_data_dir)\n    ],\n    file_outputs={}\n  ).add_volume(k8s_client.V1Volume(name='my-rw-pv',\n                                   persistent_volume_claim=k8s_client.V1PersistentVolumeClaimVolumeSource(\n                                     claim_name='my-rw-pvc'))\n               ).add_volume_mount(k8s_client.V1VolumeMount(mount_path='/mnt/vol', name='my-rw-pv')).set_gpu_limit(1)\n\n  training = dsl.ContainerOp(\n    name='training',\n    image='gcr.io/' + project_name + '/gcp-end2end-demo-training',\n    command=['python'],\n    arguments=[\n      '/scripts/train.py',\n      '--model_version', str(model_version),\n      '--input_dir', '%s/%s' % (mount_dir, processed_data_dir),\n      '--ckpt_dir', '%s/%s' % (mount_dir, ckpt_dir),\n      '--output_dir', '%s/%s/resnet_graphdef' % (storage_bucket, model_dir),\n      '--num_iter', '%s' % (num_iter),\n      '--batch_size', '%s' % (batch_size)\n    ],\n    file_outputs={}\n  ).set_gpu_limit(1).add_volume(k8s_client.V1Volume(name='my-rw-pv',\n                                                    persistent_volume_claim=k8s_client.V1PersistentVolumeClaimVolumeSource(\n                                                      claim_name='my-rwm-pvc'))\n                                ).add_volume_mount(k8s_client.V1VolumeMount(mount_path='/mnt/vol', name='my-rw-pv')).apply(gcp.use_gcp_secret('user-nvidia-sa'))\n\n  serve = dsl.ContainerOp(\n    name='serve',\n    image='gcr.io/' + project_name + '/ml-pipeline-kubeflow-trtisserve',\n    arguments=[\"--trtserver_name\", trtserver_name,\n               '--model_version', model_version,\n               '--orig_model_path', '%s/%s' % (mount_dir, model_name),\n               \"--model_path\", '%s/%s' % (storage_bucket, model_dir)\n               ]\n  )\n\n  webapp = dsl.ContainerOp(\n    name='webapp',\n    image='gcr.io/' + project_name + '/ml-pipeline-trtis-webapp-launcher',\n    arguments=[\"--workflow_name\", '%s' % ('{{workflow.name}}',),\n               \"--trtserver_name\", trtserver_name,\n               \"--model_name\", model_name,\n               \"--model_version\", str(model_version),\n               \"--webapp_prefix\", webapp_prefix,\n               \"--webapp_port\", str(webapp_port)\n               ]\n  )\n\n  training.after(preprocessing)\n  serve.after(training)\n  webapp.after(serve)\n\n\nif __name__ == '__main__':\n  import kfp.compiler as compiler\n\n  compiler.Compiler().compile(end2end_demo, __file__ + '.tar.gz')"}
{"prompt": "Generate a Kubeflow Pipeline named `ResourceOp Basic` that performs a basic example of ResourceOp usage.\n\nThe pipeline consists of a single component:\n1.  **`test-step`**: This component is a `ResourceOp` that creates a Kubernetes Job resource.\n    *   **Function**: It deploys a Kubernetes Job defined by an inline JSON manifest. This Job's container uses the `k8s.gcr.io/busybox` image and executes the `/usr/bin/env` command, which prints out environment variables.\n    *   **Inputs**: The `k8s_resource` is provided as a JSON string representing the Kubernetes Job manifest.\n    *   **Outputs**: Not explicitly defined in the DSL, but the Job's output (stdout of the `/usr/bin/env` command) would be available in the step's logs.\n    *   **Control Flow**: This is the only step in the pipeline and runs directly.\n\nThe pipeline utilizes the `kfp.dsl.ResourceOp` class for Kubernetes resource management. No external libraries like sklearn or Snowflake are used within the pipeline components themselves. The `json` library is used to parse the Kubernetes manifest.", "completion": "# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n\"\"\"\nThis example demonstrates how to use ResourceOp to specify the value of env var.\n\"\"\"\n\nimport json\nimport kfp\nimport kfp.dsl as dsl\n\n\n_CONTAINER_MANIFEST = \"\"\"\n{\n    \"apiVersion\": \"batch/v1\",\n    \"kind\": \"Job\",\n    \"metadata\": {\n        \"generateName\": \"resourceop-basic-job-\"\n    },\n    \"spec\": {\n        \"template\": {\n            \"metadata\": {\n                \"name\": \"resource-basic\"\n            },\n            \"spec\": {\n                \"containers\": [{\n                    \"name\": \"sample-container\",\n                    \"image\": \"k8s.gcr.io/busybox\",\n                    \"command\": [\"/usr/bin/env\"]\n                }],\n                \"restartPolicy\": \"Never\"\n            }\n        },\n        \"backoffLimit\": 4      \n    }\n}\n\"\"\"\n\n\n@dsl.pipeline(\n    name=\"ResourceOp Basic\",\n    description=\"A Basic Example on ResourceOp Usage.\"\n)\ndef resourceop_basic():\n\n    # Start a container. Print out env vars.\n    op = dsl.ResourceOp(\n        name='test-step',\n        k8s_resource=json.loads(_CONTAINER_MANIFEST),\n        action='create'\n    )\n\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(resourceop_basic, __file__ + '.yaml')"}
{"prompt": "Generate a Kubeflow Pipeline named `functions-with-outputs` that performs the following sequence of operations:\n\nThe pipeline consists of four components: `concat_message`, `add_numbers`, `output_artifact`, and `output_named_tuple`.\n\n1.  **`concat_message` component**:\n    *   **Function**: Concatenates two string inputs.\n    *   **Inputs**:\n        *   `first`: A string, provided by the pipeline parameter `first_message`.\n        *   `second`: A string, provided by the pipeline parameter `second_message`.\n    *   **Output**: A string, representing the concatenated message.\n\n2.  **`add_numbers` component**:\n    *   **Function**: Adds two integer inputs.\n    *   **Inputs**:\n        *   `first`: An integer, provided by the pipeline parameter `first_number`.\n        *   `second`: An integer, provided by the pipeline parameter `second_number`.\n    *   **Output**: An integer, representing the sum of the numbers.\n\n3.  **`output_artifact` component**:\n    *   **Function**: Generates a `Dataset` artifact containing a message repeated a specified number of times.\n    *   **Inputs**:\n        *   `number`: An integer, which is the output of the `add_numbers` component.\n        *   `message`: A string, which is the output of the `concat_message` component.\n    *   **Output**: A `Dataset` artifact.\n\n4.  **`output_named_tuple` component**:\n    *   **Function**: Takes a `Dataset` artifact as input and produces a named tuple containing a scalar string, metrics (in JSON format), and a `Model` artifact. It uses the contents of the input `Dataset` to construct the `Model` artifact.\n    *   **Inputs**:\n        *   `artifact`: An `Input[Dataset]`, which is the output of the `output_artifact` component.\n    *   **Output**: A `NamedTuple` with the following fields:\n        *   `scalar`: A `str`.\n        *   `metrics`: A `Metrics` object (containing accuracy).\n        *   `model`: A `Model` object.\n\n**Control Flow**:\n*   `concat_message` and `add_numbers` run in parallel.\n*   `output_artifact` depends on the completion of both `concat_message` and `add_numbers`, using their respective outputs as inputs.\n*   `output_named_tuple` depends on the completion of `output_artifact`, using its output as input.\n\nThe pipeline parameters are `first_message` (default 'first'), `second_message` (default 'second'), `first_number` (default 1), and `second_number` (default 2).\n\nThe pipeline should be compilable using `kfp.v2.compiler.Compiler`.", "completion": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Lightweight functions v2 with outputs.\"\"\"\nfrom typing import NamedTuple\nimport os\nfrom kfp import components, dsl\nfrom kfp.v2 import compiler\nfrom kfp.v2.dsl import Input, Dataset, Model, Metrics, component\n\n# In tests, we install a KFP package from the PR under test. Users should not\n# normally need to specify `kfp_package_path` in their component definitions.\n_KFP_PACKAGE_PATH = os.getenv('KFP_PACKAGE_PATH')\n\n\n@component(kfp_package_path=_KFP_PACKAGE_PATH)\ndef concat_message(first: str, second: str) -> str:\n    return first + second\n\n\n@component(kfp_package_path=_KFP_PACKAGE_PATH)\ndef add_numbers(first: int, second: int) -> int:\n    return first + second\n\n\n@component(kfp_package_path=_KFP_PACKAGE_PATH)\ndef output_artifact(number: int, message: str) -> Dataset:\n    result = [message for _ in range(number)]\n    return '\\n'.join(result)\n\n\n@component(kfp_package_path=_KFP_PACKAGE_PATH)\ndef output_named_tuple(\n    artifact: Input[Dataset]\n) -> NamedTuple(\n        'Outputs', [\n            ('scalar', str),\n            ('metrics', Metrics),\n            ('model', Model),\n        ]\n):\n    scalar = \"123\"\n\n    import json\n    metrics = json.dumps({\n        'metrics': [{\n            'name': 'accuracy',\n            'numberValue': 0.9,\n            'format': \"PERCENTAGE\",\n        }]\n    })\n\n    with open(artifact.path, 'r') as f:\n        artifact_contents = f.read()\n    model = \"Model contents: \" + artifact_contents\n\n    from collections import namedtuple\n    output = namedtuple('Outputs', ['scalar', 'metrics', 'model'])\n    return output(scalar, metrics, model)\n\n\n@dsl.pipeline(name='functions-with-outputs')\ndef pipeline(\n    first_message: str = 'first',\n    second_message: str = 'second',\n    first_number: int = 1,\n    second_number: int = 2,\n):\n    concat_task = concat_message(first=first_message, second=second_message)\n    add_numbers_task = add_numbers(first=first_number, second=second_number)\n    output_artifact_task = output_artifact(\n        number=add_numbers_task.output, message=concat_task.output\n    )\n    output_name_tuple = output_named_tuple(output_artifact_task.output)\n\n\nif __name__ == '__main__':\n    compiler.Compiler().compile(\n        pipeline_func=pipeline,\n        pipeline_root='dummy_root',\n        output_path=__file__ + '.json'\n    )"}
{"prompt": "Generate a Kubeflow Pipeline named `ML Models Pipeline` that performs machine learning model training and evaluation.\n\nThe pipeline consists of **seven** components:\n1.  **`download_data`**: This component is responsible for downloading the necessary dataset. Its input is not explicitly defined in the provided code, but it outputs the data used by subsequent ML model components. This component is loaded from `download_data/download_data.yaml`.\n2.  **`decision_tree`**: This component trains a Decision Tree classifier. It takes the output from `download_data` as its input and outputs the accuracy score of the Decision Tree model. This component is loaded from `decision_tree/decision_tree.yaml`.\n3.  **`logistic_regression`**: This component trains a Logistic Regression classifier. It takes the output from `download_data` as its input and outputs the accuracy score of the Logistic Regression model. This component is loaded from `logistic_regression/logistic_regression.yaml`.\n4.  **`svm`**: This component trains a Support Vector Machine (SVC) classifier. It takes the output from `download_data` as its input and outputs the accuracy score of the SVM model. This component is loaded from `svm/svm.yaml`.\n5.  **`naive_bayes`**: This component trains a Gaussian Naive Bayes classifier. It takes the output from `download_data` as its input and outputs the accuracy score of the Naive Bayes model. This component is loaded from `naive_bayes/naive_bayes.yaml`.\n6.  **`xg_boost`**: This component trains an XGBoost classifier. It takes the output from `download_data` as its input and outputs the accuracy score of the XGBoost model. This component is loaded from `xg_boost/xg_boost.yaml`.\n7.  **`show_results`**: This component is a Python function-based component (`@func_to_container_op`) that prints the accuracy results from all the machine learning models. It takes five float inputs: `decision_tree`, `logistic_regression`, `svm`, `naive_bayes`, and `xg_boost`, corresponding to the accuracy scores from the respective ML model components. It does not produce any visible output beyond printing to standard output.\n\nThe pipeline's control flow is as follows:\n- The `download_data` task runs first.\n- Once `download_data` completes, its output is used as input for the `decision_tree`, `logistic_regression`, `svm`, `naive_bayes`, and `xg_boost` tasks. These five tasks run in parallel, as they all depend only on the `download_data` task's completion and output.\n- After all five machine learning model tasks complete, their respective accuracy outputs are passed as arguments to the `show_results` component, which then executes to display the results.\n\nThe pipeline utilizes the `kfp` library for defining and compiling the Kubeflow Pipeline. The individual ML model components are loaded from external YAML files, suggesting that they might be pre-built Docker images or pre-defined Kubeflow component specifications. The `show_results` component is defined directly within the Python code using `@func_to_container_op`. The machine learning components likely use common ML libraries such as `scikit-learn` or `XGBoost`, although this is inferred and not explicitly stated in the provided pipeline definition.", "completion": "import kfp\nfrom kfp import dsl\nfrom kfp.components import func_to_container_op\n\n@func_to_container_op\ndef show_results(decision_tree : float, logistic_regression : float, svm : float, naive_bayes : float, xg_boost : float) -> None:\n    # Given the outputs from decision_tree, logistic regression, svm, naive_bayes, xg_boost components\n\n    print(f\"Decision tree (accuracy): {decision_tree}\")\n    print(f\"Logistic regression (accuracy): {logistic_regression}\")\n    print(f\"SVM (SVC) (accuracy): {svm}\")\n    print(f\"Naive Bayes (Gaussian) (accuracy): {naive_bayes}\")\n    print(f\"XG Boost (accuracy): {xg_boost}\")\n\n\n@dsl.pipeline(name='ML Models Pipeline', description='Applies Decision Tree, Logistic Regression, SVM, Naive Bayes, XG Boost for classification problem.')\ndef ml_models_pipeline():\n\n    # Loads the yaml manifest for each component\n    download = kfp.components.load_component_from_file('download_data/download_data.yaml')\n    decision_tree = kfp.components.load_component_from_file('decision_tree/decision_tree.yaml')\n    logistic_regression = kfp.components.load_component_from_file('logistic_regression/logistic_regression.yaml')\n    svm = kfp.components.load_component_from_file('svm/svm.yaml')\n    naive_bayes = kfp.components.load_component_from_file('naive_bayes/naive_bayes.yaml')\n    xg_boost = kfp.components.load_component_from_file('xg_boost/xg_boost.yaml')\n\n    # Run download_data task\n    download_task = download()\n\n    # Run ML models tasks with input data\n    decision_tree_task = decision_tree(download_task.output)\n    logistic_regression_task = logistic_regression(download_task.output)\n    svm_task = svm(download_task.output)\n    naive_bayes_task = naive_bayes(download_task.output)\n    xg_boost_task = xg_boost(download_task.output)\n\n    # Given the outputs from ML models tasks\n    # the component \"show_results\" is called to print the results.\n    show_results(decision_tree_task.output, logistic_regression_task.output, svm_task.output, naive_bayes_task.output, xg_boost_task.output)\n\n\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(ml_models_pipeline, 'MLModelsPipeline.yaml')"}
{"prompt": "Generate a Kubeflow Pipeline named `Kedro pipeline` that performs the execution of a Kedro project.\n\nThis pipeline consists of a dynamic number of components, where each component corresponds to a node in a Kedro pipeline. The exact number of components is determined at runtime based on the structure of the Kedro pipeline being converted.\n\nEach component:\n- Represents a single Kedro node.\n- Is named after the cleaned version of the Kedro node's name (e.g., `feature-engineering` for a node named `feature_engineering`).\n- Takes no explicit inputs or outputs as all data handling is managed internally by Kedro.\n- Executes the `kedro run --node <node_name>` command within a specified Docker image.\n- Utilizes AWS credentials by applying `aws.use_aws_secret` to inject `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` from the Kubernetes secret `aws-secrets`.\n\nThe control flow of the pipeline is dynamically constructed:\n- Dependencies between components are established using the `.after()` method, reflecting the dependencies between Kedro nodes. If Kedro node A depends on Kedro node B, then the Kubeflow component for A will run `after` the component for B.\n- All components run sequentially or in parallel as dictated by the Kedro node dependency graph. There are no explicit `parallelFor` loops or other complex control flow mechanisms defined at the Kubeflow level, beyond the sequential execution based on `after` dependencies.\n\nThe pipeline itself, named `convert_kedro_pipeline_to_kfp`, orchestrates the creation of these dynamic components and their dependencies. It leverages Kedro's internal `node_dependencies` to build the Kubeflow component graph.\n\nThe pipeline definition uses the `kfp` library for defining components and the pipeline structure, and the `re` module for cleaning names. It integrates with Kedro to dynamically obtain the project's pipeline structure.", "completion": "# python build_kubeflow_pipeline.py <project_image>\n\nimport re\nfrom pathlib import Path\nfrom typing import Dict, Set\n\nimport click\n\nfrom kfp import aws, dsl\nfrom kfp.compiler.compiler import Compiler\n\nfrom kedro.framework.project import pipelines\nfrom kedro.framework.startup import bootstrap_project\nfrom kedro.pipeline.node import Node\n\n_PIPELINE = None\n_IMAGE = None\n\n\n@click.command()\n@click.argument(\"image\", required=True)\n@click.option(\"-p\", \"--pipeline\", \"pipeline_name\", default=None)\n@click.option(\"--env\", \"-e\", type=str, default=None)\ndef generate_kfp(image: str, pipeline_name: str, env: str) -> None:\n    \"\"\"Generates a workflow spec yaml file from a Kedro pipeline.\n\n    Args:\n        image: container image name.\n        pipeline_name: pipeline name to build a workflow spec.\n        env: Kedro configuration environment name.\n\n    \"\"\"\n    global _PIPELINE\n    global _IMAGE\n    _IMAGE = image\n\n    project_path = Path.cwd()\n    metadata = bootstrap_project(project_path)\n    package_name = metadata.package_name\n\n    pipeline_name = pipeline_name or \"__default__\"\n    _PIPELINE = pipelines.get(pipeline_name)\n\n    Compiler().compile(convert_kedro_pipeline_to_kfp, package_name + \".yaml\")\n\n\n@dsl.pipeline(name=\"Kedro pipeline\", description=\"Kubeflow pipeline for Kedro project\")\ndef convert_kedro_pipeline_to_kfp() -> None:\n    \"\"\"Convert from a Kedro pipeline into a kfp container graph.\"\"\"\n    node_dependencies = _PIPELINE.node_dependencies\n    kfp_ops = _build_kfp_ops(node_dependencies)\n    for node, dependencies in node_dependencies.items():\n        for dependency in dependencies:\n            kfp_ops[node.name].after(kfp_ops[dependency.name])\n\n\ndef _build_kfp_ops(\n    node_dependencies: Dict[Node, Set[Node]]\n) -> Dict[str, dsl.ContainerOp]:\n    \"\"\"Build kfp container graph from Kedro node dependencies.\"\"\"\n    kfp_ops = {}\n\n    for node in node_dependencies:\n        name = clean_name(node.name)\n        kfp_ops[node.name] = dsl.ContainerOp(\n            name=name,\n            image=_IMAGE,\n            command=[\"kedro\"],\n            arguments=[\"run\", \"--node\", node.name],\n        ).apply(\n            # Configure the container to use AWS credentials.\n            aws.use_aws_secret(\n                \"aws-secrets\", \"AWS_ACCESS_KEY_ID\", \"AWS_SECRET_ACCESS_KEY\"\n            )\n        )\n    return kfp_ops\n\n\ndef clean_name(name: str) -> str:\n    \"\"\"Reformat a name.\n\n    Returns:\n        name: formatted name.\n\n    \"\"\"\n    return re.sub(r\"[\\W_]+\", \"-\", name).strip(\"-\")\n\n\nif __name__ == \"__main__\":\n    generate_kfp()"}
{"prompt": "Generate a Kubeflow Pipeline named `MetricsVisualizationPipeline` that performs classification tasks and visualizes their metrics.\n\nThe pipeline consists of five components:\n\n1.  **`wine_classification`**: This component performs a wine classification task.\n    *   It takes no visible inputs.\n    *   It produces a `metrics` output artifact, which contains `ConfidenceMetrics` with various `confidenceThreshold`, `falsePositiveRate`, and `recall` values. The `display_name` for this metric is \"metrics\".\n\n2.  **`iris_sgdclassifier`**: This component performs an Iris SGDClassifier task.\n    *   It takes no visible inputs.\n    *   It produces a `metrics` output artifact, which contains `ClassificationMetrics` including `accuracy`, `precision`, `recall`, `f1Score`, `falsePositiveRate`, `falseNegativeRate`, `truePositiveRate`, `trueNegativeRate`, and `areaUnderRocCurve`. The `display_name` for this metric is \"metrics\".\n\n3.  **`digit_classification`**: This component performs a digit classification task.\n    *   It takes no visible inputs.\n    *   It produces a `metrics` output artifact, which contains `RocCurveMetrics` with `falsePositiveRate` and `truePositiveRate` values, and also `ConfusionMatrixMetrics` with `labels` and `matrix` data. The `display_name` for this metric is \"metrics\".\n\n4.  **`html_visualization`**: This component creates an HTML visualization.\n    *   It takes no visible inputs.\n    *   It produces an `html_artifact` output artifact, which is an HTML file containing a simple \"Hello world\" string. The `display_name` for this artifact is \"HTML visualization\".\n\n5.  **`markdown_visualization`**: This component creates a Markdown visualization.\n    *   It takes no visible inputs.\n    *   It produces a `markdown_artifact` output artifact, which is a Markdown file containing a header \"## Hello world\". The `display_name` for this artifact is \"Markdown visualization\".\n\n**Control Flow:**\nAll five components run in parallel as they do not have any explicit dependencies on each other.\n\n**Tools/Libraries:**\nThe pipeline uses `kfp.dsl` for defining pipeline components and artifacts. The underlying classification components likely use machine learning libraries such as `scikit-learn` to generate the classification and ROC curve metrics. The visualization components likely generate simple HTML and Markdown content.", "completion": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nimport unittest.mock as mock\nfrom pprint import pprint\nimport kfp\nimport kfp_server_api\n\nfrom .metrics_visualization_v2 import metrics_visualization_pipeline\nfrom .util import run_pipeline_func, TestCase, KfpMlmdClient\nfrom ml_metadata.proto import Execution\n\n\ndef verify(run: kfp_server_api.ApiRun, mlmd_connection_config, **kwargs):\n    t = unittest.TestCase()\n    t.maxDiff = None  # we always want to see full diff\n    t.assertEqual(run.status, 'Succeeded')\n    client = KfpMlmdClient(mlmd_connection_config=mlmd_connection_config)\n    tasks = client.get_tasks(run_id=run.id)\n\n    task_names = [*tasks.keys()]\n    t.assertCountEqual(\n        task_names,\n        ['wine-classification', 'iris-sgdclassifier', 'digit-classification', 'html-visualization', 'markdown-visualization'],\n        'task names'\n    )\n\n    wine_classification = tasks['wine-classification']\n    iris_sgdclassifier = tasks['iris-sgdclassifier']\n    digit_classification = tasks['digit-classification']\n    html_visualization = tasks['html-visualization']\n    markdown_visualization = tasks['markdown-visualization']\n\n    pprint('======= wine classification task =======')\n    pprint(wine_classification.get_dict())\n    pprint('======= iris sgdclassifier task =======')\n    pprint(iris_sgdclassifier.get_dict())\n    pprint('======= digit classification task =======')\n    pprint(digit_classification.get_dict())\n    pprint('======= HTML task =======')\n    pprint(html_visualization.get_dict())\n    pprint('======= Markdown task =======')\n    pprint(markdown_visualization.get_dict())\n\n    t.assertEqual({\n        'inputs': {\n            'artifacts': [],\n            'parameters': {}\n        },\n        'name': 'wine-classification',\n        'outputs': {\n            'artifacts': [{\n                'metadata': {\n                    'display_name': 'metrics',\n                    'confidenceMetrics': {\n                        'list': [{\n                            'confidenceThreshold': 2.0,\n                            'falsePositiveRate': 0.0,\n                            'recall': 0.0\n                        }, {\n                            'confidenceThreshold': 1.0,\n                            'falsePositiveRate': 0.0,\n                            'recall': 0.33962264150943394\n                        }, {\n                            'confidenceThreshold': 0.9,\n                            'falsePositiveRate': 0.0,\n                            'recall': 0.6037735849056604\n                        }, {\n                            'confidenceThreshold': 0.8,\n                            'falsePositiveRate': 0.0,\n                            'recall': 0.8490566037735849\n                        }, {\n                            'confidenceThreshold': 0.6,\n                            'falsePositiveRate': 0.0,\n                            'recall': 0.8867924528301887\n                        }, {\n                            'confidenceThreshold': 0.5,\n                            'falsePositiveRate': 0.0125,\n                            'recall': 0.9245283018867925\n                        }, {\n                            'confidenceThreshold': 0.4,\n                            'falsePositiveRate': 0.075,\n                            'recall': 0.9622641509433962\n                        }, {\n                            'confidenceThreshold': 0.3,\n                            'falsePositiveRate': 0.0875,\n                            'recall': 1.0\n                        }, {\n                            'confidenceThreshold': 0.2,\n                            'falsePositiveRate': 0.2375,\n                            'recall': 1.0\n                        }, {\n                            'confidenceThreshold': 0.1,\n                            'falsePositiveRate': 0.475,\n                            'recall': 1.0\n                        }, {\n                            'confidenceThreshold': 0.0,\n                            'falsePositiveRate': 1.0,\n                            'recall': 1.0\n                        }]\n                    }\n                },\n                'name': 'metrics',\n                'type': 'system.ClassificationMetrics'\n            }],\n            'parameters': {}\n        },\n        'type': 'system.ContainerExecution',\n        'state': Execution.State.COMPLETE,\n    }, wine_classification.get_dict())\n    t.assertEqual(\n        {\n            'inputs': {\n                'artifacts': [],\n                'parameters': {\n                    'test_samples_fraction': 0.3\n                }\n            },\n            'name': 'iris-sgdclassifier',\n            'outputs': {\n                'artifacts': [{\n                    'metadata': {\n                        'display_name': 'metrics',\n                        'confusionMatrix': {'struct': {\n                            'annotationSpecs': [{\n                                'displayName': 'Setosa'\n                            }, {\n                                'displayName': 'Versicolour'\n                            }, {\n                                'displayName': 'Virginica'\n                            }],\n                            'rows': [{ # these numbers can be random during execution\n                                'row': [mock.ANY, mock.ANY, mock.ANY]\n                            }, {\n                                'row': [mock.ANY, mock.ANY, mock.ANY]\n                            }, {\n                                'row': [mock.ANY, mock.ANY, mock.ANY]\n                            }]\n                        }}\n                    },\n                    'name': 'metrics',\n                    'type': 'system.ClassificationMetrics'\n                }],\n                'parameters': {}\n            },\n            'type': 'system.ContainerExecution',\n            'state': Execution.State.COMPLETE,\n        }, iris_sgdclassifier.get_dict()\n    )\n    rows = iris_sgdclassifier.get_dict()['outputs']['artifacts'][0]['metadata'][\n        'confusionMatrix']['struct']['rows']\n    for i, row in enumerate(rows):\n        for j, item in enumerate(row['row']):\n            t.assertIsInstance(\n                item, float,\n                f'value of confusion matrix row {i}, col {j} is not a number'\n            )\n\n    t.assertEqual({\n        'inputs': {\n            'artifacts': [],\n            'parameters': {}\n        },\n        'name': 'digit-classification',\n        'outputs': {\n            'artifacts': [{\n                'metadata': {\n                    'display_name': 'metrics',\n                    'accuracy': 92.0,\n                },\n                'name': 'metrics',\n                'type': 'system.Metrics'\n            }],\n            'parameters': {}\n        },\n        'type': 'system.ContainerExecution',\n        'state': Execution.State.COMPLETE,\n    }, digit_classification.get_dict())\n\n    t.assertEqual({\n        'inputs': {\n            'artifacts': [], \n            'parameters': {}\n        },\n        'name': 'html-visualization',\n        'outputs': {\n            'artifacts': [{\n                'metadata': {\n                    'display_name': 'html_artifact'\n                },\n                'name': 'html_artifact',\n                'type': 'system.HTML'\n            }],\n            'parameters': {}\n        },\n        'state': Execution.State.COMPLETE,\n        'type': 'system.ContainerExecution'\n    }, html_visualization.get_dict())\n\n    t.assertEqual({\n        'inputs': {\n            'artifacts': [], \n            'parameters': {}\n        },\n        'name': 'markdown-visualization',\n        'outputs': {\n            'artifacts': [{\n                'metadata': {\n                    'display_name': 'markdown_artifact'\n                },\n                'name': 'markdown_artifact',\n                'type': 'system.Markdown'\n            }],\n            'parameters': {}\n        },\n        'state': Execution.State.COMPLETE,\n        'type': 'system.ContainerExecution'\n    }, markdown_visualization.get_dict())\n\n\nrun_pipeline_func([\n    TestCase(\n        pipeline_func=metrics_visualization_pipeline,\n        verify_func=verify,\n        mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE\n    ),\n    TestCase(\n        pipeline_func=metrics_visualization_pipeline,\n        verify_func=verify,\n        mode=kfp.dsl.PipelineExecutionMode.V2_ENGINE\n    ),\n])"}
{"prompt": "Generate a Kubeflow Pipeline named `collected_param_pipeline` that performs the following steps:\n\nThis pipeline orchestrates a larger process by calling another nested pipeline.\n\n**Pipeline Structure and Components:**\n\nThis pipeline consists of **one** top-level component, which is a call to another Kubeflow Pipeline function.\n\n1.  **`collecting_parameters` (Nested Pipeline Call):**\n    *   **Function:** This component is a call to the `collecting_parameters` Kubeflow Pipeline function.\n    *   **Inputs:** It takes `model_ids` as input, which is a string set to `'s1,s2,s3'`.\n    *   **Outputs:** The output of this nested pipeline is collected and passed to the subsequent step.\n    *   **Caching:** Caching is explicitly disabled for this component (`dag.set_caching_options(False)`).\n\n2.  **`consume_ids` (Component):**\n    *   **Function:** This component is an instance of the `consume_ids` function. It iterates through a list of IDs and prints each one.\n    *   **Inputs:** It receives a list of strings (`ids`) which is the collected output from the `collecting_parameters` nested pipeline call (`dag.output`).\n    *   **Dependencies:** This component runs *after* the `collecting_parameters` nested pipeline has completed and its output is available.\n    *   **Caching:** Caching is explicitly disabled for this component (`consume_ids_op.set_caching_options(False)`).\n\n**Nested Pipeline Details (`collecting_parameters`):**\n\nThe `collected_param_pipeline` calls a nested pipeline named `collecting_parameters` which performs the following:\n\nThis nested pipeline has a total of **four** components: `split_ids`, `prepend_id`, `consume_single_id`, and `consume_ids`.\n\n*   **Pipeline Name:** `collecting_parameters`\n*   **Pipeline Inputs:** It accepts a single string input `model_ids` with a default empty string value.\n*   **Pipeline Outputs:** It returns a list of strings, specifically the `prepend_id_op.output` collected from the `ParallelFor` loop.\n\n**Nested Pipeline Components and Control Flow:**\n\n1.  **`split_ids` (Component):**\n    *   **Function:** Splits a comma-separated string of IDs into a list of strings.\n    *   **Inputs:** Takes `ids` (a string) from the pipeline input `model_ids`.\n    *   **Outputs:** Returns a `list` of strings.\n\n2.  **`ParallelFor` Loop:**\n    *   **Control Flow:** Iterates in parallel over each `model_id` in the `list` output from the `split_ids` component (`ids_split_op.output`).\n\n    *   **Inside the `ParallelFor` Loop:**\n        *   **`prepend_id` (Component):**\n            *   **Function:** Prepends the string 'model_id_' to a given content string.\n            *   **Inputs:** Takes `content` (a string) which is the current `model_id` from the `ParallelFor` loop.\n            *   **Outputs:** Returns a `string` (e.g., 'model_id_s1').\n            *   **Caching:** Caching is explicitly disabled for this component.\n        *   **`consume_single_id` (Component):**\n            *   **Function:** Consumes (prints) a single ID.\n            *   **Inputs:** Takes `id` (a string) which is the output from the `prepend_id` component (`prepend_id_op.output`).\n            *   **Dependencies:** Runs *after* `prepend_id_op` for the current `model_id`.\n            *   **Caching:** Caching is explicitly disabled for this component.\n\n3.  **`consume_ids` (Component):**\n    *   **Function:** Iterates through a list of IDs and prints each one.\n    *   **Inputs:** Takes a `List[str]` named `ids`. This list is a `dsl.Collected` object, meaning it gathers all the outputs of `prepend_id_op.output` from all iterations of the `ParallelFor` loop.\n    *   **Dependencies:** This component implicitly runs *after* the `ParallelFor` loop has completed all its iterations and collected all `prepend_id_op.output` values.\n    *   **Caching:** Caching is explicitly disabled for this component.\n\n**General Requirements:**\n\n*   Use `kfp.dsl` for pipeline and component definitions.\n*   The code should be runnable as a Kubeflow Pipeline.\n*   Include standard imports: `typing.List`, `kfp`, `kfp.dsl`.\n*   Include the `if __name__ == '__main__':` block to compile and run the pipeline using `kfp.Client`, creating a run for `collected_param_pipeline` with caching disabled.", "completion": "from typing import List\n\nimport kfp\nfrom kfp import dsl\n\n\n@dsl.component()\ndef split_ids(ids: str) -> list:\n    return ids.split(',')\n\n\n@dsl.component()\ndef prepend_id(content: str) -> str:\n    print(f\"prepending: {content} with 'model_id'\")\n    return f'model_id_{content}'\n\n\n@dsl.component()\ndef consume_ids(ids: List[str]) -> str:\n    for id in ids:\n        print(f'Consuming: {id}')\n    return 'completed'\n\n\n@dsl.component()\ndef consume_single_id(id: str) -> str:\n    print(f'Consuming single: {id}')\n    return 'completed'\n\n\n@dsl.pipeline()\ndef collecting_parameters(model_ids: str = '',) -> List[str]:\n    ids_split_op = split_ids(ids=model_ids)\n    with dsl.ParallelFor(ids_split_op.output) as model_id:\n        prepend_id_op = prepend_id(content=model_id)\n        prepend_id_op.set_caching_options(False)\n        \n        consume_single_id_op = consume_single_id(id=prepend_id_op.output)\n        consume_single_id_op.set_caching_options(False)\n    \n    consume_ids_op = consume_ids(ids=dsl.Collected(prepend_id_op.output))\n    consume_ids_op.set_caching_options(False)\n\n    return dsl.Collected(prepend_id_op.output)\n\n\n@dsl.pipeline()\ndef collected_param_pipeline():\n    model_ids = 's1,s2,s3'\n    dag = collecting_parameters(model_ids=model_ids)\n    dag.set_caching_options(False)\n    \n    consume_ids_op = consume_ids(ids=dag.output)\n    consume_ids_op.set_caching_options(False)\n\nif __name__ == '__main__':\n    client = kfp.Client()\n    run = client.create_run_from_pipeline_func(\n        collected_param_pipeline,\n        arguments={},\n        enable_caching=False,\n    )"}
{"prompt": "Generate a Kubeflow Pipeline named `pipeline-with-after` that performs the following steps:\n\nThis pipeline consists of **three** components, all of which are instances of a single reusable component named `Print Text`.\n\nThe `Print Text` component takes one input, `text` (String type), and prints its value. Its implementation uses an `alpine` container and executes a shell command to echo the input text. This component is loaded from text using `kfp.components.load_component_from_text`.\n\nThe control flow of the pipeline is as follows:\n1.  **`task1`**: This task is an instance of `Print Text` with the input `text` set to `'1st task'`. It runs without any dependencies.\n2.  **`task2`**: This task is an instance of `Print Text` with the input `text` set to `'2nd task'`. It has an explicit dependency on `task1`, meaning it will only start after `task1` has successfully completed.\n3.  **`task3`**: This task is an instance of `Print Text` with the input `text` set to `'3rd task'`. It has explicit dependencies on both `task1` and `task2`, meaning it will only start after both `task1` and `task2` have successfully completed.\n\nThe pipeline uses the `kfp` library for defining the pipeline and components, specifically `kfp.dsl` for the pipeline definition and `kfp.components` for loading the reusable component.", "completion": "# Copyright 2020 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom kfp import components\nfrom kfp import dsl\nfrom kfp import compiler\n\ncomponent_op = components.load_component_from_text(\"\"\"\nname: Print Text\ninputs:\n- {name: text, type: String}\nimplementation:\n  container:\n    image: alpine\n    command:\n    - sh\n    - -c\n    - |\n      set -e -x\n      echo \"$0\"\n    - {inputValue: text}\n\"\"\")\n\n\n@dsl.pipeline(name='pipeline-with-after')\ndef my_pipeline():\n    task1 = component_op(text='1st task')\n    task2 = component_op(text='2nd task').after(task1)\n    task3 = component_op(text='3rd task').after(task1, task2)\n\n\nif __name__ == '__main__':\n    compiler.Compiler().compile(\n        pipeline_func=my_pipeline, output_path=__file__ + '.json')"}
{"prompt": "Generate a Kubeflow Pipeline named `[PROJECT_NAME]` that performs the following:\n\nThe pipeline consists of a single component: `[PIPELINE_NAME]`.\n\n- **`[PIPELINE_NAME]` Component**:\n    - **Function**: This component executes a Kedro pipeline. It runs a Kedro command `kedro run --env [KEDRO_ENV] --pipeline [PIPELINE_NAME] --config config.yaml`.\n    - **Image**: Uses the `[IMAGE]` Docker image.\n    - **Image Pull Policy**: `[IMAGE_PULL_POLICY]`\n    - **Inputs**: It accepts a dictionary of parameters as arguments, derived from the merged Kedro context parameters (`params.keys()`).\n    - **Outputs**: It produces file outputs based on the Kedro catalog. For each catalog entry that has a `filepath` attribute and is a local file system path, and if `store_kedro_outputs_as_kfp_artifacts` is enabled in the run configuration, it maps the Kedro output name to its corresponding file path within the container (e.g., `/home/kedro/path/to/output.csv`).\n    - **Environment Variables**: Sets container environment variables, likely related to Kedro's execution.\n    - **Caching**: Configured with a `max_cache_staleness` set by `run_config.max_cache_staleness`.\n    - **Customization**: The operation is further customized based on the `image_pull_policy` and `run_config`.\n\n**Pipeline Control Flow**:\n- The pipeline defines an exit handler that will execute cleanup or post-pipeline actions, even if the main component fails. This handler is configured with the pipeline name, image, image pull policy, run configuration, and Kedro context.\n- The `[PIPELINE_NAME]` component is the only step within this exit handler's scope.\n\n**Global Pipeline Configuration**:\n- **Name**: The pipeline's name is dynamically set as `[PROJECT_NAME]`.\n- **Description**: The pipeline's description is dynamically set by `run_config.description`.\n- **TTL**: Sets `ttl_seconds_after_finished` for the pipeline run based on `run_config.ttl`.\n- **Parameters**: The pipeline itself can accept parameters if they are present in the merged Kedro context parameters.\n- **Tools/Libraries Used**: This pipeline heavily relies on **Kedro** for its execution logic and **KFP (Kubeflow Pipelines)** for orchestration. It interacts with Kedro's context, parameters, and catalog to define its operations and artifacts.", "completion": "import logging\n\nfrom kedro.framework.context import KedroContext\nfrom kfp import dsl\n\nfrom ..utils import clean_name\nfrom .utils import (\n    create_arguments_from_parameters,\n    create_command_using_params_dumper,\n    create_container_environment,\n    create_pipeline_exit_handler,\n    customize_op,\n    is_local_fs,\n    maybe_add_params,\n    merge_namespaced_params_to_dict,\n)\n\n\nclass OnePodPipelineGenerator(object):\n    log = logging.getLogger(__name__)\n\n    def __init__(self, config, project_name, context):\n        self.project_name = project_name\n        self.context: KedroContext = context\n        dsl.ContainerOp._DISABLE_REUSABLE_COMPONENT_WARNING = True\n        self.run_config = config.run_config\n        self.catalog = context.config_loader.get(\"catalog\")\n\n    def generate_pipeline(self, pipeline, image, image_pull_policy):\n        merged_params = merge_namespaced_params_to_dict(self.context.params)\n\n        @dsl.pipeline(self.project_name, self.run_config.description)\n        @maybe_add_params(merged_params)\n        def convert_kedro_pipeline_to_kfp() -> None:\n            dsl.get_pipeline_conf().set_ttl_seconds_after_finished(self.run_config.ttl)\n            with create_pipeline_exit_handler(\n                pipeline,\n                image,\n                image_pull_policy,\n                self.run_config,\n                self.context,\n            ):\n                self._build_kfp_op(pipeline, merged_params, image, image_pull_policy)\n\n        return convert_kedro_pipeline_to_kfp\n\n    def _build_kfp_op(\n        self,\n        pipeline,\n        params,\n        image,\n        image_pull_policy,\n    ) -> dsl.ContainerOp:\n        container_op = dsl.ContainerOp(\n            name=clean_name(pipeline),\n            image=image,\n            command=create_command_using_params_dumper(\n                \"kedro \" \"run \" f\"--env {self.context.env} \" f\"--pipeline {pipeline} \" f\"--config config.yaml\"\n            ),\n            arguments=create_arguments_from_parameters(params.keys()),\n            container_kwargs={\"env\": create_container_environment()},\n            file_outputs={\n                output: f\"/home/kedro/{self.catalog[output]['filepath']}\"\n                for output in self.catalog\n                if \"filepath\" in self.catalog[output]\n                and is_local_fs(self.catalog[output][\"filepath\"])\n                and self.run_config.store_kedro_outputs_as_kfp_artifacts\n            },\n        )\n\n        container_op.execution_options.caching_strategy.max_cache_staleness = self.run_config.max_cache_staleness\n\n        return customize_op(container_op, image_pull_policy, self.run_config)"}
{"prompt": "Generate a Kubeflow Pipeline named `Tacos vs. Burritos` that performs a simple TensorFlow Convolutional Neural Network (CNN) training workflow.\n\nThe pipeline consists of **three primary components**:\n1.  **Databricks Data Processing**: This component is an `init_container` that runs a Databricks notebook.\n    *   **Function**: It's likely responsible for initial data preparation or triggering a Databricks job.\n    *   **Inputs**:\n        *   `r`: Takes the Kubeflow `RUN_ID_PLACEHOLDER` as an argument.\n        *   `p`: Takes a JSON string `{\"argument_one\":\"param one\",\"argument_two\":\"param two\"}` as an argument.\n    *   **Dependencies**: This component is the first to run and has an `init_container` named `callback` that sends a \"Training Started\" event.\n    *   **Tools/Libraries**: Integrates with Databricks using a custom secret helper `use_databricks_secret()`.\n\n2.  **TensorFlow Preprocess**: This component handles data preprocessing for the TensorFlow model.\n    *   **Function**: Prepares image data for training, including unzipping, resizing, and generating a dataset file.\n    *   **Inputs**:\n        *   `--base_path`: Uses `/mnt/azure` as the base path for data.\n        *   `--data`: Specifies `train` as the data folder.\n        *   `--target`: Specifies `train.txt` as the target dataset file.\n        *   `--img_size`: Sets the image size to `160`.\n        *   `--zipfile`: Takes the `dataset` pipeline parameter, which represents the zip file containing images.\n    *   **Dependencies**: This component runs **after** the \"Databricks Data Processing\" component.\n    *   **Tools/Libraries**: Uses Python for data processing.\n\n3.  **Exit Handler**: This component is a special `ExitHandler` that runs regardless of the pipeline's success or failure.\n    *   **Function**: Sends a \"Training Finished\" event to a specified callback URL.\n    *   **Inputs**:\n        *   `-d`: Takes a JSON payload generated by the `get_callback_payload` function, indicating a `TRAIN_FINISH_EVENT` and including workflow status, SHA, PR number, and run ID.\n        *   Callback URL: `kubemlopsbot-svc.kubeflow.svc.cluster.local:8080`.\n    *   **Control Flow**: Executes upon pipeline completion (success or failure) as part of the `dsl.ExitHandler`.\n\nThe pipeline also defines a utility function `get_callback_payload(event_type)` that generates a JSON payload for callback events, including `event_type`, `sha`, `pr_num`, `run_id`, and `status` (for finish events).\n\n**Overall Pipeline Flow**:\n*   The pipeline starts by initiating an `ExitHandler` to ensure a \"Training Finished\" callback is sent upon completion.\n*   Inside the `ExitHandler` context, a `callback` `init_container` sends a \"Training Started\" event.\n*   The \"Databricks Data Processing\" component runs first, initiated with the start callback.\n*   Upon successful completion of \"Databricks Data Processing\", the \"TensorFlow Preprocess\" component executes.\n*   Finally, the \"Exit Handler\" component is triggered when the pipeline finishes, regardless of the success of preceding steps.\n\n**Pipeline Parameters**:\n*   `resource_group`: String, no default.\n*   `workspace`: String, no default.\n*   `dataset`: String, no default (represents the data zip file).\n*   `token`: String, no default.\n\n**Kubernetes and Azure Integration**:\n*   The pipeline leverages Kubernetes client for potential future resource management.\n*   It uses custom secret helpers `use_databricks_secret()` and `use_azstorage_secret()` (though only `use_databricks_secret` is explicitly used in the provided snippet).\n*   It defines a `persistent_volume_path` as `/mnt/azure`.\n*   Container images are sourced from `k8scc01covidmlopsacr.azurecr.io/mlops`.\n*   Environment variables like `GITHUB_SHA` and `PR_NUM` are accessed within the `get_callback_payload` function.", "completion": "\"\"\"Convolutional Neural Network (CNN) Pipeline\"\"\"\nfrom kubernetes import client as k8s_client\nimport kfp.dsl as dsl\nimport kfp.compiler as compiler\nfrom kfp.azure import use_azure_secret\nimport json\nimport os\nfrom kubernetes.client.models import V1EnvVar\nfrom utils.kubernetes.secret import use_databricks_secret, use_azstorage_secret  # noqa: E501\n\n# Initially derived from https://github.com/kaizentm/kubemlops\n\n\nTRAIN_START_EVENT = \"Training Started\"\nTRAIN_FINISH_EVENT = \"Training Finished\"\n\n\n@dsl.pipeline(\n    name='Tacos vs. Burritos',\n    description='Simple TF CNN'\n)\ndef get_callback_payload(event_type):\n    payload = {}\n    payload['event_type'] = event_type\n    payload['sha'] = os.getenv('GITHUB_SHA')\n    payload['pr_num'] = os.getenv('PR_NUM')\n    payload['run_id'] = dsl.RUN_ID_PLACEHOLDER\n    if (event_type == TRAIN_FINISH_EVENT):\n        payload['status'] = '{{workflow.status}}'\n    return json.dumps(payload)\n\n\ndef cnn_train(\n    resource_group,\n    workspace,\n    dataset,\n    token\n):\n    \"\"\"Pipeline steps\"\"\"\n\n    persistent_volume_path = '/mnt/azure'\n    data_download = dataset  # noqa: E501\n    batch = 32\n    model_name = 'cnnmodel'\n    operations = {}\n    image_size = 160\n    training_folder = 'train'\n    training_dataset = 'train.txt'\n    model_folder = 'model'\n    image_repo_name = \"k8scc01covidmlopsacr.azurecr.io/mlops\"\n    callback_url = 'kubemlopsbot-svc.kubeflow.svc.cluster.local:8080'\n    mlflow_url = 'http://mlflow.mlflow:5000'\n\n    exit_op = dsl.ContainerOp(\n        name='Exit Handler',\n        image=\"curlimages/curl\",\n        command=['curl'],\n        arguments=[\n            '-d', get_callback_payload(TRAIN_FINISH_EVENT),\n            callback_url\n        ]\n    )\n\n    with dsl.ExitHandler(exit_op):\n        start_callback = \\\n            dsl.UserContainer('callback',\n                              'curlimages/curl',\n                              command=['curl'],\n                              args=['-d',\n                                    get_callback_payload(TRAIN_START_EVENT), callback_url])  # noqa: E501\n\n        operations['databricks data processing'] = dsl.ContainerOp(\n            name='databricks data processing',\n            init_containers=[start_callback],\n            image=image_repo_name + '/databricks-notebook:latest',\n            arguments=[\n                '-r', dsl.RUN_ID_PLACEHOLDER,\n                '-p', '{\"argument_one\":\"param one\",\"argument_two\":\"param two\"}'\n            ]\n        ).apply(use_databricks_secret())\n\n        operations['tensorflow preprocess'] = dsl.ContainerOp(\n            name='tensorflow preprocess',\n            image=image_repo_name + '/tensorflow-preprocess:latest',\n            command=['python'],\n            arguments=[\n                '/scripts/data.py',\n                '--base_path', persistent_volume_path,\n                '--data', training_folder,\n                '--target', training_dataset,\n                '--img_size', image_size,\n                '--zipfile', data_download\n            ]\n        )\n\n        operations['tensorflow preprocess'].after(operations['databricks data processing'])  # noqa: E501\n\n        operations['tensorflow training'] = dsl.ContainerOp(\n            name=\"tensorflow training\",\n            image=image_repo_name + '/tensorflow-training:latest',\n            command=['python'],\n            arguments=[\n                '/scripts/train.py',\n                '--base_path', persistent_volume_path,\n                '--data', training_folder,\n                '--epochs', 2,\n                '--batch', batch,\n                '--image_size', image_size,\n                '--lr', 0.0001,\n                '--outputs', model_folder,\n                '--dataset', training_dataset\n            ],\n            output_artifact_paths={\n                'mlpipeline-metrics': '/mlpipeline-metrics.json',\n                'mlpipeline-ui-metadata': '/mlpipeline-ui-metadata.json'\n            }\n            ).apply(use_azstorage_secret()).add_env_variable(V1EnvVar(name=\"RUN_ID\", value=dsl.RUN_ID_PLACEHOLDER)).add_env_variable(V1EnvVar(name=\"MLFLOW_TRACKING_TOKEN\", value=token)).add_env_variable(V1EnvVar(name=\"MLFLOW_TRACKING_URI\", value=mlflow_url)).add_env_variable(V1EnvVar(name=\"GIT_PYTHON_REFRESH\", value='quiet'))  # noqa: E501\n\n        operations['tensorflow training'].after(operations['tensorflow preprocess'])  # noqa: E501\n\n        operations['evaluate'] = dsl.ContainerOp(\n            name='evaluate',\n            image=\"busybox\",\n            command=['sh', '-c'],\n            arguments=[\n                'echo',\n                'Life is Good!'\n            ]\n\n        )\n        operations['evaluate'].after(operations['tensorflow training'])\n\n        operations['register kubeflow'] = dsl.ContainerOp(\n            name='register kubeflow',\n            image=image_repo_name + '/register-kubeflow-artifacts:latest',\n            command=['python'],\n            arguments=[\n                '/scripts/register.py',\n                '--base_path', persistent_volume_path,\n                '--model', 'latest.h5',\n                '--model_name', model_name,\n                '--data', training_folder,\n                '--dataset', training_dataset,\n                '--run_id', dsl.RUN_ID_PLACEHOLDER\n            ]\n        ).apply(use_azure_secret())\n        operations['register kubeflow'].after(operations['evaluate'])\n\n        operations['register AML'] = dsl.ContainerOp(\n            name='register AML',\n            image=image_repo_name + '/register-aml:latest',\n            command=['python'],\n            arguments=[\n                '/scripts/register.py',\n                '--base_path', persistent_volume_path,\n                '--model', 'latest.h5',\n                '--model_name', model_name,\n                '--tenant_id', \"$(AZ_TENANT_ID)\",\n                '--service_principal_id', \"$(AZ_CLIENT_ID)\",\n                '--service_principal_password', \"$(AZ_CLIENT_SECRET)\",\n                '--subscription_id', \"$(AZ_SUBSCRIPTION_ID)\",\n                '--resource_group', resource_group,\n                '--workspace', workspace,\n                '--run_id', dsl.RUN_ID_PLACEHOLDER\n            ]\n        ).apply(use_azure_secret())\n        operations['register AML'].after(operations['register kubeflow'])\n\n        operations['register mlflow'] = dsl.ContainerOp(\n            name='register mlflow',\n            image=image_repo_name + '/register-mlflow:latest',\n            command=['python'],\n            arguments=[\n                '/scripts/register.py',\n                '--model', 'model',\n                '--model_name', model_name,\n                '--experiment_name', 'mlops',\n                '--run_id', dsl.RUN_ID_PLACEHOLDER\n            ]\n        ).apply(use_azure_secret()).add_env_variable(V1EnvVar(name=\"MLFLOW_TRACKING_URI\", value=mlflow_url)).add_env_variable(V1EnvVar(name=\"MLFLOW_TRACKING_TOKEN\", value=token))  # noqa: E501\n        operations['register mlflow'].after(operations['register AML'])\n\n        operations['finalize'] = dsl.ContainerOp(\n            name='Finalize',\n            image=\"curlimages/curl\",\n            command=['curl'],\n            arguments=[\n                '-d', get_callback_payload(\"Model is registered\"),\n                callback_url\n            ]\n        )\n        operations['finalize'].after(operations['register mlflow'])\n\n    for _, op_1 in operations.items():\n        op_1.container.set_image_pull_policy(\"Always\")\n        op_1.add_volume(\n            k8s_client.V1Volume(\n              name='azure',\n              persistent_volume_claim=k8s_client.V1PersistentVolumeClaimVolumeSource(  # noqa: E501\n                claim_name='azure-managed-file')\n            )\n        ).add_volume_mount(k8s_client.V1VolumeMount(\n            mount_path='/mnt/azure', name='azure'))\n\n\nif __name__ == '__main__':\n    compiler.Compiler().compile(cnn_train, __file__ + '.tar.gz')"}
{"prompt": "Generate a Kubeflow Pipeline named `pipeline-with-after` that performs the following steps:\n\nThis pipeline consists of **three components**. All components use the `Print Text` component, which is loaded from a YAML-like string and prints a given text input.\n\n**Component 1: `task1`**\n*   **Function:** Prints the string \"1st task\".\n*   **Input:** `text='1st task'`\n\n**Component 2: `task2`**\n*   **Function:** Prints the string \"2nd task\".\n*   **Input:** `text='2nd task'`\n*   **Control Flow:** This task must execute **after** `task1` has completed successfully.\n\n**Component 3: `task3`**\n*   **Function:** Prints the string \"3rd task\".\n*   **Input:** `text='3rd task'`\n*   **Control Flow:** This task must execute **after** both `task1` and `task2` have completed successfully.\n\nThe pipeline utilizes the `kfp` library, specifically `kfp.v2.dsl` for pipeline definition and `kfp.v2.compiler` for compilation. The `Print Text` component is loaded using `components.load_component_from_text`.", "completion": "# Copyright 2020 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom kfp import components\nfrom kfp.v2 import dsl\nimport kfp.v2.compiler as compiler\n\ncomponent_op = components.load_component_from_text(\n    \"\"\"\nname: Print Text\ninputs:\n- {name: text, type: String}\nimplementation:\n  container:\n    image: alpine\n    command:\n    - sh\n    - -c\n    - |\n      set -e -x\n      echo \"$0\"\n    - {inputValue: text}\n\"\"\"\n)\n\n\n@dsl.pipeline(name='pipeline-with-after')\ndef my_pipeline():\n    task1 = component_op(text='1st task')\n    task2 = component_op(text='2nd task').after(task1)\n    task3 = component_op(text='3rd task').after(task1, task2)\n\n\nif __name__ == '__main__':\n    compiler.Compiler().compile(\n        pipeline_func=my_pipeline,\n        pipeline_root='dummy_root',\n        output_path=__file__ + '.json'\n    )"}
{"prompt": "Generate a Kubeflow Pipeline named `Docker test` that performs the following steps:\n\n**Pipeline Description:**\nThe pipeline applies Decision Tree and Logistic Regression for a classification problem.\n\n**Components:**\nThe pipeline consists of **four** components:\n1.  **`getdata`**: This component is loaded from the YAML manifest located at `getdata/getdata.yaml`. Its function is to retrieve data.\n2.  **`reshapedata`**: This component is loaded from the YAML manifest located at `reshapedata/reshapedata.yaml`. Its function is to reshape the retrieved data.\n3.  **`modelbuilding`**: This component is loaded from the YAML manifest located at `modelbuilding/modelbuilding.yaml`. Its function is to build machine learning models, specifically applying Decision Tree and Logistic Regression.\n4.  **`kserve_op`**: This component is loaded from the URL `https://raw.githubusercontent.com/kubeflow/pipelines/master/components/kserve/component.yaml`. Its function is to deploy a model using KServe. It takes the following inputs:\n    *   `action`: 'apply'\n    *   `model_name`: 'tensorflow-sample'\n    *   `model_uri`: 's3://mlpipeline/mnistdocker/models/detect-digits/'\n    *   `namespace`: 'kubeflow-user-example-com'\n    *   `framework`: 'tensorflow'\n    *   `service_account`: 'sa-minio-kserve'\n\n**Control Flow (Dependencies):**\nThe pipeline executes sequentially with explicit dependencies:\n*   `step1` (executing `getdata`) runs first.\n*   `step2` (executing `reshapedata`) runs **after** `step1` completes.\n*   `step3` (executing `modelbuilding`) runs **after** `step2` completes.\n*   The `kserve_op` component runs **after** `step3` completes.\n\n**Compilation:**\nThe pipeline should be compiled into a YAML file named `dockerten1Pipeline.yaml`.\n\n**Tools/Libraries:**\nThe pipeline primarily uses Kubeflow Pipelines (`kfp`) for orchestration and leverages KServe for model deployment. The model building step implicitly uses libraries for Decision Tree and Logistic Regression, though specific libraries like scikit-learn are not explicitly mentioned in the provided snippet. The `model_uri` suggests interaction with S3-compatible storage.", "completion": "import kfp\nfrom kfp import dsl\nimport kfp.components as components\n                                           \n\n@dsl.pipeline(name='Docker test', description='Applies Decision Tree and Logistic Regression for classification problem.')\ndef first_pipeline():\n\n    # Loads the yaml manifest for each component\n    getdata = kfp.components.load_component_from_file('getdata/getdata.yaml')\n    reshapedata = kfp.components.load_component_from_file('reshapedata/reshapedata.yaml')\n    modelbuilding = kfp.components.load_component_from_file('modelbuilding/modelbuilding.yaml')\n    kserve_op = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/'\n                                                'master/components/kserve/component.yaml') \n \n    # pipeline steps\n    step1 = getdata()\n    step2 = reshapedata()\n    step2.after(step1)\n\n    step3 = modelbuilding()\n    step3.after(step2)\n\n\n    kserve_op(action='apply',\n            model_name='tensorflow-sample',\n            model_uri='s3://mlpipeline/mnistdocker/models/detect-digits/',\n            namespace='kubeflow-user-example-com',\n            framework='tensorflow',\n            service_account='sa-minio-kserve').after(step3)\n\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(first_pipeline, 'dockerten1Pipeline.yaml')\n    # kfp.Client().create_run_from_pipeline_func(basic_pipeline, arguments={})"}
{"prompt": "I need the actual Python code for the `my_pipeline` function and its components to generate an accurate prompt. The provided snippet only shows how the pipeline is called, but not its definition.\n\nPlease provide the content of `9rince_kfp_samples_test_after_test.py` or at least the definitions of `my_pipeline` and any `@component` functions it uses, including their inputs, outputs, and internal logic (even if it's just `print` statements).\n\nOnce I have the full code, I can provide the detailed prompt.", "completion": "# Copyright 2021 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp\nfrom .after import my_pipeline\nfrom .util import run_pipeline_func, TestCase\n\nrun_pipeline_func([\n    TestCase(pipeline_func=my_pipeline),\n    TestCase(\n        pipeline_func=my_pipeline, mode=kfp.dsl.PipelineExecutionMode.V2_ENGINE\n    ),\n])"}
{"prompt": "Generate a Kubeflow Pipeline named `one-step-pipeline-with-concat-placeholder` that performs the following steps:\n\n**Pipeline Overview:**\nThis pipeline consists of a single custom component that demonstrates the use of the `concat` placeholder within its arguments.\n\n**Components:**\n\n1.  **`Component with concat placeholder`**\n    *   **Function:** This component takes two string inputs, `input_one` and `input_two`, concatenates them with a literal `+` and `=three`, and then echoes the resulting string to a temporary file `/tmp/test`. It also asserts that the concatenated string is exactly `one+two=three`.\n    *   **Inputs:**\n        *   `input_one` (type: String): Provided with the value `'one'`.\n        *   `input_two` (type: String): Provided with the value `'two'`.\n    *   **Implementation Details:**\n        *   **Image:** `registry.k8s.io/busybox`\n        *   **Command:** `sh`, `-ec`\n        *   **Arguments:**\n            *   `echo \"$0\" > /tmp/test && [[ \"$0\" == 'one+two=three' ]]` (positional argument)\n            *   `concat: [{inputValue: input_one}, '+', {inputValue: input_two}, '=three']` (this argument uses the `concat` placeholder to construct a single string from `input_one`, `'+'`, `input_two`, and `'=three'`)\n\n**Control Flow:**\n*   The pipeline executes the `component_op` directly, passing the specified input values. There are no dependencies or complex control flow constructs (e.g., parallelFor, after).\n\n**Libraries Used:**\n*   `kfp.components` for loading the component definition.\n*   `kfp.dsl` for defining the pipeline.", "completion": "# Copyright 2020 The Kubeflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom kfp import components\nfrom kfp import dsl\nimport kfp.compiler as compiler\n\ncomponent_op = components.load_component_from_text(\"\"\"\nname: Component with concat placeholder\ninputs:\n- {name: input_one, type: String}\n- {name: input_two, type: String}\nimplementation:\n  container:\n    image: registry.k8s.io/busybox\n    command:\n    - sh\n    - -ec\n    args:\n    - echo \"$0\" > /tmp/test && [[ \"$0\" == 'one+two=three' ]]\n    - concat: [{inputValue: input_one}, '+', {inputValue: input_two}, '=three']\n\"\"\")\n\n\n@dsl.pipeline(name='one-step-pipeline-with-concat-placeholder')\ndef pipeline_with_concat_placeholder():\n    component = component_op(input_one='one', input_two='two')"}
{"prompt": "Generate a Kubeflow Pipeline named `head-pose-dataset pipeline` that performs the following steps to create a TFRecord dataset for a head-pose-pipeline.\n\nThe pipeline takes the following parameters:\n- `pipeline_name` (string, default: \"head-pose-dataset-pipeline\"): The name of the pipeline.\n- `bucket_name` (string, default: \"kfp-project\"): The GCS bucket name.\n- `job_id` (string, default: \"{{JOB_ID}}\"): The unique ID for the current job run.\n- `dataset` (string, default: \"\"): The path to the input dataset.\n- `chunk_size` (integer, default: 1000): The size of data chunks for splitting.\n- `valid_ratio` (float, default: 0.1): The ratio for the validation dataset.\n\nThe pipeline consists of three components:\n\n1.  **`data-chunk-spliter`**:\n    *   **Function:** Splits the input dataset into smaller chunks.\n    *   **Inputs:** `pipeline_name`, `bucket_name`, `job_id`, `dataset`, `chunk_size`.\n    *   **Outputs:** A list of chunk file paths.\n    *   **Configuration:**\n        *   Applied to use a preemptible node pool (`gcp.use_preemptible_nodepool()`).\n        *   Configured to retry up to 2 times (`set_retry(num_retries=2)`).\n\n2.  **`pose-annotation`**:\n    *   **Function:** Processes each data chunk to perform pose annotation and generate TFRecord files.\n    *   **Inputs:** `pipeline_name`, `bucket_name`, `job_id`, `valid_ratio`, `chunk_file` (an item from the output of `data-chunk-spliter`).\n    *   **Configuration:**\n        *   Applied to use a preemptible node pool (`gcp.use_preemptible_nodepool()`).\n        *   Configured to retry up to 2 times (`set_retry(num_retries=2)`).\n\n3.  **`slack-notification`**:\n    *   **Function:** Sends a Slack notification about the pipeline's status upon completion or failure.\n    *   **Inputs:** `pipeline_name`, `job_id`, `message` (set to \"Status: {{workflow.status}}\").\n\n**Control Flow:**\n\n*   The entire pipeline execution is wrapped in an `dsl.ExitHandler`. If the pipeline completes (successfully or with failure), the `slack_notification_op` component will be executed, sending a message including the workflow status.\n*   The `data-chunk-spliter` component (`split_task`) is executed first.\n*   Once `split_task` completes, its output (the list of chunk file paths) is used as the input for a `dsl.ParallelFor` loop.\n*   Inside the `dsl.ParallelFor` loop, the `pose-annotation` component is executed concurrently for each `item` (chunk file) produced by `split_task`.\n\nThe pipeline utilizes Kubeflow Pipelines (KFP) for orchestration and integrates with Google Cloud Platform (GCP) for resource management. Component definitions are loaded from a `kfp.components.ComponentStore`.", "completion": "import kfp\nfrom kfp import dsl\nfrom kfp import gcp\n\n# Initialize component store\ncomponent_store = kfp.components.ComponentStore(\n    local_search_paths=[\"pipelines/head-pose-dataset-pipeline\", \"components\"])\n\n# Create component factories\npose_annotation_op = component_store.load_component(\"pose-annotation\")\ndata_chunk_spliter_op = component_store.load_component(\"data-chunk-spliter\")\nslack_notification_op = component_store.load_component(\"slack-notification\")\n\n\n# Define pipeline\n@dsl.pipeline(\n    name=\"head-pose-dataset pipeline\",\n    description=\"Create tfrecord dataset for head-pose-pipeline.\"\n)\ndef pipeline(\n    pipeline_name: str = \"head-pose-dataset-pipeline\",\n    bucket_name: str = \"kfp-project\",\n    job_id: str = \"{{JOB_ID}}\",\n    dataset: str = \"\",\n    chunk_size: int = 1000,\n    valid_ratio: float = 0.1,\n):\n\n    with dsl.ExitHandler(\n        exit_op=slack_notification_op(\n            pipeline_name=pipeline_name,\n            job_id=job_id,\n            message=\"Status: {{workflow.status}}\"\n        )\n    ):\n        split_task = data_chunk_spliter_op(\n            pipeline=pipeline_name,\n            bucket_name=bucket_name,\n            job_id=job_id,\n            dataset=dataset,\n            chunk_size=chunk_size,\n        ).apply(gcp.use_preemptible_nodepool()) \\\n            .set_retry(num_retries=2)\n\n        with dsl.ParallelFor(split_task.output) as item:\n            pose_annotation_op(\n                pipeline=pipeline_name,\n                bucket_name=bucket_name,\n                job_id=job_id,\n                valid_ratio=valid_ratio,\n                chunk_file=item\n            ).apply(gcp.use_preemptible_nodepool()) \\\n                .set_retry(num_retries=2)\n\n\nif __name__ == \"__main__\":\n    kfp.compiler.Compiler().compile(pipeline, \"head-pose-dataset-pipeline.yaml\")"}
{"prompt": "Generate a Kubeflow Pipeline named `train_until_good` that performs continuous model training and evaluation until a satisfactory model performance is achieved.\n\nThe pipeline consists of a total of **8 components**, including a recursive graph component.\n\n**Component Breakdown:**\n\n1.  **`chicago_taxi_dataset_op`**: This component loads the Chicago Taxi Trips dataset.\n    *   **Function:** Provides the raw dataset for the pipeline.\n    *   **Outputs:** `output_data` (CSV file containing the dataset).\n\n2.  **`pandas_transform_csv_op`**: This component transforms the CSV data using pandas.\n    *   **Function:** Likely performs data cleaning or feature engineering on the dataset.\n    *   **Inputs:** `dataframe` (from `chicago_taxi_dataset_op.output_data`), `transform_code` (Python code for transformation).\n    *   **Outputs:** `transformed_dataframe` (transformed CSV data).\n\n3.  **`drop_header_op` (Training Data)**: This component removes the header from the transformed training data.\n    *   **Function:** Prepares the training data for model consumption by removing the header row.\n    *   **Inputs:** `table` (from `pandas_transform_csv_op.output`), `has_header` (set to `True`).\n    *   **Outputs:** `table_without_header` (CSV data without header).\n\n4.  **`drop_header_op` (True Values)**: This component removes the header from the true values data.\n    *   **Function:** Isolates the true target values from the dataset, preparing them for evaluation.\n    *   **Inputs:** `table` (from `pandas_transform_csv_op.output`), `has_header` (set to `True`), `column_index` (set to `0`).\n    *   **Outputs:** `table_without_header` (CSV data representing true values without header).\n\n5.  **`xgboost_train_on_csv_op`**: This component performs initial XGBoost model training.\n    *   **Function:** Trains an initial model on the processed training data.\n    *   **Inputs:** `training_data` (from `drop_header_op` for training), `label_column` (set to `0`), `objective` (set to `'reg:squarederror'`), `num_iterations` (set to `50`).\n    *   **Outputs:** `model` (the trained XGBoost model).\n\n6.  **`train_until_low_error` (Graph Component)**: This is a recursive graph component that continuously trains, evaluates, and checks the model.\n    *   **Function:** It encapsulates a loop of model training, prediction, metric calculation, and conditional re-training based on error thresholds.\n    *   **Inputs:**\n        *   `starting_model` (initial model from `xgboost_train_on_csv_op.outputs['model']`).\n        *   `training_data` (from `drop_header_op` for training data).\n        *   `true_values` (from `drop_header_op` for true values).\n    *   **Internal Components & Logic:**\n        *   **`xgboost_train_on_csv_op` (inside graph)**: Trains the model further.\n            *   **Inputs:** `training_data`, `starting_model`, `label_column`, `objective`, `num_iterations`.\n            *   **Outputs:** `model`.\n        *   **`xgboost_predict_on_csv_op` (inside graph)**: Makes predictions using the current model.\n            *   **Inputs:** `data` (training data), `model`, `label_column`.\n            *   **Outputs:** `predictions`.\n        *   **`calculate_regression_metrics_from_csv_op` (inside graph)**: Calculates regression metrics.\n            *   **Inputs:** `true_values`, `predictions`.\n            *   **Outputs:** `metrics` (contains `mean_absolute_error`).\n        *   **`kfp.dsl.Condition`**: Checks if `metrics_task.outputs['mean_absolute_error']` is greater than `0.001`.\n            *   **If True:** The `train_until_low_error` graph component is recursively called with the newly trained model (`model`) as `starting_model`.\n            *   **If False:** The loop terminates, and the final `model` is returned.\n    *   **Outputs:** `output_model` (the final model when the error is low enough).\n\n7.  **`xgboost_predict_on_csv_op` (Final Prediction)**: This component makes final predictions using the best model.\n    *   **Function:** Uses the optimized model from the recursive training loop to make predictions on the original training data.\n    *   **Inputs:** `data` (from `drop_header_op` for training data), `model` (from `train_until_low_error.output`), `label_column` (set to `0`).\n    *   **Outputs:** `output` (the final predictions).\n\n8.  **`calculate_regression_metrics_from_csv_op` (Final Metrics)**: This component calculates the final regression metrics.\n    *   **Function:** Evaluates the performance of the best model based on its final predictions against the true values.\n    *   **Inputs:** `true_values` (from `drop_header_op` for true values), `predictions` (from `xgboost_predict_on_csv_op` final prediction).\n    *   **Outputs:** `metrics` (containing final regression metrics).\n\n**Control Flow:**\n\nThe pipeline executes sequentially:\n1.  `chicago_taxi_dataset_op` runs.\n2.  `pandas_transform_csv_op` runs, dependent on `chicago_taxi_dataset_op`.\n3.  Two instances of `drop_header_op` run in parallel, both dependent on `pandas_transform_csv_op`.\n4.  `xgboost_train_on_csv_op` (initial training) runs, dependent on the first `drop_header_op` (training data).\n5.  `train_until_low_error` (the recursive loop) runs, dependent on `xgboost_train_on_csv_op` (initial model) and both `drop_header_op` outputs.\n    *   **Crucially, the `train_until_low_error` graph component itself contains a conditional recursive call based on the `mean_absolute_error` metric. This forms a `while` loop, continuing training until the error condition is met.**\n6.  `xgboost_predict_on_csv_op` (final prediction) runs, dependent on the final model output from `train_until_low_error` and the first `drop_header_op` output (training data).\n7.  `calculate_regression_metrics_from_csv_op` (final metrics) runs, dependent on the final predictions from `xgboost_predict_on_csv_op` and the second `drop_header_op` output (true values).\n\n**Tools/Libraries:**\n\n*   **Kubeflow Pipelines (KFP):** Used for defining and orchestrating the pipeline.\n*   **Pandas:** Used within the `pandas_transform_csv_op` for data manipulation.\n*   **XGBoost:** Used for machine learning model training and prediction.\n*   **Custom Components:** Several components are loaded from external URLs (e.g., `Chicago_Taxi_Trips`, `XGBoost/Train`, `pandas/Transform_DataFrame`, `ml_metrics/Calculate_regression_metrics`, `tables/Remove_header`).\n\nThe goal is to generate the Python code for this Kubeflow Pipeline using the `@dsl.pipeline` and `@component` decorators, including the recursive graph component and its internal logic.", "completion": "#!/usr/bin/env python3\n# Copyright 2020 The Kubeflow Pipleines authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# This sample demonstrates continuous training using a train-eval-check recursive loop.\n# The main pipeline trains the initial model and then gradually trains the model\n# some more until the model evaluation metrics are good enough.\n\nimport kfp\nfrom kfp import components\n\n\nchicago_taxi_dataset_op = components.load_component_from_url('https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/datasets/Chicago_Taxi_Trips/component.yaml')\nxgboost_train_on_csv_op = components.load_component_from_url('https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/XGBoost/Train/component.yaml')\nxgboost_predict_on_csv_op = components.load_component_from_url('https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/XGBoost/Predict/component.yaml')\n\npandas_transform_csv_op = components.load_component_from_url('https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/pandas/Transform_DataFrame/in_CSV_format/component.yaml')\ndrop_header_op = kfp.components.load_component_from_url('https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/tables/Remove_header/component.yaml')\ncalculate_regression_metrics_from_csv_op = kfp.components.load_component_from_url('https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/ml_metrics/Calculate_regression_metrics/from_CSV/component.yaml')\n\n\n# This recursive sub-pipeline trains a model, evaluates it, calculates the metrics and checks them.\n# If the model error is too high, then more training is performed until the model is good.\n@kfp.dsl.graph_component\ndef train_until_low_error(starting_model, training_data, true_values):\n    # Training\n    model = xgboost_train_on_csv_op(\n        training_data=training_data,\n        starting_model=starting_model,\n        label_column=0,\n        objective='reg:squarederror',\n        num_iterations=50,\n    ).outputs['model']\n\n    # Predicting\n    predictions = xgboost_predict_on_csv_op(\n        data=training_data,\n        model=model,\n        label_column=0,\n    ).output\n\n    # Calculating the regression metrics\n    metrics_task = calculate_regression_metrics_from_csv_op(\n        true_values=true_values,\n        predicted_values=predictions,\n    )\n\n    # Checking the metrics\n    with kfp.dsl.Condition(metrics_task.outputs['mean_squared_error'] > 0.01):\n        # Training some more\n        train_until_low_error(\n            starting_model=model,\n            training_data=training_data,\n            true_values=true_values,\n        )\n\n\n# The main pipleine trains the initial model and then gradually trains the model some more until the model evaluation metrics are good enough.\ndef train_until_good_pipeline():\n    # Preparing the training data\n    training_data = chicago_taxi_dataset_op(\n        where='trip_start_timestamp >= \"2019-01-01\" AND trip_start_timestamp < \"2019-02-01\"',\n        select='tips,trip_seconds,trip_miles,pickup_community_area,dropoff_community_area,fare,tolls,extras,trip_total',\n        limit=10000,\n    ).output\n\n    # Preparing the true values\n    true_values_table = pandas_transform_csv_op(\n        table=training_data,\n        transform_code='df = df[[\"tips\"]]',\n    ).output\n\n    true_values = drop_header_op(true_values_table).output\n\n    # Initial model training\n    first_model = xgboost_train_on_csv_op(\n        training_data=training_data,\n        label_column=0,\n        objective='reg:squarederror',\n        num_iterations=100,\n    ).outputs['model']\n\n    # Recursively training until the error becomes low\n    train_until_low_error(\n        starting_model=first_model,\n        training_data=training_data,\n        true_values=true_values,\n    )\n\n\nif __name__ == '__main__':\n    kfp_endpoint=None\n    kfp.Client(host=kfp_endpoint).create_run_from_pipeline_func(train_until_good_pipeline, arguments={})"}
{"prompt": "Generate a Kubeflow Pipeline named `KServe pipeline` that performs the following steps:\n\nThis pipeline consists of **one** component.\n\n**Pipeline Description:**\nA pipeline for KServe.\n\n**Pipeline Parameters:**\n* `action`: Default value 'apply'.\n* `model_name`: Default value 'tensorflow-sample'.\n* `model_uri`: Default value 's3://mlpipeline/mnistdocker/models/detect-digits/'.\n* `namespace`: Default value 'kubeflow-user-example-com'.\n* `framework`: Default value 'tensorflow'.\n* `service_account`: Default value 'sa-minio-kserve'.\n\n**Component Details:**\n\n1.  **Component Name:** `kserve_op`\n    *   **Function:** This component is loaded from the URL `https://raw.githubusercontent.com/kubeflow/pipelines/master/components/kserve/component.yaml`. It is responsible for interacting with KServe to deploy or manage a model.\n    *   **Inputs:**\n        *   `action`: Passed directly from the pipeline parameter `action`.\n        *   `model_name`: Passed directly from the pipeline parameter `model_name`.\n        *   `model_uri`: Passed directly from the pipeline parameter `model_uri`.\n        *   `namespace`: Passed directly from the pipeline parameter `namespace`.\n        *   `framework`: Passed directly from the pipeline parameter `framework`.\n        *   `service_account`: Passed directly from the pipeline parameter `service_account`.\n    *   **Outputs:** (Not explicitly defined in the provided code, but typical KServe components might output service URL, status, etc.)\n    *   **Tools/Libraries:** KServe component.\n\n**Control Flow:**\nThe `kserve_op` component runs directly, taking all its inputs from the pipeline's input parameters. There are no explicit dependencies, parallel execution, or conditional logic defined.", "completion": "import kfp.compiler as compiler\nimport kfp.dsl as dsl\nfrom kfp import components\n\n# kfserving_op = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/'\n#                                                  'master/components/kubeflow/kfserving/component.yaml')\nkserve_op = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/'\n                                               'master/components/kserve/component.yaml')\n\n\n@dsl.pipeline(\n    name='KServe pipeline',\n    description='A pipeline for KServe.'\n)\ndef kservePipeline(\n        action='apply',\n        model_name='tensorflow-sample',\n        model_uri='s3://mlpipeline/mnistdocker/models/detect-digits/',\n        namespace='kubeflow-user-example-com',\n        framework='tensorflow',\n        service_account='sa-minio-kserve'):\n    kserve_op(action=action,\n            model_name=model_name,\n            model_uri=model_uri,\n            namespace=namespace,\n            framework=framework,\n            service_account=service_account)\n\n\n\nif __name__ == '__main__':\n    compiler.Compiler().compile(kservePipeline, 'pip.yaml')"}
{"prompt": "Generate a Kubeflow Pipeline named `End-to-End MNIST Pipeline` that performs a multi-step machine learning workflow including data download, model training, evaluation, export, and serving.\n\nThe pipeline consists of **six** main components:\n\n1.  **Download Dataset**: This component, implemented by the `download_dataset` function, takes `input_bucket` (string) and `dataset_name` (string) as inputs. It downloads the specified dataset and outputs the dataset artifact. Caching for this component is disabled (`max_cache_staleness = \"P0D\"`).\n2.  **Train Model**: This component, implemented by the `train_model` function, takes the dataset artifact output from the `Download Dataset` component as input. It trains a model and outputs the trained model artifact. Caching for this component is disabled.\n3.  **Evaluate Model**: This component, implemented by the `evaluate_model` function, takes the dataset artifact from `Download Dataset` and the trained model artifact from `Train Model` as inputs. It evaluates the model's performance and outputs the evaluation results. Caching for this component is disabled.\n4.  **Export Model**: This component, implemented by the `export_model` function, takes the trained model artifact from `Train Model`, the evaluation results from `Evaluate Model`, `export_bucket` (string), `model_name` (string), and `model_version` (string) as inputs. It exports the model to the specified location. Caching for this component is disabled.\n5.  **KFServing Deploy**: This component is loaded from a remote YAML definition (`https://raw.githubusercontent.com/kubeflow/pipelines/master/components/kubeflow/kfserving/component.yaml`). It takes `action` (string, set to \"apply\"), `model_uri` (string, constructed as `s3://{export_bucket}/{model_name}`), `model_name` (string, set to \"mnist\"), `namespace` (string, set to \"default\"), `framework` (string, set to \"tensorflow\"), and `watch_timeout` (string, set to \"300\") as inputs. Its purpose is to deploy the exported model using KFServing. This component explicitly depends on the completion of the `Export Model` component (`kfservingOp.after(exportOp)`). Caching for this component is disabled.\n\nThe pipeline defines the following default parameters:\n-   `input_bucket`: \"pipelines-tutorial-data\"\n-   `dataset_name`: \"datasets.tar.gz\"\n-   `export_bucket`: \"models\"\n-   `model_name`: \"mnist\"\n-   `model_version`: \"1\"\n\nAll user-defined components (`download_dataset`, `train_model`, `evaluate_model`, `export_model`) should use `drobnov1994/example:kubeflow_tensorflow_v1` as their `BASE_IMAGE`.\n\nThe pipeline also includes a global operation transformer named `op_transformer` that adds the `sidecar.istio.io/inject: false` pod annotation to all operations.\n\nThe pipeline should be run with a KFP client, connecting to `http://127.0.0.1:8080`, creating a run from the `train_and_serve` function with the specified arguments: `input_bucket` (from `INPUT_BUCKET` constant), `dataset_name` (\"datasets.tar.gz\"), `export_bucket` (\"models\"), `model_name` (\"mnist\"), and `model_version` (\"1\").", "completion": "import kfp\nimport kfp.components as components\nimport kfp.dsl as dsl\n\nfrom components.download_component import download_dataset\nfrom components.train_component import train_model\nfrom components.evaluate_component import evaluate_model\nfrom components.export_component import export_model\n\nBASE_IMAGE = \"drobnov1994/example:kubeflow_tensorflow_v1\"\nINPUT_BUCKET = \"pipelines-tutorial-data\"\nNAMESPACE = \"default\"\n\nkfserving = components.load_component_from_url(\n    \"https://raw.githubusercontent.com/kubeflow/pipelines/master/components/kubeflow/kfserving/component.yaml\"\n)\n\n\ndef train_and_serve(\n    input_bucket: str,\n    dataset_name: str,\n    export_bucket: str,\n    model_name: str,\n    model_version: str,\n):\n    downloadOp = components.func_to_container_op(\n        download_dataset, base_image=BASE_IMAGE\n    )(input_bucket, dataset_name)\n    downloadOp.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n    trainOp = components.func_to_container_op(train_model, base_image=BASE_IMAGE)(\n        downloadOp.output\n    )\n    trainOp.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n    evaluateOp = components.func_to_container_op(evaluate_model, base_image=BASE_IMAGE)(\n        downloadOp.output, trainOp.output\n    )\n    evaluateOp.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n    exportOp = components.func_to_container_op(export_model, base_image=BASE_IMAGE)(\n        trainOp.output, evaluateOp.output, export_bucket, model_name, model_version\n    )\n    exportOp.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n    kfservingOp = kfserving(\n        action=\"apply\",\n        model_uri=f\"s3://{export_bucket}/{model_name}\",\n        model_name=\"mnist\",\n        namespace=NAMESPACE,\n        framework=\"tensorflow\",\n        watch_timeout=\"300\",\n    )\n    kfservingOp.after(exportOp)\n    kfservingOp.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n\n\ndef op_transformer(op):\n    op.add_pod_annotation(name=\"sidecar.istio.io/inject\", value=\"false\")\n    return op\n\n\n@dsl.pipeline(\n    name=\"End-to-End MNIST Pipeline\",\n    description=\"A sample pipeline to demonstrate \"\n    \"multi-step model training,\"\n    \"evaluation, export, and serving\",\n)\ndef mnist_pipeline(\n    input_bucket: str = \"pipelines-tutorial-data\",\n    dataset_name: str = \"datasets.tar.gz\",\n    export_bucket: str = \"models\",\n    model_name: str = \"mnist\",\n    model_version: str = \"1\",\n):\n    train_and_serve(\n        input_bucket=input_bucket,\n        dataset_name=dataset_name,\n        export_bucket=export_bucket,\n        model_name=model_name,\n        model_version=model_version,\n    )\n    dsl.get_pipeline_conf().add_op_transformer(op_transformer)\n\n\narguments = {\n    \"input_bucket\": INPUT_BUCKET,\n    \"dataset_name\": \"datasets.tar.gz\",\n    \"export_bucket\": \"models\",\n    \"model_name\": \"mnist\",\n    \"model_version\": \"1\",\n}\n\nclient = kfp.Client(host=\"http://127.0.0.1:8080\")\nclient.create_run_from_pipeline_func(train_and_serve, arguments=arguments)"}
{"prompt": "Generate a Kubeflow Pipeline named `stackdemos_ml-kbf_components_golang_component` that performs the following steps:\n\nThe pipeline does not have a `@dsl.pipeline` decorator visible in the provided code snippet, but it defines two reusable component factory functions: `processing_op` and `http_download_op`.\n\nIt uses two custom component definitions:\n\n1.  **`processing_op`**:\n    *   **Function**: A template function designed to create `ContainerOp` instances for processing tasks. It dynamically generates the component name from the script name, sets the container image, command, arguments, and file outputs.\n    *   **Inputs**:\n        *   `script` (string): The path to the script to be executed as the main command.\n        *   `image` (string, optional): The container image to use. If not provided and running in an IPython environment, it attempts to retrieve `GOLANG_IMAGE` from the IPython user namespace.\n        *   `arguments` (list, optional): A list of arguments to pass to the script.\n        *   `file_outputs` (dict, optional): A dictionary mapping output names to their file paths within the container.\n    *   **Outputs**: Returns a `kfp.dsl.ContainerOp` instance.\n    *   **Tools/Libraries**: Uses `re` for name sanitization and `os` for path manipulation. It also conditionally checks for `IPython` to fetch the image.\n\n2.  **`http_download_op`**:\n    *   **Function**: Creates a `ContainerOp` to download a file from a given URL using `curl`, with an `md5sum` pre-check for idempotency.\n    *   **Inputs**:\n        *   `url` (string): The URL of the file to download.\n        *   `download_to` (string): The destination path where the file should be saved.\n        *   `md5sum` (string): The expected MD5 checksum of the file.\n    *   **Outputs**: Returns a `kfp.dsl.ContainerOp` instance.\n    *   **Tools/Libraries**: Uses `appropriate/curl` as the container image. The command involves `sh`, `test`, `awk`, and `curl` for the download and checksum verification.\n\n**Control Flow**:\nThe provided code only defines component factory functions. It does not define a pipeline decorated with `@dsl.pipeline`, so there is no explicit control flow like `after` or `parallelFor` specified at this level. These functions are intended to be called within a larger Kubeflow pipeline definition to instantiate and connect components.", "completion": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef processing_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('GOLANG_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=[script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )"}
{"prompt": "Generate a Kubeflow Pipeline named `my-pipeline` that performs the following sequence of operations:\n\nThe pipeline consists of three components.\n\n1.  **`my-out-cop0`**: This component uses the `python:alpine3.6` image. Its purpose is to generate a JSON list of integers ranging from 20 to 30 (inclusive) and output it to a file named `/tmp/out.json`. This output is then captured as the `out` artifact, which serves as an input for subsequent steps.\n\n2.  **`my-in-cop1`**: This component uses the `library/bash:4.4.23` image. It is executed in parallel (`dsl.ParallelFor`) for each item in the list produced by `my-out-cop0`. For each `item`, it simply prints a message indicating the processing of that specific item.\n\n3.  **`my-out-cop2`**: This component uses the `library/bash:4.4.23` image. Its purpose is to display the entire output list generated by the `my-out-cop0` component. It depends on `my-out-cop0` completing successfully to access its output.\n\nThe pipeline utilizes the `kfp` library for defining and compiling the pipeline. The control flow involves an initial component producing a list, followed by a parallel execution of another component over the items of that list, and finally, a component that consumes the initial list's full output.", "completion": "# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport kfp\nfrom kfp import dsl\n\n\n@dsl.pipeline(name='my-pipeline')\ndef pipeline():\n    op0 = dsl.ContainerOp(\n        name=\"my-out-cop0\",\n        image='python:alpine3.6',\n        command=[\"sh\", \"-c\"],\n        arguments=[\n            'python -c \"import json; import sys; json.dump([i for i in range(20, 31)], open(\\'/tmp/out.json\\', \\'w\\'))\"'],\n        file_outputs={'out': '/tmp/out.json'},\n    )\n\n    with dsl.ParallelFor(op0.output) as item:\n        op1 = dsl.ContainerOp(\n            name=\"my-in-cop1\",\n            image=\"library/bash:4.4.23\",\n            command=[\"sh\", \"-c\"],\n            arguments=[\"echo do output op1 item: %s\" % item],\n        )\n\n    op_out = dsl.ContainerOp(\n        name=\"my-out-cop2\",\n        image=\"library/bash:4.4.23\",\n        command=[\"sh\", \"-c\"],\n        arguments=[\"echo do output op2, outp: %s\" % op0.output],\n    )\n\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(pipeline, __file__ + '.yaml')"}
{"prompt": "Generate a Kubeflow Pipeline named `TFX Taxi Cab Classification Pipeline Example` that performs classification with model analysis based on a public BigQuery dataset.\n\nThe pipeline consists of **7 distinct components** and utilizes several external YAML definitions loaded from a GitHub repository, specifically `II-VSB-II/TaxiClassificationKubeflowPipelineMinio`. It also uses `kfp` library features like `dsl.VolumeOp` and `dsl.ContainerOp`.\n\n**Pipeline Parameters:**\n*   `project` (string)\n*   `output` (string, default: `/mnt/shared`)\n*   `column_names` (string, default: `/mnt/shared/pipelines/column-names.json`)\n*   `key_columns` (string, default: `trip_start_timestamp`)\n*   `train` (string, default: `/mnt/shared/pipelines/train.csv`)\n*   `evaluation` (string, default: `/mnt/shared/pipelines/eval.csv`)\n*   `mode` (string, default: `local`)\n*   `preprocess_module` (string, default: `/mnt/shared/pipelines/preprocessing.py`)\n*   `learning_rate` (float, default: `0.1`)\n*   `hidden_layer_size` (string, default: `1500`)\n*   `steps` (integer, default: `3000`)\n*   `analyze_slice_column` (string, default: `trip_start_hour`)\n\n**Environment Variables:**\nThe pipeline requires the following environment variables to be set, likely for MinIO integration:\n*   `MINIO_SECRET_KEY` with value `minio123`\n*   `MINIO_ACCESS_KEY` with value `minio`\n*   `MINIO_ENDPOINT` with value `minio-service:9000`\n\n**Pipeline Logic and Component Definitions:**\n\nThe pipeline's execution flow is conditional based on the `platform` variable (not explicitly a pipeline parameter but determined outside the `@dsl.pipeline` decorator).\n\n**If `platform` is NOT 'GCP'**:\n1.  **`create_pvc` (dsl.VolumeOp):**\n    *   **Function:** Creates a Persistent Volume Claim (PVC) named `pipeline-pvc` with `RWM` (ReadWriteMany) mode and size `1Gi`. This PVC is intended to store shared data for the pipeline.\n    *   **Output:** A volume that can be mounted by subsequent components.\n\n2.  **`checkout` (dsl.ContainerOp):**\n    *   **Function:** This component is conditional and only runs if `proxy` is not an empty string. Its exact function is not fully visible in the provided snippet but implies a checkout operation (e.g., cloning a Git repository) perhaps through a proxy.\n    *   **Dependencies:** Depends on the successful creation of `create_pvc` (mounts its volume).\n\n**Common Components (regardless of `platform`):**\n\nThe following components are loaded from external YAML files and are likely custom containerized operations. Each component likely sets `minio_access_key`, `minio_secret_key`, and `minio_endpoint` environment variables, and mounts the `vop.volume` (if `platform` is not 'GCP') or similar shared storage.\n\n1.  **`dataflow-tf-data-validation` (from `tfdv_component.yaml`):**\n    *   **Function:** Performs data validation using TensorFlow Data Validation (TFDV).\n    *   **Inputs:** `data` (from `train` parameter), `evaluation_data` (from `evaluation` parameter), `column_names_path` (from `column_names` parameter), `key_columns` (from `key_columns` parameter).\n    *   **Outputs:** `schema` (output path derived from `output_template`).\n    *   **Dependencies:** If `platform` is not 'GCP', it depends on `create_pvc` and optionally `checkout`.\n\n2.  **`dataflow-tf-transform` (from `tft_component.yaml`):**\n    *   **Function:** Performs data transformation using TensorFlow Transform (TFT).\n    *   **Inputs:** `train_data` (from `train` parameter), `evaluation_data` (from `evaluation` parameter), `schema` (from `dataflow-tf-data-validation`'s `schema` output), `preprocess_module` (from `preprocess_module` parameter).\n    *   **Outputs:** `transformed_data_path`, `transformed_eval_data_path`, `transform_output_path`.\n    *   **Dependencies:** Depends on `dataflow-tf-data-validation`.\n\n3.  **`tf-train` (from `dnntrainer_component.yaml`):**\n    *   **Function:** Trains a Deep Neural Network (DNN) model.\n    *   **Inputs:** `transformed_data_path`, `transformed_eval_data_path`, `transform_output_path` (all from `dataflow-tf-transform`), `schema` (from `dataflow-tf-data-validation`), `learning_rate` (from `learning_rate` parameter), `hidden_layer_size` (from `hidden_layer_size` parameter), `steps` (from `steps` parameter), `target_lambda` (defined internally), `target_class_lambda` (defined internally).\n    *   **Outputs:** `model` (output path derived from `output_template`).\n    *   **Dependencies:** Depends on `dataflow-tf-transform`.\n\n4.  **`dataflow-tf-model-analyze` (from `tfma_component.yaml`):**\n    *   **Function:** Analyzes the trained model using TensorFlow Model Analysis (TFMA).\n    *   **Inputs:** `model` (from `tf-train`), `evaluation_data` (from `evaluation` parameter), `schema` (from `dataflow-tf-data-validation`), `analyze_slice_column` (from `analyze_slice_column` parameter).\n    *   **Dependencies:** Depends on `tf-train`.\n\n5.  **`dataflow-tf-predict` (from `predict_component.yaml`):**\n    *   **Function:** Performs predictions using the trained model.\n    *   **Inputs:** `model` (from `tf-train`), `data` (from `evaluation` parameter).\n    *   **Outputs:** `predictions` (output path derived from `output_template`).\n    *   **Dependencies:** Depends on `tf-train`.\n\n6.  **`confusion-matrix` (from `confusion_matrix_component.yaml`):**\n    *   **Function:** Calculates and visualizes the confusion matrix.\n    *   **Inputs:** `predictions` (from `dataflow-tf-predict`), `target_lambda` (defined internally), `target_class_lambda` (defined internally).\n    *   **Dependencies:** Depends on `dataflow-tf-predict`.\n\n7.  **`roc` (from `roc_component.yaml`):**\n    *   **Function:** Calculates and visualizes the Receiver Operating Characteristic (ROC) curve.\n    *   **Inputs:** `predictions` (from `dataflow-tf-predict`), `target_lambda` (defined internally), `target_class_lambda` (defined internally).\n    *   **Dependencies:** Depends on `dataflow-tf-predict`.\n\n**Control Flow:**\n*   `dataflow-tf-data-validation` is the initial component (after PVC creation/checkout if applicable).\n*   `dataflow-tf-transform` runs `after` `dataflow-tf-data-validation`.\n*   `tf-train` runs `after` `dataflow-tf-transform`.\n*   `dataflow-tf-model-analyze` runs `after` `tf-train`.\n*   `dataflow-tf-predict` runs `after` `tf-train`.\n*   `confusion-matrix` and `roc` run in parallel, both `after` `dataflow-tf-predict`.\n\n**Tools and Libraries:**\n*   Kubeflow Pipelines (kfp, dsl, components)\n*   TensorFlow Data Validation (TFDV)\n*   TensorFlow Transform (TFT)\n*   TensorFlow Model Analysis (TFMA)\n*   Kubernetes (for PVC and environment variables)\n*   MinIO (implied by environment variables)\n*   Potentially other libraries within the custom components (e.g., pandas, numpy for data handling, scikit-learn for metrics).", "completion": "import kfp\nfrom kfp import components\nfrom kfp import dsl\nfrom kfp import gcp\nfrom kfp import onprem\nfrom kubernetes.client.models import V1EnvVar\n\n\nsecretKey = V1EnvVar(name='MINIO_SECRET_KEY', value='minio123')\naccessKey = V1EnvVar(name='MINIO_ACCESS_KEY', value='minio')\nminio_endpoint = V1EnvVar(name='MINIO_ENDPOINT', value='minio-service:9000')\n\nplatform = 'local'\n\n#proxy=\"http://test:8080\"\nproxy = \"\"\n\ndataflow_tf_data_validation_op  = components.load_component_from_url(\n  'https://raw.githubusercontent.com/II-VSB-II/TaxiClassificationKubeflowPipelineMinio/main/yamls/tfdv_component.yaml')\ndataflow_tf_transform_op        = components.load_component_from_url(\n  'https://raw.githubusercontent.com/II-VSB-II/TaxiClassificationKubeflowPipelineMinio/main/yamls/tft_component.yaml')\ntf_train_op                     = components.load_component_from_url(\n  'https://raw.githubusercontent.com/II-VSB-II/TaxiClassificationKubeflowPipelineMinio/main/yamls/dnntrainer_component.yaml')\ndataflow_tf_model_analyze_op    = components.load_component_from_url(\n  'https://raw.githubusercontent.com/II-VSB-II/TaxiClassificationKubeflowPipelineMinio/main/yamls/tfma_component.yaml')\ndataflow_tf_predict_op          = components.load_component_from_url(\n  'https://raw.githubusercontent.com/II-VSB-II/TaxiClassificationKubeflowPipelineMinio/main/yamls/predict_component.yaml')\n\nconfusion_matrix_op             = components.load_component_from_url(\n  'https://raw.githubusercontent.com/II-VSB-II/TaxiClassificationKubeflowPipelineMinio/main/yamls/confusion_matrix_component.yaml')\nroc_op                          = components.load_component_from_url(\n  'https://raw.githubusercontent.com/II-VSB-II/TaxiClassificationKubeflowPipelineMinio/main/yamls/roc_component.yaml')\n\n@dsl.pipeline(\n  name='TFX Taxi Cab Classification Pipeline Example',\n  description='Example pipeline that does classification with model analysis based on a public BigQuery dataset.'\n)\ndef taxi_cab_classification(\n    project,\n    output=\"/mnt/shared\",\n    column_names='/mnt/shared/pipelines/column-names.json',\n    key_columns='trip_start_timestamp',\n    train='/mnt/shared/pipelines/train.csv',\n    evaluation='/mnt/shared/pipelines/eval.csv',\n    mode='local',\n    preprocess_module='/mnt/shared/pipelines/preprocessing.py',\n    learning_rate=0.1,\n    hidden_layer_size='1500',\n    steps=3000,\n    analyze_slice_column='trip_start_hour'\n):\n    output_template = str(output) + '/{{workflow.uid}}/{{pod.name}}/data'\n    target_lambda = \"\"\"lambda x: (x['target'] > x['fare'] * 0.2)\"\"\"\n    target_class_lambda = \"\"\"lambda x: 1 if (x['target'] > x['fare'] * 0.2) else 0\"\"\"\n\n    tf_server_name = 'taxi-cab-classification-model-{{workflow.uid}}'\n\n    if platform != 'GCP':\n        vop = dsl.VolumeOp(\n            name=\"create_pvc\",\n            resource_name=\"pipeline-pvc\",\n            modes=dsl.VOLUME_MODE_RWM,\n            size=\"1Gi\"\n        )\n        if proxy != \"\":\n            checkout = dsl.ContainerOp(\n            name=\"checkout\",\n            image=\"alpine/git:latest\",\n            command=[\"git\", \"clone\", \"https://github.com/II-VSB-II/TaxiClassificationKubeflowPipelineMinio.git\", str(output) + \n                     \"/pipelines\", \"-c\", \"http.proxy={}\".format(proxy)],\n        ).apply(onprem.mount_pvc(vop.outputs[\"name\"], 'local-storage', output))\n        else:\n            checkout = dsl.ContainerOp(\n            name=\"checkout\",\n            image=\"alpine/git:latest\",\n            command=[\"git\", \"clone\", \"https://github.com/II-VSB-II/TaxiClassificationKubeflowPipelineMinio.git\", str(output) + \"/pipelines\"],\n        ).apply(onprem.mount_pvc(vop.outputs[\"name\"], 'local-storage', output))\n            \n        \n        checkout.after(vop)\n\n    validation = dataflow_tf_data_validation_op(\n        inference_data=train,\n        validation_data=evaluation,\n        column_names=column_names,\n        key_columns=key_columns,\n        gcp_project=project,\n        run_mode=mode,\n        validation_output=output_template,\n    )\n    if platform != 'GCP':\n        validation.after(checkout)\n\n    preprocess = dataflow_tf_transform_op(\n        training_data_file_pattern=train,\n        evaluation_data_file_pattern=evaluation,\n        schema=validation.outputs['schema'],\n        gcp_project=project,\n        run_mode=mode,\n        preprocessing_module=preprocess_module,\n        transformed_data_dir=output_template\n    )\n\n    training = tf_train_op(\n        transformed_data_dir=preprocess.output,\n        schema=validation.outputs['schema'],\n        learning_rate=learning_rate,\n        hidden_layer_size=hidden_layer_size,\n        steps=steps,\n        target='tips',\n        preprocessing_module=preprocess_module,\n        training_output_dir=output_template\n    )\n\n    analysis = dataflow_tf_model_analyze_op(\n        model=training.output,\n        evaluation_data=evaluation,\n        schema=validation.outputs['schema'],\n        gcp_project=project,\n        run_mode=mode,\n        slice_columns=analyze_slice_column,\n        analysis_results_dir=output_template\n    ).add_env_variable(secretKey).add_env_variable(accessKey).add_env_variable(minio_endpoint)\n\n    prediction = dataflow_tf_predict_op(\n        data_file_pattern=evaluation,\n        schema=validation.outputs['schema'],\n        target_column='tips',\n        model=training.output,\n        run_mode=mode,\n        gcp_project=project,\n        predictions_dir=output_template\n    ).add_env_variable(secretKey).add_env_variable(accessKey).add_env_variable(minio_endpoint)\n\n\n    cm = confusion_matrix_op(\n        predictions=prediction.output,\n        target_lambda=target_lambda,\n        output_dir=output_template\n    ).add_env_variable(secretKey).add_env_variable(accessKey).add_env_variable(minio_endpoint)\n\n    roc = roc_op(\n        predictions_dir=prediction.output,\n        target_lambda=target_class_lambda,\n        output_dir=output_template\n    ).add_env_variable(secretKey).add_env_variable(accessKey).add_env_variable(minio_endpoint)\n\n\n    steps = [validation, preprocess, training, analysis, prediction, cm, roc]\n    \n    for step in steps:\n        step.apply(onprem.mount_pvc(vop.outputs[\"name\"], 'local-storage', output))\n\n\nif __name__ == '__main__':\n    kfp.compiler.Compiler().compile(taxi_cab_classification, \"TaxiPipelineMinio\" + '.zip')"}
{"prompt": "Generate a Kubeflow Pipeline named `stackdemos-kfappx-components-training-component` that performs the following steps:\n\nThis pipeline includes two utility functions that define reusable container operations: `training_op` and `http_download_op`.\n\n1.  **`training_op` Function:**\n    *   **Purpose:** This function serves as a template for creating Kubeflow `ContainerOp` components that execute Python training scripts. It encapsulates common logic for defining training steps.\n    *   **Inputs:**\n        *   `script` (str): The path to the Python script to be executed.\n        *   `image` (str, optional): The Docker image to use for the container. If not provided and running in an IPython environment, it attempts to retrieve the `TRAINING_IMAGE` global variable.\n        *   `arguments` (list, optional): A list of command-line arguments to pass to the script.\n        *   `file_outputs` (dict, optional): A dictionary defining file outputs for the component.\n    *   **Functionality:**\n        *   Determines the Docker image to use, prioritizing the explicit `image` parameter, then `TRAINING_IMAGE` from IPython, and raising an error if neither is found.\n        *   Constructs a `ContainerOp` with a sanitized name derived from the script name.\n        *   Sets the command to `/usr/local/bin/python` followed by the `script` path.\n        *   Passes any specified `arguments` and defines `file_outputs`.\n    *   **Tools/Libraries:** `kfp.dsl.ContainerOp`, `urllib.parse`, `os`, `re`.\n\n2.  **`http_download_op` Function:**\n    *   **Purpose:** This function defines a Kubeflow `ContainerOp` for securely downloading artifacts from a URL with an MD5 checksum pre-check.\n    *   **Inputs:**\n        *   `url` (str): The URL of the artifact to download.\n        *   `download_to` (str): The local path where the artifact should be saved.\n        *   `md5sum` (str): The expected MD5 checksum of the artifact.\n    *   **Functionality:**\n        *   Creates a `ContainerOp` named `download-artifact`.\n        *   Uses the `appropriate/curl` Docker image.\n        *   Executes a shell script (`sh -c`) that performs the following logic:\n            *   Checks if the MD5 sum of the `download_to` file (if it exists) matches the provided `md5sum`.\n            *   If they match, it prints \"Skipping due to [download_to] has been already downloaded\".\n            *   Otherwise, it uses `curl` to download the file from the `url` to the `download_to` path, creating directories as needed (`--create-dirs`).\n    *   **Tools/Libraries:** `kfp.dsl.ContainerOp`, `curl` (within the Docker image).\n\nThe control flow is defined by how these functions are called within a larger Kubeflow pipeline; they are building blocks, not a complete pipeline themselves. Dependencies and specific sequencing (`after` or `parallelFor`) would be dictated by the pipeline definition that utilizes these functions.", "completion": "from kfp.dsl import ContainerOp\nfrom urllib.parse import urlparse\nimport os, re\n\ndef _is_ipython():\n    \"\"\"Returns whether we are running in notebook.\"\"\"\n    try:\n        import IPython\n    except ImportError:\n        return False\n    return True\n\ndef training_op(script, image=None, arguments=[], file_outputs={}):\n    \"\"\" A template function to encapsulate similar container ops\n    \"\"\"\n\n    if not image and _is_ipython():\n        from IPython import get_ipython\n        image = get_ipython().user_ns.get('TRAINING_IMAGE')\n\n    if not image:\n        raise ValueError(f\"\"\"\n            `image` parameter is missing.\n            If you run in Jupyter Notebook you can also define a global var TRAINING_IMAGE\n        \"\"\")\n\n    return ContainerOp(\n        name=re.sub(r'[\\W_]+', '-', os.path.splitext(script.lower())[0]),\n        image=image,\n        command=['/usr/local/bin/python', script],\n        arguments=arguments,\n        file_outputs=file_outputs,\n    )\n\ndef http_download_op(url, download_to, md5sum):\n    \"\"\" Download with curl and md5sum pre-check\n    \"\"\"\n    return ContainerOp(\n        name='download-artifact',\n        image='appropriate/curl',\n        command=['sh', '-c'],\n        arguments=[f'''\n            test '{md5sum}' = \"$({download_to} | awk '{{print $1;}}')\" \\\n            && echo \"Skipping due to {download_to} has been already downloaded\" \\\n            || curl -#Lv --create-dirs -o {download_to} {url}\n        ''']\n    )"}
{"prompt": "Generate a Kubeflow Pipeline named `Spark Operator job pipeline` that performs the following steps:\n\n**Pipeline Description:** \"Spark Operator job pipeline\"\n\n**Overall Structure:**\nThe pipeline orchestrates the deployment and monitoring of a Spark application using the Kubeflow Spark Operator.\n\n**Components:**\n\n1.  **`spark_job_op`**:\n    *   **Function:** Applies a Kubernetes SparkApplication manifest to create a Spark job.\n    *   **Inputs:**\n        *   `object`: A JSON string representation of the Spark job definition. This definition is dynamically generated by calling `get_spark_job_definition()` which reads `spark-job-python.yaml` and adds a unique epoch timestamp to the job name.\n    *   **Outputs:**\n        *   `name`: The name of the created Spark application.\n    *   **Tools/Libraries:** Uses `kfp.components.load_component_from_file(\"k8s-apply-component.yaml\")`.\n    *   **Caching:** Caching is disabled (`max_cache_staleness = \"P0D\"`) to ensure the Spark job is always applied.\n\n2.  **`spark_application_status_op`**:\n    *   **Function:** This is a **graph component** (recursive function) named `graph_component_spark_app_status` that continuously checks the status of the Spark application until it reaches a \"COMPLETED\" state. It introduces a 5-second delay between checks.\n    *   **Inputs:**\n        *   `input_application_name`: The name of the Spark application to monitor (obtained from `spark_job_op.outputs[\"name\"]`).\n    *   **Outputs:**\n        *   `applicationstate`: The current state of the Spark application.\n        *   `name`: The name of the application being monitored (passed through recursively).\n    *   **Tools/Libraries:** Uses `kfp.components.load_component_from_file(\"k8s-get-component.yaml\")` to query Kubernetes for the Spark application status.\n    *   **Caching:** Caching is disabled (`max_cache_staleness = \"P0D\"`) for the status check to ensure real-time updates.\n\n3.  **`print_message`**:\n    *   **Function:** Prints a message indicating that the Spark job has completed.\n    *   **Inputs:** A string message, which includes the `spark_job_name` (derived from `spark_job_op.outputs[\"name\"]`).\n    *   **Tools/Libraries:** Uses an `alpine:3.6` image with a simple `echo` command.\n    *   **Caching:** Caching is disabled (`max_cache_staleness = \"P0D\"`) to ensure the completion message is always printed.\n\n**Control Flow:**\n\n*   `spark_job_op` is the initial step, responsible for submitting the Spark application.\n*   `spark_application_status_op` runs **after** `spark_job_op`, ensuring that status monitoring only begins once the Spark job has been submitted. This component is a recursive graph component that will repeatedly check the Spark application's status until it's \"COMPLETED\".\n*   `print_message` runs **after** `spark_application_status_op`, ensuring the completion message is displayed only after the Spark application has successfully finished.\n\n**External Files/Dependencies:**\n*   `spark-job-python.yaml`: Defines the Spark job manifest.\n*   `k8s-apply-component.yaml`: Defines a Kubeflow component for applying Kubernetes resources.\n*   `k8s-get-component.yaml`: Defines a Kubeflow component for getting Kubernetes resources.\n\nThe pipeline should be compiled to `spark_job_pipeline.yaml`.", "completion": "import json\nimport time\nimport yaml\n\nimport kfp.components as comp\nimport kfp.dsl as dsl\n\nSPARK_COMPLETED_STATE = \"COMPLETED\"\nSPARK_APPLICATION_KIND = \"sparkapplications\"\n\n\ndef get_spark_job_definition():\n    \"\"\"\n    Read Spark Operator job manifest file and return the corresponding dictionary and\n    add some randomness in the job name\n    :return: dictionary defining the spark job\n    \"\"\"\n    # Read manifest file\n    with open(\"spark-job-python.yaml\", \"r\") as stream:\n        spark_job_manifest = yaml.safe_load(stream)\n\n    # Add epoch time in the job name\n    epoch = int(time.time())\n    spark_job_manifest[\"metadata\"][\"name\"] = spark_job_manifest[\"metadata\"][\"name\"].format(epoch=epoch)\n\n    return spark_job_manifest\n\n\ndef print_op(msg):\n    \"\"\"\n    Op to print a message.\n    \"\"\"\n    return dsl.ContainerOp(\n        name=\"Print message.\",\n        image=\"alpine:3.6\",\n        command=[\"echo\", msg],\n    )\n\n\n@dsl.graph_component  # Graph component decorator is used to annotate recursive functions\ndef graph_component_spark_app_status(input_application_name):\n    k8s_get_op = comp.load_component_from_file(\"k8s-get-component.yaml\")\n    check_spark_application_status_op = k8s_get_op(\n        name=input_application_name,\n        kind=SPARK_APPLICATION_KIND\n    )\n    # Remove cache\n    check_spark_application_status_op.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n\n    time.sleep(5)\n    with dsl.Condition(check_spark_application_status_op.outputs[\"applicationstate\"] != SPARK_COMPLETED_STATE):\n        graph_component_spark_app_status(check_spark_application_status_op.outputs[\"name\"])\n\n\n@dsl.pipeline(\n    name=\"Spark Operator job pipeline\",\n    description=\"Spark Operator job pipeline\"\n)\ndef spark_job_pipeline():\n\n    # Load spark job manifest\n    spark_job_definition = get_spark_job_definition()\n\n    # Load the kubernetes apply component\n    k8s_apply_op = comp.load_component_from_file(\"k8s-apply-component.yaml\")\n\n    # Execute the apply command\n    spark_job_op = k8s_apply_op(object=json.dumps(spark_job_definition))\n\n    # Fetch spark job name\n    spark_job_name = spark_job_op.outputs[\"name\"]\n\n    # Remove cache for the apply operator\n    spark_job_op.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n\n    spark_application_status_op = graph_component_spark_app_status(spark_job_op.outputs[\"name\"])\n    spark_application_status_op.after(spark_job_op)\n\n    print_message = print_op(f\"Job {spark_job_name} is completed.\")\n    print_message.after(spark_application_status_op)\n    print_message.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n\n\nif __name__ == \"__main__\":\n    # Compile the pipeline\n    import kfp.compiler as compiler\n    import logging\n    logging.basicConfig(level=logging.INFO)\n    pipeline_func = spark_job_pipeline\n    pipeline_filename = pipeline_func.__name__ + \".yaml\"\n    compiler.Compiler().compile(pipeline_func, pipeline_filename)\n    logging.info(f\"Generated pipeline file: {pipeline_filename}.\")"}
